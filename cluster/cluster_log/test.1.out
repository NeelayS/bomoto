Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=1, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 56-111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851853
Iteration 2/25 | Loss: 0.00134878
Iteration 3/25 | Loss: 0.00114841
Iteration 4/25 | Loss: 0.00112609
Iteration 5/25 | Loss: 0.00112187
Iteration 6/25 | Loss: 0.00112158
Iteration 7/25 | Loss: 0.00112158
Iteration 8/25 | Loss: 0.00112158
Iteration 9/25 | Loss: 0.00112158
Iteration 10/25 | Loss: 0.00112158
Iteration 11/25 | Loss: 0.00112158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011215789709240198, 0.0011215789709240198, 0.0011215789709240198, 0.0011215789709240198, 0.0011215789709240198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011215789709240198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94122928
Iteration 2/25 | Loss: 0.00043068
Iteration 3/25 | Loss: 0.00043067
Iteration 4/25 | Loss: 0.00043067
Iteration 5/25 | Loss: 0.00043067
Iteration 6/25 | Loss: 0.00043067
Iteration 7/25 | Loss: 0.00043067
Iteration 8/25 | Loss: 0.00043067
Iteration 9/25 | Loss: 0.00043067
Iteration 10/25 | Loss: 0.00043067
Iteration 11/25 | Loss: 0.00043067
Iteration 12/25 | Loss: 0.00043067
Iteration 13/25 | Loss: 0.00043067
Iteration 14/25 | Loss: 0.00043067
Iteration 15/25 | Loss: 0.00043067
Iteration 16/25 | Loss: 0.00043067
Iteration 17/25 | Loss: 0.00043067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004306667251512408, 0.0004306667251512408, 0.0004306667251512408, 0.0004306667251512408, 0.0004306667251512408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004306667251512408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043067
Iteration 2/1000 | Loss: 0.00002940
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001999
Iteration 7/1000 | Loss: 0.00001960
Iteration 8/1000 | Loss: 0.00001929
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001872
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001862
Iteration 15/1000 | Loss: 0.00001862
Iteration 16/1000 | Loss: 0.00001856
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001845
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001844
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001843
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001842
Iteration 40/1000 | Loss: 0.00001842
Iteration 41/1000 | Loss: 0.00001841
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001840
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001838
Iteration 53/1000 | Loss: 0.00001838
Iteration 54/1000 | Loss: 0.00001838
Iteration 55/1000 | Loss: 0.00001838
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00001836
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00001836
Iteration 77/1000 | Loss: 0.00001836
Iteration 78/1000 | Loss: 0.00001836
Iteration 79/1000 | Loss: 0.00001836
Iteration 80/1000 | Loss: 0.00001835
Iteration 81/1000 | Loss: 0.00001835
Iteration 82/1000 | Loss: 0.00001835
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001834
Iteration 88/1000 | Loss: 0.00001834
Iteration 89/1000 | Loss: 0.00001834
Iteration 90/1000 | Loss: 0.00001834
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001834
Iteration 98/1000 | Loss: 0.00001834
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00001834
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001833
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001833
Iteration 106/1000 | Loss: 0.00001833
Iteration 107/1000 | Loss: 0.00001833
Iteration 108/1000 | Loss: 0.00001833
Iteration 109/1000 | Loss: 0.00001833
Iteration 110/1000 | Loss: 0.00001833
Iteration 111/1000 | Loss: 0.00001833
Iteration 112/1000 | Loss: 0.00001833
Iteration 113/1000 | Loss: 0.00001833
Iteration 114/1000 | Loss: 0.00001833
Iteration 115/1000 | Loss: 0.00001833
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001833
Iteration 121/1000 | Loss: 0.00001833
Iteration 122/1000 | Loss: 0.00001832
Iteration 123/1000 | Loss: 0.00001832
Iteration 124/1000 | Loss: 0.00001832
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001832
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001832
Iteration 132/1000 | Loss: 0.00001832
Iteration 133/1000 | Loss: 0.00001832
Iteration 134/1000 | Loss: 0.00001832
Iteration 135/1000 | Loss: 0.00001832
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001830
Iteration 143/1000 | Loss: 0.00001830
Iteration 144/1000 | Loss: 0.00001830
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001830
Iteration 152/1000 | Loss: 0.00001830
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001829
Iteration 155/1000 | Loss: 0.00001829
Iteration 156/1000 | Loss: 0.00001829
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Iteration 165/1000 | Loss: 0.00001829
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001829
Iteration 169/1000 | Loss: 0.00001829
Iteration 170/1000 | Loss: 0.00001829
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001828
Iteration 180/1000 | Loss: 0.00001828
Iteration 181/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.828468157327734e-05, 1.828468157327734e-05, 1.828468157327734e-05, 1.828468157327734e-05, 1.828468157327734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.828468157327734e-05

Optimization complete. Final v2v error: 3.568127393722534 mm

Highest mean error: 3.8050942420959473 mm for frame 0

Lowest mean error: 3.4614949226379395 mm for frame 159

Saving results

Total time: 39.94676876068115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01213776
Iteration 2/25 | Loss: 0.00368215
Iteration 3/25 | Loss: 0.00281815
Iteration 4/25 | Loss: 0.00238329
Iteration 5/25 | Loss: 0.00200170
Iteration 6/25 | Loss: 0.00196369
Iteration 7/25 | Loss: 0.00185646
Iteration 8/25 | Loss: 0.00176228
Iteration 9/25 | Loss: 0.00171746
Iteration 10/25 | Loss: 0.00169262
Iteration 11/25 | Loss: 0.00167468
Iteration 12/25 | Loss: 0.00166620
Iteration 13/25 | Loss: 0.00165003
Iteration 14/25 | Loss: 0.00165048
Iteration 15/25 | Loss: 0.00165615
Iteration 16/25 | Loss: 0.00164043
Iteration 17/25 | Loss: 0.00164439
Iteration 18/25 | Loss: 0.00164503
Iteration 19/25 | Loss: 0.00164865
Iteration 20/25 | Loss: 0.00164557
Iteration 21/25 | Loss: 0.00163311
Iteration 22/25 | Loss: 0.00163097
Iteration 23/25 | Loss: 0.00163367
Iteration 24/25 | Loss: 0.00162966
Iteration 25/25 | Loss: 0.00162265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59270489
Iteration 2/25 | Loss: 0.00418374
Iteration 3/25 | Loss: 0.00418374
Iteration 4/25 | Loss: 0.00418374
Iteration 5/25 | Loss: 0.00418374
Iteration 6/25 | Loss: 0.00418374
Iteration 7/25 | Loss: 0.00418374
Iteration 8/25 | Loss: 0.00418374
Iteration 9/25 | Loss: 0.00418374
Iteration 10/25 | Loss: 0.00418374
Iteration 11/25 | Loss: 0.00418374
Iteration 12/25 | Loss: 0.00418374
Iteration 13/25 | Loss: 0.00418374
Iteration 14/25 | Loss: 0.00418374
Iteration 15/25 | Loss: 0.00418374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004183736629784107, 0.004183736629784107, 0.004183736629784107, 0.004183736629784107, 0.004183736629784107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004183736629784107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00418374
Iteration 2/1000 | Loss: 0.00086980
Iteration 3/1000 | Loss: 0.00064605
Iteration 4/1000 | Loss: 0.00123139
Iteration 5/1000 | Loss: 0.00086664
Iteration 6/1000 | Loss: 0.00085739
Iteration 7/1000 | Loss: 0.00047780
Iteration 8/1000 | Loss: 0.00040456
Iteration 9/1000 | Loss: 0.00092454
Iteration 10/1000 | Loss: 0.00057930
Iteration 11/1000 | Loss: 0.00092065
Iteration 12/1000 | Loss: 0.00091849
Iteration 13/1000 | Loss: 0.00094933
Iteration 14/1000 | Loss: 0.00094383
Iteration 15/1000 | Loss: 0.00066685
Iteration 16/1000 | Loss: 0.00061772
Iteration 17/1000 | Loss: 0.00114611
Iteration 18/1000 | Loss: 0.00081048
Iteration 19/1000 | Loss: 0.00126274
Iteration 20/1000 | Loss: 0.00099656
Iteration 21/1000 | Loss: 0.00211616
Iteration 22/1000 | Loss: 0.00145080
Iteration 23/1000 | Loss: 0.00129664
Iteration 24/1000 | Loss: 0.00184923
Iteration 25/1000 | Loss: 0.00073044
Iteration 26/1000 | Loss: 0.00107001
Iteration 27/1000 | Loss: 0.00054940
Iteration 28/1000 | Loss: 0.00050555
Iteration 29/1000 | Loss: 0.00023546
Iteration 30/1000 | Loss: 0.00074769
Iteration 31/1000 | Loss: 0.00086763
Iteration 32/1000 | Loss: 0.00097774
Iteration 33/1000 | Loss: 0.00107404
Iteration 34/1000 | Loss: 0.00083245
Iteration 35/1000 | Loss: 0.00077559
Iteration 36/1000 | Loss: 0.00077122
Iteration 37/1000 | Loss: 0.00193144
Iteration 38/1000 | Loss: 0.00052620
Iteration 39/1000 | Loss: 0.00109571
Iteration 40/1000 | Loss: 0.00062484
Iteration 41/1000 | Loss: 0.00047364
Iteration 42/1000 | Loss: 0.00035994
Iteration 43/1000 | Loss: 0.00159370
Iteration 44/1000 | Loss: 0.00097325
Iteration 45/1000 | Loss: 0.00063435
Iteration 46/1000 | Loss: 0.00067240
Iteration 47/1000 | Loss: 0.00085341
Iteration 48/1000 | Loss: 0.00067373
Iteration 49/1000 | Loss: 0.00080143
Iteration 50/1000 | Loss: 0.00070800
Iteration 51/1000 | Loss: 0.00074049
Iteration 52/1000 | Loss: 0.00062167
Iteration 53/1000 | Loss: 0.00148007
Iteration 54/1000 | Loss: 0.00066898
Iteration 55/1000 | Loss: 0.00074638
Iteration 56/1000 | Loss: 0.00077195
Iteration 57/1000 | Loss: 0.00117576
Iteration 58/1000 | Loss: 0.00085912
Iteration 59/1000 | Loss: 0.00085411
Iteration 60/1000 | Loss: 0.00078723
Iteration 61/1000 | Loss: 0.00101658
Iteration 62/1000 | Loss: 0.00099645
Iteration 63/1000 | Loss: 0.00126096
Iteration 64/1000 | Loss: 0.00086337
Iteration 65/1000 | Loss: 0.00244891
Iteration 66/1000 | Loss: 0.00109748
Iteration 67/1000 | Loss: 0.00091193
Iteration 68/1000 | Loss: 0.00092254
Iteration 69/1000 | Loss: 0.00097141
Iteration 70/1000 | Loss: 0.00053559
Iteration 71/1000 | Loss: 0.00057129
Iteration 72/1000 | Loss: 0.00064125
Iteration 73/1000 | Loss: 0.00080047
Iteration 74/1000 | Loss: 0.00069038
Iteration 75/1000 | Loss: 0.00069336
Iteration 76/1000 | Loss: 0.00066318
Iteration 77/1000 | Loss: 0.00129422
Iteration 78/1000 | Loss: 0.00090819
Iteration 79/1000 | Loss: 0.00044608
Iteration 80/1000 | Loss: 0.00053252
Iteration 81/1000 | Loss: 0.00055605
Iteration 82/1000 | Loss: 0.00059599
Iteration 83/1000 | Loss: 0.00080788
Iteration 84/1000 | Loss: 0.00142141
Iteration 85/1000 | Loss: 0.00092158
Iteration 86/1000 | Loss: 0.00073098
Iteration 87/1000 | Loss: 0.00088882
Iteration 88/1000 | Loss: 0.00083982
Iteration 89/1000 | Loss: 0.00048753
Iteration 90/1000 | Loss: 0.00075542
Iteration 91/1000 | Loss: 0.00074791
Iteration 92/1000 | Loss: 0.00074709
Iteration 93/1000 | Loss: 0.00174981
Iteration 94/1000 | Loss: 0.00092972
Iteration 95/1000 | Loss: 0.00084437
Iteration 96/1000 | Loss: 0.00105807
Iteration 97/1000 | Loss: 0.00088782
Iteration 98/1000 | Loss: 0.00057669
Iteration 99/1000 | Loss: 0.00075393
Iteration 100/1000 | Loss: 0.00086336
Iteration 101/1000 | Loss: 0.00099878
Iteration 102/1000 | Loss: 0.00075858
Iteration 103/1000 | Loss: 0.00058139
Iteration 104/1000 | Loss: 0.00076884
Iteration 105/1000 | Loss: 0.00068949
Iteration 106/1000 | Loss: 0.00076449
Iteration 107/1000 | Loss: 0.00072969
Iteration 108/1000 | Loss: 0.00066030
Iteration 109/1000 | Loss: 0.00098534
Iteration 110/1000 | Loss: 0.00073291
Iteration 111/1000 | Loss: 0.00054928
Iteration 112/1000 | Loss: 0.00055019
Iteration 113/1000 | Loss: 0.00057754
Iteration 114/1000 | Loss: 0.00062492
Iteration 115/1000 | Loss: 0.00043549
Iteration 116/1000 | Loss: 0.00047880
Iteration 117/1000 | Loss: 0.00074724
Iteration 118/1000 | Loss: 0.00066929
Iteration 119/1000 | Loss: 0.00027098
Iteration 120/1000 | Loss: 0.00049800
Iteration 121/1000 | Loss: 0.00081704
Iteration 122/1000 | Loss: 0.00134756
Iteration 123/1000 | Loss: 0.00115107
Iteration 124/1000 | Loss: 0.00123921
Iteration 125/1000 | Loss: 0.00125817
Iteration 126/1000 | Loss: 0.00047016
Iteration 127/1000 | Loss: 0.00068066
Iteration 128/1000 | Loss: 0.00064402
Iteration 129/1000 | Loss: 0.00033144
Iteration 130/1000 | Loss: 0.00033797
Iteration 131/1000 | Loss: 0.00053842
Iteration 132/1000 | Loss: 0.00065086
Iteration 133/1000 | Loss: 0.00047807
Iteration 134/1000 | Loss: 0.00071137
Iteration 135/1000 | Loss: 0.00042135
Iteration 136/1000 | Loss: 0.00055969
Iteration 137/1000 | Loss: 0.00068242
Iteration 138/1000 | Loss: 0.00060494
Iteration 139/1000 | Loss: 0.00034503
Iteration 140/1000 | Loss: 0.00032946
Iteration 141/1000 | Loss: 0.00028429
Iteration 142/1000 | Loss: 0.00022814
Iteration 143/1000 | Loss: 0.00031233
Iteration 144/1000 | Loss: 0.00034507
Iteration 145/1000 | Loss: 0.00029931
Iteration 146/1000 | Loss: 0.00029171
Iteration 147/1000 | Loss: 0.00025447
Iteration 148/1000 | Loss: 0.00046106
Iteration 149/1000 | Loss: 0.00071751
Iteration 150/1000 | Loss: 0.00024111
Iteration 151/1000 | Loss: 0.00044637
Iteration 152/1000 | Loss: 0.00037107
Iteration 153/1000 | Loss: 0.00031871
Iteration 154/1000 | Loss: 0.00023928
Iteration 155/1000 | Loss: 0.00023735
Iteration 156/1000 | Loss: 0.00022618
Iteration 157/1000 | Loss: 0.00020874
Iteration 158/1000 | Loss: 0.00045140
Iteration 159/1000 | Loss: 0.00037124
Iteration 160/1000 | Loss: 0.00032453
Iteration 161/1000 | Loss: 0.00021461
Iteration 162/1000 | Loss: 0.00019191
Iteration 163/1000 | Loss: 0.00024416
Iteration 164/1000 | Loss: 0.00027588
Iteration 165/1000 | Loss: 0.00028683
Iteration 166/1000 | Loss: 0.00027692
Iteration 167/1000 | Loss: 0.00031659
Iteration 168/1000 | Loss: 0.00028443
Iteration 169/1000 | Loss: 0.00032115
Iteration 170/1000 | Loss: 0.00028427
Iteration 171/1000 | Loss: 0.00029804
Iteration 172/1000 | Loss: 0.00028105
Iteration 173/1000 | Loss: 0.00030915
Iteration 174/1000 | Loss: 0.00028485
Iteration 175/1000 | Loss: 0.00035858
Iteration 176/1000 | Loss: 0.00155953
Iteration 177/1000 | Loss: 0.00041049
Iteration 178/1000 | Loss: 0.00046519
Iteration 179/1000 | Loss: 0.00042666
Iteration 180/1000 | Loss: 0.00038415
Iteration 181/1000 | Loss: 0.00037123
Iteration 182/1000 | Loss: 0.00014866
Iteration 183/1000 | Loss: 0.00013078
Iteration 184/1000 | Loss: 0.00034114
Iteration 185/1000 | Loss: 0.00030490
Iteration 186/1000 | Loss: 0.00035938
Iteration 187/1000 | Loss: 0.00031908
Iteration 188/1000 | Loss: 0.00037336
Iteration 189/1000 | Loss: 0.00048927
Iteration 190/1000 | Loss: 0.00032550
Iteration 191/1000 | Loss: 0.00026414
Iteration 192/1000 | Loss: 0.00076098
Iteration 193/1000 | Loss: 0.00060740
Iteration 194/1000 | Loss: 0.00058339
Iteration 195/1000 | Loss: 0.00029277
Iteration 196/1000 | Loss: 0.00027564
Iteration 197/1000 | Loss: 0.00093048
Iteration 198/1000 | Loss: 0.00037942
Iteration 199/1000 | Loss: 0.00041825
Iteration 200/1000 | Loss: 0.00078043
Iteration 201/1000 | Loss: 0.00052999
Iteration 202/1000 | Loss: 0.00069248
Iteration 203/1000 | Loss: 0.00044435
Iteration 204/1000 | Loss: 0.00045875
Iteration 205/1000 | Loss: 0.00049855
Iteration 206/1000 | Loss: 0.00030257
Iteration 207/1000 | Loss: 0.00047724
Iteration 208/1000 | Loss: 0.00045242
Iteration 209/1000 | Loss: 0.00047410
Iteration 210/1000 | Loss: 0.00044973
Iteration 211/1000 | Loss: 0.00054661
Iteration 212/1000 | Loss: 0.00054891
Iteration 213/1000 | Loss: 0.00103610
Iteration 214/1000 | Loss: 0.00058449
Iteration 215/1000 | Loss: 0.00097282
Iteration 216/1000 | Loss: 0.00066283
Iteration 217/1000 | Loss: 0.00058407
Iteration 218/1000 | Loss: 0.00038113
Iteration 219/1000 | Loss: 0.00060493
Iteration 220/1000 | Loss: 0.00031811
Iteration 221/1000 | Loss: 0.00067876
Iteration 222/1000 | Loss: 0.00035740
Iteration 223/1000 | Loss: 0.00072338
Iteration 224/1000 | Loss: 0.00066139
Iteration 225/1000 | Loss: 0.00041978
Iteration 226/1000 | Loss: 0.00046409
Iteration 227/1000 | Loss: 0.00046165
Iteration 228/1000 | Loss: 0.00046212
Iteration 229/1000 | Loss: 0.00057236
Iteration 230/1000 | Loss: 0.00057510
Iteration 231/1000 | Loss: 0.00055829
Iteration 232/1000 | Loss: 0.00053262
Iteration 233/1000 | Loss: 0.00034165
Iteration 234/1000 | Loss: 0.00041338
Iteration 235/1000 | Loss: 0.00035152
Iteration 236/1000 | Loss: 0.00043037
Iteration 237/1000 | Loss: 0.00044699
Iteration 238/1000 | Loss: 0.00041432
Iteration 239/1000 | Loss: 0.00018397
Iteration 240/1000 | Loss: 0.00037837
Iteration 241/1000 | Loss: 0.00021753
Iteration 242/1000 | Loss: 0.00016312
Iteration 243/1000 | Loss: 0.00029285
Iteration 244/1000 | Loss: 0.00028460
Iteration 245/1000 | Loss: 0.00030378
Iteration 246/1000 | Loss: 0.00026360
Iteration 247/1000 | Loss: 0.00032444
Iteration 248/1000 | Loss: 0.00030385
Iteration 249/1000 | Loss: 0.00029967
Iteration 250/1000 | Loss: 0.00079387
Iteration 251/1000 | Loss: 0.00029487
Iteration 252/1000 | Loss: 0.00078584
Iteration 253/1000 | Loss: 0.00027792
Iteration 254/1000 | Loss: 0.00029548
Iteration 255/1000 | Loss: 0.00039047
Iteration 256/1000 | Loss: 0.00036204
Iteration 257/1000 | Loss: 0.00043672
Iteration 258/1000 | Loss: 0.00130228
Iteration 259/1000 | Loss: 0.00043109
Iteration 260/1000 | Loss: 0.00036170
Iteration 261/1000 | Loss: 0.00035480
Iteration 262/1000 | Loss: 0.00033556
Iteration 263/1000 | Loss: 0.00023241
Iteration 264/1000 | Loss: 0.00026946
Iteration 265/1000 | Loss: 0.00033889
Iteration 266/1000 | Loss: 0.00037094
Iteration 267/1000 | Loss: 0.00050943
Iteration 268/1000 | Loss: 0.00030112
Iteration 269/1000 | Loss: 0.00033122
Iteration 270/1000 | Loss: 0.00043100
Iteration 271/1000 | Loss: 0.00023905
Iteration 272/1000 | Loss: 0.00043110
Iteration 273/1000 | Loss: 0.00026706
Iteration 274/1000 | Loss: 0.00046765
Iteration 275/1000 | Loss: 0.00032204
Iteration 276/1000 | Loss: 0.00051108
Iteration 277/1000 | Loss: 0.00049675
Iteration 278/1000 | Loss: 0.00028033
Iteration 279/1000 | Loss: 0.00046972
Iteration 280/1000 | Loss: 0.00061272
Iteration 281/1000 | Loss: 0.00053438
Iteration 282/1000 | Loss: 0.00054266
Iteration 283/1000 | Loss: 0.00053335
Iteration 284/1000 | Loss: 0.00032194
Iteration 285/1000 | Loss: 0.00039283
Iteration 286/1000 | Loss: 0.00047811
Iteration 287/1000 | Loss: 0.00042083
Iteration 288/1000 | Loss: 0.00040536
Iteration 289/1000 | Loss: 0.00034728
Iteration 290/1000 | Loss: 0.00042270
Iteration 291/1000 | Loss: 0.00032257
Iteration 292/1000 | Loss: 0.00022263
Iteration 293/1000 | Loss: 0.00058651
Iteration 294/1000 | Loss: 0.00031427
Iteration 295/1000 | Loss: 0.00048565
Iteration 296/1000 | Loss: 0.00039237
Iteration 297/1000 | Loss: 0.00041571
Iteration 298/1000 | Loss: 0.00034262
Iteration 299/1000 | Loss: 0.00037816
Iteration 300/1000 | Loss: 0.00046308
Iteration 301/1000 | Loss: 0.00031694
Iteration 302/1000 | Loss: 0.00031640
Iteration 303/1000 | Loss: 0.00031600
Iteration 304/1000 | Loss: 0.00033464
Iteration 305/1000 | Loss: 0.00040700
Iteration 306/1000 | Loss: 0.00035549
Iteration 307/1000 | Loss: 0.00031444
Iteration 308/1000 | Loss: 0.00033808
Iteration 309/1000 | Loss: 0.00035753
Iteration 310/1000 | Loss: 0.00029511
Iteration 311/1000 | Loss: 0.00031102
Iteration 312/1000 | Loss: 0.00027936
Iteration 313/1000 | Loss: 0.00040471
Iteration 314/1000 | Loss: 0.00041684
Iteration 315/1000 | Loss: 0.00033606
Iteration 316/1000 | Loss: 0.00029864
Iteration 317/1000 | Loss: 0.00055375
Iteration 318/1000 | Loss: 0.00046516
Iteration 319/1000 | Loss: 0.00053984
Iteration 320/1000 | Loss: 0.00039568
Iteration 321/1000 | Loss: 0.00051691
Iteration 322/1000 | Loss: 0.00039290
Iteration 323/1000 | Loss: 0.00052501
Iteration 324/1000 | Loss: 0.00036569
Iteration 325/1000 | Loss: 0.00052207
Iteration 326/1000 | Loss: 0.00100730
Iteration 327/1000 | Loss: 0.00141671
Iteration 328/1000 | Loss: 0.00059919
Iteration 329/1000 | Loss: 0.00049123
Iteration 330/1000 | Loss: 0.00042252
Iteration 331/1000 | Loss: 0.00051471
Iteration 332/1000 | Loss: 0.00046307
Iteration 333/1000 | Loss: 0.00051401
Iteration 334/1000 | Loss: 0.00035934
Iteration 335/1000 | Loss: 0.00036087
Iteration 336/1000 | Loss: 0.00030395
Iteration 337/1000 | Loss: 0.00055258
Iteration 338/1000 | Loss: 0.00062457
Iteration 339/1000 | Loss: 0.00087554
Iteration 340/1000 | Loss: 0.00063509
Iteration 341/1000 | Loss: 0.00027325
Iteration 342/1000 | Loss: 0.00019615
Iteration 343/1000 | Loss: 0.00015086
Iteration 344/1000 | Loss: 0.00048688
Iteration 345/1000 | Loss: 0.00049145
Iteration 346/1000 | Loss: 0.00036614
Iteration 347/1000 | Loss: 0.00034639
Iteration 348/1000 | Loss: 0.00033164
Iteration 349/1000 | Loss: 0.00028715
Iteration 350/1000 | Loss: 0.00039798
Iteration 351/1000 | Loss: 0.00035953
Iteration 352/1000 | Loss: 0.00039221
Iteration 353/1000 | Loss: 0.00039096
Iteration 354/1000 | Loss: 0.00070182
Iteration 355/1000 | Loss: 0.00054417
Iteration 356/1000 | Loss: 0.00047742
Iteration 357/1000 | Loss: 0.00034722
Iteration 358/1000 | Loss: 0.00045256
Iteration 359/1000 | Loss: 0.00041247
Iteration 360/1000 | Loss: 0.00036570
Iteration 361/1000 | Loss: 0.00044785
Iteration 362/1000 | Loss: 0.00044598
Iteration 363/1000 | Loss: 0.00053494
Iteration 364/1000 | Loss: 0.00044200
Iteration 365/1000 | Loss: 0.00049434
Iteration 366/1000 | Loss: 0.00055797
Iteration 367/1000 | Loss: 0.00038713
Iteration 368/1000 | Loss: 0.00035975
Iteration 369/1000 | Loss: 0.00029809
Iteration 370/1000 | Loss: 0.00055664
Iteration 371/1000 | Loss: 0.00039818
Iteration 372/1000 | Loss: 0.00046599
Iteration 373/1000 | Loss: 0.00044147
Iteration 374/1000 | Loss: 0.00042844
Iteration 375/1000 | Loss: 0.00046123
Iteration 376/1000 | Loss: 0.00218896
Iteration 377/1000 | Loss: 0.00095456
Iteration 378/1000 | Loss: 0.00052557
Iteration 379/1000 | Loss: 0.00027794
Iteration 380/1000 | Loss: 0.00038944
Iteration 381/1000 | Loss: 0.00030836
Iteration 382/1000 | Loss: 0.00058760
Iteration 383/1000 | Loss: 0.00052990
Iteration 384/1000 | Loss: 0.00036077
Iteration 385/1000 | Loss: 0.00029540
Iteration 386/1000 | Loss: 0.00033908
Iteration 387/1000 | Loss: 0.00038189
Iteration 388/1000 | Loss: 0.00033605
Iteration 389/1000 | Loss: 0.00040700
Iteration 390/1000 | Loss: 0.00041381
Iteration 391/1000 | Loss: 0.00034534
Iteration 392/1000 | Loss: 0.00033837
Iteration 393/1000 | Loss: 0.00025103
Iteration 394/1000 | Loss: 0.00040915
Iteration 395/1000 | Loss: 0.00050440
Iteration 396/1000 | Loss: 0.00072046
Iteration 397/1000 | Loss: 0.00032061
Iteration 398/1000 | Loss: 0.00032208
Iteration 399/1000 | Loss: 0.00052144
Iteration 400/1000 | Loss: 0.00037882
Iteration 401/1000 | Loss: 0.00055566
Iteration 402/1000 | Loss: 0.00137664
Iteration 403/1000 | Loss: 0.00072851
Iteration 404/1000 | Loss: 0.00069134
Iteration 405/1000 | Loss: 0.00024252
Iteration 406/1000 | Loss: 0.00069699
Iteration 407/1000 | Loss: 0.00041214
Iteration 408/1000 | Loss: 0.00059355
Iteration 409/1000 | Loss: 0.00072457
Iteration 410/1000 | Loss: 0.00052609
Iteration 411/1000 | Loss: 0.00043155
Iteration 412/1000 | Loss: 0.00102775
Iteration 413/1000 | Loss: 0.00057010
Iteration 414/1000 | Loss: 0.00033848
Iteration 415/1000 | Loss: 0.00028198
Iteration 416/1000 | Loss: 0.00047664
Iteration 417/1000 | Loss: 0.00035394
Iteration 418/1000 | Loss: 0.00040383
Iteration 419/1000 | Loss: 0.00039103
Iteration 420/1000 | Loss: 0.00026555
Iteration 421/1000 | Loss: 0.00046336
Iteration 422/1000 | Loss: 0.00057244
Iteration 423/1000 | Loss: 0.00032650
Iteration 424/1000 | Loss: 0.00035590
Iteration 425/1000 | Loss: 0.00029957
Iteration 426/1000 | Loss: 0.00028920
Iteration 427/1000 | Loss: 0.00038892
Iteration 428/1000 | Loss: 0.00025910
Iteration 429/1000 | Loss: 0.00036018
Iteration 430/1000 | Loss: 0.00022174
Iteration 431/1000 | Loss: 0.00023269
Iteration 432/1000 | Loss: 0.00021219
Iteration 433/1000 | Loss: 0.00021766
Iteration 434/1000 | Loss: 0.00030262
Iteration 435/1000 | Loss: 0.00032283
Iteration 436/1000 | Loss: 0.00017507
Iteration 437/1000 | Loss: 0.00044892
Iteration 438/1000 | Loss: 0.00036048
Iteration 439/1000 | Loss: 0.00023798
Iteration 440/1000 | Loss: 0.00033320
Iteration 441/1000 | Loss: 0.00033289
Iteration 442/1000 | Loss: 0.00014157
Iteration 443/1000 | Loss: 0.00015699
Iteration 444/1000 | Loss: 0.00037740
Iteration 445/1000 | Loss: 0.00036358
Iteration 446/1000 | Loss: 0.00025084
Iteration 447/1000 | Loss: 0.00063110
Iteration 448/1000 | Loss: 0.00031631
Iteration 449/1000 | Loss: 0.00019871
Iteration 450/1000 | Loss: 0.00022261
Iteration 451/1000 | Loss: 0.00017261
Iteration 452/1000 | Loss: 0.00019530
Iteration 453/1000 | Loss: 0.00030502
Iteration 454/1000 | Loss: 0.00056204
Iteration 455/1000 | Loss: 0.00049381
Iteration 456/1000 | Loss: 0.00011101
Iteration 457/1000 | Loss: 0.00013918
Iteration 458/1000 | Loss: 0.00009536
Iteration 459/1000 | Loss: 0.00021912
Iteration 460/1000 | Loss: 0.00011109
Iteration 461/1000 | Loss: 0.00009213
Iteration 462/1000 | Loss: 0.00009081
Iteration 463/1000 | Loss: 0.00009538
Iteration 464/1000 | Loss: 0.00021024
Iteration 465/1000 | Loss: 0.00021222
Iteration 466/1000 | Loss: 0.00009850
Iteration 467/1000 | Loss: 0.00015812
Iteration 468/1000 | Loss: 0.00015788
Iteration 469/1000 | Loss: 0.00018291
Iteration 470/1000 | Loss: 0.00009333
Iteration 471/1000 | Loss: 0.00008995
Iteration 472/1000 | Loss: 0.00010949
Iteration 473/1000 | Loss: 0.00009961
Iteration 474/1000 | Loss: 0.00019593
Iteration 475/1000 | Loss: 0.00017254
Iteration 476/1000 | Loss: 0.00042319
Iteration 477/1000 | Loss: 0.00020833
Iteration 478/1000 | Loss: 0.00059136
Iteration 479/1000 | Loss: 0.00026937
Iteration 480/1000 | Loss: 0.00015722
Iteration 481/1000 | Loss: 0.00022944
Iteration 482/1000 | Loss: 0.00013319
Iteration 483/1000 | Loss: 0.00016564
Iteration 484/1000 | Loss: 0.00011167
Iteration 485/1000 | Loss: 0.00013146
Iteration 486/1000 | Loss: 0.00024063
Iteration 487/1000 | Loss: 0.00009533
Iteration 488/1000 | Loss: 0.00018858
Iteration 489/1000 | Loss: 0.00196779
Iteration 490/1000 | Loss: 0.00405669
Iteration 491/1000 | Loss: 0.00060071
Iteration 492/1000 | Loss: 0.00132233
Iteration 493/1000 | Loss: 0.00051108
Iteration 494/1000 | Loss: 0.00023481
Iteration 495/1000 | Loss: 0.00010838
Iteration 496/1000 | Loss: 0.00009580
Iteration 497/1000 | Loss: 0.00026196
Iteration 498/1000 | Loss: 0.00008840
Iteration 499/1000 | Loss: 0.00008450
Iteration 500/1000 | Loss: 0.00008192
Iteration 501/1000 | Loss: 0.00008040
Iteration 502/1000 | Loss: 0.00007922
Iteration 503/1000 | Loss: 0.00007853
Iteration 504/1000 | Loss: 0.00045285
Iteration 505/1000 | Loss: 0.00008028
Iteration 506/1000 | Loss: 0.00007782
Iteration 507/1000 | Loss: 0.00007669
Iteration 508/1000 | Loss: 0.00007551
Iteration 509/1000 | Loss: 0.00007490
Iteration 510/1000 | Loss: 0.00007446
Iteration 511/1000 | Loss: 0.00007420
Iteration 512/1000 | Loss: 0.00007396
Iteration 513/1000 | Loss: 0.00040007
Iteration 514/1000 | Loss: 0.00007592
Iteration 515/1000 | Loss: 0.00007373
Iteration 516/1000 | Loss: 0.00007280
Iteration 517/1000 | Loss: 0.00007189
Iteration 518/1000 | Loss: 0.00007126
Iteration 519/1000 | Loss: 0.00007096
Iteration 520/1000 | Loss: 0.00007084
Iteration 521/1000 | Loss: 0.00007069
Iteration 522/1000 | Loss: 0.00007065
Iteration 523/1000 | Loss: 0.00007065
Iteration 524/1000 | Loss: 0.00007065
Iteration 525/1000 | Loss: 0.00007065
Iteration 526/1000 | Loss: 0.00007065
Iteration 527/1000 | Loss: 0.00007065
Iteration 528/1000 | Loss: 0.00007065
Iteration 529/1000 | Loss: 0.00007065
Iteration 530/1000 | Loss: 0.00007064
Iteration 531/1000 | Loss: 0.00007064
Iteration 532/1000 | Loss: 0.00007056
Iteration 533/1000 | Loss: 0.00007056
Iteration 534/1000 | Loss: 0.00007053
Iteration 535/1000 | Loss: 0.00007052
Iteration 536/1000 | Loss: 0.00007051
Iteration 537/1000 | Loss: 0.00007050
Iteration 538/1000 | Loss: 0.00007045
Iteration 539/1000 | Loss: 0.00007045
Iteration 540/1000 | Loss: 0.00007045
Iteration 541/1000 | Loss: 0.00007045
Iteration 542/1000 | Loss: 0.00007044
Iteration 543/1000 | Loss: 0.00007044
Iteration 544/1000 | Loss: 0.00007043
Iteration 545/1000 | Loss: 0.00007043
Iteration 546/1000 | Loss: 0.00007043
Iteration 547/1000 | Loss: 0.00007043
Iteration 548/1000 | Loss: 0.00007042
Iteration 549/1000 | Loss: 0.00007042
Iteration 550/1000 | Loss: 0.00007042
Iteration 551/1000 | Loss: 0.00007042
Iteration 552/1000 | Loss: 0.00007042
Iteration 553/1000 | Loss: 0.00007041
Iteration 554/1000 | Loss: 0.00007041
Iteration 555/1000 | Loss: 0.00007041
Iteration 556/1000 | Loss: 0.00007041
Iteration 557/1000 | Loss: 0.00007041
Iteration 558/1000 | Loss: 0.00007041
Iteration 559/1000 | Loss: 0.00007040
Iteration 560/1000 | Loss: 0.00007040
Iteration 561/1000 | Loss: 0.00007040
Iteration 562/1000 | Loss: 0.00007039
Iteration 563/1000 | Loss: 0.00007039
Iteration 564/1000 | Loss: 0.00007038
Iteration 565/1000 | Loss: 0.00007037
Iteration 566/1000 | Loss: 0.00007037
Iteration 567/1000 | Loss: 0.00007036
Iteration 568/1000 | Loss: 0.00007036
Iteration 569/1000 | Loss: 0.00007035
Iteration 570/1000 | Loss: 0.00007034
Iteration 571/1000 | Loss: 0.00007034
Iteration 572/1000 | Loss: 0.00007034
Iteration 573/1000 | Loss: 0.00007034
Iteration 574/1000 | Loss: 0.00007033
Iteration 575/1000 | Loss: 0.00007033
Iteration 576/1000 | Loss: 0.00007033
Iteration 577/1000 | Loss: 0.00007033
Iteration 578/1000 | Loss: 0.00007033
Iteration 579/1000 | Loss: 0.00007033
Iteration 580/1000 | Loss: 0.00007033
Iteration 581/1000 | Loss: 0.00007033
Iteration 582/1000 | Loss: 0.00007033
Iteration 583/1000 | Loss: 0.00007033
Iteration 584/1000 | Loss: 0.00007033
Iteration 585/1000 | Loss: 0.00007033
Iteration 586/1000 | Loss: 0.00007033
Iteration 587/1000 | Loss: 0.00007032
Iteration 588/1000 | Loss: 0.00007032
Iteration 589/1000 | Loss: 0.00007032
Iteration 590/1000 | Loss: 0.00007032
Iteration 591/1000 | Loss: 0.00007032
Iteration 592/1000 | Loss: 0.00007032
Iteration 593/1000 | Loss: 0.00007031
Iteration 594/1000 | Loss: 0.00007031
Iteration 595/1000 | Loss: 0.00007031
Iteration 596/1000 | Loss: 0.00007031
Iteration 597/1000 | Loss: 0.00007031
Iteration 598/1000 | Loss: 0.00007030
Iteration 599/1000 | Loss: 0.00007030
Iteration 600/1000 | Loss: 0.00007030
Iteration 601/1000 | Loss: 0.00007029
Iteration 602/1000 | Loss: 0.00007029
Iteration 603/1000 | Loss: 0.00007029
Iteration 604/1000 | Loss: 0.00007029
Iteration 605/1000 | Loss: 0.00007029
Iteration 606/1000 | Loss: 0.00007029
Iteration 607/1000 | Loss: 0.00007029
Iteration 608/1000 | Loss: 0.00007028
Iteration 609/1000 | Loss: 0.00007028
Iteration 610/1000 | Loss: 0.00007028
Iteration 611/1000 | Loss: 0.00007028
Iteration 612/1000 | Loss: 0.00007028
Iteration 613/1000 | Loss: 0.00007028
Iteration 614/1000 | Loss: 0.00007028
Iteration 615/1000 | Loss: 0.00007028
Iteration 616/1000 | Loss: 0.00007028
Iteration 617/1000 | Loss: 0.00007028
Iteration 618/1000 | Loss: 0.00007027
Iteration 619/1000 | Loss: 0.00007027
Iteration 620/1000 | Loss: 0.00007027
Iteration 621/1000 | Loss: 0.00007027
Iteration 622/1000 | Loss: 0.00007026
Iteration 623/1000 | Loss: 0.00007026
Iteration 624/1000 | Loss: 0.00007026
Iteration 625/1000 | Loss: 0.00007026
Iteration 626/1000 | Loss: 0.00007026
Iteration 627/1000 | Loss: 0.00007025
Iteration 628/1000 | Loss: 0.00007025
Iteration 629/1000 | Loss: 0.00007025
Iteration 630/1000 | Loss: 0.00007025
Iteration 631/1000 | Loss: 0.00007025
Iteration 632/1000 | Loss: 0.00007025
Iteration 633/1000 | Loss: 0.00007025
Iteration 634/1000 | Loss: 0.00007025
Iteration 635/1000 | Loss: 0.00007025
Iteration 636/1000 | Loss: 0.00007025
Iteration 637/1000 | Loss: 0.00007025
Iteration 638/1000 | Loss: 0.00007024
Iteration 639/1000 | Loss: 0.00007024
Iteration 640/1000 | Loss: 0.00007024
Iteration 641/1000 | Loss: 0.00007024
Iteration 642/1000 | Loss: 0.00007024
Iteration 643/1000 | Loss: 0.00007024
Iteration 644/1000 | Loss: 0.00007024
Iteration 645/1000 | Loss: 0.00007024
Iteration 646/1000 | Loss: 0.00007024
Iteration 647/1000 | Loss: 0.00007023
Iteration 648/1000 | Loss: 0.00007023
Iteration 649/1000 | Loss: 0.00007023
Iteration 650/1000 | Loss: 0.00007023
Iteration 651/1000 | Loss: 0.00007023
Iteration 652/1000 | Loss: 0.00007023
Iteration 653/1000 | Loss: 0.00007023
Iteration 654/1000 | Loss: 0.00007022
Iteration 655/1000 | Loss: 0.00007022
Iteration 656/1000 | Loss: 0.00007022
Iteration 657/1000 | Loss: 0.00007021
Iteration 658/1000 | Loss: 0.00007021
Iteration 659/1000 | Loss: 0.00007021
Iteration 660/1000 | Loss: 0.00007021
Iteration 661/1000 | Loss: 0.00007020
Iteration 662/1000 | Loss: 0.00007020
Iteration 663/1000 | Loss: 0.00007020
Iteration 664/1000 | Loss: 0.00007019
Iteration 665/1000 | Loss: 0.00007019
Iteration 666/1000 | Loss: 0.00007019
Iteration 667/1000 | Loss: 0.00007019
Iteration 668/1000 | Loss: 0.00007019
Iteration 669/1000 | Loss: 0.00007018
Iteration 670/1000 | Loss: 0.00007018
Iteration 671/1000 | Loss: 0.00007018
Iteration 672/1000 | Loss: 0.00007018
Iteration 673/1000 | Loss: 0.00007018
Iteration 674/1000 | Loss: 0.00007017
Iteration 675/1000 | Loss: 0.00007017
Iteration 676/1000 | Loss: 0.00007017
Iteration 677/1000 | Loss: 0.00007017
Iteration 678/1000 | Loss: 0.00007017
Iteration 679/1000 | Loss: 0.00007016
Iteration 680/1000 | Loss: 0.00007016
Iteration 681/1000 | Loss: 0.00007016
Iteration 682/1000 | Loss: 0.00007016
Iteration 683/1000 | Loss: 0.00007015
Iteration 684/1000 | Loss: 0.00007014
Iteration 685/1000 | Loss: 0.00007013
Iteration 686/1000 | Loss: 0.00007013
Iteration 687/1000 | Loss: 0.00007013
Iteration 688/1000 | Loss: 0.00007012
Iteration 689/1000 | Loss: 0.00007011
Iteration 690/1000 | Loss: 0.00007010
Iteration 691/1000 | Loss: 0.00007010
Iteration 692/1000 | Loss: 0.00007009
Iteration 693/1000 | Loss: 0.00007009
Iteration 694/1000 | Loss: 0.00007009
Iteration 695/1000 | Loss: 0.00007009
Iteration 696/1000 | Loss: 0.00007009
Iteration 697/1000 | Loss: 0.00007009
Iteration 698/1000 | Loss: 0.00007009
Iteration 699/1000 | Loss: 0.00007009
Iteration 700/1000 | Loss: 0.00007009
Iteration 701/1000 | Loss: 0.00007009
Iteration 702/1000 | Loss: 0.00007009
Iteration 703/1000 | Loss: 0.00007009
Iteration 704/1000 | Loss: 0.00007008
Iteration 705/1000 | Loss: 0.00007008
Iteration 706/1000 | Loss: 0.00007007
Iteration 707/1000 | Loss: 0.00007006
Iteration 708/1000 | Loss: 0.00007006
Iteration 709/1000 | Loss: 0.00007005
Iteration 710/1000 | Loss: 0.00007004
Iteration 711/1000 | Loss: 0.00038715
Iteration 712/1000 | Loss: 0.00109057
Iteration 713/1000 | Loss: 0.00522563
Iteration 714/1000 | Loss: 0.00178857
Iteration 715/1000 | Loss: 0.00046181
Iteration 716/1000 | Loss: 0.00023812
Iteration 717/1000 | Loss: 0.00321329
Iteration 718/1000 | Loss: 0.00061748
Iteration 719/1000 | Loss: 0.00015337
Iteration 720/1000 | Loss: 0.00015621
Iteration 721/1000 | Loss: 0.00365203
Iteration 722/1000 | Loss: 0.00024427
Iteration 723/1000 | Loss: 0.00014362
Iteration 724/1000 | Loss: 0.00006658
Iteration 725/1000 | Loss: 0.00005971
Iteration 726/1000 | Loss: 0.00005651
Iteration 727/1000 | Loss: 0.00005427
Iteration 728/1000 | Loss: 0.00005243
Iteration 729/1000 | Loss: 0.00005913
Iteration 730/1000 | Loss: 0.00005070
Iteration 731/1000 | Loss: 0.00004945
Iteration 732/1000 | Loss: 0.00004897
Iteration 733/1000 | Loss: 0.00004848
Iteration 734/1000 | Loss: 0.00004818
Iteration 735/1000 | Loss: 0.00004780
Iteration 736/1000 | Loss: 0.00004737
Iteration 737/1000 | Loss: 0.00004687
Iteration 738/1000 | Loss: 0.00004657
Iteration 739/1000 | Loss: 0.00004636
Iteration 740/1000 | Loss: 0.00004618
Iteration 741/1000 | Loss: 0.00004609
Iteration 742/1000 | Loss: 0.00004596
Iteration 743/1000 | Loss: 0.00004592
Iteration 744/1000 | Loss: 0.00004586
Iteration 745/1000 | Loss: 0.00004585
Iteration 746/1000 | Loss: 0.00004585
Iteration 747/1000 | Loss: 0.00004582
Iteration 748/1000 | Loss: 0.00004581
Iteration 749/1000 | Loss: 0.00004580
Iteration 750/1000 | Loss: 0.00004575
Iteration 751/1000 | Loss: 0.00004575
Iteration 752/1000 | Loss: 0.00004575
Iteration 753/1000 | Loss: 0.00004575
Iteration 754/1000 | Loss: 0.00004575
Iteration 755/1000 | Loss: 0.00004574
Iteration 756/1000 | Loss: 0.00004574
Iteration 757/1000 | Loss: 0.00004573
Iteration 758/1000 | Loss: 0.00004573
Iteration 759/1000 | Loss: 0.00004573
Iteration 760/1000 | Loss: 0.00004572
Iteration 761/1000 | Loss: 0.00004572
Iteration 762/1000 | Loss: 0.00004572
Iteration 763/1000 | Loss: 0.00004570
Iteration 764/1000 | Loss: 0.00004570
Iteration 765/1000 | Loss: 0.00004570
Iteration 766/1000 | Loss: 0.00004569
Iteration 767/1000 | Loss: 0.00004569
Iteration 768/1000 | Loss: 0.00004569
Iteration 769/1000 | Loss: 0.00004569
Iteration 770/1000 | Loss: 0.00004569
Iteration 771/1000 | Loss: 0.00004569
Iteration 772/1000 | Loss: 0.00004569
Iteration 773/1000 | Loss: 0.00004569
Iteration 774/1000 | Loss: 0.00004568
Iteration 775/1000 | Loss: 0.00004568
Iteration 776/1000 | Loss: 0.00004567
Iteration 777/1000 | Loss: 0.00004567
Iteration 778/1000 | Loss: 0.00004567
Iteration 779/1000 | Loss: 0.00004567
Iteration 780/1000 | Loss: 0.00004567
Iteration 781/1000 | Loss: 0.00004566
Iteration 782/1000 | Loss: 0.00004566
Iteration 783/1000 | Loss: 0.00004566
Iteration 784/1000 | Loss: 0.00004566
Iteration 785/1000 | Loss: 0.00004566
Iteration 786/1000 | Loss: 0.00004566
Iteration 787/1000 | Loss: 0.00004566
Iteration 788/1000 | Loss: 0.00004566
Iteration 789/1000 | Loss: 0.00004566
Iteration 790/1000 | Loss: 0.00004566
Iteration 791/1000 | Loss: 0.00004566
Iteration 792/1000 | Loss: 0.00004566
Iteration 793/1000 | Loss: 0.00004565
Iteration 794/1000 | Loss: 0.00004565
Iteration 795/1000 | Loss: 0.00004565
Iteration 796/1000 | Loss: 0.00004565
Iteration 797/1000 | Loss: 0.00004564
Iteration 798/1000 | Loss: 0.00004564
Iteration 799/1000 | Loss: 0.00004564
Iteration 800/1000 | Loss: 0.00004564
Iteration 801/1000 | Loss: 0.00004563
Iteration 802/1000 | Loss: 0.00004563
Iteration 803/1000 | Loss: 0.00004563
Iteration 804/1000 | Loss: 0.00004563
Iteration 805/1000 | Loss: 0.00004562
Iteration 806/1000 | Loss: 0.00004562
Iteration 807/1000 | Loss: 0.00004562
Iteration 808/1000 | Loss: 0.00004562
Iteration 809/1000 | Loss: 0.00004562
Iteration 810/1000 | Loss: 0.00004561
Iteration 811/1000 | Loss: 0.00004561
Iteration 812/1000 | Loss: 0.00004561
Iteration 813/1000 | Loss: 0.00004561
Iteration 814/1000 | Loss: 0.00004561
Iteration 815/1000 | Loss: 0.00004561
Iteration 816/1000 | Loss: 0.00004561
Iteration 817/1000 | Loss: 0.00004561
Iteration 818/1000 | Loss: 0.00004561
Iteration 819/1000 | Loss: 0.00004561
Iteration 820/1000 | Loss: 0.00004561
Iteration 821/1000 | Loss: 0.00004560
Iteration 822/1000 | Loss: 0.00004560
Iteration 823/1000 | Loss: 0.00004560
Iteration 824/1000 | Loss: 0.00004560
Iteration 825/1000 | Loss: 0.00004560
Iteration 826/1000 | Loss: 0.00004560
Iteration 827/1000 | Loss: 0.00004560
Iteration 828/1000 | Loss: 0.00004560
Iteration 829/1000 | Loss: 0.00004560
Iteration 830/1000 | Loss: 0.00004559
Iteration 831/1000 | Loss: 0.00004559
Iteration 832/1000 | Loss: 0.00004559
Iteration 833/1000 | Loss: 0.00004559
Iteration 834/1000 | Loss: 0.00004559
Iteration 835/1000 | Loss: 0.00004559
Iteration 836/1000 | Loss: 0.00004559
Iteration 837/1000 | Loss: 0.00004558
Iteration 838/1000 | Loss: 0.00004558
Iteration 839/1000 | Loss: 0.00004558
Iteration 840/1000 | Loss: 0.00004558
Iteration 841/1000 | Loss: 0.00004558
Iteration 842/1000 | Loss: 0.00004558
Iteration 843/1000 | Loss: 0.00004558
Iteration 844/1000 | Loss: 0.00004558
Iteration 845/1000 | Loss: 0.00004558
Iteration 846/1000 | Loss: 0.00004558
Iteration 847/1000 | Loss: 0.00004558
Iteration 848/1000 | Loss: 0.00004558
Iteration 849/1000 | Loss: 0.00004557
Iteration 850/1000 | Loss: 0.00004557
Iteration 851/1000 | Loss: 0.00004557
Iteration 852/1000 | Loss: 0.00004557
Iteration 853/1000 | Loss: 0.00004557
Iteration 854/1000 | Loss: 0.00004557
Iteration 855/1000 | Loss: 0.00004557
Iteration 856/1000 | Loss: 0.00004557
Iteration 857/1000 | Loss: 0.00004557
Iteration 858/1000 | Loss: 0.00004557
Iteration 859/1000 | Loss: 0.00004557
Iteration 860/1000 | Loss: 0.00004557
Iteration 861/1000 | Loss: 0.00004557
Iteration 862/1000 | Loss: 0.00004557
Iteration 863/1000 | Loss: 0.00004557
Iteration 864/1000 | Loss: 0.00004557
Iteration 865/1000 | Loss: 0.00004557
Iteration 866/1000 | Loss: 0.00004557
Iteration 867/1000 | Loss: 0.00004557
Iteration 868/1000 | Loss: 0.00004557
Iteration 869/1000 | Loss: 0.00004557
Iteration 870/1000 | Loss: 0.00004557
Iteration 871/1000 | Loss: 0.00004557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 871. Stopping optimization.
Last 5 losses: [4.556520070764236e-05, 4.556520070764236e-05, 4.556520070764236e-05, 4.556520070764236e-05, 4.556520070764236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.556520070764236e-05

Optimization complete. Final v2v error: 4.749025344848633 mm

Highest mean error: 11.788946151733398 mm for frame 74

Lowest mean error: 4.011093616485596 mm for frame 115

Saving results

Total time: 821.1964132785797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471691
Iteration 2/25 | Loss: 0.00113382
Iteration 3/25 | Loss: 0.00105323
Iteration 4/25 | Loss: 0.00104313
Iteration 5/25 | Loss: 0.00103994
Iteration 6/25 | Loss: 0.00103930
Iteration 7/25 | Loss: 0.00103930
Iteration 8/25 | Loss: 0.00103930
Iteration 9/25 | Loss: 0.00103930
Iteration 10/25 | Loss: 0.00103930
Iteration 11/25 | Loss: 0.00103930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010393047705292702, 0.0010393047705292702, 0.0010393047705292702, 0.0010393047705292702, 0.0010393047705292702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010393047705292702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79404712
Iteration 2/25 | Loss: 0.00065799
Iteration 3/25 | Loss: 0.00065797
Iteration 4/25 | Loss: 0.00065797
Iteration 5/25 | Loss: 0.00065797
Iteration 6/25 | Loss: 0.00065797
Iteration 7/25 | Loss: 0.00065797
Iteration 8/25 | Loss: 0.00065797
Iteration 9/25 | Loss: 0.00065797
Iteration 10/25 | Loss: 0.00065797
Iteration 11/25 | Loss: 0.00065797
Iteration 12/25 | Loss: 0.00065797
Iteration 13/25 | Loss: 0.00065797
Iteration 14/25 | Loss: 0.00065797
Iteration 15/25 | Loss: 0.00065797
Iteration 16/25 | Loss: 0.00065797
Iteration 17/25 | Loss: 0.00065797
Iteration 18/25 | Loss: 0.00065797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006579678156413138, 0.0006579678156413138, 0.0006579678156413138, 0.0006579678156413138, 0.0006579678156413138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006579678156413138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065797
Iteration 2/1000 | Loss: 0.00001785
Iteration 3/1000 | Loss: 0.00001325
Iteration 4/1000 | Loss: 0.00001214
Iteration 5/1000 | Loss: 0.00001140
Iteration 6/1000 | Loss: 0.00001101
Iteration 7/1000 | Loss: 0.00001069
Iteration 8/1000 | Loss: 0.00001033
Iteration 9/1000 | Loss: 0.00001014
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000993
Iteration 13/1000 | Loss: 0.00000985
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000982
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000981
Iteration 18/1000 | Loss: 0.00000978
Iteration 19/1000 | Loss: 0.00000976
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000974
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000973
Iteration 24/1000 | Loss: 0.00000973
Iteration 25/1000 | Loss: 0.00000973
Iteration 26/1000 | Loss: 0.00000973
Iteration 27/1000 | Loss: 0.00000973
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000972
Iteration 31/1000 | Loss: 0.00000972
Iteration 32/1000 | Loss: 0.00000971
Iteration 33/1000 | Loss: 0.00000971
Iteration 34/1000 | Loss: 0.00000971
Iteration 35/1000 | Loss: 0.00000969
Iteration 36/1000 | Loss: 0.00000969
Iteration 37/1000 | Loss: 0.00000969
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000969
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000969
Iteration 44/1000 | Loss: 0.00000969
Iteration 45/1000 | Loss: 0.00000969
Iteration 46/1000 | Loss: 0.00000969
Iteration 47/1000 | Loss: 0.00000969
Iteration 48/1000 | Loss: 0.00000969
Iteration 49/1000 | Loss: 0.00000969
Iteration 50/1000 | Loss: 0.00000969
Iteration 51/1000 | Loss: 0.00000969
Iteration 52/1000 | Loss: 0.00000969
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000969
Iteration 56/1000 | Loss: 0.00000969
Iteration 57/1000 | Loss: 0.00000968
Iteration 58/1000 | Loss: 0.00000968
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000967
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000964
Iteration 65/1000 | Loss: 0.00000964
Iteration 66/1000 | Loss: 0.00000964
Iteration 67/1000 | Loss: 0.00000964
Iteration 68/1000 | Loss: 0.00000963
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000963
Iteration 71/1000 | Loss: 0.00000962
Iteration 72/1000 | Loss: 0.00000962
Iteration 73/1000 | Loss: 0.00000961
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000958
Iteration 76/1000 | Loss: 0.00000958
Iteration 77/1000 | Loss: 0.00000957
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000956
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000955
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000954
Iteration 86/1000 | Loss: 0.00000954
Iteration 87/1000 | Loss: 0.00000954
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000953
Iteration 90/1000 | Loss: 0.00000953
Iteration 91/1000 | Loss: 0.00000953
Iteration 92/1000 | Loss: 0.00000953
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000952
Iteration 95/1000 | Loss: 0.00000952
Iteration 96/1000 | Loss: 0.00000952
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000952
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000951
Iteration 102/1000 | Loss: 0.00000951
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000948
Iteration 114/1000 | Loss: 0.00000948
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000948
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000947
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000946
Iteration 122/1000 | Loss: 0.00000946
Iteration 123/1000 | Loss: 0.00000946
Iteration 124/1000 | Loss: 0.00000946
Iteration 125/1000 | Loss: 0.00000945
Iteration 126/1000 | Loss: 0.00000945
Iteration 127/1000 | Loss: 0.00000945
Iteration 128/1000 | Loss: 0.00000944
Iteration 129/1000 | Loss: 0.00000944
Iteration 130/1000 | Loss: 0.00000944
Iteration 131/1000 | Loss: 0.00000943
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000940
Iteration 142/1000 | Loss: 0.00000940
Iteration 143/1000 | Loss: 0.00000940
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000938
Iteration 149/1000 | Loss: 0.00000938
Iteration 150/1000 | Loss: 0.00000938
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000937
Iteration 153/1000 | Loss: 0.00000937
Iteration 154/1000 | Loss: 0.00000937
Iteration 155/1000 | Loss: 0.00000937
Iteration 156/1000 | Loss: 0.00000936
Iteration 157/1000 | Loss: 0.00000936
Iteration 158/1000 | Loss: 0.00000936
Iteration 159/1000 | Loss: 0.00000936
Iteration 160/1000 | Loss: 0.00000936
Iteration 161/1000 | Loss: 0.00000936
Iteration 162/1000 | Loss: 0.00000936
Iteration 163/1000 | Loss: 0.00000936
Iteration 164/1000 | Loss: 0.00000936
Iteration 165/1000 | Loss: 0.00000935
Iteration 166/1000 | Loss: 0.00000935
Iteration 167/1000 | Loss: 0.00000935
Iteration 168/1000 | Loss: 0.00000935
Iteration 169/1000 | Loss: 0.00000935
Iteration 170/1000 | Loss: 0.00000935
Iteration 171/1000 | Loss: 0.00000935
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000935
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000934
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000934
Iteration 184/1000 | Loss: 0.00000934
Iteration 185/1000 | Loss: 0.00000934
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000934
Iteration 188/1000 | Loss: 0.00000933
Iteration 189/1000 | Loss: 0.00000933
Iteration 190/1000 | Loss: 0.00000933
Iteration 191/1000 | Loss: 0.00000933
Iteration 192/1000 | Loss: 0.00000933
Iteration 193/1000 | Loss: 0.00000933
Iteration 194/1000 | Loss: 0.00000933
Iteration 195/1000 | Loss: 0.00000933
Iteration 196/1000 | Loss: 0.00000933
Iteration 197/1000 | Loss: 0.00000933
Iteration 198/1000 | Loss: 0.00000932
Iteration 199/1000 | Loss: 0.00000932
Iteration 200/1000 | Loss: 0.00000932
Iteration 201/1000 | Loss: 0.00000932
Iteration 202/1000 | Loss: 0.00000932
Iteration 203/1000 | Loss: 0.00000932
Iteration 204/1000 | Loss: 0.00000932
Iteration 205/1000 | Loss: 0.00000932
Iteration 206/1000 | Loss: 0.00000932
Iteration 207/1000 | Loss: 0.00000932
Iteration 208/1000 | Loss: 0.00000932
Iteration 209/1000 | Loss: 0.00000932
Iteration 210/1000 | Loss: 0.00000932
Iteration 211/1000 | Loss: 0.00000932
Iteration 212/1000 | Loss: 0.00000932
Iteration 213/1000 | Loss: 0.00000932
Iteration 214/1000 | Loss: 0.00000932
Iteration 215/1000 | Loss: 0.00000932
Iteration 216/1000 | Loss: 0.00000931
Iteration 217/1000 | Loss: 0.00000931
Iteration 218/1000 | Loss: 0.00000931
Iteration 219/1000 | Loss: 0.00000931
Iteration 220/1000 | Loss: 0.00000931
Iteration 221/1000 | Loss: 0.00000931
Iteration 222/1000 | Loss: 0.00000931
Iteration 223/1000 | Loss: 0.00000931
Iteration 224/1000 | Loss: 0.00000931
Iteration 225/1000 | Loss: 0.00000931
Iteration 226/1000 | Loss: 0.00000930
Iteration 227/1000 | Loss: 0.00000930
Iteration 228/1000 | Loss: 0.00000930
Iteration 229/1000 | Loss: 0.00000930
Iteration 230/1000 | Loss: 0.00000930
Iteration 231/1000 | Loss: 0.00000930
Iteration 232/1000 | Loss: 0.00000930
Iteration 233/1000 | Loss: 0.00000930
Iteration 234/1000 | Loss: 0.00000930
Iteration 235/1000 | Loss: 0.00000930
Iteration 236/1000 | Loss: 0.00000930
Iteration 237/1000 | Loss: 0.00000930
Iteration 238/1000 | Loss: 0.00000930
Iteration 239/1000 | Loss: 0.00000930
Iteration 240/1000 | Loss: 0.00000930
Iteration 241/1000 | Loss: 0.00000930
Iteration 242/1000 | Loss: 0.00000930
Iteration 243/1000 | Loss: 0.00000930
Iteration 244/1000 | Loss: 0.00000930
Iteration 245/1000 | Loss: 0.00000930
Iteration 246/1000 | Loss: 0.00000930
Iteration 247/1000 | Loss: 0.00000930
Iteration 248/1000 | Loss: 0.00000930
Iteration 249/1000 | Loss: 0.00000930
Iteration 250/1000 | Loss: 0.00000930
Iteration 251/1000 | Loss: 0.00000930
Iteration 252/1000 | Loss: 0.00000930
Iteration 253/1000 | Loss: 0.00000930
Iteration 254/1000 | Loss: 0.00000930
Iteration 255/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [9.3006647148286e-06, 9.3006647148286e-06, 9.3006647148286e-06, 9.3006647148286e-06, 9.3006647148286e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.3006647148286e-06

Optimization complete. Final v2v error: 2.6243391036987305 mm

Highest mean error: 3.309537172317505 mm for frame 187

Lowest mean error: 2.356722831726074 mm for frame 133

Saving results

Total time: 46.62345337867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967928
Iteration 2/25 | Loss: 0.00252302
Iteration 3/25 | Loss: 0.00195284
Iteration 4/25 | Loss: 0.00179928
Iteration 5/25 | Loss: 0.00132740
Iteration 6/25 | Loss: 0.00122692
Iteration 7/25 | Loss: 0.00122114
Iteration 8/25 | Loss: 0.00117756
Iteration 9/25 | Loss: 0.00116953
Iteration 10/25 | Loss: 0.00117219
Iteration 11/25 | Loss: 0.00116677
Iteration 12/25 | Loss: 0.00115954
Iteration 13/25 | Loss: 0.00115709
Iteration 14/25 | Loss: 0.00115635
Iteration 15/25 | Loss: 0.00115618
Iteration 16/25 | Loss: 0.00115611
Iteration 17/25 | Loss: 0.00115610
Iteration 18/25 | Loss: 0.00115609
Iteration 19/25 | Loss: 0.00115609
Iteration 20/25 | Loss: 0.00115609
Iteration 21/25 | Loss: 0.00115609
Iteration 22/25 | Loss: 0.00115609
Iteration 23/25 | Loss: 0.00115608
Iteration 24/25 | Loss: 0.00115608
Iteration 25/25 | Loss: 0.00115608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39091861
Iteration 2/25 | Loss: 0.00044874
Iteration 3/25 | Loss: 0.00044874
Iteration 4/25 | Loss: 0.00044874
Iteration 5/25 | Loss: 0.00044874
Iteration 6/25 | Loss: 0.00044874
Iteration 7/25 | Loss: 0.00044874
Iteration 8/25 | Loss: 0.00044874
Iteration 9/25 | Loss: 0.00044874
Iteration 10/25 | Loss: 0.00044874
Iteration 11/25 | Loss: 0.00044874
Iteration 12/25 | Loss: 0.00044874
Iteration 13/25 | Loss: 0.00044874
Iteration 14/25 | Loss: 0.00044874
Iteration 15/25 | Loss: 0.00044874
Iteration 16/25 | Loss: 0.00044874
Iteration 17/25 | Loss: 0.00044874
Iteration 18/25 | Loss: 0.00044874
Iteration 19/25 | Loss: 0.00044874
Iteration 20/25 | Loss: 0.00044874
Iteration 21/25 | Loss: 0.00044874
Iteration 22/25 | Loss: 0.00044874
Iteration 23/25 | Loss: 0.00044874
Iteration 24/25 | Loss: 0.00044874
Iteration 25/25 | Loss: 0.00044874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044874
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00002041
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001866
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001861
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001857
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001843
Iteration 26/1000 | Loss: 0.00001843
Iteration 27/1000 | Loss: 0.00001843
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001842
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001836
Iteration 47/1000 | Loss: 0.00001836
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.826269544835668e-05, 1.826269544835668e-05, 1.826269544835668e-05, 1.826269544835668e-05, 1.826269544835668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826269544835668e-05

Optimization complete. Final v2v error: 3.6261658668518066 mm

Highest mean error: 3.7666378021240234 mm for frame 0

Lowest mean error: 3.386730432510376 mm for frame 18

Saving results

Total time: 49.63704490661621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421053
Iteration 2/25 | Loss: 0.00116305
Iteration 3/25 | Loss: 0.00108101
Iteration 4/25 | Loss: 0.00106021
Iteration 5/25 | Loss: 0.00105307
Iteration 6/25 | Loss: 0.00105160
Iteration 7/25 | Loss: 0.00105160
Iteration 8/25 | Loss: 0.00105160
Iteration 9/25 | Loss: 0.00105160
Iteration 10/25 | Loss: 0.00105160
Iteration 11/25 | Loss: 0.00105160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010516003239899874, 0.0010516003239899874, 0.0010516003239899874, 0.0010516003239899874, 0.0010516003239899874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010516003239899874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37932706
Iteration 2/25 | Loss: 0.00064303
Iteration 3/25 | Loss: 0.00064303
Iteration 4/25 | Loss: 0.00064303
Iteration 5/25 | Loss: 0.00064303
Iteration 6/25 | Loss: 0.00064303
Iteration 7/25 | Loss: 0.00064303
Iteration 8/25 | Loss: 0.00064303
Iteration 9/25 | Loss: 0.00064303
Iteration 10/25 | Loss: 0.00064303
Iteration 11/25 | Loss: 0.00064303
Iteration 12/25 | Loss: 0.00064303
Iteration 13/25 | Loss: 0.00064303
Iteration 14/25 | Loss: 0.00064303
Iteration 15/25 | Loss: 0.00064303
Iteration 16/25 | Loss: 0.00064303
Iteration 17/25 | Loss: 0.00064303
Iteration 18/25 | Loss: 0.00064303
Iteration 19/25 | Loss: 0.00064303
Iteration 20/25 | Loss: 0.00064303
Iteration 21/25 | Loss: 0.00064303
Iteration 22/25 | Loss: 0.00064303
Iteration 23/25 | Loss: 0.00064303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006430255598388612, 0.0006430255598388612, 0.0006430255598388612, 0.0006430255598388612, 0.0006430255598388612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006430255598388612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064303
Iteration 2/1000 | Loss: 0.00002476
Iteration 3/1000 | Loss: 0.00001671
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001206
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001195
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001166
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001164
Iteration 44/1000 | Loss: 0.00001164
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001141
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001138
Iteration 119/1000 | Loss: 0.00001138
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001138
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001134
Iteration 146/1000 | Loss: 0.00001134
Iteration 147/1000 | Loss: 0.00001134
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001134
Iteration 151/1000 | Loss: 0.00001134
Iteration 152/1000 | Loss: 0.00001134
Iteration 153/1000 | Loss: 0.00001134
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001133
Iteration 161/1000 | Loss: 0.00001133
Iteration 162/1000 | Loss: 0.00001133
Iteration 163/1000 | Loss: 0.00001133
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001132
Iteration 167/1000 | Loss: 0.00001132
Iteration 168/1000 | Loss: 0.00001132
Iteration 169/1000 | Loss: 0.00001132
Iteration 170/1000 | Loss: 0.00001132
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1324120350764133e-05, 1.1324120350764133e-05, 1.1324120350764133e-05, 1.1324120350764133e-05, 1.1324120350764133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1324120350764133e-05

Optimization complete. Final v2v error: 2.901073694229126 mm

Highest mean error: 3.1235311031341553 mm for frame 104

Lowest mean error: 2.747328996658325 mm for frame 156

Saving results

Total time: 38.335911989212036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974170
Iteration 2/25 | Loss: 0.00166996
Iteration 3/25 | Loss: 0.00124120
Iteration 4/25 | Loss: 0.00119949
Iteration 5/25 | Loss: 0.00119339
Iteration 6/25 | Loss: 0.00122376
Iteration 7/25 | Loss: 0.00114698
Iteration 8/25 | Loss: 0.00113948
Iteration 9/25 | Loss: 0.00112635
Iteration 10/25 | Loss: 0.00111502
Iteration 11/25 | Loss: 0.00110432
Iteration 12/25 | Loss: 0.00111300
Iteration 13/25 | Loss: 0.00109927
Iteration 14/25 | Loss: 0.00109967
Iteration 15/25 | Loss: 0.00109408
Iteration 16/25 | Loss: 0.00108760
Iteration 17/25 | Loss: 0.00108279
Iteration 18/25 | Loss: 0.00108111
Iteration 19/25 | Loss: 0.00108006
Iteration 20/25 | Loss: 0.00107923
Iteration 21/25 | Loss: 0.00107873
Iteration 22/25 | Loss: 0.00107860
Iteration 23/25 | Loss: 0.00107855
Iteration 24/25 | Loss: 0.00107855
Iteration 25/25 | Loss: 0.00107855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48494565
Iteration 2/25 | Loss: 0.00094755
Iteration 3/25 | Loss: 0.00094755
Iteration 4/25 | Loss: 0.00094755
Iteration 5/25 | Loss: 0.00094755
Iteration 6/25 | Loss: 0.00094755
Iteration 7/25 | Loss: 0.00094755
Iteration 8/25 | Loss: 0.00094755
Iteration 9/25 | Loss: 0.00094755
Iteration 10/25 | Loss: 0.00094755
Iteration 11/25 | Loss: 0.00094755
Iteration 12/25 | Loss: 0.00094755
Iteration 13/25 | Loss: 0.00094755
Iteration 14/25 | Loss: 0.00094755
Iteration 15/25 | Loss: 0.00094755
Iteration 16/25 | Loss: 0.00094755
Iteration 17/25 | Loss: 0.00094755
Iteration 18/25 | Loss: 0.00094755
Iteration 19/25 | Loss: 0.00094755
Iteration 20/25 | Loss: 0.00094755
Iteration 21/25 | Loss: 0.00094755
Iteration 22/25 | Loss: 0.00094755
Iteration 23/25 | Loss: 0.00094755
Iteration 24/25 | Loss: 0.00094755
Iteration 25/25 | Loss: 0.00094755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094755
Iteration 2/1000 | Loss: 0.00004857
Iteration 3/1000 | Loss: 0.00003215
Iteration 4/1000 | Loss: 0.00008358
Iteration 5/1000 | Loss: 0.00003066
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00008087
Iteration 8/1000 | Loss: 0.00002288
Iteration 9/1000 | Loss: 0.00002247
Iteration 10/1000 | Loss: 0.00005155
Iteration 11/1000 | Loss: 0.00009511
Iteration 12/1000 | Loss: 0.00004206
Iteration 13/1000 | Loss: 0.00002157
Iteration 14/1000 | Loss: 0.00003319
Iteration 15/1000 | Loss: 0.00002130
Iteration 16/1000 | Loss: 0.00002127
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00017303
Iteration 19/1000 | Loss: 0.00010767
Iteration 20/1000 | Loss: 0.00002508
Iteration 21/1000 | Loss: 0.00002140
Iteration 22/1000 | Loss: 0.00005300
Iteration 23/1000 | Loss: 0.00012024
Iteration 24/1000 | Loss: 0.00008987
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00004855
Iteration 27/1000 | Loss: 0.00004888
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001776
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00012028
Iteration 32/1000 | Loss: 0.00016713
Iteration 33/1000 | Loss: 0.00011923
Iteration 34/1000 | Loss: 0.00008525
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00074431
Iteration 37/1000 | Loss: 0.00053428
Iteration 38/1000 | Loss: 0.00061236
Iteration 39/1000 | Loss: 0.00054715
Iteration 40/1000 | Loss: 0.00039382
Iteration 41/1000 | Loss: 0.00109822
Iteration 42/1000 | Loss: 0.00086609
Iteration 43/1000 | Loss: 0.00070025
Iteration 44/1000 | Loss: 0.00033617
Iteration 45/1000 | Loss: 0.00003635
Iteration 46/1000 | Loss: 0.00005836
Iteration 47/1000 | Loss: 0.00002086
Iteration 48/1000 | Loss: 0.00002803
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00003110
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00008307
Iteration 58/1000 | Loss: 0.00003701
Iteration 59/1000 | Loss: 0.00011085
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00004284
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001978
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001177
Iteration 112/1000 | Loss: 0.00001177
Iteration 113/1000 | Loss: 0.00001177
Iteration 114/1000 | Loss: 0.00001177
Iteration 115/1000 | Loss: 0.00001177
Iteration 116/1000 | Loss: 0.00001176
Iteration 117/1000 | Loss: 0.00001176
Iteration 118/1000 | Loss: 0.00001176
Iteration 119/1000 | Loss: 0.00001176
Iteration 120/1000 | Loss: 0.00001176
Iteration 121/1000 | Loss: 0.00001175
Iteration 122/1000 | Loss: 0.00001175
Iteration 123/1000 | Loss: 0.00001175
Iteration 124/1000 | Loss: 0.00001175
Iteration 125/1000 | Loss: 0.00001175
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001175
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00003311
Iteration 131/1000 | Loss: 0.00005802
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001174
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001171
Iteration 142/1000 | Loss: 0.00001171
Iteration 143/1000 | Loss: 0.00002120
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001169
Iteration 166/1000 | Loss: 0.00001169
Iteration 167/1000 | Loss: 0.00001169
Iteration 168/1000 | Loss: 0.00001169
Iteration 169/1000 | Loss: 0.00001169
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Iteration 174/1000 | Loss: 0.00001168
Iteration 175/1000 | Loss: 0.00001168
Iteration 176/1000 | Loss: 0.00001168
Iteration 177/1000 | Loss: 0.00001168
Iteration 178/1000 | Loss: 0.00001168
Iteration 179/1000 | Loss: 0.00001168
Iteration 180/1000 | Loss: 0.00001168
Iteration 181/1000 | Loss: 0.00001168
Iteration 182/1000 | Loss: 0.00001168
Iteration 183/1000 | Loss: 0.00001168
Iteration 184/1000 | Loss: 0.00001168
Iteration 185/1000 | Loss: 0.00001168
Iteration 186/1000 | Loss: 0.00001168
Iteration 187/1000 | Loss: 0.00001168
Iteration 188/1000 | Loss: 0.00001168
Iteration 189/1000 | Loss: 0.00001168
Iteration 190/1000 | Loss: 0.00001167
Iteration 191/1000 | Loss: 0.00001167
Iteration 192/1000 | Loss: 0.00001167
Iteration 193/1000 | Loss: 0.00001167
Iteration 194/1000 | Loss: 0.00001167
Iteration 195/1000 | Loss: 0.00001167
Iteration 196/1000 | Loss: 0.00001167
Iteration 197/1000 | Loss: 0.00001167
Iteration 198/1000 | Loss: 0.00001167
Iteration 199/1000 | Loss: 0.00001167
Iteration 200/1000 | Loss: 0.00001167
Iteration 201/1000 | Loss: 0.00001167
Iteration 202/1000 | Loss: 0.00001167
Iteration 203/1000 | Loss: 0.00001167
Iteration 204/1000 | Loss: 0.00001167
Iteration 205/1000 | Loss: 0.00001167
Iteration 206/1000 | Loss: 0.00001167
Iteration 207/1000 | Loss: 0.00001167
Iteration 208/1000 | Loss: 0.00001167
Iteration 209/1000 | Loss: 0.00001167
Iteration 210/1000 | Loss: 0.00001167
Iteration 211/1000 | Loss: 0.00001167
Iteration 212/1000 | Loss: 0.00001166
Iteration 213/1000 | Loss: 0.00001166
Iteration 214/1000 | Loss: 0.00001166
Iteration 215/1000 | Loss: 0.00003162
Iteration 216/1000 | Loss: 0.00003162
Iteration 217/1000 | Loss: 0.00005493
Iteration 218/1000 | Loss: 0.00004097
Iteration 219/1000 | Loss: 0.00001532
Iteration 220/1000 | Loss: 0.00001173
Iteration 221/1000 | Loss: 0.00001173
Iteration 222/1000 | Loss: 0.00001173
Iteration 223/1000 | Loss: 0.00001173
Iteration 224/1000 | Loss: 0.00001173
Iteration 225/1000 | Loss: 0.00001173
Iteration 226/1000 | Loss: 0.00001173
Iteration 227/1000 | Loss: 0.00001172
Iteration 228/1000 | Loss: 0.00001172
Iteration 229/1000 | Loss: 0.00001172
Iteration 230/1000 | Loss: 0.00001172
Iteration 231/1000 | Loss: 0.00001172
Iteration 232/1000 | Loss: 0.00001172
Iteration 233/1000 | Loss: 0.00001171
Iteration 234/1000 | Loss: 0.00001167
Iteration 235/1000 | Loss: 0.00001167
Iteration 236/1000 | Loss: 0.00001166
Iteration 237/1000 | Loss: 0.00001166
Iteration 238/1000 | Loss: 0.00001166
Iteration 239/1000 | Loss: 0.00001165
Iteration 240/1000 | Loss: 0.00001165
Iteration 241/1000 | Loss: 0.00001165
Iteration 242/1000 | Loss: 0.00001164
Iteration 243/1000 | Loss: 0.00001164
Iteration 244/1000 | Loss: 0.00001164
Iteration 245/1000 | Loss: 0.00001164
Iteration 246/1000 | Loss: 0.00001164
Iteration 247/1000 | Loss: 0.00001164
Iteration 248/1000 | Loss: 0.00001164
Iteration 249/1000 | Loss: 0.00001164
Iteration 250/1000 | Loss: 0.00001164
Iteration 251/1000 | Loss: 0.00001164
Iteration 252/1000 | Loss: 0.00001164
Iteration 253/1000 | Loss: 0.00001164
Iteration 254/1000 | Loss: 0.00001164
Iteration 255/1000 | Loss: 0.00001164
Iteration 256/1000 | Loss: 0.00001164
Iteration 257/1000 | Loss: 0.00001164
Iteration 258/1000 | Loss: 0.00001164
Iteration 259/1000 | Loss: 0.00001164
Iteration 260/1000 | Loss: 0.00001163
Iteration 261/1000 | Loss: 0.00001163
Iteration 262/1000 | Loss: 0.00001163
Iteration 263/1000 | Loss: 0.00001163
Iteration 264/1000 | Loss: 0.00001163
Iteration 265/1000 | Loss: 0.00001163
Iteration 266/1000 | Loss: 0.00001163
Iteration 267/1000 | Loss: 0.00001163
Iteration 268/1000 | Loss: 0.00001163
Iteration 269/1000 | Loss: 0.00001163
Iteration 270/1000 | Loss: 0.00001163
Iteration 271/1000 | Loss: 0.00001163
Iteration 272/1000 | Loss: 0.00001163
Iteration 273/1000 | Loss: 0.00001163
Iteration 274/1000 | Loss: 0.00001163
Iteration 275/1000 | Loss: 0.00001163
Iteration 276/1000 | Loss: 0.00001163
Iteration 277/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.1634502698143478e-05, 1.1634502698143478e-05, 1.1634502698143478e-05, 1.1634502698143478e-05, 1.1634502698143478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1634502698143478e-05

Optimization complete. Final v2v error: 2.886563301086426 mm

Highest mean error: 4.499628067016602 mm for frame 53

Lowest mean error: 2.2710649967193604 mm for frame 97

Saving results

Total time: 140.1817548274994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442320
Iteration 2/25 | Loss: 0.00122014
Iteration 3/25 | Loss: 0.00112809
Iteration 4/25 | Loss: 0.00111721
Iteration 5/25 | Loss: 0.00111460
Iteration 6/25 | Loss: 0.00111406
Iteration 7/25 | Loss: 0.00111406
Iteration 8/25 | Loss: 0.00111406
Iteration 9/25 | Loss: 0.00111406
Iteration 10/25 | Loss: 0.00111406
Iteration 11/25 | Loss: 0.00111406
Iteration 12/25 | Loss: 0.00111406
Iteration 13/25 | Loss: 0.00111406
Iteration 14/25 | Loss: 0.00111406
Iteration 15/25 | Loss: 0.00111406
Iteration 16/25 | Loss: 0.00111406
Iteration 17/25 | Loss: 0.00111406
Iteration 18/25 | Loss: 0.00111406
Iteration 19/25 | Loss: 0.00111406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011140628485009074, 0.0011140628485009074, 0.0011140628485009074, 0.0011140628485009074, 0.0011140628485009074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011140628485009074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39072025
Iteration 2/25 | Loss: 0.00073295
Iteration 3/25 | Loss: 0.00073295
Iteration 4/25 | Loss: 0.00073295
Iteration 5/25 | Loss: 0.00073295
Iteration 6/25 | Loss: 0.00073295
Iteration 7/25 | Loss: 0.00073295
Iteration 8/25 | Loss: 0.00073295
Iteration 9/25 | Loss: 0.00073294
Iteration 10/25 | Loss: 0.00073294
Iteration 11/25 | Loss: 0.00073294
Iteration 12/25 | Loss: 0.00073294
Iteration 13/25 | Loss: 0.00073294
Iteration 14/25 | Loss: 0.00073294
Iteration 15/25 | Loss: 0.00073294
Iteration 16/25 | Loss: 0.00073294
Iteration 17/25 | Loss: 0.00073294
Iteration 18/25 | Loss: 0.00073294
Iteration 19/25 | Loss: 0.00073294
Iteration 20/25 | Loss: 0.00073294
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007329441723413765, 0.0007329441723413765, 0.0007329441723413765, 0.0007329441723413765, 0.0007329441723413765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007329441723413765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073294
Iteration 2/1000 | Loss: 0.00002706
Iteration 3/1000 | Loss: 0.00001798
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001482
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001444
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001433
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001424
Iteration 26/1000 | Loss: 0.00001424
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001420
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001413
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001412
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001406
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001398
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001396
Iteration 91/1000 | Loss: 0.00001396
Iteration 92/1000 | Loss: 0.00001396
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001395
Iteration 96/1000 | Loss: 0.00001395
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001390
Iteration 121/1000 | Loss: 0.00001390
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001390
Iteration 124/1000 | Loss: 0.00001389
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001389
Iteration 127/1000 | Loss: 0.00001389
Iteration 128/1000 | Loss: 0.00001388
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001388
Iteration 133/1000 | Loss: 0.00001388
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001387
Iteration 140/1000 | Loss: 0.00001387
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001386
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001386
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001385
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001384
Iteration 160/1000 | Loss: 0.00001384
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001384
Iteration 165/1000 | Loss: 0.00001384
Iteration 166/1000 | Loss: 0.00001384
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001384
Iteration 171/1000 | Loss: 0.00001384
Iteration 172/1000 | Loss: 0.00001383
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001382
Iteration 183/1000 | Loss: 0.00001382
Iteration 184/1000 | Loss: 0.00001382
Iteration 185/1000 | Loss: 0.00001382
Iteration 186/1000 | Loss: 0.00001382
Iteration 187/1000 | Loss: 0.00001382
Iteration 188/1000 | Loss: 0.00001382
Iteration 189/1000 | Loss: 0.00001382
Iteration 190/1000 | Loss: 0.00001382
Iteration 191/1000 | Loss: 0.00001382
Iteration 192/1000 | Loss: 0.00001381
Iteration 193/1000 | Loss: 0.00001381
Iteration 194/1000 | Loss: 0.00001381
Iteration 195/1000 | Loss: 0.00001381
Iteration 196/1000 | Loss: 0.00001381
Iteration 197/1000 | Loss: 0.00001381
Iteration 198/1000 | Loss: 0.00001381
Iteration 199/1000 | Loss: 0.00001381
Iteration 200/1000 | Loss: 0.00001381
Iteration 201/1000 | Loss: 0.00001381
Iteration 202/1000 | Loss: 0.00001381
Iteration 203/1000 | Loss: 0.00001381
Iteration 204/1000 | Loss: 0.00001381
Iteration 205/1000 | Loss: 0.00001381
Iteration 206/1000 | Loss: 0.00001381
Iteration 207/1000 | Loss: 0.00001381
Iteration 208/1000 | Loss: 0.00001381
Iteration 209/1000 | Loss: 0.00001381
Iteration 210/1000 | Loss: 0.00001381
Iteration 211/1000 | Loss: 0.00001381
Iteration 212/1000 | Loss: 0.00001381
Iteration 213/1000 | Loss: 0.00001381
Iteration 214/1000 | Loss: 0.00001381
Iteration 215/1000 | Loss: 0.00001381
Iteration 216/1000 | Loss: 0.00001380
Iteration 217/1000 | Loss: 0.00001380
Iteration 218/1000 | Loss: 0.00001380
Iteration 219/1000 | Loss: 0.00001380
Iteration 220/1000 | Loss: 0.00001380
Iteration 221/1000 | Loss: 0.00001380
Iteration 222/1000 | Loss: 0.00001379
Iteration 223/1000 | Loss: 0.00001379
Iteration 224/1000 | Loss: 0.00001379
Iteration 225/1000 | Loss: 0.00001379
Iteration 226/1000 | Loss: 0.00001379
Iteration 227/1000 | Loss: 0.00001379
Iteration 228/1000 | Loss: 0.00001379
Iteration 229/1000 | Loss: 0.00001379
Iteration 230/1000 | Loss: 0.00001379
Iteration 231/1000 | Loss: 0.00001379
Iteration 232/1000 | Loss: 0.00001378
Iteration 233/1000 | Loss: 0.00001378
Iteration 234/1000 | Loss: 0.00001378
Iteration 235/1000 | Loss: 0.00001378
Iteration 236/1000 | Loss: 0.00001378
Iteration 237/1000 | Loss: 0.00001378
Iteration 238/1000 | Loss: 0.00001378
Iteration 239/1000 | Loss: 0.00001378
Iteration 240/1000 | Loss: 0.00001378
Iteration 241/1000 | Loss: 0.00001378
Iteration 242/1000 | Loss: 0.00001377
Iteration 243/1000 | Loss: 0.00001377
Iteration 244/1000 | Loss: 0.00001377
Iteration 245/1000 | Loss: 0.00001377
Iteration 246/1000 | Loss: 0.00001377
Iteration 247/1000 | Loss: 0.00001377
Iteration 248/1000 | Loss: 0.00001377
Iteration 249/1000 | Loss: 0.00001377
Iteration 250/1000 | Loss: 0.00001377
Iteration 251/1000 | Loss: 0.00001377
Iteration 252/1000 | Loss: 0.00001377
Iteration 253/1000 | Loss: 0.00001377
Iteration 254/1000 | Loss: 0.00001377
Iteration 255/1000 | Loss: 0.00001377
Iteration 256/1000 | Loss: 0.00001376
Iteration 257/1000 | Loss: 0.00001376
Iteration 258/1000 | Loss: 0.00001376
Iteration 259/1000 | Loss: 0.00001376
Iteration 260/1000 | Loss: 0.00001376
Iteration 261/1000 | Loss: 0.00001376
Iteration 262/1000 | Loss: 0.00001376
Iteration 263/1000 | Loss: 0.00001376
Iteration 264/1000 | Loss: 0.00001376
Iteration 265/1000 | Loss: 0.00001376
Iteration 266/1000 | Loss: 0.00001376
Iteration 267/1000 | Loss: 0.00001376
Iteration 268/1000 | Loss: 0.00001376
Iteration 269/1000 | Loss: 0.00001376
Iteration 270/1000 | Loss: 0.00001376
Iteration 271/1000 | Loss: 0.00001376
Iteration 272/1000 | Loss: 0.00001376
Iteration 273/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [1.375778992951382e-05, 1.375778992951382e-05, 1.375778992951382e-05, 1.375778992951382e-05, 1.375778992951382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.375778992951382e-05

Optimization complete. Final v2v error: 3.0261220932006836 mm

Highest mean error: 3.455155849456787 mm for frame 143

Lowest mean error: 2.6902108192443848 mm for frame 131

Saving results

Total time: 46.44384431838989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378740
Iteration 2/25 | Loss: 0.00130904
Iteration 3/25 | Loss: 0.00109357
Iteration 4/25 | Loss: 0.00107059
Iteration 5/25 | Loss: 0.00106815
Iteration 6/25 | Loss: 0.00106815
Iteration 7/25 | Loss: 0.00106815
Iteration 8/25 | Loss: 0.00106815
Iteration 9/25 | Loss: 0.00106815
Iteration 10/25 | Loss: 0.00106815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010681513231247663, 0.0010681513231247663, 0.0010681513231247663, 0.0010681513231247663, 0.0010681513231247663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010681513231247663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36425662
Iteration 2/25 | Loss: 0.00059629
Iteration 3/25 | Loss: 0.00059629
Iteration 4/25 | Loss: 0.00059629
Iteration 5/25 | Loss: 0.00059628
Iteration 6/25 | Loss: 0.00059628
Iteration 7/25 | Loss: 0.00059628
Iteration 8/25 | Loss: 0.00059628
Iteration 9/25 | Loss: 0.00059628
Iteration 10/25 | Loss: 0.00059628
Iteration 11/25 | Loss: 0.00059628
Iteration 12/25 | Loss: 0.00059628
Iteration 13/25 | Loss: 0.00059628
Iteration 14/25 | Loss: 0.00059628
Iteration 15/25 | Loss: 0.00059628
Iteration 16/25 | Loss: 0.00059628
Iteration 17/25 | Loss: 0.00059628
Iteration 18/25 | Loss: 0.00059628
Iteration 19/25 | Loss: 0.00059628
Iteration 20/25 | Loss: 0.00059628
Iteration 21/25 | Loss: 0.00059628
Iteration 22/25 | Loss: 0.00059628
Iteration 23/25 | Loss: 0.00059628
Iteration 24/25 | Loss: 0.00059628
Iteration 25/25 | Loss: 0.00059628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059628
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001356
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001220
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001220
Iteration 175/1000 | Loss: 0.00001220
Iteration 176/1000 | Loss: 0.00001220
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001219
Iteration 182/1000 | Loss: 0.00001218
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001218
Iteration 185/1000 | Loss: 0.00001218
Iteration 186/1000 | Loss: 0.00001218
Iteration 187/1000 | Loss: 0.00001218
Iteration 188/1000 | Loss: 0.00001218
Iteration 189/1000 | Loss: 0.00001218
Iteration 190/1000 | Loss: 0.00001218
Iteration 191/1000 | Loss: 0.00001218
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001217
Iteration 194/1000 | Loss: 0.00001217
Iteration 195/1000 | Loss: 0.00001217
Iteration 196/1000 | Loss: 0.00001217
Iteration 197/1000 | Loss: 0.00001217
Iteration 198/1000 | Loss: 0.00001217
Iteration 199/1000 | Loss: 0.00001217
Iteration 200/1000 | Loss: 0.00001217
Iteration 201/1000 | Loss: 0.00001216
Iteration 202/1000 | Loss: 0.00001216
Iteration 203/1000 | Loss: 0.00001216
Iteration 204/1000 | Loss: 0.00001216
Iteration 205/1000 | Loss: 0.00001216
Iteration 206/1000 | Loss: 0.00001216
Iteration 207/1000 | Loss: 0.00001216
Iteration 208/1000 | Loss: 0.00001216
Iteration 209/1000 | Loss: 0.00001216
Iteration 210/1000 | Loss: 0.00001216
Iteration 211/1000 | Loss: 0.00001216
Iteration 212/1000 | Loss: 0.00001216
Iteration 213/1000 | Loss: 0.00001216
Iteration 214/1000 | Loss: 0.00001216
Iteration 215/1000 | Loss: 0.00001216
Iteration 216/1000 | Loss: 0.00001216
Iteration 217/1000 | Loss: 0.00001216
Iteration 218/1000 | Loss: 0.00001216
Iteration 219/1000 | Loss: 0.00001216
Iteration 220/1000 | Loss: 0.00001216
Iteration 221/1000 | Loss: 0.00001216
Iteration 222/1000 | Loss: 0.00001216
Iteration 223/1000 | Loss: 0.00001216
Iteration 224/1000 | Loss: 0.00001216
Iteration 225/1000 | Loss: 0.00001216
Iteration 226/1000 | Loss: 0.00001216
Iteration 227/1000 | Loss: 0.00001216
Iteration 228/1000 | Loss: 0.00001216
Iteration 229/1000 | Loss: 0.00001216
Iteration 230/1000 | Loss: 0.00001216
Iteration 231/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.2155600416008383e-05, 1.2155600416008383e-05, 1.2155600416008383e-05, 1.2155600416008383e-05, 1.2155600416008383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2155600416008383e-05

Optimization complete. Final v2v error: 2.910127639770508 mm

Highest mean error: 3.1695210933685303 mm for frame 0

Lowest mean error: 2.7524728775024414 mm for frame 150

Saving results

Total time: 39.16594648361206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401298
Iteration 2/25 | Loss: 0.00123408
Iteration 3/25 | Loss: 0.00109013
Iteration 4/25 | Loss: 0.00107061
Iteration 5/25 | Loss: 0.00106569
Iteration 6/25 | Loss: 0.00106407
Iteration 7/25 | Loss: 0.00106401
Iteration 8/25 | Loss: 0.00106401
Iteration 9/25 | Loss: 0.00106401
Iteration 10/25 | Loss: 0.00106401
Iteration 11/25 | Loss: 0.00106401
Iteration 12/25 | Loss: 0.00106401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010640070540830493, 0.0010640070540830493, 0.0010640070540830493, 0.0010640070540830493, 0.0010640070540830493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010640070540830493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43223798
Iteration 2/25 | Loss: 0.00077125
Iteration 3/25 | Loss: 0.00077125
Iteration 4/25 | Loss: 0.00077125
Iteration 5/25 | Loss: 0.00077125
Iteration 6/25 | Loss: 0.00077125
Iteration 7/25 | Loss: 0.00077125
Iteration 8/25 | Loss: 0.00077125
Iteration 9/25 | Loss: 0.00077125
Iteration 10/25 | Loss: 0.00077125
Iteration 11/25 | Loss: 0.00077125
Iteration 12/25 | Loss: 0.00077125
Iteration 13/25 | Loss: 0.00077125
Iteration 14/25 | Loss: 0.00077125
Iteration 15/25 | Loss: 0.00077124
Iteration 16/25 | Loss: 0.00077124
Iteration 17/25 | Loss: 0.00077124
Iteration 18/25 | Loss: 0.00077124
Iteration 19/25 | Loss: 0.00077124
Iteration 20/25 | Loss: 0.00077124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007712449878454208, 0.0007712449878454208, 0.0007712449878454208, 0.0007712449878454208, 0.0007712449878454208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007712449878454208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077124
Iteration 2/1000 | Loss: 0.00003797
Iteration 3/1000 | Loss: 0.00002342
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001210
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001149
Iteration 35/1000 | Loss: 0.00001149
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001139
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001138
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001137
Iteration 98/1000 | Loss: 0.00001137
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001137
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001137
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001137
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.1366239050403237e-05, 1.1366239050403237e-05, 1.1366239050403237e-05, 1.1366239050403237e-05, 1.1366239050403237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1366239050403237e-05

Optimization complete. Final v2v error: 2.872568130493164 mm

Highest mean error: 3.59165096282959 mm for frame 105

Lowest mean error: 2.4071507453918457 mm for frame 161

Saving results

Total time: 35.07795977592468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380049
Iteration 2/25 | Loss: 0.00112388
Iteration 3/25 | Loss: 0.00103669
Iteration 4/25 | Loss: 0.00102233
Iteration 5/25 | Loss: 0.00101707
Iteration 6/25 | Loss: 0.00101634
Iteration 7/25 | Loss: 0.00101634
Iteration 8/25 | Loss: 0.00101634
Iteration 9/25 | Loss: 0.00101634
Iteration 10/25 | Loss: 0.00101634
Iteration 11/25 | Loss: 0.00101634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010163398692384362, 0.0010163398692384362, 0.0010163398692384362, 0.0010163398692384362, 0.0010163398692384362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010163398692384362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46716619
Iteration 2/25 | Loss: 0.00071714
Iteration 3/25 | Loss: 0.00071714
Iteration 4/25 | Loss: 0.00071714
Iteration 5/25 | Loss: 0.00071714
Iteration 6/25 | Loss: 0.00071714
Iteration 7/25 | Loss: 0.00071714
Iteration 8/25 | Loss: 0.00071714
Iteration 9/25 | Loss: 0.00071714
Iteration 10/25 | Loss: 0.00071714
Iteration 11/25 | Loss: 0.00071714
Iteration 12/25 | Loss: 0.00071714
Iteration 13/25 | Loss: 0.00071714
Iteration 14/25 | Loss: 0.00071714
Iteration 15/25 | Loss: 0.00071714
Iteration 16/25 | Loss: 0.00071714
Iteration 17/25 | Loss: 0.00071714
Iteration 18/25 | Loss: 0.00071714
Iteration 19/25 | Loss: 0.00071714
Iteration 20/25 | Loss: 0.00071714
Iteration 21/25 | Loss: 0.00071714
Iteration 22/25 | Loss: 0.00071714
Iteration 23/25 | Loss: 0.00071714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007171400357037783, 0.0007171400357037783, 0.0007171400357037783, 0.0007171400357037783, 0.0007171400357037783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007171400357037783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071714
Iteration 2/1000 | Loss: 0.00001435
Iteration 3/1000 | Loss: 0.00000964
Iteration 4/1000 | Loss: 0.00000884
Iteration 5/1000 | Loss: 0.00000836
Iteration 6/1000 | Loss: 0.00000802
Iteration 7/1000 | Loss: 0.00000799
Iteration 8/1000 | Loss: 0.00000787
Iteration 9/1000 | Loss: 0.00000766
Iteration 10/1000 | Loss: 0.00000765
Iteration 11/1000 | Loss: 0.00000764
Iteration 12/1000 | Loss: 0.00000755
Iteration 13/1000 | Loss: 0.00000754
Iteration 14/1000 | Loss: 0.00000747
Iteration 15/1000 | Loss: 0.00000746
Iteration 16/1000 | Loss: 0.00000746
Iteration 17/1000 | Loss: 0.00000746
Iteration 18/1000 | Loss: 0.00000746
Iteration 19/1000 | Loss: 0.00000746
Iteration 20/1000 | Loss: 0.00000746
Iteration 21/1000 | Loss: 0.00000746
Iteration 22/1000 | Loss: 0.00000746
Iteration 23/1000 | Loss: 0.00000746
Iteration 24/1000 | Loss: 0.00000746
Iteration 25/1000 | Loss: 0.00000746
Iteration 26/1000 | Loss: 0.00000746
Iteration 27/1000 | Loss: 0.00000746
Iteration 28/1000 | Loss: 0.00000745
Iteration 29/1000 | Loss: 0.00000745
Iteration 30/1000 | Loss: 0.00000745
Iteration 31/1000 | Loss: 0.00000745
Iteration 32/1000 | Loss: 0.00000745
Iteration 33/1000 | Loss: 0.00000745
Iteration 34/1000 | Loss: 0.00000745
Iteration 35/1000 | Loss: 0.00000744
Iteration 36/1000 | Loss: 0.00000744
Iteration 37/1000 | Loss: 0.00000740
Iteration 38/1000 | Loss: 0.00000740
Iteration 39/1000 | Loss: 0.00000739
Iteration 40/1000 | Loss: 0.00000739
Iteration 41/1000 | Loss: 0.00000739
Iteration 42/1000 | Loss: 0.00000739
Iteration 43/1000 | Loss: 0.00000739
Iteration 44/1000 | Loss: 0.00000739
Iteration 45/1000 | Loss: 0.00000738
Iteration 46/1000 | Loss: 0.00000736
Iteration 47/1000 | Loss: 0.00000736
Iteration 48/1000 | Loss: 0.00000736
Iteration 49/1000 | Loss: 0.00000735
Iteration 50/1000 | Loss: 0.00000735
Iteration 51/1000 | Loss: 0.00000735
Iteration 52/1000 | Loss: 0.00000734
Iteration 53/1000 | Loss: 0.00000734
Iteration 54/1000 | Loss: 0.00000732
Iteration 55/1000 | Loss: 0.00000730
Iteration 56/1000 | Loss: 0.00000730
Iteration 57/1000 | Loss: 0.00000729
Iteration 58/1000 | Loss: 0.00000729
Iteration 59/1000 | Loss: 0.00000729
Iteration 60/1000 | Loss: 0.00000729
Iteration 61/1000 | Loss: 0.00000729
Iteration 62/1000 | Loss: 0.00000729
Iteration 63/1000 | Loss: 0.00000728
Iteration 64/1000 | Loss: 0.00000728
Iteration 65/1000 | Loss: 0.00000728
Iteration 66/1000 | Loss: 0.00000728
Iteration 67/1000 | Loss: 0.00000727
Iteration 68/1000 | Loss: 0.00000727
Iteration 69/1000 | Loss: 0.00000726
Iteration 70/1000 | Loss: 0.00000726
Iteration 71/1000 | Loss: 0.00000725
Iteration 72/1000 | Loss: 0.00000725
Iteration 73/1000 | Loss: 0.00000725
Iteration 74/1000 | Loss: 0.00000724
Iteration 75/1000 | Loss: 0.00000724
Iteration 76/1000 | Loss: 0.00000724
Iteration 77/1000 | Loss: 0.00000724
Iteration 78/1000 | Loss: 0.00000723
Iteration 79/1000 | Loss: 0.00000723
Iteration 80/1000 | Loss: 0.00000722
Iteration 81/1000 | Loss: 0.00000722
Iteration 82/1000 | Loss: 0.00000721
Iteration 83/1000 | Loss: 0.00000721
Iteration 84/1000 | Loss: 0.00000721
Iteration 85/1000 | Loss: 0.00000720
Iteration 86/1000 | Loss: 0.00000720
Iteration 87/1000 | Loss: 0.00000719
Iteration 88/1000 | Loss: 0.00000718
Iteration 89/1000 | Loss: 0.00000718
Iteration 90/1000 | Loss: 0.00000718
Iteration 91/1000 | Loss: 0.00000718
Iteration 92/1000 | Loss: 0.00000718
Iteration 93/1000 | Loss: 0.00000718
Iteration 94/1000 | Loss: 0.00000718
Iteration 95/1000 | Loss: 0.00000718
Iteration 96/1000 | Loss: 0.00000717
Iteration 97/1000 | Loss: 0.00000717
Iteration 98/1000 | Loss: 0.00000717
Iteration 99/1000 | Loss: 0.00000717
Iteration 100/1000 | Loss: 0.00000717
Iteration 101/1000 | Loss: 0.00000717
Iteration 102/1000 | Loss: 0.00000717
Iteration 103/1000 | Loss: 0.00000717
Iteration 104/1000 | Loss: 0.00000716
Iteration 105/1000 | Loss: 0.00000716
Iteration 106/1000 | Loss: 0.00000716
Iteration 107/1000 | Loss: 0.00000716
Iteration 108/1000 | Loss: 0.00000716
Iteration 109/1000 | Loss: 0.00000716
Iteration 110/1000 | Loss: 0.00000715
Iteration 111/1000 | Loss: 0.00000715
Iteration 112/1000 | Loss: 0.00000715
Iteration 113/1000 | Loss: 0.00000714
Iteration 114/1000 | Loss: 0.00000714
Iteration 115/1000 | Loss: 0.00000714
Iteration 116/1000 | Loss: 0.00000713
Iteration 117/1000 | Loss: 0.00000713
Iteration 118/1000 | Loss: 0.00000713
Iteration 119/1000 | Loss: 0.00000713
Iteration 120/1000 | Loss: 0.00000713
Iteration 121/1000 | Loss: 0.00000712
Iteration 122/1000 | Loss: 0.00000712
Iteration 123/1000 | Loss: 0.00000712
Iteration 124/1000 | Loss: 0.00000712
Iteration 125/1000 | Loss: 0.00000712
Iteration 126/1000 | Loss: 0.00000711
Iteration 127/1000 | Loss: 0.00000711
Iteration 128/1000 | Loss: 0.00000710
Iteration 129/1000 | Loss: 0.00000710
Iteration 130/1000 | Loss: 0.00000709
Iteration 131/1000 | Loss: 0.00000709
Iteration 132/1000 | Loss: 0.00000709
Iteration 133/1000 | Loss: 0.00000709
Iteration 134/1000 | Loss: 0.00000708
Iteration 135/1000 | Loss: 0.00000708
Iteration 136/1000 | Loss: 0.00000708
Iteration 137/1000 | Loss: 0.00000707
Iteration 138/1000 | Loss: 0.00000707
Iteration 139/1000 | Loss: 0.00000706
Iteration 140/1000 | Loss: 0.00000705
Iteration 141/1000 | Loss: 0.00000705
Iteration 142/1000 | Loss: 0.00000705
Iteration 143/1000 | Loss: 0.00000705
Iteration 144/1000 | Loss: 0.00000704
Iteration 145/1000 | Loss: 0.00000704
Iteration 146/1000 | Loss: 0.00000704
Iteration 147/1000 | Loss: 0.00000704
Iteration 148/1000 | Loss: 0.00000704
Iteration 149/1000 | Loss: 0.00000704
Iteration 150/1000 | Loss: 0.00000704
Iteration 151/1000 | Loss: 0.00000704
Iteration 152/1000 | Loss: 0.00000703
Iteration 153/1000 | Loss: 0.00000703
Iteration 154/1000 | Loss: 0.00000703
Iteration 155/1000 | Loss: 0.00000703
Iteration 156/1000 | Loss: 0.00000703
Iteration 157/1000 | Loss: 0.00000703
Iteration 158/1000 | Loss: 0.00000702
Iteration 159/1000 | Loss: 0.00000702
Iteration 160/1000 | Loss: 0.00000702
Iteration 161/1000 | Loss: 0.00000702
Iteration 162/1000 | Loss: 0.00000702
Iteration 163/1000 | Loss: 0.00000702
Iteration 164/1000 | Loss: 0.00000702
Iteration 165/1000 | Loss: 0.00000702
Iteration 166/1000 | Loss: 0.00000702
Iteration 167/1000 | Loss: 0.00000701
Iteration 168/1000 | Loss: 0.00000701
Iteration 169/1000 | Loss: 0.00000701
Iteration 170/1000 | Loss: 0.00000701
Iteration 171/1000 | Loss: 0.00000701
Iteration 172/1000 | Loss: 0.00000700
Iteration 173/1000 | Loss: 0.00000700
Iteration 174/1000 | Loss: 0.00000700
Iteration 175/1000 | Loss: 0.00000700
Iteration 176/1000 | Loss: 0.00000700
Iteration 177/1000 | Loss: 0.00000700
Iteration 178/1000 | Loss: 0.00000700
Iteration 179/1000 | Loss: 0.00000699
Iteration 180/1000 | Loss: 0.00000699
Iteration 181/1000 | Loss: 0.00000699
Iteration 182/1000 | Loss: 0.00000699
Iteration 183/1000 | Loss: 0.00000699
Iteration 184/1000 | Loss: 0.00000698
Iteration 185/1000 | Loss: 0.00000698
Iteration 186/1000 | Loss: 0.00000698
Iteration 187/1000 | Loss: 0.00000698
Iteration 188/1000 | Loss: 0.00000698
Iteration 189/1000 | Loss: 0.00000698
Iteration 190/1000 | Loss: 0.00000698
Iteration 191/1000 | Loss: 0.00000698
Iteration 192/1000 | Loss: 0.00000698
Iteration 193/1000 | Loss: 0.00000698
Iteration 194/1000 | Loss: 0.00000698
Iteration 195/1000 | Loss: 0.00000698
Iteration 196/1000 | Loss: 0.00000698
Iteration 197/1000 | Loss: 0.00000697
Iteration 198/1000 | Loss: 0.00000697
Iteration 199/1000 | Loss: 0.00000697
Iteration 200/1000 | Loss: 0.00000697
Iteration 201/1000 | Loss: 0.00000697
Iteration 202/1000 | Loss: 0.00000697
Iteration 203/1000 | Loss: 0.00000697
Iteration 204/1000 | Loss: 0.00000697
Iteration 205/1000 | Loss: 0.00000697
Iteration 206/1000 | Loss: 0.00000697
Iteration 207/1000 | Loss: 0.00000697
Iteration 208/1000 | Loss: 0.00000697
Iteration 209/1000 | Loss: 0.00000696
Iteration 210/1000 | Loss: 0.00000696
Iteration 211/1000 | Loss: 0.00000696
Iteration 212/1000 | Loss: 0.00000696
Iteration 213/1000 | Loss: 0.00000696
Iteration 214/1000 | Loss: 0.00000696
Iteration 215/1000 | Loss: 0.00000696
Iteration 216/1000 | Loss: 0.00000696
Iteration 217/1000 | Loss: 0.00000695
Iteration 218/1000 | Loss: 0.00000695
Iteration 219/1000 | Loss: 0.00000695
Iteration 220/1000 | Loss: 0.00000695
Iteration 221/1000 | Loss: 0.00000695
Iteration 222/1000 | Loss: 0.00000695
Iteration 223/1000 | Loss: 0.00000695
Iteration 224/1000 | Loss: 0.00000695
Iteration 225/1000 | Loss: 0.00000695
Iteration 226/1000 | Loss: 0.00000695
Iteration 227/1000 | Loss: 0.00000695
Iteration 228/1000 | Loss: 0.00000695
Iteration 229/1000 | Loss: 0.00000695
Iteration 230/1000 | Loss: 0.00000695
Iteration 231/1000 | Loss: 0.00000694
Iteration 232/1000 | Loss: 0.00000694
Iteration 233/1000 | Loss: 0.00000694
Iteration 234/1000 | Loss: 0.00000694
Iteration 235/1000 | Loss: 0.00000694
Iteration 236/1000 | Loss: 0.00000694
Iteration 237/1000 | Loss: 0.00000694
Iteration 238/1000 | Loss: 0.00000694
Iteration 239/1000 | Loss: 0.00000694
Iteration 240/1000 | Loss: 0.00000694
Iteration 241/1000 | Loss: 0.00000694
Iteration 242/1000 | Loss: 0.00000694
Iteration 243/1000 | Loss: 0.00000694
Iteration 244/1000 | Loss: 0.00000694
Iteration 245/1000 | Loss: 0.00000694
Iteration 246/1000 | Loss: 0.00000694
Iteration 247/1000 | Loss: 0.00000694
Iteration 248/1000 | Loss: 0.00000694
Iteration 249/1000 | Loss: 0.00000694
Iteration 250/1000 | Loss: 0.00000693
Iteration 251/1000 | Loss: 0.00000693
Iteration 252/1000 | Loss: 0.00000693
Iteration 253/1000 | Loss: 0.00000693
Iteration 254/1000 | Loss: 0.00000693
Iteration 255/1000 | Loss: 0.00000693
Iteration 256/1000 | Loss: 0.00000693
Iteration 257/1000 | Loss: 0.00000693
Iteration 258/1000 | Loss: 0.00000693
Iteration 259/1000 | Loss: 0.00000693
Iteration 260/1000 | Loss: 0.00000693
Iteration 261/1000 | Loss: 0.00000692
Iteration 262/1000 | Loss: 0.00000692
Iteration 263/1000 | Loss: 0.00000692
Iteration 264/1000 | Loss: 0.00000692
Iteration 265/1000 | Loss: 0.00000692
Iteration 266/1000 | Loss: 0.00000692
Iteration 267/1000 | Loss: 0.00000692
Iteration 268/1000 | Loss: 0.00000692
Iteration 269/1000 | Loss: 0.00000692
Iteration 270/1000 | Loss: 0.00000692
Iteration 271/1000 | Loss: 0.00000692
Iteration 272/1000 | Loss: 0.00000692
Iteration 273/1000 | Loss: 0.00000692
Iteration 274/1000 | Loss: 0.00000692
Iteration 275/1000 | Loss: 0.00000692
Iteration 276/1000 | Loss: 0.00000692
Iteration 277/1000 | Loss: 0.00000692
Iteration 278/1000 | Loss: 0.00000692
Iteration 279/1000 | Loss: 0.00000692
Iteration 280/1000 | Loss: 0.00000692
Iteration 281/1000 | Loss: 0.00000692
Iteration 282/1000 | Loss: 0.00000692
Iteration 283/1000 | Loss: 0.00000692
Iteration 284/1000 | Loss: 0.00000691
Iteration 285/1000 | Loss: 0.00000691
Iteration 286/1000 | Loss: 0.00000691
Iteration 287/1000 | Loss: 0.00000691
Iteration 288/1000 | Loss: 0.00000691
Iteration 289/1000 | Loss: 0.00000691
Iteration 290/1000 | Loss: 0.00000691
Iteration 291/1000 | Loss: 0.00000691
Iteration 292/1000 | Loss: 0.00000691
Iteration 293/1000 | Loss: 0.00000691
Iteration 294/1000 | Loss: 0.00000691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [6.911139735166216e-06, 6.911139735166216e-06, 6.911139735166216e-06, 6.911139735166216e-06, 6.911139735166216e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.911139735166216e-06

Optimization complete. Final v2v error: 2.2871286869049072 mm

Highest mean error: 2.348411798477173 mm for frame 17

Lowest mean error: 2.251603603363037 mm for frame 59

Saving results

Total time: 40.59895324707031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950607
Iteration 2/25 | Loss: 0.00220857
Iteration 3/25 | Loss: 0.00163219
Iteration 4/25 | Loss: 0.00156998
Iteration 5/25 | Loss: 0.00137915
Iteration 6/25 | Loss: 0.00135106
Iteration 7/25 | Loss: 0.00133250
Iteration 8/25 | Loss: 0.00134972
Iteration 9/25 | Loss: 0.00131100
Iteration 10/25 | Loss: 0.00125980
Iteration 11/25 | Loss: 0.00124625
Iteration 12/25 | Loss: 0.00123593
Iteration 13/25 | Loss: 0.00121622
Iteration 14/25 | Loss: 0.00122059
Iteration 15/25 | Loss: 0.00122254
Iteration 16/25 | Loss: 0.00120433
Iteration 17/25 | Loss: 0.00120003
Iteration 18/25 | Loss: 0.00118151
Iteration 19/25 | Loss: 0.00117620
Iteration 20/25 | Loss: 0.00116579
Iteration 21/25 | Loss: 0.00115905
Iteration 22/25 | Loss: 0.00115847
Iteration 23/25 | Loss: 0.00115702
Iteration 24/25 | Loss: 0.00115661
Iteration 25/25 | Loss: 0.00115541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36529768
Iteration 2/25 | Loss: 0.00086370
Iteration 3/25 | Loss: 0.00086370
Iteration 4/25 | Loss: 0.00086370
Iteration 5/25 | Loss: 0.00086370
Iteration 6/25 | Loss: 0.00086370
Iteration 7/25 | Loss: 0.00086370
Iteration 8/25 | Loss: 0.00086370
Iteration 9/25 | Loss: 0.00086370
Iteration 10/25 | Loss: 0.00086370
Iteration 11/25 | Loss: 0.00086370
Iteration 12/25 | Loss: 0.00086370
Iteration 13/25 | Loss: 0.00086370
Iteration 14/25 | Loss: 0.00086370
Iteration 15/25 | Loss: 0.00086370
Iteration 16/25 | Loss: 0.00086370
Iteration 17/25 | Loss: 0.00086370
Iteration 18/25 | Loss: 0.00086370
Iteration 19/25 | Loss: 0.00086370
Iteration 20/25 | Loss: 0.00086370
Iteration 21/25 | Loss: 0.00086370
Iteration 22/25 | Loss: 0.00086370
Iteration 23/25 | Loss: 0.00086370
Iteration 24/25 | Loss: 0.00086370
Iteration 25/25 | Loss: 0.00086370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086370
Iteration 2/1000 | Loss: 0.00009735
Iteration 3/1000 | Loss: 0.00007261
Iteration 4/1000 | Loss: 0.00005250
Iteration 5/1000 | Loss: 0.00005871
Iteration 6/1000 | Loss: 0.00006501
Iteration 7/1000 | Loss: 0.00004136
Iteration 8/1000 | Loss: 0.00005437
Iteration 9/1000 | Loss: 0.00004468
Iteration 10/1000 | Loss: 0.00004647
Iteration 11/1000 | Loss: 0.00004791
Iteration 12/1000 | Loss: 0.00005779
Iteration 13/1000 | Loss: 0.00005780
Iteration 14/1000 | Loss: 0.00005553
Iteration 15/1000 | Loss: 0.00031202
Iteration 16/1000 | Loss: 0.00005708
Iteration 17/1000 | Loss: 0.00005536
Iteration 18/1000 | Loss: 0.00005572
Iteration 19/1000 | Loss: 0.00003769
Iteration 20/1000 | Loss: 0.00004812
Iteration 21/1000 | Loss: 0.00005059
Iteration 22/1000 | Loss: 0.00019171
Iteration 23/1000 | Loss: 0.00004999
Iteration 24/1000 | Loss: 0.00004653
Iteration 25/1000 | Loss: 0.00004985
Iteration 26/1000 | Loss: 0.00005139
Iteration 27/1000 | Loss: 0.00004885
Iteration 28/1000 | Loss: 0.00004956
Iteration 29/1000 | Loss: 0.00004771
Iteration 30/1000 | Loss: 0.00004940
Iteration 31/1000 | Loss: 0.00004027
Iteration 32/1000 | Loss: 0.00003684
Iteration 33/1000 | Loss: 0.00004531
Iteration 34/1000 | Loss: 0.00005993
Iteration 35/1000 | Loss: 0.00005089
Iteration 36/1000 | Loss: 0.00004113
Iteration 37/1000 | Loss: 0.00004729
Iteration 38/1000 | Loss: 0.00005071
Iteration 39/1000 | Loss: 0.00006688
Iteration 40/1000 | Loss: 0.00003854
Iteration 41/1000 | Loss: 0.00003405
Iteration 42/1000 | Loss: 0.00003122
Iteration 43/1000 | Loss: 0.00002968
Iteration 44/1000 | Loss: 0.00002878
Iteration 45/1000 | Loss: 0.00002839
Iteration 46/1000 | Loss: 0.00002805
Iteration 47/1000 | Loss: 0.00002750
Iteration 48/1000 | Loss: 0.00002706
Iteration 49/1000 | Loss: 0.00002680
Iteration 50/1000 | Loss: 0.00002662
Iteration 51/1000 | Loss: 0.00002638
Iteration 52/1000 | Loss: 0.00002613
Iteration 53/1000 | Loss: 0.00002607
Iteration 54/1000 | Loss: 0.00002603
Iteration 55/1000 | Loss: 0.00002601
Iteration 56/1000 | Loss: 0.00002592
Iteration 57/1000 | Loss: 0.00002588
Iteration 58/1000 | Loss: 0.00002587
Iteration 59/1000 | Loss: 0.00002587
Iteration 60/1000 | Loss: 0.00002587
Iteration 61/1000 | Loss: 0.00002586
Iteration 62/1000 | Loss: 0.00002586
Iteration 63/1000 | Loss: 0.00002584
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002583
Iteration 66/1000 | Loss: 0.00003532
Iteration 67/1000 | Loss: 0.00002574
Iteration 68/1000 | Loss: 0.00002571
Iteration 69/1000 | Loss: 0.00068867
Iteration 70/1000 | Loss: 0.00110250
Iteration 71/1000 | Loss: 0.00085260
Iteration 72/1000 | Loss: 0.00059093
Iteration 73/1000 | Loss: 0.00004186
Iteration 74/1000 | Loss: 0.00003298
Iteration 75/1000 | Loss: 0.00002892
Iteration 76/1000 | Loss: 0.00002643
Iteration 77/1000 | Loss: 0.00003529
Iteration 78/1000 | Loss: 0.00002328
Iteration 79/1000 | Loss: 0.00004108
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00003540
Iteration 82/1000 | Loss: 0.00003220
Iteration 83/1000 | Loss: 0.00002167
Iteration 84/1000 | Loss: 0.00002137
Iteration 85/1000 | Loss: 0.00002119
Iteration 86/1000 | Loss: 0.00002215
Iteration 87/1000 | Loss: 0.00003366
Iteration 88/1000 | Loss: 0.00002081
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002052
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002040
Iteration 99/1000 | Loss: 0.00002039
Iteration 100/1000 | Loss: 0.00002039
Iteration 101/1000 | Loss: 0.00002039
Iteration 102/1000 | Loss: 0.00002948
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002024
Iteration 130/1000 | Loss: 0.00002024
Iteration 131/1000 | Loss: 0.00002024
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002024
Iteration 136/1000 | Loss: 0.00002024
Iteration 137/1000 | Loss: 0.00002024
Iteration 138/1000 | Loss: 0.00002024
Iteration 139/1000 | Loss: 0.00002024
Iteration 140/1000 | Loss: 0.00002024
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00002024
Iteration 143/1000 | Loss: 0.00002024
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002024
Iteration 149/1000 | Loss: 0.00002024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.0240961021045223e-05, 2.0240961021045223e-05, 2.0240961021045223e-05, 2.0240961021045223e-05, 2.0240961021045223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0240961021045223e-05

Optimization complete. Final v2v error: 3.591061592102051 mm

Highest mean error: 5.323265075683594 mm for frame 46

Lowest mean error: 2.3981735706329346 mm for frame 5

Saving results

Total time: 152.86396884918213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618834
Iteration 2/25 | Loss: 0.00131474
Iteration 3/25 | Loss: 0.00116203
Iteration 4/25 | Loss: 0.00114580
Iteration 5/25 | Loss: 0.00114169
Iteration 6/25 | Loss: 0.00114115
Iteration 7/25 | Loss: 0.00114115
Iteration 8/25 | Loss: 0.00114115
Iteration 9/25 | Loss: 0.00114115
Iteration 10/25 | Loss: 0.00114115
Iteration 11/25 | Loss: 0.00114115
Iteration 12/25 | Loss: 0.00114115
Iteration 13/25 | Loss: 0.00114115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011411500163376331, 0.0011411500163376331, 0.0011411500163376331, 0.0011411500163376331, 0.0011411500163376331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011411500163376331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17675853
Iteration 2/25 | Loss: 0.00053571
Iteration 3/25 | Loss: 0.00053571
Iteration 4/25 | Loss: 0.00053571
Iteration 5/25 | Loss: 0.00053571
Iteration 6/25 | Loss: 0.00053571
Iteration 7/25 | Loss: 0.00053571
Iteration 8/25 | Loss: 0.00053571
Iteration 9/25 | Loss: 0.00053571
Iteration 10/25 | Loss: 0.00053571
Iteration 11/25 | Loss: 0.00053571
Iteration 12/25 | Loss: 0.00053571
Iteration 13/25 | Loss: 0.00053571
Iteration 14/25 | Loss: 0.00053571
Iteration 15/25 | Loss: 0.00053571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005357097834348679, 0.0005357097834348679, 0.0005357097834348679, 0.0005357097834348679, 0.0005357097834348679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005357097834348679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053571
Iteration 2/1000 | Loss: 0.00002910
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001990
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001880
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001779
Iteration 13/1000 | Loss: 0.00001763
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001743
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001737
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001735
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001728
Iteration 51/1000 | Loss: 0.00001728
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001725
Iteration 64/1000 | Loss: 0.00001725
Iteration 65/1000 | Loss: 0.00001725
Iteration 66/1000 | Loss: 0.00001724
Iteration 67/1000 | Loss: 0.00001724
Iteration 68/1000 | Loss: 0.00001724
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001720
Iteration 80/1000 | Loss: 0.00001720
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001716
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001706
Iteration 125/1000 | Loss: 0.00001706
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001706
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7054697309504263e-05, 1.7054697309504263e-05, 1.7054697309504263e-05, 1.7054697309504263e-05, 1.7054697309504263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7054697309504263e-05

Optimization complete. Final v2v error: 3.4568355083465576 mm

Highest mean error: 4.086833953857422 mm for frame 185

Lowest mean error: 3.026242733001709 mm for frame 107

Saving results

Total time: 41.35199284553528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761815
Iteration 2/25 | Loss: 0.00150139
Iteration 3/25 | Loss: 0.00118438
Iteration 4/25 | Loss: 0.00114645
Iteration 5/25 | Loss: 0.00113953
Iteration 6/25 | Loss: 0.00113795
Iteration 7/25 | Loss: 0.00113795
Iteration 8/25 | Loss: 0.00113795
Iteration 9/25 | Loss: 0.00113795
Iteration 10/25 | Loss: 0.00113795
Iteration 11/25 | Loss: 0.00113795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011379519710317254, 0.0011379519710317254, 0.0011379519710317254, 0.0011379519710317254, 0.0011379519710317254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011379519710317254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09085464
Iteration 2/25 | Loss: 0.00069256
Iteration 3/25 | Loss: 0.00069255
Iteration 4/25 | Loss: 0.00069255
Iteration 5/25 | Loss: 0.00069255
Iteration 6/25 | Loss: 0.00069255
Iteration 7/25 | Loss: 0.00069255
Iteration 8/25 | Loss: 0.00069255
Iteration 9/25 | Loss: 0.00069255
Iteration 10/25 | Loss: 0.00069255
Iteration 11/25 | Loss: 0.00069255
Iteration 12/25 | Loss: 0.00069255
Iteration 13/25 | Loss: 0.00069255
Iteration 14/25 | Loss: 0.00069255
Iteration 15/25 | Loss: 0.00069255
Iteration 16/25 | Loss: 0.00069255
Iteration 17/25 | Loss: 0.00069255
Iteration 18/25 | Loss: 0.00069255
Iteration 19/25 | Loss: 0.00069255
Iteration 20/25 | Loss: 0.00069255
Iteration 21/25 | Loss: 0.00069255
Iteration 22/25 | Loss: 0.00069255
Iteration 23/25 | Loss: 0.00069255
Iteration 24/25 | Loss: 0.00069255
Iteration 25/25 | Loss: 0.00069255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069255
Iteration 2/1000 | Loss: 0.00003570
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001891
Iteration 6/1000 | Loss: 0.00001812
Iteration 7/1000 | Loss: 0.00001748
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001627
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001601
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001594
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001592
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001587
Iteration 53/1000 | Loss: 0.00001587
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001586
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001581
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001580
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001579
Iteration 86/1000 | Loss: 0.00001579
Iteration 87/1000 | Loss: 0.00001579
Iteration 88/1000 | Loss: 0.00001579
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001578
Iteration 101/1000 | Loss: 0.00001578
Iteration 102/1000 | Loss: 0.00001578
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Iteration 107/1000 | Loss: 0.00001577
Iteration 108/1000 | Loss: 0.00001577
Iteration 109/1000 | Loss: 0.00001577
Iteration 110/1000 | Loss: 0.00001577
Iteration 111/1000 | Loss: 0.00001577
Iteration 112/1000 | Loss: 0.00001577
Iteration 113/1000 | Loss: 0.00001576
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001576
Iteration 116/1000 | Loss: 0.00001576
Iteration 117/1000 | Loss: 0.00001576
Iteration 118/1000 | Loss: 0.00001576
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001576
Iteration 122/1000 | Loss: 0.00001576
Iteration 123/1000 | Loss: 0.00001576
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001576
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001576
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001576
Iteration 141/1000 | Loss: 0.00001576
Iteration 142/1000 | Loss: 0.00001576
Iteration 143/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.5756715583847836e-05, 1.5756715583847836e-05, 1.5756715583847836e-05, 1.5756715583847836e-05, 1.5756715583847836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5756715583847836e-05

Optimization complete. Final v2v error: 3.2506532669067383 mm

Highest mean error: 4.678382873535156 mm for frame 161

Lowest mean error: 2.589951753616333 mm for frame 189

Saving results

Total time: 39.15308618545532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461886
Iteration 2/25 | Loss: 0.00133559
Iteration 3/25 | Loss: 0.00111165
Iteration 4/25 | Loss: 0.00108051
Iteration 5/25 | Loss: 0.00107657
Iteration 6/25 | Loss: 0.00107530
Iteration 7/25 | Loss: 0.00107530
Iteration 8/25 | Loss: 0.00107530
Iteration 9/25 | Loss: 0.00107530
Iteration 10/25 | Loss: 0.00107530
Iteration 11/25 | Loss: 0.00107530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001075302017852664, 0.001075302017852664, 0.001075302017852664, 0.001075302017852664, 0.001075302017852664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001075302017852664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38954103
Iteration 2/25 | Loss: 0.00065065
Iteration 3/25 | Loss: 0.00065065
Iteration 4/25 | Loss: 0.00065065
Iteration 5/25 | Loss: 0.00065065
Iteration 6/25 | Loss: 0.00065064
Iteration 7/25 | Loss: 0.00065064
Iteration 8/25 | Loss: 0.00065064
Iteration 9/25 | Loss: 0.00065064
Iteration 10/25 | Loss: 0.00065064
Iteration 11/25 | Loss: 0.00065064
Iteration 12/25 | Loss: 0.00065064
Iteration 13/25 | Loss: 0.00065064
Iteration 14/25 | Loss: 0.00065064
Iteration 15/25 | Loss: 0.00065064
Iteration 16/25 | Loss: 0.00065064
Iteration 17/25 | Loss: 0.00065064
Iteration 18/25 | Loss: 0.00065064
Iteration 19/25 | Loss: 0.00065064
Iteration 20/25 | Loss: 0.00065064
Iteration 21/25 | Loss: 0.00065064
Iteration 22/25 | Loss: 0.00065064
Iteration 23/25 | Loss: 0.00065064
Iteration 24/25 | Loss: 0.00065064
Iteration 25/25 | Loss: 0.00065064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006506431382149458, 0.0006506431382149458, 0.0006506431382149458, 0.0006506431382149458, 0.0006506431382149458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006506431382149458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065064
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001330
Iteration 5/1000 | Loss: 0.00001234
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001137
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001062
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001051
Iteration 25/1000 | Loss: 0.00001048
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001043
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00001039
Iteration 40/1000 | Loss: 0.00001039
Iteration 41/1000 | Loss: 0.00001039
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001036
Iteration 53/1000 | Loss: 0.00001036
Iteration 54/1000 | Loss: 0.00001035
Iteration 55/1000 | Loss: 0.00001035
Iteration 56/1000 | Loss: 0.00001035
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001034
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001033
Iteration 67/1000 | Loss: 0.00001033
Iteration 68/1000 | Loss: 0.00001033
Iteration 69/1000 | Loss: 0.00001033
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001032
Iteration 73/1000 | Loss: 0.00001032
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001027
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001025
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001024
Iteration 113/1000 | Loss: 0.00001024
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001023
Iteration 123/1000 | Loss: 0.00001023
Iteration 124/1000 | Loss: 0.00001023
Iteration 125/1000 | Loss: 0.00001023
Iteration 126/1000 | Loss: 0.00001023
Iteration 127/1000 | Loss: 0.00001023
Iteration 128/1000 | Loss: 0.00001023
Iteration 129/1000 | Loss: 0.00001022
Iteration 130/1000 | Loss: 0.00001022
Iteration 131/1000 | Loss: 0.00001022
Iteration 132/1000 | Loss: 0.00001022
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001022
Iteration 135/1000 | Loss: 0.00001022
Iteration 136/1000 | Loss: 0.00001022
Iteration 137/1000 | Loss: 0.00001022
Iteration 138/1000 | Loss: 0.00001022
Iteration 139/1000 | Loss: 0.00001022
Iteration 140/1000 | Loss: 0.00001022
Iteration 141/1000 | Loss: 0.00001022
Iteration 142/1000 | Loss: 0.00001022
Iteration 143/1000 | Loss: 0.00001022
Iteration 144/1000 | Loss: 0.00001022
Iteration 145/1000 | Loss: 0.00001022
Iteration 146/1000 | Loss: 0.00001022
Iteration 147/1000 | Loss: 0.00001022
Iteration 148/1000 | Loss: 0.00001022
Iteration 149/1000 | Loss: 0.00001022
Iteration 150/1000 | Loss: 0.00001022
Iteration 151/1000 | Loss: 0.00001022
Iteration 152/1000 | Loss: 0.00001022
Iteration 153/1000 | Loss: 0.00001022
Iteration 154/1000 | Loss: 0.00001022
Iteration 155/1000 | Loss: 0.00001022
Iteration 156/1000 | Loss: 0.00001022
Iteration 157/1000 | Loss: 0.00001022
Iteration 158/1000 | Loss: 0.00001022
Iteration 159/1000 | Loss: 0.00001022
Iteration 160/1000 | Loss: 0.00001022
Iteration 161/1000 | Loss: 0.00001022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.0215346264885738e-05, 1.0215346264885738e-05, 1.0215346264885738e-05, 1.0215346264885738e-05, 1.0215346264885738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0215346264885738e-05

Optimization complete. Final v2v error: 2.6841073036193848 mm

Highest mean error: 3.4994874000549316 mm for frame 79

Lowest mean error: 2.3712921142578125 mm for frame 159

Saving results

Total time: 33.697838306427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564627
Iteration 2/25 | Loss: 0.00130437
Iteration 3/25 | Loss: 0.00117353
Iteration 4/25 | Loss: 0.00116373
Iteration 5/25 | Loss: 0.00116163
Iteration 6/25 | Loss: 0.00116163
Iteration 7/25 | Loss: 0.00116163
Iteration 8/25 | Loss: 0.00116163
Iteration 9/25 | Loss: 0.00116163
Iteration 10/25 | Loss: 0.00116163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011616322444751859, 0.0011616322444751859, 0.0011616322444751859, 0.0011616322444751859, 0.0011616322444751859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011616322444751859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63861221
Iteration 2/25 | Loss: 0.00069814
Iteration 3/25 | Loss: 0.00069814
Iteration 4/25 | Loss: 0.00069814
Iteration 5/25 | Loss: 0.00069814
Iteration 6/25 | Loss: 0.00069814
Iteration 7/25 | Loss: 0.00069814
Iteration 8/25 | Loss: 0.00069814
Iteration 9/25 | Loss: 0.00069813
Iteration 10/25 | Loss: 0.00069813
Iteration 11/25 | Loss: 0.00069813
Iteration 12/25 | Loss: 0.00069813
Iteration 13/25 | Loss: 0.00069813
Iteration 14/25 | Loss: 0.00069813
Iteration 15/25 | Loss: 0.00069813
Iteration 16/25 | Loss: 0.00069813
Iteration 17/25 | Loss: 0.00069813
Iteration 18/25 | Loss: 0.00069813
Iteration 19/25 | Loss: 0.00069813
Iteration 20/25 | Loss: 0.00069813
Iteration 21/25 | Loss: 0.00069813
Iteration 22/25 | Loss: 0.00069813
Iteration 23/25 | Loss: 0.00069813
Iteration 24/25 | Loss: 0.00069813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006981338374316692, 0.0006981338374316692, 0.0006981338374316692, 0.0006981338374316692, 0.0006981338374316692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006981338374316692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069813
Iteration 2/1000 | Loss: 0.00003571
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002433
Iteration 5/1000 | Loss: 0.00002320
Iteration 6/1000 | Loss: 0.00002254
Iteration 7/1000 | Loss: 0.00002215
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001953
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001925
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001924
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001924
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001893
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001880
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00001879
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001879
Iteration 113/1000 | Loss: 0.00001879
Iteration 114/1000 | Loss: 0.00001879
Iteration 115/1000 | Loss: 0.00001879
Iteration 116/1000 | Loss: 0.00001879
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.8788005036185496e-05, 1.8788005036185496e-05, 1.8788005036185496e-05, 1.8788005036185496e-05, 1.8788005036185496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8788005036185496e-05

Optimization complete. Final v2v error: 3.4883248805999756 mm

Highest mean error: 3.551543951034546 mm for frame 0

Lowest mean error: 3.4259541034698486 mm for frame 205

Saving results

Total time: 44.53398132324219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496830
Iteration 2/25 | Loss: 0.00147691
Iteration 3/25 | Loss: 0.00114853
Iteration 4/25 | Loss: 0.00110743
Iteration 5/25 | Loss: 0.00110146
Iteration 6/25 | Loss: 0.00109993
Iteration 7/25 | Loss: 0.00109982
Iteration 8/25 | Loss: 0.00109982
Iteration 9/25 | Loss: 0.00109982
Iteration 10/25 | Loss: 0.00109982
Iteration 11/25 | Loss: 0.00109982
Iteration 12/25 | Loss: 0.00109982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010998235084116459, 0.0010998235084116459, 0.0010998235084116459, 0.0010998235084116459, 0.0010998235084116459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010998235084116459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44803965
Iteration 2/25 | Loss: 0.00060153
Iteration 3/25 | Loss: 0.00060152
Iteration 4/25 | Loss: 0.00060152
Iteration 5/25 | Loss: 0.00060152
Iteration 6/25 | Loss: 0.00060152
Iteration 7/25 | Loss: 0.00060152
Iteration 8/25 | Loss: 0.00060152
Iteration 9/25 | Loss: 0.00060152
Iteration 10/25 | Loss: 0.00060152
Iteration 11/25 | Loss: 0.00060152
Iteration 12/25 | Loss: 0.00060152
Iteration 13/25 | Loss: 0.00060152
Iteration 14/25 | Loss: 0.00060152
Iteration 15/25 | Loss: 0.00060152
Iteration 16/25 | Loss: 0.00060152
Iteration 17/25 | Loss: 0.00060152
Iteration 18/25 | Loss: 0.00060152
Iteration 19/25 | Loss: 0.00060152
Iteration 20/25 | Loss: 0.00060152
Iteration 21/25 | Loss: 0.00060152
Iteration 22/25 | Loss: 0.00060152
Iteration 23/25 | Loss: 0.00060152
Iteration 24/25 | Loss: 0.00060152
Iteration 25/25 | Loss: 0.00060152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060152
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002049
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001326
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001293
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001289
Iteration 102/1000 | Loss: 0.00001289
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001288
Iteration 109/1000 | Loss: 0.00001288
Iteration 110/1000 | Loss: 0.00001288
Iteration 111/1000 | Loss: 0.00001288
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001286
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001284
Iteration 127/1000 | Loss: 0.00001284
Iteration 128/1000 | Loss: 0.00001284
Iteration 129/1000 | Loss: 0.00001284
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001283
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001282
Iteration 142/1000 | Loss: 0.00001282
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001282
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001278
Iteration 167/1000 | Loss: 0.00001278
Iteration 168/1000 | Loss: 0.00001278
Iteration 169/1000 | Loss: 0.00001278
Iteration 170/1000 | Loss: 0.00001278
Iteration 171/1000 | Loss: 0.00001278
Iteration 172/1000 | Loss: 0.00001278
Iteration 173/1000 | Loss: 0.00001278
Iteration 174/1000 | Loss: 0.00001278
Iteration 175/1000 | Loss: 0.00001278
Iteration 176/1000 | Loss: 0.00001278
Iteration 177/1000 | Loss: 0.00001278
Iteration 178/1000 | Loss: 0.00001278
Iteration 179/1000 | Loss: 0.00001278
Iteration 180/1000 | Loss: 0.00001277
Iteration 181/1000 | Loss: 0.00001277
Iteration 182/1000 | Loss: 0.00001277
Iteration 183/1000 | Loss: 0.00001277
Iteration 184/1000 | Loss: 0.00001277
Iteration 185/1000 | Loss: 0.00001277
Iteration 186/1000 | Loss: 0.00001277
Iteration 187/1000 | Loss: 0.00001276
Iteration 188/1000 | Loss: 0.00001276
Iteration 189/1000 | Loss: 0.00001276
Iteration 190/1000 | Loss: 0.00001276
Iteration 191/1000 | Loss: 0.00001276
Iteration 192/1000 | Loss: 0.00001276
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001276
Iteration 195/1000 | Loss: 0.00001276
Iteration 196/1000 | Loss: 0.00001276
Iteration 197/1000 | Loss: 0.00001276
Iteration 198/1000 | Loss: 0.00001275
Iteration 199/1000 | Loss: 0.00001275
Iteration 200/1000 | Loss: 0.00001275
Iteration 201/1000 | Loss: 0.00001275
Iteration 202/1000 | Loss: 0.00001275
Iteration 203/1000 | Loss: 0.00001275
Iteration 204/1000 | Loss: 0.00001274
Iteration 205/1000 | Loss: 0.00001274
Iteration 206/1000 | Loss: 0.00001274
Iteration 207/1000 | Loss: 0.00001274
Iteration 208/1000 | Loss: 0.00001274
Iteration 209/1000 | Loss: 0.00001274
Iteration 210/1000 | Loss: 0.00001274
Iteration 211/1000 | Loss: 0.00001274
Iteration 212/1000 | Loss: 0.00001274
Iteration 213/1000 | Loss: 0.00001274
Iteration 214/1000 | Loss: 0.00001274
Iteration 215/1000 | Loss: 0.00001274
Iteration 216/1000 | Loss: 0.00001273
Iteration 217/1000 | Loss: 0.00001273
Iteration 218/1000 | Loss: 0.00001273
Iteration 219/1000 | Loss: 0.00001273
Iteration 220/1000 | Loss: 0.00001273
Iteration 221/1000 | Loss: 0.00001273
Iteration 222/1000 | Loss: 0.00001273
Iteration 223/1000 | Loss: 0.00001273
Iteration 224/1000 | Loss: 0.00001273
Iteration 225/1000 | Loss: 0.00001273
Iteration 226/1000 | Loss: 0.00001273
Iteration 227/1000 | Loss: 0.00001273
Iteration 228/1000 | Loss: 0.00001273
Iteration 229/1000 | Loss: 0.00001273
Iteration 230/1000 | Loss: 0.00001273
Iteration 231/1000 | Loss: 0.00001273
Iteration 232/1000 | Loss: 0.00001272
Iteration 233/1000 | Loss: 0.00001272
Iteration 234/1000 | Loss: 0.00001272
Iteration 235/1000 | Loss: 0.00001272
Iteration 236/1000 | Loss: 0.00001272
Iteration 237/1000 | Loss: 0.00001272
Iteration 238/1000 | Loss: 0.00001272
Iteration 239/1000 | Loss: 0.00001272
Iteration 240/1000 | Loss: 0.00001272
Iteration 241/1000 | Loss: 0.00001272
Iteration 242/1000 | Loss: 0.00001272
Iteration 243/1000 | Loss: 0.00001272
Iteration 244/1000 | Loss: 0.00001272
Iteration 245/1000 | Loss: 0.00001272
Iteration 246/1000 | Loss: 0.00001272
Iteration 247/1000 | Loss: 0.00001272
Iteration 248/1000 | Loss: 0.00001272
Iteration 249/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.2717269783024676e-05, 1.2717269783024676e-05, 1.2717269783024676e-05, 1.2717269783024676e-05, 1.2717269783024676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2717269783024676e-05

Optimization complete. Final v2v error: 2.9730420112609863 mm

Highest mean error: 3.7194936275482178 mm for frame 76

Lowest mean error: 2.582453489303589 mm for frame 121

Saving results

Total time: 42.61316537857056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762794
Iteration 2/25 | Loss: 0.00141253
Iteration 3/25 | Loss: 0.00124958
Iteration 4/25 | Loss: 0.00120674
Iteration 5/25 | Loss: 0.00119374
Iteration 6/25 | Loss: 0.00118687
Iteration 7/25 | Loss: 0.00118534
Iteration 8/25 | Loss: 0.00118481
Iteration 9/25 | Loss: 0.00118459
Iteration 10/25 | Loss: 0.00118448
Iteration 11/25 | Loss: 0.00118448
Iteration 12/25 | Loss: 0.00118448
Iteration 13/25 | Loss: 0.00118448
Iteration 14/25 | Loss: 0.00118448
Iteration 15/25 | Loss: 0.00118448
Iteration 16/25 | Loss: 0.00118448
Iteration 17/25 | Loss: 0.00118448
Iteration 18/25 | Loss: 0.00118448
Iteration 19/25 | Loss: 0.00118448
Iteration 20/25 | Loss: 0.00118448
Iteration 21/25 | Loss: 0.00118448
Iteration 22/25 | Loss: 0.00118448
Iteration 23/25 | Loss: 0.00118448
Iteration 24/25 | Loss: 0.00118448
Iteration 25/25 | Loss: 0.00118448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85780787
Iteration 2/25 | Loss: 0.00083573
Iteration 3/25 | Loss: 0.00083556
Iteration 4/25 | Loss: 0.00083556
Iteration 5/25 | Loss: 0.00083556
Iteration 6/25 | Loss: 0.00083556
Iteration 7/25 | Loss: 0.00083556
Iteration 8/25 | Loss: 0.00083556
Iteration 9/25 | Loss: 0.00083556
Iteration 10/25 | Loss: 0.00083556
Iteration 11/25 | Loss: 0.00083556
Iteration 12/25 | Loss: 0.00083556
Iteration 13/25 | Loss: 0.00083556
Iteration 14/25 | Loss: 0.00083556
Iteration 15/25 | Loss: 0.00083556
Iteration 16/25 | Loss: 0.00083556
Iteration 17/25 | Loss: 0.00083556
Iteration 18/25 | Loss: 0.00083556
Iteration 19/25 | Loss: 0.00083556
Iteration 20/25 | Loss: 0.00083556
Iteration 21/25 | Loss: 0.00083556
Iteration 22/25 | Loss: 0.00083556
Iteration 23/25 | Loss: 0.00083556
Iteration 24/25 | Loss: 0.00083556
Iteration 25/25 | Loss: 0.00083556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083556
Iteration 2/1000 | Loss: 0.00007859
Iteration 3/1000 | Loss: 0.00005360
Iteration 4/1000 | Loss: 0.00004313
Iteration 5/1000 | Loss: 0.00003899
Iteration 6/1000 | Loss: 0.00003683
Iteration 7/1000 | Loss: 0.00003480
Iteration 8/1000 | Loss: 0.00003377
Iteration 9/1000 | Loss: 0.00003273
Iteration 10/1000 | Loss: 0.00003204
Iteration 11/1000 | Loss: 0.00003148
Iteration 12/1000 | Loss: 0.00003107
Iteration 13/1000 | Loss: 0.00003075
Iteration 14/1000 | Loss: 0.00003047
Iteration 15/1000 | Loss: 0.00003027
Iteration 16/1000 | Loss: 0.00003006
Iteration 17/1000 | Loss: 0.00002988
Iteration 18/1000 | Loss: 0.00002973
Iteration 19/1000 | Loss: 0.00002962
Iteration 20/1000 | Loss: 0.00002961
Iteration 21/1000 | Loss: 0.00002958
Iteration 22/1000 | Loss: 0.00002958
Iteration 23/1000 | Loss: 0.00002956
Iteration 24/1000 | Loss: 0.00002955
Iteration 25/1000 | Loss: 0.00002953
Iteration 26/1000 | Loss: 0.00002951
Iteration 27/1000 | Loss: 0.00002950
Iteration 28/1000 | Loss: 0.00002949
Iteration 29/1000 | Loss: 0.00002949
Iteration 30/1000 | Loss: 0.00002948
Iteration 31/1000 | Loss: 0.00002948
Iteration 32/1000 | Loss: 0.00002947
Iteration 33/1000 | Loss: 0.00002945
Iteration 34/1000 | Loss: 0.00002944
Iteration 35/1000 | Loss: 0.00002941
Iteration 36/1000 | Loss: 0.00002941
Iteration 37/1000 | Loss: 0.00002940
Iteration 38/1000 | Loss: 0.00002939
Iteration 39/1000 | Loss: 0.00002939
Iteration 40/1000 | Loss: 0.00002937
Iteration 41/1000 | Loss: 0.00002937
Iteration 42/1000 | Loss: 0.00002936
Iteration 43/1000 | Loss: 0.00002936
Iteration 44/1000 | Loss: 0.00002936
Iteration 45/1000 | Loss: 0.00002935
Iteration 46/1000 | Loss: 0.00002934
Iteration 47/1000 | Loss: 0.00002934
Iteration 48/1000 | Loss: 0.00002934
Iteration 49/1000 | Loss: 0.00002933
Iteration 50/1000 | Loss: 0.00002933
Iteration 51/1000 | Loss: 0.00002933
Iteration 52/1000 | Loss: 0.00002933
Iteration 53/1000 | Loss: 0.00002933
Iteration 54/1000 | Loss: 0.00002932
Iteration 55/1000 | Loss: 0.00002932
Iteration 56/1000 | Loss: 0.00002932
Iteration 57/1000 | Loss: 0.00002932
Iteration 58/1000 | Loss: 0.00002932
Iteration 59/1000 | Loss: 0.00002932
Iteration 60/1000 | Loss: 0.00002931
Iteration 61/1000 | Loss: 0.00002931
Iteration 62/1000 | Loss: 0.00002931
Iteration 63/1000 | Loss: 0.00002931
Iteration 64/1000 | Loss: 0.00002931
Iteration 65/1000 | Loss: 0.00002931
Iteration 66/1000 | Loss: 0.00002931
Iteration 67/1000 | Loss: 0.00002931
Iteration 68/1000 | Loss: 0.00002931
Iteration 69/1000 | Loss: 0.00002931
Iteration 70/1000 | Loss: 0.00002931
Iteration 71/1000 | Loss: 0.00002930
Iteration 72/1000 | Loss: 0.00002930
Iteration 73/1000 | Loss: 0.00002930
Iteration 74/1000 | Loss: 0.00002929
Iteration 75/1000 | Loss: 0.00002929
Iteration 76/1000 | Loss: 0.00002929
Iteration 77/1000 | Loss: 0.00002929
Iteration 78/1000 | Loss: 0.00002929
Iteration 79/1000 | Loss: 0.00002929
Iteration 80/1000 | Loss: 0.00002928
Iteration 81/1000 | Loss: 0.00002928
Iteration 82/1000 | Loss: 0.00002928
Iteration 83/1000 | Loss: 0.00002928
Iteration 84/1000 | Loss: 0.00002928
Iteration 85/1000 | Loss: 0.00002927
Iteration 86/1000 | Loss: 0.00002927
Iteration 87/1000 | Loss: 0.00002927
Iteration 88/1000 | Loss: 0.00002927
Iteration 89/1000 | Loss: 0.00002926
Iteration 90/1000 | Loss: 0.00002926
Iteration 91/1000 | Loss: 0.00002926
Iteration 92/1000 | Loss: 0.00002926
Iteration 93/1000 | Loss: 0.00002926
Iteration 94/1000 | Loss: 0.00002926
Iteration 95/1000 | Loss: 0.00002926
Iteration 96/1000 | Loss: 0.00002926
Iteration 97/1000 | Loss: 0.00002926
Iteration 98/1000 | Loss: 0.00002925
Iteration 99/1000 | Loss: 0.00002925
Iteration 100/1000 | Loss: 0.00002925
Iteration 101/1000 | Loss: 0.00002925
Iteration 102/1000 | Loss: 0.00002925
Iteration 103/1000 | Loss: 0.00002924
Iteration 104/1000 | Loss: 0.00002924
Iteration 105/1000 | Loss: 0.00002924
Iteration 106/1000 | Loss: 0.00002924
Iteration 107/1000 | Loss: 0.00002924
Iteration 108/1000 | Loss: 0.00002923
Iteration 109/1000 | Loss: 0.00002923
Iteration 110/1000 | Loss: 0.00002923
Iteration 111/1000 | Loss: 0.00002923
Iteration 112/1000 | Loss: 0.00002923
Iteration 113/1000 | Loss: 0.00002923
Iteration 114/1000 | Loss: 0.00002923
Iteration 115/1000 | Loss: 0.00002922
Iteration 116/1000 | Loss: 0.00002922
Iteration 117/1000 | Loss: 0.00002922
Iteration 118/1000 | Loss: 0.00002922
Iteration 119/1000 | Loss: 0.00002922
Iteration 120/1000 | Loss: 0.00002922
Iteration 121/1000 | Loss: 0.00002922
Iteration 122/1000 | Loss: 0.00002922
Iteration 123/1000 | Loss: 0.00002921
Iteration 124/1000 | Loss: 0.00002921
Iteration 125/1000 | Loss: 0.00002921
Iteration 126/1000 | Loss: 0.00002921
Iteration 127/1000 | Loss: 0.00002920
Iteration 128/1000 | Loss: 0.00002920
Iteration 129/1000 | Loss: 0.00002920
Iteration 130/1000 | Loss: 0.00002920
Iteration 131/1000 | Loss: 0.00002920
Iteration 132/1000 | Loss: 0.00002919
Iteration 133/1000 | Loss: 0.00002919
Iteration 134/1000 | Loss: 0.00002919
Iteration 135/1000 | Loss: 0.00002919
Iteration 136/1000 | Loss: 0.00002919
Iteration 137/1000 | Loss: 0.00002919
Iteration 138/1000 | Loss: 0.00002919
Iteration 139/1000 | Loss: 0.00002919
Iteration 140/1000 | Loss: 0.00002919
Iteration 141/1000 | Loss: 0.00002919
Iteration 142/1000 | Loss: 0.00002919
Iteration 143/1000 | Loss: 0.00002918
Iteration 144/1000 | Loss: 0.00002918
Iteration 145/1000 | Loss: 0.00002918
Iteration 146/1000 | Loss: 0.00002918
Iteration 147/1000 | Loss: 0.00002918
Iteration 148/1000 | Loss: 0.00002918
Iteration 149/1000 | Loss: 0.00002918
Iteration 150/1000 | Loss: 0.00002918
Iteration 151/1000 | Loss: 0.00002918
Iteration 152/1000 | Loss: 0.00002918
Iteration 153/1000 | Loss: 0.00002918
Iteration 154/1000 | Loss: 0.00002918
Iteration 155/1000 | Loss: 0.00002917
Iteration 156/1000 | Loss: 0.00002917
Iteration 157/1000 | Loss: 0.00002917
Iteration 158/1000 | Loss: 0.00002917
Iteration 159/1000 | Loss: 0.00002917
Iteration 160/1000 | Loss: 0.00002917
Iteration 161/1000 | Loss: 0.00002917
Iteration 162/1000 | Loss: 0.00002917
Iteration 163/1000 | Loss: 0.00002917
Iteration 164/1000 | Loss: 0.00002917
Iteration 165/1000 | Loss: 0.00002917
Iteration 166/1000 | Loss: 0.00002917
Iteration 167/1000 | Loss: 0.00002917
Iteration 168/1000 | Loss: 0.00002917
Iteration 169/1000 | Loss: 0.00002917
Iteration 170/1000 | Loss: 0.00002917
Iteration 171/1000 | Loss: 0.00002917
Iteration 172/1000 | Loss: 0.00002917
Iteration 173/1000 | Loss: 0.00002917
Iteration 174/1000 | Loss: 0.00002917
Iteration 175/1000 | Loss: 0.00002917
Iteration 176/1000 | Loss: 0.00002917
Iteration 177/1000 | Loss: 0.00002917
Iteration 178/1000 | Loss: 0.00002917
Iteration 179/1000 | Loss: 0.00002917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.916504854511004e-05, 2.916504854511004e-05, 2.916504854511004e-05, 2.916504854511004e-05, 2.916504854511004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.916504854511004e-05

Optimization complete. Final v2v error: 4.38656759262085 mm

Highest mean error: 11.83643913269043 mm for frame 9

Lowest mean error: 3.143420457839966 mm for frame 231

Saving results

Total time: 61.39716696739197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973793
Iteration 2/25 | Loss: 0.00278738
Iteration 3/25 | Loss: 0.00191964
Iteration 4/25 | Loss: 0.00179728
Iteration 5/25 | Loss: 0.00160078
Iteration 6/25 | Loss: 0.00156027
Iteration 7/25 | Loss: 0.00151624
Iteration 8/25 | Loss: 0.00144823
Iteration 9/25 | Loss: 0.00135874
Iteration 10/25 | Loss: 0.00130463
Iteration 11/25 | Loss: 0.00128393
Iteration 12/25 | Loss: 0.00126926
Iteration 13/25 | Loss: 0.00126783
Iteration 14/25 | Loss: 0.00126954
Iteration 15/25 | Loss: 0.00125120
Iteration 16/25 | Loss: 0.00125474
Iteration 17/25 | Loss: 0.00125194
Iteration 18/25 | Loss: 0.00123953
Iteration 19/25 | Loss: 0.00124421
Iteration 20/25 | Loss: 0.00123926
Iteration 21/25 | Loss: 0.00124288
Iteration 22/25 | Loss: 0.00125017
Iteration 23/25 | Loss: 0.00124733
Iteration 24/25 | Loss: 0.00123268
Iteration 25/25 | Loss: 0.00123969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34430552
Iteration 2/25 | Loss: 0.00220910
Iteration 3/25 | Loss: 0.00215786
Iteration 4/25 | Loss: 0.00215786
Iteration 5/25 | Loss: 0.00215786
Iteration 6/25 | Loss: 0.00215786
Iteration 7/25 | Loss: 0.00215786
Iteration 8/25 | Loss: 0.00215786
Iteration 9/25 | Loss: 0.00215786
Iteration 10/25 | Loss: 0.00215786
Iteration 11/25 | Loss: 0.00215786
Iteration 12/25 | Loss: 0.00215786
Iteration 13/25 | Loss: 0.00215786
Iteration 14/25 | Loss: 0.00215786
Iteration 15/25 | Loss: 0.00215786
Iteration 16/25 | Loss: 0.00215785
Iteration 17/25 | Loss: 0.00215785
Iteration 18/25 | Loss: 0.00215785
Iteration 19/25 | Loss: 0.00215785
Iteration 20/25 | Loss: 0.00215785
Iteration 21/25 | Loss: 0.00215785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002157854847609997, 0.002157854847609997, 0.002157854847609997, 0.002157854847609997, 0.002157854847609997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002157854847609997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215785
Iteration 2/1000 | Loss: 0.00063019
Iteration 3/1000 | Loss: 0.00024758
Iteration 4/1000 | Loss: 0.00044675
Iteration 5/1000 | Loss: 0.00059111
Iteration 6/1000 | Loss: 0.00031877
Iteration 7/1000 | Loss: 0.00061833
Iteration 8/1000 | Loss: 0.00049515
Iteration 9/1000 | Loss: 0.00034446
Iteration 10/1000 | Loss: 0.00031383
Iteration 11/1000 | Loss: 0.00048099
Iteration 12/1000 | Loss: 0.00029610
Iteration 13/1000 | Loss: 0.00015656
Iteration 14/1000 | Loss: 0.00039640
Iteration 15/1000 | Loss: 0.00061077
Iteration 16/1000 | Loss: 0.00036126
Iteration 17/1000 | Loss: 0.00034831
Iteration 18/1000 | Loss: 0.00016864
Iteration 19/1000 | Loss: 0.00024408
Iteration 20/1000 | Loss: 0.00019635
Iteration 21/1000 | Loss: 0.00045965
Iteration 22/1000 | Loss: 0.00018284
Iteration 23/1000 | Loss: 0.00031105
Iteration 24/1000 | Loss: 0.00026361
Iteration 25/1000 | Loss: 0.00062182
Iteration 26/1000 | Loss: 0.00057768
Iteration 27/1000 | Loss: 0.00029358
Iteration 28/1000 | Loss: 0.00026355
Iteration 29/1000 | Loss: 0.00023564
Iteration 30/1000 | Loss: 0.00040023
Iteration 31/1000 | Loss: 0.00058826
Iteration 32/1000 | Loss: 0.00162420
Iteration 33/1000 | Loss: 0.00070023
Iteration 34/1000 | Loss: 0.00012194
Iteration 35/1000 | Loss: 0.00010692
Iteration 36/1000 | Loss: 0.00036169
Iteration 37/1000 | Loss: 0.00010106
Iteration 38/1000 | Loss: 0.00009523
Iteration 39/1000 | Loss: 0.00056047
Iteration 40/1000 | Loss: 0.00040172
Iteration 41/1000 | Loss: 0.00041845
Iteration 42/1000 | Loss: 0.00018951
Iteration 43/1000 | Loss: 0.00021806
Iteration 44/1000 | Loss: 0.00054002
Iteration 45/1000 | Loss: 0.00039715
Iteration 46/1000 | Loss: 0.00018578
Iteration 47/1000 | Loss: 0.00042636
Iteration 48/1000 | Loss: 0.00017912
Iteration 49/1000 | Loss: 0.00023914
Iteration 50/1000 | Loss: 0.00021060
Iteration 51/1000 | Loss: 0.00019698
Iteration 52/1000 | Loss: 0.00015038
Iteration 53/1000 | Loss: 0.00008516
Iteration 54/1000 | Loss: 0.00007881
Iteration 55/1000 | Loss: 0.00066130
Iteration 56/1000 | Loss: 0.00066203
Iteration 57/1000 | Loss: 0.00079265
Iteration 58/1000 | Loss: 0.00059272
Iteration 59/1000 | Loss: 0.00089189
Iteration 60/1000 | Loss: 0.00058137
Iteration 61/1000 | Loss: 0.00094851
Iteration 62/1000 | Loss: 0.00058445
Iteration 63/1000 | Loss: 0.00063082
Iteration 64/1000 | Loss: 0.00023925
Iteration 65/1000 | Loss: 0.00059895
Iteration 66/1000 | Loss: 0.00077405
Iteration 67/1000 | Loss: 0.00094688
Iteration 68/1000 | Loss: 0.00080618
Iteration 69/1000 | Loss: 0.00039918
Iteration 70/1000 | Loss: 0.00031202
Iteration 71/1000 | Loss: 0.00039089
Iteration 72/1000 | Loss: 0.00037175
Iteration 73/1000 | Loss: 0.00009634
Iteration 74/1000 | Loss: 0.00008061
Iteration 75/1000 | Loss: 0.00007672
Iteration 76/1000 | Loss: 0.00017210
Iteration 77/1000 | Loss: 0.00007483
Iteration 78/1000 | Loss: 0.00007058
Iteration 79/1000 | Loss: 0.00009286
Iteration 80/1000 | Loss: 0.00025246
Iteration 81/1000 | Loss: 0.00251398
Iteration 82/1000 | Loss: 0.00022887
Iteration 83/1000 | Loss: 0.00009279
Iteration 84/1000 | Loss: 0.00007648
Iteration 85/1000 | Loss: 0.00043857
Iteration 86/1000 | Loss: 0.00053957
Iteration 87/1000 | Loss: 0.00023252
Iteration 88/1000 | Loss: 0.00008086
Iteration 89/1000 | Loss: 0.00011383
Iteration 90/1000 | Loss: 0.00008936
Iteration 91/1000 | Loss: 0.00006558
Iteration 92/1000 | Loss: 0.00006021
Iteration 93/1000 | Loss: 0.00005607
Iteration 94/1000 | Loss: 0.00005467
Iteration 95/1000 | Loss: 0.00086446
Iteration 96/1000 | Loss: 0.00005384
Iteration 97/1000 | Loss: 0.00005076
Iteration 98/1000 | Loss: 0.00039274
Iteration 99/1000 | Loss: 0.00006214
Iteration 100/1000 | Loss: 0.00005101
Iteration 101/1000 | Loss: 0.00004744
Iteration 102/1000 | Loss: 0.00004595
Iteration 103/1000 | Loss: 0.00007570
Iteration 104/1000 | Loss: 0.00005296
Iteration 105/1000 | Loss: 0.00004740
Iteration 106/1000 | Loss: 0.00007463
Iteration 107/1000 | Loss: 0.00004757
Iteration 108/1000 | Loss: 0.00004494
Iteration 109/1000 | Loss: 0.00004334
Iteration 110/1000 | Loss: 0.00004239
Iteration 111/1000 | Loss: 0.00004219
Iteration 112/1000 | Loss: 0.00004196
Iteration 113/1000 | Loss: 0.00004186
Iteration 114/1000 | Loss: 0.00004184
Iteration 115/1000 | Loss: 0.00004161
Iteration 116/1000 | Loss: 0.00004141
Iteration 117/1000 | Loss: 0.00004138
Iteration 118/1000 | Loss: 0.00004136
Iteration 119/1000 | Loss: 0.00004128
Iteration 120/1000 | Loss: 0.00004126
Iteration 121/1000 | Loss: 0.00004125
Iteration 122/1000 | Loss: 0.00004116
Iteration 123/1000 | Loss: 0.00004116
Iteration 124/1000 | Loss: 0.00004115
Iteration 125/1000 | Loss: 0.00004114
Iteration 126/1000 | Loss: 0.00004114
Iteration 127/1000 | Loss: 0.00004114
Iteration 128/1000 | Loss: 0.00004114
Iteration 129/1000 | Loss: 0.00004114
Iteration 130/1000 | Loss: 0.00004113
Iteration 131/1000 | Loss: 0.00004113
Iteration 132/1000 | Loss: 0.00004113
Iteration 133/1000 | Loss: 0.00004113
Iteration 134/1000 | Loss: 0.00004112
Iteration 135/1000 | Loss: 0.00004112
Iteration 136/1000 | Loss: 0.00004112
Iteration 137/1000 | Loss: 0.00004112
Iteration 138/1000 | Loss: 0.00004111
Iteration 139/1000 | Loss: 0.00004111
Iteration 140/1000 | Loss: 0.00004110
Iteration 141/1000 | Loss: 0.00004110
Iteration 142/1000 | Loss: 0.00004110
Iteration 143/1000 | Loss: 0.00004110
Iteration 144/1000 | Loss: 0.00004110
Iteration 145/1000 | Loss: 0.00004110
Iteration 146/1000 | Loss: 0.00004110
Iteration 147/1000 | Loss: 0.00004110
Iteration 148/1000 | Loss: 0.00004109
Iteration 149/1000 | Loss: 0.00004108
Iteration 150/1000 | Loss: 0.00004107
Iteration 151/1000 | Loss: 0.00004107
Iteration 152/1000 | Loss: 0.00004106
Iteration 153/1000 | Loss: 0.00004106
Iteration 154/1000 | Loss: 0.00004106
Iteration 155/1000 | Loss: 0.00004106
Iteration 156/1000 | Loss: 0.00004106
Iteration 157/1000 | Loss: 0.00004105
Iteration 158/1000 | Loss: 0.00004105
Iteration 159/1000 | Loss: 0.00004105
Iteration 160/1000 | Loss: 0.00004105
Iteration 161/1000 | Loss: 0.00004105
Iteration 162/1000 | Loss: 0.00004104
Iteration 163/1000 | Loss: 0.00004104
Iteration 164/1000 | Loss: 0.00004104
Iteration 165/1000 | Loss: 0.00004104
Iteration 166/1000 | Loss: 0.00004104
Iteration 167/1000 | Loss: 0.00004104
Iteration 168/1000 | Loss: 0.00004104
Iteration 169/1000 | Loss: 0.00004104
Iteration 170/1000 | Loss: 0.00004104
Iteration 171/1000 | Loss: 0.00004104
Iteration 172/1000 | Loss: 0.00004104
Iteration 173/1000 | Loss: 0.00004103
Iteration 174/1000 | Loss: 0.00004103
Iteration 175/1000 | Loss: 0.00004103
Iteration 176/1000 | Loss: 0.00004103
Iteration 177/1000 | Loss: 0.00004103
Iteration 178/1000 | Loss: 0.00004102
Iteration 179/1000 | Loss: 0.00004102
Iteration 180/1000 | Loss: 0.00004101
Iteration 181/1000 | Loss: 0.00004101
Iteration 182/1000 | Loss: 0.00004101
Iteration 183/1000 | Loss: 0.00004101
Iteration 184/1000 | Loss: 0.00004101
Iteration 185/1000 | Loss: 0.00004100
Iteration 186/1000 | Loss: 0.00004100
Iteration 187/1000 | Loss: 0.00004100
Iteration 188/1000 | Loss: 0.00004100
Iteration 189/1000 | Loss: 0.00004100
Iteration 190/1000 | Loss: 0.00004100
Iteration 191/1000 | Loss: 0.00004100
Iteration 192/1000 | Loss: 0.00004099
Iteration 193/1000 | Loss: 0.00004099
Iteration 194/1000 | Loss: 0.00004099
Iteration 195/1000 | Loss: 0.00004099
Iteration 196/1000 | Loss: 0.00004098
Iteration 197/1000 | Loss: 0.00004098
Iteration 198/1000 | Loss: 0.00004098
Iteration 199/1000 | Loss: 0.00004098
Iteration 200/1000 | Loss: 0.00004098
Iteration 201/1000 | Loss: 0.00004097
Iteration 202/1000 | Loss: 0.00004097
Iteration 203/1000 | Loss: 0.00004097
Iteration 204/1000 | Loss: 0.00004097
Iteration 205/1000 | Loss: 0.00004097
Iteration 206/1000 | Loss: 0.00004097
Iteration 207/1000 | Loss: 0.00004097
Iteration 208/1000 | Loss: 0.00004097
Iteration 209/1000 | Loss: 0.00004097
Iteration 210/1000 | Loss: 0.00004097
Iteration 211/1000 | Loss: 0.00004097
Iteration 212/1000 | Loss: 0.00004097
Iteration 213/1000 | Loss: 0.00004097
Iteration 214/1000 | Loss: 0.00004096
Iteration 215/1000 | Loss: 0.00004096
Iteration 216/1000 | Loss: 0.00004096
Iteration 217/1000 | Loss: 0.00004096
Iteration 218/1000 | Loss: 0.00004096
Iteration 219/1000 | Loss: 0.00004096
Iteration 220/1000 | Loss: 0.00004096
Iteration 221/1000 | Loss: 0.00004096
Iteration 222/1000 | Loss: 0.00004096
Iteration 223/1000 | Loss: 0.00004096
Iteration 224/1000 | Loss: 0.00004096
Iteration 225/1000 | Loss: 0.00004096
Iteration 226/1000 | Loss: 0.00004096
Iteration 227/1000 | Loss: 0.00004096
Iteration 228/1000 | Loss: 0.00004096
Iteration 229/1000 | Loss: 0.00004096
Iteration 230/1000 | Loss: 0.00004096
Iteration 231/1000 | Loss: 0.00004096
Iteration 232/1000 | Loss: 0.00004096
Iteration 233/1000 | Loss: 0.00004096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [4.09616477554664e-05, 4.09616477554664e-05, 4.09616477554664e-05, 4.09616477554664e-05, 4.09616477554664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.09616477554664e-05

Optimization complete. Final v2v error: 4.062723636627197 mm

Highest mean error: 10.72510814666748 mm for frame 46

Lowest mean error: 2.425570487976074 mm for frame 4

Saving results

Total time: 208.50571632385254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986787
Iteration 2/25 | Loss: 0.00181486
Iteration 3/25 | Loss: 0.00131862
Iteration 4/25 | Loss: 0.00126001
Iteration 5/25 | Loss: 0.00119772
Iteration 6/25 | Loss: 0.00118363
Iteration 7/25 | Loss: 0.00112953
Iteration 8/25 | Loss: 0.00112065
Iteration 9/25 | Loss: 0.00111613
Iteration 10/25 | Loss: 0.00111363
Iteration 11/25 | Loss: 0.00111612
Iteration 12/25 | Loss: 0.00111501
Iteration 13/25 | Loss: 0.00111322
Iteration 14/25 | Loss: 0.00111490
Iteration 15/25 | Loss: 0.00111264
Iteration 16/25 | Loss: 0.00111153
Iteration 17/25 | Loss: 0.00111355
Iteration 18/25 | Loss: 0.00111226
Iteration 19/25 | Loss: 0.00110979
Iteration 20/25 | Loss: 0.00111227
Iteration 21/25 | Loss: 0.00111094
Iteration 22/25 | Loss: 0.00110833
Iteration 23/25 | Loss: 0.00110751
Iteration 24/25 | Loss: 0.00110712
Iteration 25/25 | Loss: 0.00110688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39078450
Iteration 2/25 | Loss: 0.00086629
Iteration 3/25 | Loss: 0.00085481
Iteration 4/25 | Loss: 0.00085481
Iteration 5/25 | Loss: 0.00085480
Iteration 6/25 | Loss: 0.00085480
Iteration 7/25 | Loss: 0.00085480
Iteration 8/25 | Loss: 0.00085480
Iteration 9/25 | Loss: 0.00085480
Iteration 10/25 | Loss: 0.00085480
Iteration 11/25 | Loss: 0.00085480
Iteration 12/25 | Loss: 0.00085480
Iteration 13/25 | Loss: 0.00085480
Iteration 14/25 | Loss: 0.00085480
Iteration 15/25 | Loss: 0.00085480
Iteration 16/25 | Loss: 0.00085480
Iteration 17/25 | Loss: 0.00085480
Iteration 18/25 | Loss: 0.00085480
Iteration 19/25 | Loss: 0.00085480
Iteration 20/25 | Loss: 0.00085480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008548015030100942, 0.0008548015030100942, 0.0008548015030100942, 0.0008548015030100942, 0.0008548015030100942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008548015030100942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085480
Iteration 2/1000 | Loss: 0.00005163
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00002501
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002505
Iteration 8/1000 | Loss: 0.00003263
Iteration 9/1000 | Loss: 0.00010692
Iteration 10/1000 | Loss: 0.00012233
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00009778
Iteration 13/1000 | Loss: 0.00002948
Iteration 14/1000 | Loss: 0.00002262
Iteration 15/1000 | Loss: 0.00002537
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001257
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2287171557545662e-05, 1.2287171557545662e-05, 1.2287171557545662e-05, 1.2287171557545662e-05, 1.2287171557545662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2287171557545662e-05

Optimization complete. Final v2v error: 2.8838939666748047 mm

Highest mean error: 5.899989128112793 mm for frame 1

Lowest mean error: 2.337961435317993 mm for frame 237

Saving results

Total time: 98.98812675476074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565148
Iteration 2/25 | Loss: 0.00111650
Iteration 3/25 | Loss: 0.00104341
Iteration 4/25 | Loss: 0.00103332
Iteration 5/25 | Loss: 0.00102993
Iteration 6/25 | Loss: 0.00102922
Iteration 7/25 | Loss: 0.00102922
Iteration 8/25 | Loss: 0.00102922
Iteration 9/25 | Loss: 0.00102922
Iteration 10/25 | Loss: 0.00102922
Iteration 11/25 | Loss: 0.00102922
Iteration 12/25 | Loss: 0.00102922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010292226215824485, 0.0010292226215824485, 0.0010292226215824485, 0.0010292226215824485, 0.0010292226215824485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010292226215824485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87648892
Iteration 2/25 | Loss: 0.00068640
Iteration 3/25 | Loss: 0.00068640
Iteration 4/25 | Loss: 0.00068639
Iteration 5/25 | Loss: 0.00068639
Iteration 6/25 | Loss: 0.00068639
Iteration 7/25 | Loss: 0.00068639
Iteration 8/25 | Loss: 0.00068639
Iteration 9/25 | Loss: 0.00068639
Iteration 10/25 | Loss: 0.00068639
Iteration 11/25 | Loss: 0.00068639
Iteration 12/25 | Loss: 0.00068639
Iteration 13/25 | Loss: 0.00068639
Iteration 14/25 | Loss: 0.00068639
Iteration 15/25 | Loss: 0.00068639
Iteration 16/25 | Loss: 0.00068639
Iteration 17/25 | Loss: 0.00068639
Iteration 18/25 | Loss: 0.00068639
Iteration 19/25 | Loss: 0.00068639
Iteration 20/25 | Loss: 0.00068639
Iteration 21/25 | Loss: 0.00068639
Iteration 22/25 | Loss: 0.00068639
Iteration 23/25 | Loss: 0.00068639
Iteration 24/25 | Loss: 0.00068639
Iteration 25/25 | Loss: 0.00068639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068639
Iteration 2/1000 | Loss: 0.00001765
Iteration 3/1000 | Loss: 0.00001221
Iteration 4/1000 | Loss: 0.00001057
Iteration 5/1000 | Loss: 0.00000994
Iteration 6/1000 | Loss: 0.00000952
Iteration 7/1000 | Loss: 0.00000924
Iteration 8/1000 | Loss: 0.00000923
Iteration 9/1000 | Loss: 0.00000910
Iteration 10/1000 | Loss: 0.00000890
Iteration 11/1000 | Loss: 0.00000890
Iteration 12/1000 | Loss: 0.00000887
Iteration 13/1000 | Loss: 0.00000886
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000881
Iteration 16/1000 | Loss: 0.00000881
Iteration 17/1000 | Loss: 0.00000876
Iteration 18/1000 | Loss: 0.00000876
Iteration 19/1000 | Loss: 0.00000875
Iteration 20/1000 | Loss: 0.00000874
Iteration 21/1000 | Loss: 0.00000869
Iteration 22/1000 | Loss: 0.00000864
Iteration 23/1000 | Loss: 0.00000863
Iteration 24/1000 | Loss: 0.00000863
Iteration 25/1000 | Loss: 0.00000863
Iteration 26/1000 | Loss: 0.00000863
Iteration 27/1000 | Loss: 0.00000862
Iteration 28/1000 | Loss: 0.00000861
Iteration 29/1000 | Loss: 0.00000861
Iteration 30/1000 | Loss: 0.00000859
Iteration 31/1000 | Loss: 0.00000858
Iteration 32/1000 | Loss: 0.00000858
Iteration 33/1000 | Loss: 0.00000858
Iteration 34/1000 | Loss: 0.00000858
Iteration 35/1000 | Loss: 0.00000858
Iteration 36/1000 | Loss: 0.00000857
Iteration 37/1000 | Loss: 0.00000856
Iteration 38/1000 | Loss: 0.00000855
Iteration 39/1000 | Loss: 0.00000855
Iteration 40/1000 | Loss: 0.00000855
Iteration 41/1000 | Loss: 0.00000855
Iteration 42/1000 | Loss: 0.00000855
Iteration 43/1000 | Loss: 0.00000854
Iteration 44/1000 | Loss: 0.00000854
Iteration 45/1000 | Loss: 0.00000854
Iteration 46/1000 | Loss: 0.00000854
Iteration 47/1000 | Loss: 0.00000854
Iteration 48/1000 | Loss: 0.00000854
Iteration 49/1000 | Loss: 0.00000852
Iteration 50/1000 | Loss: 0.00000851
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000850
Iteration 54/1000 | Loss: 0.00000849
Iteration 55/1000 | Loss: 0.00000849
Iteration 56/1000 | Loss: 0.00000848
Iteration 57/1000 | Loss: 0.00000848
Iteration 58/1000 | Loss: 0.00000848
Iteration 59/1000 | Loss: 0.00000848
Iteration 60/1000 | Loss: 0.00000848
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000847
Iteration 63/1000 | Loss: 0.00000847
Iteration 64/1000 | Loss: 0.00000847
Iteration 65/1000 | Loss: 0.00000847
Iteration 66/1000 | Loss: 0.00000847
Iteration 67/1000 | Loss: 0.00000847
Iteration 68/1000 | Loss: 0.00000847
Iteration 69/1000 | Loss: 0.00000847
Iteration 70/1000 | Loss: 0.00000847
Iteration 71/1000 | Loss: 0.00000847
Iteration 72/1000 | Loss: 0.00000847
Iteration 73/1000 | Loss: 0.00000847
Iteration 74/1000 | Loss: 0.00000847
Iteration 75/1000 | Loss: 0.00000847
Iteration 76/1000 | Loss: 0.00000847
Iteration 77/1000 | Loss: 0.00000846
Iteration 78/1000 | Loss: 0.00000846
Iteration 79/1000 | Loss: 0.00000846
Iteration 80/1000 | Loss: 0.00000846
Iteration 81/1000 | Loss: 0.00000845
Iteration 82/1000 | Loss: 0.00000845
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000843
Iteration 90/1000 | Loss: 0.00000843
Iteration 91/1000 | Loss: 0.00000843
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000842
Iteration 94/1000 | Loss: 0.00000842
Iteration 95/1000 | Loss: 0.00000842
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000842
Iteration 102/1000 | Loss: 0.00000842
Iteration 103/1000 | Loss: 0.00000842
Iteration 104/1000 | Loss: 0.00000842
Iteration 105/1000 | Loss: 0.00000842
Iteration 106/1000 | Loss: 0.00000842
Iteration 107/1000 | Loss: 0.00000841
Iteration 108/1000 | Loss: 0.00000841
Iteration 109/1000 | Loss: 0.00000841
Iteration 110/1000 | Loss: 0.00000841
Iteration 111/1000 | Loss: 0.00000841
Iteration 112/1000 | Loss: 0.00000841
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000841
Iteration 116/1000 | Loss: 0.00000841
Iteration 117/1000 | Loss: 0.00000841
Iteration 118/1000 | Loss: 0.00000841
Iteration 119/1000 | Loss: 0.00000841
Iteration 120/1000 | Loss: 0.00000841
Iteration 121/1000 | Loss: 0.00000841
Iteration 122/1000 | Loss: 0.00000841
Iteration 123/1000 | Loss: 0.00000841
Iteration 124/1000 | Loss: 0.00000841
Iteration 125/1000 | Loss: 0.00000841
Iteration 126/1000 | Loss: 0.00000841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [8.413862815359607e-06, 8.413862815359607e-06, 8.413862815359607e-06, 8.413862815359607e-06, 8.413862815359607e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.413862815359607e-06

Optimization complete. Final v2v error: 2.51395320892334 mm

Highest mean error: 2.9008965492248535 mm for frame 118

Lowest mean error: 2.3908474445343018 mm for frame 44

Saving results

Total time: 31.497242212295532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399903
Iteration 2/25 | Loss: 0.00118362
Iteration 3/25 | Loss: 0.00106947
Iteration 4/25 | Loss: 0.00106258
Iteration 5/25 | Loss: 0.00106027
Iteration 6/25 | Loss: 0.00106027
Iteration 7/25 | Loss: 0.00106027
Iteration 8/25 | Loss: 0.00106027
Iteration 9/25 | Loss: 0.00106027
Iteration 10/25 | Loss: 0.00106027
Iteration 11/25 | Loss: 0.00106027
Iteration 12/25 | Loss: 0.00106027
Iteration 13/25 | Loss: 0.00106027
Iteration 14/25 | Loss: 0.00106027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010602707043290138, 0.0010602707043290138, 0.0010602707043290138, 0.0010602707043290138, 0.0010602707043290138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010602707043290138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67854226
Iteration 2/25 | Loss: 0.00060124
Iteration 3/25 | Loss: 0.00060124
Iteration 4/25 | Loss: 0.00060124
Iteration 5/25 | Loss: 0.00060124
Iteration 6/25 | Loss: 0.00060124
Iteration 7/25 | Loss: 0.00060124
Iteration 8/25 | Loss: 0.00060124
Iteration 9/25 | Loss: 0.00060124
Iteration 10/25 | Loss: 0.00060124
Iteration 11/25 | Loss: 0.00060124
Iteration 12/25 | Loss: 0.00060124
Iteration 13/25 | Loss: 0.00060124
Iteration 14/25 | Loss: 0.00060124
Iteration 15/25 | Loss: 0.00060124
Iteration 16/25 | Loss: 0.00060124
Iteration 17/25 | Loss: 0.00060124
Iteration 18/25 | Loss: 0.00060124
Iteration 19/25 | Loss: 0.00060124
Iteration 20/25 | Loss: 0.00060124
Iteration 21/25 | Loss: 0.00060124
Iteration 22/25 | Loss: 0.00060124
Iteration 23/25 | Loss: 0.00060124
Iteration 24/25 | Loss: 0.00060124
Iteration 25/25 | Loss: 0.00060124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006012371741235256, 0.0006012371741235256, 0.0006012371741235256, 0.0006012371741235256, 0.0006012371741235256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006012371741235256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060124
Iteration 2/1000 | Loss: 0.00002542
Iteration 3/1000 | Loss: 0.00001368
Iteration 4/1000 | Loss: 0.00001207
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001053
Iteration 7/1000 | Loss: 0.00001015
Iteration 8/1000 | Loss: 0.00001010
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000952
Iteration 11/1000 | Loss: 0.00000943
Iteration 12/1000 | Loss: 0.00000941
Iteration 13/1000 | Loss: 0.00000934
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000911
Iteration 20/1000 | Loss: 0.00000911
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000908
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000906
Iteration 26/1000 | Loss: 0.00000905
Iteration 27/1000 | Loss: 0.00000905
Iteration 28/1000 | Loss: 0.00000905
Iteration 29/1000 | Loss: 0.00000905
Iteration 30/1000 | Loss: 0.00000905
Iteration 31/1000 | Loss: 0.00000905
Iteration 32/1000 | Loss: 0.00000904
Iteration 33/1000 | Loss: 0.00000904
Iteration 34/1000 | Loss: 0.00000904
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000904
Iteration 37/1000 | Loss: 0.00000904
Iteration 38/1000 | Loss: 0.00000904
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000903
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000902
Iteration 47/1000 | Loss: 0.00000901
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000900
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000898
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000897
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000897
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000896
Iteration 65/1000 | Loss: 0.00000896
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000894
Iteration 69/1000 | Loss: 0.00000893
Iteration 70/1000 | Loss: 0.00000893
Iteration 71/1000 | Loss: 0.00000892
Iteration 72/1000 | Loss: 0.00000892
Iteration 73/1000 | Loss: 0.00000892
Iteration 74/1000 | Loss: 0.00000892
Iteration 75/1000 | Loss: 0.00000891
Iteration 76/1000 | Loss: 0.00000891
Iteration 77/1000 | Loss: 0.00000890
Iteration 78/1000 | Loss: 0.00000890
Iteration 79/1000 | Loss: 0.00000890
Iteration 80/1000 | Loss: 0.00000890
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000889
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000889
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000888
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000886
Iteration 98/1000 | Loss: 0.00000886
Iteration 99/1000 | Loss: 0.00000885
Iteration 100/1000 | Loss: 0.00000885
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000885
Iteration 103/1000 | Loss: 0.00000885
Iteration 104/1000 | Loss: 0.00000885
Iteration 105/1000 | Loss: 0.00000885
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000884
Iteration 109/1000 | Loss: 0.00000884
Iteration 110/1000 | Loss: 0.00000884
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000883
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000882
Iteration 119/1000 | Loss: 0.00000882
Iteration 120/1000 | Loss: 0.00000882
Iteration 121/1000 | Loss: 0.00000882
Iteration 122/1000 | Loss: 0.00000882
Iteration 123/1000 | Loss: 0.00000882
Iteration 124/1000 | Loss: 0.00000882
Iteration 125/1000 | Loss: 0.00000882
Iteration 126/1000 | Loss: 0.00000881
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000881
Iteration 130/1000 | Loss: 0.00000881
Iteration 131/1000 | Loss: 0.00000881
Iteration 132/1000 | Loss: 0.00000881
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000880
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000880
Iteration 141/1000 | Loss: 0.00000880
Iteration 142/1000 | Loss: 0.00000880
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000879
Iteration 145/1000 | Loss: 0.00000879
Iteration 146/1000 | Loss: 0.00000879
Iteration 147/1000 | Loss: 0.00000879
Iteration 148/1000 | Loss: 0.00000879
Iteration 149/1000 | Loss: 0.00000879
Iteration 150/1000 | Loss: 0.00000879
Iteration 151/1000 | Loss: 0.00000879
Iteration 152/1000 | Loss: 0.00000879
Iteration 153/1000 | Loss: 0.00000878
Iteration 154/1000 | Loss: 0.00000878
Iteration 155/1000 | Loss: 0.00000878
Iteration 156/1000 | Loss: 0.00000878
Iteration 157/1000 | Loss: 0.00000878
Iteration 158/1000 | Loss: 0.00000878
Iteration 159/1000 | Loss: 0.00000878
Iteration 160/1000 | Loss: 0.00000877
Iteration 161/1000 | Loss: 0.00000877
Iteration 162/1000 | Loss: 0.00000877
Iteration 163/1000 | Loss: 0.00000877
Iteration 164/1000 | Loss: 0.00000877
Iteration 165/1000 | Loss: 0.00000877
Iteration 166/1000 | Loss: 0.00000877
Iteration 167/1000 | Loss: 0.00000876
Iteration 168/1000 | Loss: 0.00000876
Iteration 169/1000 | Loss: 0.00000876
Iteration 170/1000 | Loss: 0.00000876
Iteration 171/1000 | Loss: 0.00000876
Iteration 172/1000 | Loss: 0.00000876
Iteration 173/1000 | Loss: 0.00000876
Iteration 174/1000 | Loss: 0.00000876
Iteration 175/1000 | Loss: 0.00000876
Iteration 176/1000 | Loss: 0.00000876
Iteration 177/1000 | Loss: 0.00000876
Iteration 178/1000 | Loss: 0.00000875
Iteration 179/1000 | Loss: 0.00000875
Iteration 180/1000 | Loss: 0.00000875
Iteration 181/1000 | Loss: 0.00000875
Iteration 182/1000 | Loss: 0.00000875
Iteration 183/1000 | Loss: 0.00000875
Iteration 184/1000 | Loss: 0.00000875
Iteration 185/1000 | Loss: 0.00000875
Iteration 186/1000 | Loss: 0.00000875
Iteration 187/1000 | Loss: 0.00000875
Iteration 188/1000 | Loss: 0.00000875
Iteration 189/1000 | Loss: 0.00000875
Iteration 190/1000 | Loss: 0.00000875
Iteration 191/1000 | Loss: 0.00000874
Iteration 192/1000 | Loss: 0.00000874
Iteration 193/1000 | Loss: 0.00000874
Iteration 194/1000 | Loss: 0.00000874
Iteration 195/1000 | Loss: 0.00000874
Iteration 196/1000 | Loss: 0.00000873
Iteration 197/1000 | Loss: 0.00000873
Iteration 198/1000 | Loss: 0.00000873
Iteration 199/1000 | Loss: 0.00000873
Iteration 200/1000 | Loss: 0.00000873
Iteration 201/1000 | Loss: 0.00000873
Iteration 202/1000 | Loss: 0.00000873
Iteration 203/1000 | Loss: 0.00000873
Iteration 204/1000 | Loss: 0.00000873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [8.73238968779333e-06, 8.73238968779333e-06, 8.73238968779333e-06, 8.73238968779333e-06, 8.73238968779333e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.73238968779333e-06

Optimization complete. Final v2v error: 2.559781789779663 mm

Highest mean error: 2.7482733726501465 mm for frame 19

Lowest mean error: 2.4036622047424316 mm for frame 14

Saving results

Total time: 43.01929187774658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040155
Iteration 2/25 | Loss: 0.00141887
Iteration 3/25 | Loss: 0.00120706
Iteration 4/25 | Loss: 0.00118074
Iteration 5/25 | Loss: 0.00117220
Iteration 6/25 | Loss: 0.00116966
Iteration 7/25 | Loss: 0.00116966
Iteration 8/25 | Loss: 0.00116966
Iteration 9/25 | Loss: 0.00116966
Iteration 10/25 | Loss: 0.00116966
Iteration 11/25 | Loss: 0.00116966
Iteration 12/25 | Loss: 0.00116966
Iteration 13/25 | Loss: 0.00116966
Iteration 14/25 | Loss: 0.00116966
Iteration 15/25 | Loss: 0.00116966
Iteration 16/25 | Loss: 0.00116966
Iteration 17/25 | Loss: 0.00116966
Iteration 18/25 | Loss: 0.00116966
Iteration 19/25 | Loss: 0.00116966
Iteration 20/25 | Loss: 0.00116966
Iteration 21/25 | Loss: 0.00116966
Iteration 22/25 | Loss: 0.00116966
Iteration 23/25 | Loss: 0.00116966
Iteration 24/25 | Loss: 0.00116966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011696633882820606, 0.0011696633882820606, 0.0011696633882820606, 0.0011696633882820606, 0.0011696633882820606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011696633882820606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76368964
Iteration 2/25 | Loss: 0.00079607
Iteration 3/25 | Loss: 0.00079598
Iteration 4/25 | Loss: 0.00079598
Iteration 5/25 | Loss: 0.00079598
Iteration 6/25 | Loss: 0.00079598
Iteration 7/25 | Loss: 0.00079598
Iteration 8/25 | Loss: 0.00079598
Iteration 9/25 | Loss: 0.00079598
Iteration 10/25 | Loss: 0.00079598
Iteration 11/25 | Loss: 0.00079598
Iteration 12/25 | Loss: 0.00079598
Iteration 13/25 | Loss: 0.00079598
Iteration 14/25 | Loss: 0.00079598
Iteration 15/25 | Loss: 0.00079598
Iteration 16/25 | Loss: 0.00079598
Iteration 17/25 | Loss: 0.00079598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007959757931530476, 0.0007959757931530476, 0.0007959757931530476, 0.0007959757931530476, 0.0007959757931530476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007959757931530476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079598
Iteration 2/1000 | Loss: 0.00007191
Iteration 3/1000 | Loss: 0.00004211
Iteration 4/1000 | Loss: 0.00003288
Iteration 5/1000 | Loss: 0.00003063
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00002837
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002730
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002660
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002622
Iteration 14/1000 | Loss: 0.00002618
Iteration 15/1000 | Loss: 0.00002614
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002611
Iteration 19/1000 | Loss: 0.00002611
Iteration 20/1000 | Loss: 0.00002602
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002594
Iteration 24/1000 | Loss: 0.00002591
Iteration 25/1000 | Loss: 0.00002590
Iteration 26/1000 | Loss: 0.00002589
Iteration 27/1000 | Loss: 0.00002583
Iteration 28/1000 | Loss: 0.00002579
Iteration 29/1000 | Loss: 0.00002575
Iteration 30/1000 | Loss: 0.00002574
Iteration 31/1000 | Loss: 0.00002573
Iteration 32/1000 | Loss: 0.00002571
Iteration 33/1000 | Loss: 0.00002568
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002563
Iteration 36/1000 | Loss: 0.00002563
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00002562
Iteration 39/1000 | Loss: 0.00002562
Iteration 40/1000 | Loss: 0.00002562
Iteration 41/1000 | Loss: 0.00002562
Iteration 42/1000 | Loss: 0.00002562
Iteration 43/1000 | Loss: 0.00002562
Iteration 44/1000 | Loss: 0.00002562
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00002562
Iteration 47/1000 | Loss: 0.00002562
Iteration 48/1000 | Loss: 0.00002561
Iteration 49/1000 | Loss: 0.00002559
Iteration 50/1000 | Loss: 0.00002558
Iteration 51/1000 | Loss: 0.00002558
Iteration 52/1000 | Loss: 0.00002557
Iteration 53/1000 | Loss: 0.00002556
Iteration 54/1000 | Loss: 0.00002556
Iteration 55/1000 | Loss: 0.00002554
Iteration 56/1000 | Loss: 0.00002554
Iteration 57/1000 | Loss: 0.00002554
Iteration 58/1000 | Loss: 0.00002554
Iteration 59/1000 | Loss: 0.00002554
Iteration 60/1000 | Loss: 0.00002554
Iteration 61/1000 | Loss: 0.00002553
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002553
Iteration 64/1000 | Loss: 0.00002552
Iteration 65/1000 | Loss: 0.00002552
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002551
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002550
Iteration 74/1000 | Loss: 0.00002550
Iteration 75/1000 | Loss: 0.00002550
Iteration 76/1000 | Loss: 0.00002550
Iteration 77/1000 | Loss: 0.00002550
Iteration 78/1000 | Loss: 0.00002550
Iteration 79/1000 | Loss: 0.00002550
Iteration 80/1000 | Loss: 0.00002550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.5495104637229815e-05, 2.5495104637229815e-05, 2.5495104637229815e-05, 2.5495104637229815e-05, 2.5495104637229815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5495104637229815e-05

Optimization complete. Final v2v error: 4.18839693069458 mm

Highest mean error: 5.496487617492676 mm for frame 112

Lowest mean error: 3.4931092262268066 mm for frame 68

Saving results

Total time: 41.80384635925293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796926
Iteration 2/25 | Loss: 0.00146546
Iteration 3/25 | Loss: 0.00124541
Iteration 4/25 | Loss: 0.00114950
Iteration 5/25 | Loss: 0.00111683
Iteration 6/25 | Loss: 0.00110359
Iteration 7/25 | Loss: 0.00110139
Iteration 8/25 | Loss: 0.00107804
Iteration 9/25 | Loss: 0.00107545
Iteration 10/25 | Loss: 0.00107532
Iteration 11/25 | Loss: 0.00107531
Iteration 12/25 | Loss: 0.00107531
Iteration 13/25 | Loss: 0.00107531
Iteration 14/25 | Loss: 0.00107531
Iteration 15/25 | Loss: 0.00107531
Iteration 16/25 | Loss: 0.00107531
Iteration 17/25 | Loss: 0.00107531
Iteration 18/25 | Loss: 0.00107531
Iteration 19/25 | Loss: 0.00107530
Iteration 20/25 | Loss: 0.00107530
Iteration 21/25 | Loss: 0.00107530
Iteration 22/25 | Loss: 0.00107530
Iteration 23/25 | Loss: 0.00107530
Iteration 24/25 | Loss: 0.00107530
Iteration 25/25 | Loss: 0.00107530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97398949
Iteration 2/25 | Loss: 0.00072336
Iteration 3/25 | Loss: 0.00072336
Iteration 4/25 | Loss: 0.00072336
Iteration 5/25 | Loss: 0.00072336
Iteration 6/25 | Loss: 0.00072336
Iteration 7/25 | Loss: 0.00072336
Iteration 8/25 | Loss: 0.00072336
Iteration 9/25 | Loss: 0.00072336
Iteration 10/25 | Loss: 0.00072336
Iteration 11/25 | Loss: 0.00072336
Iteration 12/25 | Loss: 0.00072336
Iteration 13/25 | Loss: 0.00072336
Iteration 14/25 | Loss: 0.00072336
Iteration 15/25 | Loss: 0.00072336
Iteration 16/25 | Loss: 0.00072336
Iteration 17/25 | Loss: 0.00072336
Iteration 18/25 | Loss: 0.00072336
Iteration 19/25 | Loss: 0.00072335
Iteration 20/25 | Loss: 0.00072335
Iteration 21/25 | Loss: 0.00072335
Iteration 22/25 | Loss: 0.00072335
Iteration 23/25 | Loss: 0.00072335
Iteration 24/25 | Loss: 0.00072335
Iteration 25/25 | Loss: 0.00072335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072335
Iteration 2/1000 | Loss: 0.00002536
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00012536
Iteration 8/1000 | Loss: 0.00002451
Iteration 9/1000 | Loss: 0.00001823
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00007620
Iteration 12/1000 | Loss: 0.00007620
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00014302
Iteration 16/1000 | Loss: 0.00003053
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00001768
Iteration 19/1000 | Loss: 0.00002033
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00008376
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001428
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001428
Iteration 43/1000 | Loss: 0.00001428
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001423
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00001419
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001411
Iteration 73/1000 | Loss: 0.00001410
Iteration 74/1000 | Loss: 0.00011688
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00008386
Iteration 98/1000 | Loss: 0.00008386
Iteration 99/1000 | Loss: 0.00008386
Iteration 100/1000 | Loss: 0.00022687
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00010529
Iteration 103/1000 | Loss: 0.00002925
Iteration 104/1000 | Loss: 0.00008399
Iteration 105/1000 | Loss: 0.00003815
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001412
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001403
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001403
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00009457
Iteration 122/1000 | Loss: 0.00007158
Iteration 123/1000 | Loss: 0.00001413
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00008024
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001399
Iteration 137/1000 | Loss: 0.00001399
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001398
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Iteration 145/1000 | Loss: 0.00001398
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001398
Iteration 163/1000 | Loss: 0.00001398
Iteration 164/1000 | Loss: 0.00001398
Iteration 165/1000 | Loss: 0.00001398
Iteration 166/1000 | Loss: 0.00001398
Iteration 167/1000 | Loss: 0.00001398
Iteration 168/1000 | Loss: 0.00001398
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.3981338270241395e-05, 1.3981338270241395e-05, 1.3981338270241395e-05, 1.3981338270241395e-05, 1.3981338270241395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3981338270241395e-05

Optimization complete. Final v2v error: 3.133939743041992 mm

Highest mean error: 4.1713433265686035 mm for frame 107

Lowest mean error: 2.6809608936309814 mm for frame 0

Saving results

Total time: 84.64799952507019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907169
Iteration 2/25 | Loss: 0.00153541
Iteration 3/25 | Loss: 0.00121565
Iteration 4/25 | Loss: 0.00118689
Iteration 5/25 | Loss: 0.00117852
Iteration 6/25 | Loss: 0.00117638
Iteration 7/25 | Loss: 0.00117638
Iteration 8/25 | Loss: 0.00117638
Iteration 9/25 | Loss: 0.00117638
Iteration 10/25 | Loss: 0.00117638
Iteration 11/25 | Loss: 0.00117638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001176382414996624, 0.001176382414996624, 0.001176382414996624, 0.001176382414996624, 0.001176382414996624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001176382414996624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08615685
Iteration 2/25 | Loss: 0.00067944
Iteration 3/25 | Loss: 0.00067944
Iteration 4/25 | Loss: 0.00067944
Iteration 5/25 | Loss: 0.00067944
Iteration 6/25 | Loss: 0.00067943
Iteration 7/25 | Loss: 0.00067943
Iteration 8/25 | Loss: 0.00067943
Iteration 9/25 | Loss: 0.00067943
Iteration 10/25 | Loss: 0.00067943
Iteration 11/25 | Loss: 0.00067943
Iteration 12/25 | Loss: 0.00067943
Iteration 13/25 | Loss: 0.00067943
Iteration 14/25 | Loss: 0.00067943
Iteration 15/25 | Loss: 0.00067943
Iteration 16/25 | Loss: 0.00067943
Iteration 17/25 | Loss: 0.00067943
Iteration 18/25 | Loss: 0.00067943
Iteration 19/25 | Loss: 0.00067943
Iteration 20/25 | Loss: 0.00067943
Iteration 21/25 | Loss: 0.00067943
Iteration 22/25 | Loss: 0.00067943
Iteration 23/25 | Loss: 0.00067943
Iteration 24/25 | Loss: 0.00067943
Iteration 25/25 | Loss: 0.00067943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067943
Iteration 2/1000 | Loss: 0.00004163
Iteration 3/1000 | Loss: 0.00002988
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002553
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00002422
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002295
Iteration 11/1000 | Loss: 0.00002264
Iteration 12/1000 | Loss: 0.00002247
Iteration 13/1000 | Loss: 0.00002225
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002171
Iteration 17/1000 | Loss: 0.00002155
Iteration 18/1000 | Loss: 0.00002147
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002136
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002121
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002112
Iteration 30/1000 | Loss: 0.00002111
Iteration 31/1000 | Loss: 0.00002110
Iteration 32/1000 | Loss: 0.00002110
Iteration 33/1000 | Loss: 0.00002110
Iteration 34/1000 | Loss: 0.00002110
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002109
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002107
Iteration 39/1000 | Loss: 0.00002107
Iteration 40/1000 | Loss: 0.00002107
Iteration 41/1000 | Loss: 0.00002107
Iteration 42/1000 | Loss: 0.00002107
Iteration 43/1000 | Loss: 0.00002107
Iteration 44/1000 | Loss: 0.00002107
Iteration 45/1000 | Loss: 0.00002107
Iteration 46/1000 | Loss: 0.00002107
Iteration 47/1000 | Loss: 0.00002107
Iteration 48/1000 | Loss: 0.00002107
Iteration 49/1000 | Loss: 0.00002107
Iteration 50/1000 | Loss: 0.00002107
Iteration 51/1000 | Loss: 0.00002105
Iteration 52/1000 | Loss: 0.00002105
Iteration 53/1000 | Loss: 0.00002105
Iteration 54/1000 | Loss: 0.00002105
Iteration 55/1000 | Loss: 0.00002105
Iteration 56/1000 | Loss: 0.00002105
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002104
Iteration 60/1000 | Loss: 0.00002104
Iteration 61/1000 | Loss: 0.00002104
Iteration 62/1000 | Loss: 0.00002104
Iteration 63/1000 | Loss: 0.00002104
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002104
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002104
Iteration 71/1000 | Loss: 0.00002104
Iteration 72/1000 | Loss: 0.00002103
Iteration 73/1000 | Loss: 0.00002103
Iteration 74/1000 | Loss: 0.00002103
Iteration 75/1000 | Loss: 0.00002102
Iteration 76/1000 | Loss: 0.00002102
Iteration 77/1000 | Loss: 0.00002102
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002102
Iteration 80/1000 | Loss: 0.00002102
Iteration 81/1000 | Loss: 0.00002102
Iteration 82/1000 | Loss: 0.00002102
Iteration 83/1000 | Loss: 0.00002102
Iteration 84/1000 | Loss: 0.00002102
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002102
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002101
Iteration 100/1000 | Loss: 0.00002101
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002101
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002101
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002101
Iteration 111/1000 | Loss: 0.00002101
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00002101
Iteration 125/1000 | Loss: 0.00002101
Iteration 126/1000 | Loss: 0.00002101
Iteration 127/1000 | Loss: 0.00002101
Iteration 128/1000 | Loss: 0.00002101
Iteration 129/1000 | Loss: 0.00002101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.1012319848523475e-05, 2.1012319848523475e-05, 2.1012319848523475e-05, 2.1012319848523475e-05, 2.1012319848523475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1012319848523475e-05

Optimization complete. Final v2v error: 3.748715877532959 mm

Highest mean error: 5.073753356933594 mm for frame 103

Lowest mean error: 2.943967580795288 mm for frame 122

Saving results

Total time: 41.80768632888794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392744
Iteration 2/25 | Loss: 0.00112093
Iteration 3/25 | Loss: 0.00103728
Iteration 4/25 | Loss: 0.00102743
Iteration 5/25 | Loss: 0.00102435
Iteration 6/25 | Loss: 0.00102416
Iteration 7/25 | Loss: 0.00102416
Iteration 8/25 | Loss: 0.00102416
Iteration 9/25 | Loss: 0.00102416
Iteration 10/25 | Loss: 0.00102409
Iteration 11/25 | Loss: 0.00102409
Iteration 12/25 | Loss: 0.00102409
Iteration 13/25 | Loss: 0.00102409
Iteration 14/25 | Loss: 0.00102409
Iteration 15/25 | Loss: 0.00102409
Iteration 16/25 | Loss: 0.00102409
Iteration 17/25 | Loss: 0.00102409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010240855626761913, 0.0010240855626761913, 0.0010240855626761913, 0.0010240855626761913, 0.0010240855626761913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010240855626761913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.99567962
Iteration 2/25 | Loss: 0.00062951
Iteration 3/25 | Loss: 0.00062950
Iteration 4/25 | Loss: 0.00062950
Iteration 5/25 | Loss: 0.00062950
Iteration 6/25 | Loss: 0.00062950
Iteration 7/25 | Loss: 0.00062950
Iteration 8/25 | Loss: 0.00062950
Iteration 9/25 | Loss: 0.00062950
Iteration 10/25 | Loss: 0.00062950
Iteration 11/25 | Loss: 0.00062950
Iteration 12/25 | Loss: 0.00062950
Iteration 13/25 | Loss: 0.00062950
Iteration 14/25 | Loss: 0.00062950
Iteration 15/25 | Loss: 0.00062950
Iteration 16/25 | Loss: 0.00062950
Iteration 17/25 | Loss: 0.00062950
Iteration 18/25 | Loss: 0.00062950
Iteration 19/25 | Loss: 0.00062950
Iteration 20/25 | Loss: 0.00062950
Iteration 21/25 | Loss: 0.00062950
Iteration 22/25 | Loss: 0.00062950
Iteration 23/25 | Loss: 0.00062950
Iteration 24/25 | Loss: 0.00062950
Iteration 25/25 | Loss: 0.00062950

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062950
Iteration 2/1000 | Loss: 0.00001628
Iteration 3/1000 | Loss: 0.00001191
Iteration 4/1000 | Loss: 0.00001102
Iteration 5/1000 | Loss: 0.00001050
Iteration 6/1000 | Loss: 0.00001013
Iteration 7/1000 | Loss: 0.00000998
Iteration 8/1000 | Loss: 0.00000994
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000971
Iteration 11/1000 | Loss: 0.00000958
Iteration 12/1000 | Loss: 0.00000951
Iteration 13/1000 | Loss: 0.00000949
Iteration 14/1000 | Loss: 0.00000940
Iteration 15/1000 | Loss: 0.00000937
Iteration 16/1000 | Loss: 0.00000936
Iteration 17/1000 | Loss: 0.00000935
Iteration 18/1000 | Loss: 0.00000929
Iteration 19/1000 | Loss: 0.00000926
Iteration 20/1000 | Loss: 0.00000926
Iteration 21/1000 | Loss: 0.00000926
Iteration 22/1000 | Loss: 0.00000925
Iteration 23/1000 | Loss: 0.00000925
Iteration 24/1000 | Loss: 0.00000924
Iteration 25/1000 | Loss: 0.00000924
Iteration 26/1000 | Loss: 0.00000922
Iteration 27/1000 | Loss: 0.00000922
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000921
Iteration 30/1000 | Loss: 0.00000921
Iteration 31/1000 | Loss: 0.00000921
Iteration 32/1000 | Loss: 0.00000921
Iteration 33/1000 | Loss: 0.00000921
Iteration 34/1000 | Loss: 0.00000921
Iteration 35/1000 | Loss: 0.00000921
Iteration 36/1000 | Loss: 0.00000921
Iteration 37/1000 | Loss: 0.00000920
Iteration 38/1000 | Loss: 0.00000920
Iteration 39/1000 | Loss: 0.00000920
Iteration 40/1000 | Loss: 0.00000919
Iteration 41/1000 | Loss: 0.00000919
Iteration 42/1000 | Loss: 0.00000919
Iteration 43/1000 | Loss: 0.00000918
Iteration 44/1000 | Loss: 0.00000917
Iteration 45/1000 | Loss: 0.00000917
Iteration 46/1000 | Loss: 0.00000917
Iteration 47/1000 | Loss: 0.00000917
Iteration 48/1000 | Loss: 0.00000917
Iteration 49/1000 | Loss: 0.00000917
Iteration 50/1000 | Loss: 0.00000917
Iteration 51/1000 | Loss: 0.00000917
Iteration 52/1000 | Loss: 0.00000917
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000917
Iteration 55/1000 | Loss: 0.00000917
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000917
Iteration 58/1000 | Loss: 0.00000917
Iteration 59/1000 | Loss: 0.00000917
Iteration 60/1000 | Loss: 0.00000917
Iteration 61/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [9.167472853732761e-06, 9.167472853732761e-06, 9.167472853732761e-06, 9.167472853732761e-06, 9.167472853732761e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.167472853732761e-06

Optimization complete. Final v2v error: 2.6128933429718018 mm

Highest mean error: 2.9645543098449707 mm for frame 81

Lowest mean error: 2.4892032146453857 mm for frame 194

Saving results

Total time: 28.159674644470215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051798
Iteration 2/25 | Loss: 0.01051798
Iteration 3/25 | Loss: 0.01051798
Iteration 4/25 | Loss: 0.00139473
Iteration 5/25 | Loss: 0.00115517
Iteration 6/25 | Loss: 0.00112929
Iteration 7/25 | Loss: 0.00112330
Iteration 8/25 | Loss: 0.00112676
Iteration 9/25 | Loss: 0.00112272
Iteration 10/25 | Loss: 0.00111929
Iteration 11/25 | Loss: 0.00111863
Iteration 12/25 | Loss: 0.00111856
Iteration 13/25 | Loss: 0.00111856
Iteration 14/25 | Loss: 0.00111856
Iteration 15/25 | Loss: 0.00111855
Iteration 16/25 | Loss: 0.00111855
Iteration 17/25 | Loss: 0.00111855
Iteration 18/25 | Loss: 0.00111855
Iteration 19/25 | Loss: 0.00111855
Iteration 20/25 | Loss: 0.00111855
Iteration 21/25 | Loss: 0.00111855
Iteration 22/25 | Loss: 0.00111855
Iteration 23/25 | Loss: 0.00111855
Iteration 24/25 | Loss: 0.00111855
Iteration 25/25 | Loss: 0.00111855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42801344
Iteration 2/25 | Loss: 0.00065587
Iteration 3/25 | Loss: 0.00065587
Iteration 4/25 | Loss: 0.00065586
Iteration 5/25 | Loss: 0.00065586
Iteration 6/25 | Loss: 0.00065586
Iteration 7/25 | Loss: 0.00065586
Iteration 8/25 | Loss: 0.00065586
Iteration 9/25 | Loss: 0.00065586
Iteration 10/25 | Loss: 0.00065586
Iteration 11/25 | Loss: 0.00065586
Iteration 12/25 | Loss: 0.00065586
Iteration 13/25 | Loss: 0.00065586
Iteration 14/25 | Loss: 0.00065586
Iteration 15/25 | Loss: 0.00065586
Iteration 16/25 | Loss: 0.00065586
Iteration 17/25 | Loss: 0.00065586
Iteration 18/25 | Loss: 0.00065586
Iteration 19/25 | Loss: 0.00065586
Iteration 20/25 | Loss: 0.00065586
Iteration 21/25 | Loss: 0.00065586
Iteration 22/25 | Loss: 0.00065586
Iteration 23/25 | Loss: 0.00065586
Iteration 24/25 | Loss: 0.00065586
Iteration 25/25 | Loss: 0.00065586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065586
Iteration 2/1000 | Loss: 0.00002455
Iteration 3/1000 | Loss: 0.00001996
Iteration 4/1000 | Loss: 0.00001882
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001669
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001668
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001658
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001654
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001649
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001645
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.64484645210905e-05, 1.64484645210905e-05, 1.64484645210905e-05, 1.64484645210905e-05, 1.64484645210905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.64484645210905e-05

Optimization complete. Final v2v error: 3.396026849746704 mm

Highest mean error: 3.960702657699585 mm for frame 15

Lowest mean error: 3.021127223968506 mm for frame 151

Saving results

Total time: 42.25679421424866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044681
Iteration 2/25 | Loss: 0.00159049
Iteration 3/25 | Loss: 0.00129164
Iteration 4/25 | Loss: 0.00125686
Iteration 5/25 | Loss: 0.00125032
Iteration 6/25 | Loss: 0.00124970
Iteration 7/25 | Loss: 0.00124970
Iteration 8/25 | Loss: 0.00124970
Iteration 9/25 | Loss: 0.00124970
Iteration 10/25 | Loss: 0.00124970
Iteration 11/25 | Loss: 0.00124970
Iteration 12/25 | Loss: 0.00124970
Iteration 13/25 | Loss: 0.00124970
Iteration 14/25 | Loss: 0.00124970
Iteration 15/25 | Loss: 0.00124970
Iteration 16/25 | Loss: 0.00124970
Iteration 17/25 | Loss: 0.00124970
Iteration 18/25 | Loss: 0.00124970
Iteration 19/25 | Loss: 0.00124970
Iteration 20/25 | Loss: 0.00124970
Iteration 21/25 | Loss: 0.00124970
Iteration 22/25 | Loss: 0.00124970
Iteration 23/25 | Loss: 0.00124970
Iteration 24/25 | Loss: 0.00124970
Iteration 25/25 | Loss: 0.00124970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51143765
Iteration 2/25 | Loss: 0.00071752
Iteration 3/25 | Loss: 0.00071749
Iteration 4/25 | Loss: 0.00071749
Iteration 5/25 | Loss: 0.00071749
Iteration 6/25 | Loss: 0.00071749
Iteration 7/25 | Loss: 0.00071749
Iteration 8/25 | Loss: 0.00071749
Iteration 9/25 | Loss: 0.00071749
Iteration 10/25 | Loss: 0.00071749
Iteration 11/25 | Loss: 0.00071749
Iteration 12/25 | Loss: 0.00071749
Iteration 13/25 | Loss: 0.00071749
Iteration 14/25 | Loss: 0.00071749
Iteration 15/25 | Loss: 0.00071749
Iteration 16/25 | Loss: 0.00071749
Iteration 17/25 | Loss: 0.00071749
Iteration 18/25 | Loss: 0.00071749
Iteration 19/25 | Loss: 0.00071749
Iteration 20/25 | Loss: 0.00071749
Iteration 21/25 | Loss: 0.00071749
Iteration 22/25 | Loss: 0.00071749
Iteration 23/25 | Loss: 0.00071749
Iteration 24/25 | Loss: 0.00071749
Iteration 25/25 | Loss: 0.00071749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071749
Iteration 2/1000 | Loss: 0.00007188
Iteration 3/1000 | Loss: 0.00004245
Iteration 4/1000 | Loss: 0.00003389
Iteration 5/1000 | Loss: 0.00003229
Iteration 6/1000 | Loss: 0.00003091
Iteration 7/1000 | Loss: 0.00003013
Iteration 8/1000 | Loss: 0.00002962
Iteration 9/1000 | Loss: 0.00002925
Iteration 10/1000 | Loss: 0.00002892
Iteration 11/1000 | Loss: 0.00002874
Iteration 12/1000 | Loss: 0.00002857
Iteration 13/1000 | Loss: 0.00002846
Iteration 14/1000 | Loss: 0.00002839
Iteration 15/1000 | Loss: 0.00002833
Iteration 16/1000 | Loss: 0.00002825
Iteration 17/1000 | Loss: 0.00002823
Iteration 18/1000 | Loss: 0.00002819
Iteration 19/1000 | Loss: 0.00002818
Iteration 20/1000 | Loss: 0.00002818
Iteration 21/1000 | Loss: 0.00002817
Iteration 22/1000 | Loss: 0.00002817
Iteration 23/1000 | Loss: 0.00002817
Iteration 24/1000 | Loss: 0.00002815
Iteration 25/1000 | Loss: 0.00002814
Iteration 26/1000 | Loss: 0.00002814
Iteration 27/1000 | Loss: 0.00002814
Iteration 28/1000 | Loss: 0.00002814
Iteration 29/1000 | Loss: 0.00002813
Iteration 30/1000 | Loss: 0.00002813
Iteration 31/1000 | Loss: 0.00002812
Iteration 32/1000 | Loss: 0.00002812
Iteration 33/1000 | Loss: 0.00002812
Iteration 34/1000 | Loss: 0.00002812
Iteration 35/1000 | Loss: 0.00002811
Iteration 36/1000 | Loss: 0.00002811
Iteration 37/1000 | Loss: 0.00002811
Iteration 38/1000 | Loss: 0.00002811
Iteration 39/1000 | Loss: 0.00002811
Iteration 40/1000 | Loss: 0.00002811
Iteration 41/1000 | Loss: 0.00002810
Iteration 42/1000 | Loss: 0.00002810
Iteration 43/1000 | Loss: 0.00002810
Iteration 44/1000 | Loss: 0.00002810
Iteration 45/1000 | Loss: 0.00002810
Iteration 46/1000 | Loss: 0.00002809
Iteration 47/1000 | Loss: 0.00002809
Iteration 48/1000 | Loss: 0.00002809
Iteration 49/1000 | Loss: 0.00002809
Iteration 50/1000 | Loss: 0.00002809
Iteration 51/1000 | Loss: 0.00002809
Iteration 52/1000 | Loss: 0.00002808
Iteration 53/1000 | Loss: 0.00002808
Iteration 54/1000 | Loss: 0.00002808
Iteration 55/1000 | Loss: 0.00002808
Iteration 56/1000 | Loss: 0.00002808
Iteration 57/1000 | Loss: 0.00002808
Iteration 58/1000 | Loss: 0.00002808
Iteration 59/1000 | Loss: 0.00002808
Iteration 60/1000 | Loss: 0.00002808
Iteration 61/1000 | Loss: 0.00002808
Iteration 62/1000 | Loss: 0.00002807
Iteration 63/1000 | Loss: 0.00002807
Iteration 64/1000 | Loss: 0.00002807
Iteration 65/1000 | Loss: 0.00002807
Iteration 66/1000 | Loss: 0.00002807
Iteration 67/1000 | Loss: 0.00002807
Iteration 68/1000 | Loss: 0.00002807
Iteration 69/1000 | Loss: 0.00002807
Iteration 70/1000 | Loss: 0.00002807
Iteration 71/1000 | Loss: 0.00002807
Iteration 72/1000 | Loss: 0.00002807
Iteration 73/1000 | Loss: 0.00002806
Iteration 74/1000 | Loss: 0.00002806
Iteration 75/1000 | Loss: 0.00002806
Iteration 76/1000 | Loss: 0.00002806
Iteration 77/1000 | Loss: 0.00002806
Iteration 78/1000 | Loss: 0.00002806
Iteration 79/1000 | Loss: 0.00002805
Iteration 80/1000 | Loss: 0.00002805
Iteration 81/1000 | Loss: 0.00002805
Iteration 82/1000 | Loss: 0.00002805
Iteration 83/1000 | Loss: 0.00002805
Iteration 84/1000 | Loss: 0.00002805
Iteration 85/1000 | Loss: 0.00002805
Iteration 86/1000 | Loss: 0.00002804
Iteration 87/1000 | Loss: 0.00002804
Iteration 88/1000 | Loss: 0.00002804
Iteration 89/1000 | Loss: 0.00002804
Iteration 90/1000 | Loss: 0.00002804
Iteration 91/1000 | Loss: 0.00002804
Iteration 92/1000 | Loss: 0.00002804
Iteration 93/1000 | Loss: 0.00002804
Iteration 94/1000 | Loss: 0.00002804
Iteration 95/1000 | Loss: 0.00002804
Iteration 96/1000 | Loss: 0.00002804
Iteration 97/1000 | Loss: 0.00002804
Iteration 98/1000 | Loss: 0.00002804
Iteration 99/1000 | Loss: 0.00002804
Iteration 100/1000 | Loss: 0.00002804
Iteration 101/1000 | Loss: 0.00002804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.803816823870875e-05, 2.803816823870875e-05, 2.803816823870875e-05, 2.803816823870875e-05, 2.803816823870875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.803816823870875e-05

Optimization complete. Final v2v error: 4.381422996520996 mm

Highest mean error: 5.352334976196289 mm for frame 194

Lowest mean error: 3.8236441612243652 mm for frame 65

Saving results

Total time: 38.733343839645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781265
Iteration 2/25 | Loss: 0.00143887
Iteration 3/25 | Loss: 0.00117102
Iteration 4/25 | Loss: 0.00113326
Iteration 5/25 | Loss: 0.00112715
Iteration 6/25 | Loss: 0.00112615
Iteration 7/25 | Loss: 0.00112615
Iteration 8/25 | Loss: 0.00112615
Iteration 9/25 | Loss: 0.00112615
Iteration 10/25 | Loss: 0.00112615
Iteration 11/25 | Loss: 0.00112615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011261515319347382, 0.0011261515319347382, 0.0011261515319347382, 0.0011261515319347382, 0.0011261515319347382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011261515319347382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25664413
Iteration 2/25 | Loss: 0.00052495
Iteration 3/25 | Loss: 0.00052493
Iteration 4/25 | Loss: 0.00052493
Iteration 5/25 | Loss: 0.00052493
Iteration 6/25 | Loss: 0.00052493
Iteration 7/25 | Loss: 0.00052493
Iteration 8/25 | Loss: 0.00052493
Iteration 9/25 | Loss: 0.00052493
Iteration 10/25 | Loss: 0.00052493
Iteration 11/25 | Loss: 0.00052493
Iteration 12/25 | Loss: 0.00052493
Iteration 13/25 | Loss: 0.00052493
Iteration 14/25 | Loss: 0.00052493
Iteration 15/25 | Loss: 0.00052493
Iteration 16/25 | Loss: 0.00052493
Iteration 17/25 | Loss: 0.00052493
Iteration 18/25 | Loss: 0.00052493
Iteration 19/25 | Loss: 0.00052493
Iteration 20/25 | Loss: 0.00052493
Iteration 21/25 | Loss: 0.00052493
Iteration 22/25 | Loss: 0.00052493
Iteration 23/25 | Loss: 0.00052493
Iteration 24/25 | Loss: 0.00052493
Iteration 25/25 | Loss: 0.00052493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052493
Iteration 2/1000 | Loss: 0.00003795
Iteration 3/1000 | Loss: 0.00002983
Iteration 4/1000 | Loss: 0.00002603
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002329
Iteration 7/1000 | Loss: 0.00002246
Iteration 8/1000 | Loss: 0.00002196
Iteration 9/1000 | Loss: 0.00002167
Iteration 10/1000 | Loss: 0.00002132
Iteration 11/1000 | Loss: 0.00002105
Iteration 12/1000 | Loss: 0.00002088
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002071
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002051
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002021
Iteration 29/1000 | Loss: 0.00002021
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002020
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002020
Iteration 36/1000 | Loss: 0.00002020
Iteration 37/1000 | Loss: 0.00002020
Iteration 38/1000 | Loss: 0.00002020
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002016
Iteration 41/1000 | Loss: 0.00002016
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002015
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002012
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002011
Iteration 58/1000 | Loss: 0.00002011
Iteration 59/1000 | Loss: 0.00002010
Iteration 60/1000 | Loss: 0.00002010
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002009
Iteration 64/1000 | Loss: 0.00002009
Iteration 65/1000 | Loss: 0.00002009
Iteration 66/1000 | Loss: 0.00002008
Iteration 67/1000 | Loss: 0.00002008
Iteration 68/1000 | Loss: 0.00002008
Iteration 69/1000 | Loss: 0.00002008
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002007
Iteration 74/1000 | Loss: 0.00002007
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002006
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002005
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002004
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002003
Iteration 104/1000 | Loss: 0.00002003
Iteration 105/1000 | Loss: 0.00002003
Iteration 106/1000 | Loss: 0.00002003
Iteration 107/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.002743713092059e-05, 2.002743713092059e-05, 2.002743713092059e-05, 2.002743713092059e-05, 2.002743713092059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.002743713092059e-05

Optimization complete. Final v2v error: 3.6904256343841553 mm

Highest mean error: 4.674067497253418 mm for frame 137

Lowest mean error: 3.1293632984161377 mm for frame 58

Saving results

Total time: 41.54429292678833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006459
Iteration 2/25 | Loss: 0.01006459
Iteration 3/25 | Loss: 0.00260431
Iteration 4/25 | Loss: 0.00189571
Iteration 5/25 | Loss: 0.00173730
Iteration 6/25 | Loss: 0.00174739
Iteration 7/25 | Loss: 0.00173124
Iteration 8/25 | Loss: 0.00160735
Iteration 9/25 | Loss: 0.00149641
Iteration 10/25 | Loss: 0.00148845
Iteration 11/25 | Loss: 0.00148377
Iteration 12/25 | Loss: 0.00145646
Iteration 13/25 | Loss: 0.00141009
Iteration 14/25 | Loss: 0.00135746
Iteration 15/25 | Loss: 0.00132974
Iteration 16/25 | Loss: 0.00131209
Iteration 17/25 | Loss: 0.00129654
Iteration 18/25 | Loss: 0.00128887
Iteration 19/25 | Loss: 0.00129119
Iteration 20/25 | Loss: 0.00129137
Iteration 21/25 | Loss: 0.00128189
Iteration 22/25 | Loss: 0.00128010
Iteration 23/25 | Loss: 0.00128110
Iteration 24/25 | Loss: 0.00127635
Iteration 25/25 | Loss: 0.00127187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36838365
Iteration 2/25 | Loss: 0.00185956
Iteration 3/25 | Loss: 0.00185462
Iteration 4/25 | Loss: 0.00185462
Iteration 5/25 | Loss: 0.00185462
Iteration 6/25 | Loss: 0.00185462
Iteration 7/25 | Loss: 0.00185462
Iteration 8/25 | Loss: 0.00185462
Iteration 9/25 | Loss: 0.00185462
Iteration 10/25 | Loss: 0.00185462
Iteration 11/25 | Loss: 0.00185462
Iteration 12/25 | Loss: 0.00185462
Iteration 13/25 | Loss: 0.00185462
Iteration 14/25 | Loss: 0.00185462
Iteration 15/25 | Loss: 0.00185462
Iteration 16/25 | Loss: 0.00185462
Iteration 17/25 | Loss: 0.00185462
Iteration 18/25 | Loss: 0.00185462
Iteration 19/25 | Loss: 0.00185462
Iteration 20/25 | Loss: 0.00185462
Iteration 21/25 | Loss: 0.00185462
Iteration 22/25 | Loss: 0.00185462
Iteration 23/25 | Loss: 0.00185462
Iteration 24/25 | Loss: 0.00185462
Iteration 25/25 | Loss: 0.00185462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185462
Iteration 2/1000 | Loss: 0.00087231
Iteration 3/1000 | Loss: 0.00086637
Iteration 4/1000 | Loss: 0.00085987
Iteration 5/1000 | Loss: 0.00076888
Iteration 6/1000 | Loss: 0.00097343
Iteration 7/1000 | Loss: 0.00070269
Iteration 8/1000 | Loss: 0.00027606
Iteration 9/1000 | Loss: 0.00074223
Iteration 10/1000 | Loss: 0.00030938
Iteration 11/1000 | Loss: 0.00053819
Iteration 12/1000 | Loss: 0.00046313
Iteration 13/1000 | Loss: 0.00039630
Iteration 14/1000 | Loss: 0.00039814
Iteration 15/1000 | Loss: 0.00018481
Iteration 16/1000 | Loss: 0.00064873
Iteration 17/1000 | Loss: 0.00019664
Iteration 18/1000 | Loss: 0.00034614
Iteration 19/1000 | Loss: 0.00028084
Iteration 20/1000 | Loss: 0.00019165
Iteration 21/1000 | Loss: 0.00047642
Iteration 22/1000 | Loss: 0.00301219
Iteration 23/1000 | Loss: 0.00382894
Iteration 24/1000 | Loss: 0.00222956
Iteration 25/1000 | Loss: 0.00062136
Iteration 26/1000 | Loss: 0.00083972
Iteration 27/1000 | Loss: 0.00023477
Iteration 28/1000 | Loss: 0.00077520
Iteration 29/1000 | Loss: 0.00048763
Iteration 30/1000 | Loss: 0.00058374
Iteration 31/1000 | Loss: 0.00105550
Iteration 32/1000 | Loss: 0.00075140
Iteration 33/1000 | Loss: 0.00046580
Iteration 34/1000 | Loss: 0.00014606
Iteration 35/1000 | Loss: 0.00023655
Iteration 36/1000 | Loss: 0.00018078
Iteration 37/1000 | Loss: 0.00012364
Iteration 38/1000 | Loss: 0.00069359
Iteration 39/1000 | Loss: 0.00022944
Iteration 40/1000 | Loss: 0.00019827
Iteration 41/1000 | Loss: 0.00119974
Iteration 42/1000 | Loss: 0.00179190
Iteration 43/1000 | Loss: 0.00062278
Iteration 44/1000 | Loss: 0.00009620
Iteration 45/1000 | Loss: 0.00007135
Iteration 46/1000 | Loss: 0.00040595
Iteration 47/1000 | Loss: 0.00039532
Iteration 48/1000 | Loss: 0.00013550
Iteration 49/1000 | Loss: 0.00023672
Iteration 50/1000 | Loss: 0.00034572
Iteration 51/1000 | Loss: 0.00066273
Iteration 52/1000 | Loss: 0.00054977
Iteration 53/1000 | Loss: 0.00016843
Iteration 54/1000 | Loss: 0.00025517
Iteration 55/1000 | Loss: 0.00024703
Iteration 56/1000 | Loss: 0.00022428
Iteration 57/1000 | Loss: 0.00015482
Iteration 58/1000 | Loss: 0.00013124
Iteration 59/1000 | Loss: 0.00089913
Iteration 60/1000 | Loss: 0.00027802
Iteration 61/1000 | Loss: 0.00004705
Iteration 62/1000 | Loss: 0.00005018
Iteration 63/1000 | Loss: 0.00013546
Iteration 64/1000 | Loss: 0.00025581
Iteration 65/1000 | Loss: 0.00020781
Iteration 66/1000 | Loss: 0.00021609
Iteration 67/1000 | Loss: 0.00017194
Iteration 68/1000 | Loss: 0.00019050
Iteration 69/1000 | Loss: 0.00014271
Iteration 70/1000 | Loss: 0.00015990
Iteration 71/1000 | Loss: 0.00014570
Iteration 72/1000 | Loss: 0.00015948
Iteration 73/1000 | Loss: 0.00005021
Iteration 74/1000 | Loss: 0.00003339
Iteration 75/1000 | Loss: 0.00004026
Iteration 76/1000 | Loss: 0.00003613
Iteration 77/1000 | Loss: 0.00003093
Iteration 78/1000 | Loss: 0.00003074
Iteration 79/1000 | Loss: 0.00027730
Iteration 80/1000 | Loss: 0.00005657
Iteration 81/1000 | Loss: 0.00003998
Iteration 82/1000 | Loss: 0.00003626
Iteration 83/1000 | Loss: 0.00003536
Iteration 84/1000 | Loss: 0.00003930
Iteration 85/1000 | Loss: 0.00005663
Iteration 86/1000 | Loss: 0.00003117
Iteration 87/1000 | Loss: 0.00005292
Iteration 88/1000 | Loss: 0.00002705
Iteration 89/1000 | Loss: 0.00002514
Iteration 90/1000 | Loss: 0.00018508
Iteration 91/1000 | Loss: 0.00011376
Iteration 92/1000 | Loss: 0.00016289
Iteration 93/1000 | Loss: 0.00003459
Iteration 94/1000 | Loss: 0.00002834
Iteration 95/1000 | Loss: 0.00002291
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00003399
Iteration 98/1000 | Loss: 0.00002176
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002423
Iteration 101/1000 | Loss: 0.00001974
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00002188
Iteration 104/1000 | Loss: 0.00001943
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00005027
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00003577
Iteration 110/1000 | Loss: 0.00001843
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001829
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001815
Iteration 118/1000 | Loss: 0.00001813
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00002510
Iteration 121/1000 | Loss: 0.00001806
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001799
Iteration 126/1000 | Loss: 0.00001799
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001986
Iteration 135/1000 | Loss: 0.00001802
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001788
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001783
Iteration 144/1000 | Loss: 0.00001782
Iteration 145/1000 | Loss: 0.00001782
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001781
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001780
Iteration 168/1000 | Loss: 0.00001780
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001780
Iteration 171/1000 | Loss: 0.00001780
Iteration 172/1000 | Loss: 0.00001780
Iteration 173/1000 | Loss: 0.00001780
Iteration 174/1000 | Loss: 0.00001780
Iteration 175/1000 | Loss: 0.00001780
Iteration 176/1000 | Loss: 0.00001780
Iteration 177/1000 | Loss: 0.00001780
Iteration 178/1000 | Loss: 0.00001780
Iteration 179/1000 | Loss: 0.00001779
Iteration 180/1000 | Loss: 0.00001779
Iteration 181/1000 | Loss: 0.00001779
Iteration 182/1000 | Loss: 0.00001779
Iteration 183/1000 | Loss: 0.00001779
Iteration 184/1000 | Loss: 0.00001779
Iteration 185/1000 | Loss: 0.00001779
Iteration 186/1000 | Loss: 0.00001779
Iteration 187/1000 | Loss: 0.00001779
Iteration 188/1000 | Loss: 0.00001779
Iteration 189/1000 | Loss: 0.00001779
Iteration 190/1000 | Loss: 0.00001779
Iteration 191/1000 | Loss: 0.00001779
Iteration 192/1000 | Loss: 0.00001779
Iteration 193/1000 | Loss: 0.00001779
Iteration 194/1000 | Loss: 0.00001778
Iteration 195/1000 | Loss: 0.00001778
Iteration 196/1000 | Loss: 0.00001778
Iteration 197/1000 | Loss: 0.00001778
Iteration 198/1000 | Loss: 0.00001778
Iteration 199/1000 | Loss: 0.00001778
Iteration 200/1000 | Loss: 0.00001778
Iteration 201/1000 | Loss: 0.00001778
Iteration 202/1000 | Loss: 0.00001778
Iteration 203/1000 | Loss: 0.00001778
Iteration 204/1000 | Loss: 0.00001777
Iteration 205/1000 | Loss: 0.00001777
Iteration 206/1000 | Loss: 0.00001777
Iteration 207/1000 | Loss: 0.00001777
Iteration 208/1000 | Loss: 0.00001777
Iteration 209/1000 | Loss: 0.00001777
Iteration 210/1000 | Loss: 0.00001777
Iteration 211/1000 | Loss: 0.00001777
Iteration 212/1000 | Loss: 0.00001777
Iteration 213/1000 | Loss: 0.00001777
Iteration 214/1000 | Loss: 0.00001777
Iteration 215/1000 | Loss: 0.00001777
Iteration 216/1000 | Loss: 0.00001777
Iteration 217/1000 | Loss: 0.00001776
Iteration 218/1000 | Loss: 0.00001776
Iteration 219/1000 | Loss: 0.00001776
Iteration 220/1000 | Loss: 0.00001776
Iteration 221/1000 | Loss: 0.00001776
Iteration 222/1000 | Loss: 0.00001776
Iteration 223/1000 | Loss: 0.00001776
Iteration 224/1000 | Loss: 0.00001776
Iteration 225/1000 | Loss: 0.00001776
Iteration 226/1000 | Loss: 0.00001776
Iteration 227/1000 | Loss: 0.00001776
Iteration 228/1000 | Loss: 0.00001776
Iteration 229/1000 | Loss: 0.00001776
Iteration 230/1000 | Loss: 0.00001776
Iteration 231/1000 | Loss: 0.00001776
Iteration 232/1000 | Loss: 0.00001776
Iteration 233/1000 | Loss: 0.00001776
Iteration 234/1000 | Loss: 0.00001775
Iteration 235/1000 | Loss: 0.00001775
Iteration 236/1000 | Loss: 0.00001775
Iteration 237/1000 | Loss: 0.00001775
Iteration 238/1000 | Loss: 0.00001775
Iteration 239/1000 | Loss: 0.00001775
Iteration 240/1000 | Loss: 0.00001775
Iteration 241/1000 | Loss: 0.00001775
Iteration 242/1000 | Loss: 0.00002342
Iteration 243/1000 | Loss: 0.00001779
Iteration 244/1000 | Loss: 0.00001776
Iteration 245/1000 | Loss: 0.00001776
Iteration 246/1000 | Loss: 0.00001776
Iteration 247/1000 | Loss: 0.00001776
Iteration 248/1000 | Loss: 0.00001776
Iteration 249/1000 | Loss: 0.00001775
Iteration 250/1000 | Loss: 0.00001775
Iteration 251/1000 | Loss: 0.00001775
Iteration 252/1000 | Loss: 0.00001775
Iteration 253/1000 | Loss: 0.00001775
Iteration 254/1000 | Loss: 0.00001775
Iteration 255/1000 | Loss: 0.00001775
Iteration 256/1000 | Loss: 0.00001774
Iteration 257/1000 | Loss: 0.00001774
Iteration 258/1000 | Loss: 0.00001774
Iteration 259/1000 | Loss: 0.00001774
Iteration 260/1000 | Loss: 0.00001774
Iteration 261/1000 | Loss: 0.00001774
Iteration 262/1000 | Loss: 0.00001774
Iteration 263/1000 | Loss: 0.00001774
Iteration 264/1000 | Loss: 0.00001774
Iteration 265/1000 | Loss: 0.00001774
Iteration 266/1000 | Loss: 0.00001774
Iteration 267/1000 | Loss: 0.00001774
Iteration 268/1000 | Loss: 0.00001774
Iteration 269/1000 | Loss: 0.00001774
Iteration 270/1000 | Loss: 0.00001774
Iteration 271/1000 | Loss: 0.00001774
Iteration 272/1000 | Loss: 0.00001774
Iteration 273/1000 | Loss: 0.00001774
Iteration 274/1000 | Loss: 0.00001774
Iteration 275/1000 | Loss: 0.00001774
Iteration 276/1000 | Loss: 0.00001774
Iteration 277/1000 | Loss: 0.00001774
Iteration 278/1000 | Loss: 0.00001774
Iteration 279/1000 | Loss: 0.00001774
Iteration 280/1000 | Loss: 0.00001774
Iteration 281/1000 | Loss: 0.00001774
Iteration 282/1000 | Loss: 0.00001774
Iteration 283/1000 | Loss: 0.00001774
Iteration 284/1000 | Loss: 0.00001774
Iteration 285/1000 | Loss: 0.00001774
Iteration 286/1000 | Loss: 0.00001774
Iteration 287/1000 | Loss: 0.00001774
Iteration 288/1000 | Loss: 0.00001774
Iteration 289/1000 | Loss: 0.00001774
Iteration 290/1000 | Loss: 0.00001774
Iteration 291/1000 | Loss: 0.00001774
Iteration 292/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.774012707755901e-05, 1.774012707755901e-05, 1.774012707755901e-05, 1.774012707755901e-05, 1.774012707755901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.774012707755901e-05

Optimization complete. Final v2v error: 3.11137318611145 mm

Highest mean error: 11.068048477172852 mm for frame 35

Lowest mean error: 2.771899938583374 mm for frame 173

Saving results

Total time: 236.88731026649475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859321
Iteration 2/25 | Loss: 0.00122252
Iteration 3/25 | Loss: 0.00111503
Iteration 4/25 | Loss: 0.00109727
Iteration 5/25 | Loss: 0.00109154
Iteration 6/25 | Loss: 0.00109081
Iteration 7/25 | Loss: 0.00109081
Iteration 8/25 | Loss: 0.00109081
Iteration 9/25 | Loss: 0.00109081
Iteration 10/25 | Loss: 0.00109081
Iteration 11/25 | Loss: 0.00109081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010908148251473904, 0.0010908148251473904, 0.0010908148251473904, 0.0010908148251473904, 0.0010908148251473904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010908148251473904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58665359
Iteration 2/25 | Loss: 0.00060166
Iteration 3/25 | Loss: 0.00060165
Iteration 4/25 | Loss: 0.00060165
Iteration 5/25 | Loss: 0.00060165
Iteration 6/25 | Loss: 0.00060165
Iteration 7/25 | Loss: 0.00060165
Iteration 8/25 | Loss: 0.00060165
Iteration 9/25 | Loss: 0.00060165
Iteration 10/25 | Loss: 0.00060165
Iteration 11/25 | Loss: 0.00060165
Iteration 12/25 | Loss: 0.00060165
Iteration 13/25 | Loss: 0.00060165
Iteration 14/25 | Loss: 0.00060165
Iteration 15/25 | Loss: 0.00060165
Iteration 16/25 | Loss: 0.00060165
Iteration 17/25 | Loss: 0.00060165
Iteration 18/25 | Loss: 0.00060165
Iteration 19/25 | Loss: 0.00060165
Iteration 20/25 | Loss: 0.00060165
Iteration 21/25 | Loss: 0.00060165
Iteration 22/25 | Loss: 0.00060165
Iteration 23/25 | Loss: 0.00060165
Iteration 24/25 | Loss: 0.00060165
Iteration 25/25 | Loss: 0.00060165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060165
Iteration 2/1000 | Loss: 0.00002680
Iteration 3/1000 | Loss: 0.00001661
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001364
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001309
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001309
Iteration 23/1000 | Loss: 0.00001309
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001305
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001300
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001300
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001296
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001276
Iteration 125/1000 | Loss: 0.00001276
Iteration 126/1000 | Loss: 0.00001276
Iteration 127/1000 | Loss: 0.00001275
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001275
Iteration 130/1000 | Loss: 0.00001275
Iteration 131/1000 | Loss: 0.00001275
Iteration 132/1000 | Loss: 0.00001275
Iteration 133/1000 | Loss: 0.00001275
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001271
Iteration 145/1000 | Loss: 0.00001271
Iteration 146/1000 | Loss: 0.00001271
Iteration 147/1000 | Loss: 0.00001271
Iteration 148/1000 | Loss: 0.00001271
Iteration 149/1000 | Loss: 0.00001271
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001270
Iteration 154/1000 | Loss: 0.00001270
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001269
Iteration 157/1000 | Loss: 0.00001269
Iteration 158/1000 | Loss: 0.00001269
Iteration 159/1000 | Loss: 0.00001269
Iteration 160/1000 | Loss: 0.00001269
Iteration 161/1000 | Loss: 0.00001269
Iteration 162/1000 | Loss: 0.00001269
Iteration 163/1000 | Loss: 0.00001269
Iteration 164/1000 | Loss: 0.00001268
Iteration 165/1000 | Loss: 0.00001268
Iteration 166/1000 | Loss: 0.00001268
Iteration 167/1000 | Loss: 0.00001268
Iteration 168/1000 | Loss: 0.00001268
Iteration 169/1000 | Loss: 0.00001268
Iteration 170/1000 | Loss: 0.00001268
Iteration 171/1000 | Loss: 0.00001268
Iteration 172/1000 | Loss: 0.00001268
Iteration 173/1000 | Loss: 0.00001268
Iteration 174/1000 | Loss: 0.00001268
Iteration 175/1000 | Loss: 0.00001267
Iteration 176/1000 | Loss: 0.00001267
Iteration 177/1000 | Loss: 0.00001267
Iteration 178/1000 | Loss: 0.00001267
Iteration 179/1000 | Loss: 0.00001267
Iteration 180/1000 | Loss: 0.00001267
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001267
Iteration 183/1000 | Loss: 0.00001267
Iteration 184/1000 | Loss: 0.00001266
Iteration 185/1000 | Loss: 0.00001266
Iteration 186/1000 | Loss: 0.00001266
Iteration 187/1000 | Loss: 0.00001266
Iteration 188/1000 | Loss: 0.00001266
Iteration 189/1000 | Loss: 0.00001266
Iteration 190/1000 | Loss: 0.00001266
Iteration 191/1000 | Loss: 0.00001265
Iteration 192/1000 | Loss: 0.00001265
Iteration 193/1000 | Loss: 0.00001265
Iteration 194/1000 | Loss: 0.00001265
Iteration 195/1000 | Loss: 0.00001265
Iteration 196/1000 | Loss: 0.00001265
Iteration 197/1000 | Loss: 0.00001265
Iteration 198/1000 | Loss: 0.00001265
Iteration 199/1000 | Loss: 0.00001265
Iteration 200/1000 | Loss: 0.00001265
Iteration 201/1000 | Loss: 0.00001265
Iteration 202/1000 | Loss: 0.00001265
Iteration 203/1000 | Loss: 0.00001265
Iteration 204/1000 | Loss: 0.00001264
Iteration 205/1000 | Loss: 0.00001264
Iteration 206/1000 | Loss: 0.00001264
Iteration 207/1000 | Loss: 0.00001264
Iteration 208/1000 | Loss: 0.00001264
Iteration 209/1000 | Loss: 0.00001264
Iteration 210/1000 | Loss: 0.00001264
Iteration 211/1000 | Loss: 0.00001264
Iteration 212/1000 | Loss: 0.00001264
Iteration 213/1000 | Loss: 0.00001264
Iteration 214/1000 | Loss: 0.00001264
Iteration 215/1000 | Loss: 0.00001264
Iteration 216/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.2638356565730646e-05, 1.2638356565730646e-05, 1.2638356565730646e-05, 1.2638356565730646e-05, 1.2638356565730646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2638356565730646e-05

Optimization complete. Final v2v error: 3.025550127029419 mm

Highest mean error: 3.4239182472229004 mm for frame 232

Lowest mean error: 2.638089418411255 mm for frame 61

Saving results

Total time: 40.811715602874756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391684
Iteration 2/25 | Loss: 0.00107117
Iteration 3/25 | Loss: 0.00102182
Iteration 4/25 | Loss: 0.00101683
Iteration 5/25 | Loss: 0.00101555
Iteration 6/25 | Loss: 0.00101555
Iteration 7/25 | Loss: 0.00101555
Iteration 8/25 | Loss: 0.00101555
Iteration 9/25 | Loss: 0.00101555
Iteration 10/25 | Loss: 0.00101555
Iteration 11/25 | Loss: 0.00101555
Iteration 12/25 | Loss: 0.00101555
Iteration 13/25 | Loss: 0.00101555
Iteration 14/25 | Loss: 0.00101555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010155546478927135, 0.0010155546478927135, 0.0010155546478927135, 0.0010155546478927135, 0.0010155546478927135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010155546478927135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04745293
Iteration 2/25 | Loss: 0.00065663
Iteration 3/25 | Loss: 0.00065662
Iteration 4/25 | Loss: 0.00065662
Iteration 5/25 | Loss: 0.00065662
Iteration 6/25 | Loss: 0.00065662
Iteration 7/25 | Loss: 0.00065662
Iteration 8/25 | Loss: 0.00065662
Iteration 9/25 | Loss: 0.00065662
Iteration 10/25 | Loss: 0.00065662
Iteration 11/25 | Loss: 0.00065662
Iteration 12/25 | Loss: 0.00065662
Iteration 13/25 | Loss: 0.00065662
Iteration 14/25 | Loss: 0.00065662
Iteration 15/25 | Loss: 0.00065662
Iteration 16/25 | Loss: 0.00065661
Iteration 17/25 | Loss: 0.00065661
Iteration 18/25 | Loss: 0.00065661
Iteration 19/25 | Loss: 0.00065661
Iteration 20/25 | Loss: 0.00065661
Iteration 21/25 | Loss: 0.00065661
Iteration 22/25 | Loss: 0.00065661
Iteration 23/25 | Loss: 0.00065661
Iteration 24/25 | Loss: 0.00065661
Iteration 25/25 | Loss: 0.00065661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065661
Iteration 2/1000 | Loss: 0.00001710
Iteration 3/1000 | Loss: 0.00001089
Iteration 4/1000 | Loss: 0.00000958
Iteration 5/1000 | Loss: 0.00000908
Iteration 6/1000 | Loss: 0.00000868
Iteration 7/1000 | Loss: 0.00000842
Iteration 8/1000 | Loss: 0.00000842
Iteration 9/1000 | Loss: 0.00000841
Iteration 10/1000 | Loss: 0.00000841
Iteration 11/1000 | Loss: 0.00000840
Iteration 12/1000 | Loss: 0.00000831
Iteration 13/1000 | Loss: 0.00000820
Iteration 14/1000 | Loss: 0.00000819
Iteration 15/1000 | Loss: 0.00000809
Iteration 16/1000 | Loss: 0.00000808
Iteration 17/1000 | Loss: 0.00000807
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000801
Iteration 20/1000 | Loss: 0.00000800
Iteration 21/1000 | Loss: 0.00000794
Iteration 22/1000 | Loss: 0.00000794
Iteration 23/1000 | Loss: 0.00000793
Iteration 24/1000 | Loss: 0.00000793
Iteration 25/1000 | Loss: 0.00000793
Iteration 26/1000 | Loss: 0.00000793
Iteration 27/1000 | Loss: 0.00000793
Iteration 28/1000 | Loss: 0.00000793
Iteration 29/1000 | Loss: 0.00000793
Iteration 30/1000 | Loss: 0.00000793
Iteration 31/1000 | Loss: 0.00000793
Iteration 32/1000 | Loss: 0.00000793
Iteration 33/1000 | Loss: 0.00000793
Iteration 34/1000 | Loss: 0.00000793
Iteration 35/1000 | Loss: 0.00000793
Iteration 36/1000 | Loss: 0.00000793
Iteration 37/1000 | Loss: 0.00000793
Iteration 38/1000 | Loss: 0.00000793
Iteration 39/1000 | Loss: 0.00000793
Iteration 40/1000 | Loss: 0.00000793
Iteration 41/1000 | Loss: 0.00000793
Iteration 42/1000 | Loss: 0.00000793
Iteration 43/1000 | Loss: 0.00000793
Iteration 44/1000 | Loss: 0.00000793
Iteration 45/1000 | Loss: 0.00000792
Iteration 46/1000 | Loss: 0.00000792
Iteration 47/1000 | Loss: 0.00000792
Iteration 48/1000 | Loss: 0.00000789
Iteration 49/1000 | Loss: 0.00000789
Iteration 50/1000 | Loss: 0.00000789
Iteration 51/1000 | Loss: 0.00000789
Iteration 52/1000 | Loss: 0.00000789
Iteration 53/1000 | Loss: 0.00000788
Iteration 54/1000 | Loss: 0.00000788
Iteration 55/1000 | Loss: 0.00000788
Iteration 56/1000 | Loss: 0.00000787
Iteration 57/1000 | Loss: 0.00000785
Iteration 58/1000 | Loss: 0.00000785
Iteration 59/1000 | Loss: 0.00000784
Iteration 60/1000 | Loss: 0.00000784
Iteration 61/1000 | Loss: 0.00000784
Iteration 62/1000 | Loss: 0.00000784
Iteration 63/1000 | Loss: 0.00000784
Iteration 64/1000 | Loss: 0.00000783
Iteration 65/1000 | Loss: 0.00000783
Iteration 66/1000 | Loss: 0.00000783
Iteration 67/1000 | Loss: 0.00000783
Iteration 68/1000 | Loss: 0.00000783
Iteration 69/1000 | Loss: 0.00000783
Iteration 70/1000 | Loss: 0.00000783
Iteration 71/1000 | Loss: 0.00000782
Iteration 72/1000 | Loss: 0.00000782
Iteration 73/1000 | Loss: 0.00000781
Iteration 74/1000 | Loss: 0.00000781
Iteration 75/1000 | Loss: 0.00000781
Iteration 76/1000 | Loss: 0.00000781
Iteration 77/1000 | Loss: 0.00000781
Iteration 78/1000 | Loss: 0.00000781
Iteration 79/1000 | Loss: 0.00000780
Iteration 80/1000 | Loss: 0.00000780
Iteration 81/1000 | Loss: 0.00000780
Iteration 82/1000 | Loss: 0.00000780
Iteration 83/1000 | Loss: 0.00000780
Iteration 84/1000 | Loss: 0.00000780
Iteration 85/1000 | Loss: 0.00000779
Iteration 86/1000 | Loss: 0.00000779
Iteration 87/1000 | Loss: 0.00000779
Iteration 88/1000 | Loss: 0.00000778
Iteration 89/1000 | Loss: 0.00000777
Iteration 90/1000 | Loss: 0.00000777
Iteration 91/1000 | Loss: 0.00000777
Iteration 92/1000 | Loss: 0.00000777
Iteration 93/1000 | Loss: 0.00000777
Iteration 94/1000 | Loss: 0.00000777
Iteration 95/1000 | Loss: 0.00000776
Iteration 96/1000 | Loss: 0.00000776
Iteration 97/1000 | Loss: 0.00000775
Iteration 98/1000 | Loss: 0.00000775
Iteration 99/1000 | Loss: 0.00000775
Iteration 100/1000 | Loss: 0.00000775
Iteration 101/1000 | Loss: 0.00000774
Iteration 102/1000 | Loss: 0.00000774
Iteration 103/1000 | Loss: 0.00000774
Iteration 104/1000 | Loss: 0.00000774
Iteration 105/1000 | Loss: 0.00000774
Iteration 106/1000 | Loss: 0.00000774
Iteration 107/1000 | Loss: 0.00000774
Iteration 108/1000 | Loss: 0.00000774
Iteration 109/1000 | Loss: 0.00000773
Iteration 110/1000 | Loss: 0.00000773
Iteration 111/1000 | Loss: 0.00000773
Iteration 112/1000 | Loss: 0.00000773
Iteration 113/1000 | Loss: 0.00000773
Iteration 114/1000 | Loss: 0.00000773
Iteration 115/1000 | Loss: 0.00000773
Iteration 116/1000 | Loss: 0.00000773
Iteration 117/1000 | Loss: 0.00000773
Iteration 118/1000 | Loss: 0.00000773
Iteration 119/1000 | Loss: 0.00000773
Iteration 120/1000 | Loss: 0.00000773
Iteration 121/1000 | Loss: 0.00000773
Iteration 122/1000 | Loss: 0.00000773
Iteration 123/1000 | Loss: 0.00000773
Iteration 124/1000 | Loss: 0.00000773
Iteration 125/1000 | Loss: 0.00000773
Iteration 126/1000 | Loss: 0.00000773
Iteration 127/1000 | Loss: 0.00000773
Iteration 128/1000 | Loss: 0.00000773
Iteration 129/1000 | Loss: 0.00000773
Iteration 130/1000 | Loss: 0.00000773
Iteration 131/1000 | Loss: 0.00000773
Iteration 132/1000 | Loss: 0.00000773
Iteration 133/1000 | Loss: 0.00000773
Iteration 134/1000 | Loss: 0.00000773
Iteration 135/1000 | Loss: 0.00000773
Iteration 136/1000 | Loss: 0.00000773
Iteration 137/1000 | Loss: 0.00000773
Iteration 138/1000 | Loss: 0.00000773
Iteration 139/1000 | Loss: 0.00000773
Iteration 140/1000 | Loss: 0.00000773
Iteration 141/1000 | Loss: 0.00000773
Iteration 142/1000 | Loss: 0.00000773
Iteration 143/1000 | Loss: 0.00000773
Iteration 144/1000 | Loss: 0.00000773
Iteration 145/1000 | Loss: 0.00000773
Iteration 146/1000 | Loss: 0.00000773
Iteration 147/1000 | Loss: 0.00000773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [7.727325282758102e-06, 7.727325282758102e-06, 7.727325282758102e-06, 7.727325282758102e-06, 7.727325282758102e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.727325282758102e-06

Optimization complete. Final v2v error: 2.388376235961914 mm

Highest mean error: 2.5088725090026855 mm for frame 61

Lowest mean error: 2.320937395095825 mm for frame 159

Saving results

Total time: 29.150444984436035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067017
Iteration 2/25 | Loss: 0.00146620
Iteration 3/25 | Loss: 0.00116648
Iteration 4/25 | Loss: 0.00113935
Iteration 5/25 | Loss: 0.00113106
Iteration 6/25 | Loss: 0.00113300
Iteration 7/25 | Loss: 0.00112618
Iteration 8/25 | Loss: 0.00112499
Iteration 9/25 | Loss: 0.00112470
Iteration 10/25 | Loss: 0.00112452
Iteration 11/25 | Loss: 0.00112436
Iteration 12/25 | Loss: 0.00112433
Iteration 13/25 | Loss: 0.00112432
Iteration 14/25 | Loss: 0.00112432
Iteration 15/25 | Loss: 0.00112432
Iteration 16/25 | Loss: 0.00112432
Iteration 17/25 | Loss: 0.00112432
Iteration 18/25 | Loss: 0.00112432
Iteration 19/25 | Loss: 0.00112432
Iteration 20/25 | Loss: 0.00112432
Iteration 21/25 | Loss: 0.00112432
Iteration 22/25 | Loss: 0.00112432
Iteration 23/25 | Loss: 0.00112432
Iteration 24/25 | Loss: 0.00112431
Iteration 25/25 | Loss: 0.00112431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40055633
Iteration 2/25 | Loss: 0.00084365
Iteration 3/25 | Loss: 0.00084365
Iteration 4/25 | Loss: 0.00084365
Iteration 5/25 | Loss: 0.00084365
Iteration 6/25 | Loss: 0.00084365
Iteration 7/25 | Loss: 0.00084365
Iteration 8/25 | Loss: 0.00084365
Iteration 9/25 | Loss: 0.00084365
Iteration 10/25 | Loss: 0.00084365
Iteration 11/25 | Loss: 0.00084365
Iteration 12/25 | Loss: 0.00084365
Iteration 13/25 | Loss: 0.00084365
Iteration 14/25 | Loss: 0.00084365
Iteration 15/25 | Loss: 0.00084365
Iteration 16/25 | Loss: 0.00084365
Iteration 17/25 | Loss: 0.00084365
Iteration 18/25 | Loss: 0.00084365
Iteration 19/25 | Loss: 0.00084365
Iteration 20/25 | Loss: 0.00084365
Iteration 21/25 | Loss: 0.00084365
Iteration 22/25 | Loss: 0.00084365
Iteration 23/25 | Loss: 0.00084365
Iteration 24/25 | Loss: 0.00084365
Iteration 25/25 | Loss: 0.00084365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084365
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00054107
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001409
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001372
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001352
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001339
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001320
Iteration 72/1000 | Loss: 0.00001320
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001319
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001310
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001308
Iteration 149/1000 | Loss: 0.00001308
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001308
Iteration 154/1000 | Loss: 0.00001308
Iteration 155/1000 | Loss: 0.00001308
Iteration 156/1000 | Loss: 0.00001308
Iteration 157/1000 | Loss: 0.00001308
Iteration 158/1000 | Loss: 0.00001308
Iteration 159/1000 | Loss: 0.00001308
Iteration 160/1000 | Loss: 0.00001308
Iteration 161/1000 | Loss: 0.00001308
Iteration 162/1000 | Loss: 0.00001308
Iteration 163/1000 | Loss: 0.00001308
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.307766251557041e-05, 1.307766251557041e-05, 1.307766251557041e-05, 1.307766251557041e-05, 1.307766251557041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.307766251557041e-05

Optimization complete. Final v2v error: 2.98807430267334 mm

Highest mean error: 3.7322542667388916 mm for frame 8

Lowest mean error: 2.530322790145874 mm for frame 74

Saving results

Total time: 47.14110469818115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837504
Iteration 2/25 | Loss: 0.00235368
Iteration 3/25 | Loss: 0.00181426
Iteration 4/25 | Loss: 0.00168623
Iteration 5/25 | Loss: 0.00152289
Iteration 6/25 | Loss: 0.00141249
Iteration 7/25 | Loss: 0.00130947
Iteration 8/25 | Loss: 0.00127009
Iteration 9/25 | Loss: 0.00126618
Iteration 10/25 | Loss: 0.00126580
Iteration 11/25 | Loss: 0.00126637
Iteration 12/25 | Loss: 0.00126415
Iteration 13/25 | Loss: 0.00125960
Iteration 14/25 | Loss: 0.00125719
Iteration 15/25 | Loss: 0.00125429
Iteration 16/25 | Loss: 0.00125388
Iteration 17/25 | Loss: 0.00125333
Iteration 18/25 | Loss: 0.00125306
Iteration 19/25 | Loss: 0.00125279
Iteration 20/25 | Loss: 0.00125397
Iteration 21/25 | Loss: 0.00125325
Iteration 22/25 | Loss: 0.00125381
Iteration 23/25 | Loss: 0.00125410
Iteration 24/25 | Loss: 0.00125329
Iteration 25/25 | Loss: 0.00125352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37533128
Iteration 2/25 | Loss: 0.00054514
Iteration 3/25 | Loss: 0.00054512
Iteration 4/25 | Loss: 0.00054512
Iteration 5/25 | Loss: 0.00054512
Iteration 6/25 | Loss: 0.00054512
Iteration 7/25 | Loss: 0.00054512
Iteration 8/25 | Loss: 0.00054512
Iteration 9/25 | Loss: 0.00054512
Iteration 10/25 | Loss: 0.00054512
Iteration 11/25 | Loss: 0.00054512
Iteration 12/25 | Loss: 0.00054512
Iteration 13/25 | Loss: 0.00054512
Iteration 14/25 | Loss: 0.00054512
Iteration 15/25 | Loss: 0.00054512
Iteration 16/25 | Loss: 0.00054512
Iteration 17/25 | Loss: 0.00054512
Iteration 18/25 | Loss: 0.00054512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005451205652207136, 0.0005451205652207136, 0.0005451205652207136, 0.0005451205652207136, 0.0005451205652207136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005451205652207136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054512
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00003313
Iteration 4/1000 | Loss: 0.00004666
Iteration 5/1000 | Loss: 0.00003801
Iteration 6/1000 | Loss: 0.00004029
Iteration 7/1000 | Loss: 0.00003451
Iteration 8/1000 | Loss: 0.00002867
Iteration 9/1000 | Loss: 0.00002274
Iteration 10/1000 | Loss: 0.00003561
Iteration 11/1000 | Loss: 0.00003894
Iteration 12/1000 | Loss: 0.00004231
Iteration 13/1000 | Loss: 0.00004175
Iteration 14/1000 | Loss: 0.00003341
Iteration 15/1000 | Loss: 0.00003011
Iteration 16/1000 | Loss: 0.00003393
Iteration 17/1000 | Loss: 0.00003265
Iteration 18/1000 | Loss: 0.00003253
Iteration 19/1000 | Loss: 0.00003514
Iteration 20/1000 | Loss: 0.00003617
Iteration 21/1000 | Loss: 0.00003811
Iteration 22/1000 | Loss: 0.00004284
Iteration 23/1000 | Loss: 0.00003567
Iteration 24/1000 | Loss: 0.00002904
Iteration 25/1000 | Loss: 0.00003623
Iteration 26/1000 | Loss: 0.00004205
Iteration 27/1000 | Loss: 0.00003966
Iteration 28/1000 | Loss: 0.00004187
Iteration 29/1000 | Loss: 0.00003820
Iteration 30/1000 | Loss: 0.00004153
Iteration 31/1000 | Loss: 0.00003784
Iteration 32/1000 | Loss: 0.00004180
Iteration 33/1000 | Loss: 0.00003791
Iteration 34/1000 | Loss: 0.00004009
Iteration 35/1000 | Loss: 0.00009787
Iteration 36/1000 | Loss: 0.00003607
Iteration 37/1000 | Loss: 0.00003159
Iteration 38/1000 | Loss: 0.00003150
Iteration 39/1000 | Loss: 0.00003231
Iteration 40/1000 | Loss: 0.00002753
Iteration 41/1000 | Loss: 0.00003002
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002838
Iteration 45/1000 | Loss: 0.00002126
Iteration 46/1000 | Loss: 0.00002446
Iteration 47/1000 | Loss: 0.00003122
Iteration 48/1000 | Loss: 0.00003202
Iteration 49/1000 | Loss: 0.00002915
Iteration 50/1000 | Loss: 0.00002501
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00003083
Iteration 53/1000 | Loss: 0.00002831
Iteration 54/1000 | Loss: 0.00002995
Iteration 55/1000 | Loss: 0.00002792
Iteration 56/1000 | Loss: 0.00002467
Iteration 57/1000 | Loss: 0.00003219
Iteration 58/1000 | Loss: 0.00003054
Iteration 59/1000 | Loss: 0.00003123
Iteration 60/1000 | Loss: 0.00002503
Iteration 61/1000 | Loss: 0.00002434
Iteration 62/1000 | Loss: 0.00003196
Iteration 63/1000 | Loss: 0.00002749
Iteration 64/1000 | Loss: 0.00003086
Iteration 65/1000 | Loss: 0.00003263
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002616
Iteration 68/1000 | Loss: 0.00003256
Iteration 69/1000 | Loss: 0.00002919
Iteration 70/1000 | Loss: 0.00003203
Iteration 71/1000 | Loss: 0.00002532
Iteration 72/1000 | Loss: 0.00002893
Iteration 73/1000 | Loss: 0.00002989
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002408
Iteration 76/1000 | Loss: 0.00002296
Iteration 77/1000 | Loss: 0.00003102
Iteration 78/1000 | Loss: 0.00003005
Iteration 79/1000 | Loss: 0.00002696
Iteration 80/1000 | Loss: 0.00002867
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002721
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00003302
Iteration 85/1000 | Loss: 0.00002495
Iteration 86/1000 | Loss: 0.00003823
Iteration 87/1000 | Loss: 0.00002512
Iteration 88/1000 | Loss: 0.00002225
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001933
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001894
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001893
Iteration 101/1000 | Loss: 0.00001893
Iteration 102/1000 | Loss: 0.00001893
Iteration 103/1000 | Loss: 0.00001892
Iteration 104/1000 | Loss: 0.00001892
Iteration 105/1000 | Loss: 0.00001892
Iteration 106/1000 | Loss: 0.00001891
Iteration 107/1000 | Loss: 0.00001891
Iteration 108/1000 | Loss: 0.00001891
Iteration 109/1000 | Loss: 0.00001891
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001888
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001887
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001887
Iteration 121/1000 | Loss: 0.00001887
Iteration 122/1000 | Loss: 0.00001887
Iteration 123/1000 | Loss: 0.00001887
Iteration 124/1000 | Loss: 0.00001887
Iteration 125/1000 | Loss: 0.00001887
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001886
Iteration 131/1000 | Loss: 0.00001886
Iteration 132/1000 | Loss: 0.00001886
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001885
Iteration 136/1000 | Loss: 0.00001885
Iteration 137/1000 | Loss: 0.00001885
Iteration 138/1000 | Loss: 0.00001885
Iteration 139/1000 | Loss: 0.00001885
Iteration 140/1000 | Loss: 0.00001885
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001885
Iteration 147/1000 | Loss: 0.00001885
Iteration 148/1000 | Loss: 0.00001885
Iteration 149/1000 | Loss: 0.00001885
Iteration 150/1000 | Loss: 0.00001885
Iteration 151/1000 | Loss: 0.00001885
Iteration 152/1000 | Loss: 0.00001885
Iteration 153/1000 | Loss: 0.00001885
Iteration 154/1000 | Loss: 0.00001885
Iteration 155/1000 | Loss: 0.00001885
Iteration 156/1000 | Loss: 0.00001884
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001884
Iteration 159/1000 | Loss: 0.00001884
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001884
Iteration 162/1000 | Loss: 0.00001884
Iteration 163/1000 | Loss: 0.00001884
Iteration 164/1000 | Loss: 0.00001884
Iteration 165/1000 | Loss: 0.00001884
Iteration 166/1000 | Loss: 0.00001884
Iteration 167/1000 | Loss: 0.00001884
Iteration 168/1000 | Loss: 0.00001884
Iteration 169/1000 | Loss: 0.00001884
Iteration 170/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.884374796645716e-05, 1.884374796645716e-05, 1.884374796645716e-05, 1.884374796645716e-05, 1.884374796645716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.884374796645716e-05

Optimization complete. Final v2v error: 3.6686851978302 mm

Highest mean error: 5.046406269073486 mm for frame 196

Lowest mean error: 3.4470529556274414 mm for frame 43

Saving results

Total time: 199.13466262817383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909460
Iteration 2/25 | Loss: 0.00163601
Iteration 3/25 | Loss: 0.00125758
Iteration 4/25 | Loss: 0.00122504
Iteration 5/25 | Loss: 0.00121417
Iteration 6/25 | Loss: 0.00121148
Iteration 7/25 | Loss: 0.00121104
Iteration 8/25 | Loss: 0.00121104
Iteration 9/25 | Loss: 0.00121104
Iteration 10/25 | Loss: 0.00121104
Iteration 11/25 | Loss: 0.00121104
Iteration 12/25 | Loss: 0.00121104
Iteration 13/25 | Loss: 0.00121104
Iteration 14/25 | Loss: 0.00121104
Iteration 15/25 | Loss: 0.00121104
Iteration 16/25 | Loss: 0.00121104
Iteration 17/25 | Loss: 0.00121104
Iteration 18/25 | Loss: 0.00121104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012110393727198243, 0.0012110393727198243, 0.0012110393727198243, 0.0012110393727198243, 0.0012110393727198243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012110393727198243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06192172
Iteration 2/25 | Loss: 0.00088290
Iteration 3/25 | Loss: 0.00088290
Iteration 4/25 | Loss: 0.00088290
Iteration 5/25 | Loss: 0.00088290
Iteration 6/25 | Loss: 0.00088290
Iteration 7/25 | Loss: 0.00088290
Iteration 8/25 | Loss: 0.00088290
Iteration 9/25 | Loss: 0.00088290
Iteration 10/25 | Loss: 0.00088290
Iteration 11/25 | Loss: 0.00088290
Iteration 12/25 | Loss: 0.00088290
Iteration 13/25 | Loss: 0.00088290
Iteration 14/25 | Loss: 0.00088290
Iteration 15/25 | Loss: 0.00088290
Iteration 16/25 | Loss: 0.00088290
Iteration 17/25 | Loss: 0.00088290
Iteration 18/25 | Loss: 0.00088290
Iteration 19/25 | Loss: 0.00088290
Iteration 20/25 | Loss: 0.00088290
Iteration 21/25 | Loss: 0.00088290
Iteration 22/25 | Loss: 0.00088290
Iteration 23/25 | Loss: 0.00088290
Iteration 24/25 | Loss: 0.00088290
Iteration 25/25 | Loss: 0.00088290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008828988065943122, 0.0008828988065943122, 0.0008828988065943122, 0.0008828988065943122, 0.0008828988065943122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008828988065943122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088290
Iteration 2/1000 | Loss: 0.00006588
Iteration 3/1000 | Loss: 0.00004047
Iteration 4/1000 | Loss: 0.00003677
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00003377
Iteration 7/1000 | Loss: 0.00003283
Iteration 8/1000 | Loss: 0.00003225
Iteration 9/1000 | Loss: 0.00003180
Iteration 10/1000 | Loss: 0.00003144
Iteration 11/1000 | Loss: 0.00003122
Iteration 12/1000 | Loss: 0.00003093
Iteration 13/1000 | Loss: 0.00003065
Iteration 14/1000 | Loss: 0.00003041
Iteration 15/1000 | Loss: 0.00003026
Iteration 16/1000 | Loss: 0.00003007
Iteration 17/1000 | Loss: 0.00002988
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002972
Iteration 20/1000 | Loss: 0.00002961
Iteration 21/1000 | Loss: 0.00002960
Iteration 22/1000 | Loss: 0.00002956
Iteration 23/1000 | Loss: 0.00002952
Iteration 24/1000 | Loss: 0.00002952
Iteration 25/1000 | Loss: 0.00002951
Iteration 26/1000 | Loss: 0.00002951
Iteration 27/1000 | Loss: 0.00002950
Iteration 28/1000 | Loss: 0.00002949
Iteration 29/1000 | Loss: 0.00002949
Iteration 30/1000 | Loss: 0.00002948
Iteration 31/1000 | Loss: 0.00002946
Iteration 32/1000 | Loss: 0.00002946
Iteration 33/1000 | Loss: 0.00002944
Iteration 34/1000 | Loss: 0.00002944
Iteration 35/1000 | Loss: 0.00002942
Iteration 36/1000 | Loss: 0.00002942
Iteration 37/1000 | Loss: 0.00002942
Iteration 38/1000 | Loss: 0.00002942
Iteration 39/1000 | Loss: 0.00002941
Iteration 40/1000 | Loss: 0.00002939
Iteration 41/1000 | Loss: 0.00002939
Iteration 42/1000 | Loss: 0.00002939
Iteration 43/1000 | Loss: 0.00002939
Iteration 44/1000 | Loss: 0.00002939
Iteration 45/1000 | Loss: 0.00002938
Iteration 46/1000 | Loss: 0.00002938
Iteration 47/1000 | Loss: 0.00002938
Iteration 48/1000 | Loss: 0.00002938
Iteration 49/1000 | Loss: 0.00002938
Iteration 50/1000 | Loss: 0.00002938
Iteration 51/1000 | Loss: 0.00002938
Iteration 52/1000 | Loss: 0.00002938
Iteration 53/1000 | Loss: 0.00002936
Iteration 54/1000 | Loss: 0.00002936
Iteration 55/1000 | Loss: 0.00002936
Iteration 56/1000 | Loss: 0.00002935
Iteration 57/1000 | Loss: 0.00002935
Iteration 58/1000 | Loss: 0.00002935
Iteration 59/1000 | Loss: 0.00002935
Iteration 60/1000 | Loss: 0.00002934
Iteration 61/1000 | Loss: 0.00002933
Iteration 62/1000 | Loss: 0.00002933
Iteration 63/1000 | Loss: 0.00002933
Iteration 64/1000 | Loss: 0.00002933
Iteration 65/1000 | Loss: 0.00002933
Iteration 66/1000 | Loss: 0.00002933
Iteration 67/1000 | Loss: 0.00002933
Iteration 68/1000 | Loss: 0.00002932
Iteration 69/1000 | Loss: 0.00002931
Iteration 70/1000 | Loss: 0.00002931
Iteration 71/1000 | Loss: 0.00002931
Iteration 72/1000 | Loss: 0.00002931
Iteration 73/1000 | Loss: 0.00002930
Iteration 74/1000 | Loss: 0.00002930
Iteration 75/1000 | Loss: 0.00002930
Iteration 76/1000 | Loss: 0.00002930
Iteration 77/1000 | Loss: 0.00002930
Iteration 78/1000 | Loss: 0.00002929
Iteration 79/1000 | Loss: 0.00002929
Iteration 80/1000 | Loss: 0.00002929
Iteration 81/1000 | Loss: 0.00002929
Iteration 82/1000 | Loss: 0.00002929
Iteration 83/1000 | Loss: 0.00002928
Iteration 84/1000 | Loss: 0.00002928
Iteration 85/1000 | Loss: 0.00002928
Iteration 86/1000 | Loss: 0.00002928
Iteration 87/1000 | Loss: 0.00002928
Iteration 88/1000 | Loss: 0.00002928
Iteration 89/1000 | Loss: 0.00002928
Iteration 90/1000 | Loss: 0.00002928
Iteration 91/1000 | Loss: 0.00002928
Iteration 92/1000 | Loss: 0.00002927
Iteration 93/1000 | Loss: 0.00002927
Iteration 94/1000 | Loss: 0.00002927
Iteration 95/1000 | Loss: 0.00002927
Iteration 96/1000 | Loss: 0.00002926
Iteration 97/1000 | Loss: 0.00002926
Iteration 98/1000 | Loss: 0.00002926
Iteration 99/1000 | Loss: 0.00002926
Iteration 100/1000 | Loss: 0.00002926
Iteration 101/1000 | Loss: 0.00002926
Iteration 102/1000 | Loss: 0.00002926
Iteration 103/1000 | Loss: 0.00002926
Iteration 104/1000 | Loss: 0.00002926
Iteration 105/1000 | Loss: 0.00002926
Iteration 106/1000 | Loss: 0.00002926
Iteration 107/1000 | Loss: 0.00002926
Iteration 108/1000 | Loss: 0.00002926
Iteration 109/1000 | Loss: 0.00002925
Iteration 110/1000 | Loss: 0.00002925
Iteration 111/1000 | Loss: 0.00002925
Iteration 112/1000 | Loss: 0.00002925
Iteration 113/1000 | Loss: 0.00002925
Iteration 114/1000 | Loss: 0.00002925
Iteration 115/1000 | Loss: 0.00002925
Iteration 116/1000 | Loss: 0.00002925
Iteration 117/1000 | Loss: 0.00002925
Iteration 118/1000 | Loss: 0.00002925
Iteration 119/1000 | Loss: 0.00002924
Iteration 120/1000 | Loss: 0.00002924
Iteration 121/1000 | Loss: 0.00002924
Iteration 122/1000 | Loss: 0.00002924
Iteration 123/1000 | Loss: 0.00002924
Iteration 124/1000 | Loss: 0.00002924
Iteration 125/1000 | Loss: 0.00002924
Iteration 126/1000 | Loss: 0.00002924
Iteration 127/1000 | Loss: 0.00002924
Iteration 128/1000 | Loss: 0.00002924
Iteration 129/1000 | Loss: 0.00002923
Iteration 130/1000 | Loss: 0.00002923
Iteration 131/1000 | Loss: 0.00002923
Iteration 132/1000 | Loss: 0.00002923
Iteration 133/1000 | Loss: 0.00002923
Iteration 134/1000 | Loss: 0.00002923
Iteration 135/1000 | Loss: 0.00002923
Iteration 136/1000 | Loss: 0.00002923
Iteration 137/1000 | Loss: 0.00002923
Iteration 138/1000 | Loss: 0.00002923
Iteration 139/1000 | Loss: 0.00002923
Iteration 140/1000 | Loss: 0.00002923
Iteration 141/1000 | Loss: 0.00002923
Iteration 142/1000 | Loss: 0.00002923
Iteration 143/1000 | Loss: 0.00002923
Iteration 144/1000 | Loss: 0.00002923
Iteration 145/1000 | Loss: 0.00002923
Iteration 146/1000 | Loss: 0.00002923
Iteration 147/1000 | Loss: 0.00002923
Iteration 148/1000 | Loss: 0.00002923
Iteration 149/1000 | Loss: 0.00002923
Iteration 150/1000 | Loss: 0.00002923
Iteration 151/1000 | Loss: 0.00002923
Iteration 152/1000 | Loss: 0.00002923
Iteration 153/1000 | Loss: 0.00002923
Iteration 154/1000 | Loss: 0.00002923
Iteration 155/1000 | Loss: 0.00002923
Iteration 156/1000 | Loss: 0.00002923
Iteration 157/1000 | Loss: 0.00002923
Iteration 158/1000 | Loss: 0.00002923
Iteration 159/1000 | Loss: 0.00002923
Iteration 160/1000 | Loss: 0.00002923
Iteration 161/1000 | Loss: 0.00002923
Iteration 162/1000 | Loss: 0.00002923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.9225982871139422e-05, 2.9225982871139422e-05, 2.9225982871139422e-05, 2.9225982871139422e-05, 2.9225982871139422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9225982871139422e-05

Optimization complete. Final v2v error: 4.393491268157959 mm

Highest mean error: 5.612655162811279 mm for frame 145

Lowest mean error: 3.5970048904418945 mm for frame 44

Saving results

Total time: 55.26415657997131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508275
Iteration 2/25 | Loss: 0.00124500
Iteration 3/25 | Loss: 0.00111446
Iteration 4/25 | Loss: 0.00109588
Iteration 5/25 | Loss: 0.00108957
Iteration 6/25 | Loss: 0.00108811
Iteration 7/25 | Loss: 0.00108782
Iteration 8/25 | Loss: 0.00108782
Iteration 9/25 | Loss: 0.00108782
Iteration 10/25 | Loss: 0.00108782
Iteration 11/25 | Loss: 0.00108782
Iteration 12/25 | Loss: 0.00108782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001087816315703094, 0.001087816315703094, 0.001087816315703094, 0.001087816315703094, 0.001087816315703094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001087816315703094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37747002
Iteration 2/25 | Loss: 0.00087097
Iteration 3/25 | Loss: 0.00087083
Iteration 4/25 | Loss: 0.00087083
Iteration 5/25 | Loss: 0.00087083
Iteration 6/25 | Loss: 0.00087083
Iteration 7/25 | Loss: 0.00087083
Iteration 8/25 | Loss: 0.00087083
Iteration 9/25 | Loss: 0.00087083
Iteration 10/25 | Loss: 0.00087083
Iteration 11/25 | Loss: 0.00087083
Iteration 12/25 | Loss: 0.00087083
Iteration 13/25 | Loss: 0.00087083
Iteration 14/25 | Loss: 0.00087083
Iteration 15/25 | Loss: 0.00087083
Iteration 16/25 | Loss: 0.00087083
Iteration 17/25 | Loss: 0.00087083
Iteration 18/25 | Loss: 0.00087083
Iteration 19/25 | Loss: 0.00087083
Iteration 20/25 | Loss: 0.00087083
Iteration 21/25 | Loss: 0.00087083
Iteration 22/25 | Loss: 0.00087083
Iteration 23/25 | Loss: 0.00087083
Iteration 24/25 | Loss: 0.00087083
Iteration 25/25 | Loss: 0.00087083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087083
Iteration 2/1000 | Loss: 0.00004212
Iteration 3/1000 | Loss: 0.00002339
Iteration 4/1000 | Loss: 0.00001735
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001351
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001291
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001284
Iteration 50/1000 | Loss: 0.00001284
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001282
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001280
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.2743884326482657e-05, 1.2743884326482657e-05, 1.2743884326482657e-05, 1.2743884326482657e-05, 1.2743884326482657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2743884326482657e-05

Optimization complete. Final v2v error: 2.997313976287842 mm

Highest mean error: 3.373091220855713 mm for frame 107

Lowest mean error: 2.6720199584960938 mm for frame 116

Saving results

Total time: 35.350157499313354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938934
Iteration 2/25 | Loss: 0.00263505
Iteration 3/25 | Loss: 0.00209974
Iteration 4/25 | Loss: 0.00179668
Iteration 5/25 | Loss: 0.00181015
Iteration 6/25 | Loss: 0.00215967
Iteration 7/25 | Loss: 0.00174755
Iteration 8/25 | Loss: 0.00142972
Iteration 9/25 | Loss: 0.00136508
Iteration 10/25 | Loss: 0.00133116
Iteration 11/25 | Loss: 0.00130844
Iteration 12/25 | Loss: 0.00126933
Iteration 13/25 | Loss: 0.00125378
Iteration 14/25 | Loss: 0.00124913
Iteration 15/25 | Loss: 0.00124635
Iteration 16/25 | Loss: 0.00123219
Iteration 17/25 | Loss: 0.00122500
Iteration 18/25 | Loss: 0.00122107
Iteration 19/25 | Loss: 0.00121687
Iteration 20/25 | Loss: 0.00122221
Iteration 21/25 | Loss: 0.00121637
Iteration 22/25 | Loss: 0.00121193
Iteration 23/25 | Loss: 0.00121132
Iteration 24/25 | Loss: 0.00121113
Iteration 25/25 | Loss: 0.00121109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36852336
Iteration 2/25 | Loss: 0.00097364
Iteration 3/25 | Loss: 0.00097364
Iteration 4/25 | Loss: 0.00097363
Iteration 5/25 | Loss: 0.00097363
Iteration 6/25 | Loss: 0.00097363
Iteration 7/25 | Loss: 0.00097363
Iteration 8/25 | Loss: 0.00097363
Iteration 9/25 | Loss: 0.00097363
Iteration 10/25 | Loss: 0.00097363
Iteration 11/25 | Loss: 0.00097363
Iteration 12/25 | Loss: 0.00097363
Iteration 13/25 | Loss: 0.00097363
Iteration 14/25 | Loss: 0.00097363
Iteration 15/25 | Loss: 0.00097363
Iteration 16/25 | Loss: 0.00097363
Iteration 17/25 | Loss: 0.00097363
Iteration 18/25 | Loss: 0.00097363
Iteration 19/25 | Loss: 0.00097363
Iteration 20/25 | Loss: 0.00097363
Iteration 21/25 | Loss: 0.00097363
Iteration 22/25 | Loss: 0.00097363
Iteration 23/25 | Loss: 0.00097363
Iteration 24/25 | Loss: 0.00097363
Iteration 25/25 | Loss: 0.00097363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097363
Iteration 2/1000 | Loss: 0.00010547
Iteration 3/1000 | Loss: 0.00007691
Iteration 4/1000 | Loss: 0.00006668
Iteration 5/1000 | Loss: 0.00006127
Iteration 6/1000 | Loss: 0.00005762
Iteration 7/1000 | Loss: 0.00005576
Iteration 8/1000 | Loss: 0.00005423
Iteration 9/1000 | Loss: 0.00005287
Iteration 10/1000 | Loss: 0.00005182
Iteration 11/1000 | Loss: 0.00005100
Iteration 12/1000 | Loss: 0.00005039
Iteration 13/1000 | Loss: 0.00052022
Iteration 14/1000 | Loss: 0.00456235
Iteration 15/1000 | Loss: 0.00098090
Iteration 16/1000 | Loss: 0.00053974
Iteration 17/1000 | Loss: 0.00021477
Iteration 18/1000 | Loss: 0.00024452
Iteration 19/1000 | Loss: 0.00007068
Iteration 20/1000 | Loss: 0.00024881
Iteration 21/1000 | Loss: 0.00005142
Iteration 22/1000 | Loss: 0.00004114
Iteration 23/1000 | Loss: 0.00029840
Iteration 24/1000 | Loss: 0.00006090
Iteration 25/1000 | Loss: 0.00003495
Iteration 26/1000 | Loss: 0.00003019
Iteration 27/1000 | Loss: 0.00002557
Iteration 28/1000 | Loss: 0.00002323
Iteration 29/1000 | Loss: 0.00002152
Iteration 30/1000 | Loss: 0.00002015
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001537
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001535
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001535
Iteration 132/1000 | Loss: 0.00001534
Iteration 133/1000 | Loss: 0.00001534
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001534
Iteration 137/1000 | Loss: 0.00001534
Iteration 138/1000 | Loss: 0.00001534
Iteration 139/1000 | Loss: 0.00001534
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001534
Iteration 142/1000 | Loss: 0.00001534
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001534
Iteration 145/1000 | Loss: 0.00001534
Iteration 146/1000 | Loss: 0.00001534
Iteration 147/1000 | Loss: 0.00001534
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001534
Iteration 156/1000 | Loss: 0.00001534
Iteration 157/1000 | Loss: 0.00001534
Iteration 158/1000 | Loss: 0.00001534
Iteration 159/1000 | Loss: 0.00001534
Iteration 160/1000 | Loss: 0.00001534
Iteration 161/1000 | Loss: 0.00001534
Iteration 162/1000 | Loss: 0.00001534
Iteration 163/1000 | Loss: 0.00001534
Iteration 164/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.534022703708615e-05, 1.534022703708615e-05, 1.534022703708615e-05, 1.534022703708615e-05, 1.534022703708615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.534022703708615e-05

Optimization complete. Final v2v error: 3.3008906841278076 mm

Highest mean error: 5.342447280883789 mm for frame 79

Lowest mean error: 3.198719024658203 mm for frame 50

Saving results

Total time: 102.06336903572083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846505
Iteration 2/25 | Loss: 0.00136959
Iteration 3/25 | Loss: 0.00118616
Iteration 4/25 | Loss: 0.00115848
Iteration 5/25 | Loss: 0.00114976
Iteration 6/25 | Loss: 0.00114763
Iteration 7/25 | Loss: 0.00114763
Iteration 8/25 | Loss: 0.00114763
Iteration 9/25 | Loss: 0.00114763
Iteration 10/25 | Loss: 0.00114763
Iteration 11/25 | Loss: 0.00114763
Iteration 12/25 | Loss: 0.00114763
Iteration 13/25 | Loss: 0.00114763
Iteration 14/25 | Loss: 0.00114763
Iteration 15/25 | Loss: 0.00114763
Iteration 16/25 | Loss: 0.00114763
Iteration 17/25 | Loss: 0.00114763
Iteration 18/25 | Loss: 0.00114763
Iteration 19/25 | Loss: 0.00114763
Iteration 20/25 | Loss: 0.00114763
Iteration 21/25 | Loss: 0.00114763
Iteration 22/25 | Loss: 0.00114763
Iteration 23/25 | Loss: 0.00114763
Iteration 24/25 | Loss: 0.00114763
Iteration 25/25 | Loss: 0.00114763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.99498892
Iteration 2/25 | Loss: 0.00083309
Iteration 3/25 | Loss: 0.00083227
Iteration 4/25 | Loss: 0.00083227
Iteration 5/25 | Loss: 0.00083227
Iteration 6/25 | Loss: 0.00083227
Iteration 7/25 | Loss: 0.00083226
Iteration 8/25 | Loss: 0.00083226
Iteration 9/25 | Loss: 0.00083226
Iteration 10/25 | Loss: 0.00083226
Iteration 11/25 | Loss: 0.00083226
Iteration 12/25 | Loss: 0.00083226
Iteration 13/25 | Loss: 0.00083226
Iteration 14/25 | Loss: 0.00083226
Iteration 15/25 | Loss: 0.00083226
Iteration 16/25 | Loss: 0.00083226
Iteration 17/25 | Loss: 0.00083226
Iteration 18/25 | Loss: 0.00083226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008322636713273823, 0.0008322636713273823, 0.0008322636713273823, 0.0008322636713273823, 0.0008322636713273823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008322636713273823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083226
Iteration 2/1000 | Loss: 0.00005702
Iteration 3/1000 | Loss: 0.00003813
Iteration 4/1000 | Loss: 0.00003369
Iteration 5/1000 | Loss: 0.00003162
Iteration 6/1000 | Loss: 0.00003013
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002838
Iteration 9/1000 | Loss: 0.00002777
Iteration 10/1000 | Loss: 0.00002738
Iteration 11/1000 | Loss: 0.00002706
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002628
Iteration 15/1000 | Loss: 0.00002609
Iteration 16/1000 | Loss: 0.00002598
Iteration 17/1000 | Loss: 0.00002597
Iteration 18/1000 | Loss: 0.00002590
Iteration 19/1000 | Loss: 0.00002581
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002580
Iteration 22/1000 | Loss: 0.00002575
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002568
Iteration 25/1000 | Loss: 0.00002567
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002564
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00002563
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002561
Iteration 33/1000 | Loss: 0.00002561
Iteration 34/1000 | Loss: 0.00002561
Iteration 35/1000 | Loss: 0.00002560
Iteration 36/1000 | Loss: 0.00002560
Iteration 37/1000 | Loss: 0.00002560
Iteration 38/1000 | Loss: 0.00002559
Iteration 39/1000 | Loss: 0.00002559
Iteration 40/1000 | Loss: 0.00002559
Iteration 41/1000 | Loss: 0.00002559
Iteration 42/1000 | Loss: 0.00002558
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002558
Iteration 45/1000 | Loss: 0.00002558
Iteration 46/1000 | Loss: 0.00002558
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002557
Iteration 49/1000 | Loss: 0.00002557
Iteration 50/1000 | Loss: 0.00002557
Iteration 51/1000 | Loss: 0.00002556
Iteration 52/1000 | Loss: 0.00002556
Iteration 53/1000 | Loss: 0.00002556
Iteration 54/1000 | Loss: 0.00002555
Iteration 55/1000 | Loss: 0.00002555
Iteration 56/1000 | Loss: 0.00002554
Iteration 57/1000 | Loss: 0.00002554
Iteration 58/1000 | Loss: 0.00002554
Iteration 59/1000 | Loss: 0.00002553
Iteration 60/1000 | Loss: 0.00002553
Iteration 61/1000 | Loss: 0.00002553
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002553
Iteration 64/1000 | Loss: 0.00002553
Iteration 65/1000 | Loss: 0.00002552
Iteration 66/1000 | Loss: 0.00002550
Iteration 67/1000 | Loss: 0.00002550
Iteration 68/1000 | Loss: 0.00002550
Iteration 69/1000 | Loss: 0.00002549
Iteration 70/1000 | Loss: 0.00002549
Iteration 71/1000 | Loss: 0.00002548
Iteration 72/1000 | Loss: 0.00002548
Iteration 73/1000 | Loss: 0.00002547
Iteration 74/1000 | Loss: 0.00002547
Iteration 75/1000 | Loss: 0.00002547
Iteration 76/1000 | Loss: 0.00002547
Iteration 77/1000 | Loss: 0.00002546
Iteration 78/1000 | Loss: 0.00002546
Iteration 79/1000 | Loss: 0.00002546
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002545
Iteration 82/1000 | Loss: 0.00002545
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002543
Iteration 87/1000 | Loss: 0.00002543
Iteration 88/1000 | Loss: 0.00002543
Iteration 89/1000 | Loss: 0.00002543
Iteration 90/1000 | Loss: 0.00002543
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00002542
Iteration 93/1000 | Loss: 0.00002542
Iteration 94/1000 | Loss: 0.00002542
Iteration 95/1000 | Loss: 0.00002542
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00002541
Iteration 99/1000 | Loss: 0.00002541
Iteration 100/1000 | Loss: 0.00002541
Iteration 101/1000 | Loss: 0.00002540
Iteration 102/1000 | Loss: 0.00002540
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002540
Iteration 105/1000 | Loss: 0.00002540
Iteration 106/1000 | Loss: 0.00002540
Iteration 107/1000 | Loss: 0.00002540
Iteration 108/1000 | Loss: 0.00002540
Iteration 109/1000 | Loss: 0.00002540
Iteration 110/1000 | Loss: 0.00002539
Iteration 111/1000 | Loss: 0.00002539
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002537
Iteration 117/1000 | Loss: 0.00002537
Iteration 118/1000 | Loss: 0.00002537
Iteration 119/1000 | Loss: 0.00002537
Iteration 120/1000 | Loss: 0.00002536
Iteration 121/1000 | Loss: 0.00002536
Iteration 122/1000 | Loss: 0.00002536
Iteration 123/1000 | Loss: 0.00002536
Iteration 124/1000 | Loss: 0.00002535
Iteration 125/1000 | Loss: 0.00002535
Iteration 126/1000 | Loss: 0.00002535
Iteration 127/1000 | Loss: 0.00002534
Iteration 128/1000 | Loss: 0.00002534
Iteration 129/1000 | Loss: 0.00002534
Iteration 130/1000 | Loss: 0.00002534
Iteration 131/1000 | Loss: 0.00002533
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002533
Iteration 134/1000 | Loss: 0.00002532
Iteration 135/1000 | Loss: 0.00002532
Iteration 136/1000 | Loss: 0.00002532
Iteration 137/1000 | Loss: 0.00002532
Iteration 138/1000 | Loss: 0.00002531
Iteration 139/1000 | Loss: 0.00002531
Iteration 140/1000 | Loss: 0.00002531
Iteration 141/1000 | Loss: 0.00002531
Iteration 142/1000 | Loss: 0.00002530
Iteration 143/1000 | Loss: 0.00002530
Iteration 144/1000 | Loss: 0.00002530
Iteration 145/1000 | Loss: 0.00002530
Iteration 146/1000 | Loss: 0.00002529
Iteration 147/1000 | Loss: 0.00002529
Iteration 148/1000 | Loss: 0.00002529
Iteration 149/1000 | Loss: 0.00002529
Iteration 150/1000 | Loss: 0.00002528
Iteration 151/1000 | Loss: 0.00002528
Iteration 152/1000 | Loss: 0.00002528
Iteration 153/1000 | Loss: 0.00002528
Iteration 154/1000 | Loss: 0.00002528
Iteration 155/1000 | Loss: 0.00002528
Iteration 156/1000 | Loss: 0.00002528
Iteration 157/1000 | Loss: 0.00002528
Iteration 158/1000 | Loss: 0.00002527
Iteration 159/1000 | Loss: 0.00002527
Iteration 160/1000 | Loss: 0.00002527
Iteration 161/1000 | Loss: 0.00002527
Iteration 162/1000 | Loss: 0.00002527
Iteration 163/1000 | Loss: 0.00002527
Iteration 164/1000 | Loss: 0.00002527
Iteration 165/1000 | Loss: 0.00002527
Iteration 166/1000 | Loss: 0.00002527
Iteration 167/1000 | Loss: 0.00002526
Iteration 168/1000 | Loss: 0.00002526
Iteration 169/1000 | Loss: 0.00002526
Iteration 170/1000 | Loss: 0.00002526
Iteration 171/1000 | Loss: 0.00002526
Iteration 172/1000 | Loss: 0.00002525
Iteration 173/1000 | Loss: 0.00002525
Iteration 174/1000 | Loss: 0.00002525
Iteration 175/1000 | Loss: 0.00002525
Iteration 176/1000 | Loss: 0.00002525
Iteration 177/1000 | Loss: 0.00002525
Iteration 178/1000 | Loss: 0.00002525
Iteration 179/1000 | Loss: 0.00002525
Iteration 180/1000 | Loss: 0.00002524
Iteration 181/1000 | Loss: 0.00002524
Iteration 182/1000 | Loss: 0.00002524
Iteration 183/1000 | Loss: 0.00002524
Iteration 184/1000 | Loss: 0.00002524
Iteration 185/1000 | Loss: 0.00002524
Iteration 186/1000 | Loss: 0.00002524
Iteration 187/1000 | Loss: 0.00002523
Iteration 188/1000 | Loss: 0.00002523
Iteration 189/1000 | Loss: 0.00002523
Iteration 190/1000 | Loss: 0.00002523
Iteration 191/1000 | Loss: 0.00002523
Iteration 192/1000 | Loss: 0.00002523
Iteration 193/1000 | Loss: 0.00002523
Iteration 194/1000 | Loss: 0.00002523
Iteration 195/1000 | Loss: 0.00002523
Iteration 196/1000 | Loss: 0.00002523
Iteration 197/1000 | Loss: 0.00002523
Iteration 198/1000 | Loss: 0.00002522
Iteration 199/1000 | Loss: 0.00002522
Iteration 200/1000 | Loss: 0.00002522
Iteration 201/1000 | Loss: 0.00002522
Iteration 202/1000 | Loss: 0.00002522
Iteration 203/1000 | Loss: 0.00002522
Iteration 204/1000 | Loss: 0.00002522
Iteration 205/1000 | Loss: 0.00002522
Iteration 206/1000 | Loss: 0.00002522
Iteration 207/1000 | Loss: 0.00002522
Iteration 208/1000 | Loss: 0.00002522
Iteration 209/1000 | Loss: 0.00002522
Iteration 210/1000 | Loss: 0.00002522
Iteration 211/1000 | Loss: 0.00002522
Iteration 212/1000 | Loss: 0.00002522
Iteration 213/1000 | Loss: 0.00002521
Iteration 214/1000 | Loss: 0.00002521
Iteration 215/1000 | Loss: 0.00002521
Iteration 216/1000 | Loss: 0.00002521
Iteration 217/1000 | Loss: 0.00002521
Iteration 218/1000 | Loss: 0.00002521
Iteration 219/1000 | Loss: 0.00002521
Iteration 220/1000 | Loss: 0.00002521
Iteration 221/1000 | Loss: 0.00002521
Iteration 222/1000 | Loss: 0.00002521
Iteration 223/1000 | Loss: 0.00002521
Iteration 224/1000 | Loss: 0.00002521
Iteration 225/1000 | Loss: 0.00002521
Iteration 226/1000 | Loss: 0.00002521
Iteration 227/1000 | Loss: 0.00002521
Iteration 228/1000 | Loss: 0.00002521
Iteration 229/1000 | Loss: 0.00002520
Iteration 230/1000 | Loss: 0.00002520
Iteration 231/1000 | Loss: 0.00002520
Iteration 232/1000 | Loss: 0.00002520
Iteration 233/1000 | Loss: 0.00002520
Iteration 234/1000 | Loss: 0.00002520
Iteration 235/1000 | Loss: 0.00002520
Iteration 236/1000 | Loss: 0.00002520
Iteration 237/1000 | Loss: 0.00002520
Iteration 238/1000 | Loss: 0.00002520
Iteration 239/1000 | Loss: 0.00002520
Iteration 240/1000 | Loss: 0.00002520
Iteration 241/1000 | Loss: 0.00002520
Iteration 242/1000 | Loss: 0.00002519
Iteration 243/1000 | Loss: 0.00002519
Iteration 244/1000 | Loss: 0.00002519
Iteration 245/1000 | Loss: 0.00002519
Iteration 246/1000 | Loss: 0.00002519
Iteration 247/1000 | Loss: 0.00002519
Iteration 248/1000 | Loss: 0.00002519
Iteration 249/1000 | Loss: 0.00002519
Iteration 250/1000 | Loss: 0.00002519
Iteration 251/1000 | Loss: 0.00002519
Iteration 252/1000 | Loss: 0.00002519
Iteration 253/1000 | Loss: 0.00002519
Iteration 254/1000 | Loss: 0.00002519
Iteration 255/1000 | Loss: 0.00002519
Iteration 256/1000 | Loss: 0.00002519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [2.5190587621182203e-05, 2.5190587621182203e-05, 2.5190587621182203e-05, 2.5190587621182203e-05, 2.5190587621182203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5190587621182203e-05

Optimization complete. Final v2v error: 4.106758117675781 mm

Highest mean error: 6.452578544616699 mm for frame 130

Lowest mean error: 2.8527679443359375 mm for frame 153

Saving results

Total time: 57.89516520500183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028323
Iteration 2/25 | Loss: 0.00241666
Iteration 3/25 | Loss: 0.00185287
Iteration 4/25 | Loss: 0.00174883
Iteration 5/25 | Loss: 0.00160887
Iteration 6/25 | Loss: 0.00147565
Iteration 7/25 | Loss: 0.00145331
Iteration 8/25 | Loss: 0.00144505
Iteration 9/25 | Loss: 0.00143913
Iteration 10/25 | Loss: 0.00141200
Iteration 11/25 | Loss: 0.00140482
Iteration 12/25 | Loss: 0.00139247
Iteration 13/25 | Loss: 0.00137380
Iteration 14/25 | Loss: 0.00136972
Iteration 15/25 | Loss: 0.00136891
Iteration 16/25 | Loss: 0.00136845
Iteration 17/25 | Loss: 0.00137412
Iteration 18/25 | Loss: 0.00137300
Iteration 19/25 | Loss: 0.00136894
Iteration 20/25 | Loss: 0.00136430
Iteration 21/25 | Loss: 0.00136350
Iteration 22/25 | Loss: 0.00136334
Iteration 23/25 | Loss: 0.00136325
Iteration 24/25 | Loss: 0.00136313
Iteration 25/25 | Loss: 0.00136298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82592797
Iteration 2/25 | Loss: 0.00235996
Iteration 3/25 | Loss: 0.00235996
Iteration 4/25 | Loss: 0.00235996
Iteration 5/25 | Loss: 0.00235996
Iteration 6/25 | Loss: 0.00235996
Iteration 7/25 | Loss: 0.00235996
Iteration 8/25 | Loss: 0.00235996
Iteration 9/25 | Loss: 0.00235996
Iteration 10/25 | Loss: 0.00235996
Iteration 11/25 | Loss: 0.00235996
Iteration 12/25 | Loss: 0.00235996
Iteration 13/25 | Loss: 0.00235996
Iteration 14/25 | Loss: 0.00235996
Iteration 15/25 | Loss: 0.00235996
Iteration 16/25 | Loss: 0.00235996
Iteration 17/25 | Loss: 0.00235996
Iteration 18/25 | Loss: 0.00235996
Iteration 19/25 | Loss: 0.00235996
Iteration 20/25 | Loss: 0.00235996
Iteration 21/25 | Loss: 0.00235996
Iteration 22/25 | Loss: 0.00235996
Iteration 23/25 | Loss: 0.00235996
Iteration 24/25 | Loss: 0.00235996
Iteration 25/25 | Loss: 0.00235996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235996
Iteration 2/1000 | Loss: 0.00046916
Iteration 3/1000 | Loss: 0.00087333
Iteration 4/1000 | Loss: 0.00044173
Iteration 5/1000 | Loss: 0.00053902
Iteration 6/1000 | Loss: 0.00017007
Iteration 7/1000 | Loss: 0.00014323
Iteration 8/1000 | Loss: 0.00012816
Iteration 9/1000 | Loss: 0.00011428
Iteration 10/1000 | Loss: 0.00010763
Iteration 11/1000 | Loss: 0.00010221
Iteration 12/1000 | Loss: 0.00009920
Iteration 13/1000 | Loss: 0.00009657
Iteration 14/1000 | Loss: 0.00009467
Iteration 15/1000 | Loss: 0.00014047
Iteration 16/1000 | Loss: 0.00009588
Iteration 17/1000 | Loss: 0.00009259
Iteration 18/1000 | Loss: 0.00009094
Iteration 19/1000 | Loss: 0.00008859
Iteration 20/1000 | Loss: 0.00008681
Iteration 21/1000 | Loss: 0.00008551
Iteration 22/1000 | Loss: 0.00008460
Iteration 23/1000 | Loss: 0.00008342
Iteration 24/1000 | Loss: 0.00008234
Iteration 25/1000 | Loss: 0.00008140
Iteration 26/1000 | Loss: 0.00046009
Iteration 27/1000 | Loss: 0.00010284
Iteration 28/1000 | Loss: 0.00008345
Iteration 29/1000 | Loss: 0.00007922
Iteration 30/1000 | Loss: 0.00007830
Iteration 31/1000 | Loss: 0.00007770
Iteration 32/1000 | Loss: 0.00007731
Iteration 33/1000 | Loss: 0.00007696
Iteration 34/1000 | Loss: 0.00007663
Iteration 35/1000 | Loss: 0.00007633
Iteration 36/1000 | Loss: 0.00007616
Iteration 37/1000 | Loss: 0.00007595
Iteration 38/1000 | Loss: 0.00007583
Iteration 39/1000 | Loss: 0.00007567
Iteration 40/1000 | Loss: 0.00007552
Iteration 41/1000 | Loss: 0.00007550
Iteration 42/1000 | Loss: 0.00007549
Iteration 43/1000 | Loss: 0.00007545
Iteration 44/1000 | Loss: 0.00007542
Iteration 45/1000 | Loss: 0.00007542
Iteration 46/1000 | Loss: 0.00007541
Iteration 47/1000 | Loss: 0.00007541
Iteration 48/1000 | Loss: 0.00007541
Iteration 49/1000 | Loss: 0.00007541
Iteration 50/1000 | Loss: 0.00007540
Iteration 51/1000 | Loss: 0.00007540
Iteration 52/1000 | Loss: 0.00007540
Iteration 53/1000 | Loss: 0.00007540
Iteration 54/1000 | Loss: 0.00007539
Iteration 55/1000 | Loss: 0.00007539
Iteration 56/1000 | Loss: 0.00007539
Iteration 57/1000 | Loss: 0.00007539
Iteration 58/1000 | Loss: 0.00007539
Iteration 59/1000 | Loss: 0.00007538
Iteration 60/1000 | Loss: 0.00007538
Iteration 61/1000 | Loss: 0.00007537
Iteration 62/1000 | Loss: 0.00007537
Iteration 63/1000 | Loss: 0.00007537
Iteration 64/1000 | Loss: 0.00007537
Iteration 65/1000 | Loss: 0.00007537
Iteration 66/1000 | Loss: 0.00007537
Iteration 67/1000 | Loss: 0.00007537
Iteration 68/1000 | Loss: 0.00007537
Iteration 69/1000 | Loss: 0.00007537
Iteration 70/1000 | Loss: 0.00007536
Iteration 71/1000 | Loss: 0.00007536
Iteration 72/1000 | Loss: 0.00007536
Iteration 73/1000 | Loss: 0.00007536
Iteration 74/1000 | Loss: 0.00007536
Iteration 75/1000 | Loss: 0.00007536
Iteration 76/1000 | Loss: 0.00007536
Iteration 77/1000 | Loss: 0.00007536
Iteration 78/1000 | Loss: 0.00007536
Iteration 79/1000 | Loss: 0.00007536
Iteration 80/1000 | Loss: 0.00007536
Iteration 81/1000 | Loss: 0.00007535
Iteration 82/1000 | Loss: 0.00007535
Iteration 83/1000 | Loss: 0.00007535
Iteration 84/1000 | Loss: 0.00007535
Iteration 85/1000 | Loss: 0.00007534
Iteration 86/1000 | Loss: 0.00007534
Iteration 87/1000 | Loss: 0.00007534
Iteration 88/1000 | Loss: 0.00007533
Iteration 89/1000 | Loss: 0.00007533
Iteration 90/1000 | Loss: 0.00007533
Iteration 91/1000 | Loss: 0.00007532
Iteration 92/1000 | Loss: 0.00007532
Iteration 93/1000 | Loss: 0.00007532
Iteration 94/1000 | Loss: 0.00007531
Iteration 95/1000 | Loss: 0.00007531
Iteration 96/1000 | Loss: 0.00007531
Iteration 97/1000 | Loss: 0.00007531
Iteration 98/1000 | Loss: 0.00007531
Iteration 99/1000 | Loss: 0.00007531
Iteration 100/1000 | Loss: 0.00007531
Iteration 101/1000 | Loss: 0.00007531
Iteration 102/1000 | Loss: 0.00007531
Iteration 103/1000 | Loss: 0.00007530
Iteration 104/1000 | Loss: 0.00007530
Iteration 105/1000 | Loss: 0.00007530
Iteration 106/1000 | Loss: 0.00007529
Iteration 107/1000 | Loss: 0.00007529
Iteration 108/1000 | Loss: 0.00007529
Iteration 109/1000 | Loss: 0.00007528
Iteration 110/1000 | Loss: 0.00007528
Iteration 111/1000 | Loss: 0.00007528
Iteration 112/1000 | Loss: 0.00007528
Iteration 113/1000 | Loss: 0.00007528
Iteration 114/1000 | Loss: 0.00007528
Iteration 115/1000 | Loss: 0.00007527
Iteration 116/1000 | Loss: 0.00007527
Iteration 117/1000 | Loss: 0.00007527
Iteration 118/1000 | Loss: 0.00007527
Iteration 119/1000 | Loss: 0.00007527
Iteration 120/1000 | Loss: 0.00007527
Iteration 121/1000 | Loss: 0.00007527
Iteration 122/1000 | Loss: 0.00007527
Iteration 123/1000 | Loss: 0.00007526
Iteration 124/1000 | Loss: 0.00007526
Iteration 125/1000 | Loss: 0.00007526
Iteration 126/1000 | Loss: 0.00007526
Iteration 127/1000 | Loss: 0.00007525
Iteration 128/1000 | Loss: 0.00007525
Iteration 129/1000 | Loss: 0.00007525
Iteration 130/1000 | Loss: 0.00007524
Iteration 131/1000 | Loss: 0.00007524
Iteration 132/1000 | Loss: 0.00007524
Iteration 133/1000 | Loss: 0.00007524
Iteration 134/1000 | Loss: 0.00007523
Iteration 135/1000 | Loss: 0.00007523
Iteration 136/1000 | Loss: 0.00007523
Iteration 137/1000 | Loss: 0.00007523
Iteration 138/1000 | Loss: 0.00007522
Iteration 139/1000 | Loss: 0.00007522
Iteration 140/1000 | Loss: 0.00007522
Iteration 141/1000 | Loss: 0.00007522
Iteration 142/1000 | Loss: 0.00007521
Iteration 143/1000 | Loss: 0.00007521
Iteration 144/1000 | Loss: 0.00007521
Iteration 145/1000 | Loss: 0.00007520
Iteration 146/1000 | Loss: 0.00007520
Iteration 147/1000 | Loss: 0.00007520
Iteration 148/1000 | Loss: 0.00007520
Iteration 149/1000 | Loss: 0.00007520
Iteration 150/1000 | Loss: 0.00007520
Iteration 151/1000 | Loss: 0.00007519
Iteration 152/1000 | Loss: 0.00007519
Iteration 153/1000 | Loss: 0.00007519
Iteration 154/1000 | Loss: 0.00007519
Iteration 155/1000 | Loss: 0.00007519
Iteration 156/1000 | Loss: 0.00007519
Iteration 157/1000 | Loss: 0.00007518
Iteration 158/1000 | Loss: 0.00007518
Iteration 159/1000 | Loss: 0.00007518
Iteration 160/1000 | Loss: 0.00007518
Iteration 161/1000 | Loss: 0.00007518
Iteration 162/1000 | Loss: 0.00007518
Iteration 163/1000 | Loss: 0.00007518
Iteration 164/1000 | Loss: 0.00007517
Iteration 165/1000 | Loss: 0.00007517
Iteration 166/1000 | Loss: 0.00007517
Iteration 167/1000 | Loss: 0.00007517
Iteration 168/1000 | Loss: 0.00007517
Iteration 169/1000 | Loss: 0.00007516
Iteration 170/1000 | Loss: 0.00007516
Iteration 171/1000 | Loss: 0.00007516
Iteration 172/1000 | Loss: 0.00007516
Iteration 173/1000 | Loss: 0.00007516
Iteration 174/1000 | Loss: 0.00007516
Iteration 175/1000 | Loss: 0.00007516
Iteration 176/1000 | Loss: 0.00007516
Iteration 177/1000 | Loss: 0.00007516
Iteration 178/1000 | Loss: 0.00007516
Iteration 179/1000 | Loss: 0.00007516
Iteration 180/1000 | Loss: 0.00007516
Iteration 181/1000 | Loss: 0.00007515
Iteration 182/1000 | Loss: 0.00007515
Iteration 183/1000 | Loss: 0.00007515
Iteration 184/1000 | Loss: 0.00007515
Iteration 185/1000 | Loss: 0.00007515
Iteration 186/1000 | Loss: 0.00007515
Iteration 187/1000 | Loss: 0.00007515
Iteration 188/1000 | Loss: 0.00007515
Iteration 189/1000 | Loss: 0.00007515
Iteration 190/1000 | Loss: 0.00007515
Iteration 191/1000 | Loss: 0.00007514
Iteration 192/1000 | Loss: 0.00007514
Iteration 193/1000 | Loss: 0.00007514
Iteration 194/1000 | Loss: 0.00007514
Iteration 195/1000 | Loss: 0.00007514
Iteration 196/1000 | Loss: 0.00014083
Iteration 197/1000 | Loss: 0.00108597
Iteration 198/1000 | Loss: 0.00705081
Iteration 199/1000 | Loss: 0.00045910
Iteration 200/1000 | Loss: 0.00017594
Iteration 201/1000 | Loss: 0.00013265
Iteration 202/1000 | Loss: 0.00008854
Iteration 203/1000 | Loss: 0.00008489
Iteration 204/1000 | Loss: 0.00005121
Iteration 205/1000 | Loss: 0.00004284
Iteration 206/1000 | Loss: 0.00003873
Iteration 207/1000 | Loss: 0.00003567
Iteration 208/1000 | Loss: 0.00003348
Iteration 209/1000 | Loss: 0.00003081
Iteration 210/1000 | Loss: 0.00002905
Iteration 211/1000 | Loss: 0.00002719
Iteration 212/1000 | Loss: 0.00002577
Iteration 213/1000 | Loss: 0.00025283
Iteration 214/1000 | Loss: 0.00021553
Iteration 215/1000 | Loss: 0.00022199
Iteration 216/1000 | Loss: 0.00021357
Iteration 217/1000 | Loss: 0.00019474
Iteration 218/1000 | Loss: 0.00017774
Iteration 219/1000 | Loss: 0.00022681
Iteration 220/1000 | Loss: 0.00003176
Iteration 221/1000 | Loss: 0.00002572
Iteration 222/1000 | Loss: 0.00002918
Iteration 223/1000 | Loss: 0.00024137
Iteration 224/1000 | Loss: 0.00003302
Iteration 225/1000 | Loss: 0.00002391
Iteration 226/1000 | Loss: 0.00002341
Iteration 227/1000 | Loss: 0.00003054
Iteration 228/1000 | Loss: 0.00002316
Iteration 229/1000 | Loss: 0.00023336
Iteration 230/1000 | Loss: 0.00011818
Iteration 231/1000 | Loss: 0.00014628
Iteration 232/1000 | Loss: 0.00003919
Iteration 233/1000 | Loss: 0.00017074
Iteration 234/1000 | Loss: 0.00002319
Iteration 235/1000 | Loss: 0.00002228
Iteration 236/1000 | Loss: 0.00002186
Iteration 237/1000 | Loss: 0.00002167
Iteration 238/1000 | Loss: 0.00002142
Iteration 239/1000 | Loss: 0.00002118
Iteration 240/1000 | Loss: 0.00002113
Iteration 241/1000 | Loss: 0.00002091
Iteration 242/1000 | Loss: 0.00002071
Iteration 243/1000 | Loss: 0.00002034
Iteration 244/1000 | Loss: 0.00001976
Iteration 245/1000 | Loss: 0.00001894
Iteration 246/1000 | Loss: 0.00001852
Iteration 247/1000 | Loss: 0.00001824
Iteration 248/1000 | Loss: 0.00001821
Iteration 249/1000 | Loss: 0.00001801
Iteration 250/1000 | Loss: 0.00001792
Iteration 251/1000 | Loss: 0.00001779
Iteration 252/1000 | Loss: 0.00001777
Iteration 253/1000 | Loss: 0.00001777
Iteration 254/1000 | Loss: 0.00001776
Iteration 255/1000 | Loss: 0.00001776
Iteration 256/1000 | Loss: 0.00001774
Iteration 257/1000 | Loss: 0.00001774
Iteration 258/1000 | Loss: 0.00001773
Iteration 259/1000 | Loss: 0.00001773
Iteration 260/1000 | Loss: 0.00001773
Iteration 261/1000 | Loss: 0.00001772
Iteration 262/1000 | Loss: 0.00001772
Iteration 263/1000 | Loss: 0.00001772
Iteration 264/1000 | Loss: 0.00001771
Iteration 265/1000 | Loss: 0.00001771
Iteration 266/1000 | Loss: 0.00001771
Iteration 267/1000 | Loss: 0.00001771
Iteration 268/1000 | Loss: 0.00001771
Iteration 269/1000 | Loss: 0.00001771
Iteration 270/1000 | Loss: 0.00001770
Iteration 271/1000 | Loss: 0.00001770
Iteration 272/1000 | Loss: 0.00001770
Iteration 273/1000 | Loss: 0.00001769
Iteration 274/1000 | Loss: 0.00001769
Iteration 275/1000 | Loss: 0.00001769
Iteration 276/1000 | Loss: 0.00001769
Iteration 277/1000 | Loss: 0.00001769
Iteration 278/1000 | Loss: 0.00001769
Iteration 279/1000 | Loss: 0.00001769
Iteration 280/1000 | Loss: 0.00001769
Iteration 281/1000 | Loss: 0.00001769
Iteration 282/1000 | Loss: 0.00001769
Iteration 283/1000 | Loss: 0.00001768
Iteration 284/1000 | Loss: 0.00001768
Iteration 285/1000 | Loss: 0.00001768
Iteration 286/1000 | Loss: 0.00001768
Iteration 287/1000 | Loss: 0.00001768
Iteration 288/1000 | Loss: 0.00001768
Iteration 289/1000 | Loss: 0.00001768
Iteration 290/1000 | Loss: 0.00001768
Iteration 291/1000 | Loss: 0.00001768
Iteration 292/1000 | Loss: 0.00001768
Iteration 293/1000 | Loss: 0.00001767
Iteration 294/1000 | Loss: 0.00001767
Iteration 295/1000 | Loss: 0.00001767
Iteration 296/1000 | Loss: 0.00001766
Iteration 297/1000 | Loss: 0.00001766
Iteration 298/1000 | Loss: 0.00001765
Iteration 299/1000 | Loss: 0.00001765
Iteration 300/1000 | Loss: 0.00001765
Iteration 301/1000 | Loss: 0.00001765
Iteration 302/1000 | Loss: 0.00001764
Iteration 303/1000 | Loss: 0.00001764
Iteration 304/1000 | Loss: 0.00001764
Iteration 305/1000 | Loss: 0.00001764
Iteration 306/1000 | Loss: 0.00001764
Iteration 307/1000 | Loss: 0.00001764
Iteration 308/1000 | Loss: 0.00001763
Iteration 309/1000 | Loss: 0.00001763
Iteration 310/1000 | Loss: 0.00001763
Iteration 311/1000 | Loss: 0.00001763
Iteration 312/1000 | Loss: 0.00001763
Iteration 313/1000 | Loss: 0.00001762
Iteration 314/1000 | Loss: 0.00001762
Iteration 315/1000 | Loss: 0.00001761
Iteration 316/1000 | Loss: 0.00001761
Iteration 317/1000 | Loss: 0.00001761
Iteration 318/1000 | Loss: 0.00001760
Iteration 319/1000 | Loss: 0.00001760
Iteration 320/1000 | Loss: 0.00001760
Iteration 321/1000 | Loss: 0.00001760
Iteration 322/1000 | Loss: 0.00001759
Iteration 323/1000 | Loss: 0.00001759
Iteration 324/1000 | Loss: 0.00001758
Iteration 325/1000 | Loss: 0.00001758
Iteration 326/1000 | Loss: 0.00001758
Iteration 327/1000 | Loss: 0.00001758
Iteration 328/1000 | Loss: 0.00001758
Iteration 329/1000 | Loss: 0.00001758
Iteration 330/1000 | Loss: 0.00001758
Iteration 331/1000 | Loss: 0.00001757
Iteration 332/1000 | Loss: 0.00001757
Iteration 333/1000 | Loss: 0.00001757
Iteration 334/1000 | Loss: 0.00001757
Iteration 335/1000 | Loss: 0.00001757
Iteration 336/1000 | Loss: 0.00001757
Iteration 337/1000 | Loss: 0.00001757
Iteration 338/1000 | Loss: 0.00001757
Iteration 339/1000 | Loss: 0.00001757
Iteration 340/1000 | Loss: 0.00001757
Iteration 341/1000 | Loss: 0.00001757
Iteration 342/1000 | Loss: 0.00001756
Iteration 343/1000 | Loss: 0.00001756
Iteration 344/1000 | Loss: 0.00001756
Iteration 345/1000 | Loss: 0.00001756
Iteration 346/1000 | Loss: 0.00001756
Iteration 347/1000 | Loss: 0.00001756
Iteration 348/1000 | Loss: 0.00001756
Iteration 349/1000 | Loss: 0.00001756
Iteration 350/1000 | Loss: 0.00001756
Iteration 351/1000 | Loss: 0.00001756
Iteration 352/1000 | Loss: 0.00001756
Iteration 353/1000 | Loss: 0.00001756
Iteration 354/1000 | Loss: 0.00001756
Iteration 355/1000 | Loss: 0.00001756
Iteration 356/1000 | Loss: 0.00001756
Iteration 357/1000 | Loss: 0.00001756
Iteration 358/1000 | Loss: 0.00001756
Iteration 359/1000 | Loss: 0.00001756
Iteration 360/1000 | Loss: 0.00001756
Iteration 361/1000 | Loss: 0.00001756
Iteration 362/1000 | Loss: 0.00001756
Iteration 363/1000 | Loss: 0.00001756
Iteration 364/1000 | Loss: 0.00001756
Iteration 365/1000 | Loss: 0.00001756
Iteration 366/1000 | Loss: 0.00001756
Iteration 367/1000 | Loss: 0.00001756
Iteration 368/1000 | Loss: 0.00001756
Iteration 369/1000 | Loss: 0.00001756
Iteration 370/1000 | Loss: 0.00001756
Iteration 371/1000 | Loss: 0.00001756
Iteration 372/1000 | Loss: 0.00001756
Iteration 373/1000 | Loss: 0.00001756
Iteration 374/1000 | Loss: 0.00001756
Iteration 375/1000 | Loss: 0.00001756
Iteration 376/1000 | Loss: 0.00001756
Iteration 377/1000 | Loss: 0.00001756
Iteration 378/1000 | Loss: 0.00001756
Iteration 379/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [1.7559768821229227e-05, 1.7559768821229227e-05, 1.7559768821229227e-05, 1.7559768821229227e-05, 1.7559768821229227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7559768821229227e-05

Optimization complete. Final v2v error: 3.4913861751556396 mm

Highest mean error: 4.356475830078125 mm for frame 17

Lowest mean error: 2.9705581665039062 mm for frame 5

Saving results

Total time: 185.90926098823547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014385
Iteration 2/25 | Loss: 0.00214480
Iteration 3/25 | Loss: 0.00201913
Iteration 4/25 | Loss: 0.00157594
Iteration 5/25 | Loss: 0.00142290
Iteration 6/25 | Loss: 0.00140701
Iteration 7/25 | Loss: 0.00154373
Iteration 8/25 | Loss: 0.00137475
Iteration 9/25 | Loss: 0.00123076
Iteration 10/25 | Loss: 0.00117950
Iteration 11/25 | Loss: 0.00116956
Iteration 12/25 | Loss: 0.00115659
Iteration 13/25 | Loss: 0.00115757
Iteration 14/25 | Loss: 0.00114657
Iteration 15/25 | Loss: 0.00114482
Iteration 16/25 | Loss: 0.00114566
Iteration 17/25 | Loss: 0.00114568
Iteration 18/25 | Loss: 0.00114532
Iteration 19/25 | Loss: 0.00114555
Iteration 20/25 | Loss: 0.00114500
Iteration 21/25 | Loss: 0.00114578
Iteration 22/25 | Loss: 0.00114515
Iteration 23/25 | Loss: 0.00114478
Iteration 24/25 | Loss: 0.00114562
Iteration 25/25 | Loss: 0.00114523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41003513
Iteration 2/25 | Loss: 0.00089479
Iteration 3/25 | Loss: 0.00089479
Iteration 4/25 | Loss: 0.00089479
Iteration 5/25 | Loss: 0.00089479
Iteration 6/25 | Loss: 0.00089479
Iteration 7/25 | Loss: 0.00089479
Iteration 8/25 | Loss: 0.00089479
Iteration 9/25 | Loss: 0.00089479
Iteration 10/25 | Loss: 0.00089479
Iteration 11/25 | Loss: 0.00089479
Iteration 12/25 | Loss: 0.00089478
Iteration 13/25 | Loss: 0.00089478
Iteration 14/25 | Loss: 0.00089478
Iteration 15/25 | Loss: 0.00089478
Iteration 16/25 | Loss: 0.00089478
Iteration 17/25 | Loss: 0.00089478
Iteration 18/25 | Loss: 0.00089478
Iteration 19/25 | Loss: 0.00089478
Iteration 20/25 | Loss: 0.00089478
Iteration 21/25 | Loss: 0.00089478
Iteration 22/25 | Loss: 0.00089478
Iteration 23/25 | Loss: 0.00089478
Iteration 24/25 | Loss: 0.00089478
Iteration 25/25 | Loss: 0.00089478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089478
Iteration 2/1000 | Loss: 0.00011340
Iteration 3/1000 | Loss: 0.00005238
Iteration 4/1000 | Loss: 0.00003765
Iteration 5/1000 | Loss: 0.00005843
Iteration 6/1000 | Loss: 0.00003225
Iteration 7/1000 | Loss: 0.00004711
Iteration 8/1000 | Loss: 0.00002723
Iteration 9/1000 | Loss: 0.00010417
Iteration 10/1000 | Loss: 0.00006677
Iteration 11/1000 | Loss: 0.00004029
Iteration 12/1000 | Loss: 0.00005506
Iteration 13/1000 | Loss: 0.00003938
Iteration 14/1000 | Loss: 0.00004698
Iteration 15/1000 | Loss: 0.00004560
Iteration 16/1000 | Loss: 0.00005795
Iteration 17/1000 | Loss: 0.00004398
Iteration 18/1000 | Loss: 0.00004367
Iteration 19/1000 | Loss: 0.00005004
Iteration 20/1000 | Loss: 0.00003759
Iteration 21/1000 | Loss: 0.00004505
Iteration 22/1000 | Loss: 0.00004238
Iteration 23/1000 | Loss: 0.00004840
Iteration 24/1000 | Loss: 0.00003996
Iteration 25/1000 | Loss: 0.00003982
Iteration 26/1000 | Loss: 0.00003899
Iteration 27/1000 | Loss: 0.00003819
Iteration 28/1000 | Loss: 0.00003810
Iteration 29/1000 | Loss: 0.00004078
Iteration 30/1000 | Loss: 0.00004047
Iteration 31/1000 | Loss: 0.00003837
Iteration 32/1000 | Loss: 0.00003777
Iteration 33/1000 | Loss: 0.00003808
Iteration 34/1000 | Loss: 0.00003777
Iteration 35/1000 | Loss: 0.00075187
Iteration 36/1000 | Loss: 0.00004918
Iteration 37/1000 | Loss: 0.00003512
Iteration 38/1000 | Loss: 0.00004120
Iteration 39/1000 | Loss: 0.00004050
Iteration 40/1000 | Loss: 0.00004332
Iteration 41/1000 | Loss: 0.00004729
Iteration 42/1000 | Loss: 0.00004106
Iteration 43/1000 | Loss: 0.00005288
Iteration 44/1000 | Loss: 0.00003785
Iteration 45/1000 | Loss: 0.00002384
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00003344
Iteration 48/1000 | Loss: 0.00004040
Iteration 49/1000 | Loss: 0.00003851
Iteration 50/1000 | Loss: 0.00004111
Iteration 51/1000 | Loss: 0.00003760
Iteration 52/1000 | Loss: 0.00003353
Iteration 53/1000 | Loss: 0.00002010
Iteration 54/1000 | Loss: 0.00003799
Iteration 55/1000 | Loss: 0.00005202
Iteration 56/1000 | Loss: 0.00003847
Iteration 57/1000 | Loss: 0.00005229
Iteration 58/1000 | Loss: 0.00003517
Iteration 59/1000 | Loss: 0.00004903
Iteration 60/1000 | Loss: 0.00003622
Iteration 61/1000 | Loss: 0.00003931
Iteration 62/1000 | Loss: 0.00005342
Iteration 63/1000 | Loss: 0.00003898
Iteration 64/1000 | Loss: 0.00004916
Iteration 65/1000 | Loss: 0.00003537
Iteration 66/1000 | Loss: 0.00003923
Iteration 67/1000 | Loss: 0.00004443
Iteration 68/1000 | Loss: 0.00003977
Iteration 69/1000 | Loss: 0.00003484
Iteration 70/1000 | Loss: 0.00002779
Iteration 71/1000 | Loss: 0.00003612
Iteration 72/1000 | Loss: 0.00004358
Iteration 73/1000 | Loss: 0.00005233
Iteration 74/1000 | Loss: 0.00003862
Iteration 75/1000 | Loss: 0.00003628
Iteration 76/1000 | Loss: 0.00003377
Iteration 77/1000 | Loss: 0.00003641
Iteration 78/1000 | Loss: 0.00003830
Iteration 79/1000 | Loss: 0.00003658
Iteration 80/1000 | Loss: 0.00003712
Iteration 81/1000 | Loss: 0.00003021
Iteration 82/1000 | Loss: 0.00003792
Iteration 83/1000 | Loss: 0.00003572
Iteration 84/1000 | Loss: 0.00006659
Iteration 85/1000 | Loss: 0.00003675
Iteration 86/1000 | Loss: 0.00003658
Iteration 87/1000 | Loss: 0.00003473
Iteration 88/1000 | Loss: 0.00005671
Iteration 89/1000 | Loss: 0.00003493
Iteration 90/1000 | Loss: 0.00004675
Iteration 91/1000 | Loss: 0.00004338
Iteration 92/1000 | Loss: 0.00004001
Iteration 93/1000 | Loss: 0.00005478
Iteration 94/1000 | Loss: 0.00003942
Iteration 95/1000 | Loss: 0.00005055
Iteration 96/1000 | Loss: 0.00003754
Iteration 97/1000 | Loss: 0.00004858
Iteration 98/1000 | Loss: 0.00003712
Iteration 99/1000 | Loss: 0.00003731
Iteration 100/1000 | Loss: 0.00003985
Iteration 101/1000 | Loss: 0.00005019
Iteration 102/1000 | Loss: 0.00004004
Iteration 103/1000 | Loss: 0.00003837
Iteration 104/1000 | Loss: 0.00003262
Iteration 105/1000 | Loss: 0.00004226
Iteration 106/1000 | Loss: 0.00003609
Iteration 107/1000 | Loss: 0.00004110
Iteration 108/1000 | Loss: 0.00003973
Iteration 109/1000 | Loss: 0.00003985
Iteration 110/1000 | Loss: 0.00005183
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001737
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001601
Iteration 121/1000 | Loss: 0.00001601
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001589
Iteration 126/1000 | Loss: 0.00001589
Iteration 127/1000 | Loss: 0.00001587
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001574
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001573
Iteration 146/1000 | Loss: 0.00001573
Iteration 147/1000 | Loss: 0.00001572
Iteration 148/1000 | Loss: 0.00001572
Iteration 149/1000 | Loss: 0.00001571
Iteration 150/1000 | Loss: 0.00001571
Iteration 151/1000 | Loss: 0.00001571
Iteration 152/1000 | Loss: 0.00001571
Iteration 153/1000 | Loss: 0.00001571
Iteration 154/1000 | Loss: 0.00001571
Iteration 155/1000 | Loss: 0.00001571
Iteration 156/1000 | Loss: 0.00001571
Iteration 157/1000 | Loss: 0.00001570
Iteration 158/1000 | Loss: 0.00001570
Iteration 159/1000 | Loss: 0.00001570
Iteration 160/1000 | Loss: 0.00001570
Iteration 161/1000 | Loss: 0.00001570
Iteration 162/1000 | Loss: 0.00001570
Iteration 163/1000 | Loss: 0.00001570
Iteration 164/1000 | Loss: 0.00001569
Iteration 165/1000 | Loss: 0.00001569
Iteration 166/1000 | Loss: 0.00001569
Iteration 167/1000 | Loss: 0.00001569
Iteration 168/1000 | Loss: 0.00001569
Iteration 169/1000 | Loss: 0.00001569
Iteration 170/1000 | Loss: 0.00001569
Iteration 171/1000 | Loss: 0.00001569
Iteration 172/1000 | Loss: 0.00001569
Iteration 173/1000 | Loss: 0.00001569
Iteration 174/1000 | Loss: 0.00001569
Iteration 175/1000 | Loss: 0.00001569
Iteration 176/1000 | Loss: 0.00001569
Iteration 177/1000 | Loss: 0.00001569
Iteration 178/1000 | Loss: 0.00001569
Iteration 179/1000 | Loss: 0.00001569
Iteration 180/1000 | Loss: 0.00001568
Iteration 181/1000 | Loss: 0.00001568
Iteration 182/1000 | Loss: 0.00001568
Iteration 183/1000 | Loss: 0.00001568
Iteration 184/1000 | Loss: 0.00001568
Iteration 185/1000 | Loss: 0.00001568
Iteration 186/1000 | Loss: 0.00001568
Iteration 187/1000 | Loss: 0.00001568
Iteration 188/1000 | Loss: 0.00001568
Iteration 189/1000 | Loss: 0.00001567
Iteration 190/1000 | Loss: 0.00001567
Iteration 191/1000 | Loss: 0.00001567
Iteration 192/1000 | Loss: 0.00001567
Iteration 193/1000 | Loss: 0.00001566
Iteration 194/1000 | Loss: 0.00001566
Iteration 195/1000 | Loss: 0.00001566
Iteration 196/1000 | Loss: 0.00001566
Iteration 197/1000 | Loss: 0.00001565
Iteration 198/1000 | Loss: 0.00001565
Iteration 199/1000 | Loss: 0.00001565
Iteration 200/1000 | Loss: 0.00001565
Iteration 201/1000 | Loss: 0.00001565
Iteration 202/1000 | Loss: 0.00001565
Iteration 203/1000 | Loss: 0.00001564
Iteration 204/1000 | Loss: 0.00001564
Iteration 205/1000 | Loss: 0.00001564
Iteration 206/1000 | Loss: 0.00001564
Iteration 207/1000 | Loss: 0.00001564
Iteration 208/1000 | Loss: 0.00001563
Iteration 209/1000 | Loss: 0.00001563
Iteration 210/1000 | Loss: 0.00001563
Iteration 211/1000 | Loss: 0.00001563
Iteration 212/1000 | Loss: 0.00001563
Iteration 213/1000 | Loss: 0.00001563
Iteration 214/1000 | Loss: 0.00001563
Iteration 215/1000 | Loss: 0.00001562
Iteration 216/1000 | Loss: 0.00001562
Iteration 217/1000 | Loss: 0.00001562
Iteration 218/1000 | Loss: 0.00001562
Iteration 219/1000 | Loss: 0.00001562
Iteration 220/1000 | Loss: 0.00001562
Iteration 221/1000 | Loss: 0.00001562
Iteration 222/1000 | Loss: 0.00001562
Iteration 223/1000 | Loss: 0.00001562
Iteration 224/1000 | Loss: 0.00001562
Iteration 225/1000 | Loss: 0.00001561
Iteration 226/1000 | Loss: 0.00001561
Iteration 227/1000 | Loss: 0.00001561
Iteration 228/1000 | Loss: 0.00001561
Iteration 229/1000 | Loss: 0.00001561
Iteration 230/1000 | Loss: 0.00001561
Iteration 231/1000 | Loss: 0.00001561
Iteration 232/1000 | Loss: 0.00001561
Iteration 233/1000 | Loss: 0.00001561
Iteration 234/1000 | Loss: 0.00001561
Iteration 235/1000 | Loss: 0.00001561
Iteration 236/1000 | Loss: 0.00001561
Iteration 237/1000 | Loss: 0.00001561
Iteration 238/1000 | Loss: 0.00001561
Iteration 239/1000 | Loss: 0.00001561
Iteration 240/1000 | Loss: 0.00001561
Iteration 241/1000 | Loss: 0.00001561
Iteration 242/1000 | Loss: 0.00001561
Iteration 243/1000 | Loss: 0.00001561
Iteration 244/1000 | Loss: 0.00001561
Iteration 245/1000 | Loss: 0.00001561
Iteration 246/1000 | Loss: 0.00001561
Iteration 247/1000 | Loss: 0.00001560
Iteration 248/1000 | Loss: 0.00001560
Iteration 249/1000 | Loss: 0.00001560
Iteration 250/1000 | Loss: 0.00001560
Iteration 251/1000 | Loss: 0.00001560
Iteration 252/1000 | Loss: 0.00001560
Iteration 253/1000 | Loss: 0.00001560
Iteration 254/1000 | Loss: 0.00001560
Iteration 255/1000 | Loss: 0.00001560
Iteration 256/1000 | Loss: 0.00001560
Iteration 257/1000 | Loss: 0.00001560
Iteration 258/1000 | Loss: 0.00001560
Iteration 259/1000 | Loss: 0.00001560
Iteration 260/1000 | Loss: 0.00001559
Iteration 261/1000 | Loss: 0.00001559
Iteration 262/1000 | Loss: 0.00001559
Iteration 263/1000 | Loss: 0.00001559
Iteration 264/1000 | Loss: 0.00001559
Iteration 265/1000 | Loss: 0.00001559
Iteration 266/1000 | Loss: 0.00001559
Iteration 267/1000 | Loss: 0.00001559
Iteration 268/1000 | Loss: 0.00001559
Iteration 269/1000 | Loss: 0.00001559
Iteration 270/1000 | Loss: 0.00001559
Iteration 271/1000 | Loss: 0.00001559
Iteration 272/1000 | Loss: 0.00001559
Iteration 273/1000 | Loss: 0.00001559
Iteration 274/1000 | Loss: 0.00001559
Iteration 275/1000 | Loss: 0.00001559
Iteration 276/1000 | Loss: 0.00001559
Iteration 277/1000 | Loss: 0.00001559
Iteration 278/1000 | Loss: 0.00001558
Iteration 279/1000 | Loss: 0.00001558
Iteration 280/1000 | Loss: 0.00001558
Iteration 281/1000 | Loss: 0.00001558
Iteration 282/1000 | Loss: 0.00001558
Iteration 283/1000 | Loss: 0.00001558
Iteration 284/1000 | Loss: 0.00001558
Iteration 285/1000 | Loss: 0.00001558
Iteration 286/1000 | Loss: 0.00001558
Iteration 287/1000 | Loss: 0.00001558
Iteration 288/1000 | Loss: 0.00001558
Iteration 289/1000 | Loss: 0.00001558
Iteration 290/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.558111034682952e-05, 1.558111034682952e-05, 1.558111034682952e-05, 1.558111034682952e-05, 1.558111034682952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558111034682952e-05

Optimization complete. Final v2v error: 3.302950859069824 mm

Highest mean error: 4.113461017608643 mm for frame 61

Lowest mean error: 2.664597749710083 mm for frame 42

Saving results

Total time: 209.89358520507812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802592
Iteration 2/25 | Loss: 0.00163988
Iteration 3/25 | Loss: 0.00130096
Iteration 4/25 | Loss: 0.00127948
Iteration 5/25 | Loss: 0.00127307
Iteration 6/25 | Loss: 0.00127117
Iteration 7/25 | Loss: 0.00127111
Iteration 8/25 | Loss: 0.00127111
Iteration 9/25 | Loss: 0.00127111
Iteration 10/25 | Loss: 0.00127111
Iteration 11/25 | Loss: 0.00127111
Iteration 12/25 | Loss: 0.00127111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001271109445951879, 0.001271109445951879, 0.001271109445951879, 0.001271109445951879, 0.001271109445951879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001271109445951879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26239181
Iteration 2/25 | Loss: 0.00091753
Iteration 3/25 | Loss: 0.00091753
Iteration 4/25 | Loss: 0.00091753
Iteration 5/25 | Loss: 0.00091753
Iteration 6/25 | Loss: 0.00091753
Iteration 7/25 | Loss: 0.00091753
Iteration 8/25 | Loss: 0.00091753
Iteration 9/25 | Loss: 0.00091753
Iteration 10/25 | Loss: 0.00091753
Iteration 11/25 | Loss: 0.00091753
Iteration 12/25 | Loss: 0.00091753
Iteration 13/25 | Loss: 0.00091753
Iteration 14/25 | Loss: 0.00091753
Iteration 15/25 | Loss: 0.00091753
Iteration 16/25 | Loss: 0.00091753
Iteration 17/25 | Loss: 0.00091753
Iteration 18/25 | Loss: 0.00091753
Iteration 19/25 | Loss: 0.00091753
Iteration 20/25 | Loss: 0.00091753
Iteration 21/25 | Loss: 0.00091753
Iteration 22/25 | Loss: 0.00091753
Iteration 23/25 | Loss: 0.00091753
Iteration 24/25 | Loss: 0.00091753
Iteration 25/25 | Loss: 0.00091753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091753
Iteration 2/1000 | Loss: 0.00006557
Iteration 3/1000 | Loss: 0.00004622
Iteration 4/1000 | Loss: 0.00004003
Iteration 5/1000 | Loss: 0.00003728
Iteration 6/1000 | Loss: 0.00003586
Iteration 7/1000 | Loss: 0.00003462
Iteration 8/1000 | Loss: 0.00003384
Iteration 9/1000 | Loss: 0.00003308
Iteration 10/1000 | Loss: 0.00003262
Iteration 11/1000 | Loss: 0.00003223
Iteration 12/1000 | Loss: 0.00003185
Iteration 13/1000 | Loss: 0.00003151
Iteration 14/1000 | Loss: 0.00003122
Iteration 15/1000 | Loss: 0.00003097
Iteration 16/1000 | Loss: 0.00003078
Iteration 17/1000 | Loss: 0.00003062
Iteration 18/1000 | Loss: 0.00003047
Iteration 19/1000 | Loss: 0.00003038
Iteration 20/1000 | Loss: 0.00003026
Iteration 21/1000 | Loss: 0.00003025
Iteration 22/1000 | Loss: 0.00003020
Iteration 23/1000 | Loss: 0.00003015
Iteration 24/1000 | Loss: 0.00003015
Iteration 25/1000 | Loss: 0.00003013
Iteration 26/1000 | Loss: 0.00003011
Iteration 27/1000 | Loss: 0.00003011
Iteration 28/1000 | Loss: 0.00003009
Iteration 29/1000 | Loss: 0.00003009
Iteration 30/1000 | Loss: 0.00003008
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003007
Iteration 33/1000 | Loss: 0.00003006
Iteration 34/1000 | Loss: 0.00003006
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003004
Iteration 39/1000 | Loss: 0.00003004
Iteration 40/1000 | Loss: 0.00003003
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003003
Iteration 43/1000 | Loss: 0.00003002
Iteration 44/1000 | Loss: 0.00003002
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003002
Iteration 47/1000 | Loss: 0.00003000
Iteration 48/1000 | Loss: 0.00003000
Iteration 49/1000 | Loss: 0.00002999
Iteration 50/1000 | Loss: 0.00002999
Iteration 51/1000 | Loss: 0.00002999
Iteration 52/1000 | Loss: 0.00002998
Iteration 53/1000 | Loss: 0.00002997
Iteration 54/1000 | Loss: 0.00002997
Iteration 55/1000 | Loss: 0.00002997
Iteration 56/1000 | Loss: 0.00002997
Iteration 57/1000 | Loss: 0.00002997
Iteration 58/1000 | Loss: 0.00002997
Iteration 59/1000 | Loss: 0.00002997
Iteration 60/1000 | Loss: 0.00002997
Iteration 61/1000 | Loss: 0.00002997
Iteration 62/1000 | Loss: 0.00002996
Iteration 63/1000 | Loss: 0.00002996
Iteration 64/1000 | Loss: 0.00002996
Iteration 65/1000 | Loss: 0.00002996
Iteration 66/1000 | Loss: 0.00002996
Iteration 67/1000 | Loss: 0.00002996
Iteration 68/1000 | Loss: 0.00002995
Iteration 69/1000 | Loss: 0.00002995
Iteration 70/1000 | Loss: 0.00002995
Iteration 71/1000 | Loss: 0.00002995
Iteration 72/1000 | Loss: 0.00002995
Iteration 73/1000 | Loss: 0.00002995
Iteration 74/1000 | Loss: 0.00002995
Iteration 75/1000 | Loss: 0.00002994
Iteration 76/1000 | Loss: 0.00002994
Iteration 77/1000 | Loss: 0.00002994
Iteration 78/1000 | Loss: 0.00002994
Iteration 79/1000 | Loss: 0.00002993
Iteration 80/1000 | Loss: 0.00002993
Iteration 81/1000 | Loss: 0.00002993
Iteration 82/1000 | Loss: 0.00002993
Iteration 83/1000 | Loss: 0.00002993
Iteration 84/1000 | Loss: 0.00002993
Iteration 85/1000 | Loss: 0.00002993
Iteration 86/1000 | Loss: 0.00002993
Iteration 87/1000 | Loss: 0.00002993
Iteration 88/1000 | Loss: 0.00002992
Iteration 89/1000 | Loss: 0.00002992
Iteration 90/1000 | Loss: 0.00002992
Iteration 91/1000 | Loss: 0.00002992
Iteration 92/1000 | Loss: 0.00002992
Iteration 93/1000 | Loss: 0.00002992
Iteration 94/1000 | Loss: 0.00002992
Iteration 95/1000 | Loss: 0.00002992
Iteration 96/1000 | Loss: 0.00002992
Iteration 97/1000 | Loss: 0.00002991
Iteration 98/1000 | Loss: 0.00002991
Iteration 99/1000 | Loss: 0.00002991
Iteration 100/1000 | Loss: 0.00002991
Iteration 101/1000 | Loss: 0.00002991
Iteration 102/1000 | Loss: 0.00002991
Iteration 103/1000 | Loss: 0.00002990
Iteration 104/1000 | Loss: 0.00002990
Iteration 105/1000 | Loss: 0.00002990
Iteration 106/1000 | Loss: 0.00002990
Iteration 107/1000 | Loss: 0.00002990
Iteration 108/1000 | Loss: 0.00002990
Iteration 109/1000 | Loss: 0.00002990
Iteration 110/1000 | Loss: 0.00002990
Iteration 111/1000 | Loss: 0.00002990
Iteration 112/1000 | Loss: 0.00002990
Iteration 113/1000 | Loss: 0.00002990
Iteration 114/1000 | Loss: 0.00002989
Iteration 115/1000 | Loss: 0.00002989
Iteration 116/1000 | Loss: 0.00002989
Iteration 117/1000 | Loss: 0.00002989
Iteration 118/1000 | Loss: 0.00002989
Iteration 119/1000 | Loss: 0.00002989
Iteration 120/1000 | Loss: 0.00002989
Iteration 121/1000 | Loss: 0.00002988
Iteration 122/1000 | Loss: 0.00002988
Iteration 123/1000 | Loss: 0.00002988
Iteration 124/1000 | Loss: 0.00002988
Iteration 125/1000 | Loss: 0.00002988
Iteration 126/1000 | Loss: 0.00002988
Iteration 127/1000 | Loss: 0.00002988
Iteration 128/1000 | Loss: 0.00002987
Iteration 129/1000 | Loss: 0.00002987
Iteration 130/1000 | Loss: 0.00002987
Iteration 131/1000 | Loss: 0.00002987
Iteration 132/1000 | Loss: 0.00002987
Iteration 133/1000 | Loss: 0.00002987
Iteration 134/1000 | Loss: 0.00002987
Iteration 135/1000 | Loss: 0.00002987
Iteration 136/1000 | Loss: 0.00002987
Iteration 137/1000 | Loss: 0.00002987
Iteration 138/1000 | Loss: 0.00002987
Iteration 139/1000 | Loss: 0.00002986
Iteration 140/1000 | Loss: 0.00002986
Iteration 141/1000 | Loss: 0.00002986
Iteration 142/1000 | Loss: 0.00002986
Iteration 143/1000 | Loss: 0.00002986
Iteration 144/1000 | Loss: 0.00002986
Iteration 145/1000 | Loss: 0.00002986
Iteration 146/1000 | Loss: 0.00002986
Iteration 147/1000 | Loss: 0.00002985
Iteration 148/1000 | Loss: 0.00002985
Iteration 149/1000 | Loss: 0.00002985
Iteration 150/1000 | Loss: 0.00002985
Iteration 151/1000 | Loss: 0.00002985
Iteration 152/1000 | Loss: 0.00002985
Iteration 153/1000 | Loss: 0.00002985
Iteration 154/1000 | Loss: 0.00002985
Iteration 155/1000 | Loss: 0.00002985
Iteration 156/1000 | Loss: 0.00002985
Iteration 157/1000 | Loss: 0.00002985
Iteration 158/1000 | Loss: 0.00002984
Iteration 159/1000 | Loss: 0.00002984
Iteration 160/1000 | Loss: 0.00002984
Iteration 161/1000 | Loss: 0.00002984
Iteration 162/1000 | Loss: 0.00002984
Iteration 163/1000 | Loss: 0.00002984
Iteration 164/1000 | Loss: 0.00002984
Iteration 165/1000 | Loss: 0.00002984
Iteration 166/1000 | Loss: 0.00002984
Iteration 167/1000 | Loss: 0.00002984
Iteration 168/1000 | Loss: 0.00002984
Iteration 169/1000 | Loss: 0.00002984
Iteration 170/1000 | Loss: 0.00002984
Iteration 171/1000 | Loss: 0.00002984
Iteration 172/1000 | Loss: 0.00002984
Iteration 173/1000 | Loss: 0.00002984
Iteration 174/1000 | Loss: 0.00002984
Iteration 175/1000 | Loss: 0.00002984
Iteration 176/1000 | Loss: 0.00002984
Iteration 177/1000 | Loss: 0.00002984
Iteration 178/1000 | Loss: 0.00002984
Iteration 179/1000 | Loss: 0.00002984
Iteration 180/1000 | Loss: 0.00002984
Iteration 181/1000 | Loss: 0.00002984
Iteration 182/1000 | Loss: 0.00002984
Iteration 183/1000 | Loss: 0.00002984
Iteration 184/1000 | Loss: 0.00002984
Iteration 185/1000 | Loss: 0.00002984
Iteration 186/1000 | Loss: 0.00002984
Iteration 187/1000 | Loss: 0.00002984
Iteration 188/1000 | Loss: 0.00002984
Iteration 189/1000 | Loss: 0.00002984
Iteration 190/1000 | Loss: 0.00002984
Iteration 191/1000 | Loss: 0.00002984
Iteration 192/1000 | Loss: 0.00002984
Iteration 193/1000 | Loss: 0.00002984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.984319689858239e-05, 2.984319689858239e-05, 2.984319689858239e-05, 2.984319689858239e-05, 2.984319689858239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.984319689858239e-05

Optimization complete. Final v2v error: 4.356485366821289 mm

Highest mean error: 5.668169975280762 mm for frame 153

Lowest mean error: 3.270139694213867 mm for frame 15

Saving results

Total time: 48.726637840270996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595122
Iteration 2/25 | Loss: 0.00118512
Iteration 3/25 | Loss: 0.00108053
Iteration 4/25 | Loss: 0.00106882
Iteration 5/25 | Loss: 0.00106645
Iteration 6/25 | Loss: 0.00106645
Iteration 7/25 | Loss: 0.00106645
Iteration 8/25 | Loss: 0.00106645
Iteration 9/25 | Loss: 0.00106645
Iteration 10/25 | Loss: 0.00106645
Iteration 11/25 | Loss: 0.00106645
Iteration 12/25 | Loss: 0.00106645
Iteration 13/25 | Loss: 0.00106645
Iteration 14/25 | Loss: 0.00106645
Iteration 15/25 | Loss: 0.00106645
Iteration 16/25 | Loss: 0.00106645
Iteration 17/25 | Loss: 0.00106645
Iteration 18/25 | Loss: 0.00106645
Iteration 19/25 | Loss: 0.00106645
Iteration 20/25 | Loss: 0.00106645
Iteration 21/25 | Loss: 0.00106645
Iteration 22/25 | Loss: 0.00106645
Iteration 23/25 | Loss: 0.00106645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010664467699825764, 0.0010664467699825764, 0.0010664467699825764, 0.0010664467699825764, 0.0010664467699825764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010664467699825764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57236528
Iteration 2/25 | Loss: 0.00048769
Iteration 3/25 | Loss: 0.00048768
Iteration 4/25 | Loss: 0.00048768
Iteration 5/25 | Loss: 0.00048767
Iteration 6/25 | Loss: 0.00048767
Iteration 7/25 | Loss: 0.00048767
Iteration 8/25 | Loss: 0.00048767
Iteration 9/25 | Loss: 0.00048767
Iteration 10/25 | Loss: 0.00048767
Iteration 11/25 | Loss: 0.00048767
Iteration 12/25 | Loss: 0.00048767
Iteration 13/25 | Loss: 0.00048767
Iteration 14/25 | Loss: 0.00048767
Iteration 15/25 | Loss: 0.00048767
Iteration 16/25 | Loss: 0.00048767
Iteration 17/25 | Loss: 0.00048767
Iteration 18/25 | Loss: 0.00048767
Iteration 19/25 | Loss: 0.00048767
Iteration 20/25 | Loss: 0.00048767
Iteration 21/25 | Loss: 0.00048767
Iteration 22/25 | Loss: 0.00048767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004876732418779284, 0.0004876732418779284, 0.0004876732418779284, 0.0004876732418779284, 0.0004876732418779284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004876732418779284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048767
Iteration 2/1000 | Loss: 0.00001906
Iteration 3/1000 | Loss: 0.00001467
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001302
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001161
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001160
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001159
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001159
Iteration 59/1000 | Loss: 0.00001159
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001157
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001156
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001148
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1428914149291813e-05, 1.1428914149291813e-05, 1.1428914149291813e-05, 1.1428914149291813e-05, 1.1428914149291813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1428914149291813e-05

Optimization complete. Final v2v error: 2.855893850326538 mm

Highest mean error: 3.017383098602295 mm for frame 168

Lowest mean error: 2.6986124515533447 mm for frame 128

Saving results

Total time: 36.41246724128723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804145
Iteration 2/25 | Loss: 0.00131271
Iteration 3/25 | Loss: 0.00107721
Iteration 4/25 | Loss: 0.00106145
Iteration 5/25 | Loss: 0.00105583
Iteration 6/25 | Loss: 0.00105407
Iteration 7/25 | Loss: 0.00105390
Iteration 8/25 | Loss: 0.00105390
Iteration 9/25 | Loss: 0.00105390
Iteration 10/25 | Loss: 0.00105390
Iteration 11/25 | Loss: 0.00105390
Iteration 12/25 | Loss: 0.00105390
Iteration 13/25 | Loss: 0.00105390
Iteration 14/25 | Loss: 0.00105390
Iteration 15/25 | Loss: 0.00105390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010539022041484714, 0.0010539022041484714, 0.0010539022041484714, 0.0010539022041484714, 0.0010539022041484714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010539022041484714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19407511
Iteration 2/25 | Loss: 0.00078530
Iteration 3/25 | Loss: 0.00078529
Iteration 4/25 | Loss: 0.00078529
Iteration 5/25 | Loss: 0.00078529
Iteration 6/25 | Loss: 0.00078529
Iteration 7/25 | Loss: 0.00078529
Iteration 8/25 | Loss: 0.00078529
Iteration 9/25 | Loss: 0.00078529
Iteration 10/25 | Loss: 0.00078529
Iteration 11/25 | Loss: 0.00078529
Iteration 12/25 | Loss: 0.00078529
Iteration 13/25 | Loss: 0.00078529
Iteration 14/25 | Loss: 0.00078529
Iteration 15/25 | Loss: 0.00078529
Iteration 16/25 | Loss: 0.00078529
Iteration 17/25 | Loss: 0.00078529
Iteration 18/25 | Loss: 0.00078529
Iteration 19/25 | Loss: 0.00078529
Iteration 20/25 | Loss: 0.00078529
Iteration 21/25 | Loss: 0.00078529
Iteration 22/25 | Loss: 0.00078529
Iteration 23/25 | Loss: 0.00078529
Iteration 24/25 | Loss: 0.00078529
Iteration 25/25 | Loss: 0.00078529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078529
Iteration 2/1000 | Loss: 0.00004456
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001378
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001227
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001206
Iteration 142/1000 | Loss: 0.00001206
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001204
Iteration 154/1000 | Loss: 0.00001204
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001203
Iteration 161/1000 | Loss: 0.00001203
Iteration 162/1000 | Loss: 0.00001203
Iteration 163/1000 | Loss: 0.00001203
Iteration 164/1000 | Loss: 0.00001203
Iteration 165/1000 | Loss: 0.00001203
Iteration 166/1000 | Loss: 0.00001203
Iteration 167/1000 | Loss: 0.00001203
Iteration 168/1000 | Loss: 0.00001203
Iteration 169/1000 | Loss: 0.00001202
Iteration 170/1000 | Loss: 0.00001202
Iteration 171/1000 | Loss: 0.00001202
Iteration 172/1000 | Loss: 0.00001202
Iteration 173/1000 | Loss: 0.00001202
Iteration 174/1000 | Loss: 0.00001202
Iteration 175/1000 | Loss: 0.00001202
Iteration 176/1000 | Loss: 0.00001202
Iteration 177/1000 | Loss: 0.00001202
Iteration 178/1000 | Loss: 0.00001202
Iteration 179/1000 | Loss: 0.00001201
Iteration 180/1000 | Loss: 0.00001201
Iteration 181/1000 | Loss: 0.00001201
Iteration 182/1000 | Loss: 0.00001201
Iteration 183/1000 | Loss: 0.00001201
Iteration 184/1000 | Loss: 0.00001201
Iteration 185/1000 | Loss: 0.00001201
Iteration 186/1000 | Loss: 0.00001201
Iteration 187/1000 | Loss: 0.00001201
Iteration 188/1000 | Loss: 0.00001201
Iteration 189/1000 | Loss: 0.00001201
Iteration 190/1000 | Loss: 0.00001201
Iteration 191/1000 | Loss: 0.00001201
Iteration 192/1000 | Loss: 0.00001201
Iteration 193/1000 | Loss: 0.00001201
Iteration 194/1000 | Loss: 0.00001201
Iteration 195/1000 | Loss: 0.00001201
Iteration 196/1000 | Loss: 0.00001201
Iteration 197/1000 | Loss: 0.00001201
Iteration 198/1000 | Loss: 0.00001201
Iteration 199/1000 | Loss: 0.00001201
Iteration 200/1000 | Loss: 0.00001201
Iteration 201/1000 | Loss: 0.00001201
Iteration 202/1000 | Loss: 0.00001201
Iteration 203/1000 | Loss: 0.00001201
Iteration 204/1000 | Loss: 0.00001201
Iteration 205/1000 | Loss: 0.00001201
Iteration 206/1000 | Loss: 0.00001201
Iteration 207/1000 | Loss: 0.00001201
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.2006224096694496e-05, 1.2006224096694496e-05, 1.2006224096694496e-05, 1.2006224096694496e-05, 1.2006224096694496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2006224096694496e-05

Optimization complete. Final v2v error: 2.809654474258423 mm

Highest mean error: 4.170543193817139 mm for frame 62

Lowest mean error: 2.2833399772644043 mm for frame 91

Saving results

Total time: 43.49522686004639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462775
Iteration 2/25 | Loss: 0.00132348
Iteration 3/25 | Loss: 0.00111837
Iteration 4/25 | Loss: 0.00109687
Iteration 5/25 | Loss: 0.00109092
Iteration 6/25 | Loss: 0.00108945
Iteration 7/25 | Loss: 0.00108945
Iteration 8/25 | Loss: 0.00108945
Iteration 9/25 | Loss: 0.00108945
Iteration 10/25 | Loss: 0.00108945
Iteration 11/25 | Loss: 0.00108945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010894520673900843, 0.0010894520673900843, 0.0010894520673900843, 0.0010894520673900843, 0.0010894520673900843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010894520673900843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38552070
Iteration 2/25 | Loss: 0.00086337
Iteration 3/25 | Loss: 0.00086337
Iteration 4/25 | Loss: 0.00086337
Iteration 5/25 | Loss: 0.00086337
Iteration 6/25 | Loss: 0.00086337
Iteration 7/25 | Loss: 0.00086337
Iteration 8/25 | Loss: 0.00086337
Iteration 9/25 | Loss: 0.00086337
Iteration 10/25 | Loss: 0.00086337
Iteration 11/25 | Loss: 0.00086337
Iteration 12/25 | Loss: 0.00086337
Iteration 13/25 | Loss: 0.00086337
Iteration 14/25 | Loss: 0.00086337
Iteration 15/25 | Loss: 0.00086337
Iteration 16/25 | Loss: 0.00086337
Iteration 17/25 | Loss: 0.00086337
Iteration 18/25 | Loss: 0.00086337
Iteration 19/25 | Loss: 0.00086337
Iteration 20/25 | Loss: 0.00086337
Iteration 21/25 | Loss: 0.00086337
Iteration 22/25 | Loss: 0.00086337
Iteration 23/25 | Loss: 0.00086337
Iteration 24/25 | Loss: 0.00086337
Iteration 25/25 | Loss: 0.00086337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086337
Iteration 2/1000 | Loss: 0.00003298
Iteration 3/1000 | Loss: 0.00002182
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001701
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001651
Iteration 12/1000 | Loss: 0.00001639
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001632
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001630
Iteration 18/1000 | Loss: 0.00001629
Iteration 19/1000 | Loss: 0.00001623
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001611
Iteration 27/1000 | Loss: 0.00001610
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001599
Iteration 37/1000 | Loss: 0.00001599
Iteration 38/1000 | Loss: 0.00001599
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001587
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001586
Iteration 65/1000 | Loss: 0.00001586
Iteration 66/1000 | Loss: 0.00001586
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001586
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001585
Iteration 72/1000 | Loss: 0.00001584
Iteration 73/1000 | Loss: 0.00001584
Iteration 74/1000 | Loss: 0.00001584
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001581
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001578
Iteration 93/1000 | Loss: 0.00001578
Iteration 94/1000 | Loss: 0.00001578
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001578
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00001576
Iteration 108/1000 | Loss: 0.00001576
Iteration 109/1000 | Loss: 0.00001576
Iteration 110/1000 | Loss: 0.00001576
Iteration 111/1000 | Loss: 0.00001576
Iteration 112/1000 | Loss: 0.00001576
Iteration 113/1000 | Loss: 0.00001576
Iteration 114/1000 | Loss: 0.00001575
Iteration 115/1000 | Loss: 0.00001575
Iteration 116/1000 | Loss: 0.00001575
Iteration 117/1000 | Loss: 0.00001575
Iteration 118/1000 | Loss: 0.00001575
Iteration 119/1000 | Loss: 0.00001574
Iteration 120/1000 | Loss: 0.00001574
Iteration 121/1000 | Loss: 0.00001574
Iteration 122/1000 | Loss: 0.00001574
Iteration 123/1000 | Loss: 0.00001574
Iteration 124/1000 | Loss: 0.00001574
Iteration 125/1000 | Loss: 0.00001574
Iteration 126/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5741043171146885e-05, 1.5741043171146885e-05, 1.5741043171146885e-05, 1.5741043171146885e-05, 1.5741043171146885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5741043171146885e-05

Optimization complete. Final v2v error: 3.2579472064971924 mm

Highest mean error: 4.185154914855957 mm for frame 215

Lowest mean error: 2.486968994140625 mm for frame 19

Saving results

Total time: 43.22766828536987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_012/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_012/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833765
Iteration 2/25 | Loss: 0.00142252
Iteration 3/25 | Loss: 0.00111658
Iteration 4/25 | Loss: 0.00109111
Iteration 5/25 | Loss: 0.00108685
Iteration 6/25 | Loss: 0.00108635
Iteration 7/25 | Loss: 0.00108635
Iteration 8/25 | Loss: 0.00108635
Iteration 9/25 | Loss: 0.00108635
Iteration 10/25 | Loss: 0.00108635
Iteration 11/25 | Loss: 0.00108635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010863477364182472, 0.0010863477364182472, 0.0010863477364182472, 0.0010863477364182472, 0.0010863477364182472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010863477364182472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37674379
Iteration 2/25 | Loss: 0.00057092
Iteration 3/25 | Loss: 0.00057090
Iteration 4/25 | Loss: 0.00057090
Iteration 5/25 | Loss: 0.00057090
Iteration 6/25 | Loss: 0.00057090
Iteration 7/25 | Loss: 0.00057090
Iteration 8/25 | Loss: 0.00057090
Iteration 9/25 | Loss: 0.00057090
Iteration 10/25 | Loss: 0.00057090
Iteration 11/25 | Loss: 0.00057090
Iteration 12/25 | Loss: 0.00057090
Iteration 13/25 | Loss: 0.00057090
Iteration 14/25 | Loss: 0.00057090
Iteration 15/25 | Loss: 0.00057090
Iteration 16/25 | Loss: 0.00057090
Iteration 17/25 | Loss: 0.00057090
Iteration 18/25 | Loss: 0.00057090
Iteration 19/25 | Loss: 0.00057090
Iteration 20/25 | Loss: 0.00057090
Iteration 21/25 | Loss: 0.00057090
Iteration 22/25 | Loss: 0.00057090
Iteration 23/25 | Loss: 0.00057090
Iteration 24/25 | Loss: 0.00057090
Iteration 25/25 | Loss: 0.00057090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057090
Iteration 2/1000 | Loss: 0.00001853
Iteration 3/1000 | Loss: 0.00001404
Iteration 4/1000 | Loss: 0.00001282
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001167
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001114
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001108
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001086
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001081
Iteration 40/1000 | Loss: 0.00001081
Iteration 41/1000 | Loss: 0.00001080
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001076
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001069
Iteration 71/1000 | Loss: 0.00001069
Iteration 72/1000 | Loss: 0.00001069
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001065
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001065
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001065
Iteration 104/1000 | Loss: 0.00001065
Iteration 105/1000 | Loss: 0.00001065
Iteration 106/1000 | Loss: 0.00001065
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.0654879588400945e-05, 1.0654879588400945e-05, 1.0654879588400945e-05, 1.0654879588400945e-05, 1.0654879588400945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0654879588400945e-05

Optimization complete. Final v2v error: 2.7916927337646484 mm

Highest mean error: 3.1443469524383545 mm for frame 118

Lowest mean error: 2.325665235519409 mm for frame 20

Saving results

Total time: 32.96096158027649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417971
Iteration 2/25 | Loss: 0.00139137
Iteration 3/25 | Loss: 0.00129880
Iteration 4/25 | Loss: 0.00128723
Iteration 5/25 | Loss: 0.00128496
Iteration 6/25 | Loss: 0.00128426
Iteration 7/25 | Loss: 0.00128426
Iteration 8/25 | Loss: 0.00128426
Iteration 9/25 | Loss: 0.00128426
Iteration 10/25 | Loss: 0.00128426
Iteration 11/25 | Loss: 0.00128426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001284259487874806, 0.001284259487874806, 0.001284259487874806, 0.001284259487874806, 0.001284259487874806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284259487874806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13373375
Iteration 2/25 | Loss: 0.00332126
Iteration 3/25 | Loss: 0.00332126
Iteration 4/25 | Loss: 0.00332126
Iteration 5/25 | Loss: 0.00332126
Iteration 6/25 | Loss: 0.00332126
Iteration 7/25 | Loss: 0.00332126
Iteration 8/25 | Loss: 0.00332126
Iteration 9/25 | Loss: 0.00332126
Iteration 10/25 | Loss: 0.00332126
Iteration 11/25 | Loss: 0.00332126
Iteration 12/25 | Loss: 0.00332126
Iteration 13/25 | Loss: 0.00332126
Iteration 14/25 | Loss: 0.00332126
Iteration 15/25 | Loss: 0.00332126
Iteration 16/25 | Loss: 0.00332126
Iteration 17/25 | Loss: 0.00332126
Iteration 18/25 | Loss: 0.00332126
Iteration 19/25 | Loss: 0.00332126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003321256721392274, 0.003321256721392274, 0.003321256721392274, 0.003321256721392274, 0.003321256721392274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003321256721392274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332126
Iteration 2/1000 | Loss: 0.00007737
Iteration 3/1000 | Loss: 0.00004096
Iteration 4/1000 | Loss: 0.00002994
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002219
Iteration 8/1000 | Loss: 0.00002157
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002042
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001940
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001874
Iteration 18/1000 | Loss: 0.00001872
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001871
Iteration 21/1000 | Loss: 0.00001870
Iteration 22/1000 | Loss: 0.00001869
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001865
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001864
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001845
Iteration 64/1000 | Loss: 0.00001845
Iteration 65/1000 | Loss: 0.00001845
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001833
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001832
Iteration 147/1000 | Loss: 0.00001832
Iteration 148/1000 | Loss: 0.00001832
Iteration 149/1000 | Loss: 0.00001832
Iteration 150/1000 | Loss: 0.00001832
Iteration 151/1000 | Loss: 0.00001832
Iteration 152/1000 | Loss: 0.00001832
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001831
Iteration 157/1000 | Loss: 0.00001831
Iteration 158/1000 | Loss: 0.00001831
Iteration 159/1000 | Loss: 0.00001831
Iteration 160/1000 | Loss: 0.00001831
Iteration 161/1000 | Loss: 0.00001831
Iteration 162/1000 | Loss: 0.00001831
Iteration 163/1000 | Loss: 0.00001831
Iteration 164/1000 | Loss: 0.00001831
Iteration 165/1000 | Loss: 0.00001831
Iteration 166/1000 | Loss: 0.00001831
Iteration 167/1000 | Loss: 0.00001831
Iteration 168/1000 | Loss: 0.00001831
Iteration 169/1000 | Loss: 0.00001831
Iteration 170/1000 | Loss: 0.00001831
Iteration 171/1000 | Loss: 0.00001831
Iteration 172/1000 | Loss: 0.00001831
Iteration 173/1000 | Loss: 0.00001831
Iteration 174/1000 | Loss: 0.00001831
Iteration 175/1000 | Loss: 0.00001831
Iteration 176/1000 | Loss: 0.00001831
Iteration 177/1000 | Loss: 0.00001831
Iteration 178/1000 | Loss: 0.00001831
Iteration 179/1000 | Loss: 0.00001831
Iteration 180/1000 | Loss: 0.00001831
Iteration 181/1000 | Loss: 0.00001831
Iteration 182/1000 | Loss: 0.00001831
Iteration 183/1000 | Loss: 0.00001831
Iteration 184/1000 | Loss: 0.00001831
Iteration 185/1000 | Loss: 0.00001831
Iteration 186/1000 | Loss: 0.00001831
Iteration 187/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.831387635320425e-05, 1.831387635320425e-05, 1.831387635320425e-05, 1.831387635320425e-05, 1.831387635320425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.831387635320425e-05

Optimization complete. Final v2v error: 3.657874584197998 mm

Highest mean error: 3.900928258895874 mm for frame 107

Lowest mean error: 3.3832967281341553 mm for frame 136

Saving results

Total time: 41.88694477081299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853726
Iteration 2/25 | Loss: 0.00137170
Iteration 3/25 | Loss: 0.00125702
Iteration 4/25 | Loss: 0.00124390
Iteration 5/25 | Loss: 0.00124046
Iteration 6/25 | Loss: 0.00123984
Iteration 7/25 | Loss: 0.00123984
Iteration 8/25 | Loss: 0.00123984
Iteration 9/25 | Loss: 0.00123984
Iteration 10/25 | Loss: 0.00123984
Iteration 11/25 | Loss: 0.00123984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001239843899384141, 0.001239843899384141, 0.001239843899384141, 0.001239843899384141, 0.001239843899384141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239843899384141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16590929
Iteration 2/25 | Loss: 0.00288113
Iteration 3/25 | Loss: 0.00288113
Iteration 4/25 | Loss: 0.00288113
Iteration 5/25 | Loss: 0.00288113
Iteration 6/25 | Loss: 0.00288113
Iteration 7/25 | Loss: 0.00288113
Iteration 8/25 | Loss: 0.00288113
Iteration 9/25 | Loss: 0.00288113
Iteration 10/25 | Loss: 0.00288113
Iteration 11/25 | Loss: 0.00288113
Iteration 12/25 | Loss: 0.00288113
Iteration 13/25 | Loss: 0.00288113
Iteration 14/25 | Loss: 0.00288113
Iteration 15/25 | Loss: 0.00288113
Iteration 16/25 | Loss: 0.00288113
Iteration 17/25 | Loss: 0.00288113
Iteration 18/25 | Loss: 0.00288113
Iteration 19/25 | Loss: 0.00288113
Iteration 20/25 | Loss: 0.00288113
Iteration 21/25 | Loss: 0.00288113
Iteration 22/25 | Loss: 0.00288113
Iteration 23/25 | Loss: 0.00288113
Iteration 24/25 | Loss: 0.00288113
Iteration 25/25 | Loss: 0.00288113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288113
Iteration 2/1000 | Loss: 0.00005578
Iteration 3/1000 | Loss: 0.00002933
Iteration 4/1000 | Loss: 0.00002284
Iteration 5/1000 | Loss: 0.00002119
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001626
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001620
Iteration 23/1000 | Loss: 0.00001619
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001619
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001608
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6083100490504876e-05, 1.6083100490504876e-05, 1.6083100490504876e-05, 1.6083100490504876e-05, 1.6083100490504876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6083100490504876e-05

Optimization complete. Final v2v error: 3.423415422439575 mm

Highest mean error: 3.6231849193573 mm for frame 45

Lowest mean error: 3.270986318588257 mm for frame 140

Saving results

Total time: 35.60810351371765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019281
Iteration 2/25 | Loss: 0.00187365
Iteration 3/25 | Loss: 0.00160776
Iteration 4/25 | Loss: 0.00160258
Iteration 5/25 | Loss: 0.00155708
Iteration 6/25 | Loss: 0.00151959
Iteration 7/25 | Loss: 0.00150838
Iteration 8/25 | Loss: 0.00150713
Iteration 9/25 | Loss: 0.00149546
Iteration 10/25 | Loss: 0.00149294
Iteration 11/25 | Loss: 0.00147943
Iteration 12/25 | Loss: 0.00147107
Iteration 13/25 | Loss: 0.00146846
Iteration 14/25 | Loss: 0.00146766
Iteration 15/25 | Loss: 0.00146714
Iteration 16/25 | Loss: 0.00146679
Iteration 17/25 | Loss: 0.00146655
Iteration 18/25 | Loss: 0.00146644
Iteration 19/25 | Loss: 0.00146643
Iteration 20/25 | Loss: 0.00146643
Iteration 21/25 | Loss: 0.00146643
Iteration 22/25 | Loss: 0.00146643
Iteration 23/25 | Loss: 0.00146642
Iteration 24/25 | Loss: 0.00146642
Iteration 25/25 | Loss: 0.00146642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.17759559
Iteration 2/25 | Loss: 0.00264480
Iteration 3/25 | Loss: 0.00264480
Iteration 4/25 | Loss: 0.00264480
Iteration 5/25 | Loss: 0.00264480
Iteration 6/25 | Loss: 0.00264480
Iteration 7/25 | Loss: 0.00264480
Iteration 8/25 | Loss: 0.00264480
Iteration 9/25 | Loss: 0.00264480
Iteration 10/25 | Loss: 0.00264480
Iteration 11/25 | Loss: 0.00264480
Iteration 12/25 | Loss: 0.00264480
Iteration 13/25 | Loss: 0.00264480
Iteration 14/25 | Loss: 0.00264480
Iteration 15/25 | Loss: 0.00264480
Iteration 16/25 | Loss: 0.00264480
Iteration 17/25 | Loss: 0.00264480
Iteration 18/25 | Loss: 0.00264480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0026447977870702744, 0.0026447977870702744, 0.0026447977870702744, 0.0026447977870702744, 0.0026447977870702744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026447977870702744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264480
Iteration 2/1000 | Loss: 0.00019573
Iteration 3/1000 | Loss: 0.00013275
Iteration 4/1000 | Loss: 0.00010463
Iteration 5/1000 | Loss: 0.00028597
Iteration 6/1000 | Loss: 0.00008014
Iteration 7/1000 | Loss: 0.00014492
Iteration 8/1000 | Loss: 0.00007209
Iteration 9/1000 | Loss: 0.00006611
Iteration 10/1000 | Loss: 0.00006169
Iteration 11/1000 | Loss: 0.00005804
Iteration 12/1000 | Loss: 0.00005547
Iteration 13/1000 | Loss: 0.00005386
Iteration 14/1000 | Loss: 0.00005283
Iteration 15/1000 | Loss: 0.00005203
Iteration 16/1000 | Loss: 0.00005911
Iteration 17/1000 | Loss: 0.00005226
Iteration 18/1000 | Loss: 0.00005054
Iteration 19/1000 | Loss: 0.00004989
Iteration 20/1000 | Loss: 0.00004950
Iteration 21/1000 | Loss: 0.00004926
Iteration 22/1000 | Loss: 0.00004903
Iteration 23/1000 | Loss: 0.00004884
Iteration 24/1000 | Loss: 0.00004879
Iteration 25/1000 | Loss: 0.00004878
Iteration 26/1000 | Loss: 0.00004875
Iteration 27/1000 | Loss: 0.00004868
Iteration 28/1000 | Loss: 0.00004866
Iteration 29/1000 | Loss: 0.00004850
Iteration 30/1000 | Loss: 0.00004847
Iteration 31/1000 | Loss: 0.00004833
Iteration 32/1000 | Loss: 0.00004833
Iteration 33/1000 | Loss: 0.00004826
Iteration 34/1000 | Loss: 0.00004826
Iteration 35/1000 | Loss: 0.00004826
Iteration 36/1000 | Loss: 0.00004824
Iteration 37/1000 | Loss: 0.00004824
Iteration 38/1000 | Loss: 0.00004823
Iteration 39/1000 | Loss: 0.00004823
Iteration 40/1000 | Loss: 0.00004823
Iteration 41/1000 | Loss: 0.00004823
Iteration 42/1000 | Loss: 0.00004822
Iteration 43/1000 | Loss: 0.00004822
Iteration 44/1000 | Loss: 0.00004822
Iteration 45/1000 | Loss: 0.00004821
Iteration 46/1000 | Loss: 0.00004820
Iteration 47/1000 | Loss: 0.00004819
Iteration 48/1000 | Loss: 0.00004819
Iteration 49/1000 | Loss: 0.00004819
Iteration 50/1000 | Loss: 0.00004819
Iteration 51/1000 | Loss: 0.00004819
Iteration 52/1000 | Loss: 0.00004819
Iteration 53/1000 | Loss: 0.00004819
Iteration 54/1000 | Loss: 0.00004819
Iteration 55/1000 | Loss: 0.00004819
Iteration 56/1000 | Loss: 0.00004818
Iteration 57/1000 | Loss: 0.00004818
Iteration 58/1000 | Loss: 0.00004818
Iteration 59/1000 | Loss: 0.00004818
Iteration 60/1000 | Loss: 0.00004818
Iteration 61/1000 | Loss: 0.00004817
Iteration 62/1000 | Loss: 0.00004816
Iteration 63/1000 | Loss: 0.00004816
Iteration 64/1000 | Loss: 0.00004816
Iteration 65/1000 | Loss: 0.00004815
Iteration 66/1000 | Loss: 0.00004815
Iteration 67/1000 | Loss: 0.00004815
Iteration 68/1000 | Loss: 0.00004815
Iteration 69/1000 | Loss: 0.00004815
Iteration 70/1000 | Loss: 0.00004815
Iteration 71/1000 | Loss: 0.00004814
Iteration 72/1000 | Loss: 0.00004814
Iteration 73/1000 | Loss: 0.00004814
Iteration 74/1000 | Loss: 0.00004814
Iteration 75/1000 | Loss: 0.00004813
Iteration 76/1000 | Loss: 0.00004813
Iteration 77/1000 | Loss: 0.00004813
Iteration 78/1000 | Loss: 0.00004813
Iteration 79/1000 | Loss: 0.00004812
Iteration 80/1000 | Loss: 0.00004812
Iteration 81/1000 | Loss: 0.00004812
Iteration 82/1000 | Loss: 0.00004812
Iteration 83/1000 | Loss: 0.00004812
Iteration 84/1000 | Loss: 0.00004812
Iteration 85/1000 | Loss: 0.00004812
Iteration 86/1000 | Loss: 0.00004811
Iteration 87/1000 | Loss: 0.00004811
Iteration 88/1000 | Loss: 0.00004811
Iteration 89/1000 | Loss: 0.00004811
Iteration 90/1000 | Loss: 0.00004811
Iteration 91/1000 | Loss: 0.00004811
Iteration 92/1000 | Loss: 0.00004811
Iteration 93/1000 | Loss: 0.00004811
Iteration 94/1000 | Loss: 0.00004810
Iteration 95/1000 | Loss: 0.00004810
Iteration 96/1000 | Loss: 0.00004810
Iteration 97/1000 | Loss: 0.00004810
Iteration 98/1000 | Loss: 0.00004810
Iteration 99/1000 | Loss: 0.00004810
Iteration 100/1000 | Loss: 0.00004810
Iteration 101/1000 | Loss: 0.00004810
Iteration 102/1000 | Loss: 0.00004810
Iteration 103/1000 | Loss: 0.00004809
Iteration 104/1000 | Loss: 0.00004809
Iteration 105/1000 | Loss: 0.00004809
Iteration 106/1000 | Loss: 0.00004809
Iteration 107/1000 | Loss: 0.00004809
Iteration 108/1000 | Loss: 0.00004809
Iteration 109/1000 | Loss: 0.00004809
Iteration 110/1000 | Loss: 0.00004809
Iteration 111/1000 | Loss: 0.00004808
Iteration 112/1000 | Loss: 0.00004808
Iteration 113/1000 | Loss: 0.00004808
Iteration 114/1000 | Loss: 0.00004808
Iteration 115/1000 | Loss: 0.00004807
Iteration 116/1000 | Loss: 0.00004807
Iteration 117/1000 | Loss: 0.00004807
Iteration 118/1000 | Loss: 0.00004807
Iteration 119/1000 | Loss: 0.00004807
Iteration 120/1000 | Loss: 0.00004807
Iteration 121/1000 | Loss: 0.00004807
Iteration 122/1000 | Loss: 0.00004807
Iteration 123/1000 | Loss: 0.00004807
Iteration 124/1000 | Loss: 0.00004807
Iteration 125/1000 | Loss: 0.00004807
Iteration 126/1000 | Loss: 0.00004807
Iteration 127/1000 | Loss: 0.00004807
Iteration 128/1000 | Loss: 0.00004807
Iteration 129/1000 | Loss: 0.00004807
Iteration 130/1000 | Loss: 0.00004807
Iteration 131/1000 | Loss: 0.00004807
Iteration 132/1000 | Loss: 0.00004807
Iteration 133/1000 | Loss: 0.00004807
Iteration 134/1000 | Loss: 0.00004807
Iteration 135/1000 | Loss: 0.00004807
Iteration 136/1000 | Loss: 0.00004807
Iteration 137/1000 | Loss: 0.00004807
Iteration 138/1000 | Loss: 0.00004807
Iteration 139/1000 | Loss: 0.00004807
Iteration 140/1000 | Loss: 0.00004807
Iteration 141/1000 | Loss: 0.00004807
Iteration 142/1000 | Loss: 0.00004807
Iteration 143/1000 | Loss: 0.00004807
Iteration 144/1000 | Loss: 0.00004807
Iteration 145/1000 | Loss: 0.00004807
Iteration 146/1000 | Loss: 0.00004807
Iteration 147/1000 | Loss: 0.00004807
Iteration 148/1000 | Loss: 0.00004807
Iteration 149/1000 | Loss: 0.00004807
Iteration 150/1000 | Loss: 0.00004807
Iteration 151/1000 | Loss: 0.00004807
Iteration 152/1000 | Loss: 0.00004807
Iteration 153/1000 | Loss: 0.00004807
Iteration 154/1000 | Loss: 0.00004807
Iteration 155/1000 | Loss: 0.00004807
Iteration 156/1000 | Loss: 0.00004807
Iteration 157/1000 | Loss: 0.00004807
Iteration 158/1000 | Loss: 0.00004807
Iteration 159/1000 | Loss: 0.00004807
Iteration 160/1000 | Loss: 0.00004807
Iteration 161/1000 | Loss: 0.00004807
Iteration 162/1000 | Loss: 0.00004807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [4.807019286090508e-05, 4.807019286090508e-05, 4.807019286090508e-05, 4.807019286090508e-05, 4.807019286090508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.807019286090508e-05

Optimization complete. Final v2v error: 5.266777992248535 mm

Highest mean error: 13.54273509979248 mm for frame 25

Lowest mean error: 4.6304755210876465 mm for frame 116

Saving results

Total time: 73.58220958709717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940288
Iteration 2/25 | Loss: 0.00416185
Iteration 3/25 | Loss: 0.00355172
Iteration 4/25 | Loss: 0.00355202
Iteration 5/25 | Loss: 0.00325740
Iteration 6/25 | Loss: 0.00313973
Iteration 7/25 | Loss: 0.00320768
Iteration 8/25 | Loss: 0.00313313
Iteration 9/25 | Loss: 0.00287905
Iteration 10/25 | Loss: 0.00288852
Iteration 11/25 | Loss: 0.00273022
Iteration 12/25 | Loss: 0.00266056
Iteration 13/25 | Loss: 0.00266170
Iteration 14/25 | Loss: 0.00258953
Iteration 15/25 | Loss: 0.00256514
Iteration 16/25 | Loss: 0.00255603
Iteration 17/25 | Loss: 0.00257540
Iteration 18/25 | Loss: 0.00255541
Iteration 19/25 | Loss: 0.00253139
Iteration 20/25 | Loss: 0.00252628
Iteration 21/25 | Loss: 0.00253315
Iteration 22/25 | Loss: 0.00251369
Iteration 23/25 | Loss: 0.00250811
Iteration 24/25 | Loss: 0.00249724
Iteration 25/25 | Loss: 0.00249268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95735890
Iteration 2/25 | Loss: 0.00590484
Iteration 3/25 | Loss: 0.00590484
Iteration 4/25 | Loss: 0.00590484
Iteration 5/25 | Loss: 0.00590484
Iteration 6/25 | Loss: 0.00590484
Iteration 7/25 | Loss: 0.00590483
Iteration 8/25 | Loss: 0.00590483
Iteration 9/25 | Loss: 0.00590483
Iteration 10/25 | Loss: 0.00590483
Iteration 11/25 | Loss: 0.00590483
Iteration 12/25 | Loss: 0.00590483
Iteration 13/25 | Loss: 0.00590483
Iteration 14/25 | Loss: 0.00590483
Iteration 15/25 | Loss: 0.00590483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005904833320528269, 0.005904833320528269, 0.005904833320528269, 0.005904833320528269, 0.005904833320528269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005904833320528269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00590483
Iteration 2/1000 | Loss: 0.00028410
Iteration 3/1000 | Loss: 0.00018166
Iteration 4/1000 | Loss: 0.00015460
Iteration 5/1000 | Loss: 0.00014379
Iteration 6/1000 | Loss: 0.00013604
Iteration 7/1000 | Loss: 0.00013228
Iteration 8/1000 | Loss: 0.00051381
Iteration 9/1000 | Loss: 0.00055677
Iteration 10/1000 | Loss: 0.00050863
Iteration 11/1000 | Loss: 0.00052573
Iteration 12/1000 | Loss: 0.00042457
Iteration 13/1000 | Loss: 0.00044473
Iteration 14/1000 | Loss: 0.00026640
Iteration 15/1000 | Loss: 0.00023408
Iteration 16/1000 | Loss: 0.00019789
Iteration 17/1000 | Loss: 0.00015191
Iteration 18/1000 | Loss: 0.00039033
Iteration 19/1000 | Loss: 0.00032593
Iteration 20/1000 | Loss: 0.00013949
Iteration 21/1000 | Loss: 0.00031406
Iteration 22/1000 | Loss: 0.00016114
Iteration 23/1000 | Loss: 0.00031035
Iteration 24/1000 | Loss: 0.00026917
Iteration 25/1000 | Loss: 0.00013131
Iteration 26/1000 | Loss: 0.00032511
Iteration 27/1000 | Loss: 0.00013800
Iteration 28/1000 | Loss: 0.00013124
Iteration 29/1000 | Loss: 0.00012720
Iteration 30/1000 | Loss: 0.00012468
Iteration 31/1000 | Loss: 0.00022908
Iteration 32/1000 | Loss: 0.00036289
Iteration 33/1000 | Loss: 0.00017664
Iteration 34/1000 | Loss: 0.00013441
Iteration 35/1000 | Loss: 0.00012823
Iteration 36/1000 | Loss: 0.00037076
Iteration 37/1000 | Loss: 0.00013623
Iteration 38/1000 | Loss: 0.00024770
Iteration 39/1000 | Loss: 0.00032407
Iteration 40/1000 | Loss: 0.00022542
Iteration 41/1000 | Loss: 0.00030287
Iteration 42/1000 | Loss: 0.00023373
Iteration 43/1000 | Loss: 0.00019357
Iteration 44/1000 | Loss: 0.00013434
Iteration 45/1000 | Loss: 0.00022619
Iteration 46/1000 | Loss: 0.00015876
Iteration 47/1000 | Loss: 0.00014160
Iteration 48/1000 | Loss: 0.00017933
Iteration 49/1000 | Loss: 0.00015084
Iteration 50/1000 | Loss: 0.00011818
Iteration 51/1000 | Loss: 0.00011646
Iteration 52/1000 | Loss: 0.00011508
Iteration 53/1000 | Loss: 0.00011380
Iteration 54/1000 | Loss: 0.00023745
Iteration 55/1000 | Loss: 0.00011859
Iteration 56/1000 | Loss: 0.00011243
Iteration 57/1000 | Loss: 0.00011035
Iteration 58/1000 | Loss: 0.00010889
Iteration 59/1000 | Loss: 0.00010781
Iteration 60/1000 | Loss: 0.00010671
Iteration 61/1000 | Loss: 0.00010569
Iteration 62/1000 | Loss: 0.00010448
Iteration 63/1000 | Loss: 0.00010328
Iteration 64/1000 | Loss: 0.00010262
Iteration 65/1000 | Loss: 0.00010152
Iteration 66/1000 | Loss: 0.00010044
Iteration 67/1000 | Loss: 0.00022043
Iteration 68/1000 | Loss: 0.00037128
Iteration 69/1000 | Loss: 0.00016651
Iteration 70/1000 | Loss: 0.00010434
Iteration 71/1000 | Loss: 0.00022276
Iteration 72/1000 | Loss: 0.00010402
Iteration 73/1000 | Loss: 0.00009822
Iteration 74/1000 | Loss: 0.00009718
Iteration 75/1000 | Loss: 0.00024384
Iteration 76/1000 | Loss: 0.00109657
Iteration 77/1000 | Loss: 0.00072722
Iteration 78/1000 | Loss: 0.00091662
Iteration 79/1000 | Loss: 0.00055122
Iteration 80/1000 | Loss: 0.00087491
Iteration 81/1000 | Loss: 0.00079847
Iteration 82/1000 | Loss: 0.00056751
Iteration 83/1000 | Loss: 0.00028273
Iteration 84/1000 | Loss: 0.00013989
Iteration 85/1000 | Loss: 0.00012050
Iteration 86/1000 | Loss: 0.00071380
Iteration 87/1000 | Loss: 0.00081548
Iteration 88/1000 | Loss: 0.00034885
Iteration 89/1000 | Loss: 0.00077708
Iteration 90/1000 | Loss: 0.00065998
Iteration 91/1000 | Loss: 0.00058172
Iteration 92/1000 | Loss: 0.00061890
Iteration 93/1000 | Loss: 0.00012936
Iteration 94/1000 | Loss: 0.00010668
Iteration 95/1000 | Loss: 0.00050006
Iteration 96/1000 | Loss: 0.00041768
Iteration 97/1000 | Loss: 0.00050585
Iteration 98/1000 | Loss: 0.00045630
Iteration 99/1000 | Loss: 0.00026209
Iteration 100/1000 | Loss: 0.00051332
Iteration 101/1000 | Loss: 0.00026788
Iteration 102/1000 | Loss: 0.00040357
Iteration 103/1000 | Loss: 0.00052107
Iteration 104/1000 | Loss: 0.00021672
Iteration 105/1000 | Loss: 0.00020714
Iteration 106/1000 | Loss: 0.00038268
Iteration 107/1000 | Loss: 0.00010823
Iteration 108/1000 | Loss: 0.00010294
Iteration 109/1000 | Loss: 0.00010981
Iteration 110/1000 | Loss: 0.00018825
Iteration 111/1000 | Loss: 0.00025801
Iteration 112/1000 | Loss: 0.00020342
Iteration 113/1000 | Loss: 0.00021066
Iteration 114/1000 | Loss: 0.00021258
Iteration 115/1000 | Loss: 0.00018571
Iteration 116/1000 | Loss: 0.00017656
Iteration 117/1000 | Loss: 0.00019440
Iteration 118/1000 | Loss: 0.00017590
Iteration 119/1000 | Loss: 0.00010510
Iteration 120/1000 | Loss: 0.00022607
Iteration 121/1000 | Loss: 0.00017188
Iteration 122/1000 | Loss: 0.00009556
Iteration 123/1000 | Loss: 0.00009445
Iteration 124/1000 | Loss: 0.00009346
Iteration 125/1000 | Loss: 0.00009240
Iteration 126/1000 | Loss: 0.00009131
Iteration 127/1000 | Loss: 0.00009048
Iteration 128/1000 | Loss: 0.00023105
Iteration 129/1000 | Loss: 0.00021679
Iteration 130/1000 | Loss: 0.00119934
Iteration 131/1000 | Loss: 0.00026974
Iteration 132/1000 | Loss: 0.00019957
Iteration 133/1000 | Loss: 0.00044742
Iteration 134/1000 | Loss: 0.00045817
Iteration 135/1000 | Loss: 0.00038256
Iteration 136/1000 | Loss: 0.00019959
Iteration 137/1000 | Loss: 0.00016647
Iteration 138/1000 | Loss: 0.00014687
Iteration 139/1000 | Loss: 0.00011970
Iteration 140/1000 | Loss: 0.00018527
Iteration 141/1000 | Loss: 0.00018790
Iteration 142/1000 | Loss: 0.00010691
Iteration 143/1000 | Loss: 0.00018943
Iteration 144/1000 | Loss: 0.00010399
Iteration 145/1000 | Loss: 0.00024789
Iteration 146/1000 | Loss: 0.00010239
Iteration 147/1000 | Loss: 0.00010156
Iteration 148/1000 | Loss: 0.00009741
Iteration 149/1000 | Loss: 0.00027234
Iteration 150/1000 | Loss: 0.00009867
Iteration 151/1000 | Loss: 0.00009359
Iteration 152/1000 | Loss: 0.00009062
Iteration 153/1000 | Loss: 0.00019290
Iteration 154/1000 | Loss: 0.00019349
Iteration 155/1000 | Loss: 0.00029498
Iteration 156/1000 | Loss: 0.00011903
Iteration 157/1000 | Loss: 0.00010354
Iteration 158/1000 | Loss: 0.00009498
Iteration 159/1000 | Loss: 0.00010750
Iteration 160/1000 | Loss: 0.00030492
Iteration 161/1000 | Loss: 0.00027957
Iteration 162/1000 | Loss: 0.00015899
Iteration 163/1000 | Loss: 0.00010038
Iteration 164/1000 | Loss: 0.00028526
Iteration 165/1000 | Loss: 0.00045741
Iteration 166/1000 | Loss: 0.00026382
Iteration 167/1000 | Loss: 0.00010079
Iteration 168/1000 | Loss: 0.00011761
Iteration 169/1000 | Loss: 0.00009442
Iteration 170/1000 | Loss: 0.00010768
Iteration 171/1000 | Loss: 0.00008811
Iteration 172/1000 | Loss: 0.00008577
Iteration 173/1000 | Loss: 0.00008447
Iteration 174/1000 | Loss: 0.00008392
Iteration 175/1000 | Loss: 0.00008327
Iteration 176/1000 | Loss: 0.00022596
Iteration 177/1000 | Loss: 0.00009387
Iteration 178/1000 | Loss: 0.00008843
Iteration 179/1000 | Loss: 0.00037363
Iteration 180/1000 | Loss: 0.00034602
Iteration 181/1000 | Loss: 0.00020984
Iteration 182/1000 | Loss: 0.00018768
Iteration 183/1000 | Loss: 0.00009549
Iteration 184/1000 | Loss: 0.00008925
Iteration 185/1000 | Loss: 0.00008696
Iteration 186/1000 | Loss: 0.00033963
Iteration 187/1000 | Loss: 0.00039158
Iteration 188/1000 | Loss: 0.00014134
Iteration 189/1000 | Loss: 0.00011252
Iteration 190/1000 | Loss: 0.00011697
Iteration 191/1000 | Loss: 0.00009800
Iteration 192/1000 | Loss: 0.00009269
Iteration 193/1000 | Loss: 0.00008912
Iteration 194/1000 | Loss: 0.00008640
Iteration 195/1000 | Loss: 0.00008510
Iteration 196/1000 | Loss: 0.00008375
Iteration 197/1000 | Loss: 0.00009050
Iteration 198/1000 | Loss: 0.00008773
Iteration 199/1000 | Loss: 0.00008575
Iteration 200/1000 | Loss: 0.00008405
Iteration 201/1000 | Loss: 0.00008286
Iteration 202/1000 | Loss: 0.00008222
Iteration 203/1000 | Loss: 0.00008170
Iteration 204/1000 | Loss: 0.00008097
Iteration 205/1000 | Loss: 0.00008068
Iteration 206/1000 | Loss: 0.00008647
Iteration 207/1000 | Loss: 0.00008289
Iteration 208/1000 | Loss: 0.00008563
Iteration 209/1000 | Loss: 0.00008245
Iteration 210/1000 | Loss: 0.00008586
Iteration 211/1000 | Loss: 0.00008585
Iteration 212/1000 | Loss: 0.00008249
Iteration 213/1000 | Loss: 0.00008499
Iteration 214/1000 | Loss: 0.00023053
Iteration 215/1000 | Loss: 0.00008640
Iteration 216/1000 | Loss: 0.00008294
Iteration 217/1000 | Loss: 0.00021135
Iteration 218/1000 | Loss: 0.00054206
Iteration 219/1000 | Loss: 0.00011473
Iteration 220/1000 | Loss: 0.00008845
Iteration 221/1000 | Loss: 0.00008374
Iteration 222/1000 | Loss: 0.00008210
Iteration 223/1000 | Loss: 0.00009366
Iteration 224/1000 | Loss: 0.00008213
Iteration 225/1000 | Loss: 0.00008055
Iteration 226/1000 | Loss: 0.00007998
Iteration 227/1000 | Loss: 0.00007958
Iteration 228/1000 | Loss: 0.00007922
Iteration 229/1000 | Loss: 0.00007881
Iteration 230/1000 | Loss: 0.00007853
Iteration 231/1000 | Loss: 0.00007842
Iteration 232/1000 | Loss: 0.00007830
Iteration 233/1000 | Loss: 0.00007823
Iteration 234/1000 | Loss: 0.00007822
Iteration 235/1000 | Loss: 0.00007821
Iteration 236/1000 | Loss: 0.00007821
Iteration 237/1000 | Loss: 0.00007821
Iteration 238/1000 | Loss: 0.00007821
Iteration 239/1000 | Loss: 0.00007821
Iteration 240/1000 | Loss: 0.00007821
Iteration 241/1000 | Loss: 0.00007821
Iteration 242/1000 | Loss: 0.00007821
Iteration 243/1000 | Loss: 0.00007820
Iteration 244/1000 | Loss: 0.00007820
Iteration 245/1000 | Loss: 0.00007820
Iteration 246/1000 | Loss: 0.00007820
Iteration 247/1000 | Loss: 0.00007820
Iteration 248/1000 | Loss: 0.00007820
Iteration 249/1000 | Loss: 0.00007819
Iteration 250/1000 | Loss: 0.00007819
Iteration 251/1000 | Loss: 0.00007819
Iteration 252/1000 | Loss: 0.00007819
Iteration 253/1000 | Loss: 0.00007819
Iteration 254/1000 | Loss: 0.00007819
Iteration 255/1000 | Loss: 0.00007819
Iteration 256/1000 | Loss: 0.00007819
Iteration 257/1000 | Loss: 0.00007819
Iteration 258/1000 | Loss: 0.00007819
Iteration 259/1000 | Loss: 0.00007819
Iteration 260/1000 | Loss: 0.00007818
Iteration 261/1000 | Loss: 0.00007818
Iteration 262/1000 | Loss: 0.00007818
Iteration 263/1000 | Loss: 0.00007818
Iteration 264/1000 | Loss: 0.00007818
Iteration 265/1000 | Loss: 0.00007818
Iteration 266/1000 | Loss: 0.00007817
Iteration 267/1000 | Loss: 0.00007817
Iteration 268/1000 | Loss: 0.00007817
Iteration 269/1000 | Loss: 0.00007817
Iteration 270/1000 | Loss: 0.00007817
Iteration 271/1000 | Loss: 0.00007816
Iteration 272/1000 | Loss: 0.00007816
Iteration 273/1000 | Loss: 0.00007816
Iteration 274/1000 | Loss: 0.00007816
Iteration 275/1000 | Loss: 0.00007816
Iteration 276/1000 | Loss: 0.00007816
Iteration 277/1000 | Loss: 0.00007816
Iteration 278/1000 | Loss: 0.00007816
Iteration 279/1000 | Loss: 0.00007816
Iteration 280/1000 | Loss: 0.00007816
Iteration 281/1000 | Loss: 0.00007816
Iteration 282/1000 | Loss: 0.00007815
Iteration 283/1000 | Loss: 0.00007815
Iteration 284/1000 | Loss: 0.00007815
Iteration 285/1000 | Loss: 0.00007815
Iteration 286/1000 | Loss: 0.00007815
Iteration 287/1000 | Loss: 0.00007815
Iteration 288/1000 | Loss: 0.00007815
Iteration 289/1000 | Loss: 0.00007815
Iteration 290/1000 | Loss: 0.00007815
Iteration 291/1000 | Loss: 0.00007815
Iteration 292/1000 | Loss: 0.00007815
Iteration 293/1000 | Loss: 0.00007815
Iteration 294/1000 | Loss: 0.00007815
Iteration 295/1000 | Loss: 0.00007815
Iteration 296/1000 | Loss: 0.00007815
Iteration 297/1000 | Loss: 0.00007814
Iteration 298/1000 | Loss: 0.00007814
Iteration 299/1000 | Loss: 0.00007814
Iteration 300/1000 | Loss: 0.00007814
Iteration 301/1000 | Loss: 0.00007814
Iteration 302/1000 | Loss: 0.00007814
Iteration 303/1000 | Loss: 0.00007813
Iteration 304/1000 | Loss: 0.00007813
Iteration 305/1000 | Loss: 0.00007813
Iteration 306/1000 | Loss: 0.00007813
Iteration 307/1000 | Loss: 0.00007812
Iteration 308/1000 | Loss: 0.00007812
Iteration 309/1000 | Loss: 0.00007812
Iteration 310/1000 | Loss: 0.00022231
Iteration 311/1000 | Loss: 0.00044005
Iteration 312/1000 | Loss: 0.00032618
Iteration 313/1000 | Loss: 0.00017879
Iteration 314/1000 | Loss: 0.00009354
Iteration 315/1000 | Loss: 0.00026988
Iteration 316/1000 | Loss: 0.00010071
Iteration 317/1000 | Loss: 0.00008852
Iteration 318/1000 | Loss: 0.00008471
Iteration 319/1000 | Loss: 0.00008200
Iteration 320/1000 | Loss: 0.00008055
Iteration 321/1000 | Loss: 0.00008000
Iteration 322/1000 | Loss: 0.00007955
Iteration 323/1000 | Loss: 0.00007933
Iteration 324/1000 | Loss: 0.00007907
Iteration 325/1000 | Loss: 0.00007888
Iteration 326/1000 | Loss: 0.00007882
Iteration 327/1000 | Loss: 0.00007874
Iteration 328/1000 | Loss: 0.00007873
Iteration 329/1000 | Loss: 0.00007873
Iteration 330/1000 | Loss: 0.00007872
Iteration 331/1000 | Loss: 0.00007871
Iteration 332/1000 | Loss: 0.00007871
Iteration 333/1000 | Loss: 0.00007867
Iteration 334/1000 | Loss: 0.00007867
Iteration 335/1000 | Loss: 0.00007867
Iteration 336/1000 | Loss: 0.00007865
Iteration 337/1000 | Loss: 0.00007865
Iteration 338/1000 | Loss: 0.00007865
Iteration 339/1000 | Loss: 0.00007865
Iteration 340/1000 | Loss: 0.00007865
Iteration 341/1000 | Loss: 0.00007865
Iteration 342/1000 | Loss: 0.00007865
Iteration 343/1000 | Loss: 0.00007865
Iteration 344/1000 | Loss: 0.00007864
Iteration 345/1000 | Loss: 0.00007864
Iteration 346/1000 | Loss: 0.00007863
Iteration 347/1000 | Loss: 0.00007863
Iteration 348/1000 | Loss: 0.00007863
Iteration 349/1000 | Loss: 0.00007862
Iteration 350/1000 | Loss: 0.00007861
Iteration 351/1000 | Loss: 0.00007861
Iteration 352/1000 | Loss: 0.00007861
Iteration 353/1000 | Loss: 0.00007861
Iteration 354/1000 | Loss: 0.00007860
Iteration 355/1000 | Loss: 0.00007860
Iteration 356/1000 | Loss: 0.00007859
Iteration 357/1000 | Loss: 0.00007854
Iteration 358/1000 | Loss: 0.00007854
Iteration 359/1000 | Loss: 0.00007853
Iteration 360/1000 | Loss: 0.00007852
Iteration 361/1000 | Loss: 0.00007851
Iteration 362/1000 | Loss: 0.00007851
Iteration 363/1000 | Loss: 0.00007851
Iteration 364/1000 | Loss: 0.00007851
Iteration 365/1000 | Loss: 0.00007851
Iteration 366/1000 | Loss: 0.00007851
Iteration 367/1000 | Loss: 0.00007851
Iteration 368/1000 | Loss: 0.00007850
Iteration 369/1000 | Loss: 0.00007850
Iteration 370/1000 | Loss: 0.00007850
Iteration 371/1000 | Loss: 0.00007850
Iteration 372/1000 | Loss: 0.00007850
Iteration 373/1000 | Loss: 0.00007849
Iteration 374/1000 | Loss: 0.00007849
Iteration 375/1000 | Loss: 0.00007849
Iteration 376/1000 | Loss: 0.00007848
Iteration 377/1000 | Loss: 0.00007847
Iteration 378/1000 | Loss: 0.00007843
Iteration 379/1000 | Loss: 0.00007843
Iteration 380/1000 | Loss: 0.00007843
Iteration 381/1000 | Loss: 0.00007843
Iteration 382/1000 | Loss: 0.00007843
Iteration 383/1000 | Loss: 0.00007842
Iteration 384/1000 | Loss: 0.00007842
Iteration 385/1000 | Loss: 0.00007842
Iteration 386/1000 | Loss: 0.00007842
Iteration 387/1000 | Loss: 0.00007842
Iteration 388/1000 | Loss: 0.00007842
Iteration 389/1000 | Loss: 0.00007833
Iteration 390/1000 | Loss: 0.00007828
Iteration 391/1000 | Loss: 0.00007813
Iteration 392/1000 | Loss: 0.00007800
Iteration 393/1000 | Loss: 0.00007788
Iteration 394/1000 | Loss: 0.00007770
Iteration 395/1000 | Loss: 0.00007769
Iteration 396/1000 | Loss: 0.00007768
Iteration 397/1000 | Loss: 0.00007767
Iteration 398/1000 | Loss: 0.00007764
Iteration 399/1000 | Loss: 0.00007762
Iteration 400/1000 | Loss: 0.00007761
Iteration 401/1000 | Loss: 0.00007754
Iteration 402/1000 | Loss: 0.00007754
Iteration 403/1000 | Loss: 0.00007754
Iteration 404/1000 | Loss: 0.00007753
Iteration 405/1000 | Loss: 0.00007753
Iteration 406/1000 | Loss: 0.00007753
Iteration 407/1000 | Loss: 0.00007748
Iteration 408/1000 | Loss: 0.00007744
Iteration 409/1000 | Loss: 0.00007743
Iteration 410/1000 | Loss: 0.00007741
Iteration 411/1000 | Loss: 0.00007740
Iteration 412/1000 | Loss: 0.00007739
Iteration 413/1000 | Loss: 0.00007738
Iteration 414/1000 | Loss: 0.00007737
Iteration 415/1000 | Loss: 0.00007735
Iteration 416/1000 | Loss: 0.00007735
Iteration 417/1000 | Loss: 0.00007735
Iteration 418/1000 | Loss: 0.00007735
Iteration 419/1000 | Loss: 0.00007735
Iteration 420/1000 | Loss: 0.00007735
Iteration 421/1000 | Loss: 0.00007735
Iteration 422/1000 | Loss: 0.00007735
Iteration 423/1000 | Loss: 0.00007735
Iteration 424/1000 | Loss: 0.00007734
Iteration 425/1000 | Loss: 0.00007734
Iteration 426/1000 | Loss: 0.00007733
Iteration 427/1000 | Loss: 0.00007733
Iteration 428/1000 | Loss: 0.00007733
Iteration 429/1000 | Loss: 0.00007733
Iteration 430/1000 | Loss: 0.00007732
Iteration 431/1000 | Loss: 0.00007732
Iteration 432/1000 | Loss: 0.00007732
Iteration 433/1000 | Loss: 0.00007732
Iteration 434/1000 | Loss: 0.00007732
Iteration 435/1000 | Loss: 0.00007732
Iteration 436/1000 | Loss: 0.00007732
Iteration 437/1000 | Loss: 0.00007731
Iteration 438/1000 | Loss: 0.00007731
Iteration 439/1000 | Loss: 0.00007731
Iteration 440/1000 | Loss: 0.00007730
Iteration 441/1000 | Loss: 0.00007730
Iteration 442/1000 | Loss: 0.00007730
Iteration 443/1000 | Loss: 0.00007730
Iteration 444/1000 | Loss: 0.00007729
Iteration 445/1000 | Loss: 0.00007729
Iteration 446/1000 | Loss: 0.00007729
Iteration 447/1000 | Loss: 0.00007729
Iteration 448/1000 | Loss: 0.00007729
Iteration 449/1000 | Loss: 0.00007728
Iteration 450/1000 | Loss: 0.00007728
Iteration 451/1000 | Loss: 0.00007728
Iteration 452/1000 | Loss: 0.00007728
Iteration 453/1000 | Loss: 0.00007728
Iteration 454/1000 | Loss: 0.00007728
Iteration 455/1000 | Loss: 0.00007728
Iteration 456/1000 | Loss: 0.00007728
Iteration 457/1000 | Loss: 0.00007728
Iteration 458/1000 | Loss: 0.00007728
Iteration 459/1000 | Loss: 0.00007728
Iteration 460/1000 | Loss: 0.00007728
Iteration 461/1000 | Loss: 0.00007728
Iteration 462/1000 | Loss: 0.00007728
Iteration 463/1000 | Loss: 0.00007728
Iteration 464/1000 | Loss: 0.00007727
Iteration 465/1000 | Loss: 0.00007727
Iteration 466/1000 | Loss: 0.00007727
Iteration 467/1000 | Loss: 0.00007727
Iteration 468/1000 | Loss: 0.00007727
Iteration 469/1000 | Loss: 0.00007727
Iteration 470/1000 | Loss: 0.00007727
Iteration 471/1000 | Loss: 0.00007727
Iteration 472/1000 | Loss: 0.00007727
Iteration 473/1000 | Loss: 0.00007727
Iteration 474/1000 | Loss: 0.00007727
Iteration 475/1000 | Loss: 0.00007727
Iteration 476/1000 | Loss: 0.00007727
Iteration 477/1000 | Loss: 0.00007727
Iteration 478/1000 | Loss: 0.00007727
Iteration 479/1000 | Loss: 0.00007727
Iteration 480/1000 | Loss: 0.00007727
Iteration 481/1000 | Loss: 0.00007726
Iteration 482/1000 | Loss: 0.00007726
Iteration 483/1000 | Loss: 0.00007726
Iteration 484/1000 | Loss: 0.00007726
Iteration 485/1000 | Loss: 0.00007726
Iteration 486/1000 | Loss: 0.00007726
Iteration 487/1000 | Loss: 0.00007726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 487. Stopping optimization.
Last 5 losses: [7.726454350631684e-05, 7.726454350631684e-05, 7.726454350631684e-05, 7.726454350631684e-05, 7.726454350631684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.726454350631684e-05

Optimization complete. Final v2v error: 6.963957786560059 mm

Highest mean error: 10.152342796325684 mm for frame 129

Lowest mean error: 5.76741886138916 mm for frame 46

Saving results

Total time: 411.78467750549316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888762
Iteration 2/25 | Loss: 0.00148362
Iteration 3/25 | Loss: 0.00134644
Iteration 4/25 | Loss: 0.00132404
Iteration 5/25 | Loss: 0.00131586
Iteration 6/25 | Loss: 0.00131369
Iteration 7/25 | Loss: 0.00131350
Iteration 8/25 | Loss: 0.00131350
Iteration 9/25 | Loss: 0.00131350
Iteration 10/25 | Loss: 0.00131350
Iteration 11/25 | Loss: 0.00131350
Iteration 12/25 | Loss: 0.00131350
Iteration 13/25 | Loss: 0.00131350
Iteration 14/25 | Loss: 0.00131350
Iteration 15/25 | Loss: 0.00131350
Iteration 16/25 | Loss: 0.00131350
Iteration 17/25 | Loss: 0.00131350
Iteration 18/25 | Loss: 0.00131350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013135034823790193, 0.0013135034823790193, 0.0013135034823790193, 0.0013135034823790193, 0.0013135034823790193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013135034823790193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30168855
Iteration 2/25 | Loss: 0.00345336
Iteration 3/25 | Loss: 0.00345336
Iteration 4/25 | Loss: 0.00345336
Iteration 5/25 | Loss: 0.00345336
Iteration 6/25 | Loss: 0.00345336
Iteration 7/25 | Loss: 0.00345336
Iteration 8/25 | Loss: 0.00345336
Iteration 9/25 | Loss: 0.00345336
Iteration 10/25 | Loss: 0.00345336
Iteration 11/25 | Loss: 0.00345336
Iteration 12/25 | Loss: 0.00345336
Iteration 13/25 | Loss: 0.00345336
Iteration 14/25 | Loss: 0.00345336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003453359706327319, 0.003453359706327319, 0.003453359706327319, 0.003453359706327319, 0.003453359706327319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003453359706327319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345336
Iteration 2/1000 | Loss: 0.00010967
Iteration 3/1000 | Loss: 0.00006038
Iteration 4/1000 | Loss: 0.00003936
Iteration 5/1000 | Loss: 0.00003332
Iteration 6/1000 | Loss: 0.00003096
Iteration 7/1000 | Loss: 0.00002979
Iteration 8/1000 | Loss: 0.00002892
Iteration 9/1000 | Loss: 0.00002822
Iteration 10/1000 | Loss: 0.00002764
Iteration 11/1000 | Loss: 0.00002724
Iteration 12/1000 | Loss: 0.00002692
Iteration 13/1000 | Loss: 0.00002657
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002621
Iteration 16/1000 | Loss: 0.00002618
Iteration 17/1000 | Loss: 0.00002618
Iteration 18/1000 | Loss: 0.00002604
Iteration 19/1000 | Loss: 0.00002603
Iteration 20/1000 | Loss: 0.00002587
Iteration 21/1000 | Loss: 0.00002581
Iteration 22/1000 | Loss: 0.00002579
Iteration 23/1000 | Loss: 0.00002578
Iteration 24/1000 | Loss: 0.00002578
Iteration 25/1000 | Loss: 0.00002577
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002576
Iteration 28/1000 | Loss: 0.00002571
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00002570
Iteration 31/1000 | Loss: 0.00002570
Iteration 32/1000 | Loss: 0.00002569
Iteration 33/1000 | Loss: 0.00002569
Iteration 34/1000 | Loss: 0.00002569
Iteration 35/1000 | Loss: 0.00002568
Iteration 36/1000 | Loss: 0.00002567
Iteration 37/1000 | Loss: 0.00002567
Iteration 38/1000 | Loss: 0.00002566
Iteration 39/1000 | Loss: 0.00002566
Iteration 40/1000 | Loss: 0.00002565
Iteration 41/1000 | Loss: 0.00002564
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002562
Iteration 44/1000 | Loss: 0.00002561
Iteration 45/1000 | Loss: 0.00002561
Iteration 46/1000 | Loss: 0.00002561
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002556
Iteration 49/1000 | Loss: 0.00002555
Iteration 50/1000 | Loss: 0.00002554
Iteration 51/1000 | Loss: 0.00002554
Iteration 52/1000 | Loss: 0.00002554
Iteration 53/1000 | Loss: 0.00002553
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002553
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002552
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002551
Iteration 61/1000 | Loss: 0.00002548
Iteration 62/1000 | Loss: 0.00002548
Iteration 63/1000 | Loss: 0.00002548
Iteration 64/1000 | Loss: 0.00002548
Iteration 65/1000 | Loss: 0.00002548
Iteration 66/1000 | Loss: 0.00002548
Iteration 67/1000 | Loss: 0.00002547
Iteration 68/1000 | Loss: 0.00002547
Iteration 69/1000 | Loss: 0.00002547
Iteration 70/1000 | Loss: 0.00002547
Iteration 71/1000 | Loss: 0.00002546
Iteration 72/1000 | Loss: 0.00002546
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002544
Iteration 77/1000 | Loss: 0.00002544
Iteration 78/1000 | Loss: 0.00002543
Iteration 79/1000 | Loss: 0.00002543
Iteration 80/1000 | Loss: 0.00002543
Iteration 81/1000 | Loss: 0.00002543
Iteration 82/1000 | Loss: 0.00002543
Iteration 83/1000 | Loss: 0.00002543
Iteration 84/1000 | Loss: 0.00002543
Iteration 85/1000 | Loss: 0.00002543
Iteration 86/1000 | Loss: 0.00002542
Iteration 87/1000 | Loss: 0.00002542
Iteration 88/1000 | Loss: 0.00002542
Iteration 89/1000 | Loss: 0.00002542
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00002541
Iteration 93/1000 | Loss: 0.00002541
Iteration 94/1000 | Loss: 0.00002541
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00002541
Iteration 99/1000 | Loss: 0.00002540
Iteration 100/1000 | Loss: 0.00002540
Iteration 101/1000 | Loss: 0.00002540
Iteration 102/1000 | Loss: 0.00002540
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002540
Iteration 105/1000 | Loss: 0.00002540
Iteration 106/1000 | Loss: 0.00002539
Iteration 107/1000 | Loss: 0.00002539
Iteration 108/1000 | Loss: 0.00002539
Iteration 109/1000 | Loss: 0.00002539
Iteration 110/1000 | Loss: 0.00002539
Iteration 111/1000 | Loss: 0.00002539
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002538
Iteration 117/1000 | Loss: 0.00002538
Iteration 118/1000 | Loss: 0.00002537
Iteration 119/1000 | Loss: 0.00002537
Iteration 120/1000 | Loss: 0.00002537
Iteration 121/1000 | Loss: 0.00002537
Iteration 122/1000 | Loss: 0.00002537
Iteration 123/1000 | Loss: 0.00002536
Iteration 124/1000 | Loss: 0.00002536
Iteration 125/1000 | Loss: 0.00002536
Iteration 126/1000 | Loss: 0.00002536
Iteration 127/1000 | Loss: 0.00002535
Iteration 128/1000 | Loss: 0.00002535
Iteration 129/1000 | Loss: 0.00002535
Iteration 130/1000 | Loss: 0.00002535
Iteration 131/1000 | Loss: 0.00002535
Iteration 132/1000 | Loss: 0.00002535
Iteration 133/1000 | Loss: 0.00002534
Iteration 134/1000 | Loss: 0.00002534
Iteration 135/1000 | Loss: 0.00002534
Iteration 136/1000 | Loss: 0.00002534
Iteration 137/1000 | Loss: 0.00002533
Iteration 138/1000 | Loss: 0.00002533
Iteration 139/1000 | Loss: 0.00002533
Iteration 140/1000 | Loss: 0.00002533
Iteration 141/1000 | Loss: 0.00002533
Iteration 142/1000 | Loss: 0.00002533
Iteration 143/1000 | Loss: 0.00002533
Iteration 144/1000 | Loss: 0.00002533
Iteration 145/1000 | Loss: 0.00002533
Iteration 146/1000 | Loss: 0.00002533
Iteration 147/1000 | Loss: 0.00002533
Iteration 148/1000 | Loss: 0.00002533
Iteration 149/1000 | Loss: 0.00002533
Iteration 150/1000 | Loss: 0.00002533
Iteration 151/1000 | Loss: 0.00002533
Iteration 152/1000 | Loss: 0.00002533
Iteration 153/1000 | Loss: 0.00002533
Iteration 154/1000 | Loss: 0.00002533
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002533
Iteration 157/1000 | Loss: 0.00002533
Iteration 158/1000 | Loss: 0.00002532
Iteration 159/1000 | Loss: 0.00002532
Iteration 160/1000 | Loss: 0.00002532
Iteration 161/1000 | Loss: 0.00002532
Iteration 162/1000 | Loss: 0.00002531
Iteration 163/1000 | Loss: 0.00002531
Iteration 164/1000 | Loss: 0.00002531
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002530
Iteration 170/1000 | Loss: 0.00002529
Iteration 171/1000 | Loss: 0.00002529
Iteration 172/1000 | Loss: 0.00002529
Iteration 173/1000 | Loss: 0.00002529
Iteration 174/1000 | Loss: 0.00002529
Iteration 175/1000 | Loss: 0.00002528
Iteration 176/1000 | Loss: 0.00002528
Iteration 177/1000 | Loss: 0.00002528
Iteration 178/1000 | Loss: 0.00002528
Iteration 179/1000 | Loss: 0.00002528
Iteration 180/1000 | Loss: 0.00002528
Iteration 181/1000 | Loss: 0.00002527
Iteration 182/1000 | Loss: 0.00002527
Iteration 183/1000 | Loss: 0.00002527
Iteration 184/1000 | Loss: 0.00002527
Iteration 185/1000 | Loss: 0.00002527
Iteration 186/1000 | Loss: 0.00002527
Iteration 187/1000 | Loss: 0.00002527
Iteration 188/1000 | Loss: 0.00002527
Iteration 189/1000 | Loss: 0.00002527
Iteration 190/1000 | Loss: 0.00002527
Iteration 191/1000 | Loss: 0.00002527
Iteration 192/1000 | Loss: 0.00002527
Iteration 193/1000 | Loss: 0.00002527
Iteration 194/1000 | Loss: 0.00002527
Iteration 195/1000 | Loss: 0.00002527
Iteration 196/1000 | Loss: 0.00002527
Iteration 197/1000 | Loss: 0.00002526
Iteration 198/1000 | Loss: 0.00002526
Iteration 199/1000 | Loss: 0.00002526
Iteration 200/1000 | Loss: 0.00002526
Iteration 201/1000 | Loss: 0.00002526
Iteration 202/1000 | Loss: 0.00002526
Iteration 203/1000 | Loss: 0.00002526
Iteration 204/1000 | Loss: 0.00002526
Iteration 205/1000 | Loss: 0.00002526
Iteration 206/1000 | Loss: 0.00002526
Iteration 207/1000 | Loss: 0.00002526
Iteration 208/1000 | Loss: 0.00002525
Iteration 209/1000 | Loss: 0.00002525
Iteration 210/1000 | Loss: 0.00002525
Iteration 211/1000 | Loss: 0.00002525
Iteration 212/1000 | Loss: 0.00002525
Iteration 213/1000 | Loss: 0.00002525
Iteration 214/1000 | Loss: 0.00002524
Iteration 215/1000 | Loss: 0.00002524
Iteration 216/1000 | Loss: 0.00002524
Iteration 217/1000 | Loss: 0.00002524
Iteration 218/1000 | Loss: 0.00002524
Iteration 219/1000 | Loss: 0.00002524
Iteration 220/1000 | Loss: 0.00002524
Iteration 221/1000 | Loss: 0.00002524
Iteration 222/1000 | Loss: 0.00002524
Iteration 223/1000 | Loss: 0.00002524
Iteration 224/1000 | Loss: 0.00002524
Iteration 225/1000 | Loss: 0.00002524
Iteration 226/1000 | Loss: 0.00002524
Iteration 227/1000 | Loss: 0.00002524
Iteration 228/1000 | Loss: 0.00002524
Iteration 229/1000 | Loss: 0.00002524
Iteration 230/1000 | Loss: 0.00002524
Iteration 231/1000 | Loss: 0.00002524
Iteration 232/1000 | Loss: 0.00002524
Iteration 233/1000 | Loss: 0.00002524
Iteration 234/1000 | Loss: 0.00002524
Iteration 235/1000 | Loss: 0.00002524
Iteration 236/1000 | Loss: 0.00002524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.5236584406229667e-05, 2.5236584406229667e-05, 2.5236584406229667e-05, 2.5236584406229667e-05, 2.5236584406229667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5236584406229667e-05

Optimization complete. Final v2v error: 4.1896281242370605 mm

Highest mean error: 6.005394458770752 mm for frame 94

Lowest mean error: 3.312427520751953 mm for frame 0

Saving results

Total time: 49.52089881896973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078049
Iteration 2/25 | Loss: 0.00197031
Iteration 3/25 | Loss: 0.00161181
Iteration 4/25 | Loss: 0.00154331
Iteration 5/25 | Loss: 0.00155014
Iteration 6/25 | Loss: 0.00151142
Iteration 7/25 | Loss: 0.00145298
Iteration 8/25 | Loss: 0.00143431
Iteration 9/25 | Loss: 0.00142535
Iteration 10/25 | Loss: 0.00142243
Iteration 11/25 | Loss: 0.00141560
Iteration 12/25 | Loss: 0.00141205
Iteration 13/25 | Loss: 0.00141094
Iteration 14/25 | Loss: 0.00141053
Iteration 15/25 | Loss: 0.00140765
Iteration 16/25 | Loss: 0.00140596
Iteration 17/25 | Loss: 0.00140401
Iteration 18/25 | Loss: 0.00140367
Iteration 19/25 | Loss: 0.00140229
Iteration 20/25 | Loss: 0.00140286
Iteration 21/25 | Loss: 0.00140346
Iteration 22/25 | Loss: 0.00140320
Iteration 23/25 | Loss: 0.00140395
Iteration 24/25 | Loss: 0.00140300
Iteration 25/25 | Loss: 0.00140322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68623614
Iteration 2/25 | Loss: 0.00269062
Iteration 3/25 | Loss: 0.00269061
Iteration 4/25 | Loss: 0.00269061
Iteration 5/25 | Loss: 0.00269061
Iteration 6/25 | Loss: 0.00269061
Iteration 7/25 | Loss: 0.00269061
Iteration 8/25 | Loss: 0.00269061
Iteration 9/25 | Loss: 0.00269061
Iteration 10/25 | Loss: 0.00269061
Iteration 11/25 | Loss: 0.00269061
Iteration 12/25 | Loss: 0.00269061
Iteration 13/25 | Loss: 0.00269061
Iteration 14/25 | Loss: 0.00269061
Iteration 15/25 | Loss: 0.00269061
Iteration 16/25 | Loss: 0.00269061
Iteration 17/25 | Loss: 0.00269061
Iteration 18/25 | Loss: 0.00269061
Iteration 19/25 | Loss: 0.00269061
Iteration 20/25 | Loss: 0.00269061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0026906102430075407, 0.0026906102430075407, 0.0026906102430075407, 0.0026906102430075407, 0.0026906102430075407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026906102430075407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269061
Iteration 2/1000 | Loss: 0.00007509
Iteration 3/1000 | Loss: 0.00024630
Iteration 4/1000 | Loss: 0.00006079
Iteration 5/1000 | Loss: 0.00005078
Iteration 6/1000 | Loss: 0.00004714
Iteration 7/1000 | Loss: 0.00004154
Iteration 8/1000 | Loss: 0.00004487
Iteration 9/1000 | Loss: 0.00004316
Iteration 10/1000 | Loss: 0.00004078
Iteration 11/1000 | Loss: 0.00003601
Iteration 12/1000 | Loss: 0.00004256
Iteration 13/1000 | Loss: 0.00004520
Iteration 14/1000 | Loss: 0.00004248
Iteration 15/1000 | Loss: 0.00003515
Iteration 16/1000 | Loss: 0.00004051
Iteration 17/1000 | Loss: 0.00004009
Iteration 18/1000 | Loss: 0.00003777
Iteration 19/1000 | Loss: 0.00003726
Iteration 20/1000 | Loss: 0.00003938
Iteration 21/1000 | Loss: 0.00003816
Iteration 22/1000 | Loss: 0.00003213
Iteration 23/1000 | Loss: 0.00004186
Iteration 24/1000 | Loss: 0.00004178
Iteration 25/1000 | Loss: 0.00003460
Iteration 26/1000 | Loss: 0.00004366
Iteration 27/1000 | Loss: 0.00003955
Iteration 28/1000 | Loss: 0.00003725
Iteration 29/1000 | Loss: 0.00004261
Iteration 30/1000 | Loss: 0.00004893
Iteration 31/1000 | Loss: 0.00013501
Iteration 32/1000 | Loss: 0.00003911
Iteration 33/1000 | Loss: 0.00003259
Iteration 34/1000 | Loss: 0.00004297
Iteration 35/1000 | Loss: 0.00004462
Iteration 36/1000 | Loss: 0.00004124
Iteration 37/1000 | Loss: 0.00004534
Iteration 38/1000 | Loss: 0.00004230
Iteration 39/1000 | Loss: 0.00004513
Iteration 40/1000 | Loss: 0.00004165
Iteration 41/1000 | Loss: 0.00003855
Iteration 42/1000 | Loss: 0.00003601
Iteration 43/1000 | Loss: 0.00004354
Iteration 44/1000 | Loss: 0.00003590
Iteration 45/1000 | Loss: 0.00003777
Iteration 46/1000 | Loss: 0.00003597
Iteration 47/1000 | Loss: 0.00003789
Iteration 48/1000 | Loss: 0.00003589
Iteration 49/1000 | Loss: 0.00003872
Iteration 50/1000 | Loss: 0.00004123
Iteration 51/1000 | Loss: 0.00003719
Iteration 52/1000 | Loss: 0.00004048
Iteration 53/1000 | Loss: 0.00003088
Iteration 54/1000 | Loss: 0.00002957
Iteration 55/1000 | Loss: 0.00002888
Iteration 56/1000 | Loss: 0.00002832
Iteration 57/1000 | Loss: 0.00020767
Iteration 58/1000 | Loss: 0.00003985
Iteration 59/1000 | Loss: 0.00003300
Iteration 60/1000 | Loss: 0.00003104
Iteration 61/1000 | Loss: 0.00003007
Iteration 62/1000 | Loss: 0.00002954
Iteration 63/1000 | Loss: 0.00002910
Iteration 64/1000 | Loss: 0.00002866
Iteration 65/1000 | Loss: 0.00002847
Iteration 66/1000 | Loss: 0.00002845
Iteration 67/1000 | Loss: 0.00002831
Iteration 68/1000 | Loss: 0.00002829
Iteration 69/1000 | Loss: 0.00002829
Iteration 70/1000 | Loss: 0.00002828
Iteration 71/1000 | Loss: 0.00002822
Iteration 72/1000 | Loss: 0.00002817
Iteration 73/1000 | Loss: 0.00002815
Iteration 74/1000 | Loss: 0.00002812
Iteration 75/1000 | Loss: 0.00002812
Iteration 76/1000 | Loss: 0.00002811
Iteration 77/1000 | Loss: 0.00002811
Iteration 78/1000 | Loss: 0.00002811
Iteration 79/1000 | Loss: 0.00002811
Iteration 80/1000 | Loss: 0.00002811
Iteration 81/1000 | Loss: 0.00002811
Iteration 82/1000 | Loss: 0.00002811
Iteration 83/1000 | Loss: 0.00002811
Iteration 84/1000 | Loss: 0.00002811
Iteration 85/1000 | Loss: 0.00002811
Iteration 86/1000 | Loss: 0.00002811
Iteration 87/1000 | Loss: 0.00002811
Iteration 88/1000 | Loss: 0.00002811
Iteration 89/1000 | Loss: 0.00002811
Iteration 90/1000 | Loss: 0.00002811
Iteration 91/1000 | Loss: 0.00002811
Iteration 92/1000 | Loss: 0.00002811
Iteration 93/1000 | Loss: 0.00002811
Iteration 94/1000 | Loss: 0.00002811
Iteration 95/1000 | Loss: 0.00002811
Iteration 96/1000 | Loss: 0.00002810
Iteration 97/1000 | Loss: 0.00002810
Iteration 98/1000 | Loss: 0.00002810
Iteration 99/1000 | Loss: 0.00002810
Iteration 100/1000 | Loss: 0.00002810
Iteration 101/1000 | Loss: 0.00002810
Iteration 102/1000 | Loss: 0.00002810
Iteration 103/1000 | Loss: 0.00002810
Iteration 104/1000 | Loss: 0.00002809
Iteration 105/1000 | Loss: 0.00002809
Iteration 106/1000 | Loss: 0.00002808
Iteration 107/1000 | Loss: 0.00002808
Iteration 108/1000 | Loss: 0.00002807
Iteration 109/1000 | Loss: 0.00002807
Iteration 110/1000 | Loss: 0.00002807
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00002806
Iteration 113/1000 | Loss: 0.00002805
Iteration 114/1000 | Loss: 0.00002805
Iteration 115/1000 | Loss: 0.00002805
Iteration 116/1000 | Loss: 0.00002804
Iteration 117/1000 | Loss: 0.00002804
Iteration 118/1000 | Loss: 0.00002803
Iteration 119/1000 | Loss: 0.00002803
Iteration 120/1000 | Loss: 0.00002803
Iteration 121/1000 | Loss: 0.00002803
Iteration 122/1000 | Loss: 0.00002802
Iteration 123/1000 | Loss: 0.00002802
Iteration 124/1000 | Loss: 0.00002802
Iteration 125/1000 | Loss: 0.00002802
Iteration 126/1000 | Loss: 0.00002802
Iteration 127/1000 | Loss: 0.00002801
Iteration 128/1000 | Loss: 0.00002801
Iteration 129/1000 | Loss: 0.00002796
Iteration 130/1000 | Loss: 0.00002796
Iteration 131/1000 | Loss: 0.00002793
Iteration 132/1000 | Loss: 0.00002792
Iteration 133/1000 | Loss: 0.00002791
Iteration 134/1000 | Loss: 0.00002790
Iteration 135/1000 | Loss: 0.00002790
Iteration 136/1000 | Loss: 0.00002790
Iteration 137/1000 | Loss: 0.00002789
Iteration 138/1000 | Loss: 0.00002786
Iteration 139/1000 | Loss: 0.00002786
Iteration 140/1000 | Loss: 0.00002785
Iteration 141/1000 | Loss: 0.00002785
Iteration 142/1000 | Loss: 0.00002785
Iteration 143/1000 | Loss: 0.00002785
Iteration 144/1000 | Loss: 0.00002785
Iteration 145/1000 | Loss: 0.00002785
Iteration 146/1000 | Loss: 0.00002783
Iteration 147/1000 | Loss: 0.00002779
Iteration 148/1000 | Loss: 0.00002778
Iteration 149/1000 | Loss: 0.00002777
Iteration 150/1000 | Loss: 0.00002777
Iteration 151/1000 | Loss: 0.00002776
Iteration 152/1000 | Loss: 0.00002774
Iteration 153/1000 | Loss: 0.00002774
Iteration 154/1000 | Loss: 0.00002774
Iteration 155/1000 | Loss: 0.00002774
Iteration 156/1000 | Loss: 0.00002773
Iteration 157/1000 | Loss: 0.00002773
Iteration 158/1000 | Loss: 0.00002773
Iteration 159/1000 | Loss: 0.00002773
Iteration 160/1000 | Loss: 0.00002773
Iteration 161/1000 | Loss: 0.00002773
Iteration 162/1000 | Loss: 0.00002773
Iteration 163/1000 | Loss: 0.00002773
Iteration 164/1000 | Loss: 0.00002772
Iteration 165/1000 | Loss: 0.00002772
Iteration 166/1000 | Loss: 0.00002772
Iteration 167/1000 | Loss: 0.00002772
Iteration 168/1000 | Loss: 0.00002772
Iteration 169/1000 | Loss: 0.00002772
Iteration 170/1000 | Loss: 0.00002772
Iteration 171/1000 | Loss: 0.00002771
Iteration 172/1000 | Loss: 0.00002771
Iteration 173/1000 | Loss: 0.00002771
Iteration 174/1000 | Loss: 0.00002771
Iteration 175/1000 | Loss: 0.00002770
Iteration 176/1000 | Loss: 0.00002770
Iteration 177/1000 | Loss: 0.00002770
Iteration 178/1000 | Loss: 0.00002769
Iteration 179/1000 | Loss: 0.00002769
Iteration 180/1000 | Loss: 0.00002769
Iteration 181/1000 | Loss: 0.00002769
Iteration 182/1000 | Loss: 0.00002769
Iteration 183/1000 | Loss: 0.00002768
Iteration 184/1000 | Loss: 0.00002768
Iteration 185/1000 | Loss: 0.00002768
Iteration 186/1000 | Loss: 0.00002768
Iteration 187/1000 | Loss: 0.00002768
Iteration 188/1000 | Loss: 0.00002767
Iteration 189/1000 | Loss: 0.00002767
Iteration 190/1000 | Loss: 0.00002767
Iteration 191/1000 | Loss: 0.00002767
Iteration 192/1000 | Loss: 0.00002767
Iteration 193/1000 | Loss: 0.00002767
Iteration 194/1000 | Loss: 0.00002767
Iteration 195/1000 | Loss: 0.00002766
Iteration 196/1000 | Loss: 0.00002766
Iteration 197/1000 | Loss: 0.00002766
Iteration 198/1000 | Loss: 0.00002766
Iteration 199/1000 | Loss: 0.00002765
Iteration 200/1000 | Loss: 0.00002765
Iteration 201/1000 | Loss: 0.00002765
Iteration 202/1000 | Loss: 0.00002765
Iteration 203/1000 | Loss: 0.00002764
Iteration 204/1000 | Loss: 0.00002764
Iteration 205/1000 | Loss: 0.00002764
Iteration 206/1000 | Loss: 0.00002764
Iteration 207/1000 | Loss: 0.00002764
Iteration 208/1000 | Loss: 0.00002764
Iteration 209/1000 | Loss: 0.00002764
Iteration 210/1000 | Loss: 0.00002764
Iteration 211/1000 | Loss: 0.00002764
Iteration 212/1000 | Loss: 0.00002764
Iteration 213/1000 | Loss: 0.00002764
Iteration 214/1000 | Loss: 0.00002764
Iteration 215/1000 | Loss: 0.00002763
Iteration 216/1000 | Loss: 0.00002763
Iteration 217/1000 | Loss: 0.00002763
Iteration 218/1000 | Loss: 0.00002763
Iteration 219/1000 | Loss: 0.00002763
Iteration 220/1000 | Loss: 0.00002763
Iteration 221/1000 | Loss: 0.00002763
Iteration 222/1000 | Loss: 0.00002762
Iteration 223/1000 | Loss: 0.00002762
Iteration 224/1000 | Loss: 0.00002762
Iteration 225/1000 | Loss: 0.00002762
Iteration 226/1000 | Loss: 0.00002762
Iteration 227/1000 | Loss: 0.00002762
Iteration 228/1000 | Loss: 0.00002761
Iteration 229/1000 | Loss: 0.00002761
Iteration 230/1000 | Loss: 0.00002761
Iteration 231/1000 | Loss: 0.00002761
Iteration 232/1000 | Loss: 0.00002761
Iteration 233/1000 | Loss: 0.00002761
Iteration 234/1000 | Loss: 0.00002761
Iteration 235/1000 | Loss: 0.00002761
Iteration 236/1000 | Loss: 0.00002761
Iteration 237/1000 | Loss: 0.00002761
Iteration 238/1000 | Loss: 0.00002761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.7611209588940255e-05, 2.7611209588940255e-05, 2.7611209588940255e-05, 2.7611209588940255e-05, 2.7611209588940255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7611209588940255e-05

Optimization complete. Final v2v error: 4.415172576904297 mm

Highest mean error: 6.601564884185791 mm for frame 84

Lowest mean error: 3.917933940887451 mm for frame 1

Saving results

Total time: 155.28799533843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027776
Iteration 2/25 | Loss: 0.00277046
Iteration 3/25 | Loss: 0.00209356
Iteration 4/25 | Loss: 0.00221222
Iteration 5/25 | Loss: 0.00179524
Iteration 6/25 | Loss: 0.00163733
Iteration 7/25 | Loss: 0.00154531
Iteration 8/25 | Loss: 0.00148457
Iteration 9/25 | Loss: 0.00147014
Iteration 10/25 | Loss: 0.00145960
Iteration 11/25 | Loss: 0.00145033
Iteration 12/25 | Loss: 0.00145206
Iteration 13/25 | Loss: 0.00145158
Iteration 14/25 | Loss: 0.00144192
Iteration 15/25 | Loss: 0.00143706
Iteration 16/25 | Loss: 0.00143764
Iteration 17/25 | Loss: 0.00143704
Iteration 18/25 | Loss: 0.00143705
Iteration 19/25 | Loss: 0.00143726
Iteration 20/25 | Loss: 0.00143676
Iteration 21/25 | Loss: 0.00143733
Iteration 22/25 | Loss: 0.00143719
Iteration 23/25 | Loss: 0.00143665
Iteration 24/25 | Loss: 0.00143677
Iteration 25/25 | Loss: 0.00143546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06853116
Iteration 2/25 | Loss: 0.00362746
Iteration 3/25 | Loss: 0.00362746
Iteration 4/25 | Loss: 0.00362746
Iteration 5/25 | Loss: 0.00362746
Iteration 6/25 | Loss: 0.00362746
Iteration 7/25 | Loss: 0.00362746
Iteration 8/25 | Loss: 0.00362746
Iteration 9/25 | Loss: 0.00362746
Iteration 10/25 | Loss: 0.00362746
Iteration 11/25 | Loss: 0.00362746
Iteration 12/25 | Loss: 0.00362746
Iteration 13/25 | Loss: 0.00362746
Iteration 14/25 | Loss: 0.00362746
Iteration 15/25 | Loss: 0.00362746
Iteration 16/25 | Loss: 0.00362746
Iteration 17/25 | Loss: 0.00362746
Iteration 18/25 | Loss: 0.00362746
Iteration 19/25 | Loss: 0.00362746
Iteration 20/25 | Loss: 0.00362746
Iteration 21/25 | Loss: 0.00362746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0036274574231356382, 0.0036274574231356382, 0.0036274574231356382, 0.0036274574231356382, 0.0036274574231356382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036274574231356382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362746
Iteration 2/1000 | Loss: 0.00057441
Iteration 3/1000 | Loss: 0.00013236
Iteration 4/1000 | Loss: 0.00009087
Iteration 5/1000 | Loss: 0.00008275
Iteration 6/1000 | Loss: 0.00007100
Iteration 7/1000 | Loss: 0.00006229
Iteration 8/1000 | Loss: 0.00007193
Iteration 9/1000 | Loss: 0.00007250
Iteration 10/1000 | Loss: 0.00007172
Iteration 11/1000 | Loss: 0.00006607
Iteration 12/1000 | Loss: 0.00006618
Iteration 13/1000 | Loss: 0.00005410
Iteration 14/1000 | Loss: 0.00016831
Iteration 15/1000 | Loss: 0.00028467
Iteration 16/1000 | Loss: 0.00015435
Iteration 17/1000 | Loss: 0.00009484
Iteration 18/1000 | Loss: 0.00007624
Iteration 19/1000 | Loss: 0.00006648
Iteration 20/1000 | Loss: 0.00005802
Iteration 21/1000 | Loss: 0.00004937
Iteration 22/1000 | Loss: 0.00005365
Iteration 23/1000 | Loss: 0.00005463
Iteration 24/1000 | Loss: 0.00005319
Iteration 25/1000 | Loss: 0.00005906
Iteration 26/1000 | Loss: 0.00005758
Iteration 27/1000 | Loss: 0.00005611
Iteration 28/1000 | Loss: 0.00006445
Iteration 29/1000 | Loss: 0.00005576
Iteration 30/1000 | Loss: 0.00004981
Iteration 31/1000 | Loss: 0.00004081
Iteration 32/1000 | Loss: 0.00003724
Iteration 33/1000 | Loss: 0.00003586
Iteration 34/1000 | Loss: 0.00003516
Iteration 35/1000 | Loss: 0.00003476
Iteration 36/1000 | Loss: 0.00003443
Iteration 37/1000 | Loss: 0.00003414
Iteration 38/1000 | Loss: 0.00003402
Iteration 39/1000 | Loss: 0.00003400
Iteration 40/1000 | Loss: 0.00003391
Iteration 41/1000 | Loss: 0.00003382
Iteration 42/1000 | Loss: 0.00003374
Iteration 43/1000 | Loss: 0.00003366
Iteration 44/1000 | Loss: 0.00003362
Iteration 45/1000 | Loss: 0.00003362
Iteration 46/1000 | Loss: 0.00003362
Iteration 47/1000 | Loss: 0.00003362
Iteration 48/1000 | Loss: 0.00003362
Iteration 49/1000 | Loss: 0.00003361
Iteration 50/1000 | Loss: 0.00003361
Iteration 51/1000 | Loss: 0.00003361
Iteration 52/1000 | Loss: 0.00003361
Iteration 53/1000 | Loss: 0.00003361
Iteration 54/1000 | Loss: 0.00003358
Iteration 55/1000 | Loss: 0.00003356
Iteration 56/1000 | Loss: 0.00003355
Iteration 57/1000 | Loss: 0.00003355
Iteration 58/1000 | Loss: 0.00003354
Iteration 59/1000 | Loss: 0.00003354
Iteration 60/1000 | Loss: 0.00003353
Iteration 61/1000 | Loss: 0.00003352
Iteration 62/1000 | Loss: 0.00003351
Iteration 63/1000 | Loss: 0.00003351
Iteration 64/1000 | Loss: 0.00003350
Iteration 65/1000 | Loss: 0.00003350
Iteration 66/1000 | Loss: 0.00003350
Iteration 67/1000 | Loss: 0.00003350
Iteration 68/1000 | Loss: 0.00003349
Iteration 69/1000 | Loss: 0.00003349
Iteration 70/1000 | Loss: 0.00003349
Iteration 71/1000 | Loss: 0.00003348
Iteration 72/1000 | Loss: 0.00003348
Iteration 73/1000 | Loss: 0.00003348
Iteration 74/1000 | Loss: 0.00003348
Iteration 75/1000 | Loss: 0.00003348
Iteration 76/1000 | Loss: 0.00003348
Iteration 77/1000 | Loss: 0.00003348
Iteration 78/1000 | Loss: 0.00003347
Iteration 79/1000 | Loss: 0.00003347
Iteration 80/1000 | Loss: 0.00003347
Iteration 81/1000 | Loss: 0.00003346
Iteration 82/1000 | Loss: 0.00003346
Iteration 83/1000 | Loss: 0.00003345
Iteration 84/1000 | Loss: 0.00003345
Iteration 85/1000 | Loss: 0.00003345
Iteration 86/1000 | Loss: 0.00003345
Iteration 87/1000 | Loss: 0.00003344
Iteration 88/1000 | Loss: 0.00003344
Iteration 89/1000 | Loss: 0.00003343
Iteration 90/1000 | Loss: 0.00003343
Iteration 91/1000 | Loss: 0.00003343
Iteration 92/1000 | Loss: 0.00003343
Iteration 93/1000 | Loss: 0.00003341
Iteration 94/1000 | Loss: 0.00003340
Iteration 95/1000 | Loss: 0.00003340
Iteration 96/1000 | Loss: 0.00003340
Iteration 97/1000 | Loss: 0.00003340
Iteration 98/1000 | Loss: 0.00003340
Iteration 99/1000 | Loss: 0.00003340
Iteration 100/1000 | Loss: 0.00003340
Iteration 101/1000 | Loss: 0.00003340
Iteration 102/1000 | Loss: 0.00003339
Iteration 103/1000 | Loss: 0.00003339
Iteration 104/1000 | Loss: 0.00003339
Iteration 105/1000 | Loss: 0.00003339
Iteration 106/1000 | Loss: 0.00003339
Iteration 107/1000 | Loss: 0.00003338
Iteration 108/1000 | Loss: 0.00003338
Iteration 109/1000 | Loss: 0.00003336
Iteration 110/1000 | Loss: 0.00003335
Iteration 111/1000 | Loss: 0.00003335
Iteration 112/1000 | Loss: 0.00003334
Iteration 113/1000 | Loss: 0.00003333
Iteration 114/1000 | Loss: 0.00003333
Iteration 115/1000 | Loss: 0.00003332
Iteration 116/1000 | Loss: 0.00003332
Iteration 117/1000 | Loss: 0.00003332
Iteration 118/1000 | Loss: 0.00003332
Iteration 119/1000 | Loss: 0.00003332
Iteration 120/1000 | Loss: 0.00003331
Iteration 121/1000 | Loss: 0.00003331
Iteration 122/1000 | Loss: 0.00003330
Iteration 123/1000 | Loss: 0.00003330
Iteration 124/1000 | Loss: 0.00003329
Iteration 125/1000 | Loss: 0.00003328
Iteration 126/1000 | Loss: 0.00003324
Iteration 127/1000 | Loss: 0.00003324
Iteration 128/1000 | Loss: 0.00003324
Iteration 129/1000 | Loss: 0.00003324
Iteration 130/1000 | Loss: 0.00003324
Iteration 131/1000 | Loss: 0.00003324
Iteration 132/1000 | Loss: 0.00003324
Iteration 133/1000 | Loss: 0.00003323
Iteration 134/1000 | Loss: 0.00003323
Iteration 135/1000 | Loss: 0.00003322
Iteration 136/1000 | Loss: 0.00003322
Iteration 137/1000 | Loss: 0.00003322
Iteration 138/1000 | Loss: 0.00003322
Iteration 139/1000 | Loss: 0.00003322
Iteration 140/1000 | Loss: 0.00003322
Iteration 141/1000 | Loss: 0.00003321
Iteration 142/1000 | Loss: 0.00003321
Iteration 143/1000 | Loss: 0.00003321
Iteration 144/1000 | Loss: 0.00003321
Iteration 145/1000 | Loss: 0.00009608
Iteration 146/1000 | Loss: 0.00005880
Iteration 147/1000 | Loss: 0.00015844
Iteration 148/1000 | Loss: 0.00013233
Iteration 149/1000 | Loss: 0.00014871
Iteration 150/1000 | Loss: 0.00007726
Iteration 151/1000 | Loss: 0.00004782
Iteration 152/1000 | Loss: 0.00003858
Iteration 153/1000 | Loss: 0.00003376
Iteration 154/1000 | Loss: 0.00003231
Iteration 155/1000 | Loss: 0.00003087
Iteration 156/1000 | Loss: 0.00002995
Iteration 157/1000 | Loss: 0.00002944
Iteration 158/1000 | Loss: 0.00002917
Iteration 159/1000 | Loss: 0.00002905
Iteration 160/1000 | Loss: 0.00002903
Iteration 161/1000 | Loss: 0.00002901
Iteration 162/1000 | Loss: 0.00002900
Iteration 163/1000 | Loss: 0.00002900
Iteration 164/1000 | Loss: 0.00002900
Iteration 165/1000 | Loss: 0.00002899
Iteration 166/1000 | Loss: 0.00002899
Iteration 167/1000 | Loss: 0.00002897
Iteration 168/1000 | Loss: 0.00002896
Iteration 169/1000 | Loss: 0.00002896
Iteration 170/1000 | Loss: 0.00002895
Iteration 171/1000 | Loss: 0.00002895
Iteration 172/1000 | Loss: 0.00002894
Iteration 173/1000 | Loss: 0.00002894
Iteration 174/1000 | Loss: 0.00002893
Iteration 175/1000 | Loss: 0.00002893
Iteration 176/1000 | Loss: 0.00002893
Iteration 177/1000 | Loss: 0.00002893
Iteration 178/1000 | Loss: 0.00002893
Iteration 179/1000 | Loss: 0.00002893
Iteration 180/1000 | Loss: 0.00002893
Iteration 181/1000 | Loss: 0.00002892
Iteration 182/1000 | Loss: 0.00002892
Iteration 183/1000 | Loss: 0.00002892
Iteration 184/1000 | Loss: 0.00002892
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002891
Iteration 188/1000 | Loss: 0.00002891
Iteration 189/1000 | Loss: 0.00002891
Iteration 190/1000 | Loss: 0.00002891
Iteration 191/1000 | Loss: 0.00002891
Iteration 192/1000 | Loss: 0.00002890
Iteration 193/1000 | Loss: 0.00002890
Iteration 194/1000 | Loss: 0.00002890
Iteration 195/1000 | Loss: 0.00002890
Iteration 196/1000 | Loss: 0.00002890
Iteration 197/1000 | Loss: 0.00002889
Iteration 198/1000 | Loss: 0.00002889
Iteration 199/1000 | Loss: 0.00002889
Iteration 200/1000 | Loss: 0.00002889
Iteration 201/1000 | Loss: 0.00002888
Iteration 202/1000 | Loss: 0.00002888
Iteration 203/1000 | Loss: 0.00002888
Iteration 204/1000 | Loss: 0.00002888
Iteration 205/1000 | Loss: 0.00002888
Iteration 206/1000 | Loss: 0.00002888
Iteration 207/1000 | Loss: 0.00002888
Iteration 208/1000 | Loss: 0.00002888
Iteration 209/1000 | Loss: 0.00002888
Iteration 210/1000 | Loss: 0.00002887
Iteration 211/1000 | Loss: 0.00002887
Iteration 212/1000 | Loss: 0.00002887
Iteration 213/1000 | Loss: 0.00002887
Iteration 214/1000 | Loss: 0.00002887
Iteration 215/1000 | Loss: 0.00002887
Iteration 216/1000 | Loss: 0.00002887
Iteration 217/1000 | Loss: 0.00002887
Iteration 218/1000 | Loss: 0.00002887
Iteration 219/1000 | Loss: 0.00002887
Iteration 220/1000 | Loss: 0.00002887
Iteration 221/1000 | Loss: 0.00002887
Iteration 222/1000 | Loss: 0.00002887
Iteration 223/1000 | Loss: 0.00002887
Iteration 224/1000 | Loss: 0.00002887
Iteration 225/1000 | Loss: 0.00002887
Iteration 226/1000 | Loss: 0.00002887
Iteration 227/1000 | Loss: 0.00002886
Iteration 228/1000 | Loss: 0.00002886
Iteration 229/1000 | Loss: 0.00002886
Iteration 230/1000 | Loss: 0.00002886
Iteration 231/1000 | Loss: 0.00002886
Iteration 232/1000 | Loss: 0.00002886
Iteration 233/1000 | Loss: 0.00002885
Iteration 234/1000 | Loss: 0.00002885
Iteration 235/1000 | Loss: 0.00002885
Iteration 236/1000 | Loss: 0.00002884
Iteration 237/1000 | Loss: 0.00002884
Iteration 238/1000 | Loss: 0.00002884
Iteration 239/1000 | Loss: 0.00002884
Iteration 240/1000 | Loss: 0.00002883
Iteration 241/1000 | Loss: 0.00002883
Iteration 242/1000 | Loss: 0.00002883
Iteration 243/1000 | Loss: 0.00002883
Iteration 244/1000 | Loss: 0.00002883
Iteration 245/1000 | Loss: 0.00002883
Iteration 246/1000 | Loss: 0.00002883
Iteration 247/1000 | Loss: 0.00002883
Iteration 248/1000 | Loss: 0.00002883
Iteration 249/1000 | Loss: 0.00002883
Iteration 250/1000 | Loss: 0.00002883
Iteration 251/1000 | Loss: 0.00002883
Iteration 252/1000 | Loss: 0.00002882
Iteration 253/1000 | Loss: 0.00002882
Iteration 254/1000 | Loss: 0.00002882
Iteration 255/1000 | Loss: 0.00002882
Iteration 256/1000 | Loss: 0.00002882
Iteration 257/1000 | Loss: 0.00002882
Iteration 258/1000 | Loss: 0.00002882
Iteration 259/1000 | Loss: 0.00002882
Iteration 260/1000 | Loss: 0.00002882
Iteration 261/1000 | Loss: 0.00002882
Iteration 262/1000 | Loss: 0.00002882
Iteration 263/1000 | Loss: 0.00002882
Iteration 264/1000 | Loss: 0.00002881
Iteration 265/1000 | Loss: 0.00002881
Iteration 266/1000 | Loss: 0.00002881
Iteration 267/1000 | Loss: 0.00002881
Iteration 268/1000 | Loss: 0.00002881
Iteration 269/1000 | Loss: 0.00002881
Iteration 270/1000 | Loss: 0.00002881
Iteration 271/1000 | Loss: 0.00002881
Iteration 272/1000 | Loss: 0.00002881
Iteration 273/1000 | Loss: 0.00002880
Iteration 274/1000 | Loss: 0.00002880
Iteration 275/1000 | Loss: 0.00002880
Iteration 276/1000 | Loss: 0.00002880
Iteration 277/1000 | Loss: 0.00002879
Iteration 278/1000 | Loss: 0.00002879
Iteration 279/1000 | Loss: 0.00002879
Iteration 280/1000 | Loss: 0.00002879
Iteration 281/1000 | Loss: 0.00002879
Iteration 282/1000 | Loss: 0.00002879
Iteration 283/1000 | Loss: 0.00002879
Iteration 284/1000 | Loss: 0.00002879
Iteration 285/1000 | Loss: 0.00002879
Iteration 286/1000 | Loss: 0.00002879
Iteration 287/1000 | Loss: 0.00002879
Iteration 288/1000 | Loss: 0.00002879
Iteration 289/1000 | Loss: 0.00002879
Iteration 290/1000 | Loss: 0.00002879
Iteration 291/1000 | Loss: 0.00002879
Iteration 292/1000 | Loss: 0.00002879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.8785008908016607e-05, 2.8785008908016607e-05, 2.8785008908016607e-05, 2.8785008908016607e-05, 2.8785008908016607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8785008908016607e-05

Optimization complete. Final v2v error: 4.013566493988037 mm

Highest mean error: 12.24002742767334 mm for frame 167

Lowest mean error: 3.5013678073883057 mm for frame 152

Saving results

Total time: 138.9007167816162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00244874
Iteration 2/25 | Loss: 0.00141623
Iteration 3/25 | Loss: 0.00133673
Iteration 4/25 | Loss: 0.00131987
Iteration 5/25 | Loss: 0.00131574
Iteration 6/25 | Loss: 0.00131496
Iteration 7/25 | Loss: 0.00131496
Iteration 8/25 | Loss: 0.00131496
Iteration 9/25 | Loss: 0.00131496
Iteration 10/25 | Loss: 0.00131496
Iteration 11/25 | Loss: 0.00131496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013149617007002234, 0.0013149617007002234, 0.0013149617007002234, 0.0013149617007002234, 0.0013149617007002234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013149617007002234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10256946
Iteration 2/25 | Loss: 0.00518315
Iteration 3/25 | Loss: 0.00518315
Iteration 4/25 | Loss: 0.00518315
Iteration 5/25 | Loss: 0.00518315
Iteration 6/25 | Loss: 0.00518315
Iteration 7/25 | Loss: 0.00518315
Iteration 8/25 | Loss: 0.00518315
Iteration 9/25 | Loss: 0.00518315
Iteration 10/25 | Loss: 0.00518315
Iteration 11/25 | Loss: 0.00518315
Iteration 12/25 | Loss: 0.00518315
Iteration 13/25 | Loss: 0.00518315
Iteration 14/25 | Loss: 0.00518315
Iteration 15/25 | Loss: 0.00518315
Iteration 16/25 | Loss: 0.00518315
Iteration 17/25 | Loss: 0.00518315
Iteration 18/25 | Loss: 0.00518315
Iteration 19/25 | Loss: 0.00518315
Iteration 20/25 | Loss: 0.00518315
Iteration 21/25 | Loss: 0.00518315
Iteration 22/25 | Loss: 0.00518315
Iteration 23/25 | Loss: 0.00518315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.005183147266507149, 0.005183147266507149, 0.005183147266507149, 0.005183147266507149, 0.005183147266507149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005183147266507149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00518315
Iteration 2/1000 | Loss: 0.00007894
Iteration 3/1000 | Loss: 0.00003602
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002294
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002199
Iteration 9/1000 | Loss: 0.00002159
Iteration 10/1000 | Loss: 0.00002132
Iteration 11/1000 | Loss: 0.00002107
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002062
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00002051
Iteration 18/1000 | Loss: 0.00002050
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002044
Iteration 22/1000 | Loss: 0.00002044
Iteration 23/1000 | Loss: 0.00002043
Iteration 24/1000 | Loss: 0.00002043
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002038
Iteration 27/1000 | Loss: 0.00002038
Iteration 28/1000 | Loss: 0.00002037
Iteration 29/1000 | Loss: 0.00002037
Iteration 30/1000 | Loss: 0.00002036
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002035
Iteration 34/1000 | Loss: 0.00002034
Iteration 35/1000 | Loss: 0.00002033
Iteration 36/1000 | Loss: 0.00002033
Iteration 37/1000 | Loss: 0.00002032
Iteration 38/1000 | Loss: 0.00002032
Iteration 39/1000 | Loss: 0.00002031
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002028
Iteration 46/1000 | Loss: 0.00002028
Iteration 47/1000 | Loss: 0.00002028
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002019
Iteration 55/1000 | Loss: 0.00002019
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002018
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002015
Iteration 62/1000 | Loss: 0.00002015
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002013
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002013
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002012
Iteration 72/1000 | Loss: 0.00002012
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002011
Iteration 75/1000 | Loss: 0.00002011
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002011
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00002010
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002008
Iteration 83/1000 | Loss: 0.00002008
Iteration 84/1000 | Loss: 0.00002008
Iteration 85/1000 | Loss: 0.00002008
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002007
Iteration 95/1000 | Loss: 0.00002007
Iteration 96/1000 | Loss: 0.00002007
Iteration 97/1000 | Loss: 0.00002007
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002006
Iteration 100/1000 | Loss: 0.00002006
Iteration 101/1000 | Loss: 0.00002006
Iteration 102/1000 | Loss: 0.00002005
Iteration 103/1000 | Loss: 0.00002005
Iteration 104/1000 | Loss: 0.00002005
Iteration 105/1000 | Loss: 0.00002005
Iteration 106/1000 | Loss: 0.00002005
Iteration 107/1000 | Loss: 0.00002005
Iteration 108/1000 | Loss: 0.00002004
Iteration 109/1000 | Loss: 0.00002004
Iteration 110/1000 | Loss: 0.00002004
Iteration 111/1000 | Loss: 0.00002004
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002003
Iteration 115/1000 | Loss: 0.00002003
Iteration 116/1000 | Loss: 0.00002003
Iteration 117/1000 | Loss: 0.00002003
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002003
Iteration 120/1000 | Loss: 0.00002003
Iteration 121/1000 | Loss: 0.00002003
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002003
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002001
Iteration 138/1000 | Loss: 0.00002000
Iteration 139/1000 | Loss: 0.00002000
Iteration 140/1000 | Loss: 0.00002000
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00001999
Iteration 146/1000 | Loss: 0.00001999
Iteration 147/1000 | Loss: 0.00001999
Iteration 148/1000 | Loss: 0.00001999
Iteration 149/1000 | Loss: 0.00001999
Iteration 150/1000 | Loss: 0.00001999
Iteration 151/1000 | Loss: 0.00001999
Iteration 152/1000 | Loss: 0.00001999
Iteration 153/1000 | Loss: 0.00001998
Iteration 154/1000 | Loss: 0.00001998
Iteration 155/1000 | Loss: 0.00001998
Iteration 156/1000 | Loss: 0.00001998
Iteration 157/1000 | Loss: 0.00001997
Iteration 158/1000 | Loss: 0.00001997
Iteration 159/1000 | Loss: 0.00001997
Iteration 160/1000 | Loss: 0.00001997
Iteration 161/1000 | Loss: 0.00001997
Iteration 162/1000 | Loss: 0.00001997
Iteration 163/1000 | Loss: 0.00001997
Iteration 164/1000 | Loss: 0.00001996
Iteration 165/1000 | Loss: 0.00001996
Iteration 166/1000 | Loss: 0.00001996
Iteration 167/1000 | Loss: 0.00001996
Iteration 168/1000 | Loss: 0.00001996
Iteration 169/1000 | Loss: 0.00001996
Iteration 170/1000 | Loss: 0.00001996
Iteration 171/1000 | Loss: 0.00001996
Iteration 172/1000 | Loss: 0.00001996
Iteration 173/1000 | Loss: 0.00001996
Iteration 174/1000 | Loss: 0.00001996
Iteration 175/1000 | Loss: 0.00001996
Iteration 176/1000 | Loss: 0.00001996
Iteration 177/1000 | Loss: 0.00001996
Iteration 178/1000 | Loss: 0.00001996
Iteration 179/1000 | Loss: 0.00001996
Iteration 180/1000 | Loss: 0.00001996
Iteration 181/1000 | Loss: 0.00001996
Iteration 182/1000 | Loss: 0.00001996
Iteration 183/1000 | Loss: 0.00001996
Iteration 184/1000 | Loss: 0.00001996
Iteration 185/1000 | Loss: 0.00001996
Iteration 186/1000 | Loss: 0.00001996
Iteration 187/1000 | Loss: 0.00001996
Iteration 188/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.996068749576807e-05, 1.996068749576807e-05, 1.996068749576807e-05, 1.996068749576807e-05, 1.996068749576807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.996068749576807e-05

Optimization complete. Final v2v error: 3.715571641921997 mm

Highest mean error: 4.089901447296143 mm for frame 44

Lowest mean error: 3.434846878051758 mm for frame 0

Saving results

Total time: 40.79627466201782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703860
Iteration 2/25 | Loss: 0.00184977
Iteration 3/25 | Loss: 0.00143783
Iteration 4/25 | Loss: 0.00131777
Iteration 5/25 | Loss: 0.00129590
Iteration 6/25 | Loss: 0.00128880
Iteration 7/25 | Loss: 0.00129204
Iteration 8/25 | Loss: 0.00128030
Iteration 9/25 | Loss: 0.00126527
Iteration 10/25 | Loss: 0.00126092
Iteration 11/25 | Loss: 0.00125994
Iteration 12/25 | Loss: 0.00125951
Iteration 13/25 | Loss: 0.00125934
Iteration 14/25 | Loss: 0.00125931
Iteration 15/25 | Loss: 0.00125930
Iteration 16/25 | Loss: 0.00125930
Iteration 17/25 | Loss: 0.00125929
Iteration 18/25 | Loss: 0.00125929
Iteration 19/25 | Loss: 0.00125929
Iteration 20/25 | Loss: 0.00125929
Iteration 21/25 | Loss: 0.00125929
Iteration 22/25 | Loss: 0.00125929
Iteration 23/25 | Loss: 0.00125929
Iteration 24/25 | Loss: 0.00125929
Iteration 25/25 | Loss: 0.00125929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.51900148
Iteration 2/25 | Loss: 0.00219337
Iteration 3/25 | Loss: 0.00219314
Iteration 4/25 | Loss: 0.00219314
Iteration 5/25 | Loss: 0.00219314
Iteration 6/25 | Loss: 0.00219314
Iteration 7/25 | Loss: 0.00219314
Iteration 8/25 | Loss: 0.00219314
Iteration 9/25 | Loss: 0.00219314
Iteration 10/25 | Loss: 0.00219314
Iteration 11/25 | Loss: 0.00219314
Iteration 12/25 | Loss: 0.00219314
Iteration 13/25 | Loss: 0.00219314
Iteration 14/25 | Loss: 0.00219314
Iteration 15/25 | Loss: 0.00219314
Iteration 16/25 | Loss: 0.00219314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002193138934671879, 0.002193138934671879, 0.002193138934671879, 0.002193138934671879, 0.002193138934671879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002193138934671879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219314
Iteration 2/1000 | Loss: 0.00005862
Iteration 3/1000 | Loss: 0.00003885
Iteration 4/1000 | Loss: 0.00003283
Iteration 5/1000 | Loss: 0.00003058
Iteration 6/1000 | Loss: 0.00002908
Iteration 7/1000 | Loss: 0.00002829
Iteration 8/1000 | Loss: 0.00002756
Iteration 9/1000 | Loss: 0.00002715
Iteration 10/1000 | Loss: 0.00002682
Iteration 11/1000 | Loss: 0.00002656
Iteration 12/1000 | Loss: 0.00002631
Iteration 13/1000 | Loss: 0.00002604
Iteration 14/1000 | Loss: 0.00002576
Iteration 15/1000 | Loss: 0.00002557
Iteration 16/1000 | Loss: 0.00002540
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002489
Iteration 20/1000 | Loss: 0.00002485
Iteration 21/1000 | Loss: 0.00002479
Iteration 22/1000 | Loss: 0.00002474
Iteration 23/1000 | Loss: 0.00002474
Iteration 24/1000 | Loss: 0.00002472
Iteration 25/1000 | Loss: 0.00002471
Iteration 26/1000 | Loss: 0.00002470
Iteration 27/1000 | Loss: 0.00002470
Iteration 28/1000 | Loss: 0.00002468
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002465
Iteration 31/1000 | Loss: 0.00002464
Iteration 32/1000 | Loss: 0.00002463
Iteration 33/1000 | Loss: 0.00002460
Iteration 34/1000 | Loss: 0.00002460
Iteration 35/1000 | Loss: 0.00002460
Iteration 36/1000 | Loss: 0.00002460
Iteration 37/1000 | Loss: 0.00002460
Iteration 38/1000 | Loss: 0.00002459
Iteration 39/1000 | Loss: 0.00002459
Iteration 40/1000 | Loss: 0.00002459
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002456
Iteration 44/1000 | Loss: 0.00002456
Iteration 45/1000 | Loss: 0.00002455
Iteration 46/1000 | Loss: 0.00002455
Iteration 47/1000 | Loss: 0.00002455
Iteration 48/1000 | Loss: 0.00002455
Iteration 49/1000 | Loss: 0.00002455
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00002452
Iteration 52/1000 | Loss: 0.00002452
Iteration 53/1000 | Loss: 0.00002452
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002452
Iteration 56/1000 | Loss: 0.00002451
Iteration 57/1000 | Loss: 0.00002451
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002448
Iteration 62/1000 | Loss: 0.00002448
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002448
Iteration 65/1000 | Loss: 0.00002448
Iteration 66/1000 | Loss: 0.00002448
Iteration 67/1000 | Loss: 0.00002448
Iteration 68/1000 | Loss: 0.00002448
Iteration 69/1000 | Loss: 0.00002448
Iteration 70/1000 | Loss: 0.00002448
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002446
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002444
Iteration 82/1000 | Loss: 0.00002444
Iteration 83/1000 | Loss: 0.00002444
Iteration 84/1000 | Loss: 0.00002444
Iteration 85/1000 | Loss: 0.00002444
Iteration 86/1000 | Loss: 0.00002444
Iteration 87/1000 | Loss: 0.00002444
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002444
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002442
Iteration 92/1000 | Loss: 0.00002442
Iteration 93/1000 | Loss: 0.00002442
Iteration 94/1000 | Loss: 0.00002442
Iteration 95/1000 | Loss: 0.00002441
Iteration 96/1000 | Loss: 0.00002441
Iteration 97/1000 | Loss: 0.00002441
Iteration 98/1000 | Loss: 0.00002440
Iteration 99/1000 | Loss: 0.00002440
Iteration 100/1000 | Loss: 0.00002440
Iteration 101/1000 | Loss: 0.00002440
Iteration 102/1000 | Loss: 0.00002440
Iteration 103/1000 | Loss: 0.00002440
Iteration 104/1000 | Loss: 0.00002439
Iteration 105/1000 | Loss: 0.00002439
Iteration 106/1000 | Loss: 0.00002439
Iteration 107/1000 | Loss: 0.00002439
Iteration 108/1000 | Loss: 0.00002439
Iteration 109/1000 | Loss: 0.00002439
Iteration 110/1000 | Loss: 0.00002439
Iteration 111/1000 | Loss: 0.00002438
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002438
Iteration 115/1000 | Loss: 0.00002438
Iteration 116/1000 | Loss: 0.00002437
Iteration 117/1000 | Loss: 0.00002437
Iteration 118/1000 | Loss: 0.00002437
Iteration 119/1000 | Loss: 0.00002437
Iteration 120/1000 | Loss: 0.00002437
Iteration 121/1000 | Loss: 0.00002437
Iteration 122/1000 | Loss: 0.00002437
Iteration 123/1000 | Loss: 0.00002437
Iteration 124/1000 | Loss: 0.00002437
Iteration 125/1000 | Loss: 0.00002437
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002436
Iteration 128/1000 | Loss: 0.00002436
Iteration 129/1000 | Loss: 0.00002436
Iteration 130/1000 | Loss: 0.00002436
Iteration 131/1000 | Loss: 0.00002436
Iteration 132/1000 | Loss: 0.00002436
Iteration 133/1000 | Loss: 0.00002436
Iteration 134/1000 | Loss: 0.00002436
Iteration 135/1000 | Loss: 0.00002436
Iteration 136/1000 | Loss: 0.00002435
Iteration 137/1000 | Loss: 0.00002435
Iteration 138/1000 | Loss: 0.00002435
Iteration 139/1000 | Loss: 0.00002435
Iteration 140/1000 | Loss: 0.00002434
Iteration 141/1000 | Loss: 0.00002434
Iteration 142/1000 | Loss: 0.00002434
Iteration 143/1000 | Loss: 0.00002434
Iteration 144/1000 | Loss: 0.00002434
Iteration 145/1000 | Loss: 0.00002434
Iteration 146/1000 | Loss: 0.00002434
Iteration 147/1000 | Loss: 0.00002433
Iteration 148/1000 | Loss: 0.00002433
Iteration 149/1000 | Loss: 0.00002433
Iteration 150/1000 | Loss: 0.00002433
Iteration 151/1000 | Loss: 0.00002432
Iteration 152/1000 | Loss: 0.00002432
Iteration 153/1000 | Loss: 0.00002432
Iteration 154/1000 | Loss: 0.00002432
Iteration 155/1000 | Loss: 0.00002431
Iteration 156/1000 | Loss: 0.00002431
Iteration 157/1000 | Loss: 0.00002431
Iteration 158/1000 | Loss: 0.00002431
Iteration 159/1000 | Loss: 0.00002431
Iteration 160/1000 | Loss: 0.00002431
Iteration 161/1000 | Loss: 0.00002431
Iteration 162/1000 | Loss: 0.00002431
Iteration 163/1000 | Loss: 0.00002431
Iteration 164/1000 | Loss: 0.00002431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.4310578737640753e-05, 2.4310578737640753e-05, 2.4310578737640753e-05, 2.4310578737640753e-05, 2.4310578737640753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4310578737640753e-05

Optimization complete. Final v2v error: 3.9746780395507812 mm

Highest mean error: 4.487215518951416 mm for frame 194

Lowest mean error: 3.3657279014587402 mm for frame 212

Saving results

Total time: 72.35994219779968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918602
Iteration 2/25 | Loss: 0.00163065
Iteration 3/25 | Loss: 0.00144095
Iteration 4/25 | Loss: 0.00141794
Iteration 5/25 | Loss: 0.00141345
Iteration 6/25 | Loss: 0.00141303
Iteration 7/25 | Loss: 0.00141303
Iteration 8/25 | Loss: 0.00141303
Iteration 9/25 | Loss: 0.00141303
Iteration 10/25 | Loss: 0.00141303
Iteration 11/25 | Loss: 0.00141303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014130346244201064, 0.0014130346244201064, 0.0014130346244201064, 0.0014130346244201064, 0.0014130346244201064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014130346244201064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09098160
Iteration 2/25 | Loss: 0.00331823
Iteration 3/25 | Loss: 0.00331823
Iteration 4/25 | Loss: 0.00331823
Iteration 5/25 | Loss: 0.00331823
Iteration 6/25 | Loss: 0.00331823
Iteration 7/25 | Loss: 0.00331823
Iteration 8/25 | Loss: 0.00331823
Iteration 9/25 | Loss: 0.00331822
Iteration 10/25 | Loss: 0.00331822
Iteration 11/25 | Loss: 0.00331822
Iteration 12/25 | Loss: 0.00331822
Iteration 13/25 | Loss: 0.00331822
Iteration 14/25 | Loss: 0.00331822
Iteration 15/25 | Loss: 0.00331822
Iteration 16/25 | Loss: 0.00331822
Iteration 17/25 | Loss: 0.00331822
Iteration 18/25 | Loss: 0.00331822
Iteration 19/25 | Loss: 0.00331822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0033182245679199696, 0.0033182245679199696, 0.0033182245679199696, 0.0033182245679199696, 0.0033182245679199696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033182245679199696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331822
Iteration 2/1000 | Loss: 0.00011672
Iteration 3/1000 | Loss: 0.00007217
Iteration 4/1000 | Loss: 0.00004895
Iteration 5/1000 | Loss: 0.00004336
Iteration 6/1000 | Loss: 0.00004033
Iteration 7/1000 | Loss: 0.00003855
Iteration 8/1000 | Loss: 0.00003744
Iteration 9/1000 | Loss: 0.00003669
Iteration 10/1000 | Loss: 0.00003612
Iteration 11/1000 | Loss: 0.00003569
Iteration 12/1000 | Loss: 0.00003531
Iteration 13/1000 | Loss: 0.00003495
Iteration 14/1000 | Loss: 0.00003463
Iteration 15/1000 | Loss: 0.00003434
Iteration 16/1000 | Loss: 0.00003408
Iteration 17/1000 | Loss: 0.00003391
Iteration 18/1000 | Loss: 0.00003386
Iteration 19/1000 | Loss: 0.00003375
Iteration 20/1000 | Loss: 0.00003374
Iteration 21/1000 | Loss: 0.00003374
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00003373
Iteration 25/1000 | Loss: 0.00003371
Iteration 26/1000 | Loss: 0.00003370
Iteration 27/1000 | Loss: 0.00003367
Iteration 28/1000 | Loss: 0.00003366
Iteration 29/1000 | Loss: 0.00003363
Iteration 30/1000 | Loss: 0.00003361
Iteration 31/1000 | Loss: 0.00003360
Iteration 32/1000 | Loss: 0.00003360
Iteration 33/1000 | Loss: 0.00003360
Iteration 34/1000 | Loss: 0.00003357
Iteration 35/1000 | Loss: 0.00003356
Iteration 36/1000 | Loss: 0.00003355
Iteration 37/1000 | Loss: 0.00003355
Iteration 38/1000 | Loss: 0.00003354
Iteration 39/1000 | Loss: 0.00003354
Iteration 40/1000 | Loss: 0.00003353
Iteration 41/1000 | Loss: 0.00003351
Iteration 42/1000 | Loss: 0.00003351
Iteration 43/1000 | Loss: 0.00003350
Iteration 44/1000 | Loss: 0.00003350
Iteration 45/1000 | Loss: 0.00003350
Iteration 46/1000 | Loss: 0.00003348
Iteration 47/1000 | Loss: 0.00003348
Iteration 48/1000 | Loss: 0.00003347
Iteration 49/1000 | Loss: 0.00003346
Iteration 50/1000 | Loss: 0.00003346
Iteration 51/1000 | Loss: 0.00003345
Iteration 52/1000 | Loss: 0.00003345
Iteration 53/1000 | Loss: 0.00003344
Iteration 54/1000 | Loss: 0.00003344
Iteration 55/1000 | Loss: 0.00003344
Iteration 56/1000 | Loss: 0.00003344
Iteration 57/1000 | Loss: 0.00003344
Iteration 58/1000 | Loss: 0.00003344
Iteration 59/1000 | Loss: 0.00003340
Iteration 60/1000 | Loss: 0.00003340
Iteration 61/1000 | Loss: 0.00003340
Iteration 62/1000 | Loss: 0.00003340
Iteration 63/1000 | Loss: 0.00003340
Iteration 64/1000 | Loss: 0.00003340
Iteration 65/1000 | Loss: 0.00003340
Iteration 66/1000 | Loss: 0.00003340
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003339
Iteration 69/1000 | Loss: 0.00003339
Iteration 70/1000 | Loss: 0.00003339
Iteration 71/1000 | Loss: 0.00003338
Iteration 72/1000 | Loss: 0.00003338
Iteration 73/1000 | Loss: 0.00003337
Iteration 74/1000 | Loss: 0.00003337
Iteration 75/1000 | Loss: 0.00003337
Iteration 76/1000 | Loss: 0.00003336
Iteration 77/1000 | Loss: 0.00003336
Iteration 78/1000 | Loss: 0.00003336
Iteration 79/1000 | Loss: 0.00003336
Iteration 80/1000 | Loss: 0.00003336
Iteration 81/1000 | Loss: 0.00003336
Iteration 82/1000 | Loss: 0.00003336
Iteration 83/1000 | Loss: 0.00003336
Iteration 84/1000 | Loss: 0.00003335
Iteration 85/1000 | Loss: 0.00003334
Iteration 86/1000 | Loss: 0.00003334
Iteration 87/1000 | Loss: 0.00003334
Iteration 88/1000 | Loss: 0.00003333
Iteration 89/1000 | Loss: 0.00003333
Iteration 90/1000 | Loss: 0.00003333
Iteration 91/1000 | Loss: 0.00003332
Iteration 92/1000 | Loss: 0.00003332
Iteration 93/1000 | Loss: 0.00003331
Iteration 94/1000 | Loss: 0.00003331
Iteration 95/1000 | Loss: 0.00003331
Iteration 96/1000 | Loss: 0.00003331
Iteration 97/1000 | Loss: 0.00003331
Iteration 98/1000 | Loss: 0.00003331
Iteration 99/1000 | Loss: 0.00003331
Iteration 100/1000 | Loss: 0.00003331
Iteration 101/1000 | Loss: 0.00003331
Iteration 102/1000 | Loss: 0.00003331
Iteration 103/1000 | Loss: 0.00003330
Iteration 104/1000 | Loss: 0.00003330
Iteration 105/1000 | Loss: 0.00003329
Iteration 106/1000 | Loss: 0.00003329
Iteration 107/1000 | Loss: 0.00003329
Iteration 108/1000 | Loss: 0.00003329
Iteration 109/1000 | Loss: 0.00003329
Iteration 110/1000 | Loss: 0.00003329
Iteration 111/1000 | Loss: 0.00003328
Iteration 112/1000 | Loss: 0.00003328
Iteration 113/1000 | Loss: 0.00003328
Iteration 114/1000 | Loss: 0.00003328
Iteration 115/1000 | Loss: 0.00003328
Iteration 116/1000 | Loss: 0.00003328
Iteration 117/1000 | Loss: 0.00003327
Iteration 118/1000 | Loss: 0.00003327
Iteration 119/1000 | Loss: 0.00003327
Iteration 120/1000 | Loss: 0.00003327
Iteration 121/1000 | Loss: 0.00003326
Iteration 122/1000 | Loss: 0.00003326
Iteration 123/1000 | Loss: 0.00003326
Iteration 124/1000 | Loss: 0.00003326
Iteration 125/1000 | Loss: 0.00003326
Iteration 126/1000 | Loss: 0.00003325
Iteration 127/1000 | Loss: 0.00003325
Iteration 128/1000 | Loss: 0.00003325
Iteration 129/1000 | Loss: 0.00003324
Iteration 130/1000 | Loss: 0.00003324
Iteration 131/1000 | Loss: 0.00003324
Iteration 132/1000 | Loss: 0.00003323
Iteration 133/1000 | Loss: 0.00003323
Iteration 134/1000 | Loss: 0.00003323
Iteration 135/1000 | Loss: 0.00003323
Iteration 136/1000 | Loss: 0.00003323
Iteration 137/1000 | Loss: 0.00003323
Iteration 138/1000 | Loss: 0.00003323
Iteration 139/1000 | Loss: 0.00003323
Iteration 140/1000 | Loss: 0.00003323
Iteration 141/1000 | Loss: 0.00003323
Iteration 142/1000 | Loss: 0.00003323
Iteration 143/1000 | Loss: 0.00003322
Iteration 144/1000 | Loss: 0.00003322
Iteration 145/1000 | Loss: 0.00003322
Iteration 146/1000 | Loss: 0.00003322
Iteration 147/1000 | Loss: 0.00003322
Iteration 148/1000 | Loss: 0.00003320
Iteration 149/1000 | Loss: 0.00003320
Iteration 150/1000 | Loss: 0.00003320
Iteration 151/1000 | Loss: 0.00003320
Iteration 152/1000 | Loss: 0.00003320
Iteration 153/1000 | Loss: 0.00003320
Iteration 154/1000 | Loss: 0.00003320
Iteration 155/1000 | Loss: 0.00003320
Iteration 156/1000 | Loss: 0.00003320
Iteration 157/1000 | Loss: 0.00003320
Iteration 158/1000 | Loss: 0.00003320
Iteration 159/1000 | Loss: 0.00003320
Iteration 160/1000 | Loss: 0.00003320
Iteration 161/1000 | Loss: 0.00003320
Iteration 162/1000 | Loss: 0.00003320
Iteration 163/1000 | Loss: 0.00003319
Iteration 164/1000 | Loss: 0.00003319
Iteration 165/1000 | Loss: 0.00003319
Iteration 166/1000 | Loss: 0.00003319
Iteration 167/1000 | Loss: 0.00003319
Iteration 168/1000 | Loss: 0.00003319
Iteration 169/1000 | Loss: 0.00003318
Iteration 170/1000 | Loss: 0.00003318
Iteration 171/1000 | Loss: 0.00003318
Iteration 172/1000 | Loss: 0.00003318
Iteration 173/1000 | Loss: 0.00003318
Iteration 174/1000 | Loss: 0.00003318
Iteration 175/1000 | Loss: 0.00003318
Iteration 176/1000 | Loss: 0.00003317
Iteration 177/1000 | Loss: 0.00003317
Iteration 178/1000 | Loss: 0.00003317
Iteration 179/1000 | Loss: 0.00003317
Iteration 180/1000 | Loss: 0.00003317
Iteration 181/1000 | Loss: 0.00003317
Iteration 182/1000 | Loss: 0.00003317
Iteration 183/1000 | Loss: 0.00003317
Iteration 184/1000 | Loss: 0.00003317
Iteration 185/1000 | Loss: 0.00003317
Iteration 186/1000 | Loss: 0.00003317
Iteration 187/1000 | Loss: 0.00003317
Iteration 188/1000 | Loss: 0.00003317
Iteration 189/1000 | Loss: 0.00003317
Iteration 190/1000 | Loss: 0.00003317
Iteration 191/1000 | Loss: 0.00003317
Iteration 192/1000 | Loss: 0.00003317
Iteration 193/1000 | Loss: 0.00003317
Iteration 194/1000 | Loss: 0.00003317
Iteration 195/1000 | Loss: 0.00003317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.316931542940438e-05, 3.316931542940438e-05, 3.316931542940438e-05, 3.316931542940438e-05, 3.316931542940438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.316931542940438e-05

Optimization complete. Final v2v error: 4.870886325836182 mm

Highest mean error: 5.254631996154785 mm for frame 130

Lowest mean error: 4.398223876953125 mm for frame 82

Saving results

Total time: 46.508305311203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118374
Iteration 2/25 | Loss: 0.00177622
Iteration 3/25 | Loss: 0.00167054
Iteration 4/25 | Loss: 0.00134260
Iteration 5/25 | Loss: 0.00129102
Iteration 6/25 | Loss: 0.00128267
Iteration 7/25 | Loss: 0.00127471
Iteration 8/25 | Loss: 0.00127462
Iteration 9/25 | Loss: 0.00127083
Iteration 10/25 | Loss: 0.00126982
Iteration 11/25 | Loss: 0.00126942
Iteration 12/25 | Loss: 0.00126934
Iteration 13/25 | Loss: 0.00126933
Iteration 14/25 | Loss: 0.00126932
Iteration 15/25 | Loss: 0.00126932
Iteration 16/25 | Loss: 0.00126931
Iteration 17/25 | Loss: 0.00126930
Iteration 18/25 | Loss: 0.00126929
Iteration 19/25 | Loss: 0.00126929
Iteration 20/25 | Loss: 0.00126929
Iteration 21/25 | Loss: 0.00126929
Iteration 22/25 | Loss: 0.00127304
Iteration 23/25 | Loss: 0.00127411
Iteration 24/25 | Loss: 0.00127191
Iteration 25/25 | Loss: 0.00127282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.32477379
Iteration 2/25 | Loss: 0.00233267
Iteration 3/25 | Loss: 0.00233267
Iteration 4/25 | Loss: 0.00233267
Iteration 5/25 | Loss: 0.00233267
Iteration 6/25 | Loss: 0.00233267
Iteration 7/25 | Loss: 0.00233267
Iteration 8/25 | Loss: 0.00233267
Iteration 9/25 | Loss: 0.00233267
Iteration 10/25 | Loss: 0.00233266
Iteration 11/25 | Loss: 0.00233266
Iteration 12/25 | Loss: 0.00233266
Iteration 13/25 | Loss: 0.00233266
Iteration 14/25 | Loss: 0.00233266
Iteration 15/25 | Loss: 0.00233266
Iteration 16/25 | Loss: 0.00233266
Iteration 17/25 | Loss: 0.00233266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023326643276959658, 0.0023326643276959658, 0.0023326643276959658, 0.0023326643276959658, 0.0023326643276959658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023326643276959658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233266
Iteration 2/1000 | Loss: 0.00024205
Iteration 3/1000 | Loss: 0.00003995
Iteration 4/1000 | Loss: 0.00003233
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002563
Iteration 7/1000 | Loss: 0.00002345
Iteration 8/1000 | Loss: 0.00002227
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002031
Iteration 15/1000 | Loss: 0.00002017
Iteration 16/1000 | Loss: 0.00002016
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00002001
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00001999
Iteration 24/1000 | Loss: 0.00001997
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001978
Iteration 36/1000 | Loss: 0.00001978
Iteration 37/1000 | Loss: 0.00001977
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001975
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001974
Iteration 47/1000 | Loss: 0.00001974
Iteration 48/1000 | Loss: 0.00001974
Iteration 49/1000 | Loss: 0.00001974
Iteration 50/1000 | Loss: 0.00001974
Iteration 51/1000 | Loss: 0.00001974
Iteration 52/1000 | Loss: 0.00001973
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001969
Iteration 65/1000 | Loss: 0.00001968
Iteration 66/1000 | Loss: 0.00001968
Iteration 67/1000 | Loss: 0.00001968
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001968
Iteration 75/1000 | Loss: 0.00001968
Iteration 76/1000 | Loss: 0.00001968
Iteration 77/1000 | Loss: 0.00001967
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001966
Iteration 83/1000 | Loss: 0.00001966
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001965
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001964
Iteration 94/1000 | Loss: 0.00001964
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001958
Iteration 124/1000 | Loss: 0.00001958
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001957
Iteration 132/1000 | Loss: 0.00001957
Iteration 133/1000 | Loss: 0.00001957
Iteration 134/1000 | Loss: 0.00001956
Iteration 135/1000 | Loss: 0.00001956
Iteration 136/1000 | Loss: 0.00001956
Iteration 137/1000 | Loss: 0.00001956
Iteration 138/1000 | Loss: 0.00001956
Iteration 139/1000 | Loss: 0.00001956
Iteration 140/1000 | Loss: 0.00001955
Iteration 141/1000 | Loss: 0.00001955
Iteration 142/1000 | Loss: 0.00001955
Iteration 143/1000 | Loss: 0.00001955
Iteration 144/1000 | Loss: 0.00001955
Iteration 145/1000 | Loss: 0.00001955
Iteration 146/1000 | Loss: 0.00001955
Iteration 147/1000 | Loss: 0.00001955
Iteration 148/1000 | Loss: 0.00001955
Iteration 149/1000 | Loss: 0.00001955
Iteration 150/1000 | Loss: 0.00001955
Iteration 151/1000 | Loss: 0.00001955
Iteration 152/1000 | Loss: 0.00001955
Iteration 153/1000 | Loss: 0.00001955
Iteration 154/1000 | Loss: 0.00001955
Iteration 155/1000 | Loss: 0.00001955
Iteration 156/1000 | Loss: 0.00001955
Iteration 157/1000 | Loss: 0.00001955
Iteration 158/1000 | Loss: 0.00001955
Iteration 159/1000 | Loss: 0.00001955
Iteration 160/1000 | Loss: 0.00001955
Iteration 161/1000 | Loss: 0.00001955
Iteration 162/1000 | Loss: 0.00001955
Iteration 163/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.9546687326510437e-05, 1.9546687326510437e-05, 1.9546687326510437e-05, 1.9546687326510437e-05, 1.9546687326510437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9546687326510437e-05

Optimization complete. Final v2v error: 3.877920389175415 mm

Highest mean error: 4.288620948791504 mm for frame 168

Lowest mean error: 3.5592525005340576 mm for frame 33

Saving results

Total time: 62.60957193374634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033447
Iteration 2/25 | Loss: 0.00182046
Iteration 3/25 | Loss: 0.00146074
Iteration 4/25 | Loss: 0.00141592
Iteration 5/25 | Loss: 0.00140576
Iteration 6/25 | Loss: 0.00140380
Iteration 7/25 | Loss: 0.00140380
Iteration 8/25 | Loss: 0.00140380
Iteration 9/25 | Loss: 0.00140380
Iteration 10/25 | Loss: 0.00140380
Iteration 11/25 | Loss: 0.00140380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00140379520598799, 0.00140379520598799, 0.00140379520598799, 0.00140379520598799, 0.00140379520598799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00140379520598799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91393620
Iteration 2/25 | Loss: 0.00197575
Iteration 3/25 | Loss: 0.00197575
Iteration 4/25 | Loss: 0.00197575
Iteration 5/25 | Loss: 0.00197575
Iteration 6/25 | Loss: 0.00197575
Iteration 7/25 | Loss: 0.00197575
Iteration 8/25 | Loss: 0.00197575
Iteration 9/25 | Loss: 0.00197575
Iteration 10/25 | Loss: 0.00197575
Iteration 11/25 | Loss: 0.00197575
Iteration 12/25 | Loss: 0.00197575
Iteration 13/25 | Loss: 0.00197575
Iteration 14/25 | Loss: 0.00197575
Iteration 15/25 | Loss: 0.00197575
Iteration 16/25 | Loss: 0.00197575
Iteration 17/25 | Loss: 0.00197575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019757472909986973, 0.0019757472909986973, 0.0019757472909986973, 0.0019757472909986973, 0.0019757472909986973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019757472909986973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197575
Iteration 2/1000 | Loss: 0.00011321
Iteration 3/1000 | Loss: 0.00007200
Iteration 4/1000 | Loss: 0.00005481
Iteration 5/1000 | Loss: 0.00005072
Iteration 6/1000 | Loss: 0.00004883
Iteration 7/1000 | Loss: 0.00004777
Iteration 8/1000 | Loss: 0.00004651
Iteration 9/1000 | Loss: 0.00004540
Iteration 10/1000 | Loss: 0.00004464
Iteration 11/1000 | Loss: 0.00004413
Iteration 12/1000 | Loss: 0.00004368
Iteration 13/1000 | Loss: 0.00004331
Iteration 14/1000 | Loss: 0.00004289
Iteration 15/1000 | Loss: 0.00004253
Iteration 16/1000 | Loss: 0.00004231
Iteration 17/1000 | Loss: 0.00004205
Iteration 18/1000 | Loss: 0.00004183
Iteration 19/1000 | Loss: 0.00004167
Iteration 20/1000 | Loss: 0.00004152
Iteration 21/1000 | Loss: 0.00004139
Iteration 22/1000 | Loss: 0.00004132
Iteration 23/1000 | Loss: 0.00004128
Iteration 24/1000 | Loss: 0.00004125
Iteration 25/1000 | Loss: 0.00004116
Iteration 26/1000 | Loss: 0.00004110
Iteration 27/1000 | Loss: 0.00004107
Iteration 28/1000 | Loss: 0.00004106
Iteration 29/1000 | Loss: 0.00004102
Iteration 30/1000 | Loss: 0.00004096
Iteration 31/1000 | Loss: 0.00004095
Iteration 32/1000 | Loss: 0.00004095
Iteration 33/1000 | Loss: 0.00004093
Iteration 34/1000 | Loss: 0.00004093
Iteration 35/1000 | Loss: 0.00004092
Iteration 36/1000 | Loss: 0.00004092
Iteration 37/1000 | Loss: 0.00004092
Iteration 38/1000 | Loss: 0.00004091
Iteration 39/1000 | Loss: 0.00004090
Iteration 40/1000 | Loss: 0.00004090
Iteration 41/1000 | Loss: 0.00004090
Iteration 42/1000 | Loss: 0.00004089
Iteration 43/1000 | Loss: 0.00004089
Iteration 44/1000 | Loss: 0.00004089
Iteration 45/1000 | Loss: 0.00004089
Iteration 46/1000 | Loss: 0.00004088
Iteration 47/1000 | Loss: 0.00004088
Iteration 48/1000 | Loss: 0.00004088
Iteration 49/1000 | Loss: 0.00004087
Iteration 50/1000 | Loss: 0.00004087
Iteration 51/1000 | Loss: 0.00004087
Iteration 52/1000 | Loss: 0.00004087
Iteration 53/1000 | Loss: 0.00004087
Iteration 54/1000 | Loss: 0.00004087
Iteration 55/1000 | Loss: 0.00004087
Iteration 56/1000 | Loss: 0.00004086
Iteration 57/1000 | Loss: 0.00004086
Iteration 58/1000 | Loss: 0.00004086
Iteration 59/1000 | Loss: 0.00004086
Iteration 60/1000 | Loss: 0.00004086
Iteration 61/1000 | Loss: 0.00004086
Iteration 62/1000 | Loss: 0.00004086
Iteration 63/1000 | Loss: 0.00004085
Iteration 64/1000 | Loss: 0.00004085
Iteration 65/1000 | Loss: 0.00004085
Iteration 66/1000 | Loss: 0.00004085
Iteration 67/1000 | Loss: 0.00004084
Iteration 68/1000 | Loss: 0.00004084
Iteration 69/1000 | Loss: 0.00004084
Iteration 70/1000 | Loss: 0.00004084
Iteration 71/1000 | Loss: 0.00004083
Iteration 72/1000 | Loss: 0.00004083
Iteration 73/1000 | Loss: 0.00004083
Iteration 74/1000 | Loss: 0.00004083
Iteration 75/1000 | Loss: 0.00004083
Iteration 76/1000 | Loss: 0.00004082
Iteration 77/1000 | Loss: 0.00004082
Iteration 78/1000 | Loss: 0.00004082
Iteration 79/1000 | Loss: 0.00004082
Iteration 80/1000 | Loss: 0.00004082
Iteration 81/1000 | Loss: 0.00004081
Iteration 82/1000 | Loss: 0.00004081
Iteration 83/1000 | Loss: 0.00004081
Iteration 84/1000 | Loss: 0.00004081
Iteration 85/1000 | Loss: 0.00004081
Iteration 86/1000 | Loss: 0.00004081
Iteration 87/1000 | Loss: 0.00004081
Iteration 88/1000 | Loss: 0.00004081
Iteration 89/1000 | Loss: 0.00004081
Iteration 90/1000 | Loss: 0.00004080
Iteration 91/1000 | Loss: 0.00004080
Iteration 92/1000 | Loss: 0.00004080
Iteration 93/1000 | Loss: 0.00004080
Iteration 94/1000 | Loss: 0.00004079
Iteration 95/1000 | Loss: 0.00004079
Iteration 96/1000 | Loss: 0.00004079
Iteration 97/1000 | Loss: 0.00004079
Iteration 98/1000 | Loss: 0.00004079
Iteration 99/1000 | Loss: 0.00004079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [4.0794337110128254e-05, 4.0794337110128254e-05, 4.0794337110128254e-05, 4.0794337110128254e-05, 4.0794337110128254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0794337110128254e-05

Optimization complete. Final v2v error: 5.215548992156982 mm

Highest mean error: 6.703876972198486 mm for frame 166

Lowest mean error: 3.98820161819458 mm for frame 17

Saving results

Total time: 53.075419902801514
