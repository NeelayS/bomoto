Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=68, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3808-3863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421045
Iteration 2/25 | Loss: 0.00137918
Iteration 3/25 | Loss: 0.00126853
Iteration 4/25 | Loss: 0.00125502
Iteration 5/25 | Loss: 0.00125078
Iteration 6/25 | Loss: 0.00124963
Iteration 7/25 | Loss: 0.00124932
Iteration 8/25 | Loss: 0.00124932
Iteration 9/25 | Loss: 0.00124932
Iteration 10/25 | Loss: 0.00124932
Iteration 11/25 | Loss: 0.00124932
Iteration 12/25 | Loss: 0.00124932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001249319757334888, 0.001249319757334888, 0.001249319757334888, 0.001249319757334888, 0.001249319757334888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001249319757334888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48348665
Iteration 2/25 | Loss: 0.00104474
Iteration 3/25 | Loss: 0.00104474
Iteration 4/25 | Loss: 0.00104474
Iteration 5/25 | Loss: 0.00104474
Iteration 6/25 | Loss: 0.00104474
Iteration 7/25 | Loss: 0.00104474
Iteration 8/25 | Loss: 0.00104474
Iteration 9/25 | Loss: 0.00104474
Iteration 10/25 | Loss: 0.00104474
Iteration 11/25 | Loss: 0.00104474
Iteration 12/25 | Loss: 0.00104474
Iteration 13/25 | Loss: 0.00104474
Iteration 14/25 | Loss: 0.00104474
Iteration 15/25 | Loss: 0.00104474
Iteration 16/25 | Loss: 0.00104474
Iteration 17/25 | Loss: 0.00104474
Iteration 18/25 | Loss: 0.00104474
Iteration 19/25 | Loss: 0.00104474
Iteration 20/25 | Loss: 0.00104474
Iteration 21/25 | Loss: 0.00104474
Iteration 22/25 | Loss: 0.00104474
Iteration 23/25 | Loss: 0.00104474
Iteration 24/25 | Loss: 0.00104474
Iteration 25/25 | Loss: 0.00104474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104474
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002309
Iteration 4/1000 | Loss: 0.00001830
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001414
Iteration 39/1000 | Loss: 0.00001413
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001412
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001410
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001409
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001408
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001405
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001404
Iteration 69/1000 | Loss: 0.00001404
Iteration 70/1000 | Loss: 0.00001404
Iteration 71/1000 | Loss: 0.00001404
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001403
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001402
Iteration 77/1000 | Loss: 0.00001402
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001396
Iteration 106/1000 | Loss: 0.00001396
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001396
Iteration 109/1000 | Loss: 0.00001396
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.394572427670937e-05, 1.394572427670937e-05, 1.394572427670937e-05, 1.394572427670937e-05, 1.394572427670937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.394572427670937e-05

Optimization complete. Final v2v error: 3.155151605606079 mm

Highest mean error: 4.3228840827941895 mm for frame 39

Lowest mean error: 2.838134765625 mm for frame 96

Saving results

Total time: 38.90065240859985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454558
Iteration 2/25 | Loss: 0.00136552
Iteration 3/25 | Loss: 0.00126340
Iteration 4/25 | Loss: 0.00124996
Iteration 5/25 | Loss: 0.00124641
Iteration 6/25 | Loss: 0.00124641
Iteration 7/25 | Loss: 0.00124641
Iteration 8/25 | Loss: 0.00124641
Iteration 9/25 | Loss: 0.00124641
Iteration 10/25 | Loss: 0.00124641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012464071623981, 0.0012464071623981, 0.0012464071623981, 0.0012464071623981, 0.0012464071623981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012464071623981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65578651
Iteration 2/25 | Loss: 0.00100380
Iteration 3/25 | Loss: 0.00100380
Iteration 4/25 | Loss: 0.00100379
Iteration 5/25 | Loss: 0.00100379
Iteration 6/25 | Loss: 0.00100379
Iteration 7/25 | Loss: 0.00100379
Iteration 8/25 | Loss: 0.00100379
Iteration 9/25 | Loss: 0.00100379
Iteration 10/25 | Loss: 0.00100379
Iteration 11/25 | Loss: 0.00100379
Iteration 12/25 | Loss: 0.00100379
Iteration 13/25 | Loss: 0.00100379
Iteration 14/25 | Loss: 0.00100379
Iteration 15/25 | Loss: 0.00100379
Iteration 16/25 | Loss: 0.00100379
Iteration 17/25 | Loss: 0.00100379
Iteration 18/25 | Loss: 0.00100379
Iteration 19/25 | Loss: 0.00100379
Iteration 20/25 | Loss: 0.00100379
Iteration 21/25 | Loss: 0.00100379
Iteration 22/25 | Loss: 0.00100379
Iteration 23/25 | Loss: 0.00100379
Iteration 24/25 | Loss: 0.00100379
Iteration 25/25 | Loss: 0.00100379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100379
Iteration 2/1000 | Loss: 0.00002300
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001351
Iteration 122/1000 | Loss: 0.00001351
Iteration 123/1000 | Loss: 0.00001351
Iteration 124/1000 | Loss: 0.00001351
Iteration 125/1000 | Loss: 0.00001351
Iteration 126/1000 | Loss: 0.00001351
Iteration 127/1000 | Loss: 0.00001351
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001350
Iteration 142/1000 | Loss: 0.00001350
Iteration 143/1000 | Loss: 0.00001350
Iteration 144/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.349544163531391e-05, 1.349544163531391e-05, 1.349544163531391e-05, 1.349544163531391e-05, 1.349544163531391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.349544163531391e-05

Optimization complete. Final v2v error: 3.1185922622680664 mm

Highest mean error: 3.4773197174072266 mm for frame 87

Lowest mean error: 2.8409714698791504 mm for frame 19

Saving results

Total time: 38.38395166397095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964872
Iteration 2/25 | Loss: 0.00279030
Iteration 3/25 | Loss: 0.00189272
Iteration 4/25 | Loss: 0.00178183
Iteration 5/25 | Loss: 0.00165021
Iteration 6/25 | Loss: 0.00163609
Iteration 7/25 | Loss: 0.00150378
Iteration 8/25 | Loss: 0.00145982
Iteration 9/25 | Loss: 0.00142350
Iteration 10/25 | Loss: 0.00138214
Iteration 11/25 | Loss: 0.00136754
Iteration 12/25 | Loss: 0.00136092
Iteration 13/25 | Loss: 0.00136377
Iteration 14/25 | Loss: 0.00136252
Iteration 15/25 | Loss: 0.00136476
Iteration 16/25 | Loss: 0.00136451
Iteration 17/25 | Loss: 0.00137415
Iteration 18/25 | Loss: 0.00136943
Iteration 19/25 | Loss: 0.00136013
Iteration 20/25 | Loss: 0.00134830
Iteration 21/25 | Loss: 0.00134575
Iteration 22/25 | Loss: 0.00134563
Iteration 23/25 | Loss: 0.00134713
Iteration 24/25 | Loss: 0.00134538
Iteration 25/25 | Loss: 0.00136008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31152368
Iteration 2/25 | Loss: 0.00192272
Iteration 3/25 | Loss: 0.00192272
Iteration 4/25 | Loss: 0.00192272
Iteration 5/25 | Loss: 0.00192272
Iteration 6/25 | Loss: 0.00192272
Iteration 7/25 | Loss: 0.00192272
Iteration 8/25 | Loss: 0.00192272
Iteration 9/25 | Loss: 0.00192272
Iteration 10/25 | Loss: 0.00192272
Iteration 11/25 | Loss: 0.00192272
Iteration 12/25 | Loss: 0.00192272
Iteration 13/25 | Loss: 0.00192272
Iteration 14/25 | Loss: 0.00192272
Iteration 15/25 | Loss: 0.00192272
Iteration 16/25 | Loss: 0.00192272
Iteration 17/25 | Loss: 0.00192272
Iteration 18/25 | Loss: 0.00192272
Iteration 19/25 | Loss: 0.00192272
Iteration 20/25 | Loss: 0.00192272
Iteration 21/25 | Loss: 0.00192272
Iteration 22/25 | Loss: 0.00192272
Iteration 23/25 | Loss: 0.00192272
Iteration 24/25 | Loss: 0.00192272
Iteration 25/25 | Loss: 0.00192272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192272
Iteration 2/1000 | Loss: 0.00061303
Iteration 3/1000 | Loss: 0.00045113
Iteration 4/1000 | Loss: 0.00011261
Iteration 5/1000 | Loss: 0.00009214
Iteration 6/1000 | Loss: 0.00008300
Iteration 7/1000 | Loss: 0.00010064
Iteration 8/1000 | Loss: 0.00008228
Iteration 9/1000 | Loss: 0.00008112
Iteration 10/1000 | Loss: 0.00034715
Iteration 11/1000 | Loss: 0.00037417
Iteration 12/1000 | Loss: 0.00024773
Iteration 13/1000 | Loss: 0.00068679
Iteration 14/1000 | Loss: 0.00031229
Iteration 15/1000 | Loss: 0.00042909
Iteration 16/1000 | Loss: 0.00040017
Iteration 17/1000 | Loss: 0.00040014
Iteration 18/1000 | Loss: 0.00028745
Iteration 19/1000 | Loss: 0.00053573
Iteration 20/1000 | Loss: 0.00069583
Iteration 21/1000 | Loss: 0.00012709
Iteration 22/1000 | Loss: 0.00009831
Iteration 23/1000 | Loss: 0.00006538
Iteration 24/1000 | Loss: 0.00007689
Iteration 25/1000 | Loss: 0.00005688
Iteration 26/1000 | Loss: 0.00005466
Iteration 27/1000 | Loss: 0.00006607
Iteration 28/1000 | Loss: 0.00006223
Iteration 29/1000 | Loss: 0.00005488
Iteration 30/1000 | Loss: 0.00007343
Iteration 31/1000 | Loss: 0.00007041
Iteration 32/1000 | Loss: 0.00005412
Iteration 33/1000 | Loss: 0.00007877
Iteration 34/1000 | Loss: 0.00007155
Iteration 35/1000 | Loss: 0.00006994
Iteration 36/1000 | Loss: 0.00006977
Iteration 37/1000 | Loss: 0.00006988
Iteration 38/1000 | Loss: 0.00007016
Iteration 39/1000 | Loss: 0.00006891
Iteration 40/1000 | Loss: 0.00006815
Iteration 41/1000 | Loss: 0.00006832
Iteration 42/1000 | Loss: 0.00006705
Iteration 43/1000 | Loss: 0.00007042
Iteration 44/1000 | Loss: 0.00006617
Iteration 45/1000 | Loss: 0.00006555
Iteration 46/1000 | Loss: 0.00007015
Iteration 47/1000 | Loss: 0.00006956
Iteration 48/1000 | Loss: 0.00005570
Iteration 49/1000 | Loss: 0.00005794
Iteration 50/1000 | Loss: 0.00007687
Iteration 51/1000 | Loss: 0.00006127
Iteration 52/1000 | Loss: 0.00005637
Iteration 53/1000 | Loss: 0.00006534
Iteration 54/1000 | Loss: 0.00006203
Iteration 55/1000 | Loss: 0.00006730
Iteration 56/1000 | Loss: 0.00006125
Iteration 57/1000 | Loss: 0.00006450
Iteration 58/1000 | Loss: 0.00006606
Iteration 59/1000 | Loss: 0.00006543
Iteration 60/1000 | Loss: 0.00008383
Iteration 61/1000 | Loss: 0.00006545
Iteration 62/1000 | Loss: 0.00006472
Iteration 63/1000 | Loss: 0.00006498
Iteration 64/1000 | Loss: 0.00006472
Iteration 65/1000 | Loss: 0.00006309
Iteration 66/1000 | Loss: 0.00006248
Iteration 67/1000 | Loss: 0.00006214
Iteration 68/1000 | Loss: 0.00006199
Iteration 69/1000 | Loss: 0.00006141
Iteration 70/1000 | Loss: 0.00006462
Iteration 71/1000 | Loss: 0.00006090
Iteration 72/1000 | Loss: 0.00005463
Iteration 73/1000 | Loss: 0.00007530
Iteration 74/1000 | Loss: 0.00006440
Iteration 75/1000 | Loss: 0.00008294
Iteration 76/1000 | Loss: 0.00006425
Iteration 77/1000 | Loss: 0.00008069
Iteration 78/1000 | Loss: 0.00005820
Iteration 79/1000 | Loss: 0.00007316
Iteration 80/1000 | Loss: 0.00006464
Iteration 81/1000 | Loss: 0.00006454
Iteration 82/1000 | Loss: 0.00006558
Iteration 83/1000 | Loss: 0.00007644
Iteration 84/1000 | Loss: 0.00006995
Iteration 85/1000 | Loss: 0.00006542
Iteration 86/1000 | Loss: 0.00006629
Iteration 87/1000 | Loss: 0.00006436
Iteration 88/1000 | Loss: 0.00006699
Iteration 89/1000 | Loss: 0.00006473
Iteration 90/1000 | Loss: 0.00006462
Iteration 91/1000 | Loss: 0.00006520
Iteration 92/1000 | Loss: 0.00006680
Iteration 93/1000 | Loss: 0.00006636
Iteration 94/1000 | Loss: 0.00006758
Iteration 95/1000 | Loss: 0.00006472
Iteration 96/1000 | Loss: 0.00007384
Iteration 97/1000 | Loss: 0.00006814
Iteration 98/1000 | Loss: 0.00006709
Iteration 99/1000 | Loss: 0.00006426
Iteration 100/1000 | Loss: 0.00006703
Iteration 101/1000 | Loss: 0.00006030
Iteration 102/1000 | Loss: 0.00007249
Iteration 103/1000 | Loss: 0.00004925
Iteration 104/1000 | Loss: 0.00005599
Iteration 105/1000 | Loss: 0.00005835
Iteration 106/1000 | Loss: 0.00005996
Iteration 107/1000 | Loss: 0.00006977
Iteration 108/1000 | Loss: 0.00006471
Iteration 109/1000 | Loss: 0.00006997
Iteration 110/1000 | Loss: 0.00006669
Iteration 111/1000 | Loss: 0.00006287
Iteration 112/1000 | Loss: 0.00006752
Iteration 113/1000 | Loss: 0.00006598
Iteration 114/1000 | Loss: 0.00006805
Iteration 115/1000 | Loss: 0.00006598
Iteration 116/1000 | Loss: 0.00006809
Iteration 117/1000 | Loss: 0.00006539
Iteration 118/1000 | Loss: 0.00006875
Iteration 119/1000 | Loss: 0.00005077
Iteration 120/1000 | Loss: 0.00257324
Iteration 121/1000 | Loss: 0.00009225
Iteration 122/1000 | Loss: 0.00006738
Iteration 123/1000 | Loss: 0.00006487
Iteration 124/1000 | Loss: 0.00006269
Iteration 125/1000 | Loss: 0.00006202
Iteration 126/1000 | Loss: 0.00005753
Iteration 127/1000 | Loss: 0.00005638
Iteration 128/1000 | Loss: 0.00004501
Iteration 129/1000 | Loss: 0.00005412
Iteration 130/1000 | Loss: 0.00005563
Iteration 131/1000 | Loss: 0.00005627
Iteration 132/1000 | Loss: 0.00007437
Iteration 133/1000 | Loss: 0.00004353
Iteration 134/1000 | Loss: 0.00003501
Iteration 135/1000 | Loss: 0.00003617
Iteration 136/1000 | Loss: 0.00004635
Iteration 137/1000 | Loss: 0.00006073
Iteration 138/1000 | Loss: 0.00004345
Iteration 139/1000 | Loss: 0.00005938
Iteration 140/1000 | Loss: 0.00003631
Iteration 141/1000 | Loss: 0.00004003
Iteration 142/1000 | Loss: 0.00005169
Iteration 143/1000 | Loss: 0.00005495
Iteration 144/1000 | Loss: 0.00004809
Iteration 145/1000 | Loss: 0.00005710
Iteration 146/1000 | Loss: 0.00003728
Iteration 147/1000 | Loss: 0.00004581
Iteration 148/1000 | Loss: 0.00005214
Iteration 149/1000 | Loss: 0.00005579
Iteration 150/1000 | Loss: 0.00005183
Iteration 151/1000 | Loss: 0.00005536
Iteration 152/1000 | Loss: 0.00038387
Iteration 153/1000 | Loss: 0.00035174
Iteration 154/1000 | Loss: 0.00005245
Iteration 155/1000 | Loss: 0.00037300
Iteration 156/1000 | Loss: 0.00033787
Iteration 157/1000 | Loss: 0.00038471
Iteration 158/1000 | Loss: 0.00036897
Iteration 159/1000 | Loss: 0.00004312
Iteration 160/1000 | Loss: 0.00039161
Iteration 161/1000 | Loss: 0.00006315
Iteration 162/1000 | Loss: 0.00032268
Iteration 163/1000 | Loss: 0.00005468
Iteration 164/1000 | Loss: 0.00035394
Iteration 165/1000 | Loss: 0.00031779
Iteration 166/1000 | Loss: 0.00035469
Iteration 167/1000 | Loss: 0.00006811
Iteration 168/1000 | Loss: 0.00005517
Iteration 169/1000 | Loss: 0.00004258
Iteration 170/1000 | Loss: 0.00005166
Iteration 171/1000 | Loss: 0.00003584
Iteration 172/1000 | Loss: 0.00004706
Iteration 173/1000 | Loss: 0.00004321
Iteration 174/1000 | Loss: 0.00005558
Iteration 175/1000 | Loss: 0.00004429
Iteration 176/1000 | Loss: 0.00004269
Iteration 177/1000 | Loss: 0.00005485
Iteration 178/1000 | Loss: 0.00004709
Iteration 179/1000 | Loss: 0.00005560
Iteration 180/1000 | Loss: 0.00005003
Iteration 181/1000 | Loss: 0.00005892
Iteration 182/1000 | Loss: 0.00004969
Iteration 183/1000 | Loss: 0.00005110
Iteration 184/1000 | Loss: 0.00005941
Iteration 185/1000 | Loss: 0.00005009
Iteration 186/1000 | Loss: 0.00006191
Iteration 187/1000 | Loss: 0.00004810
Iteration 188/1000 | Loss: 0.00005474
Iteration 189/1000 | Loss: 0.00004471
Iteration 190/1000 | Loss: 0.00003453
Iteration 191/1000 | Loss: 0.00005244
Iteration 192/1000 | Loss: 0.00005088
Iteration 193/1000 | Loss: 0.00004541
Iteration 194/1000 | Loss: 0.00004721
Iteration 195/1000 | Loss: 0.00004525
Iteration 196/1000 | Loss: 0.00005117
Iteration 197/1000 | Loss: 0.00005029
Iteration 198/1000 | Loss: 0.00004602
Iteration 199/1000 | Loss: 0.00004947
Iteration 200/1000 | Loss: 0.00005173
Iteration 201/1000 | Loss: 0.00004853
Iteration 202/1000 | Loss: 0.00005180
Iteration 203/1000 | Loss: 0.00004343
Iteration 204/1000 | Loss: 0.00004979
Iteration 205/1000 | Loss: 0.00004921
Iteration 206/1000 | Loss: 0.00005279
Iteration 207/1000 | Loss: 0.00004957
Iteration 208/1000 | Loss: 0.00005224
Iteration 209/1000 | Loss: 0.00004889
Iteration 210/1000 | Loss: 0.00005349
Iteration 211/1000 | Loss: 0.00005400
Iteration 212/1000 | Loss: 0.00004984
Iteration 213/1000 | Loss: 0.00005221
Iteration 214/1000 | Loss: 0.00003885
Iteration 215/1000 | Loss: 0.00003653
Iteration 216/1000 | Loss: 0.00005087
Iteration 217/1000 | Loss: 0.00004814
Iteration 218/1000 | Loss: 0.00003070
Iteration 219/1000 | Loss: 0.00005278
Iteration 220/1000 | Loss: 0.00004802
Iteration 221/1000 | Loss: 0.00005183
Iteration 222/1000 | Loss: 0.00005555
Iteration 223/1000 | Loss: 0.00005256
Iteration 224/1000 | Loss: 0.00004773
Iteration 225/1000 | Loss: 0.00005051
Iteration 226/1000 | Loss: 0.00005462
Iteration 227/1000 | Loss: 0.00005592
Iteration 228/1000 | Loss: 0.00003326
Iteration 229/1000 | Loss: 0.00005030
Iteration 230/1000 | Loss: 0.00004769
Iteration 231/1000 | Loss: 0.00004942
Iteration 232/1000 | Loss: 0.00005365
Iteration 233/1000 | Loss: 0.00005142
Iteration 234/1000 | Loss: 0.00004738
Iteration 235/1000 | Loss: 0.00005198
Iteration 236/1000 | Loss: 0.00004682
Iteration 237/1000 | Loss: 0.00005182
Iteration 238/1000 | Loss: 0.00003607
Iteration 239/1000 | Loss: 0.00005247
Iteration 240/1000 | Loss: 0.00004817
Iteration 241/1000 | Loss: 0.00005086
Iteration 242/1000 | Loss: 0.00004729
Iteration 243/1000 | Loss: 0.00005223
Iteration 244/1000 | Loss: 0.00004722
Iteration 245/1000 | Loss: 0.00005273
Iteration 246/1000 | Loss: 0.00004454
Iteration 247/1000 | Loss: 0.00003751
Iteration 248/1000 | Loss: 0.00004935
Iteration 249/1000 | Loss: 0.00005579
Iteration 250/1000 | Loss: 0.00003497
Iteration 251/1000 | Loss: 0.00007246
Iteration 252/1000 | Loss: 0.00003780
Iteration 253/1000 | Loss: 0.00003548
Iteration 254/1000 | Loss: 0.00003191
Iteration 255/1000 | Loss: 0.00003050
Iteration 256/1000 | Loss: 0.00002972
Iteration 257/1000 | Loss: 0.00002927
Iteration 258/1000 | Loss: 0.00002916
Iteration 259/1000 | Loss: 0.00002906
Iteration 260/1000 | Loss: 0.00002900
Iteration 261/1000 | Loss: 0.00002898
Iteration 262/1000 | Loss: 0.00002897
Iteration 263/1000 | Loss: 0.00002897
Iteration 264/1000 | Loss: 0.00002897
Iteration 265/1000 | Loss: 0.00002897
Iteration 266/1000 | Loss: 0.00002895
Iteration 267/1000 | Loss: 0.00002895
Iteration 268/1000 | Loss: 0.00002895
Iteration 269/1000 | Loss: 0.00002893
Iteration 270/1000 | Loss: 0.00002890
Iteration 271/1000 | Loss: 0.00002886
Iteration 272/1000 | Loss: 0.00002886
Iteration 273/1000 | Loss: 0.00002885
Iteration 274/1000 | Loss: 0.00002885
Iteration 275/1000 | Loss: 0.00002885
Iteration 276/1000 | Loss: 0.00002885
Iteration 277/1000 | Loss: 0.00002885
Iteration 278/1000 | Loss: 0.00002885
Iteration 279/1000 | Loss: 0.00002884
Iteration 280/1000 | Loss: 0.00002884
Iteration 281/1000 | Loss: 0.00002883
Iteration 282/1000 | Loss: 0.00002883
Iteration 283/1000 | Loss: 0.00002883
Iteration 284/1000 | Loss: 0.00002883
Iteration 285/1000 | Loss: 0.00002883
Iteration 286/1000 | Loss: 0.00002883
Iteration 287/1000 | Loss: 0.00002883
Iteration 288/1000 | Loss: 0.00002882
Iteration 289/1000 | Loss: 0.00002882
Iteration 290/1000 | Loss: 0.00002882
Iteration 291/1000 | Loss: 0.00002882
Iteration 292/1000 | Loss: 0.00002882
Iteration 293/1000 | Loss: 0.00002882
Iteration 294/1000 | Loss: 0.00002882
Iteration 295/1000 | Loss: 0.00002882
Iteration 296/1000 | Loss: 0.00002882
Iteration 297/1000 | Loss: 0.00002882
Iteration 298/1000 | Loss: 0.00002881
Iteration 299/1000 | Loss: 0.00002881
Iteration 300/1000 | Loss: 0.00002881
Iteration 301/1000 | Loss: 0.00002877
Iteration 302/1000 | Loss: 0.00002875
Iteration 303/1000 | Loss: 0.00002875
Iteration 304/1000 | Loss: 0.00002875
Iteration 305/1000 | Loss: 0.00002874
Iteration 306/1000 | Loss: 0.00002873
Iteration 307/1000 | Loss: 0.00002873
Iteration 308/1000 | Loss: 0.00002872
Iteration 309/1000 | Loss: 0.00002872
Iteration 310/1000 | Loss: 0.00002872
Iteration 311/1000 | Loss: 0.00002872
Iteration 312/1000 | Loss: 0.00002872
Iteration 313/1000 | Loss: 0.00002872
Iteration 314/1000 | Loss: 0.00002871
Iteration 315/1000 | Loss: 0.00002871
Iteration 316/1000 | Loss: 0.00002871
Iteration 317/1000 | Loss: 0.00002870
Iteration 318/1000 | Loss: 0.00002870
Iteration 319/1000 | Loss: 0.00002869
Iteration 320/1000 | Loss: 0.00002869
Iteration 321/1000 | Loss: 0.00002868
Iteration 322/1000 | Loss: 0.00002868
Iteration 323/1000 | Loss: 0.00002868
Iteration 324/1000 | Loss: 0.00002867
Iteration 325/1000 | Loss: 0.00002867
Iteration 326/1000 | Loss: 0.00002867
Iteration 327/1000 | Loss: 0.00002867
Iteration 328/1000 | Loss: 0.00002866
Iteration 329/1000 | Loss: 0.00002866
Iteration 330/1000 | Loss: 0.00002865
Iteration 331/1000 | Loss: 0.00002865
Iteration 332/1000 | Loss: 0.00002865
Iteration 333/1000 | Loss: 0.00002865
Iteration 334/1000 | Loss: 0.00002865
Iteration 335/1000 | Loss: 0.00002864
Iteration 336/1000 | Loss: 0.00002864
Iteration 337/1000 | Loss: 0.00002864
Iteration 338/1000 | Loss: 0.00002864
Iteration 339/1000 | Loss: 0.00002863
Iteration 340/1000 | Loss: 0.00002863
Iteration 341/1000 | Loss: 0.00002862
Iteration 342/1000 | Loss: 0.00002862
Iteration 343/1000 | Loss: 0.00002862
Iteration 344/1000 | Loss: 0.00002862
Iteration 345/1000 | Loss: 0.00002862
Iteration 346/1000 | Loss: 0.00002861
Iteration 347/1000 | Loss: 0.00002861
Iteration 348/1000 | Loss: 0.00002861
Iteration 349/1000 | Loss: 0.00002861
Iteration 350/1000 | Loss: 0.00002861
Iteration 351/1000 | Loss: 0.00002861
Iteration 352/1000 | Loss: 0.00002860
Iteration 353/1000 | Loss: 0.00002860
Iteration 354/1000 | Loss: 0.00002860
Iteration 355/1000 | Loss: 0.00002859
Iteration 356/1000 | Loss: 0.00002859
Iteration 357/1000 | Loss: 0.00002859
Iteration 358/1000 | Loss: 0.00002858
Iteration 359/1000 | Loss: 0.00002858
Iteration 360/1000 | Loss: 0.00002858
Iteration 361/1000 | Loss: 0.00002857
Iteration 362/1000 | Loss: 0.00002857
Iteration 363/1000 | Loss: 0.00002857
Iteration 364/1000 | Loss: 0.00003684
Iteration 365/1000 | Loss: 0.00002858
Iteration 366/1000 | Loss: 0.00002858
Iteration 367/1000 | Loss: 0.00002858
Iteration 368/1000 | Loss: 0.00002858
Iteration 369/1000 | Loss: 0.00002858
Iteration 370/1000 | Loss: 0.00002858
Iteration 371/1000 | Loss: 0.00002858
Iteration 372/1000 | Loss: 0.00002858
Iteration 373/1000 | Loss: 0.00002856
Iteration 374/1000 | Loss: 0.00002856
Iteration 375/1000 | Loss: 0.00002855
Iteration 376/1000 | Loss: 0.00002855
Iteration 377/1000 | Loss: 0.00002855
Iteration 378/1000 | Loss: 0.00002854
Iteration 379/1000 | Loss: 0.00002854
Iteration 380/1000 | Loss: 0.00002854
Iteration 381/1000 | Loss: 0.00002854
Iteration 382/1000 | Loss: 0.00002854
Iteration 383/1000 | Loss: 0.00002854
Iteration 384/1000 | Loss: 0.00002854
Iteration 385/1000 | Loss: 0.00002854
Iteration 386/1000 | Loss: 0.00002854
Iteration 387/1000 | Loss: 0.00002854
Iteration 388/1000 | Loss: 0.00002854
Iteration 389/1000 | Loss: 0.00002854
Iteration 390/1000 | Loss: 0.00002854
Iteration 391/1000 | Loss: 0.00002854
Iteration 392/1000 | Loss: 0.00002853
Iteration 393/1000 | Loss: 0.00002853
Iteration 394/1000 | Loss: 0.00002853
Iteration 395/1000 | Loss: 0.00002853
Iteration 396/1000 | Loss: 0.00002853
Iteration 397/1000 | Loss: 0.00002853
Iteration 398/1000 | Loss: 0.00002853
Iteration 399/1000 | Loss: 0.00002853
Iteration 400/1000 | Loss: 0.00002853
Iteration 401/1000 | Loss: 0.00002853
Iteration 402/1000 | Loss: 0.00002853
Iteration 403/1000 | Loss: 0.00002853
Iteration 404/1000 | Loss: 0.00002853
Iteration 405/1000 | Loss: 0.00002853
Iteration 406/1000 | Loss: 0.00002853
Iteration 407/1000 | Loss: 0.00002853
Iteration 408/1000 | Loss: 0.00002853
Iteration 409/1000 | Loss: 0.00002853
Iteration 410/1000 | Loss: 0.00002853
Iteration 411/1000 | Loss: 0.00002853
Iteration 412/1000 | Loss: 0.00002853
Iteration 413/1000 | Loss: 0.00002853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 413. Stopping optimization.
Last 5 losses: [2.8527321774163283e-05, 2.8527321774163283e-05, 2.8527321774163283e-05, 2.8527321774163283e-05, 2.8527321774163283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8527321774163283e-05

Optimization complete. Final v2v error: 4.082422733306885 mm

Highest mean error: 10.458853721618652 mm for frame 46

Lowest mean error: 2.6699154376983643 mm for frame 24

Saving results

Total time: 409.85330867767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029986
Iteration 2/25 | Loss: 0.00167169
Iteration 3/25 | Loss: 0.00143497
Iteration 4/25 | Loss: 0.00140332
Iteration 5/25 | Loss: 0.00138156
Iteration 6/25 | Loss: 0.00133991
Iteration 7/25 | Loss: 0.00134989
Iteration 8/25 | Loss: 0.00132017
Iteration 9/25 | Loss: 0.00132409
Iteration 10/25 | Loss: 0.00131763
Iteration 11/25 | Loss: 0.00131009
Iteration 12/25 | Loss: 0.00130968
Iteration 13/25 | Loss: 0.00130961
Iteration 14/25 | Loss: 0.00130961
Iteration 15/25 | Loss: 0.00130961
Iteration 16/25 | Loss: 0.00130961
Iteration 17/25 | Loss: 0.00130961
Iteration 18/25 | Loss: 0.00130961
Iteration 19/25 | Loss: 0.00130960
Iteration 20/25 | Loss: 0.00130960
Iteration 21/25 | Loss: 0.00130960
Iteration 22/25 | Loss: 0.00130960
Iteration 23/25 | Loss: 0.00130960
Iteration 24/25 | Loss: 0.00130960
Iteration 25/25 | Loss: 0.00130960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60674679
Iteration 2/25 | Loss: 0.00128238
Iteration 3/25 | Loss: 0.00128238
Iteration 4/25 | Loss: 0.00128238
Iteration 5/25 | Loss: 0.00128238
Iteration 6/25 | Loss: 0.00128238
Iteration 7/25 | Loss: 0.00128238
Iteration 8/25 | Loss: 0.00128238
Iteration 9/25 | Loss: 0.00133533
Iteration 10/25 | Loss: 0.00128238
Iteration 11/25 | Loss: 0.00128238
Iteration 12/25 | Loss: 0.00128238
Iteration 13/25 | Loss: 0.00128238
Iteration 14/25 | Loss: 0.00128238
Iteration 15/25 | Loss: 0.00128238
Iteration 16/25 | Loss: 0.00128238
Iteration 17/25 | Loss: 0.00128238
Iteration 18/25 | Loss: 0.00128238
Iteration 19/25 | Loss: 0.00128238
Iteration 20/25 | Loss: 0.00128238
Iteration 21/25 | Loss: 0.00128238
Iteration 22/25 | Loss: 0.00128238
Iteration 23/25 | Loss: 0.00128238
Iteration 24/25 | Loss: 0.00128238
Iteration 25/25 | Loss: 0.00128238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012823769357055426, 0.0012823769357055426, 0.0012823769357055426, 0.0012823769357055426, 0.0012823769357055426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012823769357055426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128238
Iteration 2/1000 | Loss: 0.00010935
Iteration 3/1000 | Loss: 0.00004017
Iteration 4/1000 | Loss: 0.00007356
Iteration 5/1000 | Loss: 0.00003554
Iteration 6/1000 | Loss: 0.00007865
Iteration 7/1000 | Loss: 0.00003449
Iteration 8/1000 | Loss: 0.00009867
Iteration 9/1000 | Loss: 0.00003360
Iteration 10/1000 | Loss: 0.00180363
Iteration 11/1000 | Loss: 0.00307612
Iteration 12/1000 | Loss: 0.00368046
Iteration 13/1000 | Loss: 0.00135851
Iteration 14/1000 | Loss: 0.00005655
Iteration 15/1000 | Loss: 0.00004578
Iteration 16/1000 | Loss: 0.00008435
Iteration 17/1000 | Loss: 0.00006121
Iteration 18/1000 | Loss: 0.00003422
Iteration 19/1000 | Loss: 0.00013726
Iteration 20/1000 | Loss: 0.00003306
Iteration 21/1000 | Loss: 0.00003236
Iteration 22/1000 | Loss: 0.00003189
Iteration 23/1000 | Loss: 0.00223336
Iteration 24/1000 | Loss: 0.00093600
Iteration 25/1000 | Loss: 0.00426073
Iteration 26/1000 | Loss: 0.00147704
Iteration 27/1000 | Loss: 0.00005520
Iteration 28/1000 | Loss: 0.00005368
Iteration 29/1000 | Loss: 0.00002583
Iteration 30/1000 | Loss: 0.00004034
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00005890
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00009390
Iteration 36/1000 | Loss: 0.00023513
Iteration 37/1000 | Loss: 0.00004856
Iteration 38/1000 | Loss: 0.00004243
Iteration 39/1000 | Loss: 0.00005159
Iteration 40/1000 | Loss: 0.00002074
Iteration 41/1000 | Loss: 0.00008836
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00010436
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00001830
Iteration 47/1000 | Loss: 0.00003072
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001741
Iteration 50/1000 | Loss: 0.00006029
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001694
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00006805
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001670
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001670
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001669
Iteration 98/1000 | Loss: 0.00001669
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001666
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001666
Iteration 107/1000 | Loss: 0.00001666
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001666
Iteration 110/1000 | Loss: 0.00001666
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001666
Iteration 119/1000 | Loss: 0.00001666
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001665
Iteration 123/1000 | Loss: 0.00001665
Iteration 124/1000 | Loss: 0.00001665
Iteration 125/1000 | Loss: 0.00001665
Iteration 126/1000 | Loss: 0.00001665
Iteration 127/1000 | Loss: 0.00006721
Iteration 128/1000 | Loss: 0.00003848
Iteration 129/1000 | Loss: 0.00001783
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00002814
Iteration 132/1000 | Loss: 0.00001671
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001661
Iteration 135/1000 | Loss: 0.00001661
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001661
Iteration 139/1000 | Loss: 0.00001661
Iteration 140/1000 | Loss: 0.00001661
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001660
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001658
Iteration 167/1000 | Loss: 0.00001658
Iteration 168/1000 | Loss: 0.00001658
Iteration 169/1000 | Loss: 0.00001658
Iteration 170/1000 | Loss: 0.00001658
Iteration 171/1000 | Loss: 0.00001658
Iteration 172/1000 | Loss: 0.00001658
Iteration 173/1000 | Loss: 0.00001658
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001658
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001657
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001657
Iteration 183/1000 | Loss: 0.00001657
Iteration 184/1000 | Loss: 0.00001657
Iteration 185/1000 | Loss: 0.00001657
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001657
Iteration 191/1000 | Loss: 0.00001657
Iteration 192/1000 | Loss: 0.00001657
Iteration 193/1000 | Loss: 0.00001657
Iteration 194/1000 | Loss: 0.00001657
Iteration 195/1000 | Loss: 0.00001657
Iteration 196/1000 | Loss: 0.00001657
Iteration 197/1000 | Loss: 0.00001657
Iteration 198/1000 | Loss: 0.00001657
Iteration 199/1000 | Loss: 0.00001657
Iteration 200/1000 | Loss: 0.00001657
Iteration 201/1000 | Loss: 0.00001657
Iteration 202/1000 | Loss: 0.00001657
Iteration 203/1000 | Loss: 0.00001657
Iteration 204/1000 | Loss: 0.00001657
Iteration 205/1000 | Loss: 0.00001657
Iteration 206/1000 | Loss: 0.00001657
Iteration 207/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.6566413250984624e-05, 1.6566413250984624e-05, 1.6566413250984624e-05, 1.6566413250984624e-05, 1.6566413250984624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6566413250984624e-05

Optimization complete. Final v2v error: 3.4196293354034424 mm

Highest mean error: 4.711328029632568 mm for frame 73

Lowest mean error: 2.8782098293304443 mm for frame 33

Saving results

Total time: 131.54831314086914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997957
Iteration 2/25 | Loss: 0.00169113
Iteration 3/25 | Loss: 0.00145184
Iteration 4/25 | Loss: 0.00138173
Iteration 5/25 | Loss: 0.00134596
Iteration 6/25 | Loss: 0.00131497
Iteration 7/25 | Loss: 0.00130563
Iteration 8/25 | Loss: 0.00130440
Iteration 9/25 | Loss: 0.00130350
Iteration 10/25 | Loss: 0.00130266
Iteration 11/25 | Loss: 0.00130222
Iteration 12/25 | Loss: 0.00132028
Iteration 13/25 | Loss: 0.00129772
Iteration 14/25 | Loss: 0.00129342
Iteration 15/25 | Loss: 0.00129310
Iteration 16/25 | Loss: 0.00129304
Iteration 17/25 | Loss: 0.00129304
Iteration 18/25 | Loss: 0.00129304
Iteration 19/25 | Loss: 0.00129303
Iteration 20/25 | Loss: 0.00129303
Iteration 21/25 | Loss: 0.00129303
Iteration 22/25 | Loss: 0.00129303
Iteration 23/25 | Loss: 0.00129303
Iteration 24/25 | Loss: 0.00129302
Iteration 25/25 | Loss: 0.00129302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58897817
Iteration 2/25 | Loss: 0.00132715
Iteration 3/25 | Loss: 0.00132715
Iteration 4/25 | Loss: 0.00132715
Iteration 5/25 | Loss: 0.00132715
Iteration 6/25 | Loss: 0.00132715
Iteration 7/25 | Loss: 0.00132715
Iteration 8/25 | Loss: 0.00132715
Iteration 9/25 | Loss: 0.00132715
Iteration 10/25 | Loss: 0.00132715
Iteration 11/25 | Loss: 0.00132715
Iteration 12/25 | Loss: 0.00132715
Iteration 13/25 | Loss: 0.00132715
Iteration 14/25 | Loss: 0.00132715
Iteration 15/25 | Loss: 0.00132715
Iteration 16/25 | Loss: 0.00132715
Iteration 17/25 | Loss: 0.00132715
Iteration 18/25 | Loss: 0.00132715
Iteration 19/25 | Loss: 0.00132715
Iteration 20/25 | Loss: 0.00132715
Iteration 21/25 | Loss: 0.00132715
Iteration 22/25 | Loss: 0.00132715
Iteration 23/25 | Loss: 0.00132715
Iteration 24/25 | Loss: 0.00132715
Iteration 25/25 | Loss: 0.00132715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132715
Iteration 2/1000 | Loss: 0.00085986
Iteration 3/1000 | Loss: 0.00005686
Iteration 4/1000 | Loss: 0.00003537
Iteration 5/1000 | Loss: 0.00091184
Iteration 6/1000 | Loss: 0.00204425
Iteration 7/1000 | Loss: 0.00240448
Iteration 8/1000 | Loss: 0.00063175
Iteration 9/1000 | Loss: 0.00009135
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00002595
Iteration 12/1000 | Loss: 0.00002198
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002031
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00031673
Iteration 21/1000 | Loss: 0.00002158
Iteration 22/1000 | Loss: 0.00001832
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001560
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001560
Iteration 81/1000 | Loss: 0.00001559
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001557
Iteration 90/1000 | Loss: 0.00001557
Iteration 91/1000 | Loss: 0.00001557
Iteration 92/1000 | Loss: 0.00001557
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001557
Iteration 95/1000 | Loss: 0.00001556
Iteration 96/1000 | Loss: 0.00001556
Iteration 97/1000 | Loss: 0.00001556
Iteration 98/1000 | Loss: 0.00001556
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001555
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001552
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001550
Iteration 130/1000 | Loss: 0.00001550
Iteration 131/1000 | Loss: 0.00001550
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001546
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001546
Iteration 164/1000 | Loss: 0.00001546
Iteration 165/1000 | Loss: 0.00001546
Iteration 166/1000 | Loss: 0.00001545
Iteration 167/1000 | Loss: 0.00001545
Iteration 168/1000 | Loss: 0.00001545
Iteration 169/1000 | Loss: 0.00001545
Iteration 170/1000 | Loss: 0.00001545
Iteration 171/1000 | Loss: 0.00001545
Iteration 172/1000 | Loss: 0.00001545
Iteration 173/1000 | Loss: 0.00001545
Iteration 174/1000 | Loss: 0.00001545
Iteration 175/1000 | Loss: 0.00001545
Iteration 176/1000 | Loss: 0.00001544
Iteration 177/1000 | Loss: 0.00001544
Iteration 178/1000 | Loss: 0.00001544
Iteration 179/1000 | Loss: 0.00001544
Iteration 180/1000 | Loss: 0.00001544
Iteration 181/1000 | Loss: 0.00001544
Iteration 182/1000 | Loss: 0.00001544
Iteration 183/1000 | Loss: 0.00001544
Iteration 184/1000 | Loss: 0.00001544
Iteration 185/1000 | Loss: 0.00001543
Iteration 186/1000 | Loss: 0.00001543
Iteration 187/1000 | Loss: 0.00001543
Iteration 188/1000 | Loss: 0.00001543
Iteration 189/1000 | Loss: 0.00001543
Iteration 190/1000 | Loss: 0.00001543
Iteration 191/1000 | Loss: 0.00001542
Iteration 192/1000 | Loss: 0.00001542
Iteration 193/1000 | Loss: 0.00001542
Iteration 194/1000 | Loss: 0.00001542
Iteration 195/1000 | Loss: 0.00001542
Iteration 196/1000 | Loss: 0.00001542
Iteration 197/1000 | Loss: 0.00001542
Iteration 198/1000 | Loss: 0.00001541
Iteration 199/1000 | Loss: 0.00001541
Iteration 200/1000 | Loss: 0.00001541
Iteration 201/1000 | Loss: 0.00001541
Iteration 202/1000 | Loss: 0.00001540
Iteration 203/1000 | Loss: 0.00001540
Iteration 204/1000 | Loss: 0.00001540
Iteration 205/1000 | Loss: 0.00001540
Iteration 206/1000 | Loss: 0.00001540
Iteration 207/1000 | Loss: 0.00001540
Iteration 208/1000 | Loss: 0.00001540
Iteration 209/1000 | Loss: 0.00001540
Iteration 210/1000 | Loss: 0.00001540
Iteration 211/1000 | Loss: 0.00001540
Iteration 212/1000 | Loss: 0.00001540
Iteration 213/1000 | Loss: 0.00001540
Iteration 214/1000 | Loss: 0.00001540
Iteration 215/1000 | Loss: 0.00001540
Iteration 216/1000 | Loss: 0.00001540
Iteration 217/1000 | Loss: 0.00001540
Iteration 218/1000 | Loss: 0.00001539
Iteration 219/1000 | Loss: 0.00001539
Iteration 220/1000 | Loss: 0.00001539
Iteration 221/1000 | Loss: 0.00001539
Iteration 222/1000 | Loss: 0.00001539
Iteration 223/1000 | Loss: 0.00001539
Iteration 224/1000 | Loss: 0.00001539
Iteration 225/1000 | Loss: 0.00001539
Iteration 226/1000 | Loss: 0.00001539
Iteration 227/1000 | Loss: 0.00001539
Iteration 228/1000 | Loss: 0.00001539
Iteration 229/1000 | Loss: 0.00001539
Iteration 230/1000 | Loss: 0.00001539
Iteration 231/1000 | Loss: 0.00001539
Iteration 232/1000 | Loss: 0.00001538
Iteration 233/1000 | Loss: 0.00001538
Iteration 234/1000 | Loss: 0.00001538
Iteration 235/1000 | Loss: 0.00001538
Iteration 236/1000 | Loss: 0.00001538
Iteration 237/1000 | Loss: 0.00001538
Iteration 238/1000 | Loss: 0.00001537
Iteration 239/1000 | Loss: 0.00001537
Iteration 240/1000 | Loss: 0.00001537
Iteration 241/1000 | Loss: 0.00001537
Iteration 242/1000 | Loss: 0.00001537
Iteration 243/1000 | Loss: 0.00001537
Iteration 244/1000 | Loss: 0.00001536
Iteration 245/1000 | Loss: 0.00001536
Iteration 246/1000 | Loss: 0.00001536
Iteration 247/1000 | Loss: 0.00001536
Iteration 248/1000 | Loss: 0.00001536
Iteration 249/1000 | Loss: 0.00001536
Iteration 250/1000 | Loss: 0.00001536
Iteration 251/1000 | Loss: 0.00001536
Iteration 252/1000 | Loss: 0.00001536
Iteration 253/1000 | Loss: 0.00001536
Iteration 254/1000 | Loss: 0.00001536
Iteration 255/1000 | Loss: 0.00001536
Iteration 256/1000 | Loss: 0.00001536
Iteration 257/1000 | Loss: 0.00001536
Iteration 258/1000 | Loss: 0.00001536
Iteration 259/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.536020135972649e-05, 1.536020135972649e-05, 1.536020135972649e-05, 1.536020135972649e-05, 1.536020135972649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536020135972649e-05

Optimization complete. Final v2v error: 3.2264416217803955 mm

Highest mean error: 5.129227638244629 mm for frame 123

Lowest mean error: 2.765359401702881 mm for frame 71

Saving results

Total time: 81.88690733909607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823876
Iteration 2/25 | Loss: 0.00129306
Iteration 3/25 | Loss: 0.00121196
Iteration 4/25 | Loss: 0.00120149
Iteration 5/25 | Loss: 0.00119789
Iteration 6/25 | Loss: 0.00119731
Iteration 7/25 | Loss: 0.00119731
Iteration 8/25 | Loss: 0.00119731
Iteration 9/25 | Loss: 0.00119731
Iteration 10/25 | Loss: 0.00119731
Iteration 11/25 | Loss: 0.00119731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011973133077844977, 0.0011973133077844977, 0.0011973133077844977, 0.0011973133077844977, 0.0011973133077844977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011973133077844977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55240297
Iteration 2/25 | Loss: 0.00106854
Iteration 3/25 | Loss: 0.00106854
Iteration 4/25 | Loss: 0.00106854
Iteration 5/25 | Loss: 0.00106853
Iteration 6/25 | Loss: 0.00106853
Iteration 7/25 | Loss: 0.00106853
Iteration 8/25 | Loss: 0.00106853
Iteration 9/25 | Loss: 0.00106853
Iteration 10/25 | Loss: 0.00106853
Iteration 11/25 | Loss: 0.00106853
Iteration 12/25 | Loss: 0.00106853
Iteration 13/25 | Loss: 0.00106853
Iteration 14/25 | Loss: 0.00106853
Iteration 15/25 | Loss: 0.00106853
Iteration 16/25 | Loss: 0.00106853
Iteration 17/25 | Loss: 0.00106853
Iteration 18/25 | Loss: 0.00106853
Iteration 19/25 | Loss: 0.00106853
Iteration 20/25 | Loss: 0.00106853
Iteration 21/25 | Loss: 0.00106853
Iteration 22/25 | Loss: 0.00106853
Iteration 23/25 | Loss: 0.00106853
Iteration 24/25 | Loss: 0.00106853
Iteration 25/25 | Loss: 0.00106853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106853
Iteration 2/1000 | Loss: 0.00002052
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001296
Iteration 5/1000 | Loss: 0.00001213
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001115
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001061
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001030
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001023
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001009
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00000999
Iteration 22/1000 | Loss: 0.00000998
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000998
Iteration 26/1000 | Loss: 0.00000997
Iteration 27/1000 | Loss: 0.00000996
Iteration 28/1000 | Loss: 0.00000996
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000988
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000987
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000986
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000985
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000984
Iteration 46/1000 | Loss: 0.00000983
Iteration 47/1000 | Loss: 0.00000983
Iteration 48/1000 | Loss: 0.00000982
Iteration 49/1000 | Loss: 0.00000982
Iteration 50/1000 | Loss: 0.00000982
Iteration 51/1000 | Loss: 0.00000981
Iteration 52/1000 | Loss: 0.00000981
Iteration 53/1000 | Loss: 0.00000980
Iteration 54/1000 | Loss: 0.00000980
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000978
Iteration 59/1000 | Loss: 0.00000978
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000977
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000976
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000974
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000973
Iteration 71/1000 | Loss: 0.00000973
Iteration 72/1000 | Loss: 0.00000973
Iteration 73/1000 | Loss: 0.00000973
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000972
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000969
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000968
Iteration 88/1000 | Loss: 0.00000968
Iteration 89/1000 | Loss: 0.00000968
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000967
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000966
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000965
Iteration 99/1000 | Loss: 0.00000965
Iteration 100/1000 | Loss: 0.00000965
Iteration 101/1000 | Loss: 0.00000965
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000963
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000960
Iteration 120/1000 | Loss: 0.00000960
Iteration 121/1000 | Loss: 0.00000960
Iteration 122/1000 | Loss: 0.00000960
Iteration 123/1000 | Loss: 0.00000960
Iteration 124/1000 | Loss: 0.00000960
Iteration 125/1000 | Loss: 0.00000960
Iteration 126/1000 | Loss: 0.00000960
Iteration 127/1000 | Loss: 0.00000960
Iteration 128/1000 | Loss: 0.00000960
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000959
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Iteration 133/1000 | Loss: 0.00000959
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000959
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000958
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000958
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000957
Iteration 152/1000 | Loss: 0.00000957
Iteration 153/1000 | Loss: 0.00000957
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000957
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000957
Iteration 159/1000 | Loss: 0.00000957
Iteration 160/1000 | Loss: 0.00000957
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.571830560162198e-06, 9.571830560162198e-06, 9.571830560162198e-06, 9.571830560162198e-06, 9.571830560162198e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.571830560162198e-06

Optimization complete. Final v2v error: 2.6789419651031494 mm

Highest mean error: 3.0461392402648926 mm for frame 51

Lowest mean error: 2.5418450832366943 mm for frame 109

Saving results

Total time: 36.26654815673828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824889
Iteration 2/25 | Loss: 0.00128097
Iteration 3/25 | Loss: 0.00121067
Iteration 4/25 | Loss: 0.00120174
Iteration 5/25 | Loss: 0.00119860
Iteration 6/25 | Loss: 0.00119819
Iteration 7/25 | Loss: 0.00119819
Iteration 8/25 | Loss: 0.00119819
Iteration 9/25 | Loss: 0.00119819
Iteration 10/25 | Loss: 0.00119819
Iteration 11/25 | Loss: 0.00119819
Iteration 12/25 | Loss: 0.00119819
Iteration 13/25 | Loss: 0.00119819
Iteration 14/25 | Loss: 0.00119819
Iteration 15/25 | Loss: 0.00119819
Iteration 16/25 | Loss: 0.00119819
Iteration 17/25 | Loss: 0.00119819
Iteration 18/25 | Loss: 0.00119819
Iteration 19/25 | Loss: 0.00119819
Iteration 20/25 | Loss: 0.00119819
Iteration 21/25 | Loss: 0.00119819
Iteration 22/25 | Loss: 0.00119819
Iteration 23/25 | Loss: 0.00119819
Iteration 24/25 | Loss: 0.00119819
Iteration 25/25 | Loss: 0.00119819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42958784
Iteration 2/25 | Loss: 0.00104579
Iteration 3/25 | Loss: 0.00104579
Iteration 4/25 | Loss: 0.00104579
Iteration 5/25 | Loss: 0.00104579
Iteration 6/25 | Loss: 0.00104579
Iteration 7/25 | Loss: 0.00104579
Iteration 8/25 | Loss: 0.00104579
Iteration 9/25 | Loss: 0.00104579
Iteration 10/25 | Loss: 0.00104579
Iteration 11/25 | Loss: 0.00104579
Iteration 12/25 | Loss: 0.00104579
Iteration 13/25 | Loss: 0.00104579
Iteration 14/25 | Loss: 0.00104579
Iteration 15/25 | Loss: 0.00104579
Iteration 16/25 | Loss: 0.00104579
Iteration 17/25 | Loss: 0.00104579
Iteration 18/25 | Loss: 0.00104579
Iteration 19/25 | Loss: 0.00104579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010457861935719848, 0.0010457861935719848, 0.0010457861935719848, 0.0010457861935719848, 0.0010457861935719848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010457861935719848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104579
Iteration 2/1000 | Loss: 0.00001985
Iteration 3/1000 | Loss: 0.00001437
Iteration 4/1000 | Loss: 0.00001287
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001166
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001132
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001090
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001035
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001028
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001025
Iteration 22/1000 | Loss: 0.00001024
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001019
Iteration 32/1000 | Loss: 0.00001019
Iteration 33/1000 | Loss: 0.00001018
Iteration 34/1000 | Loss: 0.00001018
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001017
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001013
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001011
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000993
Iteration 93/1000 | Loss: 0.00000993
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000992
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000992
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000989
Iteration 107/1000 | Loss: 0.00000989
Iteration 108/1000 | Loss: 0.00000989
Iteration 109/1000 | Loss: 0.00000989
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000989
Iteration 112/1000 | Loss: 0.00000989
Iteration 113/1000 | Loss: 0.00000989
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000988
Iteration 116/1000 | Loss: 0.00000988
Iteration 117/1000 | Loss: 0.00000988
Iteration 118/1000 | Loss: 0.00000987
Iteration 119/1000 | Loss: 0.00000987
Iteration 120/1000 | Loss: 0.00000986
Iteration 121/1000 | Loss: 0.00000986
Iteration 122/1000 | Loss: 0.00000986
Iteration 123/1000 | Loss: 0.00000986
Iteration 124/1000 | Loss: 0.00000985
Iteration 125/1000 | Loss: 0.00000985
Iteration 126/1000 | Loss: 0.00000985
Iteration 127/1000 | Loss: 0.00000985
Iteration 128/1000 | Loss: 0.00000985
Iteration 129/1000 | Loss: 0.00000985
Iteration 130/1000 | Loss: 0.00000984
Iteration 131/1000 | Loss: 0.00000984
Iteration 132/1000 | Loss: 0.00000984
Iteration 133/1000 | Loss: 0.00000984
Iteration 134/1000 | Loss: 0.00000984
Iteration 135/1000 | Loss: 0.00000984
Iteration 136/1000 | Loss: 0.00000984
Iteration 137/1000 | Loss: 0.00000983
Iteration 138/1000 | Loss: 0.00000983
Iteration 139/1000 | Loss: 0.00000983
Iteration 140/1000 | Loss: 0.00000983
Iteration 141/1000 | Loss: 0.00000983
Iteration 142/1000 | Loss: 0.00000983
Iteration 143/1000 | Loss: 0.00000983
Iteration 144/1000 | Loss: 0.00000983
Iteration 145/1000 | Loss: 0.00000982
Iteration 146/1000 | Loss: 0.00000982
Iteration 147/1000 | Loss: 0.00000982
Iteration 148/1000 | Loss: 0.00000982
Iteration 149/1000 | Loss: 0.00000982
Iteration 150/1000 | Loss: 0.00000982
Iteration 151/1000 | Loss: 0.00000982
Iteration 152/1000 | Loss: 0.00000982
Iteration 153/1000 | Loss: 0.00000981
Iteration 154/1000 | Loss: 0.00000981
Iteration 155/1000 | Loss: 0.00000981
Iteration 156/1000 | Loss: 0.00000981
Iteration 157/1000 | Loss: 0.00000981
Iteration 158/1000 | Loss: 0.00000981
Iteration 159/1000 | Loss: 0.00000980
Iteration 160/1000 | Loss: 0.00000980
Iteration 161/1000 | Loss: 0.00000980
Iteration 162/1000 | Loss: 0.00000980
Iteration 163/1000 | Loss: 0.00000980
Iteration 164/1000 | Loss: 0.00000980
Iteration 165/1000 | Loss: 0.00000980
Iteration 166/1000 | Loss: 0.00000980
Iteration 167/1000 | Loss: 0.00000980
Iteration 168/1000 | Loss: 0.00000980
Iteration 169/1000 | Loss: 0.00000980
Iteration 170/1000 | Loss: 0.00000980
Iteration 171/1000 | Loss: 0.00000980
Iteration 172/1000 | Loss: 0.00000980
Iteration 173/1000 | Loss: 0.00000980
Iteration 174/1000 | Loss: 0.00000980
Iteration 175/1000 | Loss: 0.00000980
Iteration 176/1000 | Loss: 0.00000980
Iteration 177/1000 | Loss: 0.00000979
Iteration 178/1000 | Loss: 0.00000979
Iteration 179/1000 | Loss: 0.00000979
Iteration 180/1000 | Loss: 0.00000979
Iteration 181/1000 | Loss: 0.00000979
Iteration 182/1000 | Loss: 0.00000979
Iteration 183/1000 | Loss: 0.00000979
Iteration 184/1000 | Loss: 0.00000979
Iteration 185/1000 | Loss: 0.00000979
Iteration 186/1000 | Loss: 0.00000979
Iteration 187/1000 | Loss: 0.00000979
Iteration 188/1000 | Loss: 0.00000979
Iteration 189/1000 | Loss: 0.00000979
Iteration 190/1000 | Loss: 0.00000979
Iteration 191/1000 | Loss: 0.00000979
Iteration 192/1000 | Loss: 0.00000979
Iteration 193/1000 | Loss: 0.00000979
Iteration 194/1000 | Loss: 0.00000979
Iteration 195/1000 | Loss: 0.00000979
Iteration 196/1000 | Loss: 0.00000979
Iteration 197/1000 | Loss: 0.00000979
Iteration 198/1000 | Loss: 0.00000979
Iteration 199/1000 | Loss: 0.00000979
Iteration 200/1000 | Loss: 0.00000979
Iteration 201/1000 | Loss: 0.00000978
Iteration 202/1000 | Loss: 0.00000978
Iteration 203/1000 | Loss: 0.00000978
Iteration 204/1000 | Loss: 0.00000978
Iteration 205/1000 | Loss: 0.00000978
Iteration 206/1000 | Loss: 0.00000978
Iteration 207/1000 | Loss: 0.00000978
Iteration 208/1000 | Loss: 0.00000978
Iteration 209/1000 | Loss: 0.00000978
Iteration 210/1000 | Loss: 0.00000978
Iteration 211/1000 | Loss: 0.00000978
Iteration 212/1000 | Loss: 0.00000978
Iteration 213/1000 | Loss: 0.00000978
Iteration 214/1000 | Loss: 0.00000978
Iteration 215/1000 | Loss: 0.00000978
Iteration 216/1000 | Loss: 0.00000978
Iteration 217/1000 | Loss: 0.00000978
Iteration 218/1000 | Loss: 0.00000978
Iteration 219/1000 | Loss: 0.00000978
Iteration 220/1000 | Loss: 0.00000978
Iteration 221/1000 | Loss: 0.00000977
Iteration 222/1000 | Loss: 0.00000977
Iteration 223/1000 | Loss: 0.00000977
Iteration 224/1000 | Loss: 0.00000977
Iteration 225/1000 | Loss: 0.00000977
Iteration 226/1000 | Loss: 0.00000977
Iteration 227/1000 | Loss: 0.00000977
Iteration 228/1000 | Loss: 0.00000977
Iteration 229/1000 | Loss: 0.00000977
Iteration 230/1000 | Loss: 0.00000977
Iteration 231/1000 | Loss: 0.00000977
Iteration 232/1000 | Loss: 0.00000977
Iteration 233/1000 | Loss: 0.00000977
Iteration 234/1000 | Loss: 0.00000977
Iteration 235/1000 | Loss: 0.00000977
Iteration 236/1000 | Loss: 0.00000977
Iteration 237/1000 | Loss: 0.00000977
Iteration 238/1000 | Loss: 0.00000977
Iteration 239/1000 | Loss: 0.00000977
Iteration 240/1000 | Loss: 0.00000977
Iteration 241/1000 | Loss: 0.00000977
Iteration 242/1000 | Loss: 0.00000977
Iteration 243/1000 | Loss: 0.00000977
Iteration 244/1000 | Loss: 0.00000977
Iteration 245/1000 | Loss: 0.00000976
Iteration 246/1000 | Loss: 0.00000976
Iteration 247/1000 | Loss: 0.00000976
Iteration 248/1000 | Loss: 0.00000976
Iteration 249/1000 | Loss: 0.00000976
Iteration 250/1000 | Loss: 0.00000976
Iteration 251/1000 | Loss: 0.00000976
Iteration 252/1000 | Loss: 0.00000976
Iteration 253/1000 | Loss: 0.00000976
Iteration 254/1000 | Loss: 0.00000976
Iteration 255/1000 | Loss: 0.00000976
Iteration 256/1000 | Loss: 0.00000976
Iteration 257/1000 | Loss: 0.00000975
Iteration 258/1000 | Loss: 0.00000975
Iteration 259/1000 | Loss: 0.00000975
Iteration 260/1000 | Loss: 0.00000975
Iteration 261/1000 | Loss: 0.00000975
Iteration 262/1000 | Loss: 0.00000975
Iteration 263/1000 | Loss: 0.00000975
Iteration 264/1000 | Loss: 0.00000975
Iteration 265/1000 | Loss: 0.00000975
Iteration 266/1000 | Loss: 0.00000974
Iteration 267/1000 | Loss: 0.00000974
Iteration 268/1000 | Loss: 0.00000974
Iteration 269/1000 | Loss: 0.00000974
Iteration 270/1000 | Loss: 0.00000974
Iteration 271/1000 | Loss: 0.00000974
Iteration 272/1000 | Loss: 0.00000974
Iteration 273/1000 | Loss: 0.00000974
Iteration 274/1000 | Loss: 0.00000974
Iteration 275/1000 | Loss: 0.00000974
Iteration 276/1000 | Loss: 0.00000974
Iteration 277/1000 | Loss: 0.00000974
Iteration 278/1000 | Loss: 0.00000974
Iteration 279/1000 | Loss: 0.00000974
Iteration 280/1000 | Loss: 0.00000974
Iteration 281/1000 | Loss: 0.00000974
Iteration 282/1000 | Loss: 0.00000974
Iteration 283/1000 | Loss: 0.00000974
Iteration 284/1000 | Loss: 0.00000974
Iteration 285/1000 | Loss: 0.00000974
Iteration 286/1000 | Loss: 0.00000974
Iteration 287/1000 | Loss: 0.00000974
Iteration 288/1000 | Loss: 0.00000974
Iteration 289/1000 | Loss: 0.00000974
Iteration 290/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [9.738785593071952e-06, 9.738785593071952e-06, 9.738785593071952e-06, 9.738785593071952e-06, 9.738785593071952e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.738785593071952e-06

Optimization complete. Final v2v error: 2.685950517654419 mm

Highest mean error: 3.1815078258514404 mm for frame 89

Lowest mean error: 2.5288314819335938 mm for frame 11

Saving results

Total time: 41.32090878486633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878690
Iteration 2/25 | Loss: 0.00188053
Iteration 3/25 | Loss: 0.00156531
Iteration 4/25 | Loss: 0.00148672
Iteration 5/25 | Loss: 0.00143347
Iteration 6/25 | Loss: 0.00136511
Iteration 7/25 | Loss: 0.00134784
Iteration 8/25 | Loss: 0.00134509
Iteration 9/25 | Loss: 0.00134454
Iteration 10/25 | Loss: 0.00134437
Iteration 11/25 | Loss: 0.00134426
Iteration 12/25 | Loss: 0.00134421
Iteration 13/25 | Loss: 0.00134421
Iteration 14/25 | Loss: 0.00134421
Iteration 15/25 | Loss: 0.00134421
Iteration 16/25 | Loss: 0.00134421
Iteration 17/25 | Loss: 0.00134421
Iteration 18/25 | Loss: 0.00134420
Iteration 19/25 | Loss: 0.00134420
Iteration 20/25 | Loss: 0.00134420
Iteration 21/25 | Loss: 0.00134420
Iteration 22/25 | Loss: 0.00134420
Iteration 23/25 | Loss: 0.00134420
Iteration 24/25 | Loss: 0.00134420
Iteration 25/25 | Loss: 0.00134420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57576942
Iteration 2/25 | Loss: 0.00092497
Iteration 3/25 | Loss: 0.00092497
Iteration 4/25 | Loss: 0.00092497
Iteration 5/25 | Loss: 0.00092497
Iteration 6/25 | Loss: 0.00092497
Iteration 7/25 | Loss: 0.00092497
Iteration 8/25 | Loss: 0.00092497
Iteration 9/25 | Loss: 0.00092497
Iteration 10/25 | Loss: 0.00092497
Iteration 11/25 | Loss: 0.00092497
Iteration 12/25 | Loss: 0.00092497
Iteration 13/25 | Loss: 0.00092497
Iteration 14/25 | Loss: 0.00092497
Iteration 15/25 | Loss: 0.00092497
Iteration 16/25 | Loss: 0.00092497
Iteration 17/25 | Loss: 0.00092497
Iteration 18/25 | Loss: 0.00092497
Iteration 19/25 | Loss: 0.00092497
Iteration 20/25 | Loss: 0.00092497
Iteration 21/25 | Loss: 0.00092497
Iteration 22/25 | Loss: 0.00092497
Iteration 23/25 | Loss: 0.00092497
Iteration 24/25 | Loss: 0.00092497
Iteration 25/25 | Loss: 0.00092497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092497
Iteration 2/1000 | Loss: 0.00005465
Iteration 3/1000 | Loss: 0.00003768
Iteration 4/1000 | Loss: 0.00003073
Iteration 5/1000 | Loss: 0.00002821
Iteration 6/1000 | Loss: 0.00002727
Iteration 7/1000 | Loss: 0.00002666
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002529
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00002499
Iteration 14/1000 | Loss: 0.00002497
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002474
Iteration 18/1000 | Loss: 0.00002472
Iteration 19/1000 | Loss: 0.00002471
Iteration 20/1000 | Loss: 0.00002470
Iteration 21/1000 | Loss: 0.00002468
Iteration 22/1000 | Loss: 0.00002467
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002458
Iteration 26/1000 | Loss: 0.00002455
Iteration 27/1000 | Loss: 0.00002455
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002454
Iteration 30/1000 | Loss: 0.00002454
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002454
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002453
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002451
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002450
Iteration 40/1000 | Loss: 0.00002449
Iteration 41/1000 | Loss: 0.00002449
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002449
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002448
Iteration 47/1000 | Loss: 0.00002448
Iteration 48/1000 | Loss: 0.00002447
Iteration 49/1000 | Loss: 0.00002447
Iteration 50/1000 | Loss: 0.00002447
Iteration 51/1000 | Loss: 0.00002446
Iteration 52/1000 | Loss: 0.00002446
Iteration 53/1000 | Loss: 0.00002446
Iteration 54/1000 | Loss: 0.00002446
Iteration 55/1000 | Loss: 0.00002445
Iteration 56/1000 | Loss: 0.00002445
Iteration 57/1000 | Loss: 0.00002445
Iteration 58/1000 | Loss: 0.00002444
Iteration 59/1000 | Loss: 0.00002444
Iteration 60/1000 | Loss: 0.00002443
Iteration 61/1000 | Loss: 0.00002443
Iteration 62/1000 | Loss: 0.00002443
Iteration 63/1000 | Loss: 0.00002443
Iteration 64/1000 | Loss: 0.00002443
Iteration 65/1000 | Loss: 0.00002443
Iteration 66/1000 | Loss: 0.00002443
Iteration 67/1000 | Loss: 0.00002443
Iteration 68/1000 | Loss: 0.00002443
Iteration 69/1000 | Loss: 0.00002443
Iteration 70/1000 | Loss: 0.00002443
Iteration 71/1000 | Loss: 0.00002443
Iteration 72/1000 | Loss: 0.00002442
Iteration 73/1000 | Loss: 0.00002442
Iteration 74/1000 | Loss: 0.00002441
Iteration 75/1000 | Loss: 0.00002441
Iteration 76/1000 | Loss: 0.00002441
Iteration 77/1000 | Loss: 0.00002441
Iteration 78/1000 | Loss: 0.00002441
Iteration 79/1000 | Loss: 0.00002440
Iteration 80/1000 | Loss: 0.00002440
Iteration 81/1000 | Loss: 0.00002440
Iteration 82/1000 | Loss: 0.00002440
Iteration 83/1000 | Loss: 0.00002440
Iteration 84/1000 | Loss: 0.00002440
Iteration 85/1000 | Loss: 0.00002439
Iteration 86/1000 | Loss: 0.00002439
Iteration 87/1000 | Loss: 0.00002439
Iteration 88/1000 | Loss: 0.00002439
Iteration 89/1000 | Loss: 0.00002438
Iteration 90/1000 | Loss: 0.00002438
Iteration 91/1000 | Loss: 0.00002438
Iteration 92/1000 | Loss: 0.00002437
Iteration 93/1000 | Loss: 0.00002437
Iteration 94/1000 | Loss: 0.00002437
Iteration 95/1000 | Loss: 0.00002436
Iteration 96/1000 | Loss: 0.00002436
Iteration 97/1000 | Loss: 0.00002436
Iteration 98/1000 | Loss: 0.00002436
Iteration 99/1000 | Loss: 0.00002435
Iteration 100/1000 | Loss: 0.00002435
Iteration 101/1000 | Loss: 0.00002435
Iteration 102/1000 | Loss: 0.00002435
Iteration 103/1000 | Loss: 0.00002435
Iteration 104/1000 | Loss: 0.00002434
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002434
Iteration 107/1000 | Loss: 0.00002434
Iteration 108/1000 | Loss: 0.00002434
Iteration 109/1000 | Loss: 0.00002434
Iteration 110/1000 | Loss: 0.00002434
Iteration 111/1000 | Loss: 0.00002434
Iteration 112/1000 | Loss: 0.00002434
Iteration 113/1000 | Loss: 0.00002434
Iteration 114/1000 | Loss: 0.00002433
Iteration 115/1000 | Loss: 0.00002433
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002432
Iteration 119/1000 | Loss: 0.00002432
Iteration 120/1000 | Loss: 0.00002432
Iteration 121/1000 | Loss: 0.00002431
Iteration 122/1000 | Loss: 0.00002431
Iteration 123/1000 | Loss: 0.00002431
Iteration 124/1000 | Loss: 0.00002430
Iteration 125/1000 | Loss: 0.00002430
Iteration 126/1000 | Loss: 0.00002429
Iteration 127/1000 | Loss: 0.00002429
Iteration 128/1000 | Loss: 0.00002429
Iteration 129/1000 | Loss: 0.00002429
Iteration 130/1000 | Loss: 0.00002429
Iteration 131/1000 | Loss: 0.00002429
Iteration 132/1000 | Loss: 0.00002429
Iteration 133/1000 | Loss: 0.00002429
Iteration 134/1000 | Loss: 0.00002429
Iteration 135/1000 | Loss: 0.00002429
Iteration 136/1000 | Loss: 0.00002429
Iteration 137/1000 | Loss: 0.00002428
Iteration 138/1000 | Loss: 0.00002428
Iteration 139/1000 | Loss: 0.00002428
Iteration 140/1000 | Loss: 0.00002428
Iteration 141/1000 | Loss: 0.00002428
Iteration 142/1000 | Loss: 0.00002427
Iteration 143/1000 | Loss: 0.00002427
Iteration 144/1000 | Loss: 0.00002426
Iteration 145/1000 | Loss: 0.00002426
Iteration 146/1000 | Loss: 0.00002426
Iteration 147/1000 | Loss: 0.00002426
Iteration 148/1000 | Loss: 0.00002425
Iteration 149/1000 | Loss: 0.00002425
Iteration 150/1000 | Loss: 0.00002425
Iteration 151/1000 | Loss: 0.00002425
Iteration 152/1000 | Loss: 0.00002425
Iteration 153/1000 | Loss: 0.00002424
Iteration 154/1000 | Loss: 0.00002424
Iteration 155/1000 | Loss: 0.00002424
Iteration 156/1000 | Loss: 0.00002424
Iteration 157/1000 | Loss: 0.00002424
Iteration 158/1000 | Loss: 0.00002424
Iteration 159/1000 | Loss: 0.00002424
Iteration 160/1000 | Loss: 0.00002424
Iteration 161/1000 | Loss: 0.00002424
Iteration 162/1000 | Loss: 0.00002424
Iteration 163/1000 | Loss: 0.00002424
Iteration 164/1000 | Loss: 0.00002424
Iteration 165/1000 | Loss: 0.00002424
Iteration 166/1000 | Loss: 0.00002424
Iteration 167/1000 | Loss: 0.00002424
Iteration 168/1000 | Loss: 0.00002424
Iteration 169/1000 | Loss: 0.00002424
Iteration 170/1000 | Loss: 0.00002424
Iteration 171/1000 | Loss: 0.00002424
Iteration 172/1000 | Loss: 0.00002424
Iteration 173/1000 | Loss: 0.00002424
Iteration 174/1000 | Loss: 0.00002424
Iteration 175/1000 | Loss: 0.00002424
Iteration 176/1000 | Loss: 0.00002424
Iteration 177/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.4239801859948784e-05, 2.4239801859948784e-05, 2.4239801859948784e-05, 2.4239801859948784e-05, 2.4239801859948784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4239801859948784e-05

Optimization complete. Final v2v error: 3.998141288757324 mm

Highest mean error: 5.179200172424316 mm for frame 90

Lowest mean error: 3.0017995834350586 mm for frame 0

Saving results

Total time: 51.08583188056946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045952
Iteration 2/25 | Loss: 0.01045952
Iteration 3/25 | Loss: 0.01045952
Iteration 4/25 | Loss: 0.01045951
Iteration 5/25 | Loss: 0.01045951
Iteration 6/25 | Loss: 0.01045950
Iteration 7/25 | Loss: 0.01045950
Iteration 8/25 | Loss: 0.01045950
Iteration 9/25 | Loss: 0.01045950
Iteration 10/25 | Loss: 0.01045949
Iteration 11/25 | Loss: 0.01045949
Iteration 12/25 | Loss: 0.01045949
Iteration 13/25 | Loss: 0.01045949
Iteration 14/25 | Loss: 0.01045948
Iteration 15/25 | Loss: 0.01045948
Iteration 16/25 | Loss: 0.01045948
Iteration 17/25 | Loss: 0.01045948
Iteration 18/25 | Loss: 0.01045948
Iteration 19/25 | Loss: 0.01045948
Iteration 20/25 | Loss: 0.01045947
Iteration 21/25 | Loss: 0.01045947
Iteration 22/25 | Loss: 0.01045947
Iteration 23/25 | Loss: 0.01045947
Iteration 24/25 | Loss: 0.01045947
Iteration 25/25 | Loss: 0.01045946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94258821
Iteration 2/25 | Loss: 0.09370024
Iteration 3/25 | Loss: 0.09256857
Iteration 4/25 | Loss: 0.09216230
Iteration 5/25 | Loss: 0.09216228
Iteration 6/25 | Loss: 0.09216227
Iteration 7/25 | Loss: 0.09216226
Iteration 8/25 | Loss: 0.09216226
Iteration 9/25 | Loss: 0.09216226
Iteration 10/25 | Loss: 0.09216226
Iteration 11/25 | Loss: 0.09216226
Iteration 12/25 | Loss: 0.09216227
Iteration 13/25 | Loss: 0.09216775
Iteration 14/25 | Loss: 0.09216235
Iteration 15/25 | Loss: 0.09216229
Iteration 16/25 | Loss: 0.09216227
Iteration 17/25 | Loss: 0.09216227
Iteration 18/25 | Loss: 0.09216227
Iteration 19/25 | Loss: 0.09216227
Iteration 20/25 | Loss: 0.09216226
Iteration 21/25 | Loss: 0.09216226
Iteration 22/25 | Loss: 0.09216226
Iteration 23/25 | Loss: 0.09216227
Iteration 24/25 | Loss: 0.09216225
Iteration 25/25 | Loss: 0.09216225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09216225
Iteration 2/1000 | Loss: 0.00146454
Iteration 3/1000 | Loss: 0.00139613
Iteration 4/1000 | Loss: 0.00335432
Iteration 5/1000 | Loss: 0.00343106
Iteration 6/1000 | Loss: 0.00307637
Iteration 7/1000 | Loss: 0.00101817
Iteration 8/1000 | Loss: 0.00249958
Iteration 9/1000 | Loss: 0.00195293
Iteration 10/1000 | Loss: 0.00006744
Iteration 11/1000 | Loss: 0.00068030
Iteration 12/1000 | Loss: 0.00310998
Iteration 13/1000 | Loss: 0.00013671
Iteration 14/1000 | Loss: 0.00079968
Iteration 15/1000 | Loss: 0.00183331
Iteration 16/1000 | Loss: 0.00197253
Iteration 17/1000 | Loss: 0.00041320
Iteration 18/1000 | Loss: 0.00016592
Iteration 19/1000 | Loss: 0.00065184
Iteration 20/1000 | Loss: 0.00073163
Iteration 21/1000 | Loss: 0.00007614
Iteration 22/1000 | Loss: 0.00011391
Iteration 23/1000 | Loss: 0.00015207
Iteration 24/1000 | Loss: 0.00095644
Iteration 25/1000 | Loss: 0.00005578
Iteration 26/1000 | Loss: 0.00005831
Iteration 27/1000 | Loss: 0.00004005
Iteration 28/1000 | Loss: 0.00009722
Iteration 29/1000 | Loss: 0.00236409
Iteration 30/1000 | Loss: 0.00056831
Iteration 31/1000 | Loss: 0.00016063
Iteration 32/1000 | Loss: 0.00005847
Iteration 33/1000 | Loss: 0.00004354
Iteration 34/1000 | Loss: 0.00011383
Iteration 35/1000 | Loss: 0.00054047
Iteration 36/1000 | Loss: 0.00007695
Iteration 37/1000 | Loss: 0.00022493
Iteration 38/1000 | Loss: 0.00018704
Iteration 39/1000 | Loss: 0.00007418
Iteration 40/1000 | Loss: 0.00018048
Iteration 41/1000 | Loss: 0.00003483
Iteration 42/1000 | Loss: 0.00002815
Iteration 43/1000 | Loss: 0.00019941
Iteration 44/1000 | Loss: 0.00006477
Iteration 45/1000 | Loss: 0.00002984
Iteration 46/1000 | Loss: 0.00007849
Iteration 47/1000 | Loss: 0.00003318
Iteration 48/1000 | Loss: 0.00003569
Iteration 49/1000 | Loss: 0.00002489
Iteration 50/1000 | Loss: 0.00044854
Iteration 51/1000 | Loss: 0.00046001
Iteration 52/1000 | Loss: 0.00004825
Iteration 53/1000 | Loss: 0.00014903
Iteration 54/1000 | Loss: 0.00040611
Iteration 55/1000 | Loss: 0.00002745
Iteration 56/1000 | Loss: 0.00020388
Iteration 57/1000 | Loss: 0.00016315
Iteration 58/1000 | Loss: 0.00007190
Iteration 59/1000 | Loss: 0.00005962
Iteration 60/1000 | Loss: 0.00002989
Iteration 61/1000 | Loss: 0.00008386
Iteration 62/1000 | Loss: 0.00004305
Iteration 63/1000 | Loss: 0.00040837
Iteration 64/1000 | Loss: 0.00006028
Iteration 65/1000 | Loss: 0.00003000
Iteration 66/1000 | Loss: 0.00003445
Iteration 67/1000 | Loss: 0.00004108
Iteration 68/1000 | Loss: 0.00002065
Iteration 69/1000 | Loss: 0.00009470
Iteration 70/1000 | Loss: 0.00033976
Iteration 71/1000 | Loss: 0.00002244
Iteration 72/1000 | Loss: 0.00003715
Iteration 73/1000 | Loss: 0.00003196
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00004033
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002942
Iteration 79/1000 | Loss: 0.00003966
Iteration 80/1000 | Loss: 0.00002592
Iteration 81/1000 | Loss: 0.00003120
Iteration 82/1000 | Loss: 0.00002216
Iteration 83/1000 | Loss: 0.00001927
Iteration 84/1000 | Loss: 0.00001927
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001925
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00002862
Iteration 98/1000 | Loss: 0.00022777
Iteration 99/1000 | Loss: 0.00007874
Iteration 100/1000 | Loss: 0.00006541
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00004389
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00003717
Iteration 105/1000 | Loss: 0.00006963
Iteration 106/1000 | Loss: 0.00003618
Iteration 107/1000 | Loss: 0.00003868
Iteration 108/1000 | Loss: 0.00003937
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00003094
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00002668
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001869
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001868
Iteration 131/1000 | Loss: 0.00001868
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001865
Iteration 136/1000 | Loss: 0.00001865
Iteration 137/1000 | Loss: 0.00001865
Iteration 138/1000 | Loss: 0.00001865
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00001865
Iteration 142/1000 | Loss: 0.00001862
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001861
Iteration 148/1000 | Loss: 0.00001861
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Iteration 152/1000 | Loss: 0.00001860
Iteration 153/1000 | Loss: 0.00001860
Iteration 154/1000 | Loss: 0.00001860
Iteration 155/1000 | Loss: 0.00001859
Iteration 156/1000 | Loss: 0.00001859
Iteration 157/1000 | Loss: 0.00001858
Iteration 158/1000 | Loss: 0.00001858
Iteration 159/1000 | Loss: 0.00001858
Iteration 160/1000 | Loss: 0.00001858
Iteration 161/1000 | Loss: 0.00001858
Iteration 162/1000 | Loss: 0.00001857
Iteration 163/1000 | Loss: 0.00001857
Iteration 164/1000 | Loss: 0.00001857
Iteration 165/1000 | Loss: 0.00001857
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00002119
Iteration 170/1000 | Loss: 0.00007524
Iteration 171/1000 | Loss: 0.00004563
Iteration 172/1000 | Loss: 0.00002459
Iteration 173/1000 | Loss: 0.00002069
Iteration 174/1000 | Loss: 0.00003086
Iteration 175/1000 | Loss: 0.00001897
Iteration 176/1000 | Loss: 0.00003208
Iteration 177/1000 | Loss: 0.00001885
Iteration 178/1000 | Loss: 0.00001856
Iteration 179/1000 | Loss: 0.00001864
Iteration 180/1000 | Loss: 0.00001844
Iteration 181/1000 | Loss: 0.00001844
Iteration 182/1000 | Loss: 0.00001844
Iteration 183/1000 | Loss: 0.00001844
Iteration 184/1000 | Loss: 0.00001844
Iteration 185/1000 | Loss: 0.00001844
Iteration 186/1000 | Loss: 0.00001843
Iteration 187/1000 | Loss: 0.00001843
Iteration 188/1000 | Loss: 0.00001843
Iteration 189/1000 | Loss: 0.00001843
Iteration 190/1000 | Loss: 0.00001843
Iteration 191/1000 | Loss: 0.00001843
Iteration 192/1000 | Loss: 0.00001843
Iteration 193/1000 | Loss: 0.00001843
Iteration 194/1000 | Loss: 0.00001843
Iteration 195/1000 | Loss: 0.00001843
Iteration 196/1000 | Loss: 0.00001843
Iteration 197/1000 | Loss: 0.00001843
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00001843
Iteration 203/1000 | Loss: 0.00001843
Iteration 204/1000 | Loss: 0.00001843
Iteration 205/1000 | Loss: 0.00001843
Iteration 206/1000 | Loss: 0.00001843
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001843
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.8431508578942157e-05, 1.8431508578942157e-05, 1.8431508578942157e-05, 1.8431508578942157e-05, 1.8431508578942157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8431508578942157e-05

Optimization complete. Final v2v error: 3.6505396366119385 mm

Highest mean error: 5.194579124450684 mm for frame 27

Lowest mean error: 2.860656976699829 mm for frame 237

Saving results

Total time: 190.61711525917053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802542
Iteration 2/25 | Loss: 0.00144023
Iteration 3/25 | Loss: 0.00124649
Iteration 4/25 | Loss: 0.00123090
Iteration 5/25 | Loss: 0.00122894
Iteration 6/25 | Loss: 0.00122833
Iteration 7/25 | Loss: 0.00122833
Iteration 8/25 | Loss: 0.00122833
Iteration 9/25 | Loss: 0.00122833
Iteration 10/25 | Loss: 0.00122833
Iteration 11/25 | Loss: 0.00122833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012283262331038713, 0.0012283262331038713, 0.0012283262331038713, 0.0012283262331038713, 0.0012283262331038713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012283262331038713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34940577
Iteration 2/25 | Loss: 0.00104396
Iteration 3/25 | Loss: 0.00104396
Iteration 4/25 | Loss: 0.00104396
Iteration 5/25 | Loss: 0.00104396
Iteration 6/25 | Loss: 0.00104396
Iteration 7/25 | Loss: 0.00104396
Iteration 8/25 | Loss: 0.00104396
Iteration 9/25 | Loss: 0.00104396
Iteration 10/25 | Loss: 0.00104396
Iteration 11/25 | Loss: 0.00104396
Iteration 12/25 | Loss: 0.00104396
Iteration 13/25 | Loss: 0.00104396
Iteration 14/25 | Loss: 0.00104396
Iteration 15/25 | Loss: 0.00104396
Iteration 16/25 | Loss: 0.00104396
Iteration 17/25 | Loss: 0.00104396
Iteration 18/25 | Loss: 0.00104396
Iteration 19/25 | Loss: 0.00104396
Iteration 20/25 | Loss: 0.00104396
Iteration 21/25 | Loss: 0.00104396
Iteration 22/25 | Loss: 0.00104396
Iteration 23/25 | Loss: 0.00104396
Iteration 24/25 | Loss: 0.00104396
Iteration 25/25 | Loss: 0.00104396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104396
Iteration 2/1000 | Loss: 0.00002455
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001350
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001129
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001111
Iteration 38/1000 | Loss: 0.00001110
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001109
Iteration 43/1000 | Loss: 0.00001109
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001093
Iteration 111/1000 | Loss: 0.00001093
Iteration 112/1000 | Loss: 0.00001093
Iteration 113/1000 | Loss: 0.00001093
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001092
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001090
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001089
Iteration 133/1000 | Loss: 0.00001089
Iteration 134/1000 | Loss: 0.00001089
Iteration 135/1000 | Loss: 0.00001089
Iteration 136/1000 | Loss: 0.00001089
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001088
Iteration 139/1000 | Loss: 0.00001088
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001086
Iteration 146/1000 | Loss: 0.00001086
Iteration 147/1000 | Loss: 0.00001086
Iteration 148/1000 | Loss: 0.00001085
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001083
Iteration 156/1000 | Loss: 0.00001083
Iteration 157/1000 | Loss: 0.00001083
Iteration 158/1000 | Loss: 0.00001083
Iteration 159/1000 | Loss: 0.00001083
Iteration 160/1000 | Loss: 0.00001083
Iteration 161/1000 | Loss: 0.00001082
Iteration 162/1000 | Loss: 0.00001082
Iteration 163/1000 | Loss: 0.00001082
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001081
Iteration 168/1000 | Loss: 0.00001081
Iteration 169/1000 | Loss: 0.00001081
Iteration 170/1000 | Loss: 0.00001081
Iteration 171/1000 | Loss: 0.00001081
Iteration 172/1000 | Loss: 0.00001081
Iteration 173/1000 | Loss: 0.00001081
Iteration 174/1000 | Loss: 0.00001081
Iteration 175/1000 | Loss: 0.00001081
Iteration 176/1000 | Loss: 0.00001081
Iteration 177/1000 | Loss: 0.00001081
Iteration 178/1000 | Loss: 0.00001080
Iteration 179/1000 | Loss: 0.00001080
Iteration 180/1000 | Loss: 0.00001080
Iteration 181/1000 | Loss: 0.00001080
Iteration 182/1000 | Loss: 0.00001080
Iteration 183/1000 | Loss: 0.00001080
Iteration 184/1000 | Loss: 0.00001080
Iteration 185/1000 | Loss: 0.00001080
Iteration 186/1000 | Loss: 0.00001080
Iteration 187/1000 | Loss: 0.00001080
Iteration 188/1000 | Loss: 0.00001080
Iteration 189/1000 | Loss: 0.00001080
Iteration 190/1000 | Loss: 0.00001079
Iteration 191/1000 | Loss: 0.00001079
Iteration 192/1000 | Loss: 0.00001079
Iteration 193/1000 | Loss: 0.00001079
Iteration 194/1000 | Loss: 0.00001079
Iteration 195/1000 | Loss: 0.00001079
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001079
Iteration 199/1000 | Loss: 0.00001079
Iteration 200/1000 | Loss: 0.00001079
Iteration 201/1000 | Loss: 0.00001079
Iteration 202/1000 | Loss: 0.00001079
Iteration 203/1000 | Loss: 0.00001079
Iteration 204/1000 | Loss: 0.00001079
Iteration 205/1000 | Loss: 0.00001079
Iteration 206/1000 | Loss: 0.00001078
Iteration 207/1000 | Loss: 0.00001078
Iteration 208/1000 | Loss: 0.00001078
Iteration 209/1000 | Loss: 0.00001078
Iteration 210/1000 | Loss: 0.00001078
Iteration 211/1000 | Loss: 0.00001078
Iteration 212/1000 | Loss: 0.00001078
Iteration 213/1000 | Loss: 0.00001078
Iteration 214/1000 | Loss: 0.00001077
Iteration 215/1000 | Loss: 0.00001077
Iteration 216/1000 | Loss: 0.00001077
Iteration 217/1000 | Loss: 0.00001077
Iteration 218/1000 | Loss: 0.00001077
Iteration 219/1000 | Loss: 0.00001077
Iteration 220/1000 | Loss: 0.00001077
Iteration 221/1000 | Loss: 0.00001076
Iteration 222/1000 | Loss: 0.00001076
Iteration 223/1000 | Loss: 0.00001076
Iteration 224/1000 | Loss: 0.00001076
Iteration 225/1000 | Loss: 0.00001076
Iteration 226/1000 | Loss: 0.00001076
Iteration 227/1000 | Loss: 0.00001076
Iteration 228/1000 | Loss: 0.00001076
Iteration 229/1000 | Loss: 0.00001076
Iteration 230/1000 | Loss: 0.00001076
Iteration 231/1000 | Loss: 0.00001076
Iteration 232/1000 | Loss: 0.00001076
Iteration 233/1000 | Loss: 0.00001076
Iteration 234/1000 | Loss: 0.00001076
Iteration 235/1000 | Loss: 0.00001076
Iteration 236/1000 | Loss: 0.00001076
Iteration 237/1000 | Loss: 0.00001076
Iteration 238/1000 | Loss: 0.00001076
Iteration 239/1000 | Loss: 0.00001076
Iteration 240/1000 | Loss: 0.00001076
Iteration 241/1000 | Loss: 0.00001075
Iteration 242/1000 | Loss: 0.00001075
Iteration 243/1000 | Loss: 0.00001075
Iteration 244/1000 | Loss: 0.00001075
Iteration 245/1000 | Loss: 0.00001075
Iteration 246/1000 | Loss: 0.00001075
Iteration 247/1000 | Loss: 0.00001075
Iteration 248/1000 | Loss: 0.00001075
Iteration 249/1000 | Loss: 0.00001075
Iteration 250/1000 | Loss: 0.00001075
Iteration 251/1000 | Loss: 0.00001075
Iteration 252/1000 | Loss: 0.00001075
Iteration 253/1000 | Loss: 0.00001075
Iteration 254/1000 | Loss: 0.00001074
Iteration 255/1000 | Loss: 0.00001074
Iteration 256/1000 | Loss: 0.00001074
Iteration 257/1000 | Loss: 0.00001074
Iteration 258/1000 | Loss: 0.00001074
Iteration 259/1000 | Loss: 0.00001074
Iteration 260/1000 | Loss: 0.00001074
Iteration 261/1000 | Loss: 0.00001074
Iteration 262/1000 | Loss: 0.00001074
Iteration 263/1000 | Loss: 0.00001074
Iteration 264/1000 | Loss: 0.00001074
Iteration 265/1000 | Loss: 0.00001074
Iteration 266/1000 | Loss: 0.00001074
Iteration 267/1000 | Loss: 0.00001074
Iteration 268/1000 | Loss: 0.00001074
Iteration 269/1000 | Loss: 0.00001073
Iteration 270/1000 | Loss: 0.00001073
Iteration 271/1000 | Loss: 0.00001073
Iteration 272/1000 | Loss: 0.00001073
Iteration 273/1000 | Loss: 0.00001073
Iteration 274/1000 | Loss: 0.00001073
Iteration 275/1000 | Loss: 0.00001073
Iteration 276/1000 | Loss: 0.00001072
Iteration 277/1000 | Loss: 0.00001072
Iteration 278/1000 | Loss: 0.00001072
Iteration 279/1000 | Loss: 0.00001072
Iteration 280/1000 | Loss: 0.00001072
Iteration 281/1000 | Loss: 0.00001072
Iteration 282/1000 | Loss: 0.00001072
Iteration 283/1000 | Loss: 0.00001071
Iteration 284/1000 | Loss: 0.00001071
Iteration 285/1000 | Loss: 0.00001071
Iteration 286/1000 | Loss: 0.00001071
Iteration 287/1000 | Loss: 0.00001071
Iteration 288/1000 | Loss: 0.00001071
Iteration 289/1000 | Loss: 0.00001071
Iteration 290/1000 | Loss: 0.00001071
Iteration 291/1000 | Loss: 0.00001071
Iteration 292/1000 | Loss: 0.00001071
Iteration 293/1000 | Loss: 0.00001071
Iteration 294/1000 | Loss: 0.00001071
Iteration 295/1000 | Loss: 0.00001071
Iteration 296/1000 | Loss: 0.00001071
Iteration 297/1000 | Loss: 0.00001071
Iteration 298/1000 | Loss: 0.00001070
Iteration 299/1000 | Loss: 0.00001070
Iteration 300/1000 | Loss: 0.00001070
Iteration 301/1000 | Loss: 0.00001070
Iteration 302/1000 | Loss: 0.00001070
Iteration 303/1000 | Loss: 0.00001070
Iteration 304/1000 | Loss: 0.00001070
Iteration 305/1000 | Loss: 0.00001070
Iteration 306/1000 | Loss: 0.00001070
Iteration 307/1000 | Loss: 0.00001070
Iteration 308/1000 | Loss: 0.00001070
Iteration 309/1000 | Loss: 0.00001070
Iteration 310/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [1.0703540283429902e-05, 1.0703540283429902e-05, 1.0703540283429902e-05, 1.0703540283429902e-05, 1.0703540283429902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0703540283429902e-05

Optimization complete. Final v2v error: 2.8090245723724365 mm

Highest mean error: 3.1323459148406982 mm for frame 87

Lowest mean error: 2.6774954795837402 mm for frame 1

Saving results

Total time: 45.735310792922974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809005
Iteration 2/25 | Loss: 0.00168064
Iteration 3/25 | Loss: 0.00137964
Iteration 4/25 | Loss: 0.00135382
Iteration 5/25 | Loss: 0.00134937
Iteration 6/25 | Loss: 0.00134823
Iteration 7/25 | Loss: 0.00134823
Iteration 8/25 | Loss: 0.00134823
Iteration 9/25 | Loss: 0.00134823
Iteration 10/25 | Loss: 0.00134823
Iteration 11/25 | Loss: 0.00134823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013482318026944995, 0.0013482318026944995, 0.0013482318026944995, 0.0013482318026944995, 0.0013482318026944995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013482318026944995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37533677
Iteration 2/25 | Loss: 0.00100481
Iteration 3/25 | Loss: 0.00100481
Iteration 4/25 | Loss: 0.00100481
Iteration 5/25 | Loss: 0.00100481
Iteration 6/25 | Loss: 0.00100480
Iteration 7/25 | Loss: 0.00100480
Iteration 8/25 | Loss: 0.00100480
Iteration 9/25 | Loss: 0.00100480
Iteration 10/25 | Loss: 0.00100480
Iteration 11/25 | Loss: 0.00100480
Iteration 12/25 | Loss: 0.00100480
Iteration 13/25 | Loss: 0.00100480
Iteration 14/25 | Loss: 0.00100480
Iteration 15/25 | Loss: 0.00100480
Iteration 16/25 | Loss: 0.00100480
Iteration 17/25 | Loss: 0.00100480
Iteration 18/25 | Loss: 0.00100480
Iteration 19/25 | Loss: 0.00100480
Iteration 20/25 | Loss: 0.00100480
Iteration 21/25 | Loss: 0.00100480
Iteration 22/25 | Loss: 0.00100480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010048034600913525, 0.0010048034600913525, 0.0010048034600913525, 0.0010048034600913525, 0.0010048034600913525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010048034600913525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100480
Iteration 2/1000 | Loss: 0.00005450
Iteration 3/1000 | Loss: 0.00003699
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002662
Iteration 6/1000 | Loss: 0.00002565
Iteration 7/1000 | Loss: 0.00002490
Iteration 8/1000 | Loss: 0.00002422
Iteration 9/1000 | Loss: 0.00002379
Iteration 10/1000 | Loss: 0.00002351
Iteration 11/1000 | Loss: 0.00002322
Iteration 12/1000 | Loss: 0.00002290
Iteration 13/1000 | Loss: 0.00002267
Iteration 14/1000 | Loss: 0.00002249
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002232
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002222
Iteration 19/1000 | Loss: 0.00002222
Iteration 20/1000 | Loss: 0.00002220
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002216
Iteration 23/1000 | Loss: 0.00002213
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00002207
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002205
Iteration 31/1000 | Loss: 0.00002204
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002202
Iteration 34/1000 | Loss: 0.00002202
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002200
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002195
Iteration 51/1000 | Loss: 0.00002194
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002191
Iteration 56/1000 | Loss: 0.00002191
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002191
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002189
Iteration 64/1000 | Loss: 0.00002189
Iteration 65/1000 | Loss: 0.00002189
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002188
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002185
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002185
Iteration 115/1000 | Loss: 0.00002185
Iteration 116/1000 | Loss: 0.00002185
Iteration 117/1000 | Loss: 0.00002185
Iteration 118/1000 | Loss: 0.00002185
Iteration 119/1000 | Loss: 0.00002185
Iteration 120/1000 | Loss: 0.00002185
Iteration 121/1000 | Loss: 0.00002185
Iteration 122/1000 | Loss: 0.00002185
Iteration 123/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.1853325961274095e-05, 2.1853325961274095e-05, 2.1853325961274095e-05, 2.1853325961274095e-05, 2.1853325961274095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1853325961274095e-05

Optimization complete. Final v2v error: 3.8339176177978516 mm

Highest mean error: 5.148780345916748 mm for frame 150

Lowest mean error: 2.994093418121338 mm for frame 8

Saving results

Total time: 39.50200080871582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865146
Iteration 2/25 | Loss: 0.00169948
Iteration 3/25 | Loss: 0.00129931
Iteration 4/25 | Loss: 0.00124902
Iteration 5/25 | Loss: 0.00123146
Iteration 6/25 | Loss: 0.00123365
Iteration 7/25 | Loss: 0.00122271
Iteration 8/25 | Loss: 0.00121914
Iteration 9/25 | Loss: 0.00121854
Iteration 10/25 | Loss: 0.00121928
Iteration 11/25 | Loss: 0.00121582
Iteration 12/25 | Loss: 0.00121823
Iteration 13/25 | Loss: 0.00121591
Iteration 14/25 | Loss: 0.00121552
Iteration 15/25 | Loss: 0.00121532
Iteration 16/25 | Loss: 0.00121531
Iteration 17/25 | Loss: 0.00121531
Iteration 18/25 | Loss: 0.00121530
Iteration 19/25 | Loss: 0.00121530
Iteration 20/25 | Loss: 0.00121530
Iteration 21/25 | Loss: 0.00121530
Iteration 22/25 | Loss: 0.00121530
Iteration 23/25 | Loss: 0.00121530
Iteration 24/25 | Loss: 0.00121530
Iteration 25/25 | Loss: 0.00121530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17027569
Iteration 2/25 | Loss: 0.00098087
Iteration 3/25 | Loss: 0.00098086
Iteration 4/25 | Loss: 0.00098086
Iteration 5/25 | Loss: 0.00098086
Iteration 6/25 | Loss: 0.00098086
Iteration 7/25 | Loss: 0.00098086
Iteration 8/25 | Loss: 0.00098086
Iteration 9/25 | Loss: 0.00098086
Iteration 10/25 | Loss: 0.00098086
Iteration 11/25 | Loss: 0.00098086
Iteration 12/25 | Loss: 0.00098086
Iteration 13/25 | Loss: 0.00098086
Iteration 14/25 | Loss: 0.00098086
Iteration 15/25 | Loss: 0.00098086
Iteration 16/25 | Loss: 0.00098086
Iteration 17/25 | Loss: 0.00098086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009808613685891032, 0.0009808613685891032, 0.0009808613685891032, 0.0009808613685891032, 0.0009808613685891032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009808613685891032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098086
Iteration 2/1000 | Loss: 0.00005403
Iteration 3/1000 | Loss: 0.00003919
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00012341
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00004844
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00004400
Iteration 24/1000 | Loss: 0.00007792
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001208
Iteration 37/1000 | Loss: 0.00001207
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001207
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001207
Iteration 46/1000 | Loss: 0.00001207
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001207
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001206
Iteration 55/1000 | Loss: 0.00001206
Iteration 56/1000 | Loss: 0.00001206
Iteration 57/1000 | Loss: 0.00001206
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001206
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00004143
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001195
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001194
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001193
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001193
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001190
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001189
Iteration 152/1000 | Loss: 0.00001189
Iteration 153/1000 | Loss: 0.00001189
Iteration 154/1000 | Loss: 0.00001189
Iteration 155/1000 | Loss: 0.00001189
Iteration 156/1000 | Loss: 0.00001188
Iteration 157/1000 | Loss: 0.00001188
Iteration 158/1000 | Loss: 0.00001188
Iteration 159/1000 | Loss: 0.00001188
Iteration 160/1000 | Loss: 0.00001188
Iteration 161/1000 | Loss: 0.00001188
Iteration 162/1000 | Loss: 0.00001188
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001187
Iteration 170/1000 | Loss: 0.00001187
Iteration 171/1000 | Loss: 0.00001187
Iteration 172/1000 | Loss: 0.00001187
Iteration 173/1000 | Loss: 0.00001187
Iteration 174/1000 | Loss: 0.00001187
Iteration 175/1000 | Loss: 0.00001186
Iteration 176/1000 | Loss: 0.00001186
Iteration 177/1000 | Loss: 0.00001186
Iteration 178/1000 | Loss: 0.00001186
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001185
Iteration 185/1000 | Loss: 0.00001185
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001185
Iteration 192/1000 | Loss: 0.00001184
Iteration 193/1000 | Loss: 0.00001184
Iteration 194/1000 | Loss: 0.00001184
Iteration 195/1000 | Loss: 0.00001184
Iteration 196/1000 | Loss: 0.00001184
Iteration 197/1000 | Loss: 0.00001184
Iteration 198/1000 | Loss: 0.00001184
Iteration 199/1000 | Loss: 0.00001184
Iteration 200/1000 | Loss: 0.00001184
Iteration 201/1000 | Loss: 0.00001184
Iteration 202/1000 | Loss: 0.00001184
Iteration 203/1000 | Loss: 0.00001184
Iteration 204/1000 | Loss: 0.00001183
Iteration 205/1000 | Loss: 0.00001183
Iteration 206/1000 | Loss: 0.00001183
Iteration 207/1000 | Loss: 0.00001183
Iteration 208/1000 | Loss: 0.00001183
Iteration 209/1000 | Loss: 0.00001183
Iteration 210/1000 | Loss: 0.00001183
Iteration 211/1000 | Loss: 0.00001183
Iteration 212/1000 | Loss: 0.00001183
Iteration 213/1000 | Loss: 0.00001183
Iteration 214/1000 | Loss: 0.00001183
Iteration 215/1000 | Loss: 0.00001183
Iteration 216/1000 | Loss: 0.00001183
Iteration 217/1000 | Loss: 0.00001183
Iteration 218/1000 | Loss: 0.00001183
Iteration 219/1000 | Loss: 0.00001183
Iteration 220/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.182916275865864e-05, 1.182916275865864e-05, 1.182916275865864e-05, 1.182916275865864e-05, 1.182916275865864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.182916275865864e-05

Optimization complete. Final v2v error: 2.9412713050842285 mm

Highest mean error: 3.2214090824127197 mm for frame 48

Lowest mean error: 2.7000393867492676 mm for frame 227

Saving results

Total time: 73.96994757652283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398523
Iteration 2/25 | Loss: 0.00129366
Iteration 3/25 | Loss: 0.00123182
Iteration 4/25 | Loss: 0.00122436
Iteration 5/25 | Loss: 0.00122247
Iteration 6/25 | Loss: 0.00122247
Iteration 7/25 | Loss: 0.00122247
Iteration 8/25 | Loss: 0.00122247
Iteration 9/25 | Loss: 0.00122247
Iteration 10/25 | Loss: 0.00122247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012224692618474364, 0.0012224692618474364, 0.0012224692618474364, 0.0012224692618474364, 0.0012224692618474364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012224692618474364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35694969
Iteration 2/25 | Loss: 0.00101325
Iteration 3/25 | Loss: 0.00101325
Iteration 4/25 | Loss: 0.00101325
Iteration 5/25 | Loss: 0.00101325
Iteration 6/25 | Loss: 0.00101325
Iteration 7/25 | Loss: 0.00101325
Iteration 8/25 | Loss: 0.00101324
Iteration 9/25 | Loss: 0.00101324
Iteration 10/25 | Loss: 0.00101324
Iteration 11/25 | Loss: 0.00101324
Iteration 12/25 | Loss: 0.00101324
Iteration 13/25 | Loss: 0.00101324
Iteration 14/25 | Loss: 0.00101324
Iteration 15/25 | Loss: 0.00101324
Iteration 16/25 | Loss: 0.00101324
Iteration 17/25 | Loss: 0.00101324
Iteration 18/25 | Loss: 0.00101324
Iteration 19/25 | Loss: 0.00101324
Iteration 20/25 | Loss: 0.00101324
Iteration 21/25 | Loss: 0.00101324
Iteration 22/25 | Loss: 0.00101324
Iteration 23/25 | Loss: 0.00101324
Iteration 24/25 | Loss: 0.00101324
Iteration 25/25 | Loss: 0.00101324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101324
Iteration 2/1000 | Loss: 0.00001947
Iteration 3/1000 | Loss: 0.00001471
Iteration 4/1000 | Loss: 0.00001361
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001263
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001119
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001108
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001090
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001089
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001080
Iteration 40/1000 | Loss: 0.00001080
Iteration 41/1000 | Loss: 0.00001080
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00001080
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001070
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001066
Iteration 83/1000 | Loss: 0.00001066
Iteration 84/1000 | Loss: 0.00001066
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001057
Iteration 101/1000 | Loss: 0.00001057
Iteration 102/1000 | Loss: 0.00001057
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001055
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001054
Iteration 117/1000 | Loss: 0.00001054
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001053
Iteration 132/1000 | Loss: 0.00001053
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001053
Iteration 136/1000 | Loss: 0.00001053
Iteration 137/1000 | Loss: 0.00001053
Iteration 138/1000 | Loss: 0.00001053
Iteration 139/1000 | Loss: 0.00001053
Iteration 140/1000 | Loss: 0.00001053
Iteration 141/1000 | Loss: 0.00001053
Iteration 142/1000 | Loss: 0.00001053
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001052
Iteration 145/1000 | Loss: 0.00001052
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001052
Iteration 148/1000 | Loss: 0.00001052
Iteration 149/1000 | Loss: 0.00001052
Iteration 150/1000 | Loss: 0.00001051
Iteration 151/1000 | Loss: 0.00001051
Iteration 152/1000 | Loss: 0.00001051
Iteration 153/1000 | Loss: 0.00001051
Iteration 154/1000 | Loss: 0.00001051
Iteration 155/1000 | Loss: 0.00001051
Iteration 156/1000 | Loss: 0.00001051
Iteration 157/1000 | Loss: 0.00001051
Iteration 158/1000 | Loss: 0.00001051
Iteration 159/1000 | Loss: 0.00001051
Iteration 160/1000 | Loss: 0.00001051
Iteration 161/1000 | Loss: 0.00001051
Iteration 162/1000 | Loss: 0.00001051
Iteration 163/1000 | Loss: 0.00001051
Iteration 164/1000 | Loss: 0.00001050
Iteration 165/1000 | Loss: 0.00001050
Iteration 166/1000 | Loss: 0.00001050
Iteration 167/1000 | Loss: 0.00001050
Iteration 168/1000 | Loss: 0.00001050
Iteration 169/1000 | Loss: 0.00001050
Iteration 170/1000 | Loss: 0.00001050
Iteration 171/1000 | Loss: 0.00001050
Iteration 172/1000 | Loss: 0.00001050
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001050
Iteration 177/1000 | Loss: 0.00001050
Iteration 178/1000 | Loss: 0.00001050
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001049
Iteration 182/1000 | Loss: 0.00001049
Iteration 183/1000 | Loss: 0.00001049
Iteration 184/1000 | Loss: 0.00001049
Iteration 185/1000 | Loss: 0.00001049
Iteration 186/1000 | Loss: 0.00001049
Iteration 187/1000 | Loss: 0.00001049
Iteration 188/1000 | Loss: 0.00001049
Iteration 189/1000 | Loss: 0.00001049
Iteration 190/1000 | Loss: 0.00001049
Iteration 191/1000 | Loss: 0.00001049
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001048
Iteration 196/1000 | Loss: 0.00001048
Iteration 197/1000 | Loss: 0.00001048
Iteration 198/1000 | Loss: 0.00001048
Iteration 199/1000 | Loss: 0.00001048
Iteration 200/1000 | Loss: 0.00001048
Iteration 201/1000 | Loss: 0.00001048
Iteration 202/1000 | Loss: 0.00001048
Iteration 203/1000 | Loss: 0.00001048
Iteration 204/1000 | Loss: 0.00001048
Iteration 205/1000 | Loss: 0.00001048
Iteration 206/1000 | Loss: 0.00001048
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001048
Iteration 210/1000 | Loss: 0.00001048
Iteration 211/1000 | Loss: 0.00001048
Iteration 212/1000 | Loss: 0.00001048
Iteration 213/1000 | Loss: 0.00001048
Iteration 214/1000 | Loss: 0.00001048
Iteration 215/1000 | Loss: 0.00001048
Iteration 216/1000 | Loss: 0.00001048
Iteration 217/1000 | Loss: 0.00001048
Iteration 218/1000 | Loss: 0.00001047
Iteration 219/1000 | Loss: 0.00001047
Iteration 220/1000 | Loss: 0.00001047
Iteration 221/1000 | Loss: 0.00001047
Iteration 222/1000 | Loss: 0.00001047
Iteration 223/1000 | Loss: 0.00001047
Iteration 224/1000 | Loss: 0.00001047
Iteration 225/1000 | Loss: 0.00001047
Iteration 226/1000 | Loss: 0.00001047
Iteration 227/1000 | Loss: 0.00001047
Iteration 228/1000 | Loss: 0.00001047
Iteration 229/1000 | Loss: 0.00001047
Iteration 230/1000 | Loss: 0.00001047
Iteration 231/1000 | Loss: 0.00001047
Iteration 232/1000 | Loss: 0.00001047
Iteration 233/1000 | Loss: 0.00001047
Iteration 234/1000 | Loss: 0.00001047
Iteration 235/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0470817869645543e-05, 1.0470817869645543e-05, 1.0470817869645543e-05, 1.0470817869645543e-05, 1.0470817869645543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0470817869645543e-05

Optimization complete. Final v2v error: 2.8161356449127197 mm

Highest mean error: 3.0254855155944824 mm for frame 128

Lowest mean error: 2.7002458572387695 mm for frame 228

Saving results

Total time: 46.93513226509094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431537
Iteration 2/25 | Loss: 0.00132785
Iteration 3/25 | Loss: 0.00124730
Iteration 4/25 | Loss: 0.00123497
Iteration 5/25 | Loss: 0.00123128
Iteration 6/25 | Loss: 0.00123114
Iteration 7/25 | Loss: 0.00123114
Iteration 8/25 | Loss: 0.00123114
Iteration 9/25 | Loss: 0.00123114
Iteration 10/25 | Loss: 0.00123114
Iteration 11/25 | Loss: 0.00123114
Iteration 12/25 | Loss: 0.00123114
Iteration 13/25 | Loss: 0.00123114
Iteration 14/25 | Loss: 0.00123114
Iteration 15/25 | Loss: 0.00123114
Iteration 16/25 | Loss: 0.00123114
Iteration 17/25 | Loss: 0.00123114
Iteration 18/25 | Loss: 0.00123114
Iteration 19/25 | Loss: 0.00123114
Iteration 20/25 | Loss: 0.00123114
Iteration 21/25 | Loss: 0.00123114
Iteration 22/25 | Loss: 0.00123114
Iteration 23/25 | Loss: 0.00123114
Iteration 24/25 | Loss: 0.00123114
Iteration 25/25 | Loss: 0.00123114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32871246
Iteration 2/25 | Loss: 0.00124908
Iteration 3/25 | Loss: 0.00124908
Iteration 4/25 | Loss: 0.00124908
Iteration 5/25 | Loss: 0.00124908
Iteration 6/25 | Loss: 0.00124908
Iteration 7/25 | Loss: 0.00124908
Iteration 8/25 | Loss: 0.00124908
Iteration 9/25 | Loss: 0.00124908
Iteration 10/25 | Loss: 0.00124908
Iteration 11/25 | Loss: 0.00124908
Iteration 12/25 | Loss: 0.00124908
Iteration 13/25 | Loss: 0.00124908
Iteration 14/25 | Loss: 0.00124908
Iteration 15/25 | Loss: 0.00124908
Iteration 16/25 | Loss: 0.00124908
Iteration 17/25 | Loss: 0.00124908
Iteration 18/25 | Loss: 0.00124908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012490759836509824, 0.0012490759836509824, 0.0012490759836509824, 0.0012490759836509824, 0.0012490759836509824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012490759836509824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124908
Iteration 2/1000 | Loss: 0.00003529
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001475
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001357
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001283
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001282
Iteration 54/1000 | Loss: 0.00001282
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001273
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001270
Iteration 91/1000 | Loss: 0.00001270
Iteration 92/1000 | Loss: 0.00001270
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001258
Iteration 138/1000 | Loss: 0.00001258
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001258
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001256
Iteration 147/1000 | Loss: 0.00001256
Iteration 148/1000 | Loss: 0.00001256
Iteration 149/1000 | Loss: 0.00001256
Iteration 150/1000 | Loss: 0.00001256
Iteration 151/1000 | Loss: 0.00001256
Iteration 152/1000 | Loss: 0.00001256
Iteration 153/1000 | Loss: 0.00001256
Iteration 154/1000 | Loss: 0.00001256
Iteration 155/1000 | Loss: 0.00001256
Iteration 156/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2562582924147137e-05, 1.2562582924147137e-05, 1.2562582924147137e-05, 1.2562582924147137e-05, 1.2562582924147137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2562582924147137e-05

Optimization complete. Final v2v error: 3.002504348754883 mm

Highest mean error: 3.525904655456543 mm for frame 60

Lowest mean error: 2.6758060455322266 mm for frame 41

Saving results

Total time: 43.94379448890686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497057
Iteration 2/25 | Loss: 0.00130914
Iteration 3/25 | Loss: 0.00124136
Iteration 4/25 | Loss: 0.00122998
Iteration 5/25 | Loss: 0.00122585
Iteration 6/25 | Loss: 0.00122515
Iteration 7/25 | Loss: 0.00122515
Iteration 8/25 | Loss: 0.00122515
Iteration 9/25 | Loss: 0.00122515
Iteration 10/25 | Loss: 0.00122515
Iteration 11/25 | Loss: 0.00122515
Iteration 12/25 | Loss: 0.00122515
Iteration 13/25 | Loss: 0.00122515
Iteration 14/25 | Loss: 0.00122515
Iteration 15/25 | Loss: 0.00122515
Iteration 16/25 | Loss: 0.00122515
Iteration 17/25 | Loss: 0.00122515
Iteration 18/25 | Loss: 0.00122515
Iteration 19/25 | Loss: 0.00122515
Iteration 20/25 | Loss: 0.00122515
Iteration 21/25 | Loss: 0.00122515
Iteration 22/25 | Loss: 0.00122515
Iteration 23/25 | Loss: 0.00122515
Iteration 24/25 | Loss: 0.00122515
Iteration 25/25 | Loss: 0.00122515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.51549387
Iteration 2/25 | Loss: 0.00104352
Iteration 3/25 | Loss: 0.00104349
Iteration 4/25 | Loss: 0.00104348
Iteration 5/25 | Loss: 0.00104348
Iteration 6/25 | Loss: 0.00104348
Iteration 7/25 | Loss: 0.00104348
Iteration 8/25 | Loss: 0.00104348
Iteration 9/25 | Loss: 0.00104348
Iteration 10/25 | Loss: 0.00104348
Iteration 11/25 | Loss: 0.00104348
Iteration 12/25 | Loss: 0.00104348
Iteration 13/25 | Loss: 0.00104348
Iteration 14/25 | Loss: 0.00104348
Iteration 15/25 | Loss: 0.00104348
Iteration 16/25 | Loss: 0.00104348
Iteration 17/25 | Loss: 0.00104348
Iteration 18/25 | Loss: 0.00104348
Iteration 19/25 | Loss: 0.00104348
Iteration 20/25 | Loss: 0.00104348
Iteration 21/25 | Loss: 0.00104348
Iteration 22/25 | Loss: 0.00104348
Iteration 23/25 | Loss: 0.00104348
Iteration 24/25 | Loss: 0.00104348
Iteration 25/25 | Loss: 0.00104348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104348
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001661
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001309
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001272
Iteration 107/1000 | Loss: 0.00001272
Iteration 108/1000 | Loss: 0.00001272
Iteration 109/1000 | Loss: 0.00001272
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001271
Iteration 112/1000 | Loss: 0.00001271
Iteration 113/1000 | Loss: 0.00001271
Iteration 114/1000 | Loss: 0.00001271
Iteration 115/1000 | Loss: 0.00001271
Iteration 116/1000 | Loss: 0.00001271
Iteration 117/1000 | Loss: 0.00001271
Iteration 118/1000 | Loss: 0.00001271
Iteration 119/1000 | Loss: 0.00001271
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001270
Iteration 122/1000 | Loss: 0.00001270
Iteration 123/1000 | Loss: 0.00001270
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001269
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001268
Iteration 135/1000 | Loss: 0.00001268
Iteration 136/1000 | Loss: 0.00001268
Iteration 137/1000 | Loss: 0.00001268
Iteration 138/1000 | Loss: 0.00001268
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001268
Iteration 144/1000 | Loss: 0.00001268
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001267
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001266
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001264
Iteration 174/1000 | Loss: 0.00001264
Iteration 175/1000 | Loss: 0.00001264
Iteration 176/1000 | Loss: 0.00001264
Iteration 177/1000 | Loss: 0.00001264
Iteration 178/1000 | Loss: 0.00001264
Iteration 179/1000 | Loss: 0.00001264
Iteration 180/1000 | Loss: 0.00001264
Iteration 181/1000 | Loss: 0.00001264
Iteration 182/1000 | Loss: 0.00001264
Iteration 183/1000 | Loss: 0.00001264
Iteration 184/1000 | Loss: 0.00001264
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001263
Iteration 187/1000 | Loss: 0.00001263
Iteration 188/1000 | Loss: 0.00001263
Iteration 189/1000 | Loss: 0.00001263
Iteration 190/1000 | Loss: 0.00001263
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001263
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001263
Iteration 195/1000 | Loss: 0.00001263
Iteration 196/1000 | Loss: 0.00001262
Iteration 197/1000 | Loss: 0.00001262
Iteration 198/1000 | Loss: 0.00001262
Iteration 199/1000 | Loss: 0.00001262
Iteration 200/1000 | Loss: 0.00001262
Iteration 201/1000 | Loss: 0.00001262
Iteration 202/1000 | Loss: 0.00001262
Iteration 203/1000 | Loss: 0.00001262
Iteration 204/1000 | Loss: 0.00001262
Iteration 205/1000 | Loss: 0.00001262
Iteration 206/1000 | Loss: 0.00001262
Iteration 207/1000 | Loss: 0.00001261
Iteration 208/1000 | Loss: 0.00001261
Iteration 209/1000 | Loss: 0.00001261
Iteration 210/1000 | Loss: 0.00001261
Iteration 211/1000 | Loss: 0.00001261
Iteration 212/1000 | Loss: 0.00001261
Iteration 213/1000 | Loss: 0.00001261
Iteration 214/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.261455599887995e-05, 1.261455599887995e-05, 1.261455599887995e-05, 1.261455599887995e-05, 1.261455599887995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.261455599887995e-05

Optimization complete. Final v2v error: 3.0244336128234863 mm

Highest mean error: 3.529008150100708 mm for frame 119

Lowest mean error: 2.587836265563965 mm for frame 4

Saving results

Total time: 43.609803915023804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825567
Iteration 2/25 | Loss: 0.00146447
Iteration 3/25 | Loss: 0.00135106
Iteration 4/25 | Loss: 0.00133446
Iteration 5/25 | Loss: 0.00132939
Iteration 6/25 | Loss: 0.00132859
Iteration 7/25 | Loss: 0.00132859
Iteration 8/25 | Loss: 0.00132859
Iteration 9/25 | Loss: 0.00132859
Iteration 10/25 | Loss: 0.00132859
Iteration 11/25 | Loss: 0.00132859
Iteration 12/25 | Loss: 0.00132859
Iteration 13/25 | Loss: 0.00132859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001328588929027319, 0.001328588929027319, 0.001328588929027319, 0.001328588929027319, 0.001328588929027319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328588929027319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30251801
Iteration 2/25 | Loss: 0.00090798
Iteration 3/25 | Loss: 0.00090790
Iteration 4/25 | Loss: 0.00090790
Iteration 5/25 | Loss: 0.00090790
Iteration 6/25 | Loss: 0.00090790
Iteration 7/25 | Loss: 0.00090790
Iteration 8/25 | Loss: 0.00090790
Iteration 9/25 | Loss: 0.00090790
Iteration 10/25 | Loss: 0.00090790
Iteration 11/25 | Loss: 0.00090790
Iteration 12/25 | Loss: 0.00090790
Iteration 13/25 | Loss: 0.00090790
Iteration 14/25 | Loss: 0.00090790
Iteration 15/25 | Loss: 0.00090790
Iteration 16/25 | Loss: 0.00090790
Iteration 17/25 | Loss: 0.00090790
Iteration 18/25 | Loss: 0.00090790
Iteration 19/25 | Loss: 0.00090790
Iteration 20/25 | Loss: 0.00090790
Iteration 21/25 | Loss: 0.00090790
Iteration 22/25 | Loss: 0.00090790
Iteration 23/25 | Loss: 0.00090790
Iteration 24/25 | Loss: 0.00090790
Iteration 25/25 | Loss: 0.00090790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090790
Iteration 2/1000 | Loss: 0.00005145
Iteration 3/1000 | Loss: 0.00003731
Iteration 4/1000 | Loss: 0.00003401
Iteration 5/1000 | Loss: 0.00003284
Iteration 6/1000 | Loss: 0.00003173
Iteration 7/1000 | Loss: 0.00003078
Iteration 8/1000 | Loss: 0.00003024
Iteration 9/1000 | Loss: 0.00002968
Iteration 10/1000 | Loss: 0.00002933
Iteration 11/1000 | Loss: 0.00002905
Iteration 12/1000 | Loss: 0.00002883
Iteration 13/1000 | Loss: 0.00002877
Iteration 14/1000 | Loss: 0.00002865
Iteration 15/1000 | Loss: 0.00002864
Iteration 16/1000 | Loss: 0.00002863
Iteration 17/1000 | Loss: 0.00002863
Iteration 18/1000 | Loss: 0.00002862
Iteration 19/1000 | Loss: 0.00002862
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002844
Iteration 22/1000 | Loss: 0.00002838
Iteration 23/1000 | Loss: 0.00002835
Iteration 24/1000 | Loss: 0.00002835
Iteration 25/1000 | Loss: 0.00002835
Iteration 26/1000 | Loss: 0.00002835
Iteration 27/1000 | Loss: 0.00002830
Iteration 28/1000 | Loss: 0.00002829
Iteration 29/1000 | Loss: 0.00002826
Iteration 30/1000 | Loss: 0.00002826
Iteration 31/1000 | Loss: 0.00002825
Iteration 32/1000 | Loss: 0.00002823
Iteration 33/1000 | Loss: 0.00002820
Iteration 34/1000 | Loss: 0.00002819
Iteration 35/1000 | Loss: 0.00002819
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002818
Iteration 38/1000 | Loss: 0.00002818
Iteration 39/1000 | Loss: 0.00002817
Iteration 40/1000 | Loss: 0.00002817
Iteration 41/1000 | Loss: 0.00002817
Iteration 42/1000 | Loss: 0.00002817
Iteration 43/1000 | Loss: 0.00002816
Iteration 44/1000 | Loss: 0.00002816
Iteration 45/1000 | Loss: 0.00002816
Iteration 46/1000 | Loss: 0.00002816
Iteration 47/1000 | Loss: 0.00002815
Iteration 48/1000 | Loss: 0.00002815
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002814
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002814
Iteration 54/1000 | Loss: 0.00002814
Iteration 55/1000 | Loss: 0.00002814
Iteration 56/1000 | Loss: 0.00002814
Iteration 57/1000 | Loss: 0.00002814
Iteration 58/1000 | Loss: 0.00002814
Iteration 59/1000 | Loss: 0.00002814
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002813
Iteration 62/1000 | Loss: 0.00002812
Iteration 63/1000 | Loss: 0.00002811
Iteration 64/1000 | Loss: 0.00002811
Iteration 65/1000 | Loss: 0.00002811
Iteration 66/1000 | Loss: 0.00002811
Iteration 67/1000 | Loss: 0.00002811
Iteration 68/1000 | Loss: 0.00002811
Iteration 69/1000 | Loss: 0.00002811
Iteration 70/1000 | Loss: 0.00002811
Iteration 71/1000 | Loss: 0.00002810
Iteration 72/1000 | Loss: 0.00002810
Iteration 73/1000 | Loss: 0.00002809
Iteration 74/1000 | Loss: 0.00002809
Iteration 75/1000 | Loss: 0.00002809
Iteration 76/1000 | Loss: 0.00002809
Iteration 77/1000 | Loss: 0.00002809
Iteration 78/1000 | Loss: 0.00002808
Iteration 79/1000 | Loss: 0.00002808
Iteration 80/1000 | Loss: 0.00002808
Iteration 81/1000 | Loss: 0.00002808
Iteration 82/1000 | Loss: 0.00002807
Iteration 83/1000 | Loss: 0.00002807
Iteration 84/1000 | Loss: 0.00002807
Iteration 85/1000 | Loss: 0.00002807
Iteration 86/1000 | Loss: 0.00002807
Iteration 87/1000 | Loss: 0.00002807
Iteration 88/1000 | Loss: 0.00002807
Iteration 89/1000 | Loss: 0.00002807
Iteration 90/1000 | Loss: 0.00002807
Iteration 91/1000 | Loss: 0.00002807
Iteration 92/1000 | Loss: 0.00002806
Iteration 93/1000 | Loss: 0.00002806
Iteration 94/1000 | Loss: 0.00002806
Iteration 95/1000 | Loss: 0.00002806
Iteration 96/1000 | Loss: 0.00002806
Iteration 97/1000 | Loss: 0.00002806
Iteration 98/1000 | Loss: 0.00002806
Iteration 99/1000 | Loss: 0.00002806
Iteration 100/1000 | Loss: 0.00002806
Iteration 101/1000 | Loss: 0.00002806
Iteration 102/1000 | Loss: 0.00002806
Iteration 103/1000 | Loss: 0.00002806
Iteration 104/1000 | Loss: 0.00002806
Iteration 105/1000 | Loss: 0.00002806
Iteration 106/1000 | Loss: 0.00002805
Iteration 107/1000 | Loss: 0.00002805
Iteration 108/1000 | Loss: 0.00002805
Iteration 109/1000 | Loss: 0.00002805
Iteration 110/1000 | Loss: 0.00002805
Iteration 111/1000 | Loss: 0.00002805
Iteration 112/1000 | Loss: 0.00002805
Iteration 113/1000 | Loss: 0.00002805
Iteration 114/1000 | Loss: 0.00002805
Iteration 115/1000 | Loss: 0.00002805
Iteration 116/1000 | Loss: 0.00002804
Iteration 117/1000 | Loss: 0.00002804
Iteration 118/1000 | Loss: 0.00002804
Iteration 119/1000 | Loss: 0.00002804
Iteration 120/1000 | Loss: 0.00002803
Iteration 121/1000 | Loss: 0.00002803
Iteration 122/1000 | Loss: 0.00002803
Iteration 123/1000 | Loss: 0.00002803
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00002803
Iteration 126/1000 | Loss: 0.00002803
Iteration 127/1000 | Loss: 0.00002803
Iteration 128/1000 | Loss: 0.00002803
Iteration 129/1000 | Loss: 0.00002803
Iteration 130/1000 | Loss: 0.00002803
Iteration 131/1000 | Loss: 0.00002802
Iteration 132/1000 | Loss: 0.00002802
Iteration 133/1000 | Loss: 0.00002802
Iteration 134/1000 | Loss: 0.00002802
Iteration 135/1000 | Loss: 0.00002802
Iteration 136/1000 | Loss: 0.00002802
Iteration 137/1000 | Loss: 0.00002802
Iteration 138/1000 | Loss: 0.00002802
Iteration 139/1000 | Loss: 0.00002802
Iteration 140/1000 | Loss: 0.00002802
Iteration 141/1000 | Loss: 0.00002802
Iteration 142/1000 | Loss: 0.00002802
Iteration 143/1000 | Loss: 0.00002802
Iteration 144/1000 | Loss: 0.00002802
Iteration 145/1000 | Loss: 0.00002801
Iteration 146/1000 | Loss: 0.00002801
Iteration 147/1000 | Loss: 0.00002801
Iteration 148/1000 | Loss: 0.00002801
Iteration 149/1000 | Loss: 0.00002801
Iteration 150/1000 | Loss: 0.00002801
Iteration 151/1000 | Loss: 0.00002801
Iteration 152/1000 | Loss: 0.00002801
Iteration 153/1000 | Loss: 0.00002800
Iteration 154/1000 | Loss: 0.00002800
Iteration 155/1000 | Loss: 0.00002800
Iteration 156/1000 | Loss: 0.00002800
Iteration 157/1000 | Loss: 0.00002800
Iteration 158/1000 | Loss: 0.00002800
Iteration 159/1000 | Loss: 0.00002800
Iteration 160/1000 | Loss: 0.00002800
Iteration 161/1000 | Loss: 0.00002800
Iteration 162/1000 | Loss: 0.00002800
Iteration 163/1000 | Loss: 0.00002800
Iteration 164/1000 | Loss: 0.00002800
Iteration 165/1000 | Loss: 0.00002800
Iteration 166/1000 | Loss: 0.00002800
Iteration 167/1000 | Loss: 0.00002799
Iteration 168/1000 | Loss: 0.00002799
Iteration 169/1000 | Loss: 0.00002799
Iteration 170/1000 | Loss: 0.00002799
Iteration 171/1000 | Loss: 0.00002799
Iteration 172/1000 | Loss: 0.00002798
Iteration 173/1000 | Loss: 0.00002798
Iteration 174/1000 | Loss: 0.00002798
Iteration 175/1000 | Loss: 0.00002798
Iteration 176/1000 | Loss: 0.00002798
Iteration 177/1000 | Loss: 0.00002798
Iteration 178/1000 | Loss: 0.00002798
Iteration 179/1000 | Loss: 0.00002798
Iteration 180/1000 | Loss: 0.00002798
Iteration 181/1000 | Loss: 0.00002798
Iteration 182/1000 | Loss: 0.00002798
Iteration 183/1000 | Loss: 0.00002798
Iteration 184/1000 | Loss: 0.00002797
Iteration 185/1000 | Loss: 0.00002797
Iteration 186/1000 | Loss: 0.00002797
Iteration 187/1000 | Loss: 0.00002797
Iteration 188/1000 | Loss: 0.00002797
Iteration 189/1000 | Loss: 0.00002797
Iteration 190/1000 | Loss: 0.00002797
Iteration 191/1000 | Loss: 0.00002797
Iteration 192/1000 | Loss: 0.00002797
Iteration 193/1000 | Loss: 0.00002796
Iteration 194/1000 | Loss: 0.00002796
Iteration 195/1000 | Loss: 0.00002796
Iteration 196/1000 | Loss: 0.00002796
Iteration 197/1000 | Loss: 0.00002796
Iteration 198/1000 | Loss: 0.00002796
Iteration 199/1000 | Loss: 0.00002796
Iteration 200/1000 | Loss: 0.00002796
Iteration 201/1000 | Loss: 0.00002796
Iteration 202/1000 | Loss: 0.00002796
Iteration 203/1000 | Loss: 0.00002796
Iteration 204/1000 | Loss: 0.00002796
Iteration 205/1000 | Loss: 0.00002796
Iteration 206/1000 | Loss: 0.00002796
Iteration 207/1000 | Loss: 0.00002796
Iteration 208/1000 | Loss: 0.00002796
Iteration 209/1000 | Loss: 0.00002796
Iteration 210/1000 | Loss: 0.00002796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.796056287479587e-05, 2.796056287479587e-05, 2.796056287479587e-05, 2.796056287479587e-05, 2.796056287479587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.796056287479587e-05

Optimization complete. Final v2v error: 4.384393692016602 mm

Highest mean error: 4.632870674133301 mm for frame 22

Lowest mean error: 4.0645365715026855 mm for frame 120

Saving results

Total time: 44.03820490837097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598937
Iteration 2/25 | Loss: 0.00134208
Iteration 3/25 | Loss: 0.00128548
Iteration 4/25 | Loss: 0.00128206
Iteration 5/25 | Loss: 0.00128096
Iteration 6/25 | Loss: 0.00128096
Iteration 7/25 | Loss: 0.00128096
Iteration 8/25 | Loss: 0.00128096
Iteration 9/25 | Loss: 0.00128096
Iteration 10/25 | Loss: 0.00128096
Iteration 11/25 | Loss: 0.00128096
Iteration 12/25 | Loss: 0.00128096
Iteration 13/25 | Loss: 0.00128096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001280962023884058, 0.001280962023884058, 0.001280962023884058, 0.001280962023884058, 0.001280962023884058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001280962023884058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.07773113
Iteration 2/25 | Loss: 0.00100275
Iteration 3/25 | Loss: 0.00100274
Iteration 4/25 | Loss: 0.00100274
Iteration 5/25 | Loss: 0.00100274
Iteration 6/25 | Loss: 0.00100274
Iteration 7/25 | Loss: 0.00100274
Iteration 8/25 | Loss: 0.00100274
Iteration 9/25 | Loss: 0.00100274
Iteration 10/25 | Loss: 0.00100274
Iteration 11/25 | Loss: 0.00100274
Iteration 12/25 | Loss: 0.00100274
Iteration 13/25 | Loss: 0.00100274
Iteration 14/25 | Loss: 0.00100274
Iteration 15/25 | Loss: 0.00100274
Iteration 16/25 | Loss: 0.00100274
Iteration 17/25 | Loss: 0.00100274
Iteration 18/25 | Loss: 0.00100274
Iteration 19/25 | Loss: 0.00100274
Iteration 20/25 | Loss: 0.00100274
Iteration 21/25 | Loss: 0.00100274
Iteration 22/25 | Loss: 0.00100274
Iteration 23/25 | Loss: 0.00100274
Iteration 24/25 | Loss: 0.00100274
Iteration 25/25 | Loss: 0.00100274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100274
Iteration 2/1000 | Loss: 0.00002516
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001440
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001294
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001275
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001250
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001202
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001197
Iteration 138/1000 | Loss: 0.00001197
Iteration 139/1000 | Loss: 0.00001197
Iteration 140/1000 | Loss: 0.00001197
Iteration 141/1000 | Loss: 0.00001197
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001196
Iteration 147/1000 | Loss: 0.00001196
Iteration 148/1000 | Loss: 0.00001196
Iteration 149/1000 | Loss: 0.00001196
Iteration 150/1000 | Loss: 0.00001196
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001195
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001195
Iteration 167/1000 | Loss: 0.00001194
Iteration 168/1000 | Loss: 0.00001194
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001193
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001193
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00001193
Iteration 195/1000 | Loss: 0.00001193
Iteration 196/1000 | Loss: 0.00001192
Iteration 197/1000 | Loss: 0.00001192
Iteration 198/1000 | Loss: 0.00001192
Iteration 199/1000 | Loss: 0.00001192
Iteration 200/1000 | Loss: 0.00001192
Iteration 201/1000 | Loss: 0.00001192
Iteration 202/1000 | Loss: 0.00001192
Iteration 203/1000 | Loss: 0.00001192
Iteration 204/1000 | Loss: 0.00001192
Iteration 205/1000 | Loss: 0.00001192
Iteration 206/1000 | Loss: 0.00001192
Iteration 207/1000 | Loss: 0.00001192
Iteration 208/1000 | Loss: 0.00001192
Iteration 209/1000 | Loss: 0.00001192
Iteration 210/1000 | Loss: 0.00001192
Iteration 211/1000 | Loss: 0.00001192
Iteration 212/1000 | Loss: 0.00001192
Iteration 213/1000 | Loss: 0.00001192
Iteration 214/1000 | Loss: 0.00001192
Iteration 215/1000 | Loss: 0.00001192
Iteration 216/1000 | Loss: 0.00001192
Iteration 217/1000 | Loss: 0.00001192
Iteration 218/1000 | Loss: 0.00001192
Iteration 219/1000 | Loss: 0.00001192
Iteration 220/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.1922263183805626e-05, 1.1922263183805626e-05, 1.1922263183805626e-05, 1.1922263183805626e-05, 1.1922263183805626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1922263183805626e-05

Optimization complete. Final v2v error: 2.9384660720825195 mm

Highest mean error: 3.0936169624328613 mm for frame 133

Lowest mean error: 2.700169324874878 mm for frame 83

Saving results

Total time: 43.93938660621643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836168
Iteration 2/25 | Loss: 0.00253095
Iteration 3/25 | Loss: 0.00194384
Iteration 4/25 | Loss: 0.00182731
Iteration 5/25 | Loss: 0.00168046
Iteration 6/25 | Loss: 0.00159760
Iteration 7/25 | Loss: 0.00155378
Iteration 8/25 | Loss: 0.00152536
Iteration 9/25 | Loss: 0.00150596
Iteration 10/25 | Loss: 0.00149837
Iteration 11/25 | Loss: 0.00149060
Iteration 12/25 | Loss: 0.00148752
Iteration 13/25 | Loss: 0.00148524
Iteration 14/25 | Loss: 0.00148436
Iteration 15/25 | Loss: 0.00148112
Iteration 16/25 | Loss: 0.00147696
Iteration 17/25 | Loss: 0.00147560
Iteration 18/25 | Loss: 0.00147523
Iteration 19/25 | Loss: 0.00147604
Iteration 20/25 | Loss: 0.00147530
Iteration 21/25 | Loss: 0.00147466
Iteration 22/25 | Loss: 0.00147377
Iteration 23/25 | Loss: 0.00147352
Iteration 24/25 | Loss: 0.00147342
Iteration 25/25 | Loss: 0.00147341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94663960
Iteration 2/25 | Loss: 0.00175868
Iteration 3/25 | Loss: 0.00175868
Iteration 4/25 | Loss: 0.00175868
Iteration 5/25 | Loss: 0.00175868
Iteration 6/25 | Loss: 0.00175868
Iteration 7/25 | Loss: 0.00175868
Iteration 8/25 | Loss: 0.00175868
Iteration 9/25 | Loss: 0.00175868
Iteration 10/25 | Loss: 0.00175867
Iteration 11/25 | Loss: 0.00175867
Iteration 12/25 | Loss: 0.00175867
Iteration 13/25 | Loss: 0.00175867
Iteration 14/25 | Loss: 0.00175867
Iteration 15/25 | Loss: 0.00175867
Iteration 16/25 | Loss: 0.00175867
Iteration 17/25 | Loss: 0.00175867
Iteration 18/25 | Loss: 0.00175867
Iteration 19/25 | Loss: 0.00175867
Iteration 20/25 | Loss: 0.00175867
Iteration 21/25 | Loss: 0.00175867
Iteration 22/25 | Loss: 0.00175867
Iteration 23/25 | Loss: 0.00175867
Iteration 24/25 | Loss: 0.00175867
Iteration 25/25 | Loss: 0.00175867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175867
Iteration 2/1000 | Loss: 0.00005480
Iteration 3/1000 | Loss: 0.00003503
Iteration 4/1000 | Loss: 0.00002640
Iteration 5/1000 | Loss: 0.00002483
Iteration 6/1000 | Loss: 0.00002387
Iteration 7/1000 | Loss: 0.00002316
Iteration 8/1000 | Loss: 0.00002270
Iteration 9/1000 | Loss: 0.00002230
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002180
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002161
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002156
Iteration 16/1000 | Loss: 0.00002150
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002148
Iteration 19/1000 | Loss: 0.00002144
Iteration 20/1000 | Loss: 0.00002144
Iteration 21/1000 | Loss: 0.00002143
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002133
Iteration 25/1000 | Loss: 0.00002132
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002130
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00002121
Iteration 37/1000 | Loss: 0.00002121
Iteration 38/1000 | Loss: 0.00002121
Iteration 39/1000 | Loss: 0.00002121
Iteration 40/1000 | Loss: 0.00002121
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002119
Iteration 56/1000 | Loss: 0.00002119
Iteration 57/1000 | Loss: 0.00002119
Iteration 58/1000 | Loss: 0.00002119
Iteration 59/1000 | Loss: 0.00002118
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002118
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002118
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002117
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002116
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002110
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002110
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002109
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002108
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002108
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002107
Iteration 110/1000 | Loss: 0.00002107
Iteration 111/1000 | Loss: 0.00002107
Iteration 112/1000 | Loss: 0.00002107
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002106
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00002106
Iteration 117/1000 | Loss: 0.00002106
Iteration 118/1000 | Loss: 0.00002106
Iteration 119/1000 | Loss: 0.00002106
Iteration 120/1000 | Loss: 0.00002106
Iteration 121/1000 | Loss: 0.00002106
Iteration 122/1000 | Loss: 0.00002105
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002105
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002104
Iteration 130/1000 | Loss: 0.00002104
Iteration 131/1000 | Loss: 0.00002104
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002103
Iteration 138/1000 | Loss: 0.00002103
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Iteration 150/1000 | Loss: 0.00002102
Iteration 151/1000 | Loss: 0.00002102
Iteration 152/1000 | Loss: 0.00002102
Iteration 153/1000 | Loss: 0.00002102
Iteration 154/1000 | Loss: 0.00002102
Iteration 155/1000 | Loss: 0.00002102
Iteration 156/1000 | Loss: 0.00002102
Iteration 157/1000 | Loss: 0.00002102
Iteration 158/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.1015088350395672e-05, 2.1015088350395672e-05, 2.1015088350395672e-05, 2.1015088350395672e-05, 2.1015088350395672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1015088350395672e-05

Optimization complete. Final v2v error: 3.8928003311157227 mm

Highest mean error: 4.680415153503418 mm for frame 81

Lowest mean error: 3.4896678924560547 mm for frame 210

Saving results

Total time: 81.06388449668884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911174
Iteration 2/25 | Loss: 0.00179814
Iteration 3/25 | Loss: 0.00148968
Iteration 4/25 | Loss: 0.00146846
Iteration 5/25 | Loss: 0.00146441
Iteration 6/25 | Loss: 0.00146317
Iteration 7/25 | Loss: 0.00146317
Iteration 8/25 | Loss: 0.00146317
Iteration 9/25 | Loss: 0.00146317
Iteration 10/25 | Loss: 0.00146317
Iteration 11/25 | Loss: 0.00146317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014631702797487378, 0.0014631702797487378, 0.0014631702797487378, 0.0014631702797487378, 0.0014631702797487378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014631702797487378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91826469
Iteration 2/25 | Loss: 0.00186246
Iteration 3/25 | Loss: 0.00186245
Iteration 4/25 | Loss: 0.00186245
Iteration 5/25 | Loss: 0.00186245
Iteration 6/25 | Loss: 0.00186244
Iteration 7/25 | Loss: 0.00186244
Iteration 8/25 | Loss: 0.00186244
Iteration 9/25 | Loss: 0.00186244
Iteration 10/25 | Loss: 0.00186244
Iteration 11/25 | Loss: 0.00186244
Iteration 12/25 | Loss: 0.00186244
Iteration 13/25 | Loss: 0.00186244
Iteration 14/25 | Loss: 0.00186244
Iteration 15/25 | Loss: 0.00186244
Iteration 16/25 | Loss: 0.00186244
Iteration 17/25 | Loss: 0.00186244
Iteration 18/25 | Loss: 0.00186244
Iteration 19/25 | Loss: 0.00186244
Iteration 20/25 | Loss: 0.00186244
Iteration 21/25 | Loss: 0.00186244
Iteration 22/25 | Loss: 0.00186244
Iteration 23/25 | Loss: 0.00186244
Iteration 24/25 | Loss: 0.00186244
Iteration 25/25 | Loss: 0.00186244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186244
Iteration 2/1000 | Loss: 0.00005187
Iteration 3/1000 | Loss: 0.00003677
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002720
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002477
Iteration 8/1000 | Loss: 0.00002395
Iteration 9/1000 | Loss: 0.00002330
Iteration 10/1000 | Loss: 0.00002294
Iteration 11/1000 | Loss: 0.00002259
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002182
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002140
Iteration 18/1000 | Loss: 0.00002137
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002116
Iteration 25/1000 | Loss: 0.00002108
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002102
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002089
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002087
Iteration 33/1000 | Loss: 0.00002085
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002082
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002079
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00002078
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002078
Iteration 46/1000 | Loss: 0.00002078
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002077
Iteration 49/1000 | Loss: 0.00002077
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002076
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002070
Iteration 75/1000 | Loss: 0.00002070
Iteration 76/1000 | Loss: 0.00002070
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002066
Iteration 110/1000 | Loss: 0.00002066
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002065
Iteration 124/1000 | Loss: 0.00002065
Iteration 125/1000 | Loss: 0.00002065
Iteration 126/1000 | Loss: 0.00002065
Iteration 127/1000 | Loss: 0.00002065
Iteration 128/1000 | Loss: 0.00002065
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002065
Iteration 131/1000 | Loss: 0.00002065
Iteration 132/1000 | Loss: 0.00002065
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002065
Iteration 136/1000 | Loss: 0.00002065
Iteration 137/1000 | Loss: 0.00002065
Iteration 138/1000 | Loss: 0.00002065
Iteration 139/1000 | Loss: 0.00002065
Iteration 140/1000 | Loss: 0.00002065
Iteration 141/1000 | Loss: 0.00002065
Iteration 142/1000 | Loss: 0.00002065
Iteration 143/1000 | Loss: 0.00002065
Iteration 144/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.06451859412482e-05, 2.06451859412482e-05, 2.06451859412482e-05, 2.06451859412482e-05, 2.06451859412482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.06451859412482e-05

Optimization complete. Final v2v error: 3.85160756111145 mm

Highest mean error: 4.345329761505127 mm for frame 137

Lowest mean error: 3.1657679080963135 mm for frame 27

Saving results

Total time: 44.270774364471436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428743
Iteration 2/25 | Loss: 0.00144160
Iteration 3/25 | Loss: 0.00138115
Iteration 4/25 | Loss: 0.00136874
Iteration 5/25 | Loss: 0.00136408
Iteration 6/25 | Loss: 0.00136298
Iteration 7/25 | Loss: 0.00136298
Iteration 8/25 | Loss: 0.00136298
Iteration 9/25 | Loss: 0.00136298
Iteration 10/25 | Loss: 0.00136298
Iteration 11/25 | Loss: 0.00136298
Iteration 12/25 | Loss: 0.00136298
Iteration 13/25 | Loss: 0.00136298
Iteration 14/25 | Loss: 0.00136298
Iteration 15/25 | Loss: 0.00136298
Iteration 16/25 | Loss: 0.00136298
Iteration 17/25 | Loss: 0.00136298
Iteration 18/25 | Loss: 0.00136298
Iteration 19/25 | Loss: 0.00136298
Iteration 20/25 | Loss: 0.00136298
Iteration 21/25 | Loss: 0.00136298
Iteration 22/25 | Loss: 0.00136298
Iteration 23/25 | Loss: 0.00136298
Iteration 24/25 | Loss: 0.00136298
Iteration 25/25 | Loss: 0.00136298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52018118
Iteration 2/25 | Loss: 0.00219884
Iteration 3/25 | Loss: 0.00219884
Iteration 4/25 | Loss: 0.00219884
Iteration 5/25 | Loss: 0.00219884
Iteration 6/25 | Loss: 0.00219883
Iteration 7/25 | Loss: 0.00219883
Iteration 8/25 | Loss: 0.00219883
Iteration 9/25 | Loss: 0.00219883
Iteration 10/25 | Loss: 0.00219883
Iteration 11/25 | Loss: 0.00219883
Iteration 12/25 | Loss: 0.00219883
Iteration 13/25 | Loss: 0.00219883
Iteration 14/25 | Loss: 0.00219883
Iteration 15/25 | Loss: 0.00219883
Iteration 16/25 | Loss: 0.00219883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002198833506554365, 0.002198833506554365, 0.002198833506554365, 0.002198833506554365, 0.002198833506554365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002198833506554365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219883
Iteration 2/1000 | Loss: 0.00002363
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001667
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001425
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001312
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001281
Iteration 48/1000 | Loss: 0.00001280
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001278
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001269
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001267
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001267
Iteration 88/1000 | Loss: 0.00001267
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001266
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001265
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001260
Iteration 136/1000 | Loss: 0.00001260
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001259
Iteration 144/1000 | Loss: 0.00001259
Iteration 145/1000 | Loss: 0.00001259
Iteration 146/1000 | Loss: 0.00001259
Iteration 147/1000 | Loss: 0.00001259
Iteration 148/1000 | Loss: 0.00001259
Iteration 149/1000 | Loss: 0.00001259
Iteration 150/1000 | Loss: 0.00001259
Iteration 151/1000 | Loss: 0.00001259
Iteration 152/1000 | Loss: 0.00001259
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001259
Iteration 157/1000 | Loss: 0.00001258
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001258
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001258
Iteration 166/1000 | Loss: 0.00001258
Iteration 167/1000 | Loss: 0.00001258
Iteration 168/1000 | Loss: 0.00001258
Iteration 169/1000 | Loss: 0.00001258
Iteration 170/1000 | Loss: 0.00001258
Iteration 171/1000 | Loss: 0.00001258
Iteration 172/1000 | Loss: 0.00001258
Iteration 173/1000 | Loss: 0.00001258
Iteration 174/1000 | Loss: 0.00001258
Iteration 175/1000 | Loss: 0.00001258
Iteration 176/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.2581111150211655e-05, 1.2581111150211655e-05, 1.2581111150211655e-05, 1.2581111150211655e-05, 1.2581111150211655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2581111150211655e-05

Optimization complete. Final v2v error: 3.0889055728912354 mm

Highest mean error: 3.5826005935668945 mm for frame 62

Lowest mean error: 2.935077428817749 mm for frame 90

Saving results

Total time: 38.75588822364807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479364
Iteration 2/25 | Loss: 0.00151070
Iteration 3/25 | Loss: 0.00142573
Iteration 4/25 | Loss: 0.00141817
Iteration 5/25 | Loss: 0.00141647
Iteration 6/25 | Loss: 0.00141647
Iteration 7/25 | Loss: 0.00141647
Iteration 8/25 | Loss: 0.00141647
Iteration 9/25 | Loss: 0.00141647
Iteration 10/25 | Loss: 0.00141647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014164677122607827, 0.0014164677122607827, 0.0014164677122607827, 0.0014164677122607827, 0.0014164677122607827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014164677122607827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24570024
Iteration 2/25 | Loss: 0.00209749
Iteration 3/25 | Loss: 0.00209748
Iteration 4/25 | Loss: 0.00209748
Iteration 5/25 | Loss: 0.00209748
Iteration 6/25 | Loss: 0.00209747
Iteration 7/25 | Loss: 0.00209747
Iteration 8/25 | Loss: 0.00209747
Iteration 9/25 | Loss: 0.00209747
Iteration 10/25 | Loss: 0.00209747
Iteration 11/25 | Loss: 0.00209747
Iteration 12/25 | Loss: 0.00209747
Iteration 13/25 | Loss: 0.00209747
Iteration 14/25 | Loss: 0.00209747
Iteration 15/25 | Loss: 0.00209747
Iteration 16/25 | Loss: 0.00209747
Iteration 17/25 | Loss: 0.00209747
Iteration 18/25 | Loss: 0.00209747
Iteration 19/25 | Loss: 0.00209747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002097472781315446, 0.002097472781315446, 0.002097472781315446, 0.002097472781315446, 0.002097472781315446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002097472781315446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209747
Iteration 2/1000 | Loss: 0.00003161
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001651
Iteration 22/1000 | Loss: 0.00001646
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001645
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001622
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001616
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001609
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001604
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001602
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001598
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001597
Iteration 85/1000 | Loss: 0.00001597
Iteration 86/1000 | Loss: 0.00001597
Iteration 87/1000 | Loss: 0.00001597
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001592
Iteration 101/1000 | Loss: 0.00001592
Iteration 102/1000 | Loss: 0.00001591
Iteration 103/1000 | Loss: 0.00001591
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001590
Iteration 106/1000 | Loss: 0.00001590
Iteration 107/1000 | Loss: 0.00001589
Iteration 108/1000 | Loss: 0.00001589
Iteration 109/1000 | Loss: 0.00001589
Iteration 110/1000 | Loss: 0.00001589
Iteration 111/1000 | Loss: 0.00001589
Iteration 112/1000 | Loss: 0.00001589
Iteration 113/1000 | Loss: 0.00001589
Iteration 114/1000 | Loss: 0.00001588
Iteration 115/1000 | Loss: 0.00001588
Iteration 116/1000 | Loss: 0.00001588
Iteration 117/1000 | Loss: 0.00001588
Iteration 118/1000 | Loss: 0.00001588
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001587
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001586
Iteration 128/1000 | Loss: 0.00001586
Iteration 129/1000 | Loss: 0.00001586
Iteration 130/1000 | Loss: 0.00001586
Iteration 131/1000 | Loss: 0.00001586
Iteration 132/1000 | Loss: 0.00001585
Iteration 133/1000 | Loss: 0.00001585
Iteration 134/1000 | Loss: 0.00001585
Iteration 135/1000 | Loss: 0.00001585
Iteration 136/1000 | Loss: 0.00001584
Iteration 137/1000 | Loss: 0.00001584
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001583
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001583
Iteration 146/1000 | Loss: 0.00001583
Iteration 147/1000 | Loss: 0.00001583
Iteration 148/1000 | Loss: 0.00001583
Iteration 149/1000 | Loss: 0.00001583
Iteration 150/1000 | Loss: 0.00001583
Iteration 151/1000 | Loss: 0.00001583
Iteration 152/1000 | Loss: 0.00001583
Iteration 153/1000 | Loss: 0.00001583
Iteration 154/1000 | Loss: 0.00001583
Iteration 155/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.583167613716796e-05, 1.583167613716796e-05, 1.583167613716796e-05, 1.583167613716796e-05, 1.583167613716796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.583167613716796e-05

Optimization complete. Final v2v error: 3.316288709640503 mm

Highest mean error: 4.058378219604492 mm for frame 58

Lowest mean error: 3.0496325492858887 mm for frame 12

Saving results

Total time: 44.69989371299744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803024
Iteration 2/25 | Loss: 0.00165346
Iteration 3/25 | Loss: 0.00146061
Iteration 4/25 | Loss: 0.00144657
Iteration 5/25 | Loss: 0.00144146
Iteration 6/25 | Loss: 0.00145332
Iteration 7/25 | Loss: 0.00144054
Iteration 8/25 | Loss: 0.00143540
Iteration 9/25 | Loss: 0.00143498
Iteration 10/25 | Loss: 0.00143488
Iteration 11/25 | Loss: 0.00143487
Iteration 12/25 | Loss: 0.00143487
Iteration 13/25 | Loss: 0.00143487
Iteration 14/25 | Loss: 0.00143487
Iteration 15/25 | Loss: 0.00143487
Iteration 16/25 | Loss: 0.00143487
Iteration 17/25 | Loss: 0.00143487
Iteration 18/25 | Loss: 0.00143487
Iteration 19/25 | Loss: 0.00143487
Iteration 20/25 | Loss: 0.00143487
Iteration 21/25 | Loss: 0.00143487
Iteration 22/25 | Loss: 0.00143487
Iteration 23/25 | Loss: 0.00143487
Iteration 24/25 | Loss: 0.00143487
Iteration 25/25 | Loss: 0.00143487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.26960492
Iteration 2/25 | Loss: 0.00185929
Iteration 3/25 | Loss: 0.00185926
Iteration 4/25 | Loss: 0.00185926
Iteration 5/25 | Loss: 0.00185926
Iteration 6/25 | Loss: 0.00185926
Iteration 7/25 | Loss: 0.00185926
Iteration 8/25 | Loss: 0.00185926
Iteration 9/25 | Loss: 0.00185926
Iteration 10/25 | Loss: 0.00185926
Iteration 11/25 | Loss: 0.00185926
Iteration 12/25 | Loss: 0.00185926
Iteration 13/25 | Loss: 0.00185926
Iteration 14/25 | Loss: 0.00185926
Iteration 15/25 | Loss: 0.00185926
Iteration 16/25 | Loss: 0.00185926
Iteration 17/25 | Loss: 0.00185926
Iteration 18/25 | Loss: 0.00185926
Iteration 19/25 | Loss: 0.00185926
Iteration 20/25 | Loss: 0.00185926
Iteration 21/25 | Loss: 0.00185926
Iteration 22/25 | Loss: 0.00185926
Iteration 23/25 | Loss: 0.00185926
Iteration 24/25 | Loss: 0.00185926
Iteration 25/25 | Loss: 0.00185926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185926
Iteration 2/1000 | Loss: 0.00004326
Iteration 3/1000 | Loss: 0.00002645
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002107
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001903
Iteration 10/1000 | Loss: 0.00001873
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001824
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001801
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001794
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001763
Iteration 122/1000 | Loss: 0.00001763
Iteration 123/1000 | Loss: 0.00001763
Iteration 124/1000 | Loss: 0.00001763
Iteration 125/1000 | Loss: 0.00001763
Iteration 126/1000 | Loss: 0.00001763
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001762
Iteration 130/1000 | Loss: 0.00001762
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001762
Iteration 133/1000 | Loss: 0.00001762
Iteration 134/1000 | Loss: 0.00001762
Iteration 135/1000 | Loss: 0.00001761
Iteration 136/1000 | Loss: 0.00001761
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001760
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00001760
Iteration 145/1000 | Loss: 0.00001760
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001760
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001759
Iteration 153/1000 | Loss: 0.00001759
Iteration 154/1000 | Loss: 0.00001759
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001759
Iteration 157/1000 | Loss: 0.00001759
Iteration 158/1000 | Loss: 0.00001759
Iteration 159/1000 | Loss: 0.00001759
Iteration 160/1000 | Loss: 0.00001759
Iteration 161/1000 | Loss: 0.00001759
Iteration 162/1000 | Loss: 0.00001759
Iteration 163/1000 | Loss: 0.00001759
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001759
Iteration 168/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.7587017282494344e-05, 1.7587017282494344e-05, 1.7587017282494344e-05, 1.7587017282494344e-05, 1.7587017282494344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7587017282494344e-05

Optimization complete. Final v2v error: 3.504241704940796 mm

Highest mean error: 4.1251444816589355 mm for frame 96

Lowest mean error: 3.08662486076355 mm for frame 48

Saving results

Total time: 46.42044401168823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786008
Iteration 2/25 | Loss: 0.00192627
Iteration 3/25 | Loss: 0.00152481
Iteration 4/25 | Loss: 0.00148395
Iteration 5/25 | Loss: 0.00147939
Iteration 6/25 | Loss: 0.00147888
Iteration 7/25 | Loss: 0.00147888
Iteration 8/25 | Loss: 0.00147888
Iteration 9/25 | Loss: 0.00147888
Iteration 10/25 | Loss: 0.00147888
Iteration 11/25 | Loss: 0.00147888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014788839034736156, 0.0014788839034736156, 0.0014788839034736156, 0.0014788839034736156, 0.0014788839034736156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014788839034736156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11028719
Iteration 2/25 | Loss: 0.00159202
Iteration 3/25 | Loss: 0.00159199
Iteration 4/25 | Loss: 0.00159199
Iteration 5/25 | Loss: 0.00159199
Iteration 6/25 | Loss: 0.00159199
Iteration 7/25 | Loss: 0.00159199
Iteration 8/25 | Loss: 0.00159199
Iteration 9/25 | Loss: 0.00159199
Iteration 10/25 | Loss: 0.00159199
Iteration 11/25 | Loss: 0.00159198
Iteration 12/25 | Loss: 0.00159198
Iteration 13/25 | Loss: 0.00159198
Iteration 14/25 | Loss: 0.00159198
Iteration 15/25 | Loss: 0.00159198
Iteration 16/25 | Loss: 0.00159198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015919849975034595, 0.0015919849975034595, 0.0015919849975034595, 0.0015919849975034595, 0.0015919849975034595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015919849975034595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159198
Iteration 2/1000 | Loss: 0.00004471
Iteration 3/1000 | Loss: 0.00003104
Iteration 4/1000 | Loss: 0.00002784
Iteration 5/1000 | Loss: 0.00002611
Iteration 6/1000 | Loss: 0.00002514
Iteration 7/1000 | Loss: 0.00002458
Iteration 8/1000 | Loss: 0.00002410
Iteration 9/1000 | Loss: 0.00002368
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002303
Iteration 12/1000 | Loss: 0.00002278
Iteration 13/1000 | Loss: 0.00002264
Iteration 14/1000 | Loss: 0.00002251
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002250
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002228
Iteration 19/1000 | Loss: 0.00002225
Iteration 20/1000 | Loss: 0.00002224
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002220
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00002220
Iteration 28/1000 | Loss: 0.00002220
Iteration 29/1000 | Loss: 0.00002220
Iteration 30/1000 | Loss: 0.00002220
Iteration 31/1000 | Loss: 0.00002220
Iteration 32/1000 | Loss: 0.00002219
Iteration 33/1000 | Loss: 0.00002219
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002211
Iteration 37/1000 | Loss: 0.00002210
Iteration 38/1000 | Loss: 0.00002210
Iteration 39/1000 | Loss: 0.00002210
Iteration 40/1000 | Loss: 0.00002210
Iteration 41/1000 | Loss: 0.00002210
Iteration 42/1000 | Loss: 0.00002209
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002209
Iteration 45/1000 | Loss: 0.00002209
Iteration 46/1000 | Loss: 0.00002208
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002207
Iteration 51/1000 | Loss: 0.00002207
Iteration 52/1000 | Loss: 0.00002207
Iteration 53/1000 | Loss: 0.00002207
Iteration 54/1000 | Loss: 0.00002207
Iteration 55/1000 | Loss: 0.00002207
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002204
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002203
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002203
Iteration 73/1000 | Loss: 0.00002203
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002202
Iteration 76/1000 | Loss: 0.00002202
Iteration 77/1000 | Loss: 0.00002202
Iteration 78/1000 | Loss: 0.00002202
Iteration 79/1000 | Loss: 0.00002202
Iteration 80/1000 | Loss: 0.00002202
Iteration 81/1000 | Loss: 0.00002202
Iteration 82/1000 | Loss: 0.00002202
Iteration 83/1000 | Loss: 0.00002202
Iteration 84/1000 | Loss: 0.00002202
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002201
Iteration 87/1000 | Loss: 0.00002201
Iteration 88/1000 | Loss: 0.00002201
Iteration 89/1000 | Loss: 0.00002201
Iteration 90/1000 | Loss: 0.00002201
Iteration 91/1000 | Loss: 0.00002201
Iteration 92/1000 | Loss: 0.00002201
Iteration 93/1000 | Loss: 0.00002200
Iteration 94/1000 | Loss: 0.00002200
Iteration 95/1000 | Loss: 0.00002200
Iteration 96/1000 | Loss: 0.00002200
Iteration 97/1000 | Loss: 0.00002200
Iteration 98/1000 | Loss: 0.00002200
Iteration 99/1000 | Loss: 0.00002200
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002199
Iteration 103/1000 | Loss: 0.00002199
Iteration 104/1000 | Loss: 0.00002199
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002199
Iteration 107/1000 | Loss: 0.00002199
Iteration 108/1000 | Loss: 0.00002199
Iteration 109/1000 | Loss: 0.00002199
Iteration 110/1000 | Loss: 0.00002199
Iteration 111/1000 | Loss: 0.00002199
Iteration 112/1000 | Loss: 0.00002199
Iteration 113/1000 | Loss: 0.00002199
Iteration 114/1000 | Loss: 0.00002199
Iteration 115/1000 | Loss: 0.00002199
Iteration 116/1000 | Loss: 0.00002199
Iteration 117/1000 | Loss: 0.00002199
Iteration 118/1000 | Loss: 0.00002199
Iteration 119/1000 | Loss: 0.00002199
Iteration 120/1000 | Loss: 0.00002198
Iteration 121/1000 | Loss: 0.00002198
Iteration 122/1000 | Loss: 0.00002198
Iteration 123/1000 | Loss: 0.00002198
Iteration 124/1000 | Loss: 0.00002198
Iteration 125/1000 | Loss: 0.00002198
Iteration 126/1000 | Loss: 0.00002198
Iteration 127/1000 | Loss: 0.00002197
Iteration 128/1000 | Loss: 0.00002197
Iteration 129/1000 | Loss: 0.00002197
Iteration 130/1000 | Loss: 0.00002197
Iteration 131/1000 | Loss: 0.00002197
Iteration 132/1000 | Loss: 0.00002197
Iteration 133/1000 | Loss: 0.00002197
Iteration 134/1000 | Loss: 0.00002196
Iteration 135/1000 | Loss: 0.00002196
Iteration 136/1000 | Loss: 0.00002196
Iteration 137/1000 | Loss: 0.00002196
Iteration 138/1000 | Loss: 0.00002196
Iteration 139/1000 | Loss: 0.00002196
Iteration 140/1000 | Loss: 0.00002196
Iteration 141/1000 | Loss: 0.00002196
Iteration 142/1000 | Loss: 0.00002196
Iteration 143/1000 | Loss: 0.00002195
Iteration 144/1000 | Loss: 0.00002195
Iteration 145/1000 | Loss: 0.00002195
Iteration 146/1000 | Loss: 0.00002195
Iteration 147/1000 | Loss: 0.00002195
Iteration 148/1000 | Loss: 0.00002195
Iteration 149/1000 | Loss: 0.00002195
Iteration 150/1000 | Loss: 0.00002194
Iteration 151/1000 | Loss: 0.00002194
Iteration 152/1000 | Loss: 0.00002194
Iteration 153/1000 | Loss: 0.00002194
Iteration 154/1000 | Loss: 0.00002194
Iteration 155/1000 | Loss: 0.00002194
Iteration 156/1000 | Loss: 0.00002194
Iteration 157/1000 | Loss: 0.00002194
Iteration 158/1000 | Loss: 0.00002194
Iteration 159/1000 | Loss: 0.00002194
Iteration 160/1000 | Loss: 0.00002194
Iteration 161/1000 | Loss: 0.00002194
Iteration 162/1000 | Loss: 0.00002194
Iteration 163/1000 | Loss: 0.00002194
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002194
Iteration 168/1000 | Loss: 0.00002194
Iteration 169/1000 | Loss: 0.00002194
Iteration 170/1000 | Loss: 0.00002194
Iteration 171/1000 | Loss: 0.00002194
Iteration 172/1000 | Loss: 0.00002194
Iteration 173/1000 | Loss: 0.00002194
Iteration 174/1000 | Loss: 0.00002194
Iteration 175/1000 | Loss: 0.00002194
Iteration 176/1000 | Loss: 0.00002194
Iteration 177/1000 | Loss: 0.00002194
Iteration 178/1000 | Loss: 0.00002194
Iteration 179/1000 | Loss: 0.00002194
Iteration 180/1000 | Loss: 0.00002194
Iteration 181/1000 | Loss: 0.00002194
Iteration 182/1000 | Loss: 0.00002194
Iteration 183/1000 | Loss: 0.00002194
Iteration 184/1000 | Loss: 0.00002194
Iteration 185/1000 | Loss: 0.00002194
Iteration 186/1000 | Loss: 0.00002194
Iteration 187/1000 | Loss: 0.00002194
Iteration 188/1000 | Loss: 0.00002194
Iteration 189/1000 | Loss: 0.00002194
Iteration 190/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.193551517848391e-05, 2.193551517848391e-05, 2.193551517848391e-05, 2.193551517848391e-05, 2.193551517848391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.193551517848391e-05

Optimization complete. Final v2v error: 3.975228786468506 mm

Highest mean error: 4.324911117553711 mm for frame 112

Lowest mean error: 3.7157857418060303 mm for frame 45

Saving results

Total time: 38.32496619224548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521256
Iteration 2/25 | Loss: 0.00156010
Iteration 3/25 | Loss: 0.00143510
Iteration 4/25 | Loss: 0.00142594
Iteration 5/25 | Loss: 0.00142476
Iteration 6/25 | Loss: 0.00142476
Iteration 7/25 | Loss: 0.00142476
Iteration 8/25 | Loss: 0.00142476
Iteration 9/25 | Loss: 0.00142476
Iteration 10/25 | Loss: 0.00142476
Iteration 11/25 | Loss: 0.00142476
Iteration 12/25 | Loss: 0.00142476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014247623039409518, 0.0014247623039409518, 0.0014247623039409518, 0.0014247623039409518, 0.0014247623039409518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014247623039409518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.97993422
Iteration 2/25 | Loss: 0.00131056
Iteration 3/25 | Loss: 0.00131056
Iteration 4/25 | Loss: 0.00131056
Iteration 5/25 | Loss: 0.00131055
Iteration 6/25 | Loss: 0.00131055
Iteration 7/25 | Loss: 0.00131055
Iteration 8/25 | Loss: 0.00131055
Iteration 9/25 | Loss: 0.00131055
Iteration 10/25 | Loss: 0.00131055
Iteration 11/25 | Loss: 0.00131055
Iteration 12/25 | Loss: 0.00131055
Iteration 13/25 | Loss: 0.00131055
Iteration 14/25 | Loss: 0.00131055
Iteration 15/25 | Loss: 0.00131055
Iteration 16/25 | Loss: 0.00131055
Iteration 17/25 | Loss: 0.00131055
Iteration 18/25 | Loss: 0.00131055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013105537509545684, 0.0013105537509545684, 0.0013105537509545684, 0.0013105537509545684, 0.0013105537509545684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013105537509545684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131055
Iteration 2/1000 | Loss: 0.00004109
Iteration 3/1000 | Loss: 0.00002769
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002263
Iteration 6/1000 | Loss: 0.00002189
Iteration 7/1000 | Loss: 0.00002139
Iteration 8/1000 | Loss: 0.00002090
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002017
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001985
Iteration 13/1000 | Loss: 0.00001984
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001921
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001912
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001911
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001903
Iteration 49/1000 | Loss: 0.00001903
Iteration 50/1000 | Loss: 0.00001902
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001895
Iteration 61/1000 | Loss: 0.00001895
Iteration 62/1000 | Loss: 0.00001894
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001894
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001894
Iteration 67/1000 | Loss: 0.00001894
Iteration 68/1000 | Loss: 0.00001894
Iteration 69/1000 | Loss: 0.00001894
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001892
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001892
Iteration 80/1000 | Loss: 0.00001892
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001890
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001890
Iteration 116/1000 | Loss: 0.00001890
Iteration 117/1000 | Loss: 0.00001890
Iteration 118/1000 | Loss: 0.00001890
Iteration 119/1000 | Loss: 0.00001890
Iteration 120/1000 | Loss: 0.00001890
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001890
Iteration 124/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8903176169260405e-05, 1.8903176169260405e-05, 1.8903176169260405e-05, 1.8903176169260405e-05, 1.8903176169260405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8903176169260405e-05

Optimization complete. Final v2v error: 3.685732841491699 mm

Highest mean error: 4.043742656707764 mm for frame 83

Lowest mean error: 3.414203405380249 mm for frame 52

Saving results

Total time: 36.121912240982056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787872
Iteration 2/25 | Loss: 0.00147419
Iteration 3/25 | Loss: 0.00139461
Iteration 4/25 | Loss: 0.00138424
Iteration 5/25 | Loss: 0.00138142
Iteration 6/25 | Loss: 0.00138090
Iteration 7/25 | Loss: 0.00138090
Iteration 8/25 | Loss: 0.00138090
Iteration 9/25 | Loss: 0.00138090
Iteration 10/25 | Loss: 0.00138090
Iteration 11/25 | Loss: 0.00138090
Iteration 12/25 | Loss: 0.00138090
Iteration 13/25 | Loss: 0.00138090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001380904228426516, 0.001380904228426516, 0.001380904228426516, 0.001380904228426516, 0.001380904228426516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001380904228426516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33618915
Iteration 2/25 | Loss: 0.00212881
Iteration 3/25 | Loss: 0.00212881
Iteration 4/25 | Loss: 0.00212881
Iteration 5/25 | Loss: 0.00212881
Iteration 6/25 | Loss: 0.00212881
Iteration 7/25 | Loss: 0.00212881
Iteration 8/25 | Loss: 0.00212881
Iteration 9/25 | Loss: 0.00212880
Iteration 10/25 | Loss: 0.00212880
Iteration 11/25 | Loss: 0.00212880
Iteration 12/25 | Loss: 0.00212880
Iteration 13/25 | Loss: 0.00212880
Iteration 14/25 | Loss: 0.00212880
Iteration 15/25 | Loss: 0.00212880
Iteration 16/25 | Loss: 0.00212880
Iteration 17/25 | Loss: 0.00212880
Iteration 18/25 | Loss: 0.00212880
Iteration 19/25 | Loss: 0.00212880
Iteration 20/25 | Loss: 0.00212880
Iteration 21/25 | Loss: 0.00212880
Iteration 22/25 | Loss: 0.00212880
Iteration 23/25 | Loss: 0.00212880
Iteration 24/25 | Loss: 0.00212880
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002128804102540016, 0.002128804102540016, 0.002128804102540016, 0.002128804102540016, 0.002128804102540016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002128804102540016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212880
Iteration 2/1000 | Loss: 0.00003336
Iteration 3/1000 | Loss: 0.00002394
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001921
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001763
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001567
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001565
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001561
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001535
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001515
Iteration 110/1000 | Loss: 0.00001515
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001514
Iteration 113/1000 | Loss: 0.00001513
Iteration 114/1000 | Loss: 0.00001513
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001511
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001509
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001506
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001505
Iteration 141/1000 | Loss: 0.00001504
Iteration 142/1000 | Loss: 0.00001504
Iteration 143/1000 | Loss: 0.00001504
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001503
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001503
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001500
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001499
Iteration 171/1000 | Loss: 0.00001499
Iteration 172/1000 | Loss: 0.00001499
Iteration 173/1000 | Loss: 0.00001499
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001497
Iteration 185/1000 | Loss: 0.00001497
Iteration 186/1000 | Loss: 0.00001497
Iteration 187/1000 | Loss: 0.00001497
Iteration 188/1000 | Loss: 0.00001497
Iteration 189/1000 | Loss: 0.00001497
Iteration 190/1000 | Loss: 0.00001497
Iteration 191/1000 | Loss: 0.00001497
Iteration 192/1000 | Loss: 0.00001497
Iteration 193/1000 | Loss: 0.00001497
Iteration 194/1000 | Loss: 0.00001497
Iteration 195/1000 | Loss: 0.00001497
Iteration 196/1000 | Loss: 0.00001496
Iteration 197/1000 | Loss: 0.00001496
Iteration 198/1000 | Loss: 0.00001496
Iteration 199/1000 | Loss: 0.00001496
Iteration 200/1000 | Loss: 0.00001496
Iteration 201/1000 | Loss: 0.00001496
Iteration 202/1000 | Loss: 0.00001496
Iteration 203/1000 | Loss: 0.00001496
Iteration 204/1000 | Loss: 0.00001495
Iteration 205/1000 | Loss: 0.00001495
Iteration 206/1000 | Loss: 0.00001495
Iteration 207/1000 | Loss: 0.00001495
Iteration 208/1000 | Loss: 0.00001495
Iteration 209/1000 | Loss: 0.00001495
Iteration 210/1000 | Loss: 0.00001495
Iteration 211/1000 | Loss: 0.00001495
Iteration 212/1000 | Loss: 0.00001495
Iteration 213/1000 | Loss: 0.00001495
Iteration 214/1000 | Loss: 0.00001495
Iteration 215/1000 | Loss: 0.00001495
Iteration 216/1000 | Loss: 0.00001494
Iteration 217/1000 | Loss: 0.00001494
Iteration 218/1000 | Loss: 0.00001494
Iteration 219/1000 | Loss: 0.00001494
Iteration 220/1000 | Loss: 0.00001494
Iteration 221/1000 | Loss: 0.00001494
Iteration 222/1000 | Loss: 0.00001494
Iteration 223/1000 | Loss: 0.00001494
Iteration 224/1000 | Loss: 0.00001494
Iteration 225/1000 | Loss: 0.00001494
Iteration 226/1000 | Loss: 0.00001494
Iteration 227/1000 | Loss: 0.00001494
Iteration 228/1000 | Loss: 0.00001494
Iteration 229/1000 | Loss: 0.00001494
Iteration 230/1000 | Loss: 0.00001494
Iteration 231/1000 | Loss: 0.00001494
Iteration 232/1000 | Loss: 0.00001494
Iteration 233/1000 | Loss: 0.00001494
Iteration 234/1000 | Loss: 0.00001494
Iteration 235/1000 | Loss: 0.00001494
Iteration 236/1000 | Loss: 0.00001494
Iteration 237/1000 | Loss: 0.00001494
Iteration 238/1000 | Loss: 0.00001494
Iteration 239/1000 | Loss: 0.00001494
Iteration 240/1000 | Loss: 0.00001494
Iteration 241/1000 | Loss: 0.00001494
Iteration 242/1000 | Loss: 0.00001494
Iteration 243/1000 | Loss: 0.00001494
Iteration 244/1000 | Loss: 0.00001494
Iteration 245/1000 | Loss: 0.00001494
Iteration 246/1000 | Loss: 0.00001494
Iteration 247/1000 | Loss: 0.00001494
Iteration 248/1000 | Loss: 0.00001494
Iteration 249/1000 | Loss: 0.00001494
Iteration 250/1000 | Loss: 0.00001494
Iteration 251/1000 | Loss: 0.00001494
Iteration 252/1000 | Loss: 0.00001494
Iteration 253/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.4935912076907698e-05, 1.4935912076907698e-05, 1.4935912076907698e-05, 1.4935912076907698e-05, 1.4935912076907698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4935912076907698e-05

Optimization complete. Final v2v error: 3.259599447250366 mm

Highest mean error: 4.381755828857422 mm for frame 136

Lowest mean error: 2.683285713195801 mm for frame 199

Saving results

Total time: 54.93223762512207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887865
Iteration 2/25 | Loss: 0.00148529
Iteration 3/25 | Loss: 0.00140664
Iteration 4/25 | Loss: 0.00139211
Iteration 5/25 | Loss: 0.00138729
Iteration 6/25 | Loss: 0.00138597
Iteration 7/25 | Loss: 0.00138597
Iteration 8/25 | Loss: 0.00138597
Iteration 9/25 | Loss: 0.00138597
Iteration 10/25 | Loss: 0.00138597
Iteration 11/25 | Loss: 0.00138597
Iteration 12/25 | Loss: 0.00138597
Iteration 13/25 | Loss: 0.00138597
Iteration 14/25 | Loss: 0.00138597
Iteration 15/25 | Loss: 0.00138597
Iteration 16/25 | Loss: 0.00138597
Iteration 17/25 | Loss: 0.00138597
Iteration 18/25 | Loss: 0.00138597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013859730679541826, 0.0013859730679541826, 0.0013859730679541826, 0.0013859730679541826, 0.0013859730679541826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013859730679541826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26695430
Iteration 2/25 | Loss: 0.00216037
Iteration 3/25 | Loss: 0.00216037
Iteration 4/25 | Loss: 0.00216037
Iteration 5/25 | Loss: 0.00216037
Iteration 6/25 | Loss: 0.00216037
Iteration 7/25 | Loss: 0.00216036
Iteration 8/25 | Loss: 0.00216036
Iteration 9/25 | Loss: 0.00216036
Iteration 10/25 | Loss: 0.00216036
Iteration 11/25 | Loss: 0.00216036
Iteration 12/25 | Loss: 0.00216036
Iteration 13/25 | Loss: 0.00216036
Iteration 14/25 | Loss: 0.00216036
Iteration 15/25 | Loss: 0.00216036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002160363830626011, 0.002160363830626011, 0.002160363830626011, 0.002160363830626011, 0.002160363830626011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002160363830626011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216036
Iteration 2/1000 | Loss: 0.00004233
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002165
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001818
Iteration 12/1000 | Loss: 0.00001791
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001689
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001687
Iteration 43/1000 | Loss: 0.00001686
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001685
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001671
Iteration 87/1000 | Loss: 0.00001671
Iteration 88/1000 | Loss: 0.00001671
Iteration 89/1000 | Loss: 0.00001671
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001670
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001666
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001666
Iteration 107/1000 | Loss: 0.00001666
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001666
Iteration 110/1000 | Loss: 0.00001666
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001662
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001662
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001661
Iteration 141/1000 | Loss: 0.00001661
Iteration 142/1000 | Loss: 0.00001661
Iteration 143/1000 | Loss: 0.00001661
Iteration 144/1000 | Loss: 0.00001661
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001660
Iteration 149/1000 | Loss: 0.00001660
Iteration 150/1000 | Loss: 0.00001660
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001659
Iteration 164/1000 | Loss: 0.00001659
Iteration 165/1000 | Loss: 0.00001659
Iteration 166/1000 | Loss: 0.00001659
Iteration 167/1000 | Loss: 0.00001658
Iteration 168/1000 | Loss: 0.00001658
Iteration 169/1000 | Loss: 0.00001658
Iteration 170/1000 | Loss: 0.00001658
Iteration 171/1000 | Loss: 0.00001658
Iteration 172/1000 | Loss: 0.00001658
Iteration 173/1000 | Loss: 0.00001658
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001657
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001657
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001657
Iteration 183/1000 | Loss: 0.00001657
Iteration 184/1000 | Loss: 0.00001657
Iteration 185/1000 | Loss: 0.00001657
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001657
Iteration 191/1000 | Loss: 0.00001657
Iteration 192/1000 | Loss: 0.00001657
Iteration 193/1000 | Loss: 0.00001656
Iteration 194/1000 | Loss: 0.00001656
Iteration 195/1000 | Loss: 0.00001656
Iteration 196/1000 | Loss: 0.00001656
Iteration 197/1000 | Loss: 0.00001656
Iteration 198/1000 | Loss: 0.00001656
Iteration 199/1000 | Loss: 0.00001656
Iteration 200/1000 | Loss: 0.00001656
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001655
Iteration 204/1000 | Loss: 0.00001655
Iteration 205/1000 | Loss: 0.00001655
Iteration 206/1000 | Loss: 0.00001655
Iteration 207/1000 | Loss: 0.00001655
Iteration 208/1000 | Loss: 0.00001655
Iteration 209/1000 | Loss: 0.00001655
Iteration 210/1000 | Loss: 0.00001655
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001655
Iteration 213/1000 | Loss: 0.00001655
Iteration 214/1000 | Loss: 0.00001655
Iteration 215/1000 | Loss: 0.00001655
Iteration 216/1000 | Loss: 0.00001654
Iteration 217/1000 | Loss: 0.00001654
Iteration 218/1000 | Loss: 0.00001654
Iteration 219/1000 | Loss: 0.00001654
Iteration 220/1000 | Loss: 0.00001654
Iteration 221/1000 | Loss: 0.00001654
Iteration 222/1000 | Loss: 0.00001654
Iteration 223/1000 | Loss: 0.00001654
Iteration 224/1000 | Loss: 0.00001654
Iteration 225/1000 | Loss: 0.00001654
Iteration 226/1000 | Loss: 0.00001654
Iteration 227/1000 | Loss: 0.00001654
Iteration 228/1000 | Loss: 0.00001654
Iteration 229/1000 | Loss: 0.00001654
Iteration 230/1000 | Loss: 0.00001654
Iteration 231/1000 | Loss: 0.00001653
Iteration 232/1000 | Loss: 0.00001653
Iteration 233/1000 | Loss: 0.00001653
Iteration 234/1000 | Loss: 0.00001653
Iteration 235/1000 | Loss: 0.00001653
Iteration 236/1000 | Loss: 0.00001653
Iteration 237/1000 | Loss: 0.00001653
Iteration 238/1000 | Loss: 0.00001653
Iteration 239/1000 | Loss: 0.00001653
Iteration 240/1000 | Loss: 0.00001653
Iteration 241/1000 | Loss: 0.00001653
Iteration 242/1000 | Loss: 0.00001653
Iteration 243/1000 | Loss: 0.00001653
Iteration 244/1000 | Loss: 0.00001653
Iteration 245/1000 | Loss: 0.00001653
Iteration 246/1000 | Loss: 0.00001653
Iteration 247/1000 | Loss: 0.00001653
Iteration 248/1000 | Loss: 0.00001653
Iteration 249/1000 | Loss: 0.00001653
Iteration 250/1000 | Loss: 0.00001653
Iteration 251/1000 | Loss: 0.00001653
Iteration 252/1000 | Loss: 0.00001653
Iteration 253/1000 | Loss: 0.00001653
Iteration 254/1000 | Loss: 0.00001653
Iteration 255/1000 | Loss: 0.00001653
Iteration 256/1000 | Loss: 0.00001653
Iteration 257/1000 | Loss: 0.00001653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.6527204934391193e-05, 1.6527204934391193e-05, 1.6527204934391193e-05, 1.6527204934391193e-05, 1.6527204934391193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6527204934391193e-05

Optimization complete. Final v2v error: 3.4573206901550293 mm

Highest mean error: 5.206733703613281 mm for frame 77

Lowest mean error: 2.999938726425171 mm for frame 102

Saving results

Total time: 46.24365758895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751078
Iteration 2/25 | Loss: 0.00162683
Iteration 3/25 | Loss: 0.00143639
Iteration 4/25 | Loss: 0.00142685
Iteration 5/25 | Loss: 0.00143897
Iteration 6/25 | Loss: 0.00142409
Iteration 7/25 | Loss: 0.00142612
Iteration 8/25 | Loss: 0.00141962
Iteration 9/25 | Loss: 0.00141627
Iteration 10/25 | Loss: 0.00141952
Iteration 11/25 | Loss: 0.00141779
Iteration 12/25 | Loss: 0.00141537
Iteration 13/25 | Loss: 0.00141482
Iteration 14/25 | Loss: 0.00141834
Iteration 15/25 | Loss: 0.00141691
Iteration 16/25 | Loss: 0.00141528
Iteration 17/25 | Loss: 0.00141385
Iteration 18/25 | Loss: 0.00141360
Iteration 19/25 | Loss: 0.00141354
Iteration 20/25 | Loss: 0.00141354
Iteration 21/25 | Loss: 0.00141354
Iteration 22/25 | Loss: 0.00141353
Iteration 23/25 | Loss: 0.00141353
Iteration 24/25 | Loss: 0.00141353
Iteration 25/25 | Loss: 0.00141353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.37433529
Iteration 2/25 | Loss: 0.00147225
Iteration 3/25 | Loss: 0.00147197
Iteration 4/25 | Loss: 0.00147197
Iteration 5/25 | Loss: 0.00147197
Iteration 6/25 | Loss: 0.00147197
Iteration 7/25 | Loss: 0.00147197
Iteration 8/25 | Loss: 0.00147197
Iteration 9/25 | Loss: 0.00147197
Iteration 10/25 | Loss: 0.00147197
Iteration 11/25 | Loss: 0.00147197
Iteration 12/25 | Loss: 0.00147197
Iteration 13/25 | Loss: 0.00147197
Iteration 14/25 | Loss: 0.00147197
Iteration 15/25 | Loss: 0.00147197
Iteration 16/25 | Loss: 0.00147197
Iteration 17/25 | Loss: 0.00147197
Iteration 18/25 | Loss: 0.00147197
Iteration 19/25 | Loss: 0.00147197
Iteration 20/25 | Loss: 0.00147197
Iteration 21/25 | Loss: 0.00147197
Iteration 22/25 | Loss: 0.00147197
Iteration 23/25 | Loss: 0.00147197
Iteration 24/25 | Loss: 0.00147197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014719691826030612, 0.0014719691826030612, 0.0014719691826030612, 0.0014719691826030612, 0.0014719691826030612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014719691826030612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147197
Iteration 2/1000 | Loss: 0.00002973
Iteration 3/1000 | Loss: 0.00002225
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001912
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001758
Iteration 8/1000 | Loss: 0.00001715
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001617
Iteration 13/1000 | Loss: 0.00001612
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001574
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001559
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001550
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001549
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001549
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001548
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001536
Iteration 66/1000 | Loss: 0.00001536
Iteration 67/1000 | Loss: 0.00001536
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001534
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.533778413431719e-05, 1.533778413431719e-05, 1.533778413431719e-05, 1.533778413431719e-05, 1.533778413431719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.533778413431719e-05

Optimization complete. Final v2v error: 3.3656654357910156 mm

Highest mean error: 3.689817428588867 mm for frame 4

Lowest mean error: 3.150703191757202 mm for frame 135

Saving results

Total time: 66.16054153442383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839603
Iteration 2/25 | Loss: 0.00144587
Iteration 3/25 | Loss: 0.00136902
Iteration 4/25 | Loss: 0.00135991
Iteration 5/25 | Loss: 0.00135682
Iteration 6/25 | Loss: 0.00135628
Iteration 7/25 | Loss: 0.00135628
Iteration 8/25 | Loss: 0.00135628
Iteration 9/25 | Loss: 0.00135628
Iteration 10/25 | Loss: 0.00135628
Iteration 11/25 | Loss: 0.00135628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013562818057835102, 0.0013562818057835102, 0.0013562818057835102, 0.0013562818057835102, 0.0013562818057835102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013562818057835102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32904029
Iteration 2/25 | Loss: 0.00214450
Iteration 3/25 | Loss: 0.00214450
Iteration 4/25 | Loss: 0.00214450
Iteration 5/25 | Loss: 0.00214450
Iteration 6/25 | Loss: 0.00214450
Iteration 7/25 | Loss: 0.00214450
Iteration 8/25 | Loss: 0.00214450
Iteration 9/25 | Loss: 0.00214450
Iteration 10/25 | Loss: 0.00214450
Iteration 11/25 | Loss: 0.00214450
Iteration 12/25 | Loss: 0.00214450
Iteration 13/25 | Loss: 0.00214450
Iteration 14/25 | Loss: 0.00214450
Iteration 15/25 | Loss: 0.00214450
Iteration 16/25 | Loss: 0.00214450
Iteration 17/25 | Loss: 0.00214450
Iteration 18/25 | Loss: 0.00214450
Iteration 19/25 | Loss: 0.00214450
Iteration 20/25 | Loss: 0.00214450
Iteration 21/25 | Loss: 0.00214450
Iteration 22/25 | Loss: 0.00214450
Iteration 23/25 | Loss: 0.00214450
Iteration 24/25 | Loss: 0.00214450
Iteration 25/25 | Loss: 0.00214450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214450
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001285
Iteration 19/1000 | Loss: 0.00001284
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001275
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001254
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001239
Iteration 63/1000 | Loss: 0.00001239
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001233
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001228
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001225
Iteration 108/1000 | Loss: 0.00001225
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001224
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001223
Iteration 117/1000 | Loss: 0.00001223
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001222
Iteration 124/1000 | Loss: 0.00001222
Iteration 125/1000 | Loss: 0.00001222
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001222
Iteration 134/1000 | Loss: 0.00001222
Iteration 135/1000 | Loss: 0.00001221
Iteration 136/1000 | Loss: 0.00001221
Iteration 137/1000 | Loss: 0.00001221
Iteration 138/1000 | Loss: 0.00001221
Iteration 139/1000 | Loss: 0.00001221
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001220
Iteration 142/1000 | Loss: 0.00001220
Iteration 143/1000 | Loss: 0.00001220
Iteration 144/1000 | Loss: 0.00001220
Iteration 145/1000 | Loss: 0.00001220
Iteration 146/1000 | Loss: 0.00001220
Iteration 147/1000 | Loss: 0.00001220
Iteration 148/1000 | Loss: 0.00001220
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001220
Iteration 154/1000 | Loss: 0.00001220
Iteration 155/1000 | Loss: 0.00001220
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Iteration 164/1000 | Loss: 0.00001218
Iteration 165/1000 | Loss: 0.00001218
Iteration 166/1000 | Loss: 0.00001218
Iteration 167/1000 | Loss: 0.00001218
Iteration 168/1000 | Loss: 0.00001218
Iteration 169/1000 | Loss: 0.00001218
Iteration 170/1000 | Loss: 0.00001218
Iteration 171/1000 | Loss: 0.00001218
Iteration 172/1000 | Loss: 0.00001218
Iteration 173/1000 | Loss: 0.00001218
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001217
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001217
Iteration 178/1000 | Loss: 0.00001217
Iteration 179/1000 | Loss: 0.00001217
Iteration 180/1000 | Loss: 0.00001217
Iteration 181/1000 | Loss: 0.00001217
Iteration 182/1000 | Loss: 0.00001216
Iteration 183/1000 | Loss: 0.00001216
Iteration 184/1000 | Loss: 0.00001216
Iteration 185/1000 | Loss: 0.00001216
Iteration 186/1000 | Loss: 0.00001216
Iteration 187/1000 | Loss: 0.00001216
Iteration 188/1000 | Loss: 0.00001216
Iteration 189/1000 | Loss: 0.00001216
Iteration 190/1000 | Loss: 0.00001216
Iteration 191/1000 | Loss: 0.00001216
Iteration 192/1000 | Loss: 0.00001216
Iteration 193/1000 | Loss: 0.00001216
Iteration 194/1000 | Loss: 0.00001216
Iteration 195/1000 | Loss: 0.00001216
Iteration 196/1000 | Loss: 0.00001216
Iteration 197/1000 | Loss: 0.00001216
Iteration 198/1000 | Loss: 0.00001216
Iteration 199/1000 | Loss: 0.00001216
Iteration 200/1000 | Loss: 0.00001216
Iteration 201/1000 | Loss: 0.00001215
Iteration 202/1000 | Loss: 0.00001215
Iteration 203/1000 | Loss: 0.00001215
Iteration 204/1000 | Loss: 0.00001215
Iteration 205/1000 | Loss: 0.00001215
Iteration 206/1000 | Loss: 0.00001215
Iteration 207/1000 | Loss: 0.00001215
Iteration 208/1000 | Loss: 0.00001215
Iteration 209/1000 | Loss: 0.00001215
Iteration 210/1000 | Loss: 0.00001215
Iteration 211/1000 | Loss: 0.00001215
Iteration 212/1000 | Loss: 0.00001215
Iteration 213/1000 | Loss: 0.00001215
Iteration 214/1000 | Loss: 0.00001215
Iteration 215/1000 | Loss: 0.00001215
Iteration 216/1000 | Loss: 0.00001215
Iteration 217/1000 | Loss: 0.00001215
Iteration 218/1000 | Loss: 0.00001215
Iteration 219/1000 | Loss: 0.00001215
Iteration 220/1000 | Loss: 0.00001215
Iteration 221/1000 | Loss: 0.00001215
Iteration 222/1000 | Loss: 0.00001215
Iteration 223/1000 | Loss: 0.00001214
Iteration 224/1000 | Loss: 0.00001214
Iteration 225/1000 | Loss: 0.00001214
Iteration 226/1000 | Loss: 0.00001214
Iteration 227/1000 | Loss: 0.00001214
Iteration 228/1000 | Loss: 0.00001214
Iteration 229/1000 | Loss: 0.00001214
Iteration 230/1000 | Loss: 0.00001214
Iteration 231/1000 | Loss: 0.00001214
Iteration 232/1000 | Loss: 0.00001213
Iteration 233/1000 | Loss: 0.00001213
Iteration 234/1000 | Loss: 0.00001213
Iteration 235/1000 | Loss: 0.00001213
Iteration 236/1000 | Loss: 0.00001213
Iteration 237/1000 | Loss: 0.00001213
Iteration 238/1000 | Loss: 0.00001213
Iteration 239/1000 | Loss: 0.00001213
Iteration 240/1000 | Loss: 0.00001213
Iteration 241/1000 | Loss: 0.00001213
Iteration 242/1000 | Loss: 0.00001213
Iteration 243/1000 | Loss: 0.00001213
Iteration 244/1000 | Loss: 0.00001213
Iteration 245/1000 | Loss: 0.00001213
Iteration 246/1000 | Loss: 0.00001213
Iteration 247/1000 | Loss: 0.00001213
Iteration 248/1000 | Loss: 0.00001213
Iteration 249/1000 | Loss: 0.00001213
Iteration 250/1000 | Loss: 0.00001213
Iteration 251/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.2131825315009337e-05, 1.2131825315009337e-05, 1.2131825315009337e-05, 1.2131825315009337e-05, 1.2131825315009337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2131825315009337e-05

Optimization complete. Final v2v error: 3.013986110687256 mm

Highest mean error: 3.3274004459381104 mm for frame 83

Lowest mean error: 2.934864044189453 mm for frame 77

Saving results

Total time: 43.88778495788574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385271
Iteration 2/25 | Loss: 0.00144904
Iteration 3/25 | Loss: 0.00139099
Iteration 4/25 | Loss: 0.00138004
Iteration 5/25 | Loss: 0.00137608
Iteration 6/25 | Loss: 0.00137574
Iteration 7/25 | Loss: 0.00137574
Iteration 8/25 | Loss: 0.00137574
Iteration 9/25 | Loss: 0.00137574
Iteration 10/25 | Loss: 0.00137574
Iteration 11/25 | Loss: 0.00137574
Iteration 12/25 | Loss: 0.00137574
Iteration 13/25 | Loss: 0.00137574
Iteration 14/25 | Loss: 0.00137574
Iteration 15/25 | Loss: 0.00137574
Iteration 16/25 | Loss: 0.00137574
Iteration 17/25 | Loss: 0.00137574
Iteration 18/25 | Loss: 0.00137574
Iteration 19/25 | Loss: 0.00137574
Iteration 20/25 | Loss: 0.00137574
Iteration 21/25 | Loss: 0.00137574
Iteration 22/25 | Loss: 0.00137574
Iteration 23/25 | Loss: 0.00137574
Iteration 24/25 | Loss: 0.00137574
Iteration 25/25 | Loss: 0.00137574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34915698
Iteration 2/25 | Loss: 0.00275458
Iteration 3/25 | Loss: 0.00275457
Iteration 4/25 | Loss: 0.00275457
Iteration 5/25 | Loss: 0.00275457
Iteration 6/25 | Loss: 0.00275457
Iteration 7/25 | Loss: 0.00275457
Iteration 8/25 | Loss: 0.00275457
Iteration 9/25 | Loss: 0.00275457
Iteration 10/25 | Loss: 0.00275457
Iteration 11/25 | Loss: 0.00275457
Iteration 12/25 | Loss: 0.00275457
Iteration 13/25 | Loss: 0.00275457
Iteration 14/25 | Loss: 0.00275457
Iteration 15/25 | Loss: 0.00275457
Iteration 16/25 | Loss: 0.00275457
Iteration 17/25 | Loss: 0.00275457
Iteration 18/25 | Loss: 0.00275457
Iteration 19/25 | Loss: 0.00275457
Iteration 20/25 | Loss: 0.00275457
Iteration 21/25 | Loss: 0.00275457
Iteration 22/25 | Loss: 0.00275457
Iteration 23/25 | Loss: 0.00275457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0027545669581741095, 0.0027545669581741095, 0.0027545669581741095, 0.0027545669581741095, 0.0027545669581741095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027545669581741095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275457
Iteration 2/1000 | Loss: 0.00004220
Iteration 3/1000 | Loss: 0.00002829
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001875
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001638
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001602
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001562
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001561
Iteration 67/1000 | Loss: 0.00001561
Iteration 68/1000 | Loss: 0.00001561
Iteration 69/1000 | Loss: 0.00001561
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001560
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001560
Iteration 77/1000 | Loss: 0.00001560
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001559
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001559
Iteration 86/1000 | Loss: 0.00001559
Iteration 87/1000 | Loss: 0.00001559
Iteration 88/1000 | Loss: 0.00001559
Iteration 89/1000 | Loss: 0.00001559
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.5592784620821476e-05, 1.5592784620821476e-05, 1.5592784620821476e-05, 1.5592784620821476e-05, 1.5592784620821476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5592784620821476e-05

Optimization complete. Final v2v error: 3.3806207180023193 mm

Highest mean error: 3.663071393966675 mm for frame 39

Lowest mean error: 2.8064610958099365 mm for frame 3

Saving results

Total time: 38.91904067993164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874511
Iteration 2/25 | Loss: 0.00150045
Iteration 3/25 | Loss: 0.00141088
Iteration 4/25 | Loss: 0.00139331
Iteration 5/25 | Loss: 0.00138793
Iteration 6/25 | Loss: 0.00138646
Iteration 7/25 | Loss: 0.00138646
Iteration 8/25 | Loss: 0.00138646
Iteration 9/25 | Loss: 0.00138646
Iteration 10/25 | Loss: 0.00138646
Iteration 11/25 | Loss: 0.00138646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013864595675840974, 0.0013864595675840974, 0.0013864595675840974, 0.0013864595675840974, 0.0013864595675840974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013864595675840974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27215683
Iteration 2/25 | Loss: 0.00209889
Iteration 3/25 | Loss: 0.00209888
Iteration 4/25 | Loss: 0.00209888
Iteration 5/25 | Loss: 0.00209888
Iteration 6/25 | Loss: 0.00209888
Iteration 7/25 | Loss: 0.00209888
Iteration 8/25 | Loss: 0.00209888
Iteration 9/25 | Loss: 0.00209888
Iteration 10/25 | Loss: 0.00209888
Iteration 11/25 | Loss: 0.00209888
Iteration 12/25 | Loss: 0.00209888
Iteration 13/25 | Loss: 0.00209888
Iteration 14/25 | Loss: 0.00209888
Iteration 15/25 | Loss: 0.00209888
Iteration 16/25 | Loss: 0.00209888
Iteration 17/25 | Loss: 0.00209888
Iteration 18/25 | Loss: 0.00209888
Iteration 19/25 | Loss: 0.00209888
Iteration 20/25 | Loss: 0.00209888
Iteration 21/25 | Loss: 0.00209888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002098877914249897, 0.002098877914249897, 0.002098877914249897, 0.002098877914249897, 0.002098877914249897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002098877914249897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209888
Iteration 2/1000 | Loss: 0.00004315
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00002164
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001828
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001800
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00001787
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001758
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001748
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001739
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001739
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001739
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001738
Iteration 102/1000 | Loss: 0.00001738
Iteration 103/1000 | Loss: 0.00001738
Iteration 104/1000 | Loss: 0.00001738
Iteration 105/1000 | Loss: 0.00001738
Iteration 106/1000 | Loss: 0.00001738
Iteration 107/1000 | Loss: 0.00001738
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001736
Iteration 114/1000 | Loss: 0.00001736
Iteration 115/1000 | Loss: 0.00001736
Iteration 116/1000 | Loss: 0.00001736
Iteration 117/1000 | Loss: 0.00001736
Iteration 118/1000 | Loss: 0.00001736
Iteration 119/1000 | Loss: 0.00001736
Iteration 120/1000 | Loss: 0.00001736
Iteration 121/1000 | Loss: 0.00001735
Iteration 122/1000 | Loss: 0.00001735
Iteration 123/1000 | Loss: 0.00001735
Iteration 124/1000 | Loss: 0.00001735
Iteration 125/1000 | Loss: 0.00001735
Iteration 126/1000 | Loss: 0.00001735
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001733
Iteration 132/1000 | Loss: 0.00001733
Iteration 133/1000 | Loss: 0.00001733
Iteration 134/1000 | Loss: 0.00001733
Iteration 135/1000 | Loss: 0.00001733
Iteration 136/1000 | Loss: 0.00001733
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001732
Iteration 146/1000 | Loss: 0.00001732
Iteration 147/1000 | Loss: 0.00001732
Iteration 148/1000 | Loss: 0.00001732
Iteration 149/1000 | Loss: 0.00001732
Iteration 150/1000 | Loss: 0.00001732
Iteration 151/1000 | Loss: 0.00001732
Iteration 152/1000 | Loss: 0.00001732
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001732
Iteration 155/1000 | Loss: 0.00001732
Iteration 156/1000 | Loss: 0.00001731
Iteration 157/1000 | Loss: 0.00001731
Iteration 158/1000 | Loss: 0.00001731
Iteration 159/1000 | Loss: 0.00001731
Iteration 160/1000 | Loss: 0.00001731
Iteration 161/1000 | Loss: 0.00001731
Iteration 162/1000 | Loss: 0.00001731
Iteration 163/1000 | Loss: 0.00001731
Iteration 164/1000 | Loss: 0.00001731
Iteration 165/1000 | Loss: 0.00001731
Iteration 166/1000 | Loss: 0.00001731
Iteration 167/1000 | Loss: 0.00001731
Iteration 168/1000 | Loss: 0.00001731
Iteration 169/1000 | Loss: 0.00001731
Iteration 170/1000 | Loss: 0.00001730
Iteration 171/1000 | Loss: 0.00001730
Iteration 172/1000 | Loss: 0.00001730
Iteration 173/1000 | Loss: 0.00001730
Iteration 174/1000 | Loss: 0.00001730
Iteration 175/1000 | Loss: 0.00001730
Iteration 176/1000 | Loss: 0.00001730
Iteration 177/1000 | Loss: 0.00001730
Iteration 178/1000 | Loss: 0.00001730
Iteration 179/1000 | Loss: 0.00001730
Iteration 180/1000 | Loss: 0.00001730
Iteration 181/1000 | Loss: 0.00001729
Iteration 182/1000 | Loss: 0.00001729
Iteration 183/1000 | Loss: 0.00001729
Iteration 184/1000 | Loss: 0.00001729
Iteration 185/1000 | Loss: 0.00001729
Iteration 186/1000 | Loss: 0.00001729
Iteration 187/1000 | Loss: 0.00001729
Iteration 188/1000 | Loss: 0.00001729
Iteration 189/1000 | Loss: 0.00001729
Iteration 190/1000 | Loss: 0.00001729
Iteration 191/1000 | Loss: 0.00001729
Iteration 192/1000 | Loss: 0.00001729
Iteration 193/1000 | Loss: 0.00001729
Iteration 194/1000 | Loss: 0.00001729
Iteration 195/1000 | Loss: 0.00001729
Iteration 196/1000 | Loss: 0.00001729
Iteration 197/1000 | Loss: 0.00001729
Iteration 198/1000 | Loss: 0.00001729
Iteration 199/1000 | Loss: 0.00001728
Iteration 200/1000 | Loss: 0.00001728
Iteration 201/1000 | Loss: 0.00001728
Iteration 202/1000 | Loss: 0.00001728
Iteration 203/1000 | Loss: 0.00001728
Iteration 204/1000 | Loss: 0.00001728
Iteration 205/1000 | Loss: 0.00001728
Iteration 206/1000 | Loss: 0.00001728
Iteration 207/1000 | Loss: 0.00001728
Iteration 208/1000 | Loss: 0.00001728
Iteration 209/1000 | Loss: 0.00001728
Iteration 210/1000 | Loss: 0.00001728
Iteration 211/1000 | Loss: 0.00001728
Iteration 212/1000 | Loss: 0.00001728
Iteration 213/1000 | Loss: 0.00001728
Iteration 214/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.7278302038903348e-05, 1.7278302038903348e-05, 1.7278302038903348e-05, 1.7278302038903348e-05, 1.7278302038903348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7278302038903348e-05

Optimization complete. Final v2v error: 3.5518460273742676 mm

Highest mean error: 5.199406147003174 mm for frame 69

Lowest mean error: 3.1177492141723633 mm for frame 44

Saving results

Total time: 45.615718841552734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042112
Iteration 2/25 | Loss: 0.00181302
Iteration 3/25 | Loss: 0.00153227
Iteration 4/25 | Loss: 0.00151235
Iteration 5/25 | Loss: 0.00150712
Iteration 6/25 | Loss: 0.00150668
Iteration 7/25 | Loss: 0.00150668
Iteration 8/25 | Loss: 0.00150668
Iteration 9/25 | Loss: 0.00150668
Iteration 10/25 | Loss: 0.00150668
Iteration 11/25 | Loss: 0.00150668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015066774794831872, 0.0015066774794831872, 0.0015066774794831872, 0.0015066774794831872, 0.0015066774794831872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015066774794831872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90021306
Iteration 2/25 | Loss: 0.00208831
Iteration 3/25 | Loss: 0.00208831
Iteration 4/25 | Loss: 0.00208831
Iteration 5/25 | Loss: 0.00208831
Iteration 6/25 | Loss: 0.00208831
Iteration 7/25 | Loss: 0.00208831
Iteration 8/25 | Loss: 0.00208831
Iteration 9/25 | Loss: 0.00208831
Iteration 10/25 | Loss: 0.00208831
Iteration 11/25 | Loss: 0.00208831
Iteration 12/25 | Loss: 0.00208831
Iteration 13/25 | Loss: 0.00208831
Iteration 14/25 | Loss: 0.00208831
Iteration 15/25 | Loss: 0.00208831
Iteration 16/25 | Loss: 0.00208831
Iteration 17/25 | Loss: 0.00208831
Iteration 18/25 | Loss: 0.00208831
Iteration 19/25 | Loss: 0.00208831
Iteration 20/25 | Loss: 0.00208831
Iteration 21/25 | Loss: 0.00208831
Iteration 22/25 | Loss: 0.00208831
Iteration 23/25 | Loss: 0.00208831
Iteration 24/25 | Loss: 0.00208831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020883092656731606, 0.0020883092656731606, 0.0020883092656731606, 0.0020883092656731606, 0.0020883092656731606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020883092656731606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208831
Iteration 2/1000 | Loss: 0.00004408
Iteration 3/1000 | Loss: 0.00003298
Iteration 4/1000 | Loss: 0.00003008
Iteration 5/1000 | Loss: 0.00002859
Iteration 6/1000 | Loss: 0.00002758
Iteration 7/1000 | Loss: 0.00002690
Iteration 8/1000 | Loss: 0.00002642
Iteration 9/1000 | Loss: 0.00002596
Iteration 10/1000 | Loss: 0.00002552
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002501
Iteration 13/1000 | Loss: 0.00002486
Iteration 14/1000 | Loss: 0.00002466
Iteration 15/1000 | Loss: 0.00002448
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002419
Iteration 19/1000 | Loss: 0.00002417
Iteration 20/1000 | Loss: 0.00002407
Iteration 21/1000 | Loss: 0.00002406
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002405
Iteration 24/1000 | Loss: 0.00002405
Iteration 25/1000 | Loss: 0.00002400
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002391
Iteration 31/1000 | Loss: 0.00002391
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002389
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002383
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002381
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002381
Iteration 41/1000 | Loss: 0.00002380
Iteration 42/1000 | Loss: 0.00002380
Iteration 43/1000 | Loss: 0.00002380
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002380
Iteration 46/1000 | Loss: 0.00002377
Iteration 47/1000 | Loss: 0.00002377
Iteration 48/1000 | Loss: 0.00002376
Iteration 49/1000 | Loss: 0.00002375
Iteration 50/1000 | Loss: 0.00002375
Iteration 51/1000 | Loss: 0.00002375
Iteration 52/1000 | Loss: 0.00002375
Iteration 53/1000 | Loss: 0.00002375
Iteration 54/1000 | Loss: 0.00002375
Iteration 55/1000 | Loss: 0.00002375
Iteration 56/1000 | Loss: 0.00002375
Iteration 57/1000 | Loss: 0.00002374
Iteration 58/1000 | Loss: 0.00002374
Iteration 59/1000 | Loss: 0.00002374
Iteration 60/1000 | Loss: 0.00002374
Iteration 61/1000 | Loss: 0.00002374
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002374
Iteration 64/1000 | Loss: 0.00002374
Iteration 65/1000 | Loss: 0.00002373
Iteration 66/1000 | Loss: 0.00002373
Iteration 67/1000 | Loss: 0.00002373
Iteration 68/1000 | Loss: 0.00002372
Iteration 69/1000 | Loss: 0.00002372
Iteration 70/1000 | Loss: 0.00002371
Iteration 71/1000 | Loss: 0.00002371
Iteration 72/1000 | Loss: 0.00002371
Iteration 73/1000 | Loss: 0.00002371
Iteration 74/1000 | Loss: 0.00002370
Iteration 75/1000 | Loss: 0.00002370
Iteration 76/1000 | Loss: 0.00002370
Iteration 77/1000 | Loss: 0.00002370
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002370
Iteration 80/1000 | Loss: 0.00002370
Iteration 81/1000 | Loss: 0.00002370
Iteration 82/1000 | Loss: 0.00002370
Iteration 83/1000 | Loss: 0.00002370
Iteration 84/1000 | Loss: 0.00002369
Iteration 85/1000 | Loss: 0.00002369
Iteration 86/1000 | Loss: 0.00002369
Iteration 87/1000 | Loss: 0.00002369
Iteration 88/1000 | Loss: 0.00002369
Iteration 89/1000 | Loss: 0.00002369
Iteration 90/1000 | Loss: 0.00002369
Iteration 91/1000 | Loss: 0.00002369
Iteration 92/1000 | Loss: 0.00002369
Iteration 93/1000 | Loss: 0.00002369
Iteration 94/1000 | Loss: 0.00002368
Iteration 95/1000 | Loss: 0.00002368
Iteration 96/1000 | Loss: 0.00002368
Iteration 97/1000 | Loss: 0.00002368
Iteration 98/1000 | Loss: 0.00002368
Iteration 99/1000 | Loss: 0.00002367
Iteration 100/1000 | Loss: 0.00002367
Iteration 101/1000 | Loss: 0.00002367
Iteration 102/1000 | Loss: 0.00002367
Iteration 103/1000 | Loss: 0.00002367
Iteration 104/1000 | Loss: 0.00002367
Iteration 105/1000 | Loss: 0.00002367
Iteration 106/1000 | Loss: 0.00002367
Iteration 107/1000 | Loss: 0.00002366
Iteration 108/1000 | Loss: 0.00002366
Iteration 109/1000 | Loss: 0.00002366
Iteration 110/1000 | Loss: 0.00002366
Iteration 111/1000 | Loss: 0.00002366
Iteration 112/1000 | Loss: 0.00002366
Iteration 113/1000 | Loss: 0.00002366
Iteration 114/1000 | Loss: 0.00002365
Iteration 115/1000 | Loss: 0.00002365
Iteration 116/1000 | Loss: 0.00002365
Iteration 117/1000 | Loss: 0.00002365
Iteration 118/1000 | Loss: 0.00002365
Iteration 119/1000 | Loss: 0.00002365
Iteration 120/1000 | Loss: 0.00002365
Iteration 121/1000 | Loss: 0.00002365
Iteration 122/1000 | Loss: 0.00002364
Iteration 123/1000 | Loss: 0.00002364
Iteration 124/1000 | Loss: 0.00002364
Iteration 125/1000 | Loss: 0.00002364
Iteration 126/1000 | Loss: 0.00002364
Iteration 127/1000 | Loss: 0.00002364
Iteration 128/1000 | Loss: 0.00002364
Iteration 129/1000 | Loss: 0.00002364
Iteration 130/1000 | Loss: 0.00002364
Iteration 131/1000 | Loss: 0.00002364
Iteration 132/1000 | Loss: 0.00002364
Iteration 133/1000 | Loss: 0.00002364
Iteration 134/1000 | Loss: 0.00002363
Iteration 135/1000 | Loss: 0.00002363
Iteration 136/1000 | Loss: 0.00002363
Iteration 137/1000 | Loss: 0.00002363
Iteration 138/1000 | Loss: 0.00002363
Iteration 139/1000 | Loss: 0.00002363
Iteration 140/1000 | Loss: 0.00002363
Iteration 141/1000 | Loss: 0.00002363
Iteration 142/1000 | Loss: 0.00002363
Iteration 143/1000 | Loss: 0.00002363
Iteration 144/1000 | Loss: 0.00002363
Iteration 145/1000 | Loss: 0.00002363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.3634775061509572e-05, 2.3634775061509572e-05, 2.3634775061509572e-05, 2.3634775061509572e-05, 2.3634775061509572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3634775061509572e-05

Optimization complete. Final v2v error: 4.071244716644287 mm

Highest mean error: 4.827177047729492 mm for frame 138

Lowest mean error: 3.4589273929595947 mm for frame 26

Saving results

Total time: 43.229220151901245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363263
Iteration 2/25 | Loss: 0.00149759
Iteration 3/25 | Loss: 0.00138922
Iteration 4/25 | Loss: 0.00136554
Iteration 5/25 | Loss: 0.00135754
Iteration 6/25 | Loss: 0.00135564
Iteration 7/25 | Loss: 0.00135564
Iteration 8/25 | Loss: 0.00135564
Iteration 9/25 | Loss: 0.00135564
Iteration 10/25 | Loss: 0.00135564
Iteration 11/25 | Loss: 0.00135564
Iteration 12/25 | Loss: 0.00135564
Iteration 13/25 | Loss: 0.00135564
Iteration 14/25 | Loss: 0.00135564
Iteration 15/25 | Loss: 0.00135564
Iteration 16/25 | Loss: 0.00135564
Iteration 17/25 | Loss: 0.00135564
Iteration 18/25 | Loss: 0.00135564
Iteration 19/25 | Loss: 0.00135564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013556421035900712, 0.0013556421035900712, 0.0013556421035900712, 0.0013556421035900712, 0.0013556421035900712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013556421035900712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22334933
Iteration 2/25 | Loss: 0.00319600
Iteration 3/25 | Loss: 0.00319600
Iteration 4/25 | Loss: 0.00319600
Iteration 5/25 | Loss: 0.00319600
Iteration 6/25 | Loss: 0.00319600
Iteration 7/25 | Loss: 0.00319600
Iteration 8/25 | Loss: 0.00319600
Iteration 9/25 | Loss: 0.00319600
Iteration 10/25 | Loss: 0.00319600
Iteration 11/25 | Loss: 0.00319600
Iteration 12/25 | Loss: 0.00319600
Iteration 13/25 | Loss: 0.00319600
Iteration 14/25 | Loss: 0.00319600
Iteration 15/25 | Loss: 0.00319600
Iteration 16/25 | Loss: 0.00319600
Iteration 17/25 | Loss: 0.00319600
Iteration 18/25 | Loss: 0.00319600
Iteration 19/25 | Loss: 0.00319600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0031959975603967905, 0.0031959975603967905, 0.0031959975603967905, 0.0031959975603967905, 0.0031959975603967905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031959975603967905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319600
Iteration 2/1000 | Loss: 0.00005766
Iteration 3/1000 | Loss: 0.00003996
Iteration 4/1000 | Loss: 0.00002961
Iteration 5/1000 | Loss: 0.00002739
Iteration 6/1000 | Loss: 0.00002587
Iteration 7/1000 | Loss: 0.00002452
Iteration 8/1000 | Loss: 0.00002379
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00002285
Iteration 11/1000 | Loss: 0.00002253
Iteration 12/1000 | Loss: 0.00002215
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002196
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00002189
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002163
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002145
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002122
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002115
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002109
Iteration 30/1000 | Loss: 0.00002106
Iteration 31/1000 | Loss: 0.00002101
Iteration 32/1000 | Loss: 0.00002100
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002097
Iteration 35/1000 | Loss: 0.00002096
Iteration 36/1000 | Loss: 0.00002094
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002089
Iteration 43/1000 | Loss: 0.00002089
Iteration 44/1000 | Loss: 0.00002088
Iteration 45/1000 | Loss: 0.00002086
Iteration 46/1000 | Loss: 0.00002086
Iteration 47/1000 | Loss: 0.00002086
Iteration 48/1000 | Loss: 0.00002085
Iteration 49/1000 | Loss: 0.00002085
Iteration 50/1000 | Loss: 0.00002085
Iteration 51/1000 | Loss: 0.00002084
Iteration 52/1000 | Loss: 0.00002084
Iteration 53/1000 | Loss: 0.00002084
Iteration 54/1000 | Loss: 0.00002084
Iteration 55/1000 | Loss: 0.00002084
Iteration 56/1000 | Loss: 0.00002084
Iteration 57/1000 | Loss: 0.00002084
Iteration 58/1000 | Loss: 0.00002084
Iteration 59/1000 | Loss: 0.00002084
Iteration 60/1000 | Loss: 0.00002084
Iteration 61/1000 | Loss: 0.00002084
Iteration 62/1000 | Loss: 0.00002084
Iteration 63/1000 | Loss: 0.00002083
Iteration 64/1000 | Loss: 0.00002083
Iteration 65/1000 | Loss: 0.00002083
Iteration 66/1000 | Loss: 0.00002083
Iteration 67/1000 | Loss: 0.00002083
Iteration 68/1000 | Loss: 0.00002083
Iteration 69/1000 | Loss: 0.00002083
Iteration 70/1000 | Loss: 0.00002083
Iteration 71/1000 | Loss: 0.00002083
Iteration 72/1000 | Loss: 0.00002082
Iteration 73/1000 | Loss: 0.00002082
Iteration 74/1000 | Loss: 0.00002082
Iteration 75/1000 | Loss: 0.00002081
Iteration 76/1000 | Loss: 0.00002081
Iteration 77/1000 | Loss: 0.00002081
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002080
Iteration 81/1000 | Loss: 0.00002080
Iteration 82/1000 | Loss: 0.00002080
Iteration 83/1000 | Loss: 0.00002080
Iteration 84/1000 | Loss: 0.00002080
Iteration 85/1000 | Loss: 0.00002079
Iteration 86/1000 | Loss: 0.00002079
Iteration 87/1000 | Loss: 0.00002078
Iteration 88/1000 | Loss: 0.00002078
Iteration 89/1000 | Loss: 0.00002078
Iteration 90/1000 | Loss: 0.00002078
Iteration 91/1000 | Loss: 0.00002078
Iteration 92/1000 | Loss: 0.00002078
Iteration 93/1000 | Loss: 0.00002077
Iteration 94/1000 | Loss: 0.00002077
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002076
Iteration 97/1000 | Loss: 0.00002076
Iteration 98/1000 | Loss: 0.00002076
Iteration 99/1000 | Loss: 0.00002076
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002075
Iteration 102/1000 | Loss: 0.00002075
Iteration 103/1000 | Loss: 0.00002075
Iteration 104/1000 | Loss: 0.00002075
Iteration 105/1000 | Loss: 0.00002075
Iteration 106/1000 | Loss: 0.00002074
Iteration 107/1000 | Loss: 0.00002074
Iteration 108/1000 | Loss: 0.00002074
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002074
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00002073
Iteration 115/1000 | Loss: 0.00002073
Iteration 116/1000 | Loss: 0.00002073
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002073
Iteration 119/1000 | Loss: 0.00002072
Iteration 120/1000 | Loss: 0.00002072
Iteration 121/1000 | Loss: 0.00002072
Iteration 122/1000 | Loss: 0.00002072
Iteration 123/1000 | Loss: 0.00002072
Iteration 124/1000 | Loss: 0.00002071
Iteration 125/1000 | Loss: 0.00002071
Iteration 126/1000 | Loss: 0.00002071
Iteration 127/1000 | Loss: 0.00002071
Iteration 128/1000 | Loss: 0.00002071
Iteration 129/1000 | Loss: 0.00002071
Iteration 130/1000 | Loss: 0.00002071
Iteration 131/1000 | Loss: 0.00002070
Iteration 132/1000 | Loss: 0.00002070
Iteration 133/1000 | Loss: 0.00002070
Iteration 134/1000 | Loss: 0.00002069
Iteration 135/1000 | Loss: 0.00002069
Iteration 136/1000 | Loss: 0.00002069
Iteration 137/1000 | Loss: 0.00002068
Iteration 138/1000 | Loss: 0.00002068
Iteration 139/1000 | Loss: 0.00002068
Iteration 140/1000 | Loss: 0.00002068
Iteration 141/1000 | Loss: 0.00002068
Iteration 142/1000 | Loss: 0.00002067
Iteration 143/1000 | Loss: 0.00002067
Iteration 144/1000 | Loss: 0.00002067
Iteration 145/1000 | Loss: 0.00002067
Iteration 146/1000 | Loss: 0.00002067
Iteration 147/1000 | Loss: 0.00002067
Iteration 148/1000 | Loss: 0.00002067
Iteration 149/1000 | Loss: 0.00002066
Iteration 150/1000 | Loss: 0.00002066
Iteration 151/1000 | Loss: 0.00002066
Iteration 152/1000 | Loss: 0.00002066
Iteration 153/1000 | Loss: 0.00002065
Iteration 154/1000 | Loss: 0.00002065
Iteration 155/1000 | Loss: 0.00002065
Iteration 156/1000 | Loss: 0.00002064
Iteration 157/1000 | Loss: 0.00002064
Iteration 158/1000 | Loss: 0.00002064
Iteration 159/1000 | Loss: 0.00002063
Iteration 160/1000 | Loss: 0.00002063
Iteration 161/1000 | Loss: 0.00002063
Iteration 162/1000 | Loss: 0.00002063
Iteration 163/1000 | Loss: 0.00002062
Iteration 164/1000 | Loss: 0.00002062
Iteration 165/1000 | Loss: 0.00002062
Iteration 166/1000 | Loss: 0.00002062
Iteration 167/1000 | Loss: 0.00002062
Iteration 168/1000 | Loss: 0.00002062
Iteration 169/1000 | Loss: 0.00002062
Iteration 170/1000 | Loss: 0.00002062
Iteration 171/1000 | Loss: 0.00002062
Iteration 172/1000 | Loss: 0.00002061
Iteration 173/1000 | Loss: 0.00002061
Iteration 174/1000 | Loss: 0.00002061
Iteration 175/1000 | Loss: 0.00002061
Iteration 176/1000 | Loss: 0.00002061
Iteration 177/1000 | Loss: 0.00002061
Iteration 178/1000 | Loss: 0.00002061
Iteration 179/1000 | Loss: 0.00002061
Iteration 180/1000 | Loss: 0.00002061
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002060
Iteration 184/1000 | Loss: 0.00002060
Iteration 185/1000 | Loss: 0.00002060
Iteration 186/1000 | Loss: 0.00002060
Iteration 187/1000 | Loss: 0.00002060
Iteration 188/1000 | Loss: 0.00002060
Iteration 189/1000 | Loss: 0.00002059
Iteration 190/1000 | Loss: 0.00002059
Iteration 191/1000 | Loss: 0.00002059
Iteration 192/1000 | Loss: 0.00002059
Iteration 193/1000 | Loss: 0.00002059
Iteration 194/1000 | Loss: 0.00002058
Iteration 195/1000 | Loss: 0.00002058
Iteration 196/1000 | Loss: 0.00002058
Iteration 197/1000 | Loss: 0.00002058
Iteration 198/1000 | Loss: 0.00002058
Iteration 199/1000 | Loss: 0.00002058
Iteration 200/1000 | Loss: 0.00002058
Iteration 201/1000 | Loss: 0.00002058
Iteration 202/1000 | Loss: 0.00002058
Iteration 203/1000 | Loss: 0.00002058
Iteration 204/1000 | Loss: 0.00002058
Iteration 205/1000 | Loss: 0.00002058
Iteration 206/1000 | Loss: 0.00002058
Iteration 207/1000 | Loss: 0.00002058
Iteration 208/1000 | Loss: 0.00002058
Iteration 209/1000 | Loss: 0.00002058
Iteration 210/1000 | Loss: 0.00002057
Iteration 211/1000 | Loss: 0.00002057
Iteration 212/1000 | Loss: 0.00002057
Iteration 213/1000 | Loss: 0.00002057
Iteration 214/1000 | Loss: 0.00002057
Iteration 215/1000 | Loss: 0.00002057
Iteration 216/1000 | Loss: 0.00002056
Iteration 217/1000 | Loss: 0.00002056
Iteration 218/1000 | Loss: 0.00002056
Iteration 219/1000 | Loss: 0.00002056
Iteration 220/1000 | Loss: 0.00002056
Iteration 221/1000 | Loss: 0.00002056
Iteration 222/1000 | Loss: 0.00002056
Iteration 223/1000 | Loss: 0.00002056
Iteration 224/1000 | Loss: 0.00002055
Iteration 225/1000 | Loss: 0.00002055
Iteration 226/1000 | Loss: 0.00002055
Iteration 227/1000 | Loss: 0.00002055
Iteration 228/1000 | Loss: 0.00002055
Iteration 229/1000 | Loss: 0.00002055
Iteration 230/1000 | Loss: 0.00002055
Iteration 231/1000 | Loss: 0.00002055
Iteration 232/1000 | Loss: 0.00002055
Iteration 233/1000 | Loss: 0.00002055
Iteration 234/1000 | Loss: 0.00002055
Iteration 235/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.0552763089654036e-05, 2.0552763089654036e-05, 2.0552763089654036e-05, 2.0552763089654036e-05, 2.0552763089654036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0552763089654036e-05

Optimization complete. Final v2v error: 3.80474853515625 mm

Highest mean error: 5.000248908996582 mm for frame 125

Lowest mean error: 2.7902166843414307 mm for frame 222

Saving results

Total time: 58.84368109703064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00577803
Iteration 2/25 | Loss: 0.00151549
Iteration 3/25 | Loss: 0.00142813
Iteration 4/25 | Loss: 0.00140519
Iteration 5/25 | Loss: 0.00139579
Iteration 6/25 | Loss: 0.00139498
Iteration 7/25 | Loss: 0.00139498
Iteration 8/25 | Loss: 0.00139498
Iteration 9/25 | Loss: 0.00139498
Iteration 10/25 | Loss: 0.00139498
Iteration 11/25 | Loss: 0.00139498
Iteration 12/25 | Loss: 0.00139498
Iteration 13/25 | Loss: 0.00139498
Iteration 14/25 | Loss: 0.00139498
Iteration 15/25 | Loss: 0.00139498
Iteration 16/25 | Loss: 0.00139498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001394982566125691, 0.001394982566125691, 0.001394982566125691, 0.001394982566125691, 0.001394982566125691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001394982566125691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.56603289
Iteration 2/25 | Loss: 0.00238123
Iteration 3/25 | Loss: 0.00238123
Iteration 4/25 | Loss: 0.00238122
Iteration 5/25 | Loss: 0.00238122
Iteration 6/25 | Loss: 0.00238122
Iteration 7/25 | Loss: 0.00238122
Iteration 8/25 | Loss: 0.00238122
Iteration 9/25 | Loss: 0.00238122
Iteration 10/25 | Loss: 0.00238122
Iteration 11/25 | Loss: 0.00238122
Iteration 12/25 | Loss: 0.00238122
Iteration 13/25 | Loss: 0.00238122
Iteration 14/25 | Loss: 0.00238122
Iteration 15/25 | Loss: 0.00238122
Iteration 16/25 | Loss: 0.00238122
Iteration 17/25 | Loss: 0.00238122
Iteration 18/25 | Loss: 0.00238122
Iteration 19/25 | Loss: 0.00238122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023812223225831985, 0.0023812223225831985, 0.0023812223225831985, 0.0023812223225831985, 0.0023812223225831985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023812223225831985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238122
Iteration 2/1000 | Loss: 0.00003015
Iteration 3/1000 | Loss: 0.00002590
Iteration 4/1000 | Loss: 0.00002404
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002264
Iteration 7/1000 | Loss: 0.00002219
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00002091
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002007
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001989
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001983
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001981
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001980
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001973
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001972
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001964
Iteration 54/1000 | Loss: 0.00001964
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001963
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001962
Iteration 60/1000 | Loss: 0.00001962
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001958
Iteration 69/1000 | Loss: 0.00001958
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001958
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001957
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001956
Iteration 79/1000 | Loss: 0.00001956
Iteration 80/1000 | Loss: 0.00001956
Iteration 81/1000 | Loss: 0.00001956
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001955
Iteration 84/1000 | Loss: 0.00001955
Iteration 85/1000 | Loss: 0.00001955
Iteration 86/1000 | Loss: 0.00001955
Iteration 87/1000 | Loss: 0.00001955
Iteration 88/1000 | Loss: 0.00001955
Iteration 89/1000 | Loss: 0.00001955
Iteration 90/1000 | Loss: 0.00001954
Iteration 91/1000 | Loss: 0.00001954
Iteration 92/1000 | Loss: 0.00001954
Iteration 93/1000 | Loss: 0.00001954
Iteration 94/1000 | Loss: 0.00001954
Iteration 95/1000 | Loss: 0.00001954
Iteration 96/1000 | Loss: 0.00001954
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001954
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001953
Iteration 101/1000 | Loss: 0.00001953
Iteration 102/1000 | Loss: 0.00001953
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001952
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001952
Iteration 107/1000 | Loss: 0.00001952
Iteration 108/1000 | Loss: 0.00001952
Iteration 109/1000 | Loss: 0.00001952
Iteration 110/1000 | Loss: 0.00001952
Iteration 111/1000 | Loss: 0.00001952
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001951
Iteration 114/1000 | Loss: 0.00001951
Iteration 115/1000 | Loss: 0.00001951
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00001951
Iteration 118/1000 | Loss: 0.00001951
Iteration 119/1000 | Loss: 0.00001951
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001950
Iteration 122/1000 | Loss: 0.00001950
Iteration 123/1000 | Loss: 0.00001950
Iteration 124/1000 | Loss: 0.00001950
Iteration 125/1000 | Loss: 0.00001950
Iteration 126/1000 | Loss: 0.00001950
Iteration 127/1000 | Loss: 0.00001950
Iteration 128/1000 | Loss: 0.00001950
Iteration 129/1000 | Loss: 0.00001950
Iteration 130/1000 | Loss: 0.00001950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.949528268596623e-05, 1.949528268596623e-05, 1.949528268596623e-05, 1.949528268596623e-05, 1.949528268596623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.949528268596623e-05

Optimization complete. Final v2v error: 3.7998950481414795 mm

Highest mean error: 4.224953651428223 mm for frame 162

Lowest mean error: 3.46803879737854 mm for frame 36

Saving results

Total time: 43.83897423744202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428200
Iteration 2/25 | Loss: 0.00147350
Iteration 3/25 | Loss: 0.00139226
Iteration 4/25 | Loss: 0.00137569
Iteration 5/25 | Loss: 0.00136951
Iteration 6/25 | Loss: 0.00136868
Iteration 7/25 | Loss: 0.00136868
Iteration 8/25 | Loss: 0.00136868
Iteration 9/25 | Loss: 0.00136868
Iteration 10/25 | Loss: 0.00136868
Iteration 11/25 | Loss: 0.00136868
Iteration 12/25 | Loss: 0.00136868
Iteration 13/25 | Loss: 0.00136868
Iteration 14/25 | Loss: 0.00136868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013686795718967915, 0.0013686795718967915, 0.0013686795718967915, 0.0013686795718967915, 0.0013686795718967915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013686795718967915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21043384
Iteration 2/25 | Loss: 0.00326734
Iteration 3/25 | Loss: 0.00326734
Iteration 4/25 | Loss: 0.00326734
Iteration 5/25 | Loss: 0.00326734
Iteration 6/25 | Loss: 0.00326734
Iteration 7/25 | Loss: 0.00326734
Iteration 8/25 | Loss: 0.00326734
Iteration 9/25 | Loss: 0.00326734
Iteration 10/25 | Loss: 0.00326734
Iteration 11/25 | Loss: 0.00326734
Iteration 12/25 | Loss: 0.00326734
Iteration 13/25 | Loss: 0.00326734
Iteration 14/25 | Loss: 0.00326734
Iteration 15/25 | Loss: 0.00326734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003267340362071991, 0.003267340362071991, 0.003267340362071991, 0.003267340362071991, 0.003267340362071991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003267340362071991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326734
Iteration 2/1000 | Loss: 0.00005405
Iteration 3/1000 | Loss: 0.00003787
Iteration 4/1000 | Loss: 0.00003169
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002604
Iteration 8/1000 | Loss: 0.00002511
Iteration 9/1000 | Loss: 0.00002452
Iteration 10/1000 | Loss: 0.00002406
Iteration 11/1000 | Loss: 0.00002361
Iteration 12/1000 | Loss: 0.00002323
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002236
Iteration 17/1000 | Loss: 0.00002235
Iteration 18/1000 | Loss: 0.00002234
Iteration 19/1000 | Loss: 0.00002228
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002217
Iteration 22/1000 | Loss: 0.00002213
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002207
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002200
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002198
Iteration 32/1000 | Loss: 0.00002198
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002196
Iteration 36/1000 | Loss: 0.00002195
Iteration 37/1000 | Loss: 0.00002195
Iteration 38/1000 | Loss: 0.00002195
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002193
Iteration 42/1000 | Loss: 0.00002192
Iteration 43/1000 | Loss: 0.00002192
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002184
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002183
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002172
Iteration 78/1000 | Loss: 0.00002172
Iteration 79/1000 | Loss: 0.00002171
Iteration 80/1000 | Loss: 0.00002171
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002170
Iteration 83/1000 | Loss: 0.00002169
Iteration 84/1000 | Loss: 0.00002169
Iteration 85/1000 | Loss: 0.00002168
Iteration 86/1000 | Loss: 0.00002168
Iteration 87/1000 | Loss: 0.00002167
Iteration 88/1000 | Loss: 0.00002166
Iteration 89/1000 | Loss: 0.00002166
Iteration 90/1000 | Loss: 0.00002166
Iteration 91/1000 | Loss: 0.00002166
Iteration 92/1000 | Loss: 0.00002166
Iteration 93/1000 | Loss: 0.00002165
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002162
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002162
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002161
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002159
Iteration 106/1000 | Loss: 0.00002159
Iteration 107/1000 | Loss: 0.00002159
Iteration 108/1000 | Loss: 0.00002159
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002158
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002157
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002157
Iteration 118/1000 | Loss: 0.00002157
Iteration 119/1000 | Loss: 0.00002157
Iteration 120/1000 | Loss: 0.00002157
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002155
Iteration 126/1000 | Loss: 0.00002155
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002154
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002154
Iteration 133/1000 | Loss: 0.00002153
Iteration 134/1000 | Loss: 0.00002153
Iteration 135/1000 | Loss: 0.00002153
Iteration 136/1000 | Loss: 0.00002153
Iteration 137/1000 | Loss: 0.00002153
Iteration 138/1000 | Loss: 0.00002153
Iteration 139/1000 | Loss: 0.00002153
Iteration 140/1000 | Loss: 0.00002153
Iteration 141/1000 | Loss: 0.00002153
Iteration 142/1000 | Loss: 0.00002153
Iteration 143/1000 | Loss: 0.00002152
Iteration 144/1000 | Loss: 0.00002152
Iteration 145/1000 | Loss: 0.00002152
Iteration 146/1000 | Loss: 0.00002152
Iteration 147/1000 | Loss: 0.00002152
Iteration 148/1000 | Loss: 0.00002152
Iteration 149/1000 | Loss: 0.00002152
Iteration 150/1000 | Loss: 0.00002152
Iteration 151/1000 | Loss: 0.00002152
Iteration 152/1000 | Loss: 0.00002152
Iteration 153/1000 | Loss: 0.00002151
Iteration 154/1000 | Loss: 0.00002151
Iteration 155/1000 | Loss: 0.00002151
Iteration 156/1000 | Loss: 0.00002151
Iteration 157/1000 | Loss: 0.00002151
Iteration 158/1000 | Loss: 0.00002151
Iteration 159/1000 | Loss: 0.00002151
Iteration 160/1000 | Loss: 0.00002151
Iteration 161/1000 | Loss: 0.00002151
Iteration 162/1000 | Loss: 0.00002151
Iteration 163/1000 | Loss: 0.00002151
Iteration 164/1000 | Loss: 0.00002151
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002151
Iteration 167/1000 | Loss: 0.00002151
Iteration 168/1000 | Loss: 0.00002151
Iteration 169/1000 | Loss: 0.00002150
Iteration 170/1000 | Loss: 0.00002150
Iteration 171/1000 | Loss: 0.00002150
Iteration 172/1000 | Loss: 0.00002150
Iteration 173/1000 | Loss: 0.00002150
Iteration 174/1000 | Loss: 0.00002150
Iteration 175/1000 | Loss: 0.00002150
Iteration 176/1000 | Loss: 0.00002150
Iteration 177/1000 | Loss: 0.00002150
Iteration 178/1000 | Loss: 0.00002150
Iteration 179/1000 | Loss: 0.00002150
Iteration 180/1000 | Loss: 0.00002150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.1503097741515376e-05, 2.1503097741515376e-05, 2.1503097741515376e-05, 2.1503097741515376e-05, 2.1503097741515376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1503097741515376e-05

Optimization complete. Final v2v error: 3.828787326812744 mm

Highest mean error: 4.474435806274414 mm for frame 199

Lowest mean error: 3.1262218952178955 mm for frame 133

Saving results

Total time: 52.43769669532776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00748885
Iteration 2/25 | Loss: 0.00203896
Iteration 3/25 | Loss: 0.00149832
Iteration 4/25 | Loss: 0.00142855
Iteration 5/25 | Loss: 0.00142258
Iteration 6/25 | Loss: 0.00137079
Iteration 7/25 | Loss: 0.00135385
Iteration 8/25 | Loss: 0.00135904
Iteration 9/25 | Loss: 0.00136066
Iteration 10/25 | Loss: 0.00135662
Iteration 11/25 | Loss: 0.00135561
Iteration 12/25 | Loss: 0.00135758
Iteration 13/25 | Loss: 0.00136087
Iteration 14/25 | Loss: 0.00136173
Iteration 15/25 | Loss: 0.00136425
Iteration 16/25 | Loss: 0.00137476
Iteration 17/25 | Loss: 0.00136634
Iteration 18/25 | Loss: 0.00135625
Iteration 19/25 | Loss: 0.00134714
Iteration 20/25 | Loss: 0.00134433
Iteration 21/25 | Loss: 0.00134318
Iteration 22/25 | Loss: 0.00134119
Iteration 23/25 | Loss: 0.00134037
Iteration 24/25 | Loss: 0.00134017
Iteration 25/25 | Loss: 0.00134012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06970406
Iteration 2/25 | Loss: 0.00207669
Iteration 3/25 | Loss: 0.00207669
Iteration 4/25 | Loss: 0.00207669
Iteration 5/25 | Loss: 0.00207668
Iteration 6/25 | Loss: 0.00207668
Iteration 7/25 | Loss: 0.00207668
Iteration 8/25 | Loss: 0.00207668
Iteration 9/25 | Loss: 0.00207668
Iteration 10/25 | Loss: 0.00207668
Iteration 11/25 | Loss: 0.00207668
Iteration 12/25 | Loss: 0.00207668
Iteration 13/25 | Loss: 0.00207668
Iteration 14/25 | Loss: 0.00207668
Iteration 15/25 | Loss: 0.00207668
Iteration 16/25 | Loss: 0.00207668
Iteration 17/25 | Loss: 0.00207668
Iteration 18/25 | Loss: 0.00207668
Iteration 19/25 | Loss: 0.00207668
Iteration 20/25 | Loss: 0.00207668
Iteration 21/25 | Loss: 0.00207668
Iteration 22/25 | Loss: 0.00207668
Iteration 23/25 | Loss: 0.00207668
Iteration 24/25 | Loss: 0.00207668
Iteration 25/25 | Loss: 0.00207668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207668
Iteration 2/1000 | Loss: 0.00006433
Iteration 3/1000 | Loss: 0.00001816
Iteration 4/1000 | Loss: 0.00005962
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00006658
Iteration 7/1000 | Loss: 0.00001524
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001283
Iteration 24/1000 | Loss: 0.00001237
Iteration 25/1000 | Loss: 0.00001228
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001228
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001211
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001208
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001201
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001199
Iteration 128/1000 | Loss: 0.00001199
Iteration 129/1000 | Loss: 0.00001199
Iteration 130/1000 | Loss: 0.00001199
Iteration 131/1000 | Loss: 0.00001199
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001198
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.196922403323697e-05, 1.196922403323697e-05, 1.196922403323697e-05, 1.196922403323697e-05, 1.196922403323697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.196922403323697e-05

Optimization complete. Final v2v error: 3.0174031257629395 mm

Highest mean error: 3.3637824058532715 mm for frame 164

Lowest mean error: 2.7824933528900146 mm for frame 208

Saving results

Total time: 88.16856408119202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860149
Iteration 2/25 | Loss: 0.00163382
Iteration 3/25 | Loss: 0.00141992
Iteration 4/25 | Loss: 0.00140092
Iteration 5/25 | Loss: 0.00139737
Iteration 6/25 | Loss: 0.00139729
Iteration 7/25 | Loss: 0.00139729
Iteration 8/25 | Loss: 0.00139729
Iteration 9/25 | Loss: 0.00139729
Iteration 10/25 | Loss: 0.00139729
Iteration 11/25 | Loss: 0.00139729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013972908491268754, 0.0013972908491268754, 0.0013972908491268754, 0.0013972908491268754, 0.0013972908491268754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013972908491268754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83283371
Iteration 2/25 | Loss: 0.00134045
Iteration 3/25 | Loss: 0.00134044
Iteration 4/25 | Loss: 0.00134044
Iteration 5/25 | Loss: 0.00134044
Iteration 6/25 | Loss: 0.00134044
Iteration 7/25 | Loss: 0.00134044
Iteration 8/25 | Loss: 0.00134044
Iteration 9/25 | Loss: 0.00134044
Iteration 10/25 | Loss: 0.00134044
Iteration 11/25 | Loss: 0.00134044
Iteration 12/25 | Loss: 0.00134044
Iteration 13/25 | Loss: 0.00134044
Iteration 14/25 | Loss: 0.00134044
Iteration 15/25 | Loss: 0.00134044
Iteration 16/25 | Loss: 0.00134044
Iteration 17/25 | Loss: 0.00134044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013404417550191283, 0.0013404417550191283, 0.0013404417550191283, 0.0013404417550191283, 0.0013404417550191283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013404417550191283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134044
Iteration 2/1000 | Loss: 0.00004198
Iteration 3/1000 | Loss: 0.00003467
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00003099
Iteration 6/1000 | Loss: 0.00002998
Iteration 7/1000 | Loss: 0.00002930
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00002861
Iteration 10/1000 | Loss: 0.00002835
Iteration 11/1000 | Loss: 0.00002832
Iteration 12/1000 | Loss: 0.00002808
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00002775
Iteration 15/1000 | Loss: 0.00002771
Iteration 16/1000 | Loss: 0.00002769
Iteration 17/1000 | Loss: 0.00002769
Iteration 18/1000 | Loss: 0.00002767
Iteration 19/1000 | Loss: 0.00002766
Iteration 20/1000 | Loss: 0.00002766
Iteration 21/1000 | Loss: 0.00002765
Iteration 22/1000 | Loss: 0.00002765
Iteration 23/1000 | Loss: 0.00002765
Iteration 24/1000 | Loss: 0.00002765
Iteration 25/1000 | Loss: 0.00002765
Iteration 26/1000 | Loss: 0.00002765
Iteration 27/1000 | Loss: 0.00002765
Iteration 28/1000 | Loss: 0.00002765
Iteration 29/1000 | Loss: 0.00002765
Iteration 30/1000 | Loss: 0.00002764
Iteration 31/1000 | Loss: 0.00002763
Iteration 32/1000 | Loss: 0.00002763
Iteration 33/1000 | Loss: 0.00002762
Iteration 34/1000 | Loss: 0.00002762
Iteration 35/1000 | Loss: 0.00002762
Iteration 36/1000 | Loss: 0.00002762
Iteration 37/1000 | Loss: 0.00002761
Iteration 38/1000 | Loss: 0.00002760
Iteration 39/1000 | Loss: 0.00002759
Iteration 40/1000 | Loss: 0.00002759
Iteration 41/1000 | Loss: 0.00002759
Iteration 42/1000 | Loss: 0.00002759
Iteration 43/1000 | Loss: 0.00002759
Iteration 44/1000 | Loss: 0.00002759
Iteration 45/1000 | Loss: 0.00002759
Iteration 46/1000 | Loss: 0.00002759
Iteration 47/1000 | Loss: 0.00002759
Iteration 48/1000 | Loss: 0.00002759
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00002759
Iteration 51/1000 | Loss: 0.00002759
Iteration 52/1000 | Loss: 0.00002759
Iteration 53/1000 | Loss: 0.00002759
Iteration 54/1000 | Loss: 0.00002759
Iteration 55/1000 | Loss: 0.00002759
Iteration 56/1000 | Loss: 0.00002759
Iteration 57/1000 | Loss: 0.00002759
Iteration 58/1000 | Loss: 0.00002759
Iteration 59/1000 | Loss: 0.00002759
Iteration 60/1000 | Loss: 0.00002759
Iteration 61/1000 | Loss: 0.00002759
Iteration 62/1000 | Loss: 0.00002759
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002759
Iteration 66/1000 | Loss: 0.00002759
Iteration 67/1000 | Loss: 0.00002759
Iteration 68/1000 | Loss: 0.00002759
Iteration 69/1000 | Loss: 0.00002759
Iteration 70/1000 | Loss: 0.00002759
Iteration 71/1000 | Loss: 0.00002759
Iteration 72/1000 | Loss: 0.00002759
Iteration 73/1000 | Loss: 0.00002759
Iteration 74/1000 | Loss: 0.00002759
Iteration 75/1000 | Loss: 0.00002759
Iteration 76/1000 | Loss: 0.00002759
Iteration 77/1000 | Loss: 0.00002759
Iteration 78/1000 | Loss: 0.00002759
Iteration 79/1000 | Loss: 0.00002759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.758632217592094e-05, 2.758632217592094e-05, 2.758632217592094e-05, 2.758632217592094e-05, 2.758632217592094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.758632217592094e-05

Optimization complete. Final v2v error: 4.516728401184082 mm

Highest mean error: 4.818997383117676 mm for frame 107

Lowest mean error: 4.215093612670898 mm for frame 81

Saving results

Total time: 29.36435556411743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774393
Iteration 2/25 | Loss: 0.00156920
Iteration 3/25 | Loss: 0.00146898
Iteration 4/25 | Loss: 0.00146093
Iteration 5/25 | Loss: 0.00146073
Iteration 6/25 | Loss: 0.00146073
Iteration 7/25 | Loss: 0.00146073
Iteration 8/25 | Loss: 0.00146073
Iteration 9/25 | Loss: 0.00146073
Iteration 10/25 | Loss: 0.00146073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014607309130951762, 0.0014607309130951762, 0.0014607309130951762, 0.0014607309130951762, 0.0014607309130951762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014607309130951762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85049343
Iteration 2/25 | Loss: 0.00173595
Iteration 3/25 | Loss: 0.00173593
Iteration 4/25 | Loss: 0.00173593
Iteration 5/25 | Loss: 0.00173592
Iteration 6/25 | Loss: 0.00173592
Iteration 7/25 | Loss: 0.00173592
Iteration 8/25 | Loss: 0.00173592
Iteration 9/25 | Loss: 0.00173592
Iteration 10/25 | Loss: 0.00173592
Iteration 11/25 | Loss: 0.00173592
Iteration 12/25 | Loss: 0.00173592
Iteration 13/25 | Loss: 0.00173592
Iteration 14/25 | Loss: 0.00173592
Iteration 15/25 | Loss: 0.00173592
Iteration 16/25 | Loss: 0.00173592
Iteration 17/25 | Loss: 0.00173592
Iteration 18/25 | Loss: 0.00173592
Iteration 19/25 | Loss: 0.00173592
Iteration 20/25 | Loss: 0.00173592
Iteration 21/25 | Loss: 0.00173592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017359215999022126, 0.0017359215999022126, 0.0017359215999022126, 0.0017359215999022126, 0.0017359215999022126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017359215999022126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173592
Iteration 2/1000 | Loss: 0.00003152
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00002040
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001971
Iteration 9/1000 | Loss: 0.00001937
Iteration 10/1000 | Loss: 0.00001905
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001881
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001819
Iteration 21/1000 | Loss: 0.00001819
Iteration 22/1000 | Loss: 0.00001814
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001801
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001790
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001788
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001786
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001784
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001784
Iteration 85/1000 | Loss: 0.00001784
Iteration 86/1000 | Loss: 0.00001784
Iteration 87/1000 | Loss: 0.00001784
Iteration 88/1000 | Loss: 0.00001784
Iteration 89/1000 | Loss: 0.00001784
Iteration 90/1000 | Loss: 0.00001784
Iteration 91/1000 | Loss: 0.00001784
Iteration 92/1000 | Loss: 0.00001784
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.783844163583126e-05, 1.783844163583126e-05, 1.783844163583126e-05, 1.783844163583126e-05, 1.783844163583126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.783844163583126e-05

Optimization complete. Final v2v error: 3.523785352706909 mm

Highest mean error: 3.917008638381958 mm for frame 173

Lowest mean error: 3.27260684967041 mm for frame 52

Saving results

Total time: 39.52317810058594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810650
Iteration 2/25 | Loss: 0.00179694
Iteration 3/25 | Loss: 0.00150329
Iteration 4/25 | Loss: 0.00148350
Iteration 5/25 | Loss: 0.00147939
Iteration 6/25 | Loss: 0.00147316
Iteration 7/25 | Loss: 0.00146937
Iteration 8/25 | Loss: 0.00146913
Iteration 9/25 | Loss: 0.00146902
Iteration 10/25 | Loss: 0.00146902
Iteration 11/25 | Loss: 0.00146901
Iteration 12/25 | Loss: 0.00146901
Iteration 13/25 | Loss: 0.00146901
Iteration 14/25 | Loss: 0.00146901
Iteration 15/25 | Loss: 0.00146901
Iteration 16/25 | Loss: 0.00146901
Iteration 17/25 | Loss: 0.00146901
Iteration 18/25 | Loss: 0.00146901
Iteration 19/25 | Loss: 0.00146901
Iteration 20/25 | Loss: 0.00146901
Iteration 21/25 | Loss: 0.00146901
Iteration 22/25 | Loss: 0.00146900
Iteration 23/25 | Loss: 0.00146900
Iteration 24/25 | Loss: 0.00146900
Iteration 25/25 | Loss: 0.00146900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16154039
Iteration 2/25 | Loss: 0.00189687
Iteration 3/25 | Loss: 0.00189686
Iteration 4/25 | Loss: 0.00189686
Iteration 5/25 | Loss: 0.00189686
Iteration 6/25 | Loss: 0.00189686
Iteration 7/25 | Loss: 0.00189686
Iteration 8/25 | Loss: 0.00189686
Iteration 9/25 | Loss: 0.00189686
Iteration 10/25 | Loss: 0.00189686
Iteration 11/25 | Loss: 0.00189686
Iteration 12/25 | Loss: 0.00189686
Iteration 13/25 | Loss: 0.00189686
Iteration 14/25 | Loss: 0.00189686
Iteration 15/25 | Loss: 0.00189686
Iteration 16/25 | Loss: 0.00189686
Iteration 17/25 | Loss: 0.00189686
Iteration 18/25 | Loss: 0.00189686
Iteration 19/25 | Loss: 0.00189686
Iteration 20/25 | Loss: 0.00189686
Iteration 21/25 | Loss: 0.00189686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018968589138239622, 0.0018968589138239622, 0.0018968589138239622, 0.0018968589138239622, 0.0018968589138239622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018968589138239622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189686
Iteration 2/1000 | Loss: 0.00004005
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002056
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001936
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001833
Iteration 12/1000 | Loss: 0.00001817
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001776
Iteration 48/1000 | Loss: 0.00001776
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.7687329091131687e-05, 1.7687329091131687e-05, 1.7687329091131687e-05, 1.7687329091131687e-05, 1.7687329091131687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7687329091131687e-05

Optimization complete. Final v2v error: 3.5778441429138184 mm

Highest mean error: 4.229179382324219 mm for frame 195

Lowest mean error: 3.2233707904815674 mm for frame 232

Saving results

Total time: 45.400209188461304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387092
Iteration 2/25 | Loss: 0.00169832
Iteration 3/25 | Loss: 0.00142227
Iteration 4/25 | Loss: 0.00138648
Iteration 5/25 | Loss: 0.00137789
Iteration 6/25 | Loss: 0.00137571
Iteration 7/25 | Loss: 0.00137502
Iteration 8/25 | Loss: 0.00137502
Iteration 9/25 | Loss: 0.00137502
Iteration 10/25 | Loss: 0.00137502
Iteration 11/25 | Loss: 0.00137502
Iteration 12/25 | Loss: 0.00137502
Iteration 13/25 | Loss: 0.00137502
Iteration 14/25 | Loss: 0.00137502
Iteration 15/25 | Loss: 0.00137502
Iteration 16/25 | Loss: 0.00137502
Iteration 17/25 | Loss: 0.00137502
Iteration 18/25 | Loss: 0.00137502
Iteration 19/25 | Loss: 0.00137502
Iteration 20/25 | Loss: 0.00137502
Iteration 21/25 | Loss: 0.00137502
Iteration 22/25 | Loss: 0.00137502
Iteration 23/25 | Loss: 0.00137502
Iteration 24/25 | Loss: 0.00137502
Iteration 25/25 | Loss: 0.00137502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19534636
Iteration 2/25 | Loss: 0.00197282
Iteration 3/25 | Loss: 0.00197282
Iteration 4/25 | Loss: 0.00197282
Iteration 5/25 | Loss: 0.00197282
Iteration 6/25 | Loss: 0.00197282
Iteration 7/25 | Loss: 0.00197282
Iteration 8/25 | Loss: 0.00197282
Iteration 9/25 | Loss: 0.00197282
Iteration 10/25 | Loss: 0.00197282
Iteration 11/25 | Loss: 0.00197282
Iteration 12/25 | Loss: 0.00197282
Iteration 13/25 | Loss: 0.00197282
Iteration 14/25 | Loss: 0.00197282
Iteration 15/25 | Loss: 0.00197282
Iteration 16/25 | Loss: 0.00197282
Iteration 17/25 | Loss: 0.00197282
Iteration 18/25 | Loss: 0.00197282
Iteration 19/25 | Loss: 0.00197282
Iteration 20/25 | Loss: 0.00197282
Iteration 21/25 | Loss: 0.00197282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019728210754692554, 0.0019728210754692554, 0.0019728210754692554, 0.0019728210754692554, 0.0019728210754692554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019728210754692554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197282
Iteration 2/1000 | Loss: 0.00005371
Iteration 3/1000 | Loss: 0.00002770
Iteration 4/1000 | Loss: 0.00002220
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001712
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001473
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001457
Iteration 49/1000 | Loss: 0.00001457
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001452
Iteration 95/1000 | Loss: 0.00001452
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001449
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001449
Iteration 148/1000 | Loss: 0.00001449
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001449
Iteration 151/1000 | Loss: 0.00001449
Iteration 152/1000 | Loss: 0.00001449
Iteration 153/1000 | Loss: 0.00001449
Iteration 154/1000 | Loss: 0.00001449
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001448
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001448
Iteration 164/1000 | Loss: 0.00001448
Iteration 165/1000 | Loss: 0.00001448
Iteration 166/1000 | Loss: 0.00001448
Iteration 167/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.44824807648547e-05, 1.44824807648547e-05, 1.44824807648547e-05, 1.44824807648547e-05, 1.44824807648547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.44824807648547e-05

Optimization complete. Final v2v error: 3.252415895462036 mm

Highest mean error: 3.458840847015381 mm for frame 103

Lowest mean error: 3.1148746013641357 mm for frame 4

Saving results

Total time: 38.11187243461609
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112200
Iteration 2/25 | Loss: 0.01112200
Iteration 3/25 | Loss: 0.01112200
Iteration 4/25 | Loss: 0.01112200
Iteration 5/25 | Loss: 0.01112200
Iteration 6/25 | Loss: 0.01112200
Iteration 7/25 | Loss: 0.01112200
Iteration 8/25 | Loss: 0.01112200
Iteration 9/25 | Loss: 0.01112200
Iteration 10/25 | Loss: 0.01112200
Iteration 11/25 | Loss: 0.01112200
Iteration 12/25 | Loss: 0.01112200
Iteration 13/25 | Loss: 0.01112200
Iteration 14/25 | Loss: 0.01112200
Iteration 15/25 | Loss: 0.01112200
Iteration 16/25 | Loss: 0.01112199
Iteration 17/25 | Loss: 0.01112199
Iteration 18/25 | Loss: 0.01112199
Iteration 19/25 | Loss: 0.01112199
Iteration 20/25 | Loss: 0.01112199
Iteration 21/25 | Loss: 0.01112199
Iteration 22/25 | Loss: 0.01112199
Iteration 23/25 | Loss: 0.01112199
Iteration 24/25 | Loss: 0.01112199
Iteration 25/25 | Loss: 0.01112199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.56540585
Iteration 2/25 | Loss: 0.18796675
Iteration 3/25 | Loss: 0.18747213
Iteration 4/25 | Loss: 0.18747211
Iteration 5/25 | Loss: 0.18747211
Iteration 6/25 | Loss: 0.18747211
Iteration 7/25 | Loss: 0.18747206
Iteration 8/25 | Loss: 0.18747206
Iteration 9/25 | Loss: 0.18747206
Iteration 10/25 | Loss: 0.18747206
Iteration 11/25 | Loss: 0.18747203
Iteration 12/25 | Loss: 0.18747205
Iteration 13/25 | Loss: 0.18747203
Iteration 14/25 | Loss: 0.18747203
Iteration 15/25 | Loss: 0.18747203
Iteration 16/25 | Loss: 0.18747203
Iteration 17/25 | Loss: 0.18747203
Iteration 18/25 | Loss: 0.18747203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.18747203052043915, 0.18747203052043915, 0.18747203052043915, 0.18747203052043915, 0.18747203052043915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18747203052043915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18747203
Iteration 2/1000 | Loss: 0.00656755
Iteration 3/1000 | Loss: 0.00169558
Iteration 4/1000 | Loss: 0.00030594
Iteration 5/1000 | Loss: 0.00060647
Iteration 6/1000 | Loss: 0.00025288
Iteration 7/1000 | Loss: 0.00006270
Iteration 8/1000 | Loss: 0.00005957
Iteration 9/1000 | Loss: 0.00014470
Iteration 10/1000 | Loss: 0.00003686
Iteration 11/1000 | Loss: 0.00003417
Iteration 12/1000 | Loss: 0.00008267
Iteration 13/1000 | Loss: 0.00002782
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00005169
Iteration 16/1000 | Loss: 0.00002455
Iteration 17/1000 | Loss: 0.00009757
Iteration 18/1000 | Loss: 0.00002440
Iteration 19/1000 | Loss: 0.00006120
Iteration 20/1000 | Loss: 0.00003004
Iteration 21/1000 | Loss: 0.00002224
Iteration 22/1000 | Loss: 0.00012830
Iteration 23/1000 | Loss: 0.00002543
Iteration 24/1000 | Loss: 0.00002885
Iteration 25/1000 | Loss: 0.00004103
Iteration 26/1000 | Loss: 0.00005480
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00001987
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00002133
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00004501
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00007725
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00004031
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001805
Iteration 42/1000 | Loss: 0.00001805
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001804
Iteration 46/1000 | Loss: 0.00001804
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001804
Iteration 49/1000 | Loss: 0.00001804
Iteration 50/1000 | Loss: 0.00001804
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001803
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00005644
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00005195
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001782
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001781
Iteration 74/1000 | Loss: 0.00001781
Iteration 75/1000 | Loss: 0.00001781
Iteration 76/1000 | Loss: 0.00001781
Iteration 77/1000 | Loss: 0.00001781
Iteration 78/1000 | Loss: 0.00001781
Iteration 79/1000 | Loss: 0.00001781
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001779
Iteration 86/1000 | Loss: 0.00001779
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Iteration 89/1000 | Loss: 0.00001778
Iteration 90/1000 | Loss: 0.00001777
Iteration 91/1000 | Loss: 0.00001777
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001775
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001773
Iteration 110/1000 | Loss: 0.00005462
Iteration 111/1000 | Loss: 0.00005108
Iteration 112/1000 | Loss: 0.00004088
Iteration 113/1000 | Loss: 0.00002338
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001913
Iteration 116/1000 | Loss: 0.00001913
Iteration 117/1000 | Loss: 0.00004609
Iteration 118/1000 | Loss: 0.00006216
Iteration 119/1000 | Loss: 0.00032053
Iteration 120/1000 | Loss: 0.00004808
Iteration 121/1000 | Loss: 0.00001807
Iteration 122/1000 | Loss: 0.00004851
Iteration 123/1000 | Loss: 0.00001777
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.763719046721235e-05, 1.763719046721235e-05, 1.763719046721235e-05, 1.763719046721235e-05, 1.763719046721235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.763719046721235e-05

Optimization complete. Final v2v error: 3.5206139087677 mm

Highest mean error: 9.107009887695312 mm for frame 140

Lowest mean error: 3.2633841037750244 mm for frame 211

Saving results

Total time: 101.85345029830933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567106
Iteration 2/25 | Loss: 0.00156501
Iteration 3/25 | Loss: 0.00145008
Iteration 4/25 | Loss: 0.00143292
Iteration 5/25 | Loss: 0.00142731
Iteration 6/25 | Loss: 0.00142685
Iteration 7/25 | Loss: 0.00142685
Iteration 8/25 | Loss: 0.00142685
Iteration 9/25 | Loss: 0.00142685
Iteration 10/25 | Loss: 0.00142685
Iteration 11/25 | Loss: 0.00142685
Iteration 12/25 | Loss: 0.00142685
Iteration 13/25 | Loss: 0.00142685
Iteration 14/25 | Loss: 0.00142685
Iteration 15/25 | Loss: 0.00142685
Iteration 16/25 | Loss: 0.00142685
Iteration 17/25 | Loss: 0.00142685
Iteration 18/25 | Loss: 0.00142685
Iteration 19/25 | Loss: 0.00142685
Iteration 20/25 | Loss: 0.00142685
Iteration 21/25 | Loss: 0.00142685
Iteration 22/25 | Loss: 0.00142685
Iteration 23/25 | Loss: 0.00142685
Iteration 24/25 | Loss: 0.00142685
Iteration 25/25 | Loss: 0.00142685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21792662
Iteration 2/25 | Loss: 0.00290646
Iteration 3/25 | Loss: 0.00290641
Iteration 4/25 | Loss: 0.00290641
Iteration 5/25 | Loss: 0.00290641
Iteration 6/25 | Loss: 0.00290641
Iteration 7/25 | Loss: 0.00290641
Iteration 8/25 | Loss: 0.00290641
Iteration 9/25 | Loss: 0.00290641
Iteration 10/25 | Loss: 0.00290641
Iteration 11/25 | Loss: 0.00290641
Iteration 12/25 | Loss: 0.00290641
Iteration 13/25 | Loss: 0.00290641
Iteration 14/25 | Loss: 0.00290641
Iteration 15/25 | Loss: 0.00290641
Iteration 16/25 | Loss: 0.00290641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029064100235700607, 0.0029064100235700607, 0.0029064100235700607, 0.0029064100235700607, 0.0029064100235700607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029064100235700607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290641
Iteration 2/1000 | Loss: 0.00004770
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00002595
Iteration 6/1000 | Loss: 0.00002466
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002312
Iteration 9/1000 | Loss: 0.00002253
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002068
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002045
Iteration 19/1000 | Loss: 0.00002032
Iteration 20/1000 | Loss: 0.00002028
Iteration 21/1000 | Loss: 0.00002028
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002026
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002022
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002019
Iteration 34/1000 | Loss: 0.00002017
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002016
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002012
Iteration 42/1000 | Loss: 0.00002012
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002010
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002010
Iteration 49/1000 | Loss: 0.00002009
Iteration 50/1000 | Loss: 0.00002009
Iteration 51/1000 | Loss: 0.00002008
Iteration 52/1000 | Loss: 0.00002008
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002007
Iteration 55/1000 | Loss: 0.00002007
Iteration 56/1000 | Loss: 0.00002007
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002006
Iteration 61/1000 | Loss: 0.00002006
Iteration 62/1000 | Loss: 0.00002006
Iteration 63/1000 | Loss: 0.00002005
Iteration 64/1000 | Loss: 0.00002004
Iteration 65/1000 | Loss: 0.00002004
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002004
Iteration 71/1000 | Loss: 0.00002004
Iteration 72/1000 | Loss: 0.00002004
Iteration 73/1000 | Loss: 0.00002004
Iteration 74/1000 | Loss: 0.00002004
Iteration 75/1000 | Loss: 0.00002004
Iteration 76/1000 | Loss: 0.00002004
Iteration 77/1000 | Loss: 0.00002004
Iteration 78/1000 | Loss: 0.00002004
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002004
Iteration 82/1000 | Loss: 0.00002004
Iteration 83/1000 | Loss: 0.00002004
Iteration 84/1000 | Loss: 0.00002004
Iteration 85/1000 | Loss: 0.00002004
Iteration 86/1000 | Loss: 0.00002004
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.0036413843627088e-05, 2.0036413843627088e-05, 2.0036413843627088e-05, 2.0036413843627088e-05, 2.0036413843627088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0036413843627088e-05

Optimization complete. Final v2v error: 3.801356077194214 mm

Highest mean error: 4.419741630554199 mm for frame 14

Lowest mean error: 3.420131206512451 mm for frame 114

Saving results

Total time: 42.77090811729431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441885
Iteration 2/25 | Loss: 0.00143565
Iteration 3/25 | Loss: 0.00137833
Iteration 4/25 | Loss: 0.00136906
Iteration 5/25 | Loss: 0.00136624
Iteration 6/25 | Loss: 0.00136586
Iteration 7/25 | Loss: 0.00136586
Iteration 8/25 | Loss: 0.00136586
Iteration 9/25 | Loss: 0.00136586
Iteration 10/25 | Loss: 0.00136586
Iteration 11/25 | Loss: 0.00136586
Iteration 12/25 | Loss: 0.00136586
Iteration 13/25 | Loss: 0.00136586
Iteration 14/25 | Loss: 0.00136586
Iteration 15/25 | Loss: 0.00136586
Iteration 16/25 | Loss: 0.00136586
Iteration 17/25 | Loss: 0.00136586
Iteration 18/25 | Loss: 0.00136586
Iteration 19/25 | Loss: 0.00136586
Iteration 20/25 | Loss: 0.00136586
Iteration 21/25 | Loss: 0.00136586
Iteration 22/25 | Loss: 0.00136586
Iteration 23/25 | Loss: 0.00136586
Iteration 24/25 | Loss: 0.00136586
Iteration 25/25 | Loss: 0.00136586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41802073
Iteration 2/25 | Loss: 0.00216235
Iteration 3/25 | Loss: 0.00216235
Iteration 4/25 | Loss: 0.00216235
Iteration 5/25 | Loss: 0.00216235
Iteration 6/25 | Loss: 0.00216235
Iteration 7/25 | Loss: 0.00216235
Iteration 8/25 | Loss: 0.00216235
Iteration 9/25 | Loss: 0.00216235
Iteration 10/25 | Loss: 0.00216235
Iteration 11/25 | Loss: 0.00216235
Iteration 12/25 | Loss: 0.00216235
Iteration 13/25 | Loss: 0.00216235
Iteration 14/25 | Loss: 0.00216235
Iteration 15/25 | Loss: 0.00216235
Iteration 16/25 | Loss: 0.00216235
Iteration 17/25 | Loss: 0.00216235
Iteration 18/25 | Loss: 0.00216235
Iteration 19/25 | Loss: 0.00216235
Iteration 20/25 | Loss: 0.00216235
Iteration 21/25 | Loss: 0.00216235
Iteration 22/25 | Loss: 0.00216235
Iteration 23/25 | Loss: 0.00216235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002162348246201873, 0.002162348246201873, 0.002162348246201873, 0.002162348246201873, 0.002162348246201873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002162348246201873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216235
Iteration 2/1000 | Loss: 0.00002791
Iteration 3/1000 | Loss: 0.00002126
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001479
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001446
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001427
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001382
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001373
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001357
Iteration 74/1000 | Loss: 0.00001357
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001351
Iteration 119/1000 | Loss: 0.00001351
Iteration 120/1000 | Loss: 0.00001351
Iteration 121/1000 | Loss: 0.00001351
Iteration 122/1000 | Loss: 0.00001351
Iteration 123/1000 | Loss: 0.00001351
Iteration 124/1000 | Loss: 0.00001351
Iteration 125/1000 | Loss: 0.00001351
Iteration 126/1000 | Loss: 0.00001351
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Iteration 154/1000 | Loss: 0.00001347
Iteration 155/1000 | Loss: 0.00001347
Iteration 156/1000 | Loss: 0.00001347
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001346
Iteration 165/1000 | Loss: 0.00001346
Iteration 166/1000 | Loss: 0.00001346
Iteration 167/1000 | Loss: 0.00001346
Iteration 168/1000 | Loss: 0.00001346
Iteration 169/1000 | Loss: 0.00001346
Iteration 170/1000 | Loss: 0.00001346
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001345
Iteration 176/1000 | Loss: 0.00001345
Iteration 177/1000 | Loss: 0.00001345
Iteration 178/1000 | Loss: 0.00001345
Iteration 179/1000 | Loss: 0.00001345
Iteration 180/1000 | Loss: 0.00001345
Iteration 181/1000 | Loss: 0.00001345
Iteration 182/1000 | Loss: 0.00001345
Iteration 183/1000 | Loss: 0.00001345
Iteration 184/1000 | Loss: 0.00001345
Iteration 185/1000 | Loss: 0.00001344
Iteration 186/1000 | Loss: 0.00001344
Iteration 187/1000 | Loss: 0.00001344
Iteration 188/1000 | Loss: 0.00001344
Iteration 189/1000 | Loss: 0.00001344
Iteration 190/1000 | Loss: 0.00001344
Iteration 191/1000 | Loss: 0.00001344
Iteration 192/1000 | Loss: 0.00001344
Iteration 193/1000 | Loss: 0.00001344
Iteration 194/1000 | Loss: 0.00001343
Iteration 195/1000 | Loss: 0.00001343
Iteration 196/1000 | Loss: 0.00001343
Iteration 197/1000 | Loss: 0.00001343
Iteration 198/1000 | Loss: 0.00001343
Iteration 199/1000 | Loss: 0.00001343
Iteration 200/1000 | Loss: 0.00001343
Iteration 201/1000 | Loss: 0.00001343
Iteration 202/1000 | Loss: 0.00001343
Iteration 203/1000 | Loss: 0.00001343
Iteration 204/1000 | Loss: 0.00001343
Iteration 205/1000 | Loss: 0.00001343
Iteration 206/1000 | Loss: 0.00001343
Iteration 207/1000 | Loss: 0.00001343
Iteration 208/1000 | Loss: 0.00001343
Iteration 209/1000 | Loss: 0.00001343
Iteration 210/1000 | Loss: 0.00001343
Iteration 211/1000 | Loss: 0.00001343
Iteration 212/1000 | Loss: 0.00001343
Iteration 213/1000 | Loss: 0.00001343
Iteration 214/1000 | Loss: 0.00001343
Iteration 215/1000 | Loss: 0.00001343
Iteration 216/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3431796105578542e-05, 1.3431796105578542e-05, 1.3431796105578542e-05, 1.3431796105578542e-05, 1.3431796105578542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3431796105578542e-05

Optimization complete. Final v2v error: 3.1759419441223145 mm

Highest mean error: 3.5316014289855957 mm for frame 113

Lowest mean error: 3.083644390106201 mm for frame 89

Saving results

Total time: 42.68634390830994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411579
Iteration 2/25 | Loss: 0.00146735
Iteration 3/25 | Loss: 0.00138886
Iteration 4/25 | Loss: 0.00137411
Iteration 5/25 | Loss: 0.00136956
Iteration 6/25 | Loss: 0.00136834
Iteration 7/25 | Loss: 0.00136834
Iteration 8/25 | Loss: 0.00136834
Iteration 9/25 | Loss: 0.00136834
Iteration 10/25 | Loss: 0.00136834
Iteration 11/25 | Loss: 0.00136834
Iteration 12/25 | Loss: 0.00136834
Iteration 13/25 | Loss: 0.00136834
Iteration 14/25 | Loss: 0.00136834
Iteration 15/25 | Loss: 0.00136834
Iteration 16/25 | Loss: 0.00136834
Iteration 17/25 | Loss: 0.00136834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013683437136933208, 0.0013683437136933208, 0.0013683437136933208, 0.0013683437136933208, 0.0013683437136933208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013683437136933208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24656725
Iteration 2/25 | Loss: 0.00243484
Iteration 3/25 | Loss: 0.00243482
Iteration 4/25 | Loss: 0.00243482
Iteration 5/25 | Loss: 0.00243482
Iteration 6/25 | Loss: 0.00243481
Iteration 7/25 | Loss: 0.00243481
Iteration 8/25 | Loss: 0.00243481
Iteration 9/25 | Loss: 0.00243481
Iteration 10/25 | Loss: 0.00243481
Iteration 11/25 | Loss: 0.00243481
Iteration 12/25 | Loss: 0.00243481
Iteration 13/25 | Loss: 0.00243481
Iteration 14/25 | Loss: 0.00243481
Iteration 15/25 | Loss: 0.00243481
Iteration 16/25 | Loss: 0.00243481
Iteration 17/25 | Loss: 0.00243481
Iteration 18/25 | Loss: 0.00243481
Iteration 19/25 | Loss: 0.00243481
Iteration 20/25 | Loss: 0.00243481
Iteration 21/25 | Loss: 0.00243481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024348138831555843, 0.0024348138831555843, 0.0024348138831555843, 0.0024348138831555843, 0.0024348138831555843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024348138831555843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243481
Iteration 2/1000 | Loss: 0.00005769
Iteration 3/1000 | Loss: 0.00003853
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002252
Iteration 7/1000 | Loss: 0.00002086
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001880
Iteration 11/1000 | Loss: 0.00001842
Iteration 12/1000 | Loss: 0.00001817
Iteration 13/1000 | Loss: 0.00001781
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001757
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001706
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001700
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001698
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001694
Iteration 48/1000 | Loss: 0.00001694
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001680
Iteration 66/1000 | Loss: 0.00001680
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001679
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001679
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001678
Iteration 73/1000 | Loss: 0.00001678
Iteration 74/1000 | Loss: 0.00001678
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001676
Iteration 82/1000 | Loss: 0.00001676
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001669
Iteration 99/1000 | Loss: 0.00001669
Iteration 100/1000 | Loss: 0.00001669
Iteration 101/1000 | Loss: 0.00001668
Iteration 102/1000 | Loss: 0.00001668
Iteration 103/1000 | Loss: 0.00001668
Iteration 104/1000 | Loss: 0.00001668
Iteration 105/1000 | Loss: 0.00001668
Iteration 106/1000 | Loss: 0.00001668
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001667
Iteration 113/1000 | Loss: 0.00001667
Iteration 114/1000 | Loss: 0.00001667
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001664
Iteration 119/1000 | Loss: 0.00001664
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001661
Iteration 130/1000 | Loss: 0.00001661
Iteration 131/1000 | Loss: 0.00001661
Iteration 132/1000 | Loss: 0.00001661
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001658
Iteration 154/1000 | Loss: 0.00001658
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001658
Iteration 158/1000 | Loss: 0.00001658
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001657
Iteration 162/1000 | Loss: 0.00001657
Iteration 163/1000 | Loss: 0.00001657
Iteration 164/1000 | Loss: 0.00001657
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001657
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001656
Iteration 171/1000 | Loss: 0.00001656
Iteration 172/1000 | Loss: 0.00001656
Iteration 173/1000 | Loss: 0.00001656
Iteration 174/1000 | Loss: 0.00001656
Iteration 175/1000 | Loss: 0.00001656
Iteration 176/1000 | Loss: 0.00001656
Iteration 177/1000 | Loss: 0.00001656
Iteration 178/1000 | Loss: 0.00001656
Iteration 179/1000 | Loss: 0.00001656
Iteration 180/1000 | Loss: 0.00001656
Iteration 181/1000 | Loss: 0.00001656
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001655
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001654
Iteration 193/1000 | Loss: 0.00001654
Iteration 194/1000 | Loss: 0.00001654
Iteration 195/1000 | Loss: 0.00001653
Iteration 196/1000 | Loss: 0.00001653
Iteration 197/1000 | Loss: 0.00001653
Iteration 198/1000 | Loss: 0.00001653
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001653
Iteration 201/1000 | Loss: 0.00001653
Iteration 202/1000 | Loss: 0.00001653
Iteration 203/1000 | Loss: 0.00001652
Iteration 204/1000 | Loss: 0.00001652
Iteration 205/1000 | Loss: 0.00001652
Iteration 206/1000 | Loss: 0.00001652
Iteration 207/1000 | Loss: 0.00001652
Iteration 208/1000 | Loss: 0.00001652
Iteration 209/1000 | Loss: 0.00001652
Iteration 210/1000 | Loss: 0.00001652
Iteration 211/1000 | Loss: 0.00001651
Iteration 212/1000 | Loss: 0.00001651
Iteration 213/1000 | Loss: 0.00001651
Iteration 214/1000 | Loss: 0.00001651
Iteration 215/1000 | Loss: 0.00001650
Iteration 216/1000 | Loss: 0.00001650
Iteration 217/1000 | Loss: 0.00001650
Iteration 218/1000 | Loss: 0.00001650
Iteration 219/1000 | Loss: 0.00001650
Iteration 220/1000 | Loss: 0.00001650
Iteration 221/1000 | Loss: 0.00001650
Iteration 222/1000 | Loss: 0.00001650
Iteration 223/1000 | Loss: 0.00001650
Iteration 224/1000 | Loss: 0.00001650
Iteration 225/1000 | Loss: 0.00001650
Iteration 226/1000 | Loss: 0.00001650
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001650
Iteration 229/1000 | Loss: 0.00001649
Iteration 230/1000 | Loss: 0.00001649
Iteration 231/1000 | Loss: 0.00001649
Iteration 232/1000 | Loss: 0.00001649
Iteration 233/1000 | Loss: 0.00001649
Iteration 234/1000 | Loss: 0.00001649
Iteration 235/1000 | Loss: 0.00001649
Iteration 236/1000 | Loss: 0.00001649
Iteration 237/1000 | Loss: 0.00001649
Iteration 238/1000 | Loss: 0.00001648
Iteration 239/1000 | Loss: 0.00001648
Iteration 240/1000 | Loss: 0.00001648
Iteration 241/1000 | Loss: 0.00001648
Iteration 242/1000 | Loss: 0.00001648
Iteration 243/1000 | Loss: 0.00001647
Iteration 244/1000 | Loss: 0.00001647
Iteration 245/1000 | Loss: 0.00001647
Iteration 246/1000 | Loss: 0.00001647
Iteration 247/1000 | Loss: 0.00001647
Iteration 248/1000 | Loss: 0.00001647
Iteration 249/1000 | Loss: 0.00001647
Iteration 250/1000 | Loss: 0.00001647
Iteration 251/1000 | Loss: 0.00001647
Iteration 252/1000 | Loss: 0.00001647
Iteration 253/1000 | Loss: 0.00001647
Iteration 254/1000 | Loss: 0.00001647
Iteration 255/1000 | Loss: 0.00001647
Iteration 256/1000 | Loss: 0.00001647
Iteration 257/1000 | Loss: 0.00001647
Iteration 258/1000 | Loss: 0.00001647
Iteration 259/1000 | Loss: 0.00001647
Iteration 260/1000 | Loss: 0.00001647
Iteration 261/1000 | Loss: 0.00001647
Iteration 262/1000 | Loss: 0.00001646
Iteration 263/1000 | Loss: 0.00001646
Iteration 264/1000 | Loss: 0.00001646
Iteration 265/1000 | Loss: 0.00001646
Iteration 266/1000 | Loss: 0.00001646
Iteration 267/1000 | Loss: 0.00001646
Iteration 268/1000 | Loss: 0.00001646
Iteration 269/1000 | Loss: 0.00001646
Iteration 270/1000 | Loss: 0.00001646
Iteration 271/1000 | Loss: 0.00001646
Iteration 272/1000 | Loss: 0.00001646
Iteration 273/1000 | Loss: 0.00001646
Iteration 274/1000 | Loss: 0.00001646
Iteration 275/1000 | Loss: 0.00001646
Iteration 276/1000 | Loss: 0.00001646
Iteration 277/1000 | Loss: 0.00001645
Iteration 278/1000 | Loss: 0.00001645
Iteration 279/1000 | Loss: 0.00001645
Iteration 280/1000 | Loss: 0.00001645
Iteration 281/1000 | Loss: 0.00001645
Iteration 282/1000 | Loss: 0.00001645
Iteration 283/1000 | Loss: 0.00001645
Iteration 284/1000 | Loss: 0.00001645
Iteration 285/1000 | Loss: 0.00001645
Iteration 286/1000 | Loss: 0.00001645
Iteration 287/1000 | Loss: 0.00001645
Iteration 288/1000 | Loss: 0.00001645
Iteration 289/1000 | Loss: 0.00001645
Iteration 290/1000 | Loss: 0.00001645
Iteration 291/1000 | Loss: 0.00001645
Iteration 292/1000 | Loss: 0.00001645
Iteration 293/1000 | Loss: 0.00001645
Iteration 294/1000 | Loss: 0.00001645
Iteration 295/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [1.644912117626518e-05, 1.644912117626518e-05, 1.644912117626518e-05, 1.644912117626518e-05, 1.644912117626518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.644912117626518e-05

Optimization complete. Final v2v error: 3.346311569213867 mm

Highest mean error: 5.310859680175781 mm for frame 87

Lowest mean error: 2.7414097785949707 mm for frame 37

Saving results

Total time: 52.718156814575195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819363
Iteration 2/25 | Loss: 0.00153475
Iteration 3/25 | Loss: 0.00138638
Iteration 4/25 | Loss: 0.00136085
Iteration 5/25 | Loss: 0.00135483
Iteration 6/25 | Loss: 0.00135427
Iteration 7/25 | Loss: 0.00135427
Iteration 8/25 | Loss: 0.00135427
Iteration 9/25 | Loss: 0.00135427
Iteration 10/25 | Loss: 0.00135427
Iteration 11/25 | Loss: 0.00135427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013542728265747428, 0.0013542728265747428, 0.0013542728265747428, 0.0013542728265747428, 0.0013542728265747428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013542728265747428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88589191
Iteration 2/25 | Loss: 0.00141523
Iteration 3/25 | Loss: 0.00141522
Iteration 4/25 | Loss: 0.00141522
Iteration 5/25 | Loss: 0.00141522
Iteration 6/25 | Loss: 0.00141522
Iteration 7/25 | Loss: 0.00141522
Iteration 8/25 | Loss: 0.00141522
Iteration 9/25 | Loss: 0.00141522
Iteration 10/25 | Loss: 0.00141522
Iteration 11/25 | Loss: 0.00141521
Iteration 12/25 | Loss: 0.00141521
Iteration 13/25 | Loss: 0.00141521
Iteration 14/25 | Loss: 0.00141521
Iteration 15/25 | Loss: 0.00141521
Iteration 16/25 | Loss: 0.00141521
Iteration 17/25 | Loss: 0.00141521
Iteration 18/25 | Loss: 0.00141521
Iteration 19/25 | Loss: 0.00141521
Iteration 20/25 | Loss: 0.00141521
Iteration 21/25 | Loss: 0.00141521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014152146177366376, 0.0014152146177366376, 0.0014152146177366376, 0.0014152146177366376, 0.0014152146177366376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014152146177366376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141521
Iteration 2/1000 | Loss: 0.00003615
Iteration 3/1000 | Loss: 0.00003045
Iteration 4/1000 | Loss: 0.00002827
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00002377
Iteration 9/1000 | Loss: 0.00002324
Iteration 10/1000 | Loss: 0.00002280
Iteration 11/1000 | Loss: 0.00002245
Iteration 12/1000 | Loss: 0.00002222
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002173
Iteration 20/1000 | Loss: 0.00002172
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002168
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002161
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002160
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002159
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002159
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002159
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002158
Iteration 50/1000 | Loss: 0.00002158
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002157
Iteration 53/1000 | Loss: 0.00002157
Iteration 54/1000 | Loss: 0.00002157
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002156
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002156
Iteration 59/1000 | Loss: 0.00002156
Iteration 60/1000 | Loss: 0.00002156
Iteration 61/1000 | Loss: 0.00002156
Iteration 62/1000 | Loss: 0.00002154
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002153
Iteration 65/1000 | Loss: 0.00002153
Iteration 66/1000 | Loss: 0.00002153
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002152
Iteration 69/1000 | Loss: 0.00002152
Iteration 70/1000 | Loss: 0.00002152
Iteration 71/1000 | Loss: 0.00002152
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002151
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002150
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002150
Iteration 82/1000 | Loss: 0.00002150
Iteration 83/1000 | Loss: 0.00002149
Iteration 84/1000 | Loss: 0.00002149
Iteration 85/1000 | Loss: 0.00002148
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002145
Iteration 92/1000 | Loss: 0.00002145
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002144
Iteration 101/1000 | Loss: 0.00002144
Iteration 102/1000 | Loss: 0.00002142
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002141
Iteration 107/1000 | Loss: 0.00002141
Iteration 108/1000 | Loss: 0.00002141
Iteration 109/1000 | Loss: 0.00002140
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002140
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002138
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002137
Iteration 129/1000 | Loss: 0.00002137
Iteration 130/1000 | Loss: 0.00002137
Iteration 131/1000 | Loss: 0.00002137
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002137
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002135
Iteration 140/1000 | Loss: 0.00002135
Iteration 141/1000 | Loss: 0.00002135
Iteration 142/1000 | Loss: 0.00002135
Iteration 143/1000 | Loss: 0.00002135
Iteration 144/1000 | Loss: 0.00002135
Iteration 145/1000 | Loss: 0.00002135
Iteration 146/1000 | Loss: 0.00002135
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002134
Iteration 151/1000 | Loss: 0.00002134
Iteration 152/1000 | Loss: 0.00002134
Iteration 153/1000 | Loss: 0.00002134
Iteration 154/1000 | Loss: 0.00002134
Iteration 155/1000 | Loss: 0.00002134
Iteration 156/1000 | Loss: 0.00002134
Iteration 157/1000 | Loss: 0.00002133
Iteration 158/1000 | Loss: 0.00002133
Iteration 159/1000 | Loss: 0.00002133
Iteration 160/1000 | Loss: 0.00002133
Iteration 161/1000 | Loss: 0.00002133
Iteration 162/1000 | Loss: 0.00002133
Iteration 163/1000 | Loss: 0.00002133
Iteration 164/1000 | Loss: 0.00002133
Iteration 165/1000 | Loss: 0.00002133
Iteration 166/1000 | Loss: 0.00002133
Iteration 167/1000 | Loss: 0.00002133
Iteration 168/1000 | Loss: 0.00002133
Iteration 169/1000 | Loss: 0.00002133
Iteration 170/1000 | Loss: 0.00002132
Iteration 171/1000 | Loss: 0.00002132
Iteration 172/1000 | Loss: 0.00002132
Iteration 173/1000 | Loss: 0.00002132
Iteration 174/1000 | Loss: 0.00002132
Iteration 175/1000 | Loss: 0.00002132
Iteration 176/1000 | Loss: 0.00002132
Iteration 177/1000 | Loss: 0.00002132
Iteration 178/1000 | Loss: 0.00002132
Iteration 179/1000 | Loss: 0.00002132
Iteration 180/1000 | Loss: 0.00002132
Iteration 181/1000 | Loss: 0.00002132
Iteration 182/1000 | Loss: 0.00002132
Iteration 183/1000 | Loss: 0.00002132
Iteration 184/1000 | Loss: 0.00002132
Iteration 185/1000 | Loss: 0.00002132
Iteration 186/1000 | Loss: 0.00002132
Iteration 187/1000 | Loss: 0.00002132
Iteration 188/1000 | Loss: 0.00002131
Iteration 189/1000 | Loss: 0.00002131
Iteration 190/1000 | Loss: 0.00002131
Iteration 191/1000 | Loss: 0.00002131
Iteration 192/1000 | Loss: 0.00002131
Iteration 193/1000 | Loss: 0.00002131
Iteration 194/1000 | Loss: 0.00002131
Iteration 195/1000 | Loss: 0.00002131
Iteration 196/1000 | Loss: 0.00002131
Iteration 197/1000 | Loss: 0.00002131
Iteration 198/1000 | Loss: 0.00002131
Iteration 199/1000 | Loss: 0.00002131
Iteration 200/1000 | Loss: 0.00002131
Iteration 201/1000 | Loss: 0.00002131
Iteration 202/1000 | Loss: 0.00002131
Iteration 203/1000 | Loss: 0.00002131
Iteration 204/1000 | Loss: 0.00002131
Iteration 205/1000 | Loss: 0.00002131
Iteration 206/1000 | Loss: 0.00002131
Iteration 207/1000 | Loss: 0.00002131
Iteration 208/1000 | Loss: 0.00002131
Iteration 209/1000 | Loss: 0.00002131
Iteration 210/1000 | Loss: 0.00002131
Iteration 211/1000 | Loss: 0.00002131
Iteration 212/1000 | Loss: 0.00002131
Iteration 213/1000 | Loss: 0.00002131
Iteration 214/1000 | Loss: 0.00002131
Iteration 215/1000 | Loss: 0.00002131
Iteration 216/1000 | Loss: 0.00002131
Iteration 217/1000 | Loss: 0.00002131
Iteration 218/1000 | Loss: 0.00002131
Iteration 219/1000 | Loss: 0.00002131
Iteration 220/1000 | Loss: 0.00002131
Iteration 221/1000 | Loss: 0.00002131
Iteration 222/1000 | Loss: 0.00002131
Iteration 223/1000 | Loss: 0.00002131
Iteration 224/1000 | Loss: 0.00002131
Iteration 225/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.1310312149580568e-05, 2.1310312149580568e-05, 2.1310312149580568e-05, 2.1310312149580568e-05, 2.1310312149580568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1310312149580568e-05

Optimization complete. Final v2v error: 4.029224872589111 mm

Highest mean error: 4.167354106903076 mm for frame 42

Lowest mean error: 3.804654121398926 mm for frame 91

Saving results

Total time: 41.36215615272522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015802
Iteration 2/25 | Loss: 0.00212249
Iteration 3/25 | Loss: 0.00170907
Iteration 4/25 | Loss: 0.00164842
Iteration 5/25 | Loss: 0.00159131
Iteration 6/25 | Loss: 0.00155600
Iteration 7/25 | Loss: 0.00147226
Iteration 8/25 | Loss: 0.00143980
Iteration 9/25 | Loss: 0.00139838
Iteration 10/25 | Loss: 0.00136955
Iteration 11/25 | Loss: 0.00136850
Iteration 12/25 | Loss: 0.00136229
Iteration 13/25 | Loss: 0.00135820
Iteration 14/25 | Loss: 0.00138888
Iteration 15/25 | Loss: 0.00135825
Iteration 16/25 | Loss: 0.00135769
Iteration 17/25 | Loss: 0.00135750
Iteration 18/25 | Loss: 0.00135744
Iteration 19/25 | Loss: 0.00135743
Iteration 20/25 | Loss: 0.00135743
Iteration 21/25 | Loss: 0.00135743
Iteration 22/25 | Loss: 0.00135742
Iteration 23/25 | Loss: 0.00135742
Iteration 24/25 | Loss: 0.00135742
Iteration 25/25 | Loss: 0.00135742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76620436
Iteration 2/25 | Loss: 0.00213158
Iteration 3/25 | Loss: 0.00208346
Iteration 4/25 | Loss: 0.00208348
Iteration 5/25 | Loss: 0.00206964
Iteration 6/25 | Loss: 0.00206964
Iteration 7/25 | Loss: 0.00206964
Iteration 8/25 | Loss: 0.00206963
Iteration 9/25 | Loss: 0.00206963
Iteration 10/25 | Loss: 0.00206963
Iteration 11/25 | Loss: 0.00206963
Iteration 12/25 | Loss: 0.00206963
Iteration 13/25 | Loss: 0.00206963
Iteration 14/25 | Loss: 0.00206963
Iteration 15/25 | Loss: 0.00206963
Iteration 16/25 | Loss: 0.00206963
Iteration 17/25 | Loss: 0.00206963
Iteration 18/25 | Loss: 0.00206963
Iteration 19/25 | Loss: 0.00206963
Iteration 20/25 | Loss: 0.00206963
Iteration 21/25 | Loss: 0.00206963
Iteration 22/25 | Loss: 0.00206963
Iteration 23/25 | Loss: 0.00206963
Iteration 24/25 | Loss: 0.00206963
Iteration 25/25 | Loss: 0.00206963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206963
Iteration 2/1000 | Loss: 0.00012279
Iteration 3/1000 | Loss: 0.00008927
Iteration 4/1000 | Loss: 0.00003694
Iteration 5/1000 | Loss: 0.00010930
Iteration 6/1000 | Loss: 0.00076974
Iteration 7/1000 | Loss: 0.00053989
Iteration 8/1000 | Loss: 0.00100461
Iteration 9/1000 | Loss: 0.00045050
Iteration 10/1000 | Loss: 0.00103987
Iteration 11/1000 | Loss: 0.00103769
Iteration 12/1000 | Loss: 0.00034190
Iteration 13/1000 | Loss: 0.00005549
Iteration 14/1000 | Loss: 0.00018768
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00002261
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00016630
Iteration 22/1000 | Loss: 0.00006954
Iteration 23/1000 | Loss: 0.00004626
Iteration 24/1000 | Loss: 0.00007758
Iteration 25/1000 | Loss: 0.00001748
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00002094
Iteration 28/1000 | Loss: 0.00002986
Iteration 29/1000 | Loss: 0.00004093
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00004267
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00001387
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001366
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001365
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001350
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00002421
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001326
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001326
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001326
Iteration 121/1000 | Loss: 0.00001326
Iteration 122/1000 | Loss: 0.00001326
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3264620974950958e-05, 1.3264620974950958e-05, 1.3264620974950958e-05, 1.3264620974950958e-05, 1.3264620974950958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3264620974950958e-05

Optimization complete. Final v2v error: 3.1610782146453857 mm

Highest mean error: 4.164695739746094 mm for frame 104

Lowest mean error: 2.877891778945923 mm for frame 129

Saving results

Total time: 95.87689018249512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081622
Iteration 2/25 | Loss: 0.00209729
Iteration 3/25 | Loss: 0.00155731
Iteration 4/25 | Loss: 0.00153170
Iteration 5/25 | Loss: 0.00155575
Iteration 6/25 | Loss: 0.00154949
Iteration 7/25 | Loss: 0.00152827
Iteration 8/25 | Loss: 0.00152298
Iteration 9/25 | Loss: 0.00152106
Iteration 10/25 | Loss: 0.00151856
Iteration 11/25 | Loss: 0.00151882
Iteration 12/25 | Loss: 0.00153355
Iteration 13/25 | Loss: 0.00151828
Iteration 14/25 | Loss: 0.00151386
Iteration 15/25 | Loss: 0.00151643
Iteration 16/25 | Loss: 0.00153316
Iteration 17/25 | Loss: 0.00152087
Iteration 18/25 | Loss: 0.00151687
Iteration 19/25 | Loss: 0.00151560
Iteration 20/25 | Loss: 0.00151509
Iteration 21/25 | Loss: 0.00151442
Iteration 22/25 | Loss: 0.00151396
Iteration 23/25 | Loss: 0.00150828
Iteration 24/25 | Loss: 0.00150794
Iteration 25/25 | Loss: 0.00150793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21747732
Iteration 2/25 | Loss: 0.00209604
Iteration 3/25 | Loss: 0.00209604
Iteration 4/25 | Loss: 0.00209604
Iteration 5/25 | Loss: 0.00209604
Iteration 6/25 | Loss: 0.00209603
Iteration 7/25 | Loss: 0.00209603
Iteration 8/25 | Loss: 0.00209603
Iteration 9/25 | Loss: 0.00209603
Iteration 10/25 | Loss: 0.00209603
Iteration 11/25 | Loss: 0.00209603
Iteration 12/25 | Loss: 0.00209603
Iteration 13/25 | Loss: 0.00209603
Iteration 14/25 | Loss: 0.00209603
Iteration 15/25 | Loss: 0.00209603
Iteration 16/25 | Loss: 0.00209603
Iteration 17/25 | Loss: 0.00209603
Iteration 18/25 | Loss: 0.00209603
Iteration 19/25 | Loss: 0.00209603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020960336551070213, 0.0020960336551070213, 0.0020960336551070213, 0.0020960336551070213, 0.0020960336551070213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020960336551070213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209603
Iteration 2/1000 | Loss: 0.00004338
Iteration 3/1000 | Loss: 0.00002802
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002356
Iteration 6/1000 | Loss: 0.00002287
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002132
Iteration 11/1000 | Loss: 0.00002114
Iteration 12/1000 | Loss: 0.00002108
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002106
Iteration 15/1000 | Loss: 0.00002104
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002101
Iteration 19/1000 | Loss: 0.00002101
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002100
Iteration 22/1000 | Loss: 0.00002099
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002078
Iteration 25/1000 | Loss: 0.00002077
Iteration 26/1000 | Loss: 0.00002077
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002066
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002064
Iteration 31/1000 | Loss: 0.00002063
Iteration 32/1000 | Loss: 0.00002062
Iteration 33/1000 | Loss: 0.00002062
Iteration 34/1000 | Loss: 0.00002061
Iteration 35/1000 | Loss: 0.00002061
Iteration 36/1000 | Loss: 0.00002061
Iteration 37/1000 | Loss: 0.00002061
Iteration 38/1000 | Loss: 0.00002061
Iteration 39/1000 | Loss: 0.00002061
Iteration 40/1000 | Loss: 0.00002060
Iteration 41/1000 | Loss: 0.00002060
Iteration 42/1000 | Loss: 0.00002060
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002060
Iteration 45/1000 | Loss: 0.00002060
Iteration 46/1000 | Loss: 0.00002060
Iteration 47/1000 | Loss: 0.00002059
Iteration 48/1000 | Loss: 0.00002059
Iteration 49/1000 | Loss: 0.00002058
Iteration 50/1000 | Loss: 0.00002058
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002054
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002053
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002052
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002052
Iteration 78/1000 | Loss: 0.00002052
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002050
Iteration 84/1000 | Loss: 0.00002050
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002049
Iteration 87/1000 | Loss: 0.00002049
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002044
Iteration 105/1000 | Loss: 0.00002044
Iteration 106/1000 | Loss: 0.00002043
Iteration 107/1000 | Loss: 0.00002043
Iteration 108/1000 | Loss: 0.00002043
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002042
Iteration 112/1000 | Loss: 0.00002042
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002040
Iteration 127/1000 | Loss: 0.00002040
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002039
Iteration 130/1000 | Loss: 0.00002039
Iteration 131/1000 | Loss: 0.00002039
Iteration 132/1000 | Loss: 0.00002038
Iteration 133/1000 | Loss: 0.00002038
Iteration 134/1000 | Loss: 0.00002038
Iteration 135/1000 | Loss: 0.00002037
Iteration 136/1000 | Loss: 0.00002037
Iteration 137/1000 | Loss: 0.00002037
Iteration 138/1000 | Loss: 0.00002037
Iteration 139/1000 | Loss: 0.00002037
Iteration 140/1000 | Loss: 0.00002036
Iteration 141/1000 | Loss: 0.00002036
Iteration 142/1000 | Loss: 0.00002035
Iteration 143/1000 | Loss: 0.00002035
Iteration 144/1000 | Loss: 0.00002035
Iteration 145/1000 | Loss: 0.00002035
Iteration 146/1000 | Loss: 0.00002034
Iteration 147/1000 | Loss: 0.00002034
Iteration 148/1000 | Loss: 0.00002034
Iteration 149/1000 | Loss: 0.00002034
Iteration 150/1000 | Loss: 0.00002033
Iteration 151/1000 | Loss: 0.00002033
Iteration 152/1000 | Loss: 0.00002033
Iteration 153/1000 | Loss: 0.00002032
Iteration 154/1000 | Loss: 0.00002032
Iteration 155/1000 | Loss: 0.00002032
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002031
Iteration 159/1000 | Loss: 0.00002031
Iteration 160/1000 | Loss: 0.00002031
Iteration 161/1000 | Loss: 0.00002031
Iteration 162/1000 | Loss: 0.00002031
Iteration 163/1000 | Loss: 0.00002031
Iteration 164/1000 | Loss: 0.00002031
Iteration 165/1000 | Loss: 0.00002031
Iteration 166/1000 | Loss: 0.00002031
Iteration 167/1000 | Loss: 0.00002031
Iteration 168/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.03094841708662e-05, 2.03094841708662e-05, 2.03094841708662e-05, 2.03094841708662e-05, 2.03094841708662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.03094841708662e-05

Optimization complete. Final v2v error: 3.674329996109009 mm

Highest mean error: 4.366868495941162 mm for frame 73

Lowest mean error: 3.2378549575805664 mm for frame 149

Saving results

Total time: 82.83668947219849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038207
Iteration 2/25 | Loss: 0.00172375
Iteration 3/25 | Loss: 0.00156031
Iteration 4/25 | Loss: 0.00153918
Iteration 5/25 | Loss: 0.00153141
Iteration 6/25 | Loss: 0.00153029
Iteration 7/25 | Loss: 0.00153029
Iteration 8/25 | Loss: 0.00153029
Iteration 9/25 | Loss: 0.00153029
Iteration 10/25 | Loss: 0.00153029
Iteration 11/25 | Loss: 0.00153029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015302858082577586, 0.0015302858082577586, 0.0015302858082577586, 0.0015302858082577586, 0.0015302858082577586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015302858082577586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14706898
Iteration 2/25 | Loss: 0.00184039
Iteration 3/25 | Loss: 0.00184039
Iteration 4/25 | Loss: 0.00184039
Iteration 5/25 | Loss: 0.00184039
Iteration 6/25 | Loss: 0.00184039
Iteration 7/25 | Loss: 0.00184038
Iteration 8/25 | Loss: 0.00184038
Iteration 9/25 | Loss: 0.00184038
Iteration 10/25 | Loss: 0.00184038
Iteration 11/25 | Loss: 0.00184038
Iteration 12/25 | Loss: 0.00184038
Iteration 13/25 | Loss: 0.00184038
Iteration 14/25 | Loss: 0.00184038
Iteration 15/25 | Loss: 0.00184038
Iteration 16/25 | Loss: 0.00184038
Iteration 17/25 | Loss: 0.00184038
Iteration 18/25 | Loss: 0.00184038
Iteration 19/25 | Loss: 0.00184038
Iteration 20/25 | Loss: 0.00184038
Iteration 21/25 | Loss: 0.00184038
Iteration 22/25 | Loss: 0.00184038
Iteration 23/25 | Loss: 0.00184038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018403835128992796, 0.0018403835128992796, 0.0018403835128992796, 0.0018403835128992796, 0.0018403835128992796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018403835128992796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184038
Iteration 2/1000 | Loss: 0.00004614
Iteration 3/1000 | Loss: 0.00003485
Iteration 4/1000 | Loss: 0.00003160
Iteration 5/1000 | Loss: 0.00003042
Iteration 6/1000 | Loss: 0.00002947
Iteration 7/1000 | Loss: 0.00002888
Iteration 8/1000 | Loss: 0.00002847
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002783
Iteration 11/1000 | Loss: 0.00002769
Iteration 12/1000 | Loss: 0.00002756
Iteration 13/1000 | Loss: 0.00002754
Iteration 14/1000 | Loss: 0.00002750
Iteration 15/1000 | Loss: 0.00002750
Iteration 16/1000 | Loss: 0.00002750
Iteration 17/1000 | Loss: 0.00002750
Iteration 18/1000 | Loss: 0.00002750
Iteration 19/1000 | Loss: 0.00002750
Iteration 20/1000 | Loss: 0.00002749
Iteration 21/1000 | Loss: 0.00002749
Iteration 22/1000 | Loss: 0.00002747
Iteration 23/1000 | Loss: 0.00002745
Iteration 24/1000 | Loss: 0.00002741
Iteration 25/1000 | Loss: 0.00002741
Iteration 26/1000 | Loss: 0.00002740
Iteration 27/1000 | Loss: 0.00002736
Iteration 28/1000 | Loss: 0.00002735
Iteration 29/1000 | Loss: 0.00002734
Iteration 30/1000 | Loss: 0.00002734
Iteration 31/1000 | Loss: 0.00002734
Iteration 32/1000 | Loss: 0.00002733
Iteration 33/1000 | Loss: 0.00002733
Iteration 34/1000 | Loss: 0.00002732
Iteration 35/1000 | Loss: 0.00002732
Iteration 36/1000 | Loss: 0.00002731
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002727
Iteration 42/1000 | Loss: 0.00002725
Iteration 43/1000 | Loss: 0.00002725
Iteration 44/1000 | Loss: 0.00002725
Iteration 45/1000 | Loss: 0.00002725
Iteration 46/1000 | Loss: 0.00002725
Iteration 47/1000 | Loss: 0.00002725
Iteration 48/1000 | Loss: 0.00002725
Iteration 49/1000 | Loss: 0.00002724
Iteration 50/1000 | Loss: 0.00002723
Iteration 51/1000 | Loss: 0.00002722
Iteration 52/1000 | Loss: 0.00002722
Iteration 53/1000 | Loss: 0.00002722
Iteration 54/1000 | Loss: 0.00002722
Iteration 55/1000 | Loss: 0.00002722
Iteration 56/1000 | Loss: 0.00002721
Iteration 57/1000 | Loss: 0.00002721
Iteration 58/1000 | Loss: 0.00002721
Iteration 59/1000 | Loss: 0.00002720
Iteration 60/1000 | Loss: 0.00002720
Iteration 61/1000 | Loss: 0.00002720
Iteration 62/1000 | Loss: 0.00002719
Iteration 63/1000 | Loss: 0.00002719
Iteration 64/1000 | Loss: 0.00002719
Iteration 65/1000 | Loss: 0.00002718
Iteration 66/1000 | Loss: 0.00002718
Iteration 67/1000 | Loss: 0.00002718
Iteration 68/1000 | Loss: 0.00002718
Iteration 69/1000 | Loss: 0.00002717
Iteration 70/1000 | Loss: 0.00002717
Iteration 71/1000 | Loss: 0.00002716
Iteration 72/1000 | Loss: 0.00002716
Iteration 73/1000 | Loss: 0.00002716
Iteration 74/1000 | Loss: 0.00002716
Iteration 75/1000 | Loss: 0.00002716
Iteration 76/1000 | Loss: 0.00002716
Iteration 77/1000 | Loss: 0.00002715
Iteration 78/1000 | Loss: 0.00002715
Iteration 79/1000 | Loss: 0.00002714
Iteration 80/1000 | Loss: 0.00002714
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002714
Iteration 83/1000 | Loss: 0.00002713
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002712
Iteration 88/1000 | Loss: 0.00002712
Iteration 89/1000 | Loss: 0.00002712
Iteration 90/1000 | Loss: 0.00002712
Iteration 91/1000 | Loss: 0.00002711
Iteration 92/1000 | Loss: 0.00002711
Iteration 93/1000 | Loss: 0.00002710
Iteration 94/1000 | Loss: 0.00002710
Iteration 95/1000 | Loss: 0.00002710
Iteration 96/1000 | Loss: 0.00002710
Iteration 97/1000 | Loss: 0.00002709
Iteration 98/1000 | Loss: 0.00002709
Iteration 99/1000 | Loss: 0.00002709
Iteration 100/1000 | Loss: 0.00002708
Iteration 101/1000 | Loss: 0.00002708
Iteration 102/1000 | Loss: 0.00002708
Iteration 103/1000 | Loss: 0.00002707
Iteration 104/1000 | Loss: 0.00002707
Iteration 105/1000 | Loss: 0.00002707
Iteration 106/1000 | Loss: 0.00002707
Iteration 107/1000 | Loss: 0.00002707
Iteration 108/1000 | Loss: 0.00002707
Iteration 109/1000 | Loss: 0.00002707
Iteration 110/1000 | Loss: 0.00002707
Iteration 111/1000 | Loss: 0.00002707
Iteration 112/1000 | Loss: 0.00002707
Iteration 113/1000 | Loss: 0.00002707
Iteration 114/1000 | Loss: 0.00002707
Iteration 115/1000 | Loss: 0.00002707
Iteration 116/1000 | Loss: 0.00002707
Iteration 117/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.707030944293365e-05, 2.707030944293365e-05, 2.707030944293365e-05, 2.707030944293365e-05, 2.707030944293365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.707030944293365e-05

Optimization complete. Final v2v error: 4.247812747955322 mm

Highest mean error: 4.819969177246094 mm for frame 151

Lowest mean error: 3.7525391578674316 mm for frame 7

Saving results

Total time: 38.736655473709106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392229
Iteration 2/25 | Loss: 0.00142845
Iteration 3/25 | Loss: 0.00136792
Iteration 4/25 | Loss: 0.00135989
Iteration 5/25 | Loss: 0.00135667
Iteration 6/25 | Loss: 0.00135667
Iteration 7/25 | Loss: 0.00135667
Iteration 8/25 | Loss: 0.00135667
Iteration 9/25 | Loss: 0.00135667
Iteration 10/25 | Loss: 0.00135667
Iteration 11/25 | Loss: 0.00135667
Iteration 12/25 | Loss: 0.00135667
Iteration 13/25 | Loss: 0.00135667
Iteration 14/25 | Loss: 0.00135667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013566705165430903, 0.0013566705165430903, 0.0013566705165430903, 0.0013566705165430903, 0.0013566705165430903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013566705165430903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36970782
Iteration 2/25 | Loss: 0.00204599
Iteration 3/25 | Loss: 0.00204599
Iteration 4/25 | Loss: 0.00204599
Iteration 5/25 | Loss: 0.00204599
Iteration 6/25 | Loss: 0.00204599
Iteration 7/25 | Loss: 0.00204599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0020459925290197134, 0.0020459925290197134, 0.0020459925290197134, 0.0020459925290197134, 0.0020459925290197134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020459925290197134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204599
Iteration 2/1000 | Loss: 0.00002801
Iteration 3/1000 | Loss: 0.00001775
Iteration 4/1000 | Loss: 0.00001589
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001420
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001177
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001175
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001174
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001173
Iteration 139/1000 | Loss: 0.00001173
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001171
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001168
Iteration 147/1000 | Loss: 0.00001168
Iteration 148/1000 | Loss: 0.00001168
Iteration 149/1000 | Loss: 0.00001168
Iteration 150/1000 | Loss: 0.00001168
Iteration 151/1000 | Loss: 0.00001168
Iteration 152/1000 | Loss: 0.00001168
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001167
Iteration 156/1000 | Loss: 0.00001167
Iteration 157/1000 | Loss: 0.00001167
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001165
Iteration 167/1000 | Loss: 0.00001165
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001165
Iteration 171/1000 | Loss: 0.00001165
Iteration 172/1000 | Loss: 0.00001165
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001164
Iteration 181/1000 | Loss: 0.00001164
Iteration 182/1000 | Loss: 0.00001164
Iteration 183/1000 | Loss: 0.00001164
Iteration 184/1000 | Loss: 0.00001164
Iteration 185/1000 | Loss: 0.00001164
Iteration 186/1000 | Loss: 0.00001164
Iteration 187/1000 | Loss: 0.00001164
Iteration 188/1000 | Loss: 0.00001164
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001163
Iteration 197/1000 | Loss: 0.00001163
Iteration 198/1000 | Loss: 0.00001163
Iteration 199/1000 | Loss: 0.00001163
Iteration 200/1000 | Loss: 0.00001163
Iteration 201/1000 | Loss: 0.00001163
Iteration 202/1000 | Loss: 0.00001163
Iteration 203/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.162996886705514e-05, 1.162996886705514e-05, 1.162996886705514e-05, 1.162996886705514e-05, 1.162996886705514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162996886705514e-05

Optimization complete. Final v2v error: 2.940728187561035 mm

Highest mean error: 3.1023173332214355 mm for frame 178

Lowest mean error: 2.8168349266052246 mm for frame 164

Saving results

Total time: 46.39305114746094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407606
Iteration 2/25 | Loss: 0.00141560
Iteration 3/25 | Loss: 0.00134561
Iteration 4/25 | Loss: 0.00134098
Iteration 5/25 | Loss: 0.00133954
Iteration 6/25 | Loss: 0.00133948
Iteration 7/25 | Loss: 0.00133948
Iteration 8/25 | Loss: 0.00133948
Iteration 9/25 | Loss: 0.00133948
Iteration 10/25 | Loss: 0.00133948
Iteration 11/25 | Loss: 0.00133948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013394771376624703, 0.0013394771376624703, 0.0013394771376624703, 0.0013394771376624703, 0.0013394771376624703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013394771376624703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25691116
Iteration 2/25 | Loss: 0.00231840
Iteration 3/25 | Loss: 0.00231839
Iteration 4/25 | Loss: 0.00231839
Iteration 5/25 | Loss: 0.00231839
Iteration 6/25 | Loss: 0.00231839
Iteration 7/25 | Loss: 0.00231839
Iteration 8/25 | Loss: 0.00231839
Iteration 9/25 | Loss: 0.00231839
Iteration 10/25 | Loss: 0.00231839
Iteration 11/25 | Loss: 0.00231839
Iteration 12/25 | Loss: 0.00231839
Iteration 13/25 | Loss: 0.00231839
Iteration 14/25 | Loss: 0.00231839
Iteration 15/25 | Loss: 0.00231839
Iteration 16/25 | Loss: 0.00231839
Iteration 17/25 | Loss: 0.00231839
Iteration 18/25 | Loss: 0.00231839
Iteration 19/25 | Loss: 0.00231839
Iteration 20/25 | Loss: 0.00231839
Iteration 21/25 | Loss: 0.00231839
Iteration 22/25 | Loss: 0.00231839
Iteration 23/25 | Loss: 0.00231839
Iteration 24/25 | Loss: 0.00231839
Iteration 25/25 | Loss: 0.00231839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231839
Iteration 2/1000 | Loss: 0.00002765
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001220
Iteration 8/1000 | Loss: 0.00001165
Iteration 9/1000 | Loss: 0.00001143
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001062
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001049
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001048
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001041
Iteration 31/1000 | Loss: 0.00001040
Iteration 32/1000 | Loss: 0.00001039
Iteration 33/1000 | Loss: 0.00001039
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001037
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001028
Iteration 43/1000 | Loss: 0.00001028
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001025
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001024
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001024
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001023
Iteration 79/1000 | Loss: 0.00001023
Iteration 80/1000 | Loss: 0.00001023
Iteration 81/1000 | Loss: 0.00001023
Iteration 82/1000 | Loss: 0.00001023
Iteration 83/1000 | Loss: 0.00001023
Iteration 84/1000 | Loss: 0.00001023
Iteration 85/1000 | Loss: 0.00001023
Iteration 86/1000 | Loss: 0.00001023
Iteration 87/1000 | Loss: 0.00001023
Iteration 88/1000 | Loss: 0.00001023
Iteration 89/1000 | Loss: 0.00001023
Iteration 90/1000 | Loss: 0.00001023
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001023
Iteration 102/1000 | Loss: 0.00001023
Iteration 103/1000 | Loss: 0.00001023
Iteration 104/1000 | Loss: 0.00001023
Iteration 105/1000 | Loss: 0.00001023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.0231367014057469e-05, 1.0231367014057469e-05, 1.0231367014057469e-05, 1.0231367014057469e-05, 1.0231367014057469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0231367014057469e-05

Optimization complete. Final v2v error: 2.7721107006073 mm

Highest mean error: 3.705134153366089 mm for frame 61

Lowest mean error: 2.5979697704315186 mm for frame 92

Saving results

Total time: 32.565248250961304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013885
Iteration 2/25 | Loss: 0.00210909
Iteration 3/25 | Loss: 0.00170589
Iteration 4/25 | Loss: 0.00181402
Iteration 5/25 | Loss: 0.00166254
Iteration 6/25 | Loss: 0.00150545
Iteration 7/25 | Loss: 0.00151295
Iteration 8/25 | Loss: 0.00150526
Iteration 9/25 | Loss: 0.00145986
Iteration 10/25 | Loss: 0.00144531
Iteration 11/25 | Loss: 0.00144569
Iteration 12/25 | Loss: 0.00146135
Iteration 13/25 | Loss: 0.00141765
Iteration 14/25 | Loss: 0.00147531
Iteration 15/25 | Loss: 0.00146292
Iteration 16/25 | Loss: 0.00144820
Iteration 17/25 | Loss: 0.00139778
Iteration 18/25 | Loss: 0.00139394
Iteration 19/25 | Loss: 0.00139335
Iteration 20/25 | Loss: 0.00139312
Iteration 21/25 | Loss: 0.00146236
Iteration 22/25 | Loss: 0.00142164
Iteration 23/25 | Loss: 0.00137762
Iteration 24/25 | Loss: 0.00137637
Iteration 25/25 | Loss: 0.00137621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43566370
Iteration 2/25 | Loss: 0.00233738
Iteration 3/25 | Loss: 0.00233738
Iteration 4/25 | Loss: 0.00233738
Iteration 5/25 | Loss: 0.00233738
Iteration 6/25 | Loss: 0.00233738
Iteration 7/25 | Loss: 0.00233737
Iteration 8/25 | Loss: 0.00233737
Iteration 9/25 | Loss: 0.00233737
Iteration 10/25 | Loss: 0.00233737
Iteration 11/25 | Loss: 0.00233737
Iteration 12/25 | Loss: 0.00233737
Iteration 13/25 | Loss: 0.00233737
Iteration 14/25 | Loss: 0.00233737
Iteration 15/25 | Loss: 0.00233737
Iteration 16/25 | Loss: 0.00233737
Iteration 17/25 | Loss: 0.00233737
Iteration 18/25 | Loss: 0.00233737
Iteration 19/25 | Loss: 0.00233737
Iteration 20/25 | Loss: 0.00233737
Iteration 21/25 | Loss: 0.00233737
Iteration 22/25 | Loss: 0.00233737
Iteration 23/25 | Loss: 0.00233737
Iteration 24/25 | Loss: 0.00233737
Iteration 25/25 | Loss: 0.00233737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233737
Iteration 2/1000 | Loss: 0.00003171
Iteration 3/1000 | Loss: 0.00029026
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00001981
Iteration 7/1000 | Loss: 0.00003309
Iteration 8/1000 | Loss: 0.00003136
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001813
Iteration 11/1000 | Loss: 0.00004579
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001687
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001660
Iteration 19/1000 | Loss: 0.00062444
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00002211
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001222
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001221
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00002967
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001182
Iteration 142/1000 | Loss: 0.00001182
Iteration 143/1000 | Loss: 0.00001182
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00002977
Iteration 153/1000 | Loss: 0.00001348
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001180
Iteration 159/1000 | Loss: 0.00001180
Iteration 160/1000 | Loss: 0.00001180
Iteration 161/1000 | Loss: 0.00001180
Iteration 162/1000 | Loss: 0.00001180
Iteration 163/1000 | Loss: 0.00001180
Iteration 164/1000 | Loss: 0.00001179
Iteration 165/1000 | Loss: 0.00001179
Iteration 166/1000 | Loss: 0.00001579
Iteration 167/1000 | Loss: 0.00001385
Iteration 168/1000 | Loss: 0.00002410
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001180
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001179
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001179
Iteration 176/1000 | Loss: 0.00001178
Iteration 177/1000 | Loss: 0.00001178
Iteration 178/1000 | Loss: 0.00001178
Iteration 179/1000 | Loss: 0.00001178
Iteration 180/1000 | Loss: 0.00001178
Iteration 181/1000 | Loss: 0.00001178
Iteration 182/1000 | Loss: 0.00001178
Iteration 183/1000 | Loss: 0.00001177
Iteration 184/1000 | Loss: 0.00001177
Iteration 185/1000 | Loss: 0.00001177
Iteration 186/1000 | Loss: 0.00001177
Iteration 187/1000 | Loss: 0.00001177
Iteration 188/1000 | Loss: 0.00001177
Iteration 189/1000 | Loss: 0.00001177
Iteration 190/1000 | Loss: 0.00001177
Iteration 191/1000 | Loss: 0.00001177
Iteration 192/1000 | Loss: 0.00001177
Iteration 193/1000 | Loss: 0.00001177
Iteration 194/1000 | Loss: 0.00001177
Iteration 195/1000 | Loss: 0.00001177
Iteration 196/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.1772623111028224e-05, 1.1772623111028224e-05, 1.1772623111028224e-05, 1.1772623111028224e-05, 1.1772623111028224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1772623111028224e-05

Optimization complete. Final v2v error: 2.9462032318115234 mm

Highest mean error: 3.532346487045288 mm for frame 96

Lowest mean error: 2.68326735496521 mm for frame 134

Saving results

Total time: 97.61627149581909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913346
Iteration 2/25 | Loss: 0.00157003
Iteration 3/25 | Loss: 0.00146655
Iteration 4/25 | Loss: 0.00145300
Iteration 5/25 | Loss: 0.00144836
Iteration 6/25 | Loss: 0.00144703
Iteration 7/25 | Loss: 0.00144703
Iteration 8/25 | Loss: 0.00144703
Iteration 9/25 | Loss: 0.00144703
Iteration 10/25 | Loss: 0.00144703
Iteration 11/25 | Loss: 0.00144703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014470260357484221, 0.0014470260357484221, 0.0014470260357484221, 0.0014470260357484221, 0.0014470260357484221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014470260357484221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15313709
Iteration 2/25 | Loss: 0.00194037
Iteration 3/25 | Loss: 0.00194036
Iteration 4/25 | Loss: 0.00194036
Iteration 5/25 | Loss: 0.00194036
Iteration 6/25 | Loss: 0.00194036
Iteration 7/25 | Loss: 0.00194036
Iteration 8/25 | Loss: 0.00194036
Iteration 9/25 | Loss: 0.00194036
Iteration 10/25 | Loss: 0.00194036
Iteration 11/25 | Loss: 0.00194036
Iteration 12/25 | Loss: 0.00194036
Iteration 13/25 | Loss: 0.00194036
Iteration 14/25 | Loss: 0.00194036
Iteration 15/25 | Loss: 0.00194036
Iteration 16/25 | Loss: 0.00194036
Iteration 17/25 | Loss: 0.00194036
Iteration 18/25 | Loss: 0.00194036
Iteration 19/25 | Loss: 0.00194036
Iteration 20/25 | Loss: 0.00194036
Iteration 21/25 | Loss: 0.00194036
Iteration 22/25 | Loss: 0.00194036
Iteration 23/25 | Loss: 0.00194036
Iteration 24/25 | Loss: 0.00194036
Iteration 25/25 | Loss: 0.00194036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194036
Iteration 2/1000 | Loss: 0.00005932
Iteration 3/1000 | Loss: 0.00004100
Iteration 4/1000 | Loss: 0.00003533
Iteration 5/1000 | Loss: 0.00003288
Iteration 6/1000 | Loss: 0.00003199
Iteration 7/1000 | Loss: 0.00003069
Iteration 8/1000 | Loss: 0.00003016
Iteration 9/1000 | Loss: 0.00002959
Iteration 10/1000 | Loss: 0.00002907
Iteration 11/1000 | Loss: 0.00002869
Iteration 12/1000 | Loss: 0.00002847
Iteration 13/1000 | Loss: 0.00002821
Iteration 14/1000 | Loss: 0.00002799
Iteration 15/1000 | Loss: 0.00002784
Iteration 16/1000 | Loss: 0.00002781
Iteration 17/1000 | Loss: 0.00002759
Iteration 18/1000 | Loss: 0.00002743
Iteration 19/1000 | Loss: 0.00002740
Iteration 20/1000 | Loss: 0.00002735
Iteration 21/1000 | Loss: 0.00002719
Iteration 22/1000 | Loss: 0.00002718
Iteration 23/1000 | Loss: 0.00002717
Iteration 24/1000 | Loss: 0.00002716
Iteration 25/1000 | Loss: 0.00002716
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00002710
Iteration 28/1000 | Loss: 0.00002709
Iteration 29/1000 | Loss: 0.00002708
Iteration 30/1000 | Loss: 0.00002706
Iteration 31/1000 | Loss: 0.00002705
Iteration 32/1000 | Loss: 0.00002705
Iteration 33/1000 | Loss: 0.00002704
Iteration 34/1000 | Loss: 0.00002703
Iteration 35/1000 | Loss: 0.00002703
Iteration 36/1000 | Loss: 0.00002703
Iteration 37/1000 | Loss: 0.00002702
Iteration 38/1000 | Loss: 0.00002700
Iteration 39/1000 | Loss: 0.00002700
Iteration 40/1000 | Loss: 0.00002699
Iteration 41/1000 | Loss: 0.00002699
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002697
Iteration 44/1000 | Loss: 0.00002697
Iteration 45/1000 | Loss: 0.00002697
Iteration 46/1000 | Loss: 0.00002696
Iteration 47/1000 | Loss: 0.00002696
Iteration 48/1000 | Loss: 0.00002696
Iteration 49/1000 | Loss: 0.00002696
Iteration 50/1000 | Loss: 0.00002696
Iteration 51/1000 | Loss: 0.00002695
Iteration 52/1000 | Loss: 0.00002695
Iteration 53/1000 | Loss: 0.00002695
Iteration 54/1000 | Loss: 0.00002695
Iteration 55/1000 | Loss: 0.00002695
Iteration 56/1000 | Loss: 0.00002695
Iteration 57/1000 | Loss: 0.00002694
Iteration 58/1000 | Loss: 0.00002694
Iteration 59/1000 | Loss: 0.00002694
Iteration 60/1000 | Loss: 0.00002694
Iteration 61/1000 | Loss: 0.00002694
Iteration 62/1000 | Loss: 0.00002694
Iteration 63/1000 | Loss: 0.00002694
Iteration 64/1000 | Loss: 0.00002694
Iteration 65/1000 | Loss: 0.00002694
Iteration 66/1000 | Loss: 0.00002694
Iteration 67/1000 | Loss: 0.00002694
Iteration 68/1000 | Loss: 0.00002694
Iteration 69/1000 | Loss: 0.00002694
Iteration 70/1000 | Loss: 0.00002694
Iteration 71/1000 | Loss: 0.00002694
Iteration 72/1000 | Loss: 0.00002694
Iteration 73/1000 | Loss: 0.00002694
Iteration 74/1000 | Loss: 0.00002694
Iteration 75/1000 | Loss: 0.00002694
Iteration 76/1000 | Loss: 0.00002694
Iteration 77/1000 | Loss: 0.00002694
Iteration 78/1000 | Loss: 0.00002694
Iteration 79/1000 | Loss: 0.00002694
Iteration 80/1000 | Loss: 0.00002694
Iteration 81/1000 | Loss: 0.00002694
Iteration 82/1000 | Loss: 0.00002694
Iteration 83/1000 | Loss: 0.00002694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.6939171220874414e-05, 2.6939171220874414e-05, 2.6939171220874414e-05, 2.6939171220874414e-05, 2.6939171220874414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6939171220874414e-05

Optimization complete. Final v2v error: 4.343300819396973 mm

Highest mean error: 4.689489841461182 mm for frame 44

Lowest mean error: 3.901710033416748 mm for frame 8

Saving results

Total time: 37.56570506095886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774929
Iteration 2/25 | Loss: 0.00165936
Iteration 3/25 | Loss: 0.00142391
Iteration 4/25 | Loss: 0.00139898
Iteration 5/25 | Loss: 0.00139469
Iteration 6/25 | Loss: 0.00139340
Iteration 7/25 | Loss: 0.00139305
Iteration 8/25 | Loss: 0.00139303
Iteration 9/25 | Loss: 0.00139303
Iteration 10/25 | Loss: 0.00139303
Iteration 11/25 | Loss: 0.00139303
Iteration 12/25 | Loss: 0.00139303
Iteration 13/25 | Loss: 0.00139303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013930266723036766, 0.0013930266723036766, 0.0013930266723036766, 0.0013930266723036766, 0.0013930266723036766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013930266723036766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48761845
Iteration 2/25 | Loss: 0.00226876
Iteration 3/25 | Loss: 0.00226875
Iteration 4/25 | Loss: 0.00226875
Iteration 5/25 | Loss: 0.00226875
Iteration 6/25 | Loss: 0.00226875
Iteration 7/25 | Loss: 0.00226875
Iteration 8/25 | Loss: 0.00226875
Iteration 9/25 | Loss: 0.00226875
Iteration 10/25 | Loss: 0.00226875
Iteration 11/25 | Loss: 0.00226874
Iteration 12/25 | Loss: 0.00226874
Iteration 13/25 | Loss: 0.00226874
Iteration 14/25 | Loss: 0.00226874
Iteration 15/25 | Loss: 0.00226874
Iteration 16/25 | Loss: 0.00226874
Iteration 17/25 | Loss: 0.00226874
Iteration 18/25 | Loss: 0.00226874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002268744632601738, 0.002268744632601738, 0.002268744632601738, 0.002268744632601738, 0.002268744632601738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002268744632601738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226874
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00015477
Iteration 6/1000 | Loss: 0.00002508
Iteration 7/1000 | Loss: 0.00002073
Iteration 8/1000 | Loss: 0.00001945
Iteration 9/1000 | Loss: 0.00014632
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00016973
Iteration 13/1000 | Loss: 0.00018363
Iteration 14/1000 | Loss: 0.00016444
Iteration 15/1000 | Loss: 0.00018106
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001720
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001675
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001597
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001596
Iteration 47/1000 | Loss: 0.00001596
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001594
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001593
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001592
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001591
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00015085
Iteration 69/1000 | Loss: 0.00009580
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001786
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00018180
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001578
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001531
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001529
Iteration 98/1000 | Loss: 0.00001529
Iteration 99/1000 | Loss: 0.00001529
Iteration 100/1000 | Loss: 0.00001529
Iteration 101/1000 | Loss: 0.00001528
Iteration 102/1000 | Loss: 0.00001528
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001528
Iteration 105/1000 | Loss: 0.00001528
Iteration 106/1000 | Loss: 0.00001528
Iteration 107/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.5283250831998885e-05, 1.5283250831998885e-05, 1.5283250831998885e-05, 1.5283250831998885e-05, 1.5283250831998885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5283250831998885e-05

Optimization complete. Final v2v error: 3.28017520904541 mm

Highest mean error: 5.437506675720215 mm for frame 57

Lowest mean error: 2.929229497909546 mm for frame 139

Saving results

Total time: 89.83722066879272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775743
Iteration 2/25 | Loss: 0.00159578
Iteration 3/25 | Loss: 0.00139741
Iteration 4/25 | Loss: 0.00136554
Iteration 5/25 | Loss: 0.00135810
Iteration 6/25 | Loss: 0.00135604
Iteration 7/25 | Loss: 0.00135553
Iteration 8/25 | Loss: 0.00135553
Iteration 9/25 | Loss: 0.00135553
Iteration 10/25 | Loss: 0.00135553
Iteration 11/25 | Loss: 0.00135553
Iteration 12/25 | Loss: 0.00135553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013555288314819336, 0.0013555288314819336, 0.0013555288314819336, 0.0013555288314819336, 0.0013555288314819336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013555288314819336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17432606
Iteration 2/25 | Loss: 0.00260070
Iteration 3/25 | Loss: 0.00260069
Iteration 4/25 | Loss: 0.00260069
Iteration 5/25 | Loss: 0.00260069
Iteration 6/25 | Loss: 0.00260069
Iteration 7/25 | Loss: 0.00260069
Iteration 8/25 | Loss: 0.00260069
Iteration 9/25 | Loss: 0.00260069
Iteration 10/25 | Loss: 0.00260069
Iteration 11/25 | Loss: 0.00260069
Iteration 12/25 | Loss: 0.00260069
Iteration 13/25 | Loss: 0.00260069
Iteration 14/25 | Loss: 0.00260069
Iteration 15/25 | Loss: 0.00260069
Iteration 16/25 | Loss: 0.00260069
Iteration 17/25 | Loss: 0.00260069
Iteration 18/25 | Loss: 0.00260069
Iteration 19/25 | Loss: 0.00260069
Iteration 20/25 | Loss: 0.00260069
Iteration 21/25 | Loss: 0.00260069
Iteration 22/25 | Loss: 0.00260069
Iteration 23/25 | Loss: 0.00260069
Iteration 24/25 | Loss: 0.00260069
Iteration 25/25 | Loss: 0.00260069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260069
Iteration 2/1000 | Loss: 0.00004792
Iteration 3/1000 | Loss: 0.00003601
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002489
Iteration 6/1000 | Loss: 0.00002345
Iteration 7/1000 | Loss: 0.00002235
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002117
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002043
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001952
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001919
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00001916
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001907
Iteration 40/1000 | Loss: 0.00001906
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001903
Iteration 52/1000 | Loss: 0.00001903
Iteration 53/1000 | Loss: 0.00001903
Iteration 54/1000 | Loss: 0.00001903
Iteration 55/1000 | Loss: 0.00001903
Iteration 56/1000 | Loss: 0.00001902
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001898
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001898
Iteration 78/1000 | Loss: 0.00001897
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001897
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001896
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001894
Iteration 98/1000 | Loss: 0.00001894
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001894
Iteration 104/1000 | Loss: 0.00001894
Iteration 105/1000 | Loss: 0.00001894
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001894
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001893
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001892
Iteration 129/1000 | Loss: 0.00001892
Iteration 130/1000 | Loss: 0.00001892
Iteration 131/1000 | Loss: 0.00001892
Iteration 132/1000 | Loss: 0.00001892
Iteration 133/1000 | Loss: 0.00001892
Iteration 134/1000 | Loss: 0.00001892
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.892172804218717e-05, 1.892172804218717e-05, 1.892172804218717e-05, 1.892172804218717e-05, 1.892172804218717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.892172804218717e-05

Optimization complete. Final v2v error: 3.7213735580444336 mm

Highest mean error: 4.302964687347412 mm for frame 72

Lowest mean error: 2.947976589202881 mm for frame 133

Saving results

Total time: 41.95619511604309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593921
Iteration 2/25 | Loss: 0.00142373
Iteration 3/25 | Loss: 0.00135637
Iteration 4/25 | Loss: 0.00134873
Iteration 5/25 | Loss: 0.00134590
Iteration 6/25 | Loss: 0.00134544
Iteration 7/25 | Loss: 0.00134544
Iteration 8/25 | Loss: 0.00134544
Iteration 9/25 | Loss: 0.00134544
Iteration 10/25 | Loss: 0.00134544
Iteration 11/25 | Loss: 0.00134544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013454398140311241, 0.0013454398140311241, 0.0013454398140311241, 0.0013454398140311241, 0.0013454398140311241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013454398140311241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.99078178
Iteration 2/25 | Loss: 0.00223269
Iteration 3/25 | Loss: 0.00223269
Iteration 4/25 | Loss: 0.00223269
Iteration 5/25 | Loss: 0.00223269
Iteration 6/25 | Loss: 0.00223269
Iteration 7/25 | Loss: 0.00223268
Iteration 8/25 | Loss: 0.00223268
Iteration 9/25 | Loss: 0.00223268
Iteration 10/25 | Loss: 0.00223268
Iteration 11/25 | Loss: 0.00223268
Iteration 12/25 | Loss: 0.00223268
Iteration 13/25 | Loss: 0.00223268
Iteration 14/25 | Loss: 0.00223268
Iteration 15/25 | Loss: 0.00223268
Iteration 16/25 | Loss: 0.00223268
Iteration 17/25 | Loss: 0.00223268
Iteration 18/25 | Loss: 0.00223268
Iteration 19/25 | Loss: 0.00223268
Iteration 20/25 | Loss: 0.00223268
Iteration 21/25 | Loss: 0.00223268
Iteration 22/25 | Loss: 0.00223268
Iteration 23/25 | Loss: 0.00223268
Iteration 24/25 | Loss: 0.00223268
Iteration 25/25 | Loss: 0.00223268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223268
Iteration 2/1000 | Loss: 0.00002349
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001568
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001290
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001175
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001158
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001155
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001136
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001134
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001133
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001132
Iteration 93/1000 | Loss: 0.00001132
Iteration 94/1000 | Loss: 0.00001132
Iteration 95/1000 | Loss: 0.00001132
Iteration 96/1000 | Loss: 0.00001131
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001131
Iteration 99/1000 | Loss: 0.00001131
Iteration 100/1000 | Loss: 0.00001131
Iteration 101/1000 | Loss: 0.00001131
Iteration 102/1000 | Loss: 0.00001131
Iteration 103/1000 | Loss: 0.00001131
Iteration 104/1000 | Loss: 0.00001131
Iteration 105/1000 | Loss: 0.00001131
Iteration 106/1000 | Loss: 0.00001131
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001131
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001130
Iteration 112/1000 | Loss: 0.00001130
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001130
Iteration 115/1000 | Loss: 0.00001130
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001125
Iteration 162/1000 | Loss: 0.00001125
Iteration 163/1000 | Loss: 0.00001125
Iteration 164/1000 | Loss: 0.00001125
Iteration 165/1000 | Loss: 0.00001125
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001123
Iteration 176/1000 | Loss: 0.00001123
Iteration 177/1000 | Loss: 0.00001123
Iteration 178/1000 | Loss: 0.00001123
Iteration 179/1000 | Loss: 0.00001122
Iteration 180/1000 | Loss: 0.00001122
Iteration 181/1000 | Loss: 0.00001122
Iteration 182/1000 | Loss: 0.00001122
Iteration 183/1000 | Loss: 0.00001122
Iteration 184/1000 | Loss: 0.00001122
Iteration 185/1000 | Loss: 0.00001122
Iteration 186/1000 | Loss: 0.00001122
Iteration 187/1000 | Loss: 0.00001122
Iteration 188/1000 | Loss: 0.00001122
Iteration 189/1000 | Loss: 0.00001122
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001121
Iteration 194/1000 | Loss: 0.00001121
Iteration 195/1000 | Loss: 0.00001121
Iteration 196/1000 | Loss: 0.00001121
Iteration 197/1000 | Loss: 0.00001121
Iteration 198/1000 | Loss: 0.00001121
Iteration 199/1000 | Loss: 0.00001121
Iteration 200/1000 | Loss: 0.00001121
Iteration 201/1000 | Loss: 0.00001121
Iteration 202/1000 | Loss: 0.00001121
Iteration 203/1000 | Loss: 0.00001121
Iteration 204/1000 | Loss: 0.00001121
Iteration 205/1000 | Loss: 0.00001121
Iteration 206/1000 | Loss: 0.00001121
Iteration 207/1000 | Loss: 0.00001121
Iteration 208/1000 | Loss: 0.00001121
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001121
Iteration 212/1000 | Loss: 0.00001121
Iteration 213/1000 | Loss: 0.00001121
Iteration 214/1000 | Loss: 0.00001121
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001121
Iteration 218/1000 | Loss: 0.00001121
Iteration 219/1000 | Loss: 0.00001121
Iteration 220/1000 | Loss: 0.00001121
Iteration 221/1000 | Loss: 0.00001121
Iteration 222/1000 | Loss: 0.00001121
Iteration 223/1000 | Loss: 0.00001121
Iteration 224/1000 | Loss: 0.00001121
Iteration 225/1000 | Loss: 0.00001121
Iteration 226/1000 | Loss: 0.00001121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1210264347027987e-05, 1.1210264347027987e-05, 1.1210264347027987e-05, 1.1210264347027987e-05, 1.1210264347027987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1210264347027987e-05

Optimization complete. Final v2v error: 2.9162559509277344 mm

Highest mean error: 3.151951551437378 mm for frame 61

Lowest mean error: 2.7800776958465576 mm for frame 26

Saving results

Total time: 42.235138177871704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833965
Iteration 2/25 | Loss: 0.00143710
Iteration 3/25 | Loss: 0.00136676
Iteration 4/25 | Loss: 0.00135412
Iteration 5/25 | Loss: 0.00134988
Iteration 6/25 | Loss: 0.00134877
Iteration 7/25 | Loss: 0.00134873
Iteration 8/25 | Loss: 0.00134873
Iteration 9/25 | Loss: 0.00134873
Iteration 10/25 | Loss: 0.00134873
Iteration 11/25 | Loss: 0.00134873
Iteration 12/25 | Loss: 0.00134873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013487295946106315, 0.0013487295946106315, 0.0013487295946106315, 0.0013487295946106315, 0.0013487295946106315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013487295946106315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90316069
Iteration 2/25 | Loss: 0.00214748
Iteration 3/25 | Loss: 0.00214747
Iteration 4/25 | Loss: 0.00214747
Iteration 5/25 | Loss: 0.00214747
Iteration 6/25 | Loss: 0.00214747
Iteration 7/25 | Loss: 0.00214747
Iteration 8/25 | Loss: 0.00214747
Iteration 9/25 | Loss: 0.00214747
Iteration 10/25 | Loss: 0.00214747
Iteration 11/25 | Loss: 0.00214747
Iteration 12/25 | Loss: 0.00214747
Iteration 13/25 | Loss: 0.00214747
Iteration 14/25 | Loss: 0.00214747
Iteration 15/25 | Loss: 0.00214747
Iteration 16/25 | Loss: 0.00214747
Iteration 17/25 | Loss: 0.00214747
Iteration 18/25 | Loss: 0.00214747
Iteration 19/25 | Loss: 0.00214747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002147465944290161, 0.002147465944290161, 0.002147465944290161, 0.002147465944290161, 0.002147465944290161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002147465944290161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214747
Iteration 2/1000 | Loss: 0.00002267
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001299
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001196
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001181
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001164
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001161
Iteration 146/1000 | Loss: 0.00001161
Iteration 147/1000 | Loss: 0.00001161
Iteration 148/1000 | Loss: 0.00001161
Iteration 149/1000 | Loss: 0.00001161
Iteration 150/1000 | Loss: 0.00001161
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001159
Iteration 161/1000 | Loss: 0.00001159
Iteration 162/1000 | Loss: 0.00001159
Iteration 163/1000 | Loss: 0.00001159
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001158
Iteration 169/1000 | Loss: 0.00001158
Iteration 170/1000 | Loss: 0.00001158
Iteration 171/1000 | Loss: 0.00001158
Iteration 172/1000 | Loss: 0.00001158
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001158
Iteration 177/1000 | Loss: 0.00001158
Iteration 178/1000 | Loss: 0.00001158
Iteration 179/1000 | Loss: 0.00001158
Iteration 180/1000 | Loss: 0.00001158
Iteration 181/1000 | Loss: 0.00001158
Iteration 182/1000 | Loss: 0.00001158
Iteration 183/1000 | Loss: 0.00001158
Iteration 184/1000 | Loss: 0.00001158
Iteration 185/1000 | Loss: 0.00001158
Iteration 186/1000 | Loss: 0.00001158
Iteration 187/1000 | Loss: 0.00001158
Iteration 188/1000 | Loss: 0.00001158
Iteration 189/1000 | Loss: 0.00001158
Iteration 190/1000 | Loss: 0.00001158
Iteration 191/1000 | Loss: 0.00001158
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1577159057196695e-05, 1.1577159057196695e-05, 1.1577159057196695e-05, 1.1577159057196695e-05, 1.1577159057196695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1577159057196695e-05

Optimization complete. Final v2v error: 2.9362926483154297 mm

Highest mean error: 3.414813995361328 mm for frame 67

Lowest mean error: 2.747286081314087 mm for frame 100

Saving results

Total time: 40.13077449798584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451933
Iteration 2/25 | Loss: 0.00152198
Iteration 3/25 | Loss: 0.00140324
Iteration 4/25 | Loss: 0.00139070
Iteration 5/25 | Loss: 0.00138664
Iteration 6/25 | Loss: 0.00138621
Iteration 7/25 | Loss: 0.00138621
Iteration 8/25 | Loss: 0.00138621
Iteration 9/25 | Loss: 0.00138621
Iteration 10/25 | Loss: 0.00138621
Iteration 11/25 | Loss: 0.00138621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013862100895494223, 0.0013862100895494223, 0.0013862100895494223, 0.0013862100895494223, 0.0013862100895494223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013862100895494223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25414407
Iteration 2/25 | Loss: 0.00230871
Iteration 3/25 | Loss: 0.00230871
Iteration 4/25 | Loss: 0.00230871
Iteration 5/25 | Loss: 0.00230871
Iteration 6/25 | Loss: 0.00230870
Iteration 7/25 | Loss: 0.00230870
Iteration 8/25 | Loss: 0.00230870
Iteration 9/25 | Loss: 0.00230870
Iteration 10/25 | Loss: 0.00230870
Iteration 11/25 | Loss: 0.00230870
Iteration 12/25 | Loss: 0.00230870
Iteration 13/25 | Loss: 0.00230870
Iteration 14/25 | Loss: 0.00230870
Iteration 15/25 | Loss: 0.00230870
Iteration 16/25 | Loss: 0.00230870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023087027948349714, 0.0023087027948349714, 0.0023087027948349714, 0.0023087027948349714, 0.0023087027948349714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023087027948349714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230870
Iteration 2/1000 | Loss: 0.00002426
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001423
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.32930108520668e-05, 1.32930108520668e-05, 1.32930108520668e-05, 1.32930108520668e-05, 1.32930108520668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.32930108520668e-05

Optimization complete. Final v2v error: 3.0842385292053223 mm

Highest mean error: 3.543959379196167 mm for frame 77

Lowest mean error: 2.815796375274658 mm for frame 160

Saving results

Total time: 39.07404589653015
