Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=3, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 168-223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002431
Iteration 2/25 | Loss: 0.00159675
Iteration 3/25 | Loss: 0.00126248
Iteration 4/25 | Loss: 0.00117573
Iteration 5/25 | Loss: 0.00113105
Iteration 6/25 | Loss: 0.00111969
Iteration 7/25 | Loss: 0.00111918
Iteration 8/25 | Loss: 0.00111577
Iteration 9/25 | Loss: 0.00111279
Iteration 10/25 | Loss: 0.00111198
Iteration 11/25 | Loss: 0.00111161
Iteration 12/25 | Loss: 0.00111144
Iteration 13/25 | Loss: 0.00111127
Iteration 14/25 | Loss: 0.00111121
Iteration 15/25 | Loss: 0.00111121
Iteration 16/25 | Loss: 0.00111120
Iteration 17/25 | Loss: 0.00111120
Iteration 18/25 | Loss: 0.00111120
Iteration 19/25 | Loss: 0.00111120
Iteration 20/25 | Loss: 0.00111120
Iteration 21/25 | Loss: 0.00111120
Iteration 22/25 | Loss: 0.00111120
Iteration 23/25 | Loss: 0.00111120
Iteration 24/25 | Loss: 0.00111120
Iteration 25/25 | Loss: 0.00111120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41363323
Iteration 2/25 | Loss: 0.00111939
Iteration 3/25 | Loss: 0.00111921
Iteration 4/25 | Loss: 0.00111921
Iteration 5/25 | Loss: 0.00111921
Iteration 6/25 | Loss: 0.00111921
Iteration 7/25 | Loss: 0.00111921
Iteration 8/25 | Loss: 0.00111921
Iteration 9/25 | Loss: 0.00111921
Iteration 10/25 | Loss: 0.00111921
Iteration 11/25 | Loss: 0.00111921
Iteration 12/25 | Loss: 0.00111921
Iteration 13/25 | Loss: 0.00111921
Iteration 14/25 | Loss: 0.00111921
Iteration 15/25 | Loss: 0.00111921
Iteration 16/25 | Loss: 0.00111921
Iteration 17/25 | Loss: 0.00111921
Iteration 18/25 | Loss: 0.00111921
Iteration 19/25 | Loss: 0.00111921
Iteration 20/25 | Loss: 0.00111921
Iteration 21/25 | Loss: 0.00111921
Iteration 22/25 | Loss: 0.00111921
Iteration 23/25 | Loss: 0.00111921
Iteration 24/25 | Loss: 0.00111921
Iteration 25/25 | Loss: 0.00111921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111921
Iteration 2/1000 | Loss: 0.00018173
Iteration 3/1000 | Loss: 0.00012409
Iteration 4/1000 | Loss: 0.00010269
Iteration 5/1000 | Loss: 0.00009269
Iteration 6/1000 | Loss: 0.00047185
Iteration 7/1000 | Loss: 0.00109488
Iteration 8/1000 | Loss: 0.00149301
Iteration 9/1000 | Loss: 0.00055095
Iteration 10/1000 | Loss: 0.00069616
Iteration 11/1000 | Loss: 0.00215841
Iteration 12/1000 | Loss: 0.00161484
Iteration 13/1000 | Loss: 0.00009322
Iteration 14/1000 | Loss: 0.00008263
Iteration 15/1000 | Loss: 0.00090980
Iteration 16/1000 | Loss: 0.00098531
Iteration 17/1000 | Loss: 0.00070844
Iteration 18/1000 | Loss: 0.00094241
Iteration 19/1000 | Loss: 0.00057531
Iteration 20/1000 | Loss: 0.00143979
Iteration 21/1000 | Loss: 0.00088035
Iteration 22/1000 | Loss: 0.00111214
Iteration 23/1000 | Loss: 0.00082030
Iteration 24/1000 | Loss: 0.00009774
Iteration 25/1000 | Loss: 0.00022636
Iteration 26/1000 | Loss: 0.00065542
Iteration 27/1000 | Loss: 0.00029905
Iteration 28/1000 | Loss: 0.00009034
Iteration 29/1000 | Loss: 0.00008412
Iteration 30/1000 | Loss: 0.00007914
Iteration 31/1000 | Loss: 0.00032182
Iteration 32/1000 | Loss: 0.00043970
Iteration 33/1000 | Loss: 0.00053140
Iteration 34/1000 | Loss: 0.00037091
Iteration 35/1000 | Loss: 0.00007601
Iteration 36/1000 | Loss: 0.00043721
Iteration 37/1000 | Loss: 0.00030634
Iteration 38/1000 | Loss: 0.00037222
Iteration 39/1000 | Loss: 0.00040277
Iteration 40/1000 | Loss: 0.00032213
Iteration 41/1000 | Loss: 0.00043543
Iteration 42/1000 | Loss: 0.00029391
Iteration 43/1000 | Loss: 0.00042412
Iteration 44/1000 | Loss: 0.00014638
Iteration 45/1000 | Loss: 0.00053877
Iteration 46/1000 | Loss: 0.00076886
Iteration 47/1000 | Loss: 0.00009963
Iteration 48/1000 | Loss: 0.00080816
Iteration 49/1000 | Loss: 0.00088956
Iteration 50/1000 | Loss: 0.00085827
Iteration 51/1000 | Loss: 0.00063585
Iteration 52/1000 | Loss: 0.00050503
Iteration 53/1000 | Loss: 0.00054658
Iteration 54/1000 | Loss: 0.00041555
Iteration 55/1000 | Loss: 0.00007925
Iteration 56/1000 | Loss: 0.00007369
Iteration 57/1000 | Loss: 0.00082430
Iteration 58/1000 | Loss: 0.00064035
Iteration 59/1000 | Loss: 0.00028071
Iteration 60/1000 | Loss: 0.00007649
Iteration 61/1000 | Loss: 0.00006910
Iteration 62/1000 | Loss: 0.00048025
Iteration 63/1000 | Loss: 0.00021959
Iteration 64/1000 | Loss: 0.00007935
Iteration 65/1000 | Loss: 0.00033804
Iteration 66/1000 | Loss: 0.00006519
Iteration 67/1000 | Loss: 0.00006278
Iteration 68/1000 | Loss: 0.00010069
Iteration 69/1000 | Loss: 0.00006149
Iteration 70/1000 | Loss: 0.00028919
Iteration 71/1000 | Loss: 0.00006070
Iteration 72/1000 | Loss: 0.00034025
Iteration 73/1000 | Loss: 0.00038236
Iteration 74/1000 | Loss: 0.00010851
Iteration 75/1000 | Loss: 0.00020363
Iteration 76/1000 | Loss: 0.00048299
Iteration 77/1000 | Loss: 0.00048369
Iteration 78/1000 | Loss: 0.00013357
Iteration 79/1000 | Loss: 0.00041960
Iteration 80/1000 | Loss: 0.00017518
Iteration 81/1000 | Loss: 0.00026244
Iteration 82/1000 | Loss: 0.00021734
Iteration 83/1000 | Loss: 0.00009805
Iteration 84/1000 | Loss: 0.00011186
Iteration 85/1000 | Loss: 0.00011798
Iteration 86/1000 | Loss: 0.00009473
Iteration 87/1000 | Loss: 0.00008443
Iteration 88/1000 | Loss: 0.00032243
Iteration 89/1000 | Loss: 0.00047225
Iteration 90/1000 | Loss: 0.00055703
Iteration 91/1000 | Loss: 0.00008336
Iteration 92/1000 | Loss: 0.00006622
Iteration 93/1000 | Loss: 0.00006143
Iteration 94/1000 | Loss: 0.00006090
Iteration 95/1000 | Loss: 0.00005789
Iteration 96/1000 | Loss: 0.00005649
Iteration 97/1000 | Loss: 0.00005559
Iteration 98/1000 | Loss: 0.00005508
Iteration 99/1000 | Loss: 0.00066007
Iteration 100/1000 | Loss: 0.00005662
Iteration 101/1000 | Loss: 0.00005403
Iteration 102/1000 | Loss: 0.00005343
Iteration 103/1000 | Loss: 0.00005282
Iteration 104/1000 | Loss: 0.00005241
Iteration 105/1000 | Loss: 0.00005220
Iteration 106/1000 | Loss: 0.00005210
Iteration 107/1000 | Loss: 0.00005207
Iteration 108/1000 | Loss: 0.00005205
Iteration 109/1000 | Loss: 0.00005204
Iteration 110/1000 | Loss: 0.00005200
Iteration 111/1000 | Loss: 0.00005195
Iteration 112/1000 | Loss: 0.00005195
Iteration 113/1000 | Loss: 0.00005195
Iteration 114/1000 | Loss: 0.00005194
Iteration 115/1000 | Loss: 0.00005194
Iteration 116/1000 | Loss: 0.00005193
Iteration 117/1000 | Loss: 0.00005193
Iteration 118/1000 | Loss: 0.00005192
Iteration 119/1000 | Loss: 0.00005186
Iteration 120/1000 | Loss: 0.00005184
Iteration 121/1000 | Loss: 0.00005184
Iteration 122/1000 | Loss: 0.00005183
Iteration 123/1000 | Loss: 0.00005183
Iteration 124/1000 | Loss: 0.00005183
Iteration 125/1000 | Loss: 0.00005182
Iteration 126/1000 | Loss: 0.00005182
Iteration 127/1000 | Loss: 0.00005181
Iteration 128/1000 | Loss: 0.00005181
Iteration 129/1000 | Loss: 0.00005180
Iteration 130/1000 | Loss: 0.00005180
Iteration 131/1000 | Loss: 0.00005179
Iteration 132/1000 | Loss: 0.00005178
Iteration 133/1000 | Loss: 0.00005176
Iteration 134/1000 | Loss: 0.00005174
Iteration 135/1000 | Loss: 0.00005174
Iteration 136/1000 | Loss: 0.00005173
Iteration 137/1000 | Loss: 0.00005173
Iteration 138/1000 | Loss: 0.00005172
Iteration 139/1000 | Loss: 0.00005171
Iteration 140/1000 | Loss: 0.00005171
Iteration 141/1000 | Loss: 0.00005170
Iteration 142/1000 | Loss: 0.00005169
Iteration 143/1000 | Loss: 0.00005168
Iteration 144/1000 | Loss: 0.00005168
Iteration 145/1000 | Loss: 0.00005168
Iteration 146/1000 | Loss: 0.00005167
Iteration 147/1000 | Loss: 0.00005165
Iteration 148/1000 | Loss: 0.00005165
Iteration 149/1000 | Loss: 0.00005163
Iteration 150/1000 | Loss: 0.00005162
Iteration 151/1000 | Loss: 0.00005161
Iteration 152/1000 | Loss: 0.00005161
Iteration 153/1000 | Loss: 0.00005161
Iteration 154/1000 | Loss: 0.00005161
Iteration 155/1000 | Loss: 0.00005161
Iteration 156/1000 | Loss: 0.00005160
Iteration 157/1000 | Loss: 0.00005160
Iteration 158/1000 | Loss: 0.00005160
Iteration 159/1000 | Loss: 0.00005160
Iteration 160/1000 | Loss: 0.00005160
Iteration 161/1000 | Loss: 0.00005160
Iteration 162/1000 | Loss: 0.00005160
Iteration 163/1000 | Loss: 0.00005160
Iteration 164/1000 | Loss: 0.00005160
Iteration 165/1000 | Loss: 0.00005160
Iteration 166/1000 | Loss: 0.00005160
Iteration 167/1000 | Loss: 0.00005160
Iteration 168/1000 | Loss: 0.00005160
Iteration 169/1000 | Loss: 0.00005160
Iteration 170/1000 | Loss: 0.00005160
Iteration 171/1000 | Loss: 0.00005159
Iteration 172/1000 | Loss: 0.00005159
Iteration 173/1000 | Loss: 0.00005159
Iteration 174/1000 | Loss: 0.00005159
Iteration 175/1000 | Loss: 0.00005159
Iteration 176/1000 | Loss: 0.00005159
Iteration 177/1000 | Loss: 0.00005159
Iteration 178/1000 | Loss: 0.00005158
Iteration 179/1000 | Loss: 0.00005158
Iteration 180/1000 | Loss: 0.00005158
Iteration 181/1000 | Loss: 0.00005158
Iteration 182/1000 | Loss: 0.00005158
Iteration 183/1000 | Loss: 0.00005158
Iteration 184/1000 | Loss: 0.00005158
Iteration 185/1000 | Loss: 0.00005158
Iteration 186/1000 | Loss: 0.00005158
Iteration 187/1000 | Loss: 0.00005158
Iteration 188/1000 | Loss: 0.00005158
Iteration 189/1000 | Loss: 0.00005158
Iteration 190/1000 | Loss: 0.00005158
Iteration 191/1000 | Loss: 0.00005157
Iteration 192/1000 | Loss: 0.00005157
Iteration 193/1000 | Loss: 0.00005157
Iteration 194/1000 | Loss: 0.00005157
Iteration 195/1000 | Loss: 0.00005157
Iteration 196/1000 | Loss: 0.00005157
Iteration 197/1000 | Loss: 0.00005157
Iteration 198/1000 | Loss: 0.00005157
Iteration 199/1000 | Loss: 0.00005157
Iteration 200/1000 | Loss: 0.00005156
Iteration 201/1000 | Loss: 0.00005156
Iteration 202/1000 | Loss: 0.00005156
Iteration 203/1000 | Loss: 0.00005156
Iteration 204/1000 | Loss: 0.00005155
Iteration 205/1000 | Loss: 0.00005155
Iteration 206/1000 | Loss: 0.00005155
Iteration 207/1000 | Loss: 0.00005155
Iteration 208/1000 | Loss: 0.00005155
Iteration 209/1000 | Loss: 0.00005155
Iteration 210/1000 | Loss: 0.00005155
Iteration 211/1000 | Loss: 0.00005155
Iteration 212/1000 | Loss: 0.00005155
Iteration 213/1000 | Loss: 0.00005155
Iteration 214/1000 | Loss: 0.00005155
Iteration 215/1000 | Loss: 0.00005155
Iteration 216/1000 | Loss: 0.00005155
Iteration 217/1000 | Loss: 0.00005155
Iteration 218/1000 | Loss: 0.00005155
Iteration 219/1000 | Loss: 0.00005155
Iteration 220/1000 | Loss: 0.00005155
Iteration 221/1000 | Loss: 0.00005155
Iteration 222/1000 | Loss: 0.00005155
Iteration 223/1000 | Loss: 0.00005155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [5.154505925020203e-05, 5.154505925020203e-05, 5.154505925020203e-05, 5.154505925020203e-05, 5.154505925020203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.154505925020203e-05

Optimization complete. Final v2v error: 5.683901786804199 mm

Highest mean error: 7.792541980743408 mm for frame 143

Lowest mean error: 3.7268640995025635 mm for frame 40

Saving results

Total time: 208.86437916755676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771120
Iteration 2/25 | Loss: 0.00137447
Iteration 3/25 | Loss: 0.00101325
Iteration 4/25 | Loss: 0.00093055
Iteration 5/25 | Loss: 0.00090958
Iteration 6/25 | Loss: 0.00089677
Iteration 7/25 | Loss: 0.00089804
Iteration 8/25 | Loss: 0.00089726
Iteration 9/25 | Loss: 0.00089506
Iteration 10/25 | Loss: 0.00089434
Iteration 11/25 | Loss: 0.00089316
Iteration 12/25 | Loss: 0.00090768
Iteration 13/25 | Loss: 0.00088852
Iteration 14/25 | Loss: 0.00088406
Iteration 15/25 | Loss: 0.00088352
Iteration 16/25 | Loss: 0.00088349
Iteration 17/25 | Loss: 0.00088349
Iteration 18/25 | Loss: 0.00088349
Iteration 19/25 | Loss: 0.00088349
Iteration 20/25 | Loss: 0.00088349
Iteration 21/25 | Loss: 0.00088349
Iteration 22/25 | Loss: 0.00088349
Iteration 23/25 | Loss: 0.00088349
Iteration 24/25 | Loss: 0.00088349
Iteration 25/25 | Loss: 0.00088349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.08965015
Iteration 2/25 | Loss: 0.00058310
Iteration 3/25 | Loss: 0.00058303
Iteration 4/25 | Loss: 0.00058303
Iteration 5/25 | Loss: 0.00058303
Iteration 6/25 | Loss: 0.00058303
Iteration 7/25 | Loss: 0.00058303
Iteration 8/25 | Loss: 0.00058303
Iteration 9/25 | Loss: 0.00058303
Iteration 10/25 | Loss: 0.00058303
Iteration 11/25 | Loss: 0.00058303
Iteration 12/25 | Loss: 0.00058303
Iteration 13/25 | Loss: 0.00058303
Iteration 14/25 | Loss: 0.00058303
Iteration 15/25 | Loss: 0.00058303
Iteration 16/25 | Loss: 0.00058303
Iteration 17/25 | Loss: 0.00058303
Iteration 18/25 | Loss: 0.00058303
Iteration 19/25 | Loss: 0.00058303
Iteration 20/25 | Loss: 0.00058303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005830260924994946, 0.0005830260924994946, 0.0005830260924994946, 0.0005830260924994946, 0.0005830260924994946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005830260924994946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058303
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00007095
Iteration 5/1000 | Loss: 0.00004235
Iteration 6/1000 | Loss: 0.00002270
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00001977
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00007396
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001886
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00005049
Iteration 15/1000 | Loss: 0.00002977
Iteration 16/1000 | Loss: 0.00002717
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001820
Iteration 29/1000 | Loss: 0.00001819
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001809
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00008654
Iteration 40/1000 | Loss: 0.00003228
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001800
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001797
Iteration 61/1000 | Loss: 0.00001797
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001797
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.7965428924071603e-05, 1.7965428924071603e-05, 1.7965428924071603e-05, 1.7965428924071603e-05, 1.7965428924071603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7965428924071603e-05

Optimization complete. Final v2v error: 3.548870325088501 mm

Highest mean error: 4.483016014099121 mm for frame 137

Lowest mean error: 2.9944732189178467 mm for frame 231

Saving results

Total time: 65.93313646316528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021224
Iteration 2/25 | Loss: 0.00237459
Iteration 3/25 | Loss: 0.00156247
Iteration 4/25 | Loss: 0.00138625
Iteration 5/25 | Loss: 0.00144056
Iteration 6/25 | Loss: 0.00132751
Iteration 7/25 | Loss: 0.00124327
Iteration 8/25 | Loss: 0.00118618
Iteration 9/25 | Loss: 0.00112339
Iteration 10/25 | Loss: 0.00103976
Iteration 11/25 | Loss: 0.00100213
Iteration 12/25 | Loss: 0.00098250
Iteration 13/25 | Loss: 0.00097411
Iteration 14/25 | Loss: 0.00097180
Iteration 15/25 | Loss: 0.00096810
Iteration 16/25 | Loss: 0.00096302
Iteration 17/25 | Loss: 0.00096135
Iteration 18/25 | Loss: 0.00096071
Iteration 19/25 | Loss: 0.00096056
Iteration 20/25 | Loss: 0.00096055
Iteration 21/25 | Loss: 0.00096055
Iteration 22/25 | Loss: 0.00096054
Iteration 23/25 | Loss: 0.00096054
Iteration 24/25 | Loss: 0.00096054
Iteration 25/25 | Loss: 0.00096054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50346363
Iteration 2/25 | Loss: 0.00056983
Iteration 3/25 | Loss: 0.00056982
Iteration 4/25 | Loss: 0.00056982
Iteration 5/25 | Loss: 0.00056982
Iteration 6/25 | Loss: 0.00056982
Iteration 7/25 | Loss: 0.00056982
Iteration 8/25 | Loss: 0.00056982
Iteration 9/25 | Loss: 0.00056982
Iteration 10/25 | Loss: 0.00056982
Iteration 11/25 | Loss: 0.00056982
Iteration 12/25 | Loss: 0.00056982
Iteration 13/25 | Loss: 0.00056982
Iteration 14/25 | Loss: 0.00056982
Iteration 15/25 | Loss: 0.00056982
Iteration 16/25 | Loss: 0.00056982
Iteration 17/25 | Loss: 0.00056982
Iteration 18/25 | Loss: 0.00056982
Iteration 19/25 | Loss: 0.00056982
Iteration 20/25 | Loss: 0.00056982
Iteration 21/25 | Loss: 0.00056982
Iteration 22/25 | Loss: 0.00056982
Iteration 23/25 | Loss: 0.00056982
Iteration 24/25 | Loss: 0.00056982
Iteration 25/25 | Loss: 0.00056982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056982
Iteration 2/1000 | Loss: 0.00003168
Iteration 3/1000 | Loss: 0.00002608
Iteration 4/1000 | Loss: 0.00002446
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002297
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002279
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002279
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002260
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002189
Iteration 17/1000 | Loss: 0.00002189
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002186
Iteration 20/1000 | Loss: 0.00002183
Iteration 21/1000 | Loss: 0.00002182
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002182
Iteration 24/1000 | Loss: 0.00002182
Iteration 25/1000 | Loss: 0.00002182
Iteration 26/1000 | Loss: 0.00002182
Iteration 27/1000 | Loss: 0.00002182
Iteration 28/1000 | Loss: 0.00002182
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00002180
Iteration 32/1000 | Loss: 0.00002179
Iteration 33/1000 | Loss: 0.00002179
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002176
Iteration 36/1000 | Loss: 0.00002176
Iteration 37/1000 | Loss: 0.00002175
Iteration 38/1000 | Loss: 0.00002175
Iteration 39/1000 | Loss: 0.00002173
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00002168
Iteration 42/1000 | Loss: 0.00002168
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002167
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002166
Iteration 47/1000 | Loss: 0.00002166
Iteration 48/1000 | Loss: 0.00002165
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00002165
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00002165
Iteration 54/1000 | Loss: 0.00002165
Iteration 55/1000 | Loss: 0.00002165
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002165
Iteration 59/1000 | Loss: 0.00002164
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002164
Iteration 62/1000 | Loss: 0.00002164
Iteration 63/1000 | Loss: 0.00002164
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002164
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002163
Iteration 81/1000 | Loss: 0.00002163
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002163
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002162
Iteration 93/1000 | Loss: 0.00002162
Iteration 94/1000 | Loss: 0.00002162
Iteration 95/1000 | Loss: 0.00002162
Iteration 96/1000 | Loss: 0.00002162
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002161
Iteration 100/1000 | Loss: 0.00002161
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002161
Iteration 103/1000 | Loss: 0.00002161
Iteration 104/1000 | Loss: 0.00002161
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002161
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002160
Iteration 109/1000 | Loss: 0.00002160
Iteration 110/1000 | Loss: 0.00002160
Iteration 111/1000 | Loss: 0.00002160
Iteration 112/1000 | Loss: 0.00002160
Iteration 113/1000 | Loss: 0.00002160
Iteration 114/1000 | Loss: 0.00002160
Iteration 115/1000 | Loss: 0.00002160
Iteration 116/1000 | Loss: 0.00002159
Iteration 117/1000 | Loss: 0.00002159
Iteration 118/1000 | Loss: 0.00002159
Iteration 119/1000 | Loss: 0.00002159
Iteration 120/1000 | Loss: 0.00002159
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002159
Iteration 126/1000 | Loss: 0.00002159
Iteration 127/1000 | Loss: 0.00002159
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002159
Iteration 131/1000 | Loss: 0.00002159
Iteration 132/1000 | Loss: 0.00002158
Iteration 133/1000 | Loss: 0.00002158
Iteration 134/1000 | Loss: 0.00002158
Iteration 135/1000 | Loss: 0.00002158
Iteration 136/1000 | Loss: 0.00002158
Iteration 137/1000 | Loss: 0.00002158
Iteration 138/1000 | Loss: 0.00002158
Iteration 139/1000 | Loss: 0.00002158
Iteration 140/1000 | Loss: 0.00002158
Iteration 141/1000 | Loss: 0.00002158
Iteration 142/1000 | Loss: 0.00002158
Iteration 143/1000 | Loss: 0.00002158
Iteration 144/1000 | Loss: 0.00002158
Iteration 145/1000 | Loss: 0.00002158
Iteration 146/1000 | Loss: 0.00002158
Iteration 147/1000 | Loss: 0.00002158
Iteration 148/1000 | Loss: 0.00002158
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00002158
Iteration 151/1000 | Loss: 0.00002158
Iteration 152/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.1578225641860627e-05, 2.1578225641860627e-05, 2.1578225641860627e-05, 2.1578225641860627e-05, 2.1578225641860627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1578225641860627e-05

Optimization complete. Final v2v error: 3.833909749984741 mm

Highest mean error: 3.8965466022491455 mm for frame 53

Lowest mean error: 3.7727127075195312 mm for frame 93

Saving results

Total time: 67.20189762115479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416670
Iteration 2/25 | Loss: 0.00092120
Iteration 3/25 | Loss: 0.00084280
Iteration 4/25 | Loss: 0.00083301
Iteration 5/25 | Loss: 0.00082922
Iteration 6/25 | Loss: 0.00082885
Iteration 7/25 | Loss: 0.00082885
Iteration 8/25 | Loss: 0.00082885
Iteration 9/25 | Loss: 0.00082885
Iteration 10/25 | Loss: 0.00082885
Iteration 11/25 | Loss: 0.00082885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008288505487143993, 0.0008288505487143993, 0.0008288505487143993, 0.0008288505487143993, 0.0008288505487143993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008288505487143993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51353621
Iteration 2/25 | Loss: 0.00052458
Iteration 3/25 | Loss: 0.00052458
Iteration 4/25 | Loss: 0.00052458
Iteration 5/25 | Loss: 0.00052458
Iteration 6/25 | Loss: 0.00052458
Iteration 7/25 | Loss: 0.00052458
Iteration 8/25 | Loss: 0.00052458
Iteration 9/25 | Loss: 0.00052458
Iteration 10/25 | Loss: 0.00052458
Iteration 11/25 | Loss: 0.00052458
Iteration 12/25 | Loss: 0.00052458
Iteration 13/25 | Loss: 0.00052458
Iteration 14/25 | Loss: 0.00052458
Iteration 15/25 | Loss: 0.00052458
Iteration 16/25 | Loss: 0.00052458
Iteration 17/25 | Loss: 0.00052458
Iteration 18/25 | Loss: 0.00052458
Iteration 19/25 | Loss: 0.00052458
Iteration 20/25 | Loss: 0.00052458
Iteration 21/25 | Loss: 0.00052458
Iteration 22/25 | Loss: 0.00052458
Iteration 23/25 | Loss: 0.00052458
Iteration 24/25 | Loss: 0.00052458
Iteration 25/25 | Loss: 0.00052458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052458
Iteration 2/1000 | Loss: 0.00002135
Iteration 3/1000 | Loss: 0.00001569
Iteration 4/1000 | Loss: 0.00001425
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001207
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001179
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001174
Iteration 125/1000 | Loss: 0.00001174
Iteration 126/1000 | Loss: 0.00001174
Iteration 127/1000 | Loss: 0.00001174
Iteration 128/1000 | Loss: 0.00001174
Iteration 129/1000 | Loss: 0.00001174
Iteration 130/1000 | Loss: 0.00001174
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.1743383765860926e-05, 1.1743383765860926e-05, 1.1743383765860926e-05, 1.1743383765860926e-05, 1.1743383765860926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1743383765860926e-05

Optimization complete. Final v2v error: 2.9521846771240234 mm

Highest mean error: 3.1029443740844727 mm for frame 182

Lowest mean error: 2.8775429725646973 mm for frame 44

Saving results

Total time: 37.184791564941406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433981
Iteration 2/25 | Loss: 0.00098276
Iteration 3/25 | Loss: 0.00087826
Iteration 4/25 | Loss: 0.00086896
Iteration 5/25 | Loss: 0.00086578
Iteration 6/25 | Loss: 0.00086504
Iteration 7/25 | Loss: 0.00086504
Iteration 8/25 | Loss: 0.00086504
Iteration 9/25 | Loss: 0.00086504
Iteration 10/25 | Loss: 0.00086504
Iteration 11/25 | Loss: 0.00086504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008650381350889802, 0.0008650381350889802, 0.0008650381350889802, 0.0008650381350889802, 0.0008650381350889802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008650381350889802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53691483
Iteration 2/25 | Loss: 0.00053852
Iteration 3/25 | Loss: 0.00053852
Iteration 4/25 | Loss: 0.00053852
Iteration 5/25 | Loss: 0.00053852
Iteration 6/25 | Loss: 0.00053852
Iteration 7/25 | Loss: 0.00053852
Iteration 8/25 | Loss: 0.00053852
Iteration 9/25 | Loss: 0.00053852
Iteration 10/25 | Loss: 0.00053852
Iteration 11/25 | Loss: 0.00053852
Iteration 12/25 | Loss: 0.00053852
Iteration 13/25 | Loss: 0.00053852
Iteration 14/25 | Loss: 0.00053852
Iteration 15/25 | Loss: 0.00053852
Iteration 16/25 | Loss: 0.00053852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005385206313803792, 0.0005385206313803792, 0.0005385206313803792, 0.0005385206313803792, 0.0005385206313803792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005385206313803792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053852
Iteration 2/1000 | Loss: 0.00002296
Iteration 3/1000 | Loss: 0.00001801
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001518
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001486
Iteration 12/1000 | Loss: 0.00001482
Iteration 13/1000 | Loss: 0.00001479
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001476
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001475
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001472
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001472
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001471
Iteration 60/1000 | Loss: 0.00001471
Iteration 61/1000 | Loss: 0.00001471
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001471
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001471
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001471
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001471
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001471
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.47135315273772e-05, 1.47135315273772e-05, 1.47135315273772e-05, 1.47135315273772e-05, 1.47135315273772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.47135315273772e-05

Optimization complete. Final v2v error: 3.2522385120391846 mm

Highest mean error: 3.320019245147705 mm for frame 170

Lowest mean error: 3.200371742248535 mm for frame 13

Saving results

Total time: 26.708112716674805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067187
Iteration 2/25 | Loss: 0.01067186
Iteration 3/25 | Loss: 0.01067186
Iteration 4/25 | Loss: 0.01067186
Iteration 5/25 | Loss: 0.01067186
Iteration 6/25 | Loss: 0.01067186
Iteration 7/25 | Loss: 0.01067185
Iteration 8/25 | Loss: 0.01067185
Iteration 9/25 | Loss: 0.01067185
Iteration 10/25 | Loss: 0.01067185
Iteration 11/25 | Loss: 0.01067185
Iteration 12/25 | Loss: 0.01067185
Iteration 13/25 | Loss: 0.01067184
Iteration 14/25 | Loss: 0.01067184
Iteration 15/25 | Loss: 0.01067184
Iteration 16/25 | Loss: 0.01067184
Iteration 17/25 | Loss: 0.01067184
Iteration 18/25 | Loss: 0.01067184
Iteration 19/25 | Loss: 0.01067183
Iteration 20/25 | Loss: 0.01067183
Iteration 21/25 | Loss: 0.01067183
Iteration 22/25 | Loss: 0.01067183
Iteration 23/25 | Loss: 0.01067183
Iteration 24/25 | Loss: 0.01067183
Iteration 25/25 | Loss: 0.01067182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11353207
Iteration 2/25 | Loss: 0.18179880
Iteration 3/25 | Loss: 0.17480157
Iteration 4/25 | Loss: 0.17290781
Iteration 5/25 | Loss: 0.17278054
Iteration 6/25 | Loss: 0.17278053
Iteration 7/25 | Loss: 0.17278048
Iteration 8/25 | Loss: 0.17278048
Iteration 9/25 | Loss: 0.17278048
Iteration 10/25 | Loss: 0.17278048
Iteration 11/25 | Loss: 0.17278047
Iteration 12/25 | Loss: 0.17278047
Iteration 13/25 | Loss: 0.17278047
Iteration 14/25 | Loss: 0.17278047
Iteration 15/25 | Loss: 0.17278044
Iteration 16/25 | Loss: 0.17278044
Iteration 17/25 | Loss: 0.17278044
Iteration 18/25 | Loss: 0.17278044
Iteration 19/25 | Loss: 0.17278044
Iteration 20/25 | Loss: 0.17278044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.17278043925762177, 0.17278043925762177, 0.17278043925762177, 0.17278043925762177, 0.17278043925762177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17278043925762177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17278044
Iteration 2/1000 | Loss: 0.00632196
Iteration 3/1000 | Loss: 0.00452768
Iteration 4/1000 | Loss: 0.00191958
Iteration 5/1000 | Loss: 0.00387007
Iteration 6/1000 | Loss: 0.00278258
Iteration 7/1000 | Loss: 0.00073275
Iteration 8/1000 | Loss: 0.00074061
Iteration 9/1000 | Loss: 0.00089489
Iteration 10/1000 | Loss: 0.00018151
Iteration 11/1000 | Loss: 0.00016611
Iteration 12/1000 | Loss: 0.00006079
Iteration 13/1000 | Loss: 0.00054347
Iteration 14/1000 | Loss: 0.00274111
Iteration 15/1000 | Loss: 0.00012322
Iteration 16/1000 | Loss: 0.00004526
Iteration 17/1000 | Loss: 0.00026842
Iteration 18/1000 | Loss: 0.00006910
Iteration 19/1000 | Loss: 0.00047617
Iteration 20/1000 | Loss: 0.00005803
Iteration 21/1000 | Loss: 0.00033966
Iteration 22/1000 | Loss: 0.00013880
Iteration 23/1000 | Loss: 0.00028913
Iteration 24/1000 | Loss: 0.00003247
Iteration 25/1000 | Loss: 0.00003120
Iteration 26/1000 | Loss: 0.00002998
Iteration 27/1000 | Loss: 0.00003268
Iteration 28/1000 | Loss: 0.00003058
Iteration 29/1000 | Loss: 0.00002756
Iteration 30/1000 | Loss: 0.00002993
Iteration 31/1000 | Loss: 0.00016674
Iteration 32/1000 | Loss: 0.00009146
Iteration 33/1000 | Loss: 0.00011303
Iteration 34/1000 | Loss: 0.00002541
Iteration 35/1000 | Loss: 0.00015379
Iteration 36/1000 | Loss: 0.00038571
Iteration 37/1000 | Loss: 0.00021726
Iteration 38/1000 | Loss: 0.00003137
Iteration 39/1000 | Loss: 0.00003244
Iteration 40/1000 | Loss: 0.00002413
Iteration 41/1000 | Loss: 0.00011837
Iteration 42/1000 | Loss: 0.00002670
Iteration 43/1000 | Loss: 0.00002366
Iteration 44/1000 | Loss: 0.00008287
Iteration 45/1000 | Loss: 0.00002367
Iteration 46/1000 | Loss: 0.00002459
Iteration 47/1000 | Loss: 0.00002313
Iteration 48/1000 | Loss: 0.00002626
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00002265
Iteration 51/1000 | Loss: 0.00002259
Iteration 52/1000 | Loss: 0.00002255
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002250
Iteration 56/1000 | Loss: 0.00002249
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002246
Iteration 62/1000 | Loss: 0.00002245
Iteration 63/1000 | Loss: 0.00002244
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002367
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00006266
Iteration 68/1000 | Loss: 0.00002251
Iteration 69/1000 | Loss: 0.00002234
Iteration 70/1000 | Loss: 0.00003876
Iteration 71/1000 | Loss: 0.00002231
Iteration 72/1000 | Loss: 0.00002231
Iteration 73/1000 | Loss: 0.00002231
Iteration 74/1000 | Loss: 0.00002231
Iteration 75/1000 | Loss: 0.00002231
Iteration 76/1000 | Loss: 0.00002231
Iteration 77/1000 | Loss: 0.00002231
Iteration 78/1000 | Loss: 0.00002231
Iteration 79/1000 | Loss: 0.00002289
Iteration 80/1000 | Loss: 0.00002242
Iteration 81/1000 | Loss: 0.00002230
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002240
Iteration 85/1000 | Loss: 0.00002229
Iteration 86/1000 | Loss: 0.00002229
Iteration 87/1000 | Loss: 0.00002229
Iteration 88/1000 | Loss: 0.00002230
Iteration 89/1000 | Loss: 0.00002230
Iteration 90/1000 | Loss: 0.00002230
Iteration 91/1000 | Loss: 0.00002228
Iteration 92/1000 | Loss: 0.00002228
Iteration 93/1000 | Loss: 0.00002228
Iteration 94/1000 | Loss: 0.00002228
Iteration 95/1000 | Loss: 0.00002228
Iteration 96/1000 | Loss: 0.00002228
Iteration 97/1000 | Loss: 0.00002228
Iteration 98/1000 | Loss: 0.00002228
Iteration 99/1000 | Loss: 0.00002228
Iteration 100/1000 | Loss: 0.00002228
Iteration 101/1000 | Loss: 0.00002228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.2280986740952358e-05, 2.2280986740952358e-05, 2.2280986740952358e-05, 2.2280986740952358e-05, 2.2280986740952358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2280986740952358e-05

Optimization complete. Final v2v error: 3.8743059635162354 mm

Highest mean error: 6.039419651031494 mm for frame 225

Lowest mean error: 2.92327880859375 mm for frame 37

Saving results

Total time: 103.3390543460846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963143
Iteration 2/25 | Loss: 0.00124117
Iteration 3/25 | Loss: 0.00101033
Iteration 4/25 | Loss: 0.00093992
Iteration 5/25 | Loss: 0.00091966
Iteration 6/25 | Loss: 0.00091641
Iteration 7/25 | Loss: 0.00091555
Iteration 8/25 | Loss: 0.00091555
Iteration 9/25 | Loss: 0.00091555
Iteration 10/25 | Loss: 0.00091555
Iteration 11/25 | Loss: 0.00091555
Iteration 12/25 | Loss: 0.00091555
Iteration 13/25 | Loss: 0.00091555
Iteration 14/25 | Loss: 0.00091555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009155462612397969, 0.0009155462612397969, 0.0009155462612397969, 0.0009155462612397969, 0.0009155462612397969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009155462612397969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15942049
Iteration 2/25 | Loss: 0.00062090
Iteration 3/25 | Loss: 0.00062090
Iteration 4/25 | Loss: 0.00062090
Iteration 5/25 | Loss: 0.00062090
Iteration 6/25 | Loss: 0.00062090
Iteration 7/25 | Loss: 0.00062090
Iteration 8/25 | Loss: 0.00062090
Iteration 9/25 | Loss: 0.00062090
Iteration 10/25 | Loss: 0.00062090
Iteration 11/25 | Loss: 0.00062090
Iteration 12/25 | Loss: 0.00062090
Iteration 13/25 | Loss: 0.00062090
Iteration 14/25 | Loss: 0.00062090
Iteration 15/25 | Loss: 0.00062090
Iteration 16/25 | Loss: 0.00062090
Iteration 17/25 | Loss: 0.00062090
Iteration 18/25 | Loss: 0.00062090
Iteration 19/25 | Loss: 0.00062090
Iteration 20/25 | Loss: 0.00062090
Iteration 21/25 | Loss: 0.00062090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006208958802744746, 0.0006208958802744746, 0.0006208958802744746, 0.0006208958802744746, 0.0006208958802744746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006208958802744746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062090
Iteration 2/1000 | Loss: 0.00003776
Iteration 3/1000 | Loss: 0.00002969
Iteration 4/1000 | Loss: 0.00002813
Iteration 5/1000 | Loss: 0.00002712
Iteration 6/1000 | Loss: 0.00002661
Iteration 7/1000 | Loss: 0.00002612
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002500
Iteration 11/1000 | Loss: 0.00002482
Iteration 12/1000 | Loss: 0.00002464
Iteration 13/1000 | Loss: 0.00002455
Iteration 14/1000 | Loss: 0.00002454
Iteration 15/1000 | Loss: 0.00002449
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002447
Iteration 18/1000 | Loss: 0.00002444
Iteration 19/1000 | Loss: 0.00002444
Iteration 20/1000 | Loss: 0.00002443
Iteration 21/1000 | Loss: 0.00002443
Iteration 22/1000 | Loss: 0.00002440
Iteration 23/1000 | Loss: 0.00002439
Iteration 24/1000 | Loss: 0.00002439
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002438
Iteration 28/1000 | Loss: 0.00002436
Iteration 29/1000 | Loss: 0.00002436
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002433
Iteration 34/1000 | Loss: 0.00002433
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002433
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00002432
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002430
Iteration 47/1000 | Loss: 0.00002430
Iteration 48/1000 | Loss: 0.00002430
Iteration 49/1000 | Loss: 0.00002429
Iteration 50/1000 | Loss: 0.00002429
Iteration 51/1000 | Loss: 0.00002429
Iteration 52/1000 | Loss: 0.00002429
Iteration 53/1000 | Loss: 0.00002429
Iteration 54/1000 | Loss: 0.00002428
Iteration 55/1000 | Loss: 0.00002428
Iteration 56/1000 | Loss: 0.00002427
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002427
Iteration 61/1000 | Loss: 0.00002427
Iteration 62/1000 | Loss: 0.00002427
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002427
Iteration 65/1000 | Loss: 0.00002427
Iteration 66/1000 | Loss: 0.00002426
Iteration 67/1000 | Loss: 0.00002426
Iteration 68/1000 | Loss: 0.00002426
Iteration 69/1000 | Loss: 0.00002426
Iteration 70/1000 | Loss: 0.00002426
Iteration 71/1000 | Loss: 0.00002426
Iteration 72/1000 | Loss: 0.00002426
Iteration 73/1000 | Loss: 0.00002426
Iteration 74/1000 | Loss: 0.00002426
Iteration 75/1000 | Loss: 0.00002425
Iteration 76/1000 | Loss: 0.00002425
Iteration 77/1000 | Loss: 0.00002425
Iteration 78/1000 | Loss: 0.00002425
Iteration 79/1000 | Loss: 0.00002425
Iteration 80/1000 | Loss: 0.00002425
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002425
Iteration 83/1000 | Loss: 0.00002425
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002425
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002424
Iteration 88/1000 | Loss: 0.00002424
Iteration 89/1000 | Loss: 0.00002424
Iteration 90/1000 | Loss: 0.00002424
Iteration 91/1000 | Loss: 0.00002424
Iteration 92/1000 | Loss: 0.00002424
Iteration 93/1000 | Loss: 0.00002424
Iteration 94/1000 | Loss: 0.00002424
Iteration 95/1000 | Loss: 0.00002424
Iteration 96/1000 | Loss: 0.00002424
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002423
Iteration 99/1000 | Loss: 0.00002423
Iteration 100/1000 | Loss: 0.00002423
Iteration 101/1000 | Loss: 0.00002423
Iteration 102/1000 | Loss: 0.00002423
Iteration 103/1000 | Loss: 0.00002423
Iteration 104/1000 | Loss: 0.00002423
Iteration 105/1000 | Loss: 0.00002423
Iteration 106/1000 | Loss: 0.00002423
Iteration 107/1000 | Loss: 0.00002423
Iteration 108/1000 | Loss: 0.00002423
Iteration 109/1000 | Loss: 0.00002423
Iteration 110/1000 | Loss: 0.00002423
Iteration 111/1000 | Loss: 0.00002423
Iteration 112/1000 | Loss: 0.00002423
Iteration 113/1000 | Loss: 0.00002423
Iteration 114/1000 | Loss: 0.00002423
Iteration 115/1000 | Loss: 0.00002423
Iteration 116/1000 | Loss: 0.00002422
Iteration 117/1000 | Loss: 0.00002422
Iteration 118/1000 | Loss: 0.00002422
Iteration 119/1000 | Loss: 0.00002422
Iteration 120/1000 | Loss: 0.00002422
Iteration 121/1000 | Loss: 0.00002422
Iteration 122/1000 | Loss: 0.00002422
Iteration 123/1000 | Loss: 0.00002422
Iteration 124/1000 | Loss: 0.00002422
Iteration 125/1000 | Loss: 0.00002422
Iteration 126/1000 | Loss: 0.00002422
Iteration 127/1000 | Loss: 0.00002422
Iteration 128/1000 | Loss: 0.00002421
Iteration 129/1000 | Loss: 0.00002421
Iteration 130/1000 | Loss: 0.00002421
Iteration 131/1000 | Loss: 0.00002421
Iteration 132/1000 | Loss: 0.00002420
Iteration 133/1000 | Loss: 0.00002420
Iteration 134/1000 | Loss: 0.00002420
Iteration 135/1000 | Loss: 0.00002420
Iteration 136/1000 | Loss: 0.00002419
Iteration 137/1000 | Loss: 0.00002419
Iteration 138/1000 | Loss: 0.00002419
Iteration 139/1000 | Loss: 0.00002419
Iteration 140/1000 | Loss: 0.00002419
Iteration 141/1000 | Loss: 0.00002419
Iteration 142/1000 | Loss: 0.00002419
Iteration 143/1000 | Loss: 0.00002419
Iteration 144/1000 | Loss: 0.00002419
Iteration 145/1000 | Loss: 0.00002418
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002418
Iteration 148/1000 | Loss: 0.00002418
Iteration 149/1000 | Loss: 0.00002418
Iteration 150/1000 | Loss: 0.00002418
Iteration 151/1000 | Loss: 0.00002418
Iteration 152/1000 | Loss: 0.00002418
Iteration 153/1000 | Loss: 0.00002418
Iteration 154/1000 | Loss: 0.00002417
Iteration 155/1000 | Loss: 0.00002417
Iteration 156/1000 | Loss: 0.00002417
Iteration 157/1000 | Loss: 0.00002417
Iteration 158/1000 | Loss: 0.00002417
Iteration 159/1000 | Loss: 0.00002417
Iteration 160/1000 | Loss: 0.00002417
Iteration 161/1000 | Loss: 0.00002417
Iteration 162/1000 | Loss: 0.00002417
Iteration 163/1000 | Loss: 0.00002417
Iteration 164/1000 | Loss: 0.00002417
Iteration 165/1000 | Loss: 0.00002417
Iteration 166/1000 | Loss: 0.00002417
Iteration 167/1000 | Loss: 0.00002417
Iteration 168/1000 | Loss: 0.00002417
Iteration 169/1000 | Loss: 0.00002417
Iteration 170/1000 | Loss: 0.00002417
Iteration 171/1000 | Loss: 0.00002417
Iteration 172/1000 | Loss: 0.00002416
Iteration 173/1000 | Loss: 0.00002416
Iteration 174/1000 | Loss: 0.00002416
Iteration 175/1000 | Loss: 0.00002416
Iteration 176/1000 | Loss: 0.00002416
Iteration 177/1000 | Loss: 0.00002416
Iteration 178/1000 | Loss: 0.00002416
Iteration 179/1000 | Loss: 0.00002416
Iteration 180/1000 | Loss: 0.00002416
Iteration 181/1000 | Loss: 0.00002416
Iteration 182/1000 | Loss: 0.00002416
Iteration 183/1000 | Loss: 0.00002416
Iteration 184/1000 | Loss: 0.00002415
Iteration 185/1000 | Loss: 0.00002415
Iteration 186/1000 | Loss: 0.00002415
Iteration 187/1000 | Loss: 0.00002415
Iteration 188/1000 | Loss: 0.00002415
Iteration 189/1000 | Loss: 0.00002415
Iteration 190/1000 | Loss: 0.00002415
Iteration 191/1000 | Loss: 0.00002415
Iteration 192/1000 | Loss: 0.00002415
Iteration 193/1000 | Loss: 0.00002415
Iteration 194/1000 | Loss: 0.00002415
Iteration 195/1000 | Loss: 0.00002415
Iteration 196/1000 | Loss: 0.00002415
Iteration 197/1000 | Loss: 0.00002415
Iteration 198/1000 | Loss: 0.00002415
Iteration 199/1000 | Loss: 0.00002415
Iteration 200/1000 | Loss: 0.00002415
Iteration 201/1000 | Loss: 0.00002415
Iteration 202/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.414975460851565e-05, 2.414975460851565e-05, 2.414975460851565e-05, 2.414975460851565e-05, 2.414975460851565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.414975460851565e-05

Optimization complete. Final v2v error: 4.127296447753906 mm

Highest mean error: 4.523448467254639 mm for frame 44

Lowest mean error: 3.817551374435425 mm for frame 125

Saving results

Total time: 39.010841369628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483592
Iteration 2/25 | Loss: 0.00125169
Iteration 3/25 | Loss: 0.00091269
Iteration 4/25 | Loss: 0.00086378
Iteration 5/25 | Loss: 0.00085373
Iteration 6/25 | Loss: 0.00084982
Iteration 7/25 | Loss: 0.00084836
Iteration 8/25 | Loss: 0.00084807
Iteration 9/25 | Loss: 0.00084807
Iteration 10/25 | Loss: 0.00084807
Iteration 11/25 | Loss: 0.00084807
Iteration 12/25 | Loss: 0.00084807
Iteration 13/25 | Loss: 0.00084807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008480667602270842, 0.0008480667602270842, 0.0008480667602270842, 0.0008480667602270842, 0.0008480667602270842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008480667602270842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52947557
Iteration 2/25 | Loss: 0.00053737
Iteration 3/25 | Loss: 0.00053737
Iteration 4/25 | Loss: 0.00053737
Iteration 5/25 | Loss: 0.00053737
Iteration 6/25 | Loss: 0.00053737
Iteration 7/25 | Loss: 0.00053737
Iteration 8/25 | Loss: 0.00053737
Iteration 9/25 | Loss: 0.00053737
Iteration 10/25 | Loss: 0.00053737
Iteration 11/25 | Loss: 0.00053737
Iteration 12/25 | Loss: 0.00053737
Iteration 13/25 | Loss: 0.00053737
Iteration 14/25 | Loss: 0.00053737
Iteration 15/25 | Loss: 0.00053737
Iteration 16/25 | Loss: 0.00053737
Iteration 17/25 | Loss: 0.00053737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005373710882849991, 0.0005373710882849991, 0.0005373710882849991, 0.0005373710882849991, 0.0005373710882849991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005373710882849991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053737
Iteration 2/1000 | Loss: 0.00003128
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00001935
Iteration 5/1000 | Loss: 0.00001837
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001613
Iteration 11/1000 | Loss: 0.00001595
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001548
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001547
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001546
Iteration 35/1000 | Loss: 0.00001546
Iteration 36/1000 | Loss: 0.00001546
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001544
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001544
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001541
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001540
Iteration 75/1000 | Loss: 0.00001540
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001536
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001533
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001530
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001529
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001527
Iteration 131/1000 | Loss: 0.00001527
Iteration 132/1000 | Loss: 0.00001527
Iteration 133/1000 | Loss: 0.00001527
Iteration 134/1000 | Loss: 0.00001527
Iteration 135/1000 | Loss: 0.00001527
Iteration 136/1000 | Loss: 0.00001527
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001527
Iteration 148/1000 | Loss: 0.00001527
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001527
Iteration 151/1000 | Loss: 0.00001527
Iteration 152/1000 | Loss: 0.00001527
Iteration 153/1000 | Loss: 0.00001527
Iteration 154/1000 | Loss: 0.00001527
Iteration 155/1000 | Loss: 0.00001527
Iteration 156/1000 | Loss: 0.00001527
Iteration 157/1000 | Loss: 0.00001527
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001527
Iteration 161/1000 | Loss: 0.00001527
Iteration 162/1000 | Loss: 0.00001527
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001527
Iteration 165/1000 | Loss: 0.00001527
Iteration 166/1000 | Loss: 0.00001527
Iteration 167/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.526549931440968e-05, 1.526549931440968e-05, 1.526549931440968e-05, 1.526549931440968e-05, 1.526549931440968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.526549931440968e-05

Optimization complete. Final v2v error: 3.1052253246307373 mm

Highest mean error: 4.970495700836182 mm for frame 72

Lowest mean error: 2.5978307723999023 mm for frame 160

Saving results

Total time: 40.15431332588196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419144
Iteration 2/25 | Loss: 0.00093630
Iteration 3/25 | Loss: 0.00084847
Iteration 4/25 | Loss: 0.00082914
Iteration 5/25 | Loss: 0.00082226
Iteration 6/25 | Loss: 0.00082081
Iteration 7/25 | Loss: 0.00082038
Iteration 8/25 | Loss: 0.00082038
Iteration 9/25 | Loss: 0.00082038
Iteration 10/25 | Loss: 0.00082038
Iteration 11/25 | Loss: 0.00082038
Iteration 12/25 | Loss: 0.00082038
Iteration 13/25 | Loss: 0.00082038
Iteration 14/25 | Loss: 0.00082038
Iteration 15/25 | Loss: 0.00082038
Iteration 16/25 | Loss: 0.00082038
Iteration 17/25 | Loss: 0.00082038
Iteration 18/25 | Loss: 0.00082038
Iteration 19/25 | Loss: 0.00082038
Iteration 20/25 | Loss: 0.00082038
Iteration 21/25 | Loss: 0.00082038
Iteration 22/25 | Loss: 0.00082038
Iteration 23/25 | Loss: 0.00082038
Iteration 24/25 | Loss: 0.00082038
Iteration 25/25 | Loss: 0.00082038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85352504
Iteration 2/25 | Loss: 0.00051617
Iteration 3/25 | Loss: 0.00051617
Iteration 4/25 | Loss: 0.00051617
Iteration 5/25 | Loss: 0.00051617
Iteration 6/25 | Loss: 0.00051617
Iteration 7/25 | Loss: 0.00051617
Iteration 8/25 | Loss: 0.00051617
Iteration 9/25 | Loss: 0.00051617
Iteration 10/25 | Loss: 0.00051617
Iteration 11/25 | Loss: 0.00051617
Iteration 12/25 | Loss: 0.00051617
Iteration 13/25 | Loss: 0.00051617
Iteration 14/25 | Loss: 0.00051617
Iteration 15/25 | Loss: 0.00051617
Iteration 16/25 | Loss: 0.00051617
Iteration 17/25 | Loss: 0.00051617
Iteration 18/25 | Loss: 0.00051617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005161650478839874, 0.0005161650478839874, 0.0005161650478839874, 0.0005161650478839874, 0.0005161650478839874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005161650478839874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051617
Iteration 2/1000 | Loss: 0.00003069
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001661
Iteration 6/1000 | Loss: 0.00001574
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001443
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001433
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001414
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001404
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001383
Iteration 51/1000 | Loss: 0.00001383
Iteration 52/1000 | Loss: 0.00001383
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001381
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001377
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001371
Iteration 94/1000 | Loss: 0.00001371
Iteration 95/1000 | Loss: 0.00001371
Iteration 96/1000 | Loss: 0.00001371
Iteration 97/1000 | Loss: 0.00001370
Iteration 98/1000 | Loss: 0.00001370
Iteration 99/1000 | Loss: 0.00001370
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001366
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001365
Iteration 135/1000 | Loss: 0.00001365
Iteration 136/1000 | Loss: 0.00001365
Iteration 137/1000 | Loss: 0.00001365
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001364
Iteration 143/1000 | Loss: 0.00001364
Iteration 144/1000 | Loss: 0.00001364
Iteration 145/1000 | Loss: 0.00001364
Iteration 146/1000 | Loss: 0.00001364
Iteration 147/1000 | Loss: 0.00001364
Iteration 148/1000 | Loss: 0.00001364
Iteration 149/1000 | Loss: 0.00001364
Iteration 150/1000 | Loss: 0.00001364
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001363
Iteration 157/1000 | Loss: 0.00001363
Iteration 158/1000 | Loss: 0.00001363
Iteration 159/1000 | Loss: 0.00001363
Iteration 160/1000 | Loss: 0.00001363
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001362
Iteration 172/1000 | Loss: 0.00001362
Iteration 173/1000 | Loss: 0.00001362
Iteration 174/1000 | Loss: 0.00001362
Iteration 175/1000 | Loss: 0.00001362
Iteration 176/1000 | Loss: 0.00001362
Iteration 177/1000 | Loss: 0.00001362
Iteration 178/1000 | Loss: 0.00001362
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3621889593196101e-05, 1.3621889593196101e-05, 1.3621889593196101e-05, 1.3621889593196101e-05, 1.3621889593196101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3621889593196101e-05

Optimization complete. Final v2v error: 3.142625093460083 mm

Highest mean error: 3.892611026763916 mm for frame 76

Lowest mean error: 2.8653275966644287 mm for frame 96

Saving results

Total time: 39.519790172576904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996067
Iteration 2/25 | Loss: 0.00297339
Iteration 3/25 | Loss: 0.00208380
Iteration 4/25 | Loss: 0.00177498
Iteration 5/25 | Loss: 0.00165845
Iteration 6/25 | Loss: 0.00154190
Iteration 7/25 | Loss: 0.00146034
Iteration 8/25 | Loss: 0.00153401
Iteration 9/25 | Loss: 0.00145592
Iteration 10/25 | Loss: 0.00128598
Iteration 11/25 | Loss: 0.00117186
Iteration 12/25 | Loss: 0.00114459
Iteration 13/25 | Loss: 0.00109895
Iteration 14/25 | Loss: 0.00104978
Iteration 15/25 | Loss: 0.00103884
Iteration 16/25 | Loss: 0.00103833
Iteration 17/25 | Loss: 0.00103488
Iteration 18/25 | Loss: 0.00103274
Iteration 19/25 | Loss: 0.00102818
Iteration 20/25 | Loss: 0.00102504
Iteration 21/25 | Loss: 0.00101920
Iteration 22/25 | Loss: 0.00101077
Iteration 23/25 | Loss: 0.00099958
Iteration 24/25 | Loss: 0.00098748
Iteration 25/25 | Loss: 0.00099766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49405360
Iteration 2/25 | Loss: 0.00218718
Iteration 3/25 | Loss: 0.00214469
Iteration 4/25 | Loss: 0.00214469
Iteration 5/25 | Loss: 0.00214469
Iteration 6/25 | Loss: 0.00214469
Iteration 7/25 | Loss: 0.00214469
Iteration 8/25 | Loss: 0.00214468
Iteration 9/25 | Loss: 0.00214468
Iteration 10/25 | Loss: 0.00214468
Iteration 11/25 | Loss: 0.00214468
Iteration 12/25 | Loss: 0.00214468
Iteration 13/25 | Loss: 0.00214468
Iteration 14/25 | Loss: 0.00214468
Iteration 15/25 | Loss: 0.00214468
Iteration 16/25 | Loss: 0.00214468
Iteration 17/25 | Loss: 0.00214468
Iteration 18/25 | Loss: 0.00214468
Iteration 19/25 | Loss: 0.00214468
Iteration 20/25 | Loss: 0.00214468
Iteration 21/25 | Loss: 0.00214468
Iteration 22/25 | Loss: 0.00214468
Iteration 23/25 | Loss: 0.00214468
Iteration 24/25 | Loss: 0.00214468
Iteration 25/25 | Loss: 0.00214468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214468
Iteration 2/1000 | Loss: 0.00037053
Iteration 3/1000 | Loss: 0.00013691
Iteration 4/1000 | Loss: 0.00051438
Iteration 5/1000 | Loss: 0.00010006
Iteration 6/1000 | Loss: 0.00070010
Iteration 7/1000 | Loss: 0.00009810
Iteration 8/1000 | Loss: 0.00020597
Iteration 9/1000 | Loss: 0.00059996
Iteration 10/1000 | Loss: 0.00045194
Iteration 11/1000 | Loss: 0.00008963
Iteration 12/1000 | Loss: 0.00007499
Iteration 13/1000 | Loss: 0.00010042
Iteration 14/1000 | Loss: 0.00040750
Iteration 15/1000 | Loss: 0.00024827
Iteration 16/1000 | Loss: 0.00092654
Iteration 17/1000 | Loss: 0.00038426
Iteration 18/1000 | Loss: 0.00029722
Iteration 19/1000 | Loss: 0.00046016
Iteration 20/1000 | Loss: 0.00015244
Iteration 21/1000 | Loss: 0.00009587
Iteration 22/1000 | Loss: 0.00013663
Iteration 23/1000 | Loss: 0.00012559
Iteration 24/1000 | Loss: 0.00005468
Iteration 25/1000 | Loss: 0.00005525
Iteration 26/1000 | Loss: 0.00005871
Iteration 27/1000 | Loss: 0.00004990
Iteration 28/1000 | Loss: 0.00007004
Iteration 29/1000 | Loss: 0.00007072
Iteration 30/1000 | Loss: 0.00006867
Iteration 31/1000 | Loss: 0.00005956
Iteration 32/1000 | Loss: 0.00005023
Iteration 33/1000 | Loss: 0.00006159
Iteration 34/1000 | Loss: 0.00037317
Iteration 35/1000 | Loss: 0.00021842
Iteration 36/1000 | Loss: 0.00005717
Iteration 37/1000 | Loss: 0.00004311
Iteration 38/1000 | Loss: 0.00017519
Iteration 39/1000 | Loss: 0.00060956
Iteration 40/1000 | Loss: 0.00026015
Iteration 41/1000 | Loss: 0.00024268
Iteration 42/1000 | Loss: 0.00027654
Iteration 43/1000 | Loss: 0.00006250
Iteration 44/1000 | Loss: 0.00005707
Iteration 45/1000 | Loss: 0.00005153
Iteration 46/1000 | Loss: 0.00003751
Iteration 47/1000 | Loss: 0.00020639
Iteration 48/1000 | Loss: 0.00024273
Iteration 49/1000 | Loss: 0.00023285
Iteration 50/1000 | Loss: 0.00005696
Iteration 51/1000 | Loss: 0.00004823
Iteration 52/1000 | Loss: 0.00005448
Iteration 53/1000 | Loss: 0.00003418
Iteration 54/1000 | Loss: 0.00004504
Iteration 55/1000 | Loss: 0.00004602
Iteration 56/1000 | Loss: 0.00004325
Iteration 57/1000 | Loss: 0.00003984
Iteration 58/1000 | Loss: 0.00005001
Iteration 59/1000 | Loss: 0.00004819
Iteration 60/1000 | Loss: 0.00003434
Iteration 61/1000 | Loss: 0.00006362
Iteration 62/1000 | Loss: 0.00003598
Iteration 63/1000 | Loss: 0.00003078
Iteration 64/1000 | Loss: 0.00002939
Iteration 65/1000 | Loss: 0.00017290
Iteration 66/1000 | Loss: 0.00018402
Iteration 67/1000 | Loss: 0.00010465
Iteration 68/1000 | Loss: 0.00012777
Iteration 69/1000 | Loss: 0.00024558
Iteration 70/1000 | Loss: 0.00067485
Iteration 71/1000 | Loss: 0.00006426
Iteration 72/1000 | Loss: 0.00020944
Iteration 73/1000 | Loss: 0.00003348
Iteration 74/1000 | Loss: 0.00003094
Iteration 75/1000 | Loss: 0.00019397
Iteration 76/1000 | Loss: 0.00003434
Iteration 77/1000 | Loss: 0.00003659
Iteration 78/1000 | Loss: 0.00004713
Iteration 79/1000 | Loss: 0.00004889
Iteration 80/1000 | Loss: 0.00002943
Iteration 81/1000 | Loss: 0.00002787
Iteration 82/1000 | Loss: 0.00002623
Iteration 83/1000 | Loss: 0.00004217
Iteration 84/1000 | Loss: 0.00002514
Iteration 85/1000 | Loss: 0.00004051
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002411
Iteration 89/1000 | Loss: 0.00002408
Iteration 90/1000 | Loss: 0.00002403
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002368
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002360
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002357
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002355
Iteration 102/1000 | Loss: 0.00002355
Iteration 103/1000 | Loss: 0.00002355
Iteration 104/1000 | Loss: 0.00002354
Iteration 105/1000 | Loss: 0.00002354
Iteration 106/1000 | Loss: 0.00002354
Iteration 107/1000 | Loss: 0.00002354
Iteration 108/1000 | Loss: 0.00002353
Iteration 109/1000 | Loss: 0.00002353
Iteration 110/1000 | Loss: 0.00002353
Iteration 111/1000 | Loss: 0.00002353
Iteration 112/1000 | Loss: 0.00002349
Iteration 113/1000 | Loss: 0.00002345
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002343
Iteration 116/1000 | Loss: 0.00002340
Iteration 117/1000 | Loss: 0.00002340
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00026924
Iteration 120/1000 | Loss: 0.00009040
Iteration 121/1000 | Loss: 0.00039383
Iteration 122/1000 | Loss: 0.00002543
Iteration 123/1000 | Loss: 0.00002380
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002297
Iteration 126/1000 | Loss: 0.00002284
Iteration 127/1000 | Loss: 0.00002278
Iteration 128/1000 | Loss: 0.00002277
Iteration 129/1000 | Loss: 0.00002277
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002273
Iteration 132/1000 | Loss: 0.00002272
Iteration 133/1000 | Loss: 0.00002272
Iteration 134/1000 | Loss: 0.00002271
Iteration 135/1000 | Loss: 0.00002271
Iteration 136/1000 | Loss: 0.00002270
Iteration 137/1000 | Loss: 0.00002270
Iteration 138/1000 | Loss: 0.00002270
Iteration 139/1000 | Loss: 0.00002270
Iteration 140/1000 | Loss: 0.00002269
Iteration 141/1000 | Loss: 0.00002269
Iteration 142/1000 | Loss: 0.00002269
Iteration 143/1000 | Loss: 0.00002268
Iteration 144/1000 | Loss: 0.00002268
Iteration 145/1000 | Loss: 0.00002268
Iteration 146/1000 | Loss: 0.00002267
Iteration 147/1000 | Loss: 0.00002267
Iteration 148/1000 | Loss: 0.00002267
Iteration 149/1000 | Loss: 0.00002264
Iteration 150/1000 | Loss: 0.00002264
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002262
Iteration 154/1000 | Loss: 0.00002261
Iteration 155/1000 | Loss: 0.00002261
Iteration 156/1000 | Loss: 0.00002261
Iteration 157/1000 | Loss: 0.00002261
Iteration 158/1000 | Loss: 0.00002261
Iteration 159/1000 | Loss: 0.00002260
Iteration 160/1000 | Loss: 0.00002260
Iteration 161/1000 | Loss: 0.00002260
Iteration 162/1000 | Loss: 0.00002260
Iteration 163/1000 | Loss: 0.00002259
Iteration 164/1000 | Loss: 0.00002259
Iteration 165/1000 | Loss: 0.00002259
Iteration 166/1000 | Loss: 0.00002259
Iteration 167/1000 | Loss: 0.00002259
Iteration 168/1000 | Loss: 0.00002259
Iteration 169/1000 | Loss: 0.00002259
Iteration 170/1000 | Loss: 0.00002259
Iteration 171/1000 | Loss: 0.00002259
Iteration 172/1000 | Loss: 0.00002258
Iteration 173/1000 | Loss: 0.00002258
Iteration 174/1000 | Loss: 0.00002258
Iteration 175/1000 | Loss: 0.00002258
Iteration 176/1000 | Loss: 0.00002258
Iteration 177/1000 | Loss: 0.00002258
Iteration 178/1000 | Loss: 0.00002258
Iteration 179/1000 | Loss: 0.00002258
Iteration 180/1000 | Loss: 0.00002258
Iteration 181/1000 | Loss: 0.00002257
Iteration 182/1000 | Loss: 0.00002257
Iteration 183/1000 | Loss: 0.00002257
Iteration 184/1000 | Loss: 0.00002257
Iteration 185/1000 | Loss: 0.00002256
Iteration 186/1000 | Loss: 0.00002256
Iteration 187/1000 | Loss: 0.00002256
Iteration 188/1000 | Loss: 0.00002256
Iteration 189/1000 | Loss: 0.00002256
Iteration 190/1000 | Loss: 0.00002256
Iteration 191/1000 | Loss: 0.00002256
Iteration 192/1000 | Loss: 0.00002256
Iteration 193/1000 | Loss: 0.00002255
Iteration 194/1000 | Loss: 0.00002255
Iteration 195/1000 | Loss: 0.00002255
Iteration 196/1000 | Loss: 0.00002255
Iteration 197/1000 | Loss: 0.00002255
Iteration 198/1000 | Loss: 0.00002255
Iteration 199/1000 | Loss: 0.00002255
Iteration 200/1000 | Loss: 0.00002255
Iteration 201/1000 | Loss: 0.00002255
Iteration 202/1000 | Loss: 0.00002254
Iteration 203/1000 | Loss: 0.00002254
Iteration 204/1000 | Loss: 0.00002254
Iteration 205/1000 | Loss: 0.00002254
Iteration 206/1000 | Loss: 0.00002254
Iteration 207/1000 | Loss: 0.00002254
Iteration 208/1000 | Loss: 0.00002254
Iteration 209/1000 | Loss: 0.00002253
Iteration 210/1000 | Loss: 0.00002253
Iteration 211/1000 | Loss: 0.00002253
Iteration 212/1000 | Loss: 0.00002252
Iteration 213/1000 | Loss: 0.00002252
Iteration 214/1000 | Loss: 0.00002252
Iteration 215/1000 | Loss: 0.00002252
Iteration 216/1000 | Loss: 0.00002252
Iteration 217/1000 | Loss: 0.00002252
Iteration 218/1000 | Loss: 0.00002252
Iteration 219/1000 | Loss: 0.00002252
Iteration 220/1000 | Loss: 0.00002252
Iteration 221/1000 | Loss: 0.00002252
Iteration 222/1000 | Loss: 0.00002252
Iteration 223/1000 | Loss: 0.00002252
Iteration 224/1000 | Loss: 0.00002252
Iteration 225/1000 | Loss: 0.00002252
Iteration 226/1000 | Loss: 0.00002252
Iteration 227/1000 | Loss: 0.00002252
Iteration 228/1000 | Loss: 0.00002251
Iteration 229/1000 | Loss: 0.00002251
Iteration 230/1000 | Loss: 0.00002251
Iteration 231/1000 | Loss: 0.00002251
Iteration 232/1000 | Loss: 0.00002251
Iteration 233/1000 | Loss: 0.00002250
Iteration 234/1000 | Loss: 0.00002250
Iteration 235/1000 | Loss: 0.00002250
Iteration 236/1000 | Loss: 0.00002250
Iteration 237/1000 | Loss: 0.00002250
Iteration 238/1000 | Loss: 0.00002250
Iteration 239/1000 | Loss: 0.00002250
Iteration 240/1000 | Loss: 0.00002250
Iteration 241/1000 | Loss: 0.00002250
Iteration 242/1000 | Loss: 0.00002250
Iteration 243/1000 | Loss: 0.00002250
Iteration 244/1000 | Loss: 0.00002250
Iteration 245/1000 | Loss: 0.00002250
Iteration 246/1000 | Loss: 0.00002250
Iteration 247/1000 | Loss: 0.00002250
Iteration 248/1000 | Loss: 0.00002250
Iteration 249/1000 | Loss: 0.00002250
Iteration 250/1000 | Loss: 0.00002250
Iteration 251/1000 | Loss: 0.00002250
Iteration 252/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.2497006284538656e-05, 2.2497006284538656e-05, 2.2497006284538656e-05, 2.2497006284538656e-05, 2.2497006284538656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2497006284538656e-05

Optimization complete. Final v2v error: 3.6834263801574707 mm

Highest mean error: 11.642497062683105 mm for frame 125

Lowest mean error: 3.215414047241211 mm for frame 95

Saving results

Total time: 223.37943863868713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832165
Iteration 2/25 | Loss: 0.00124139
Iteration 3/25 | Loss: 0.00088555
Iteration 4/25 | Loss: 0.00083989
Iteration 5/25 | Loss: 0.00083447
Iteration 6/25 | Loss: 0.00083271
Iteration 7/25 | Loss: 0.00083191
Iteration 8/25 | Loss: 0.00083171
Iteration 9/25 | Loss: 0.00083171
Iteration 10/25 | Loss: 0.00083171
Iteration 11/25 | Loss: 0.00083171
Iteration 12/25 | Loss: 0.00083171
Iteration 13/25 | Loss: 0.00083171
Iteration 14/25 | Loss: 0.00083171
Iteration 15/25 | Loss: 0.00083171
Iteration 16/25 | Loss: 0.00083171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000831708253826946, 0.000831708253826946, 0.000831708253826946, 0.000831708253826946, 0.000831708253826946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000831708253826946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51083899
Iteration 2/25 | Loss: 0.00053952
Iteration 3/25 | Loss: 0.00053952
Iteration 4/25 | Loss: 0.00053952
Iteration 5/25 | Loss: 0.00053952
Iteration 6/25 | Loss: 0.00053951
Iteration 7/25 | Loss: 0.00053951
Iteration 8/25 | Loss: 0.00053951
Iteration 9/25 | Loss: 0.00053951
Iteration 10/25 | Loss: 0.00053951
Iteration 11/25 | Loss: 0.00053951
Iteration 12/25 | Loss: 0.00053951
Iteration 13/25 | Loss: 0.00053951
Iteration 14/25 | Loss: 0.00053951
Iteration 15/25 | Loss: 0.00053951
Iteration 16/25 | Loss: 0.00053951
Iteration 17/25 | Loss: 0.00053951
Iteration 18/25 | Loss: 0.00053951
Iteration 19/25 | Loss: 0.00053951
Iteration 20/25 | Loss: 0.00053951
Iteration 21/25 | Loss: 0.00053951
Iteration 22/25 | Loss: 0.00053951
Iteration 23/25 | Loss: 0.00053951
Iteration 24/25 | Loss: 0.00053951
Iteration 25/25 | Loss: 0.00053951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053951
Iteration 2/1000 | Loss: 0.00002314
Iteration 3/1000 | Loss: 0.00001550
Iteration 4/1000 | Loss: 0.00001407
Iteration 5/1000 | Loss: 0.00001329
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001163
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001129
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001127
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001122
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001122
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001121
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001113
Iteration 143/1000 | Loss: 0.00001113
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001112
Iteration 149/1000 | Loss: 0.00001111
Iteration 150/1000 | Loss: 0.00001111
Iteration 151/1000 | Loss: 0.00001111
Iteration 152/1000 | Loss: 0.00001111
Iteration 153/1000 | Loss: 0.00001111
Iteration 154/1000 | Loss: 0.00001111
Iteration 155/1000 | Loss: 0.00001111
Iteration 156/1000 | Loss: 0.00001111
Iteration 157/1000 | Loss: 0.00001111
Iteration 158/1000 | Loss: 0.00001111
Iteration 159/1000 | Loss: 0.00001111
Iteration 160/1000 | Loss: 0.00001111
Iteration 161/1000 | Loss: 0.00001111
Iteration 162/1000 | Loss: 0.00001111
Iteration 163/1000 | Loss: 0.00001110
Iteration 164/1000 | Loss: 0.00001110
Iteration 165/1000 | Loss: 0.00001110
Iteration 166/1000 | Loss: 0.00001110
Iteration 167/1000 | Loss: 0.00001110
Iteration 168/1000 | Loss: 0.00001110
Iteration 169/1000 | Loss: 0.00001110
Iteration 170/1000 | Loss: 0.00001110
Iteration 171/1000 | Loss: 0.00001110
Iteration 172/1000 | Loss: 0.00001110
Iteration 173/1000 | Loss: 0.00001110
Iteration 174/1000 | Loss: 0.00001110
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001109
Iteration 177/1000 | Loss: 0.00001109
Iteration 178/1000 | Loss: 0.00001109
Iteration 179/1000 | Loss: 0.00001109
Iteration 180/1000 | Loss: 0.00001109
Iteration 181/1000 | Loss: 0.00001109
Iteration 182/1000 | Loss: 0.00001109
Iteration 183/1000 | Loss: 0.00001109
Iteration 184/1000 | Loss: 0.00001109
Iteration 185/1000 | Loss: 0.00001109
Iteration 186/1000 | Loss: 0.00001109
Iteration 187/1000 | Loss: 0.00001109
Iteration 188/1000 | Loss: 0.00001109
Iteration 189/1000 | Loss: 0.00001109
Iteration 190/1000 | Loss: 0.00001109
Iteration 191/1000 | Loss: 0.00001109
Iteration 192/1000 | Loss: 0.00001109
Iteration 193/1000 | Loss: 0.00001109
Iteration 194/1000 | Loss: 0.00001109
Iteration 195/1000 | Loss: 0.00001108
Iteration 196/1000 | Loss: 0.00001108
Iteration 197/1000 | Loss: 0.00001108
Iteration 198/1000 | Loss: 0.00001108
Iteration 199/1000 | Loss: 0.00001108
Iteration 200/1000 | Loss: 0.00001108
Iteration 201/1000 | Loss: 0.00001108
Iteration 202/1000 | Loss: 0.00001108
Iteration 203/1000 | Loss: 0.00001108
Iteration 204/1000 | Loss: 0.00001108
Iteration 205/1000 | Loss: 0.00001108
Iteration 206/1000 | Loss: 0.00001108
Iteration 207/1000 | Loss: 0.00001108
Iteration 208/1000 | Loss: 0.00001108
Iteration 209/1000 | Loss: 0.00001108
Iteration 210/1000 | Loss: 0.00001108
Iteration 211/1000 | Loss: 0.00001108
Iteration 212/1000 | Loss: 0.00001108
Iteration 213/1000 | Loss: 0.00001108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.1078772331529763e-05, 1.1078772331529763e-05, 1.1078772331529763e-05, 1.1078772331529763e-05, 1.1078772331529763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1078772331529763e-05

Optimization complete. Final v2v error: 2.823930501937866 mm

Highest mean error: 3.1237568855285645 mm for frame 88

Lowest mean error: 2.685114622116089 mm for frame 40

Saving results

Total time: 42.014315605163574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400600
Iteration 2/25 | Loss: 0.00102572
Iteration 3/25 | Loss: 0.00085295
Iteration 4/25 | Loss: 0.00083140
Iteration 5/25 | Loss: 0.00082466
Iteration 6/25 | Loss: 0.00082253
Iteration 7/25 | Loss: 0.00082201
Iteration 8/25 | Loss: 0.00082201
Iteration 9/25 | Loss: 0.00082201
Iteration 10/25 | Loss: 0.00082201
Iteration 11/25 | Loss: 0.00082201
Iteration 12/25 | Loss: 0.00082201
Iteration 13/25 | Loss: 0.00082201
Iteration 14/25 | Loss: 0.00082201
Iteration 15/25 | Loss: 0.00082201
Iteration 16/25 | Loss: 0.00082201
Iteration 17/25 | Loss: 0.00082201
Iteration 18/25 | Loss: 0.00082201
Iteration 19/25 | Loss: 0.00082201
Iteration 20/25 | Loss: 0.00082201
Iteration 21/25 | Loss: 0.00082201
Iteration 22/25 | Loss: 0.00082201
Iteration 23/25 | Loss: 0.00082201
Iteration 24/25 | Loss: 0.00082201
Iteration 25/25 | Loss: 0.00082201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38108838
Iteration 2/25 | Loss: 0.00048446
Iteration 3/25 | Loss: 0.00048444
Iteration 4/25 | Loss: 0.00048444
Iteration 5/25 | Loss: 0.00048444
Iteration 6/25 | Loss: 0.00048444
Iteration 7/25 | Loss: 0.00048444
Iteration 8/25 | Loss: 0.00048444
Iteration 9/25 | Loss: 0.00048444
Iteration 10/25 | Loss: 0.00048444
Iteration 11/25 | Loss: 0.00048444
Iteration 12/25 | Loss: 0.00048444
Iteration 13/25 | Loss: 0.00048444
Iteration 14/25 | Loss: 0.00048444
Iteration 15/25 | Loss: 0.00048444
Iteration 16/25 | Loss: 0.00048444
Iteration 17/25 | Loss: 0.00048444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004844358772970736, 0.0004844358772970736, 0.0004844358772970736, 0.0004844358772970736, 0.0004844358772970736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004844358772970736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048444
Iteration 2/1000 | Loss: 0.00003559
Iteration 3/1000 | Loss: 0.00002173
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001612
Iteration 8/1000 | Loss: 0.00001579
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001530
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001521
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001481
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001481
Iteration 100/1000 | Loss: 0.00001481
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001480
Iteration 106/1000 | Loss: 0.00001480
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001476
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001475
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001473
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Iteration 163/1000 | Loss: 0.00001473
Iteration 164/1000 | Loss: 0.00001473
Iteration 165/1000 | Loss: 0.00001473
Iteration 166/1000 | Loss: 0.00001473
Iteration 167/1000 | Loss: 0.00001473
Iteration 168/1000 | Loss: 0.00001473
Iteration 169/1000 | Loss: 0.00001473
Iteration 170/1000 | Loss: 0.00001473
Iteration 171/1000 | Loss: 0.00001473
Iteration 172/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4725862456543837e-05, 1.4725862456543837e-05, 1.4725862456543837e-05, 1.4725862456543837e-05, 1.4725862456543837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4725862456543837e-05

Optimization complete. Final v2v error: 3.086625576019287 mm

Highest mean error: 5.222746849060059 mm for frame 87

Lowest mean error: 2.4833662509918213 mm for frame 127

Saving results

Total time: 38.71338415145874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810629
Iteration 2/25 | Loss: 0.00161343
Iteration 3/25 | Loss: 0.00116719
Iteration 4/25 | Loss: 0.00111087
Iteration 5/25 | Loss: 0.00109883
Iteration 6/25 | Loss: 0.00109602
Iteration 7/25 | Loss: 0.00109548
Iteration 8/25 | Loss: 0.00109548
Iteration 9/25 | Loss: 0.00109548
Iteration 10/25 | Loss: 0.00109548
Iteration 11/25 | Loss: 0.00109548
Iteration 12/25 | Loss: 0.00109548
Iteration 13/25 | Loss: 0.00109548
Iteration 14/25 | Loss: 0.00109548
Iteration 15/25 | Loss: 0.00109548
Iteration 16/25 | Loss: 0.00109548
Iteration 17/25 | Loss: 0.00109548
Iteration 18/25 | Loss: 0.00109548
Iteration 19/25 | Loss: 0.00109548
Iteration 20/25 | Loss: 0.00109548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001095483428798616, 0.001095483428798616, 0.001095483428798616, 0.001095483428798616, 0.001095483428798616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095483428798616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.27098799
Iteration 2/25 | Loss: 0.00072503
Iteration 3/25 | Loss: 0.00072502
Iteration 4/25 | Loss: 0.00072502
Iteration 5/25 | Loss: 0.00072502
Iteration 6/25 | Loss: 0.00072502
Iteration 7/25 | Loss: 0.00072502
Iteration 8/25 | Loss: 0.00072502
Iteration 9/25 | Loss: 0.00072502
Iteration 10/25 | Loss: 0.00072502
Iteration 11/25 | Loss: 0.00072502
Iteration 12/25 | Loss: 0.00072502
Iteration 13/25 | Loss: 0.00072502
Iteration 14/25 | Loss: 0.00072502
Iteration 15/25 | Loss: 0.00072502
Iteration 16/25 | Loss: 0.00072502
Iteration 17/25 | Loss: 0.00072502
Iteration 18/25 | Loss: 0.00072502
Iteration 19/25 | Loss: 0.00072502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007250193157233298, 0.0007250193157233298, 0.0007250193157233298, 0.0007250193157233298, 0.0007250193157233298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007250193157233298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072502
Iteration 2/1000 | Loss: 0.00007133
Iteration 3/1000 | Loss: 0.00004583
Iteration 4/1000 | Loss: 0.00004008
Iteration 5/1000 | Loss: 0.00003707
Iteration 6/1000 | Loss: 0.00003513
Iteration 7/1000 | Loss: 0.00003384
Iteration 8/1000 | Loss: 0.00003316
Iteration 9/1000 | Loss: 0.00003268
Iteration 10/1000 | Loss: 0.00003238
Iteration 11/1000 | Loss: 0.00003212
Iteration 12/1000 | Loss: 0.00003182
Iteration 13/1000 | Loss: 0.00003159
Iteration 14/1000 | Loss: 0.00003143
Iteration 15/1000 | Loss: 0.00003136
Iteration 16/1000 | Loss: 0.00003132
Iteration 17/1000 | Loss: 0.00003132
Iteration 18/1000 | Loss: 0.00003132
Iteration 19/1000 | Loss: 0.00003131
Iteration 20/1000 | Loss: 0.00003131
Iteration 21/1000 | Loss: 0.00003131
Iteration 22/1000 | Loss: 0.00003130
Iteration 23/1000 | Loss: 0.00003130
Iteration 24/1000 | Loss: 0.00003129
Iteration 25/1000 | Loss: 0.00003128
Iteration 26/1000 | Loss: 0.00003128
Iteration 27/1000 | Loss: 0.00003127
Iteration 28/1000 | Loss: 0.00003127
Iteration 29/1000 | Loss: 0.00003127
Iteration 30/1000 | Loss: 0.00003126
Iteration 31/1000 | Loss: 0.00003126
Iteration 32/1000 | Loss: 0.00003126
Iteration 33/1000 | Loss: 0.00003126
Iteration 34/1000 | Loss: 0.00003126
Iteration 35/1000 | Loss: 0.00003126
Iteration 36/1000 | Loss: 0.00003126
Iteration 37/1000 | Loss: 0.00003125
Iteration 38/1000 | Loss: 0.00003125
Iteration 39/1000 | Loss: 0.00003125
Iteration 40/1000 | Loss: 0.00003124
Iteration 41/1000 | Loss: 0.00003124
Iteration 42/1000 | Loss: 0.00003123
Iteration 43/1000 | Loss: 0.00003123
Iteration 44/1000 | Loss: 0.00003123
Iteration 45/1000 | Loss: 0.00003123
Iteration 46/1000 | Loss: 0.00003123
Iteration 47/1000 | Loss: 0.00003122
Iteration 48/1000 | Loss: 0.00003122
Iteration 49/1000 | Loss: 0.00003122
Iteration 50/1000 | Loss: 0.00003121
Iteration 51/1000 | Loss: 0.00003121
Iteration 52/1000 | Loss: 0.00003121
Iteration 53/1000 | Loss: 0.00003120
Iteration 54/1000 | Loss: 0.00003120
Iteration 55/1000 | Loss: 0.00003119
Iteration 56/1000 | Loss: 0.00003118
Iteration 57/1000 | Loss: 0.00003118
Iteration 58/1000 | Loss: 0.00003117
Iteration 59/1000 | Loss: 0.00003117
Iteration 60/1000 | Loss: 0.00003116
Iteration 61/1000 | Loss: 0.00003116
Iteration 62/1000 | Loss: 0.00003115
Iteration 63/1000 | Loss: 0.00003115
Iteration 64/1000 | Loss: 0.00003115
Iteration 65/1000 | Loss: 0.00003115
Iteration 66/1000 | Loss: 0.00003115
Iteration 67/1000 | Loss: 0.00003115
Iteration 68/1000 | Loss: 0.00003115
Iteration 69/1000 | Loss: 0.00003115
Iteration 70/1000 | Loss: 0.00003114
Iteration 71/1000 | Loss: 0.00003114
Iteration 72/1000 | Loss: 0.00003114
Iteration 73/1000 | Loss: 0.00003114
Iteration 74/1000 | Loss: 0.00003114
Iteration 75/1000 | Loss: 0.00003114
Iteration 76/1000 | Loss: 0.00003114
Iteration 77/1000 | Loss: 0.00003113
Iteration 78/1000 | Loss: 0.00003113
Iteration 79/1000 | Loss: 0.00003113
Iteration 80/1000 | Loss: 0.00003113
Iteration 81/1000 | Loss: 0.00003113
Iteration 82/1000 | Loss: 0.00003113
Iteration 83/1000 | Loss: 0.00003112
Iteration 84/1000 | Loss: 0.00003112
Iteration 85/1000 | Loss: 0.00003112
Iteration 86/1000 | Loss: 0.00003112
Iteration 87/1000 | Loss: 0.00003112
Iteration 88/1000 | Loss: 0.00003112
Iteration 89/1000 | Loss: 0.00003112
Iteration 90/1000 | Loss: 0.00003111
Iteration 91/1000 | Loss: 0.00003111
Iteration 92/1000 | Loss: 0.00003111
Iteration 93/1000 | Loss: 0.00003111
Iteration 94/1000 | Loss: 0.00003111
Iteration 95/1000 | Loss: 0.00003110
Iteration 96/1000 | Loss: 0.00003110
Iteration 97/1000 | Loss: 0.00003110
Iteration 98/1000 | Loss: 0.00003110
Iteration 99/1000 | Loss: 0.00003110
Iteration 100/1000 | Loss: 0.00003110
Iteration 101/1000 | Loss: 0.00003110
Iteration 102/1000 | Loss: 0.00003110
Iteration 103/1000 | Loss: 0.00003110
Iteration 104/1000 | Loss: 0.00003110
Iteration 105/1000 | Loss: 0.00003110
Iteration 106/1000 | Loss: 0.00003110
Iteration 107/1000 | Loss: 0.00003109
Iteration 108/1000 | Loss: 0.00003109
Iteration 109/1000 | Loss: 0.00003109
Iteration 110/1000 | Loss: 0.00003109
Iteration 111/1000 | Loss: 0.00003109
Iteration 112/1000 | Loss: 0.00003109
Iteration 113/1000 | Loss: 0.00003109
Iteration 114/1000 | Loss: 0.00003109
Iteration 115/1000 | Loss: 0.00003109
Iteration 116/1000 | Loss: 0.00003108
Iteration 117/1000 | Loss: 0.00003108
Iteration 118/1000 | Loss: 0.00003108
Iteration 119/1000 | Loss: 0.00003108
Iteration 120/1000 | Loss: 0.00003108
Iteration 121/1000 | Loss: 0.00003108
Iteration 122/1000 | Loss: 0.00003108
Iteration 123/1000 | Loss: 0.00003108
Iteration 124/1000 | Loss: 0.00003108
Iteration 125/1000 | Loss: 0.00003107
Iteration 126/1000 | Loss: 0.00003107
Iteration 127/1000 | Loss: 0.00003107
Iteration 128/1000 | Loss: 0.00003107
Iteration 129/1000 | Loss: 0.00003107
Iteration 130/1000 | Loss: 0.00003107
Iteration 131/1000 | Loss: 0.00003107
Iteration 132/1000 | Loss: 0.00003107
Iteration 133/1000 | Loss: 0.00003106
Iteration 134/1000 | Loss: 0.00003106
Iteration 135/1000 | Loss: 0.00003106
Iteration 136/1000 | Loss: 0.00003106
Iteration 137/1000 | Loss: 0.00003106
Iteration 138/1000 | Loss: 0.00003106
Iteration 139/1000 | Loss: 0.00003106
Iteration 140/1000 | Loss: 0.00003106
Iteration 141/1000 | Loss: 0.00003106
Iteration 142/1000 | Loss: 0.00003106
Iteration 143/1000 | Loss: 0.00003106
Iteration 144/1000 | Loss: 0.00003106
Iteration 145/1000 | Loss: 0.00003106
Iteration 146/1000 | Loss: 0.00003106
Iteration 147/1000 | Loss: 0.00003106
Iteration 148/1000 | Loss: 0.00003105
Iteration 149/1000 | Loss: 0.00003105
Iteration 150/1000 | Loss: 0.00003105
Iteration 151/1000 | Loss: 0.00003105
Iteration 152/1000 | Loss: 0.00003105
Iteration 153/1000 | Loss: 0.00003105
Iteration 154/1000 | Loss: 0.00003105
Iteration 155/1000 | Loss: 0.00003105
Iteration 156/1000 | Loss: 0.00003105
Iteration 157/1000 | Loss: 0.00003105
Iteration 158/1000 | Loss: 0.00003105
Iteration 159/1000 | Loss: 0.00003104
Iteration 160/1000 | Loss: 0.00003104
Iteration 161/1000 | Loss: 0.00003104
Iteration 162/1000 | Loss: 0.00003104
Iteration 163/1000 | Loss: 0.00003104
Iteration 164/1000 | Loss: 0.00003104
Iteration 165/1000 | Loss: 0.00003104
Iteration 166/1000 | Loss: 0.00003104
Iteration 167/1000 | Loss: 0.00003104
Iteration 168/1000 | Loss: 0.00003103
Iteration 169/1000 | Loss: 0.00003103
Iteration 170/1000 | Loss: 0.00003103
Iteration 171/1000 | Loss: 0.00003103
Iteration 172/1000 | Loss: 0.00003103
Iteration 173/1000 | Loss: 0.00003103
Iteration 174/1000 | Loss: 0.00003103
Iteration 175/1000 | Loss: 0.00003103
Iteration 176/1000 | Loss: 0.00003103
Iteration 177/1000 | Loss: 0.00003103
Iteration 178/1000 | Loss: 0.00003103
Iteration 179/1000 | Loss: 0.00003103
Iteration 180/1000 | Loss: 0.00003103
Iteration 181/1000 | Loss: 0.00003103
Iteration 182/1000 | Loss: 0.00003103
Iteration 183/1000 | Loss: 0.00003103
Iteration 184/1000 | Loss: 0.00003103
Iteration 185/1000 | Loss: 0.00003103
Iteration 186/1000 | Loss: 0.00003103
Iteration 187/1000 | Loss: 0.00003103
Iteration 188/1000 | Loss: 0.00003103
Iteration 189/1000 | Loss: 0.00003103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [3.102626578765921e-05, 3.102626578765921e-05, 3.102626578765921e-05, 3.102626578765921e-05, 3.102626578765921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.102626578765921e-05

Optimization complete. Final v2v error: 4.34249210357666 mm

Highest mean error: 5.2480244636535645 mm for frame 8

Lowest mean error: 3.503237247467041 mm for frame 154

Saving results

Total time: 41.85612106323242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849836
Iteration 2/25 | Loss: 0.00156669
Iteration 3/25 | Loss: 0.00106236
Iteration 4/25 | Loss: 0.00097913
Iteration 5/25 | Loss: 0.00095243
Iteration 6/25 | Loss: 0.00094334
Iteration 7/25 | Loss: 0.00095275
Iteration 8/25 | Loss: 0.00094151
Iteration 9/25 | Loss: 0.00093707
Iteration 10/25 | Loss: 0.00093589
Iteration 11/25 | Loss: 0.00093554
Iteration 12/25 | Loss: 0.00093545
Iteration 13/25 | Loss: 0.00093545
Iteration 14/25 | Loss: 0.00093545
Iteration 15/25 | Loss: 0.00093545
Iteration 16/25 | Loss: 0.00093545
Iteration 17/25 | Loss: 0.00093545
Iteration 18/25 | Loss: 0.00093545
Iteration 19/25 | Loss: 0.00093545
Iteration 20/25 | Loss: 0.00093545
Iteration 21/25 | Loss: 0.00093545
Iteration 22/25 | Loss: 0.00093545
Iteration 23/25 | Loss: 0.00093545
Iteration 24/25 | Loss: 0.00093545
Iteration 25/25 | Loss: 0.00093545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52532601
Iteration 2/25 | Loss: 0.00053078
Iteration 3/25 | Loss: 0.00053077
Iteration 4/25 | Loss: 0.00053077
Iteration 5/25 | Loss: 0.00053077
Iteration 6/25 | Loss: 0.00053077
Iteration 7/25 | Loss: 0.00053077
Iteration 8/25 | Loss: 0.00053077
Iteration 9/25 | Loss: 0.00053077
Iteration 10/25 | Loss: 0.00053077
Iteration 11/25 | Loss: 0.00053077
Iteration 12/25 | Loss: 0.00053077
Iteration 13/25 | Loss: 0.00053077
Iteration 14/25 | Loss: 0.00053077
Iteration 15/25 | Loss: 0.00053077
Iteration 16/25 | Loss: 0.00053077
Iteration 17/25 | Loss: 0.00053077
Iteration 18/25 | Loss: 0.00053077
Iteration 19/25 | Loss: 0.00053077
Iteration 20/25 | Loss: 0.00053077
Iteration 21/25 | Loss: 0.00053077
Iteration 22/25 | Loss: 0.00053077
Iteration 23/25 | Loss: 0.00053077
Iteration 24/25 | Loss: 0.00053077
Iteration 25/25 | Loss: 0.00053077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005307665560394526, 0.0005307665560394526, 0.0005307665560394526, 0.0005307665560394526, 0.0005307665560394526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005307665560394526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053077
Iteration 2/1000 | Loss: 0.00004410
Iteration 3/1000 | Loss: 0.00003056
Iteration 4/1000 | Loss: 0.00002754
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002475
Iteration 8/1000 | Loss: 0.00002416
Iteration 9/1000 | Loss: 0.00002386
Iteration 10/1000 | Loss: 0.00002360
Iteration 11/1000 | Loss: 0.00002333
Iteration 12/1000 | Loss: 0.00002316
Iteration 13/1000 | Loss: 0.00002307
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002294
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002292
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002292
Iteration 28/1000 | Loss: 0.00002291
Iteration 29/1000 | Loss: 0.00002287
Iteration 30/1000 | Loss: 0.00002287
Iteration 31/1000 | Loss: 0.00002284
Iteration 32/1000 | Loss: 0.00002284
Iteration 33/1000 | Loss: 0.00002284
Iteration 34/1000 | Loss: 0.00002283
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002281
Iteration 38/1000 | Loss: 0.00002279
Iteration 39/1000 | Loss: 0.00002277
Iteration 40/1000 | Loss: 0.00002277
Iteration 41/1000 | Loss: 0.00002276
Iteration 42/1000 | Loss: 0.00002276
Iteration 43/1000 | Loss: 0.00002275
Iteration 44/1000 | Loss: 0.00002275
Iteration 45/1000 | Loss: 0.00002275
Iteration 46/1000 | Loss: 0.00002274
Iteration 47/1000 | Loss: 0.00002274
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002273
Iteration 50/1000 | Loss: 0.00002273
Iteration 51/1000 | Loss: 0.00002273
Iteration 52/1000 | Loss: 0.00002273
Iteration 53/1000 | Loss: 0.00002273
Iteration 54/1000 | Loss: 0.00002273
Iteration 55/1000 | Loss: 0.00002273
Iteration 56/1000 | Loss: 0.00002272
Iteration 57/1000 | Loss: 0.00002272
Iteration 58/1000 | Loss: 0.00002272
Iteration 59/1000 | Loss: 0.00002272
Iteration 60/1000 | Loss: 0.00002271
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00002270
Iteration 64/1000 | Loss: 0.00002270
Iteration 65/1000 | Loss: 0.00002270
Iteration 66/1000 | Loss: 0.00002270
Iteration 67/1000 | Loss: 0.00002270
Iteration 68/1000 | Loss: 0.00002270
Iteration 69/1000 | Loss: 0.00002270
Iteration 70/1000 | Loss: 0.00002270
Iteration 71/1000 | Loss: 0.00002270
Iteration 72/1000 | Loss: 0.00002270
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002269
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002269
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002268
Iteration 84/1000 | Loss: 0.00002267
Iteration 85/1000 | Loss: 0.00002267
Iteration 86/1000 | Loss: 0.00002267
Iteration 87/1000 | Loss: 0.00002267
Iteration 88/1000 | Loss: 0.00002267
Iteration 89/1000 | Loss: 0.00002267
Iteration 90/1000 | Loss: 0.00002267
Iteration 91/1000 | Loss: 0.00002267
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002267
Iteration 95/1000 | Loss: 0.00002267
Iteration 96/1000 | Loss: 0.00002267
Iteration 97/1000 | Loss: 0.00002266
Iteration 98/1000 | Loss: 0.00002266
Iteration 99/1000 | Loss: 0.00002266
Iteration 100/1000 | Loss: 0.00002266
Iteration 101/1000 | Loss: 0.00002266
Iteration 102/1000 | Loss: 0.00002266
Iteration 103/1000 | Loss: 0.00002266
Iteration 104/1000 | Loss: 0.00002266
Iteration 105/1000 | Loss: 0.00002266
Iteration 106/1000 | Loss: 0.00002266
Iteration 107/1000 | Loss: 0.00002266
Iteration 108/1000 | Loss: 0.00002265
Iteration 109/1000 | Loss: 0.00002265
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002265
Iteration 121/1000 | Loss: 0.00002265
Iteration 122/1000 | Loss: 0.00002265
Iteration 123/1000 | Loss: 0.00002265
Iteration 124/1000 | Loss: 0.00002265
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002265
Iteration 133/1000 | Loss: 0.00002265
Iteration 134/1000 | Loss: 0.00002265
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Iteration 140/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.2646363504463807e-05, 2.2646363504463807e-05, 2.2646363504463807e-05, 2.2646363504463807e-05, 2.2646363504463807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2646363504463807e-05

Optimization complete. Final v2v error: 3.921043634414673 mm

Highest mean error: 4.810214519500732 mm for frame 56

Lowest mean error: 3.1910080909729004 mm for frame 38

Saving results

Total time: 47.549503803253174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059920
Iteration 2/25 | Loss: 0.01059919
Iteration 3/25 | Loss: 0.00222700
Iteration 4/25 | Loss: 0.00152016
Iteration 5/25 | Loss: 0.00136915
Iteration 6/25 | Loss: 0.00126551
Iteration 7/25 | Loss: 0.00123313
Iteration 8/25 | Loss: 0.00123032
Iteration 9/25 | Loss: 0.00115840
Iteration 10/25 | Loss: 0.00113637
Iteration 11/25 | Loss: 0.00109533
Iteration 12/25 | Loss: 0.00108264
Iteration 13/25 | Loss: 0.00105867
Iteration 14/25 | Loss: 0.00104557
Iteration 15/25 | Loss: 0.00104193
Iteration 16/25 | Loss: 0.00104482
Iteration 17/25 | Loss: 0.00102683
Iteration 18/25 | Loss: 0.00103187
Iteration 19/25 | Loss: 0.00102932
Iteration 20/25 | Loss: 0.00103125
Iteration 21/25 | Loss: 0.00102492
Iteration 22/25 | Loss: 0.00098634
Iteration 23/25 | Loss: 0.00098158
Iteration 24/25 | Loss: 0.00098038
Iteration 25/25 | Loss: 0.00097957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50806451
Iteration 2/25 | Loss: 0.00188395
Iteration 3/25 | Loss: 0.00183212
Iteration 4/25 | Loss: 0.00183212
Iteration 5/25 | Loss: 0.00183212
Iteration 6/25 | Loss: 0.00183212
Iteration 7/25 | Loss: 0.00183212
Iteration 8/25 | Loss: 0.00183212
Iteration 9/25 | Loss: 0.00183212
Iteration 10/25 | Loss: 0.00183212
Iteration 11/25 | Loss: 0.00183212
Iteration 12/25 | Loss: 0.00183212
Iteration 13/25 | Loss: 0.00183212
Iteration 14/25 | Loss: 0.00183212
Iteration 15/25 | Loss: 0.00183212
Iteration 16/25 | Loss: 0.00183212
Iteration 17/25 | Loss: 0.00183212
Iteration 18/25 | Loss: 0.00183212
Iteration 19/25 | Loss: 0.00183212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018321183742955327, 0.0018321183742955327, 0.0018321183742955327, 0.0018321183742955327, 0.0018321183742955327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018321183742955327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183212
Iteration 2/1000 | Loss: 0.00290738
Iteration 3/1000 | Loss: 0.00049568
Iteration 4/1000 | Loss: 0.00010867
Iteration 5/1000 | Loss: 0.00008051
Iteration 6/1000 | Loss: 0.00006163
Iteration 7/1000 | Loss: 0.00124849
Iteration 8/1000 | Loss: 0.00080011
Iteration 9/1000 | Loss: 0.00007672
Iteration 10/1000 | Loss: 0.00004987
Iteration 11/1000 | Loss: 0.00004055
Iteration 12/1000 | Loss: 0.00003202
Iteration 13/1000 | Loss: 0.00002837
Iteration 14/1000 | Loss: 0.00002690
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00036352
Iteration 17/1000 | Loss: 0.00093064
Iteration 18/1000 | Loss: 0.00075910
Iteration 19/1000 | Loss: 0.00060081
Iteration 20/1000 | Loss: 0.00003565
Iteration 21/1000 | Loss: 0.00002689
Iteration 22/1000 | Loss: 0.00002148
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001855
Iteration 28/1000 | Loss: 0.00001834
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001807
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001796
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001792
Iteration 59/1000 | Loss: 0.00001792
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001791
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001791
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001790
Iteration 67/1000 | Loss: 0.00001790
Iteration 68/1000 | Loss: 0.00001790
Iteration 69/1000 | Loss: 0.00001789
Iteration 70/1000 | Loss: 0.00001789
Iteration 71/1000 | Loss: 0.00001789
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001788
Iteration 74/1000 | Loss: 0.00001788
Iteration 75/1000 | Loss: 0.00001788
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.7808069969760254e-05, 1.7808069969760254e-05, 1.7808069969760254e-05, 1.7808069969760254e-05, 1.7808069969760254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7808069969760254e-05

Optimization complete. Final v2v error: 3.559298038482666 mm

Highest mean error: 4.003475666046143 mm for frame 152

Lowest mean error: 3.2352547645568848 mm for frame 225

Saving results

Total time: 108.30622458457947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931966
Iteration 2/25 | Loss: 0.00140779
Iteration 3/25 | Loss: 0.00095465
Iteration 4/25 | Loss: 0.00090425
Iteration 5/25 | Loss: 0.00089252
Iteration 6/25 | Loss: 0.00088882
Iteration 7/25 | Loss: 0.00088817
Iteration 8/25 | Loss: 0.00088817
Iteration 9/25 | Loss: 0.00088817
Iteration 10/25 | Loss: 0.00088817
Iteration 11/25 | Loss: 0.00088817
Iteration 12/25 | Loss: 0.00088817
Iteration 13/25 | Loss: 0.00088817
Iteration 14/25 | Loss: 0.00088817
Iteration 15/25 | Loss: 0.00088817
Iteration 16/25 | Loss: 0.00088817
Iteration 17/25 | Loss: 0.00088817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008881686953827739, 0.0008881686953827739, 0.0008881686953827739, 0.0008881686953827739, 0.0008881686953827739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008881686953827739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23638451
Iteration 2/25 | Loss: 0.00052662
Iteration 3/25 | Loss: 0.00052662
Iteration 4/25 | Loss: 0.00052662
Iteration 5/25 | Loss: 0.00052662
Iteration 6/25 | Loss: 0.00052662
Iteration 7/25 | Loss: 0.00052662
Iteration 8/25 | Loss: 0.00052662
Iteration 9/25 | Loss: 0.00052662
Iteration 10/25 | Loss: 0.00052662
Iteration 11/25 | Loss: 0.00052662
Iteration 12/25 | Loss: 0.00052662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005266159423626959, 0.0005266159423626959, 0.0005266159423626959, 0.0005266159423626959, 0.0005266159423626959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005266159423626959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052662
Iteration 2/1000 | Loss: 0.00003473
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002108
Iteration 6/1000 | Loss: 0.00002035
Iteration 7/1000 | Loss: 0.00001993
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001931
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001862
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001858
Iteration 23/1000 | Loss: 0.00001858
Iteration 24/1000 | Loss: 0.00001858
Iteration 25/1000 | Loss: 0.00001858
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001857
Iteration 28/1000 | Loss: 0.00001857
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001855
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001855
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001853
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001838
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001836
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001832
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001830
Iteration 95/1000 | Loss: 0.00001830
Iteration 96/1000 | Loss: 0.00001830
Iteration 97/1000 | Loss: 0.00001830
Iteration 98/1000 | Loss: 0.00001830
Iteration 99/1000 | Loss: 0.00001830
Iteration 100/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8304957848158665e-05, 1.8304957848158665e-05, 1.8304957848158665e-05, 1.8304957848158665e-05, 1.8304957848158665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8304957848158665e-05

Optimization complete. Final v2v error: 3.541141986846924 mm

Highest mean error: 4.8325300216674805 mm for frame 128

Lowest mean error: 2.7975337505340576 mm for frame 164

Saving results

Total time: 35.230692625045776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857965
Iteration 2/25 | Loss: 0.00106085
Iteration 3/25 | Loss: 0.00087339
Iteration 4/25 | Loss: 0.00085480
Iteration 5/25 | Loss: 0.00084951
Iteration 6/25 | Loss: 0.00084760
Iteration 7/25 | Loss: 0.00084720
Iteration 8/25 | Loss: 0.00084720
Iteration 9/25 | Loss: 0.00084720
Iteration 10/25 | Loss: 0.00084720
Iteration 11/25 | Loss: 0.00084720
Iteration 12/25 | Loss: 0.00084720
Iteration 13/25 | Loss: 0.00084720
Iteration 14/25 | Loss: 0.00084720
Iteration 15/25 | Loss: 0.00084720
Iteration 16/25 | Loss: 0.00084720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008471994660794735, 0.0008471994660794735, 0.0008471994660794735, 0.0008471994660794735, 0.0008471994660794735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008471994660794735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61173403
Iteration 2/25 | Loss: 0.00061831
Iteration 3/25 | Loss: 0.00061830
Iteration 4/25 | Loss: 0.00061830
Iteration 5/25 | Loss: 0.00061830
Iteration 6/25 | Loss: 0.00061830
Iteration 7/25 | Loss: 0.00061830
Iteration 8/25 | Loss: 0.00061830
Iteration 9/25 | Loss: 0.00061830
Iteration 10/25 | Loss: 0.00061830
Iteration 11/25 | Loss: 0.00061830
Iteration 12/25 | Loss: 0.00061830
Iteration 13/25 | Loss: 0.00061830
Iteration 14/25 | Loss: 0.00061830
Iteration 15/25 | Loss: 0.00061830
Iteration 16/25 | Loss: 0.00061830
Iteration 17/25 | Loss: 0.00061830
Iteration 18/25 | Loss: 0.00061830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006183012155815959, 0.0006183012155815959, 0.0006183012155815959, 0.0006183012155815959, 0.0006183012155815959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006183012155815959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061830
Iteration 2/1000 | Loss: 0.00003130
Iteration 3/1000 | Loss: 0.00001696
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001166
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001163
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001149
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001147
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001135
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001119
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001118
Iteration 54/1000 | Loss: 0.00001118
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001117
Iteration 57/1000 | Loss: 0.00001117
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001116
Iteration 60/1000 | Loss: 0.00001116
Iteration 61/1000 | Loss: 0.00001116
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001116
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001114
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001101
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001100
Iteration 153/1000 | Loss: 0.00001100
Iteration 154/1000 | Loss: 0.00001100
Iteration 155/1000 | Loss: 0.00001100
Iteration 156/1000 | Loss: 0.00001100
Iteration 157/1000 | Loss: 0.00001100
Iteration 158/1000 | Loss: 0.00001100
Iteration 159/1000 | Loss: 0.00001100
Iteration 160/1000 | Loss: 0.00001100
Iteration 161/1000 | Loss: 0.00001100
Iteration 162/1000 | Loss: 0.00001100
Iteration 163/1000 | Loss: 0.00001099
Iteration 164/1000 | Loss: 0.00001099
Iteration 165/1000 | Loss: 0.00001099
Iteration 166/1000 | Loss: 0.00001099
Iteration 167/1000 | Loss: 0.00001099
Iteration 168/1000 | Loss: 0.00001099
Iteration 169/1000 | Loss: 0.00001098
Iteration 170/1000 | Loss: 0.00001098
Iteration 171/1000 | Loss: 0.00001098
Iteration 172/1000 | Loss: 0.00001098
Iteration 173/1000 | Loss: 0.00001098
Iteration 174/1000 | Loss: 0.00001098
Iteration 175/1000 | Loss: 0.00001098
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001097
Iteration 179/1000 | Loss: 0.00001097
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001096
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001095
Iteration 191/1000 | Loss: 0.00001095
Iteration 192/1000 | Loss: 0.00001095
Iteration 193/1000 | Loss: 0.00001095
Iteration 194/1000 | Loss: 0.00001095
Iteration 195/1000 | Loss: 0.00001095
Iteration 196/1000 | Loss: 0.00001095
Iteration 197/1000 | Loss: 0.00001095
Iteration 198/1000 | Loss: 0.00001095
Iteration 199/1000 | Loss: 0.00001095
Iteration 200/1000 | Loss: 0.00001095
Iteration 201/1000 | Loss: 0.00001095
Iteration 202/1000 | Loss: 0.00001095
Iteration 203/1000 | Loss: 0.00001095
Iteration 204/1000 | Loss: 0.00001095
Iteration 205/1000 | Loss: 0.00001095
Iteration 206/1000 | Loss: 0.00001095
Iteration 207/1000 | Loss: 0.00001095
Iteration 208/1000 | Loss: 0.00001095
Iteration 209/1000 | Loss: 0.00001095
Iteration 210/1000 | Loss: 0.00001094
Iteration 211/1000 | Loss: 0.00001094
Iteration 212/1000 | Loss: 0.00001094
Iteration 213/1000 | Loss: 0.00001094
Iteration 214/1000 | Loss: 0.00001094
Iteration 215/1000 | Loss: 0.00001094
Iteration 216/1000 | Loss: 0.00001094
Iteration 217/1000 | Loss: 0.00001094
Iteration 218/1000 | Loss: 0.00001094
Iteration 219/1000 | Loss: 0.00001094
Iteration 220/1000 | Loss: 0.00001094
Iteration 221/1000 | Loss: 0.00001094
Iteration 222/1000 | Loss: 0.00001094
Iteration 223/1000 | Loss: 0.00001094
Iteration 224/1000 | Loss: 0.00001094
Iteration 225/1000 | Loss: 0.00001094
Iteration 226/1000 | Loss: 0.00001094
Iteration 227/1000 | Loss: 0.00001094
Iteration 228/1000 | Loss: 0.00001094
Iteration 229/1000 | Loss: 0.00001094
Iteration 230/1000 | Loss: 0.00001094
Iteration 231/1000 | Loss: 0.00001094
Iteration 232/1000 | Loss: 0.00001094
Iteration 233/1000 | Loss: 0.00001094
Iteration 234/1000 | Loss: 0.00001094
Iteration 235/1000 | Loss: 0.00001094
Iteration 236/1000 | Loss: 0.00001094
Iteration 237/1000 | Loss: 0.00001094
Iteration 238/1000 | Loss: 0.00001094
Iteration 239/1000 | Loss: 0.00001094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0941764230665285e-05, 1.0941764230665285e-05, 1.0941764230665285e-05, 1.0941764230665285e-05, 1.0941764230665285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0941764230665285e-05

Optimization complete. Final v2v error: 2.8018908500671387 mm

Highest mean error: 3.764313220977783 mm for frame 112

Lowest mean error: 2.538184881210327 mm for frame 171

Saving results

Total time: 41.951008319854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928441
Iteration 2/25 | Loss: 0.00166406
Iteration 3/25 | Loss: 0.00118759
Iteration 4/25 | Loss: 0.00109418
Iteration 5/25 | Loss: 0.00107140
Iteration 6/25 | Loss: 0.00105004
Iteration 7/25 | Loss: 0.00104153
Iteration 8/25 | Loss: 0.00103309
Iteration 9/25 | Loss: 0.00103143
Iteration 10/25 | Loss: 0.00104211
Iteration 11/25 | Loss: 0.00102390
Iteration 12/25 | Loss: 0.00102602
Iteration 13/25 | Loss: 0.00101539
Iteration 14/25 | Loss: 0.00104047
Iteration 15/25 | Loss: 0.00103846
Iteration 16/25 | Loss: 0.00102443
Iteration 17/25 | Loss: 0.00101956
Iteration 18/25 | Loss: 0.00101724
Iteration 19/25 | Loss: 0.00100902
Iteration 20/25 | Loss: 0.00100276
Iteration 21/25 | Loss: 0.00099935
Iteration 22/25 | Loss: 0.00100664
Iteration 23/25 | Loss: 0.00100369
Iteration 24/25 | Loss: 0.00100011
Iteration 25/25 | Loss: 0.00099903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43558764
Iteration 2/25 | Loss: 0.00111404
Iteration 3/25 | Loss: 0.00111385
Iteration 4/25 | Loss: 0.00111385
Iteration 5/25 | Loss: 0.00111385
Iteration 6/25 | Loss: 0.00111385
Iteration 7/25 | Loss: 0.00111385
Iteration 8/25 | Loss: 0.00111385
Iteration 9/25 | Loss: 0.00111385
Iteration 10/25 | Loss: 0.00111385
Iteration 11/25 | Loss: 0.00111385
Iteration 12/25 | Loss: 0.00111385
Iteration 13/25 | Loss: 0.00111385
Iteration 14/25 | Loss: 0.00111385
Iteration 15/25 | Loss: 0.00111385
Iteration 16/25 | Loss: 0.00111385
Iteration 17/25 | Loss: 0.00111385
Iteration 18/25 | Loss: 0.00111385
Iteration 19/25 | Loss: 0.00111385
Iteration 20/25 | Loss: 0.00111385
Iteration 21/25 | Loss: 0.00111385
Iteration 22/25 | Loss: 0.00111385
Iteration 23/25 | Loss: 0.00111385
Iteration 24/25 | Loss: 0.00111385
Iteration 25/25 | Loss: 0.00111385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111385
Iteration 2/1000 | Loss: 0.00610230
Iteration 3/1000 | Loss: 0.01131309
Iteration 4/1000 | Loss: 0.00145844
Iteration 5/1000 | Loss: 0.00219851
Iteration 6/1000 | Loss: 0.00171686
Iteration 7/1000 | Loss: 0.00093504
Iteration 8/1000 | Loss: 0.00044156
Iteration 9/1000 | Loss: 0.00239262
Iteration 10/1000 | Loss: 0.00209328
Iteration 11/1000 | Loss: 0.00237615
Iteration 12/1000 | Loss: 0.00251422
Iteration 13/1000 | Loss: 0.00152920
Iteration 14/1000 | Loss: 0.00364715
Iteration 15/1000 | Loss: 0.00219710
Iteration 16/1000 | Loss: 0.00300910
Iteration 17/1000 | Loss: 0.00208618
Iteration 18/1000 | Loss: 0.00089410
Iteration 19/1000 | Loss: 0.00693135
Iteration 20/1000 | Loss: 0.00497982
Iteration 21/1000 | Loss: 0.00270766
Iteration 22/1000 | Loss: 0.00130327
Iteration 23/1000 | Loss: 0.00255630
Iteration 24/1000 | Loss: 0.00105376
Iteration 25/1000 | Loss: 0.00204167
Iteration 26/1000 | Loss: 0.00236011
Iteration 27/1000 | Loss: 0.00219771
Iteration 28/1000 | Loss: 0.00281962
Iteration 29/1000 | Loss: 0.00748385
Iteration 30/1000 | Loss: 0.00473103
Iteration 31/1000 | Loss: 0.00395514
Iteration 32/1000 | Loss: 0.00792643
Iteration 33/1000 | Loss: 0.00521417
Iteration 34/1000 | Loss: 0.00561096
Iteration 35/1000 | Loss: 0.00609278
Iteration 36/1000 | Loss: 0.00910653
Iteration 37/1000 | Loss: 0.00254862
Iteration 38/1000 | Loss: 0.00308170
Iteration 39/1000 | Loss: 0.00646741
Iteration 40/1000 | Loss: 0.00579871
Iteration 41/1000 | Loss: 0.00299785
Iteration 42/1000 | Loss: 0.00474776
Iteration 43/1000 | Loss: 0.00408070
Iteration 44/1000 | Loss: 0.00303370
Iteration 45/1000 | Loss: 0.00528398
Iteration 46/1000 | Loss: 0.00502679
Iteration 47/1000 | Loss: 0.00317142
Iteration 48/1000 | Loss: 0.00415146
Iteration 49/1000 | Loss: 0.00178119
Iteration 50/1000 | Loss: 0.00136733
Iteration 51/1000 | Loss: 0.00294759
Iteration 52/1000 | Loss: 0.00718030
Iteration 53/1000 | Loss: 0.00354289
Iteration 54/1000 | Loss: 0.00298484
Iteration 55/1000 | Loss: 0.00570386
Iteration 56/1000 | Loss: 0.00617384
Iteration 57/1000 | Loss: 0.00072827
Iteration 58/1000 | Loss: 0.00232300
Iteration 59/1000 | Loss: 0.00137887
Iteration 60/1000 | Loss: 0.00151049
Iteration 61/1000 | Loss: 0.00196460
Iteration 62/1000 | Loss: 0.00176342
Iteration 63/1000 | Loss: 0.00206975
Iteration 64/1000 | Loss: 0.00219002
Iteration 65/1000 | Loss: 0.00200320
Iteration 66/1000 | Loss: 0.00582563
Iteration 67/1000 | Loss: 0.00121656
Iteration 68/1000 | Loss: 0.00381901
Iteration 69/1000 | Loss: 0.00334496
Iteration 70/1000 | Loss: 0.00069631
Iteration 71/1000 | Loss: 0.00226879
Iteration 72/1000 | Loss: 0.00251161
Iteration 73/1000 | Loss: 0.00225127
Iteration 74/1000 | Loss: 0.00257637
Iteration 75/1000 | Loss: 0.00206957
Iteration 76/1000 | Loss: 0.00312869
Iteration 77/1000 | Loss: 0.00117863
Iteration 78/1000 | Loss: 0.00282332
Iteration 79/1000 | Loss: 0.00465862
Iteration 80/1000 | Loss: 0.00043537
Iteration 81/1000 | Loss: 0.00196955
Iteration 82/1000 | Loss: 0.00117943
Iteration 83/1000 | Loss: 0.00209798
Iteration 84/1000 | Loss: 0.00263063
Iteration 85/1000 | Loss: 0.00306157
Iteration 86/1000 | Loss: 0.00290221
Iteration 87/1000 | Loss: 0.00215706
Iteration 88/1000 | Loss: 0.00332166
Iteration 89/1000 | Loss: 0.00267444
Iteration 90/1000 | Loss: 0.00240496
Iteration 91/1000 | Loss: 0.00275510
Iteration 92/1000 | Loss: 0.00231694
Iteration 93/1000 | Loss: 0.00231943
Iteration 94/1000 | Loss: 0.00152807
Iteration 95/1000 | Loss: 0.00329873
Iteration 96/1000 | Loss: 0.00214568
Iteration 97/1000 | Loss: 0.00016282
Iteration 98/1000 | Loss: 0.00060650
Iteration 99/1000 | Loss: 0.00016422
Iteration 100/1000 | Loss: 0.00411245
Iteration 101/1000 | Loss: 0.00268945
Iteration 102/1000 | Loss: 0.00220727
Iteration 103/1000 | Loss: 0.00382608
Iteration 104/1000 | Loss: 0.00280716
Iteration 105/1000 | Loss: 0.00023842
Iteration 106/1000 | Loss: 0.00060350
Iteration 107/1000 | Loss: 0.00017615
Iteration 108/1000 | Loss: 0.00187157
Iteration 109/1000 | Loss: 0.00150257
Iteration 110/1000 | Loss: 0.00133111
Iteration 111/1000 | Loss: 0.00230278
Iteration 112/1000 | Loss: 0.00130896
Iteration 113/1000 | Loss: 0.00044416
Iteration 114/1000 | Loss: 0.00035341
Iteration 115/1000 | Loss: 0.00044184
Iteration 116/1000 | Loss: 0.00133823
Iteration 117/1000 | Loss: 0.00081835
Iteration 118/1000 | Loss: 0.00147176
Iteration 119/1000 | Loss: 0.00026638
Iteration 120/1000 | Loss: 0.00023387
Iteration 121/1000 | Loss: 0.00029006
Iteration 122/1000 | Loss: 0.00023798
Iteration 123/1000 | Loss: 0.00131974
Iteration 124/1000 | Loss: 0.00176206
Iteration 125/1000 | Loss: 0.00078284
Iteration 126/1000 | Loss: 0.00046357
Iteration 127/1000 | Loss: 0.00064640
Iteration 128/1000 | Loss: 0.00013691
Iteration 129/1000 | Loss: 0.00066416
Iteration 130/1000 | Loss: 0.00067002
Iteration 131/1000 | Loss: 0.00071037
Iteration 132/1000 | Loss: 0.00153848
Iteration 133/1000 | Loss: 0.00083774
Iteration 134/1000 | Loss: 0.00129287
Iteration 135/1000 | Loss: 0.00065741
Iteration 136/1000 | Loss: 0.00089541
Iteration 137/1000 | Loss: 0.00039276
Iteration 138/1000 | Loss: 0.00028549
Iteration 139/1000 | Loss: 0.00103699
Iteration 140/1000 | Loss: 0.00092665
Iteration 141/1000 | Loss: 0.00183038
Iteration 142/1000 | Loss: 0.00100161
Iteration 143/1000 | Loss: 0.00193026
Iteration 144/1000 | Loss: 0.00119958
Iteration 145/1000 | Loss: 0.00084968
Iteration 146/1000 | Loss: 0.00127587
Iteration 147/1000 | Loss: 0.00098975
Iteration 148/1000 | Loss: 0.00115066
Iteration 149/1000 | Loss: 0.00112382
Iteration 150/1000 | Loss: 0.00092931
Iteration 151/1000 | Loss: 0.00110075
Iteration 152/1000 | Loss: 0.00130012
Iteration 153/1000 | Loss: 0.00114898
Iteration 154/1000 | Loss: 0.00102270
Iteration 155/1000 | Loss: 0.00078337
Iteration 156/1000 | Loss: 0.00012841
Iteration 157/1000 | Loss: 0.00039543
Iteration 158/1000 | Loss: 0.00042907
Iteration 159/1000 | Loss: 0.00163719
Iteration 160/1000 | Loss: 0.00077589
Iteration 161/1000 | Loss: 0.00076646
Iteration 162/1000 | Loss: 0.00124213
Iteration 163/1000 | Loss: 0.00104152
Iteration 164/1000 | Loss: 0.00083102
Iteration 165/1000 | Loss: 0.00046215
Iteration 166/1000 | Loss: 0.00143947
Iteration 167/1000 | Loss: 0.00119361
Iteration 168/1000 | Loss: 0.00157142
Iteration 169/1000 | Loss: 0.00141780
Iteration 170/1000 | Loss: 0.00144346
Iteration 171/1000 | Loss: 0.00097121
Iteration 172/1000 | Loss: 0.00120337
Iteration 173/1000 | Loss: 0.00092247
Iteration 174/1000 | Loss: 0.00052368
Iteration 175/1000 | Loss: 0.00164500
Iteration 176/1000 | Loss: 0.00144366
Iteration 177/1000 | Loss: 0.00147674
Iteration 178/1000 | Loss: 0.00102843
Iteration 179/1000 | Loss: 0.00136887
Iteration 180/1000 | Loss: 0.00026906
Iteration 181/1000 | Loss: 0.00127454
Iteration 182/1000 | Loss: 0.00084466
Iteration 183/1000 | Loss: 0.00019240
Iteration 184/1000 | Loss: 0.00039499
Iteration 185/1000 | Loss: 0.00027962
Iteration 186/1000 | Loss: 0.00029280
Iteration 187/1000 | Loss: 0.00026070
Iteration 188/1000 | Loss: 0.00022998
Iteration 189/1000 | Loss: 0.00006960
Iteration 190/1000 | Loss: 0.00061690
Iteration 191/1000 | Loss: 0.00039768
Iteration 192/1000 | Loss: 0.00037659
Iteration 193/1000 | Loss: 0.00024331
Iteration 194/1000 | Loss: 0.00036930
Iteration 195/1000 | Loss: 0.00163319
Iteration 196/1000 | Loss: 0.00061999
Iteration 197/1000 | Loss: 0.00058737
Iteration 198/1000 | Loss: 0.00029765
Iteration 199/1000 | Loss: 0.00016337
Iteration 200/1000 | Loss: 0.00006106
Iteration 201/1000 | Loss: 0.00005494
Iteration 202/1000 | Loss: 0.00004869
Iteration 203/1000 | Loss: 0.00004591
Iteration 204/1000 | Loss: 0.00004221
Iteration 205/1000 | Loss: 0.00003959
Iteration 206/1000 | Loss: 0.00026918
Iteration 207/1000 | Loss: 0.00004909
Iteration 208/1000 | Loss: 0.00006094
Iteration 209/1000 | Loss: 0.00004111
Iteration 210/1000 | Loss: 0.00003804
Iteration 211/1000 | Loss: 0.00003717
Iteration 212/1000 | Loss: 0.00003605
Iteration 213/1000 | Loss: 0.00003487
Iteration 214/1000 | Loss: 0.00010422
Iteration 215/1000 | Loss: 0.00023508
Iteration 216/1000 | Loss: 0.00015136
Iteration 217/1000 | Loss: 0.00021997
Iteration 218/1000 | Loss: 0.00011763
Iteration 219/1000 | Loss: 0.00018055
Iteration 220/1000 | Loss: 0.00038703
Iteration 221/1000 | Loss: 0.00023013
Iteration 222/1000 | Loss: 0.00023085
Iteration 223/1000 | Loss: 0.00026310
Iteration 224/1000 | Loss: 0.00033011
Iteration 225/1000 | Loss: 0.00042181
Iteration 226/1000 | Loss: 0.00029220
Iteration 227/1000 | Loss: 0.00012677
Iteration 228/1000 | Loss: 0.00021507
Iteration 229/1000 | Loss: 0.00008296
Iteration 230/1000 | Loss: 0.00008536
Iteration 231/1000 | Loss: 0.00003585
Iteration 232/1000 | Loss: 0.00003451
Iteration 233/1000 | Loss: 0.00004233
Iteration 234/1000 | Loss: 0.00003339
Iteration 235/1000 | Loss: 0.00003230
Iteration 236/1000 | Loss: 0.00003143
Iteration 237/1000 | Loss: 0.00003053
Iteration 238/1000 | Loss: 0.00002973
Iteration 239/1000 | Loss: 0.00139507
Iteration 240/1000 | Loss: 0.00034508
Iteration 241/1000 | Loss: 0.00002998
Iteration 242/1000 | Loss: 0.00002916
Iteration 243/1000 | Loss: 0.00002896
Iteration 244/1000 | Loss: 0.00002873
Iteration 245/1000 | Loss: 0.00002869
Iteration 246/1000 | Loss: 0.00137797
Iteration 247/1000 | Loss: 0.00149472
Iteration 248/1000 | Loss: 0.00023060
Iteration 249/1000 | Loss: 0.00004894
Iteration 250/1000 | Loss: 0.00003956
Iteration 251/1000 | Loss: 0.00003452
Iteration 252/1000 | Loss: 0.00003269
Iteration 253/1000 | Loss: 0.00003199
Iteration 254/1000 | Loss: 0.00003174
Iteration 255/1000 | Loss: 0.00003145
Iteration 256/1000 | Loss: 0.00003123
Iteration 257/1000 | Loss: 0.00003117
Iteration 258/1000 | Loss: 0.00003113
Iteration 259/1000 | Loss: 0.00024884
Iteration 260/1000 | Loss: 0.00023592
Iteration 261/1000 | Loss: 0.00008225
Iteration 262/1000 | Loss: 0.00018736
Iteration 263/1000 | Loss: 0.00031683
Iteration 264/1000 | Loss: 0.00084641
Iteration 265/1000 | Loss: 0.00022979
Iteration 266/1000 | Loss: 0.00078352
Iteration 267/1000 | Loss: 0.00005172
Iteration 268/1000 | Loss: 0.00003722
Iteration 269/1000 | Loss: 0.00003368
Iteration 270/1000 | Loss: 0.00003223
Iteration 271/1000 | Loss: 0.00003113
Iteration 272/1000 | Loss: 0.00003031
Iteration 273/1000 | Loss: 0.00002957
Iteration 274/1000 | Loss: 0.00005322
Iteration 275/1000 | Loss: 0.00005048
Iteration 276/1000 | Loss: 0.00005343
Iteration 277/1000 | Loss: 0.00003792
Iteration 278/1000 | Loss: 0.00005312
Iteration 279/1000 | Loss: 0.00003734
Iteration 280/1000 | Loss: 0.00005863
Iteration 281/1000 | Loss: 0.00003022
Iteration 282/1000 | Loss: 0.00005519
Iteration 283/1000 | Loss: 0.00004414
Iteration 284/1000 | Loss: 0.00146130
Iteration 285/1000 | Loss: 0.00006619
Iteration 286/1000 | Loss: 0.00004358
Iteration 287/1000 | Loss: 0.00003933
Iteration 288/1000 | Loss: 0.00003657
Iteration 289/1000 | Loss: 0.00003505
Iteration 290/1000 | Loss: 0.00003309
Iteration 291/1000 | Loss: 0.00003107
Iteration 292/1000 | Loss: 0.00002990
Iteration 293/1000 | Loss: 0.00002890
Iteration 294/1000 | Loss: 0.00002792
Iteration 295/1000 | Loss: 0.00002719
Iteration 296/1000 | Loss: 0.00002667
Iteration 297/1000 | Loss: 0.00002629
Iteration 298/1000 | Loss: 0.00002602
Iteration 299/1000 | Loss: 0.00002584
Iteration 300/1000 | Loss: 0.00002565
Iteration 301/1000 | Loss: 0.00002563
Iteration 302/1000 | Loss: 0.00002545
Iteration 303/1000 | Loss: 0.00002529
Iteration 304/1000 | Loss: 0.00002513
Iteration 305/1000 | Loss: 0.00002510
Iteration 306/1000 | Loss: 0.00002510
Iteration 307/1000 | Loss: 0.00002502
Iteration 308/1000 | Loss: 0.00002494
Iteration 309/1000 | Loss: 0.00002491
Iteration 310/1000 | Loss: 0.00002490
Iteration 311/1000 | Loss: 0.00002490
Iteration 312/1000 | Loss: 0.00002489
Iteration 313/1000 | Loss: 0.00002489
Iteration 314/1000 | Loss: 0.00002489
Iteration 315/1000 | Loss: 0.00002488
Iteration 316/1000 | Loss: 0.00002488
Iteration 317/1000 | Loss: 0.00002488
Iteration 318/1000 | Loss: 0.00002487
Iteration 319/1000 | Loss: 0.00002487
Iteration 320/1000 | Loss: 0.00002487
Iteration 321/1000 | Loss: 0.00002487
Iteration 322/1000 | Loss: 0.00002486
Iteration 323/1000 | Loss: 0.00002486
Iteration 324/1000 | Loss: 0.00002486
Iteration 325/1000 | Loss: 0.00002486
Iteration 326/1000 | Loss: 0.00002486
Iteration 327/1000 | Loss: 0.00002485
Iteration 328/1000 | Loss: 0.00002485
Iteration 329/1000 | Loss: 0.00002485
Iteration 330/1000 | Loss: 0.00002485
Iteration 331/1000 | Loss: 0.00002485
Iteration 332/1000 | Loss: 0.00002485
Iteration 333/1000 | Loss: 0.00002485
Iteration 334/1000 | Loss: 0.00002485
Iteration 335/1000 | Loss: 0.00002485
Iteration 336/1000 | Loss: 0.00002485
Iteration 337/1000 | Loss: 0.00002484
Iteration 338/1000 | Loss: 0.00002484
Iteration 339/1000 | Loss: 0.00002484
Iteration 340/1000 | Loss: 0.00002484
Iteration 341/1000 | Loss: 0.00002484
Iteration 342/1000 | Loss: 0.00002484
Iteration 343/1000 | Loss: 0.00002484
Iteration 344/1000 | Loss: 0.00002484
Iteration 345/1000 | Loss: 0.00002484
Iteration 346/1000 | Loss: 0.00002484
Iteration 347/1000 | Loss: 0.00002484
Iteration 348/1000 | Loss: 0.00002484
Iteration 349/1000 | Loss: 0.00002484
Iteration 350/1000 | Loss: 0.00002483
Iteration 351/1000 | Loss: 0.00002483
Iteration 352/1000 | Loss: 0.00002483
Iteration 353/1000 | Loss: 0.00002483
Iteration 354/1000 | Loss: 0.00002483
Iteration 355/1000 | Loss: 0.00002483
Iteration 356/1000 | Loss: 0.00002483
Iteration 357/1000 | Loss: 0.00002483
Iteration 358/1000 | Loss: 0.00002483
Iteration 359/1000 | Loss: 0.00002483
Iteration 360/1000 | Loss: 0.00002483
Iteration 361/1000 | Loss: 0.00002482
Iteration 362/1000 | Loss: 0.00002482
Iteration 363/1000 | Loss: 0.00002482
Iteration 364/1000 | Loss: 0.00002482
Iteration 365/1000 | Loss: 0.00002481
Iteration 366/1000 | Loss: 0.00002481
Iteration 367/1000 | Loss: 0.00002481
Iteration 368/1000 | Loss: 0.00002481
Iteration 369/1000 | Loss: 0.00002481
Iteration 370/1000 | Loss: 0.00002480
Iteration 371/1000 | Loss: 0.00002480
Iteration 372/1000 | Loss: 0.00002480
Iteration 373/1000 | Loss: 0.00002479
Iteration 374/1000 | Loss: 0.00002479
Iteration 375/1000 | Loss: 0.00002479
Iteration 376/1000 | Loss: 0.00002478
Iteration 377/1000 | Loss: 0.00002478
Iteration 378/1000 | Loss: 0.00002478
Iteration 379/1000 | Loss: 0.00002478
Iteration 380/1000 | Loss: 0.00002478
Iteration 381/1000 | Loss: 0.00002477
Iteration 382/1000 | Loss: 0.00002477
Iteration 383/1000 | Loss: 0.00002477
Iteration 384/1000 | Loss: 0.00002476
Iteration 385/1000 | Loss: 0.00002476
Iteration 386/1000 | Loss: 0.00002475
Iteration 387/1000 | Loss: 0.00002473
Iteration 388/1000 | Loss: 0.00002473
Iteration 389/1000 | Loss: 0.00002473
Iteration 390/1000 | Loss: 0.00002471
Iteration 391/1000 | Loss: 0.00002471
Iteration 392/1000 | Loss: 0.00002471
Iteration 393/1000 | Loss: 0.00002471
Iteration 394/1000 | Loss: 0.00002471
Iteration 395/1000 | Loss: 0.00002470
Iteration 396/1000 | Loss: 0.00002470
Iteration 397/1000 | Loss: 0.00002470
Iteration 398/1000 | Loss: 0.00002470
Iteration 399/1000 | Loss: 0.00002469
Iteration 400/1000 | Loss: 0.00002469
Iteration 401/1000 | Loss: 0.00002469
Iteration 402/1000 | Loss: 0.00002469
Iteration 403/1000 | Loss: 0.00002469
Iteration 404/1000 | Loss: 0.00002469
Iteration 405/1000 | Loss: 0.00002469
Iteration 406/1000 | Loss: 0.00002469
Iteration 407/1000 | Loss: 0.00002468
Iteration 408/1000 | Loss: 0.00002468
Iteration 409/1000 | Loss: 0.00002468
Iteration 410/1000 | Loss: 0.00002468
Iteration 411/1000 | Loss: 0.00002468
Iteration 412/1000 | Loss: 0.00002468
Iteration 413/1000 | Loss: 0.00002468
Iteration 414/1000 | Loss: 0.00002468
Iteration 415/1000 | Loss: 0.00002468
Iteration 416/1000 | Loss: 0.00002468
Iteration 417/1000 | Loss: 0.00002468
Iteration 418/1000 | Loss: 0.00002467
Iteration 419/1000 | Loss: 0.00002467
Iteration 420/1000 | Loss: 0.00002467
Iteration 421/1000 | Loss: 0.00002467
Iteration 422/1000 | Loss: 0.00002466
Iteration 423/1000 | Loss: 0.00002466
Iteration 424/1000 | Loss: 0.00002466
Iteration 425/1000 | Loss: 0.00002466
Iteration 426/1000 | Loss: 0.00002466
Iteration 427/1000 | Loss: 0.00002466
Iteration 428/1000 | Loss: 0.00002466
Iteration 429/1000 | Loss: 0.00002466
Iteration 430/1000 | Loss: 0.00002466
Iteration 431/1000 | Loss: 0.00002466
Iteration 432/1000 | Loss: 0.00002466
Iteration 433/1000 | Loss: 0.00002466
Iteration 434/1000 | Loss: 0.00002465
Iteration 435/1000 | Loss: 0.00002465
Iteration 436/1000 | Loss: 0.00002465
Iteration 437/1000 | Loss: 0.00002465
Iteration 438/1000 | Loss: 0.00002465
Iteration 439/1000 | Loss: 0.00002465
Iteration 440/1000 | Loss: 0.00002464
Iteration 441/1000 | Loss: 0.00002464
Iteration 442/1000 | Loss: 0.00002464
Iteration 443/1000 | Loss: 0.00002464
Iteration 444/1000 | Loss: 0.00002464
Iteration 445/1000 | Loss: 0.00002464
Iteration 446/1000 | Loss: 0.00002463
Iteration 447/1000 | Loss: 0.00002463
Iteration 448/1000 | Loss: 0.00002463
Iteration 449/1000 | Loss: 0.00002463
Iteration 450/1000 | Loss: 0.00002463
Iteration 451/1000 | Loss: 0.00002463
Iteration 452/1000 | Loss: 0.00002463
Iteration 453/1000 | Loss: 0.00002463
Iteration 454/1000 | Loss: 0.00002463
Iteration 455/1000 | Loss: 0.00002463
Iteration 456/1000 | Loss: 0.00002463
Iteration 457/1000 | Loss: 0.00002463
Iteration 458/1000 | Loss: 0.00002463
Iteration 459/1000 | Loss: 0.00002463
Iteration 460/1000 | Loss: 0.00002463
Iteration 461/1000 | Loss: 0.00002463
Iteration 462/1000 | Loss: 0.00002463
Iteration 463/1000 | Loss: 0.00002463
Iteration 464/1000 | Loss: 0.00002462
Iteration 465/1000 | Loss: 0.00002462
Iteration 466/1000 | Loss: 0.00002462
Iteration 467/1000 | Loss: 0.00002462
Iteration 468/1000 | Loss: 0.00002462
Iteration 469/1000 | Loss: 0.00002462
Iteration 470/1000 | Loss: 0.00002462
Iteration 471/1000 | Loss: 0.00002462
Iteration 472/1000 | Loss: 0.00002462
Iteration 473/1000 | Loss: 0.00002462
Iteration 474/1000 | Loss: 0.00002462
Iteration 475/1000 | Loss: 0.00002462
Iteration 476/1000 | Loss: 0.00002462
Iteration 477/1000 | Loss: 0.00002462
Iteration 478/1000 | Loss: 0.00002461
Iteration 479/1000 | Loss: 0.00002461
Iteration 480/1000 | Loss: 0.00002461
Iteration 481/1000 | Loss: 0.00002461
Iteration 482/1000 | Loss: 0.00002461
Iteration 483/1000 | Loss: 0.00002461
Iteration 484/1000 | Loss: 0.00002461
Iteration 485/1000 | Loss: 0.00002461
Iteration 486/1000 | Loss: 0.00002461
Iteration 487/1000 | Loss: 0.00002461
Iteration 488/1000 | Loss: 0.00002461
Iteration 489/1000 | Loss: 0.00002461
Iteration 490/1000 | Loss: 0.00002461
Iteration 491/1000 | Loss: 0.00002461
Iteration 492/1000 | Loss: 0.00002461
Iteration 493/1000 | Loss: 0.00002461
Iteration 494/1000 | Loss: 0.00002461
Iteration 495/1000 | Loss: 0.00002461
Iteration 496/1000 | Loss: 0.00002461
Iteration 497/1000 | Loss: 0.00002461
Iteration 498/1000 | Loss: 0.00002461
Iteration 499/1000 | Loss: 0.00002461
Iteration 500/1000 | Loss: 0.00002461
Iteration 501/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 501. Stopping optimization.
Last 5 losses: [2.4605256840004586e-05, 2.4605256840004586e-05, 2.4605256840004586e-05, 2.4605256840004586e-05, 2.4605256840004586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4605256840004586e-05

Optimization complete. Final v2v error: 4.090597629547119 mm

Highest mean error: 5.689819812774658 mm for frame 97

Lowest mean error: 3.0928118228912354 mm for frame 144

Saving results

Total time: 468.11297273635864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440842
Iteration 2/25 | Loss: 0.00095453
Iteration 3/25 | Loss: 0.00087517
Iteration 4/25 | Loss: 0.00085441
Iteration 5/25 | Loss: 0.00084700
Iteration 6/25 | Loss: 0.00084547
Iteration 7/25 | Loss: 0.00084528
Iteration 8/25 | Loss: 0.00084528
Iteration 9/25 | Loss: 0.00084528
Iteration 10/25 | Loss: 0.00084528
Iteration 11/25 | Loss: 0.00084528
Iteration 12/25 | Loss: 0.00084528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008452811744064093, 0.0008452811744064093, 0.0008452811744064093, 0.0008452811744064093, 0.0008452811744064093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008452811744064093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50914359
Iteration 2/25 | Loss: 0.00059592
Iteration 3/25 | Loss: 0.00059592
Iteration 4/25 | Loss: 0.00059592
Iteration 5/25 | Loss: 0.00059592
Iteration 6/25 | Loss: 0.00059592
Iteration 7/25 | Loss: 0.00059592
Iteration 8/25 | Loss: 0.00059592
Iteration 9/25 | Loss: 0.00059592
Iteration 10/25 | Loss: 0.00059592
Iteration 11/25 | Loss: 0.00059592
Iteration 12/25 | Loss: 0.00059592
Iteration 13/25 | Loss: 0.00059592
Iteration 14/25 | Loss: 0.00059592
Iteration 15/25 | Loss: 0.00059592
Iteration 16/25 | Loss: 0.00059592
Iteration 17/25 | Loss: 0.00059592
Iteration 18/25 | Loss: 0.00059592
Iteration 19/25 | Loss: 0.00059592
Iteration 20/25 | Loss: 0.00059592
Iteration 21/25 | Loss: 0.00059592
Iteration 22/25 | Loss: 0.00059592
Iteration 23/25 | Loss: 0.00059592
Iteration 24/25 | Loss: 0.00059592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005959198460914195, 0.0005959198460914195, 0.0005959198460914195, 0.0005959198460914195, 0.0005959198460914195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005959198460914195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059592
Iteration 2/1000 | Loss: 0.00002989
Iteration 3/1000 | Loss: 0.00002165
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001982
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001747
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001703
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001702
Iteration 100/1000 | Loss: 0.00001702
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001702
Iteration 110/1000 | Loss: 0.00001702
Iteration 111/1000 | Loss: 0.00001702
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001700
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001700
Iteration 131/1000 | Loss: 0.00001700
Iteration 132/1000 | Loss: 0.00001700
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.7002799722831696e-05, 1.7002799722831696e-05, 1.7002799722831696e-05, 1.7002799722831696e-05, 1.7002799722831696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7002799722831696e-05

Optimization complete. Final v2v error: 3.4880664348602295 mm

Highest mean error: 3.6905159950256348 mm for frame 140

Lowest mean error: 3.311854600906372 mm for frame 47

Saving results

Total time: 35.823108196258545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753055
Iteration 2/25 | Loss: 0.00152548
Iteration 3/25 | Loss: 0.00124700
Iteration 4/25 | Loss: 0.00117335
Iteration 5/25 | Loss: 0.00116431
Iteration 6/25 | Loss: 0.00114993
Iteration 7/25 | Loss: 0.00111253
Iteration 8/25 | Loss: 0.00109719
Iteration 9/25 | Loss: 0.00104205
Iteration 10/25 | Loss: 0.00105577
Iteration 11/25 | Loss: 0.00106253
Iteration 12/25 | Loss: 0.00101746
Iteration 13/25 | Loss: 0.00101574
Iteration 14/25 | Loss: 0.00100748
Iteration 15/25 | Loss: 0.00100404
Iteration 16/25 | Loss: 0.00100330
Iteration 17/25 | Loss: 0.00100293
Iteration 18/25 | Loss: 0.00100275
Iteration 19/25 | Loss: 0.00100258
Iteration 20/25 | Loss: 0.00100250
Iteration 21/25 | Loss: 0.00100246
Iteration 22/25 | Loss: 0.00100245
Iteration 23/25 | Loss: 0.00100245
Iteration 24/25 | Loss: 0.00100245
Iteration 25/25 | Loss: 0.00100245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60759401
Iteration 2/25 | Loss: 0.00097689
Iteration 3/25 | Loss: 0.00097688
Iteration 4/25 | Loss: 0.00097688
Iteration 5/25 | Loss: 0.00097688
Iteration 6/25 | Loss: 0.00097688
Iteration 7/25 | Loss: 0.00097688
Iteration 8/25 | Loss: 0.00097688
Iteration 9/25 | Loss: 0.00097688
Iteration 10/25 | Loss: 0.00097687
Iteration 11/25 | Loss: 0.00097687
Iteration 12/25 | Loss: 0.00097687
Iteration 13/25 | Loss: 0.00097687
Iteration 14/25 | Loss: 0.00097687
Iteration 15/25 | Loss: 0.00097687
Iteration 16/25 | Loss: 0.00097687
Iteration 17/25 | Loss: 0.00097687
Iteration 18/25 | Loss: 0.00097687
Iteration 19/25 | Loss: 0.00097687
Iteration 20/25 | Loss: 0.00097687
Iteration 21/25 | Loss: 0.00097687
Iteration 22/25 | Loss: 0.00097687
Iteration 23/25 | Loss: 0.00097687
Iteration 24/25 | Loss: 0.00097687
Iteration 25/25 | Loss: 0.00097687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097687
Iteration 2/1000 | Loss: 0.00507602
Iteration 3/1000 | Loss: 0.01050728
Iteration 4/1000 | Loss: 0.01964752
Iteration 5/1000 | Loss: 0.00119843
Iteration 6/1000 | Loss: 0.00069719
Iteration 7/1000 | Loss: 0.00010840
Iteration 8/1000 | Loss: 0.00008139
Iteration 9/1000 | Loss: 0.00199609
Iteration 10/1000 | Loss: 0.00515393
Iteration 11/1000 | Loss: 0.00339647
Iteration 12/1000 | Loss: 0.00369312
Iteration 13/1000 | Loss: 0.00672040
Iteration 14/1000 | Loss: 0.00421564
Iteration 15/1000 | Loss: 0.00266528
Iteration 16/1000 | Loss: 0.00306876
Iteration 17/1000 | Loss: 0.00278537
Iteration 18/1000 | Loss: 0.00496509
Iteration 19/1000 | Loss: 0.00343330
Iteration 20/1000 | Loss: 0.00583869
Iteration 21/1000 | Loss: 0.00879392
Iteration 22/1000 | Loss: 0.00724895
Iteration 23/1000 | Loss: 0.00359911
Iteration 24/1000 | Loss: 0.00532260
Iteration 25/1000 | Loss: 0.00255561
Iteration 26/1000 | Loss: 0.00351275
Iteration 27/1000 | Loss: 0.00276794
Iteration 28/1000 | Loss: 0.00157666
Iteration 29/1000 | Loss: 0.00113930
Iteration 30/1000 | Loss: 0.00428461
Iteration 31/1000 | Loss: 0.00208393
Iteration 32/1000 | Loss: 0.00396049
Iteration 33/1000 | Loss: 0.00114236
Iteration 34/1000 | Loss: 0.00456609
Iteration 35/1000 | Loss: 0.00382080
Iteration 36/1000 | Loss: 0.00021838
Iteration 37/1000 | Loss: 0.00009824
Iteration 38/1000 | Loss: 0.00016101
Iteration 39/1000 | Loss: 0.00016912
Iteration 40/1000 | Loss: 0.00334249
Iteration 41/1000 | Loss: 0.00309196
Iteration 42/1000 | Loss: 0.00593200
Iteration 43/1000 | Loss: 0.00169756
Iteration 44/1000 | Loss: 0.00127271
Iteration 45/1000 | Loss: 0.00266316
Iteration 46/1000 | Loss: 0.00275404
Iteration 47/1000 | Loss: 0.00060741
Iteration 48/1000 | Loss: 0.00162223
Iteration 49/1000 | Loss: 0.00035106
Iteration 50/1000 | Loss: 0.00106643
Iteration 51/1000 | Loss: 0.00056979
Iteration 52/1000 | Loss: 0.00012017
Iteration 53/1000 | Loss: 0.00013517
Iteration 54/1000 | Loss: 0.00215919
Iteration 55/1000 | Loss: 0.00286757
Iteration 56/1000 | Loss: 0.00267534
Iteration 57/1000 | Loss: 0.00298358
Iteration 58/1000 | Loss: 0.00225213
Iteration 59/1000 | Loss: 0.00465446
Iteration 60/1000 | Loss: 0.00052407
Iteration 61/1000 | Loss: 0.00035081
Iteration 62/1000 | Loss: 0.00505725
Iteration 63/1000 | Loss: 0.00010645
Iteration 64/1000 | Loss: 0.00008136
Iteration 65/1000 | Loss: 0.00006162
Iteration 66/1000 | Loss: 0.00005423
Iteration 67/1000 | Loss: 0.00486845
Iteration 68/1000 | Loss: 0.00153764
Iteration 69/1000 | Loss: 0.00006570
Iteration 70/1000 | Loss: 0.00005228
Iteration 71/1000 | Loss: 0.00005054
Iteration 72/1000 | Loss: 0.00004951
Iteration 73/1000 | Loss: 0.00013632
Iteration 74/1000 | Loss: 0.00537703
Iteration 75/1000 | Loss: 0.00032883
Iteration 76/1000 | Loss: 0.00263622
Iteration 77/1000 | Loss: 0.00163409
Iteration 78/1000 | Loss: 0.00350314
Iteration 79/1000 | Loss: 0.00237383
Iteration 80/1000 | Loss: 0.00096092
Iteration 81/1000 | Loss: 0.00177061
Iteration 82/1000 | Loss: 0.00580606
Iteration 83/1000 | Loss: 0.00551990
Iteration 84/1000 | Loss: 0.00182743
Iteration 85/1000 | Loss: 0.00249543
Iteration 86/1000 | Loss: 0.00214992
Iteration 87/1000 | Loss: 0.00186303
Iteration 88/1000 | Loss: 0.00340582
Iteration 89/1000 | Loss: 0.00386751
Iteration 90/1000 | Loss: 0.00237946
Iteration 91/1000 | Loss: 0.00078727
Iteration 92/1000 | Loss: 0.00176242
Iteration 93/1000 | Loss: 0.00129931
Iteration 94/1000 | Loss: 0.00093649
Iteration 95/1000 | Loss: 0.00195844
Iteration 96/1000 | Loss: 0.00090497
Iteration 97/1000 | Loss: 0.00314843
Iteration 98/1000 | Loss: 0.00148727
Iteration 99/1000 | Loss: 0.00311476
Iteration 100/1000 | Loss: 0.00433248
Iteration 101/1000 | Loss: 0.00148674
Iteration 102/1000 | Loss: 0.00053034
Iteration 103/1000 | Loss: 0.00482492
Iteration 104/1000 | Loss: 0.00077720
Iteration 105/1000 | Loss: 0.00030938
Iteration 106/1000 | Loss: 0.00009249
Iteration 107/1000 | Loss: 0.00020541
Iteration 108/1000 | Loss: 0.00015693
Iteration 109/1000 | Loss: 0.00482643
Iteration 110/1000 | Loss: 0.00522834
Iteration 111/1000 | Loss: 0.00215777
Iteration 112/1000 | Loss: 0.00450596
Iteration 113/1000 | Loss: 0.00027695
Iteration 114/1000 | Loss: 0.00007331
Iteration 115/1000 | Loss: 0.00012342
Iteration 116/1000 | Loss: 0.00023306
Iteration 117/1000 | Loss: 0.00013453
Iteration 118/1000 | Loss: 0.00019773
Iteration 119/1000 | Loss: 0.00055143
Iteration 120/1000 | Loss: 0.00374896
Iteration 121/1000 | Loss: 0.00541090
Iteration 122/1000 | Loss: 0.00168289
Iteration 123/1000 | Loss: 0.00006935
Iteration 124/1000 | Loss: 0.00021247
Iteration 125/1000 | Loss: 0.00004479
Iteration 126/1000 | Loss: 0.00004067
Iteration 127/1000 | Loss: 0.00003918
Iteration 128/1000 | Loss: 0.00003827
Iteration 129/1000 | Loss: 0.00003757
Iteration 130/1000 | Loss: 0.00003657
Iteration 131/1000 | Loss: 0.00004228
Iteration 132/1000 | Loss: 0.00004852
Iteration 133/1000 | Loss: 0.00003638
Iteration 134/1000 | Loss: 0.00003520
Iteration 135/1000 | Loss: 0.00003480
Iteration 136/1000 | Loss: 0.00003556
Iteration 137/1000 | Loss: 0.00003429
Iteration 138/1000 | Loss: 0.00003386
Iteration 139/1000 | Loss: 0.00010706
Iteration 140/1000 | Loss: 0.00010324
Iteration 141/1000 | Loss: 0.00003601
Iteration 142/1000 | Loss: 0.00011255
Iteration 143/1000 | Loss: 0.00014475
Iteration 144/1000 | Loss: 0.00003885
Iteration 145/1000 | Loss: 0.00006987
Iteration 146/1000 | Loss: 0.00004161
Iteration 147/1000 | Loss: 0.00003388
Iteration 148/1000 | Loss: 0.00003353
Iteration 149/1000 | Loss: 0.00003328
Iteration 150/1000 | Loss: 0.00003325
Iteration 151/1000 | Loss: 0.00003325
Iteration 152/1000 | Loss: 0.00003324
Iteration 153/1000 | Loss: 0.00003320
Iteration 154/1000 | Loss: 0.00011471
Iteration 155/1000 | Loss: 0.00008188
Iteration 156/1000 | Loss: 0.00003323
Iteration 157/1000 | Loss: 0.00010810
Iteration 158/1000 | Loss: 0.00005956
Iteration 159/1000 | Loss: 0.00003943
Iteration 160/1000 | Loss: 0.00004219
Iteration 161/1000 | Loss: 0.00003379
Iteration 162/1000 | Loss: 0.00011239
Iteration 163/1000 | Loss: 0.00003550
Iteration 164/1000 | Loss: 0.00006750
Iteration 165/1000 | Loss: 0.00004949
Iteration 166/1000 | Loss: 0.00004009
Iteration 167/1000 | Loss: 0.00006403
Iteration 168/1000 | Loss: 0.00004781
Iteration 169/1000 | Loss: 0.00005597
Iteration 170/1000 | Loss: 0.00006401
Iteration 171/1000 | Loss: 0.00004873
Iteration 172/1000 | Loss: 0.00006801
Iteration 173/1000 | Loss: 0.00005233
Iteration 174/1000 | Loss: 0.00004371
Iteration 175/1000 | Loss: 0.00004943
Iteration 176/1000 | Loss: 0.00004605
Iteration 177/1000 | Loss: 0.00006048
Iteration 178/1000 | Loss: 0.00004969
Iteration 179/1000 | Loss: 0.00005737
Iteration 180/1000 | Loss: 0.00004069
Iteration 181/1000 | Loss: 0.00003441
Iteration 182/1000 | Loss: 0.00003402
Iteration 183/1000 | Loss: 0.00003363
Iteration 184/1000 | Loss: 0.00003352
Iteration 185/1000 | Loss: 0.00003335
Iteration 186/1000 | Loss: 0.00003328
Iteration 187/1000 | Loss: 0.00003312
Iteration 188/1000 | Loss: 0.00003312
Iteration 189/1000 | Loss: 0.00003311
Iteration 190/1000 | Loss: 0.00003311
Iteration 191/1000 | Loss: 0.00003310
Iteration 192/1000 | Loss: 0.00003310
Iteration 193/1000 | Loss: 0.00003309
Iteration 194/1000 | Loss: 0.00003309
Iteration 195/1000 | Loss: 0.00003309
Iteration 196/1000 | Loss: 0.00003309
Iteration 197/1000 | Loss: 0.00003309
Iteration 198/1000 | Loss: 0.00003309
Iteration 199/1000 | Loss: 0.00003308
Iteration 200/1000 | Loss: 0.00003308
Iteration 201/1000 | Loss: 0.00003308
Iteration 202/1000 | Loss: 0.00003307
Iteration 203/1000 | Loss: 0.00003307
Iteration 204/1000 | Loss: 0.00003306
Iteration 205/1000 | Loss: 0.00003306
Iteration 206/1000 | Loss: 0.00003306
Iteration 207/1000 | Loss: 0.00003305
Iteration 208/1000 | Loss: 0.00003305
Iteration 209/1000 | Loss: 0.00003305
Iteration 210/1000 | Loss: 0.00003305
Iteration 211/1000 | Loss: 0.00003305
Iteration 212/1000 | Loss: 0.00003304
Iteration 213/1000 | Loss: 0.00003304
Iteration 214/1000 | Loss: 0.00003304
Iteration 215/1000 | Loss: 0.00003304
Iteration 216/1000 | Loss: 0.00003304
Iteration 217/1000 | Loss: 0.00003304
Iteration 218/1000 | Loss: 0.00003304
Iteration 219/1000 | Loss: 0.00003304
Iteration 220/1000 | Loss: 0.00003303
Iteration 221/1000 | Loss: 0.00003303
Iteration 222/1000 | Loss: 0.00003303
Iteration 223/1000 | Loss: 0.00003303
Iteration 224/1000 | Loss: 0.00003303
Iteration 225/1000 | Loss: 0.00003302
Iteration 226/1000 | Loss: 0.00003302
Iteration 227/1000 | Loss: 0.00003302
Iteration 228/1000 | Loss: 0.00003302
Iteration 229/1000 | Loss: 0.00003302
Iteration 230/1000 | Loss: 0.00003301
Iteration 231/1000 | Loss: 0.00003301
Iteration 232/1000 | Loss: 0.00003301
Iteration 233/1000 | Loss: 0.00003301
Iteration 234/1000 | Loss: 0.00003301
Iteration 235/1000 | Loss: 0.00003301
Iteration 236/1000 | Loss: 0.00003300
Iteration 237/1000 | Loss: 0.00003300
Iteration 238/1000 | Loss: 0.00003299
Iteration 239/1000 | Loss: 0.00003299
Iteration 240/1000 | Loss: 0.00003297
Iteration 241/1000 | Loss: 0.00003297
Iteration 242/1000 | Loss: 0.00003297
Iteration 243/1000 | Loss: 0.00003295
Iteration 244/1000 | Loss: 0.00003294
Iteration 245/1000 | Loss: 0.00003293
Iteration 246/1000 | Loss: 0.00003293
Iteration 247/1000 | Loss: 0.00003293
Iteration 248/1000 | Loss: 0.00003292
Iteration 249/1000 | Loss: 0.00003292
Iteration 250/1000 | Loss: 0.00003292
Iteration 251/1000 | Loss: 0.00003291
Iteration 252/1000 | Loss: 0.00003291
Iteration 253/1000 | Loss: 0.00003291
Iteration 254/1000 | Loss: 0.00003291
Iteration 255/1000 | Loss: 0.00003291
Iteration 256/1000 | Loss: 0.00003291
Iteration 257/1000 | Loss: 0.00003291
Iteration 258/1000 | Loss: 0.00003290
Iteration 259/1000 | Loss: 0.00003290
Iteration 260/1000 | Loss: 0.00003290
Iteration 261/1000 | Loss: 0.00003290
Iteration 262/1000 | Loss: 0.00003290
Iteration 263/1000 | Loss: 0.00003289
Iteration 264/1000 | Loss: 0.00003289
Iteration 265/1000 | Loss: 0.00003289
Iteration 266/1000 | Loss: 0.00003288
Iteration 267/1000 | Loss: 0.00003288
Iteration 268/1000 | Loss: 0.00003287
Iteration 269/1000 | Loss: 0.00003287
Iteration 270/1000 | Loss: 0.00003287
Iteration 271/1000 | Loss: 0.00003286
Iteration 272/1000 | Loss: 0.00003286
Iteration 273/1000 | Loss: 0.00003285
Iteration 274/1000 | Loss: 0.00003284
Iteration 275/1000 | Loss: 0.00003284
Iteration 276/1000 | Loss: 0.00003283
Iteration 277/1000 | Loss: 0.00003283
Iteration 278/1000 | Loss: 0.00003283
Iteration 279/1000 | Loss: 0.00003283
Iteration 280/1000 | Loss: 0.00003283
Iteration 281/1000 | Loss: 0.00003282
Iteration 282/1000 | Loss: 0.00003282
Iteration 283/1000 | Loss: 0.00003282
Iteration 284/1000 | Loss: 0.00003281
Iteration 285/1000 | Loss: 0.00003281
Iteration 286/1000 | Loss: 0.00003281
Iteration 287/1000 | Loss: 0.00003281
Iteration 288/1000 | Loss: 0.00003280
Iteration 289/1000 | Loss: 0.00003280
Iteration 290/1000 | Loss: 0.00003279
Iteration 291/1000 | Loss: 0.00003279
Iteration 292/1000 | Loss: 0.00003278
Iteration 293/1000 | Loss: 0.00003277
Iteration 294/1000 | Loss: 0.00003277
Iteration 295/1000 | Loss: 0.00003277
Iteration 296/1000 | Loss: 0.00003277
Iteration 297/1000 | Loss: 0.00003276
Iteration 298/1000 | Loss: 0.00003276
Iteration 299/1000 | Loss: 0.00003276
Iteration 300/1000 | Loss: 0.00003275
Iteration 301/1000 | Loss: 0.00003274
Iteration 302/1000 | Loss: 0.00006493
Iteration 303/1000 | Loss: 0.00005913
Iteration 304/1000 | Loss: 0.00005754
Iteration 305/1000 | Loss: 0.00005575
Iteration 306/1000 | Loss: 0.00006067
Iteration 307/1000 | Loss: 0.00006121
Iteration 308/1000 | Loss: 0.00005761
Iteration 309/1000 | Loss: 0.00006163
Iteration 310/1000 | Loss: 0.00009710
Iteration 311/1000 | Loss: 0.00004632
Iteration 312/1000 | Loss: 0.00003725
Iteration 313/1000 | Loss: 0.00006588
Iteration 314/1000 | Loss: 0.00005415
Iteration 315/1000 | Loss: 0.00005763
Iteration 316/1000 | Loss: 0.00004497
Iteration 317/1000 | Loss: 0.00004012
Iteration 318/1000 | Loss: 0.00005933
Iteration 319/1000 | Loss: 0.00003657
Iteration 320/1000 | Loss: 0.00003500
Iteration 321/1000 | Loss: 0.00003420
Iteration 322/1000 | Loss: 0.00003376
Iteration 323/1000 | Loss: 0.00003360
Iteration 324/1000 | Loss: 0.00003348
Iteration 325/1000 | Loss: 0.00003341
Iteration 326/1000 | Loss: 0.00003336
Iteration 327/1000 | Loss: 0.00003331
Iteration 328/1000 | Loss: 0.00003329
Iteration 329/1000 | Loss: 0.00003328
Iteration 330/1000 | Loss: 0.00003327
Iteration 331/1000 | Loss: 0.00003326
Iteration 332/1000 | Loss: 0.00003325
Iteration 333/1000 | Loss: 0.00003323
Iteration 334/1000 | Loss: 0.00003322
Iteration 335/1000 | Loss: 0.00003321
Iteration 336/1000 | Loss: 0.00003320
Iteration 337/1000 | Loss: 0.00003320
Iteration 338/1000 | Loss: 0.00003320
Iteration 339/1000 | Loss: 0.00003320
Iteration 340/1000 | Loss: 0.00003320
Iteration 341/1000 | Loss: 0.00003318
Iteration 342/1000 | Loss: 0.00003316
Iteration 343/1000 | Loss: 0.00003315
Iteration 344/1000 | Loss: 0.00003315
Iteration 345/1000 | Loss: 0.00003315
Iteration 346/1000 | Loss: 0.00003314
Iteration 347/1000 | Loss: 0.00003314
Iteration 348/1000 | Loss: 0.00003313
Iteration 349/1000 | Loss: 0.00003313
Iteration 350/1000 | Loss: 0.00003313
Iteration 351/1000 | Loss: 0.00003313
Iteration 352/1000 | Loss: 0.00003312
Iteration 353/1000 | Loss: 0.00003312
Iteration 354/1000 | Loss: 0.00003312
Iteration 355/1000 | Loss: 0.00003311
Iteration 356/1000 | Loss: 0.00003305
Iteration 357/1000 | Loss: 0.00003303
Iteration 358/1000 | Loss: 0.00003303
Iteration 359/1000 | Loss: 0.00003302
Iteration 360/1000 | Loss: 0.00003300
Iteration 361/1000 | Loss: 0.00003300
Iteration 362/1000 | Loss: 0.00003299
Iteration 363/1000 | Loss: 0.00003295
Iteration 364/1000 | Loss: 0.00003295
Iteration 365/1000 | Loss: 0.00003294
Iteration 366/1000 | Loss: 0.00003294
Iteration 367/1000 | Loss: 0.00003293
Iteration 368/1000 | Loss: 0.00003293
Iteration 369/1000 | Loss: 0.00003293
Iteration 370/1000 | Loss: 0.00003293
Iteration 371/1000 | Loss: 0.00003293
Iteration 372/1000 | Loss: 0.00003293
Iteration 373/1000 | Loss: 0.00003292
Iteration 374/1000 | Loss: 0.00003292
Iteration 375/1000 | Loss: 0.00003292
Iteration 376/1000 | Loss: 0.00003292
Iteration 377/1000 | Loss: 0.00003292
Iteration 378/1000 | Loss: 0.00003292
Iteration 379/1000 | Loss: 0.00003291
Iteration 380/1000 | Loss: 0.00003291
Iteration 381/1000 | Loss: 0.00003291
Iteration 382/1000 | Loss: 0.00003291
Iteration 383/1000 | Loss: 0.00003291
Iteration 384/1000 | Loss: 0.00003290
Iteration 385/1000 | Loss: 0.00003290
Iteration 386/1000 | Loss: 0.00003289
Iteration 387/1000 | Loss: 0.00003289
Iteration 388/1000 | Loss: 0.00003288
Iteration 389/1000 | Loss: 0.00003288
Iteration 390/1000 | Loss: 0.00003288
Iteration 391/1000 | Loss: 0.00003288
Iteration 392/1000 | Loss: 0.00003288
Iteration 393/1000 | Loss: 0.00003287
Iteration 394/1000 | Loss: 0.00003287
Iteration 395/1000 | Loss: 0.00003286
Iteration 396/1000 | Loss: 0.00003286
Iteration 397/1000 | Loss: 0.00003286
Iteration 398/1000 | Loss: 0.00003286
Iteration 399/1000 | Loss: 0.00003285
Iteration 400/1000 | Loss: 0.00003285
Iteration 401/1000 | Loss: 0.00003285
Iteration 402/1000 | Loss: 0.00003285
Iteration 403/1000 | Loss: 0.00003285
Iteration 404/1000 | Loss: 0.00003285
Iteration 405/1000 | Loss: 0.00003285
Iteration 406/1000 | Loss: 0.00003284
Iteration 407/1000 | Loss: 0.00003284
Iteration 408/1000 | Loss: 0.00003284
Iteration 409/1000 | Loss: 0.00003283
Iteration 410/1000 | Loss: 0.00003283
Iteration 411/1000 | Loss: 0.00003283
Iteration 412/1000 | Loss: 0.00003283
Iteration 413/1000 | Loss: 0.00003283
Iteration 414/1000 | Loss: 0.00003282
Iteration 415/1000 | Loss: 0.00003282
Iteration 416/1000 | Loss: 0.00003282
Iteration 417/1000 | Loss: 0.00003282
Iteration 418/1000 | Loss: 0.00003281
Iteration 419/1000 | Loss: 0.00003281
Iteration 420/1000 | Loss: 0.00003281
Iteration 421/1000 | Loss: 0.00003281
Iteration 422/1000 | Loss: 0.00003281
Iteration 423/1000 | Loss: 0.00003281
Iteration 424/1000 | Loss: 0.00003280
Iteration 425/1000 | Loss: 0.00003280
Iteration 426/1000 | Loss: 0.00003280
Iteration 427/1000 | Loss: 0.00003279
Iteration 428/1000 | Loss: 0.00003279
Iteration 429/1000 | Loss: 0.00003279
Iteration 430/1000 | Loss: 0.00003278
Iteration 431/1000 | Loss: 0.00003278
Iteration 432/1000 | Loss: 0.00003278
Iteration 433/1000 | Loss: 0.00003278
Iteration 434/1000 | Loss: 0.00003277
Iteration 435/1000 | Loss: 0.00003277
Iteration 436/1000 | Loss: 0.00003276
Iteration 437/1000 | Loss: 0.00003276
Iteration 438/1000 | Loss: 0.00003275
Iteration 439/1000 | Loss: 0.00003275
Iteration 440/1000 | Loss: 0.00003275
Iteration 441/1000 | Loss: 0.00003274
Iteration 442/1000 | Loss: 0.00003274
Iteration 443/1000 | Loss: 0.00003273
Iteration 444/1000 | Loss: 0.00003273
Iteration 445/1000 | Loss: 0.00003263
Iteration 446/1000 | Loss: 0.00003260
Iteration 447/1000 | Loss: 0.00003260
Iteration 448/1000 | Loss: 0.00003260
Iteration 449/1000 | Loss: 0.00003260
Iteration 450/1000 | Loss: 0.00003259
Iteration 451/1000 | Loss: 0.00003259
Iteration 452/1000 | Loss: 0.00003259
Iteration 453/1000 | Loss: 0.00003259
Iteration 454/1000 | Loss: 0.00003257
Iteration 455/1000 | Loss: 0.00003257
Iteration 456/1000 | Loss: 0.00003254
Iteration 457/1000 | Loss: 0.00003246
Iteration 458/1000 | Loss: 0.00003238
Iteration 459/1000 | Loss: 0.00003237
Iteration 460/1000 | Loss: 0.00003229
Iteration 461/1000 | Loss: 0.00003223
Iteration 462/1000 | Loss: 0.00003221
Iteration 463/1000 | Loss: 0.00003220
Iteration 464/1000 | Loss: 0.00003216
Iteration 465/1000 | Loss: 0.00003213
Iteration 466/1000 | Loss: 0.00003212
Iteration 467/1000 | Loss: 0.00003212
Iteration 468/1000 | Loss: 0.00003212
Iteration 469/1000 | Loss: 0.00003209
Iteration 470/1000 | Loss: 0.00003208
Iteration 471/1000 | Loss: 0.00003208
Iteration 472/1000 | Loss: 0.00003207
Iteration 473/1000 | Loss: 0.00003206
Iteration 474/1000 | Loss: 0.00003206
Iteration 475/1000 | Loss: 0.00003205
Iteration 476/1000 | Loss: 0.00003205
Iteration 477/1000 | Loss: 0.00003204
Iteration 478/1000 | Loss: 0.00003204
Iteration 479/1000 | Loss: 0.00003204
Iteration 480/1000 | Loss: 0.00003204
Iteration 481/1000 | Loss: 0.00003203
Iteration 482/1000 | Loss: 0.00003203
Iteration 483/1000 | Loss: 0.00003203
Iteration 484/1000 | Loss: 0.00003203
Iteration 485/1000 | Loss: 0.00003202
Iteration 486/1000 | Loss: 0.00003202
Iteration 487/1000 | Loss: 0.00003202
Iteration 488/1000 | Loss: 0.00003202
Iteration 489/1000 | Loss: 0.00003202
Iteration 490/1000 | Loss: 0.00003201
Iteration 491/1000 | Loss: 0.00003201
Iteration 492/1000 | Loss: 0.00003201
Iteration 493/1000 | Loss: 0.00003201
Iteration 494/1000 | Loss: 0.00003201
Iteration 495/1000 | Loss: 0.00003201
Iteration 496/1000 | Loss: 0.00003201
Iteration 497/1000 | Loss: 0.00003201
Iteration 498/1000 | Loss: 0.00003201
Iteration 499/1000 | Loss: 0.00003200
Iteration 500/1000 | Loss: 0.00003200
Iteration 501/1000 | Loss: 0.00003200
Iteration 502/1000 | Loss: 0.00003200
Iteration 503/1000 | Loss: 0.00003200
Iteration 504/1000 | Loss: 0.00003200
Iteration 505/1000 | Loss: 0.00003200
Iteration 506/1000 | Loss: 0.00003199
Iteration 507/1000 | Loss: 0.00003199
Iteration 508/1000 | Loss: 0.00003199
Iteration 509/1000 | Loss: 0.00003199
Iteration 510/1000 | Loss: 0.00003199
Iteration 511/1000 | Loss: 0.00003199
Iteration 512/1000 | Loss: 0.00003199
Iteration 513/1000 | Loss: 0.00003198
Iteration 514/1000 | Loss: 0.00003198
Iteration 515/1000 | Loss: 0.00003198
Iteration 516/1000 | Loss: 0.00003198
Iteration 517/1000 | Loss: 0.00003198
Iteration 518/1000 | Loss: 0.00003198
Iteration 519/1000 | Loss: 0.00003198
Iteration 520/1000 | Loss: 0.00003197
Iteration 521/1000 | Loss: 0.00003197
Iteration 522/1000 | Loss: 0.00003197
Iteration 523/1000 | Loss: 0.00003197
Iteration 524/1000 | Loss: 0.00003197
Iteration 525/1000 | Loss: 0.00003197
Iteration 526/1000 | Loss: 0.00003197
Iteration 527/1000 | Loss: 0.00003197
Iteration 528/1000 | Loss: 0.00003197
Iteration 529/1000 | Loss: 0.00003197
Iteration 530/1000 | Loss: 0.00003197
Iteration 531/1000 | Loss: 0.00003197
Iteration 532/1000 | Loss: 0.00003197
Iteration 533/1000 | Loss: 0.00003196
Iteration 534/1000 | Loss: 0.00003196
Iteration 535/1000 | Loss: 0.00003196
Iteration 536/1000 | Loss: 0.00003196
Iteration 537/1000 | Loss: 0.00003196
Iteration 538/1000 | Loss: 0.00003196
Iteration 539/1000 | Loss: 0.00003196
Iteration 540/1000 | Loss: 0.00003196
Iteration 541/1000 | Loss: 0.00003196
Iteration 542/1000 | Loss: 0.00003196
Iteration 543/1000 | Loss: 0.00003196
Iteration 544/1000 | Loss: 0.00003196
Iteration 545/1000 | Loss: 0.00003196
Iteration 546/1000 | Loss: 0.00003196
Iteration 547/1000 | Loss: 0.00003195
Iteration 548/1000 | Loss: 0.00003195
Iteration 549/1000 | Loss: 0.00003195
Iteration 550/1000 | Loss: 0.00003195
Iteration 551/1000 | Loss: 0.00003195
Iteration 552/1000 | Loss: 0.00003195
Iteration 553/1000 | Loss: 0.00003195
Iteration 554/1000 | Loss: 0.00003195
Iteration 555/1000 | Loss: 0.00003195
Iteration 556/1000 | Loss: 0.00003195
Iteration 557/1000 | Loss: 0.00003195
Iteration 558/1000 | Loss: 0.00003195
Iteration 559/1000 | Loss: 0.00003195
Iteration 560/1000 | Loss: 0.00003195
Iteration 561/1000 | Loss: 0.00003195
Iteration 562/1000 | Loss: 0.00003195
Iteration 563/1000 | Loss: 0.00003195
Iteration 564/1000 | Loss: 0.00003195
Iteration 565/1000 | Loss: 0.00003195
Iteration 566/1000 | Loss: 0.00003195
Iteration 567/1000 | Loss: 0.00003195
Iteration 568/1000 | Loss: 0.00003195
Iteration 569/1000 | Loss: 0.00003195
Iteration 570/1000 | Loss: 0.00003195
Iteration 571/1000 | Loss: 0.00003195
Iteration 572/1000 | Loss: 0.00003195
Iteration 573/1000 | Loss: 0.00003194
Iteration 574/1000 | Loss: 0.00003194
Iteration 575/1000 | Loss: 0.00003194
Iteration 576/1000 | Loss: 0.00003194
Iteration 577/1000 | Loss: 0.00003194
Iteration 578/1000 | Loss: 0.00003194
Iteration 579/1000 | Loss: 0.00003194
Iteration 580/1000 | Loss: 0.00003194
Iteration 581/1000 | Loss: 0.00003194
Iteration 582/1000 | Loss: 0.00003194
Iteration 583/1000 | Loss: 0.00003194
Iteration 584/1000 | Loss: 0.00003194
Iteration 585/1000 | Loss: 0.00003194
Iteration 586/1000 | Loss: 0.00003194
Iteration 587/1000 | Loss: 0.00003194
Iteration 588/1000 | Loss: 0.00003194
Iteration 589/1000 | Loss: 0.00003194
Iteration 590/1000 | Loss: 0.00003194
Iteration 591/1000 | Loss: 0.00003194
Iteration 592/1000 | Loss: 0.00003194
Iteration 593/1000 | Loss: 0.00003194
Iteration 594/1000 | Loss: 0.00003193
Iteration 595/1000 | Loss: 0.00003193
Iteration 596/1000 | Loss: 0.00003193
Iteration 597/1000 | Loss: 0.00003193
Iteration 598/1000 | Loss: 0.00003193
Iteration 599/1000 | Loss: 0.00003193
Iteration 600/1000 | Loss: 0.00003193
Iteration 601/1000 | Loss: 0.00003193
Iteration 602/1000 | Loss: 0.00003193
Iteration 603/1000 | Loss: 0.00003193
Iteration 604/1000 | Loss: 0.00003193
Iteration 605/1000 | Loss: 0.00003193
Iteration 606/1000 | Loss: 0.00003192
Iteration 607/1000 | Loss: 0.00003192
Iteration 608/1000 | Loss: 0.00003192
Iteration 609/1000 | Loss: 0.00003192
Iteration 610/1000 | Loss: 0.00003192
Iteration 611/1000 | Loss: 0.00003192
Iteration 612/1000 | Loss: 0.00003192
Iteration 613/1000 | Loss: 0.00003192
Iteration 614/1000 | Loss: 0.00003192
Iteration 615/1000 | Loss: 0.00003192
Iteration 616/1000 | Loss: 0.00003192
Iteration 617/1000 | Loss: 0.00003192
Iteration 618/1000 | Loss: 0.00003192
Iteration 619/1000 | Loss: 0.00003192
Iteration 620/1000 | Loss: 0.00003192
Iteration 621/1000 | Loss: 0.00003192
Iteration 622/1000 | Loss: 0.00003192
Iteration 623/1000 | Loss: 0.00003192
Iteration 624/1000 | Loss: 0.00003192
Iteration 625/1000 | Loss: 0.00003192
Iteration 626/1000 | Loss: 0.00003192
Iteration 627/1000 | Loss: 0.00003192
Iteration 628/1000 | Loss: 0.00003192
Iteration 629/1000 | Loss: 0.00003192
Iteration 630/1000 | Loss: 0.00003192
Iteration 631/1000 | Loss: 0.00003192
Iteration 632/1000 | Loss: 0.00003192
Iteration 633/1000 | Loss: 0.00003192
Iteration 634/1000 | Loss: 0.00003192
Iteration 635/1000 | Loss: 0.00003192
Iteration 636/1000 | Loss: 0.00003192
Iteration 637/1000 | Loss: 0.00003192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 637. Stopping optimization.
Last 5 losses: [3.19153223244939e-05, 3.19153223244939e-05, 3.19153223244939e-05, 3.19153223244939e-05, 3.19153223244939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.19153223244939e-05

Optimization complete. Final v2v error: 4.6424946784973145 mm

Highest mean error: 5.55023193359375 mm for frame 61

Lowest mean error: 3.726863384246826 mm for frame 4

Saving results

Total time: 351.03345489501953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039849
Iteration 2/25 | Loss: 0.01039849
Iteration 3/25 | Loss: 0.01039848
Iteration 4/25 | Loss: 0.01039848
Iteration 5/25 | Loss: 0.01039848
Iteration 6/25 | Loss: 0.01039848
Iteration 7/25 | Loss: 0.00428532
Iteration 8/25 | Loss: 0.00320037
Iteration 9/25 | Loss: 0.00216976
Iteration 10/25 | Loss: 0.00192837
Iteration 11/25 | Loss: 0.00186701
Iteration 12/25 | Loss: 0.00193821
Iteration 13/25 | Loss: 0.00188792
Iteration 14/25 | Loss: 0.00177087
Iteration 15/25 | Loss: 0.00169420
Iteration 16/25 | Loss: 0.00164618
Iteration 17/25 | Loss: 0.00153792
Iteration 18/25 | Loss: 0.00145976
Iteration 19/25 | Loss: 0.00141487
Iteration 20/25 | Loss: 0.00139542
Iteration 21/25 | Loss: 0.00134461
Iteration 22/25 | Loss: 0.00132223
Iteration 23/25 | Loss: 0.00130500
Iteration 24/25 | Loss: 0.00127035
Iteration 25/25 | Loss: 0.00128589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47086298
Iteration 2/25 | Loss: 0.00448028
Iteration 3/25 | Loss: 0.00341437
Iteration 4/25 | Loss: 0.00341207
Iteration 5/25 | Loss: 0.00341207
Iteration 6/25 | Loss: 0.00341207
Iteration 7/25 | Loss: 0.00341207
Iteration 8/25 | Loss: 0.00341207
Iteration 9/25 | Loss: 0.00341207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0034120725467801094, 0.0034120725467801094, 0.0034120725467801094, 0.0034120725467801094, 0.0034120725467801094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034120725467801094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00341207
Iteration 2/1000 | Loss: 0.00732844
Iteration 3/1000 | Loss: 0.00144870
Iteration 4/1000 | Loss: 0.00264627
Iteration 5/1000 | Loss: 0.00498225
Iteration 6/1000 | Loss: 0.00440460
Iteration 7/1000 | Loss: 0.00126881
Iteration 8/1000 | Loss: 0.00092919
Iteration 9/1000 | Loss: 0.00083818
Iteration 10/1000 | Loss: 0.00117366
Iteration 11/1000 | Loss: 0.00191035
Iteration 12/1000 | Loss: 0.00115464
Iteration 13/1000 | Loss: 0.00059159
Iteration 14/1000 | Loss: 0.00051444
Iteration 15/1000 | Loss: 0.00132488
Iteration 16/1000 | Loss: 0.00032008
Iteration 17/1000 | Loss: 0.00024316
Iteration 18/1000 | Loss: 0.00060522
Iteration 19/1000 | Loss: 0.00034136
Iteration 20/1000 | Loss: 0.00079623
Iteration 21/1000 | Loss: 0.00115820
Iteration 22/1000 | Loss: 0.00100484
Iteration 23/1000 | Loss: 0.00051302
Iteration 24/1000 | Loss: 0.00080688
Iteration 25/1000 | Loss: 0.00030443
Iteration 26/1000 | Loss: 0.00083080
Iteration 27/1000 | Loss: 0.00224217
Iteration 28/1000 | Loss: 0.00050716
Iteration 29/1000 | Loss: 0.00049245
Iteration 30/1000 | Loss: 0.00033682
Iteration 31/1000 | Loss: 0.00071817
Iteration 32/1000 | Loss: 0.00056103
Iteration 33/1000 | Loss: 0.00078573
Iteration 34/1000 | Loss: 0.00184074
Iteration 35/1000 | Loss: 0.00142671
Iteration 36/1000 | Loss: 0.00227809
Iteration 37/1000 | Loss: 0.00026278
Iteration 38/1000 | Loss: 0.00034750
Iteration 39/1000 | Loss: 0.00023649
Iteration 40/1000 | Loss: 0.00035549
Iteration 41/1000 | Loss: 0.00009909
Iteration 42/1000 | Loss: 0.00037666
Iteration 43/1000 | Loss: 0.00100666
Iteration 44/1000 | Loss: 0.00051426
Iteration 45/1000 | Loss: 0.00053312
Iteration 46/1000 | Loss: 0.00035067
Iteration 47/1000 | Loss: 0.00006747
Iteration 48/1000 | Loss: 0.00006932
Iteration 49/1000 | Loss: 0.00012618
Iteration 50/1000 | Loss: 0.00006764
Iteration 51/1000 | Loss: 0.00029177
Iteration 52/1000 | Loss: 0.00020833
Iteration 53/1000 | Loss: 0.00009432
Iteration 54/1000 | Loss: 0.00006486
Iteration 55/1000 | Loss: 0.00005681
Iteration 56/1000 | Loss: 0.00047804
Iteration 57/1000 | Loss: 0.00020443
Iteration 58/1000 | Loss: 0.00029086
Iteration 59/1000 | Loss: 0.00036142
Iteration 60/1000 | Loss: 0.00010470
Iteration 61/1000 | Loss: 0.00006175
Iteration 62/1000 | Loss: 0.00006924
Iteration 63/1000 | Loss: 0.00006356
Iteration 64/1000 | Loss: 0.00033028
Iteration 65/1000 | Loss: 0.00011502
Iteration 66/1000 | Loss: 0.00009632
Iteration 67/1000 | Loss: 0.00005385
Iteration 68/1000 | Loss: 0.00005188
Iteration 69/1000 | Loss: 0.00007978
Iteration 70/1000 | Loss: 0.00020852
Iteration 71/1000 | Loss: 0.00017601
Iteration 72/1000 | Loss: 0.00036300
Iteration 73/1000 | Loss: 0.00004678
Iteration 74/1000 | Loss: 0.00043780
Iteration 75/1000 | Loss: 0.00029660
Iteration 76/1000 | Loss: 0.00013381
Iteration 77/1000 | Loss: 0.00006473
Iteration 78/1000 | Loss: 0.00020068
Iteration 79/1000 | Loss: 0.00015954
Iteration 80/1000 | Loss: 0.00004172
Iteration 81/1000 | Loss: 0.00046150
Iteration 82/1000 | Loss: 0.00016681
Iteration 83/1000 | Loss: 0.00020832
Iteration 84/1000 | Loss: 0.00007041
Iteration 85/1000 | Loss: 0.00030413
Iteration 86/1000 | Loss: 0.00022183
Iteration 87/1000 | Loss: 0.00051604
Iteration 88/1000 | Loss: 0.00030160
Iteration 89/1000 | Loss: 0.00005362
Iteration 90/1000 | Loss: 0.00006586
Iteration 91/1000 | Loss: 0.00016047
Iteration 92/1000 | Loss: 0.00016866
Iteration 93/1000 | Loss: 0.00009115
Iteration 94/1000 | Loss: 0.00006951
Iteration 95/1000 | Loss: 0.00019855
Iteration 96/1000 | Loss: 0.00017180
Iteration 97/1000 | Loss: 0.00020714
Iteration 98/1000 | Loss: 0.00017046
Iteration 99/1000 | Loss: 0.00007663
Iteration 100/1000 | Loss: 0.00016100
Iteration 101/1000 | Loss: 0.00015560
Iteration 102/1000 | Loss: 0.00005130
Iteration 103/1000 | Loss: 0.00005902
Iteration 104/1000 | Loss: 0.00004716
Iteration 105/1000 | Loss: 0.00005880
Iteration 106/1000 | Loss: 0.00004413
Iteration 107/1000 | Loss: 0.00004379
Iteration 108/1000 | Loss: 0.00005155
Iteration 109/1000 | Loss: 0.00003081
Iteration 110/1000 | Loss: 0.00003646
Iteration 111/1000 | Loss: 0.00002833
Iteration 112/1000 | Loss: 0.00008998
Iteration 113/1000 | Loss: 0.00005162
Iteration 114/1000 | Loss: 0.00002490
Iteration 115/1000 | Loss: 0.00004746
Iteration 116/1000 | Loss: 0.00004468
Iteration 117/1000 | Loss: 0.00033529
Iteration 118/1000 | Loss: 0.00007416
Iteration 119/1000 | Loss: 0.00006289
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002861
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002262
Iteration 124/1000 | Loss: 0.00002262
Iteration 125/1000 | Loss: 0.00002262
Iteration 126/1000 | Loss: 0.00002262
Iteration 127/1000 | Loss: 0.00002262
Iteration 128/1000 | Loss: 0.00002262
Iteration 129/1000 | Loss: 0.00002262
Iteration 130/1000 | Loss: 0.00004970
Iteration 131/1000 | Loss: 0.00002343
Iteration 132/1000 | Loss: 0.00002351
Iteration 133/1000 | Loss: 0.00002278
Iteration 134/1000 | Loss: 0.00003544
Iteration 135/1000 | Loss: 0.00003544
Iteration 136/1000 | Loss: 0.00003544
Iteration 137/1000 | Loss: 0.00005645
Iteration 138/1000 | Loss: 0.00015626
Iteration 139/1000 | Loss: 0.00013170
Iteration 140/1000 | Loss: 0.00003173
Iteration 141/1000 | Loss: 0.00004925
Iteration 142/1000 | Loss: 0.00002733
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00003214
Iteration 145/1000 | Loss: 0.00002219
Iteration 146/1000 | Loss: 0.00002218
Iteration 147/1000 | Loss: 0.00002218
Iteration 148/1000 | Loss: 0.00002218
Iteration 149/1000 | Loss: 0.00002218
Iteration 150/1000 | Loss: 0.00002218
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00002218
Iteration 153/1000 | Loss: 0.00002218
Iteration 154/1000 | Loss: 0.00002218
Iteration 155/1000 | Loss: 0.00002217
Iteration 156/1000 | Loss: 0.00002217
Iteration 157/1000 | Loss: 0.00002217
Iteration 158/1000 | Loss: 0.00002216
Iteration 159/1000 | Loss: 0.00002216
Iteration 160/1000 | Loss: 0.00002233
Iteration 161/1000 | Loss: 0.00002233
Iteration 162/1000 | Loss: 0.00002683
Iteration 163/1000 | Loss: 0.00010115
Iteration 164/1000 | Loss: 0.00003183
Iteration 165/1000 | Loss: 0.00002277
Iteration 166/1000 | Loss: 0.00002584
Iteration 167/1000 | Loss: 0.00002999
Iteration 168/1000 | Loss: 0.00002223
Iteration 169/1000 | Loss: 0.00002223
Iteration 170/1000 | Loss: 0.00002223
Iteration 171/1000 | Loss: 0.00002209
Iteration 172/1000 | Loss: 0.00002209
Iteration 173/1000 | Loss: 0.00002209
Iteration 174/1000 | Loss: 0.00002208
Iteration 175/1000 | Loss: 0.00002208
Iteration 176/1000 | Loss: 0.00002208
Iteration 177/1000 | Loss: 0.00002208
Iteration 178/1000 | Loss: 0.00002208
Iteration 179/1000 | Loss: 0.00002208
Iteration 180/1000 | Loss: 0.00002208
Iteration 181/1000 | Loss: 0.00002208
Iteration 182/1000 | Loss: 0.00002208
Iteration 183/1000 | Loss: 0.00002208
Iteration 184/1000 | Loss: 0.00002208
Iteration 185/1000 | Loss: 0.00002208
Iteration 186/1000 | Loss: 0.00002208
Iteration 187/1000 | Loss: 0.00002208
Iteration 188/1000 | Loss: 0.00002208
Iteration 189/1000 | Loss: 0.00002208
Iteration 190/1000 | Loss: 0.00002208
Iteration 191/1000 | Loss: 0.00002208
Iteration 192/1000 | Loss: 0.00002208
Iteration 193/1000 | Loss: 0.00002208
Iteration 194/1000 | Loss: 0.00002208
Iteration 195/1000 | Loss: 0.00002208
Iteration 196/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.2081749193603173e-05, 2.2081749193603173e-05, 2.2081749193603173e-05, 2.2081749193603173e-05, 2.2081749193603173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2081749193603173e-05

Optimization complete. Final v2v error: 3.935760736465454 mm

Highest mean error: 6.233066558837891 mm for frame 99

Lowest mean error: 3.5602242946624756 mm for frame 30

Saving results

Total time: 263.36431074142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828543
Iteration 2/25 | Loss: 0.00131989
Iteration 3/25 | Loss: 0.00101847
Iteration 4/25 | Loss: 0.00095344
Iteration 5/25 | Loss: 0.00093408
Iteration 6/25 | Loss: 0.00093295
Iteration 7/25 | Loss: 0.00092596
Iteration 8/25 | Loss: 0.00092221
Iteration 9/25 | Loss: 0.00091496
Iteration 10/25 | Loss: 0.00091311
Iteration 11/25 | Loss: 0.00091135
Iteration 12/25 | Loss: 0.00091072
Iteration 13/25 | Loss: 0.00091028
Iteration 14/25 | Loss: 0.00090922
Iteration 15/25 | Loss: 0.00091977
Iteration 16/25 | Loss: 0.00091187
Iteration 17/25 | Loss: 0.00090645
Iteration 18/25 | Loss: 0.00090697
Iteration 19/25 | Loss: 0.00090604
Iteration 20/25 | Loss: 0.00090334
Iteration 21/25 | Loss: 0.00090661
Iteration 22/25 | Loss: 0.00090661
Iteration 23/25 | Loss: 0.00090484
Iteration 24/25 | Loss: 0.00090372
Iteration 25/25 | Loss: 0.00090387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59096336
Iteration 2/25 | Loss: 0.00075542
Iteration 3/25 | Loss: 0.00075541
Iteration 4/25 | Loss: 0.00075541
Iteration 5/25 | Loss: 0.00075541
Iteration 6/25 | Loss: 0.00075541
Iteration 7/25 | Loss: 0.00075541
Iteration 8/25 | Loss: 0.00075541
Iteration 9/25 | Loss: 0.00075541
Iteration 10/25 | Loss: 0.00075541
Iteration 11/25 | Loss: 0.00075541
Iteration 12/25 | Loss: 0.00075541
Iteration 13/25 | Loss: 0.00075541
Iteration 14/25 | Loss: 0.00075541
Iteration 15/25 | Loss: 0.00075541
Iteration 16/25 | Loss: 0.00075541
Iteration 17/25 | Loss: 0.00075541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007554052863270044, 0.0007554052863270044, 0.0007554052863270044, 0.0007554052863270044, 0.0007554052863270044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007554052863270044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075541
Iteration 2/1000 | Loss: 0.00007923
Iteration 3/1000 | Loss: 0.00006126
Iteration 4/1000 | Loss: 0.00005433
Iteration 5/1000 | Loss: 0.00005020
Iteration 6/1000 | Loss: 0.00004702
Iteration 7/1000 | Loss: 0.00004449
Iteration 8/1000 | Loss: 0.00004217
Iteration 9/1000 | Loss: 0.00004022
Iteration 10/1000 | Loss: 0.00003874
Iteration 11/1000 | Loss: 0.00048592
Iteration 12/1000 | Loss: 0.00173770
Iteration 13/1000 | Loss: 0.00041046
Iteration 14/1000 | Loss: 0.00006271
Iteration 15/1000 | Loss: 0.00004548
Iteration 16/1000 | Loss: 0.00003788
Iteration 17/1000 | Loss: 0.00003263
Iteration 18/1000 | Loss: 0.00002977
Iteration 19/1000 | Loss: 0.00002780
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002608
Iteration 22/1000 | Loss: 0.00002566
Iteration 23/1000 | Loss: 0.00002537
Iteration 24/1000 | Loss: 0.00002519
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002502
Iteration 27/1000 | Loss: 0.00002489
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002478
Iteration 32/1000 | Loss: 0.00002477
Iteration 33/1000 | Loss: 0.00002477
Iteration 34/1000 | Loss: 0.00002476
Iteration 35/1000 | Loss: 0.00002476
Iteration 36/1000 | Loss: 0.00002476
Iteration 37/1000 | Loss: 0.00002476
Iteration 38/1000 | Loss: 0.00002475
Iteration 39/1000 | Loss: 0.00002475
Iteration 40/1000 | Loss: 0.00002474
Iteration 41/1000 | Loss: 0.00002474
Iteration 42/1000 | Loss: 0.00002474
Iteration 43/1000 | Loss: 0.00002474
Iteration 44/1000 | Loss: 0.00002474
Iteration 45/1000 | Loss: 0.00002473
Iteration 46/1000 | Loss: 0.00002473
Iteration 47/1000 | Loss: 0.00002473
Iteration 48/1000 | Loss: 0.00002472
Iteration 49/1000 | Loss: 0.00002472
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002471
Iteration 53/1000 | Loss: 0.00002471
Iteration 54/1000 | Loss: 0.00002471
Iteration 55/1000 | Loss: 0.00002471
Iteration 56/1000 | Loss: 0.00002470
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00002469
Iteration 59/1000 | Loss: 0.00002469
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002469
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002469
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002469
Iteration 69/1000 | Loss: 0.00002469
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002467
Iteration 74/1000 | Loss: 0.00002467
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002466
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002466
Iteration 86/1000 | Loss: 0.00002466
Iteration 87/1000 | Loss: 0.00002466
Iteration 88/1000 | Loss: 0.00002465
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002465
Iteration 93/1000 | Loss: 0.00002465
Iteration 94/1000 | Loss: 0.00002465
Iteration 95/1000 | Loss: 0.00002465
Iteration 96/1000 | Loss: 0.00002465
Iteration 97/1000 | Loss: 0.00002465
Iteration 98/1000 | Loss: 0.00002465
Iteration 99/1000 | Loss: 0.00002465
Iteration 100/1000 | Loss: 0.00002465
Iteration 101/1000 | Loss: 0.00002464
Iteration 102/1000 | Loss: 0.00002464
Iteration 103/1000 | Loss: 0.00002464
Iteration 104/1000 | Loss: 0.00002464
Iteration 105/1000 | Loss: 0.00002464
Iteration 106/1000 | Loss: 0.00002464
Iteration 107/1000 | Loss: 0.00002464
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002464
Iteration 110/1000 | Loss: 0.00002464
Iteration 111/1000 | Loss: 0.00002464
Iteration 112/1000 | Loss: 0.00002464
Iteration 113/1000 | Loss: 0.00002464
Iteration 114/1000 | Loss: 0.00002464
Iteration 115/1000 | Loss: 0.00002464
Iteration 116/1000 | Loss: 0.00002464
Iteration 117/1000 | Loss: 0.00002464
Iteration 118/1000 | Loss: 0.00002464
Iteration 119/1000 | Loss: 0.00002464
Iteration 120/1000 | Loss: 0.00002464
Iteration 121/1000 | Loss: 0.00002464
Iteration 122/1000 | Loss: 0.00002463
Iteration 123/1000 | Loss: 0.00002463
Iteration 124/1000 | Loss: 0.00002463
Iteration 125/1000 | Loss: 0.00002463
Iteration 126/1000 | Loss: 0.00002463
Iteration 127/1000 | Loss: 0.00002463
Iteration 128/1000 | Loss: 0.00002463
Iteration 129/1000 | Loss: 0.00002463
Iteration 130/1000 | Loss: 0.00002463
Iteration 131/1000 | Loss: 0.00002463
Iteration 132/1000 | Loss: 0.00002463
Iteration 133/1000 | Loss: 0.00002463
Iteration 134/1000 | Loss: 0.00002463
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002463
Iteration 137/1000 | Loss: 0.00002462
Iteration 138/1000 | Loss: 0.00002462
Iteration 139/1000 | Loss: 0.00002462
Iteration 140/1000 | Loss: 0.00002462
Iteration 141/1000 | Loss: 0.00002462
Iteration 142/1000 | Loss: 0.00002462
Iteration 143/1000 | Loss: 0.00002462
Iteration 144/1000 | Loss: 0.00002462
Iteration 145/1000 | Loss: 0.00002462
Iteration 146/1000 | Loss: 0.00002462
Iteration 147/1000 | Loss: 0.00002462
Iteration 148/1000 | Loss: 0.00002462
Iteration 149/1000 | Loss: 0.00002461
Iteration 150/1000 | Loss: 0.00002461
Iteration 151/1000 | Loss: 0.00002461
Iteration 152/1000 | Loss: 0.00002461
Iteration 153/1000 | Loss: 0.00002461
Iteration 154/1000 | Loss: 0.00002460
Iteration 155/1000 | Loss: 0.00002460
Iteration 156/1000 | Loss: 0.00002460
Iteration 157/1000 | Loss: 0.00002460
Iteration 158/1000 | Loss: 0.00002459
Iteration 159/1000 | Loss: 0.00002459
Iteration 160/1000 | Loss: 0.00002459
Iteration 161/1000 | Loss: 0.00002459
Iteration 162/1000 | Loss: 0.00002459
Iteration 163/1000 | Loss: 0.00002459
Iteration 164/1000 | Loss: 0.00002459
Iteration 165/1000 | Loss: 0.00002459
Iteration 166/1000 | Loss: 0.00002459
Iteration 167/1000 | Loss: 0.00002459
Iteration 168/1000 | Loss: 0.00002459
Iteration 169/1000 | Loss: 0.00002458
Iteration 170/1000 | Loss: 0.00002458
Iteration 171/1000 | Loss: 0.00002458
Iteration 172/1000 | Loss: 0.00002458
Iteration 173/1000 | Loss: 0.00002458
Iteration 174/1000 | Loss: 0.00002458
Iteration 175/1000 | Loss: 0.00002458
Iteration 176/1000 | Loss: 0.00002458
Iteration 177/1000 | Loss: 0.00002458
Iteration 178/1000 | Loss: 0.00002458
Iteration 179/1000 | Loss: 0.00002458
Iteration 180/1000 | Loss: 0.00002458
Iteration 181/1000 | Loss: 0.00002458
Iteration 182/1000 | Loss: 0.00002458
Iteration 183/1000 | Loss: 0.00002458
Iteration 184/1000 | Loss: 0.00002458
Iteration 185/1000 | Loss: 0.00002457
Iteration 186/1000 | Loss: 0.00002457
Iteration 187/1000 | Loss: 0.00002457
Iteration 188/1000 | Loss: 0.00002457
Iteration 189/1000 | Loss: 0.00002457
Iteration 190/1000 | Loss: 0.00002457
Iteration 191/1000 | Loss: 0.00002457
Iteration 192/1000 | Loss: 0.00002457
Iteration 193/1000 | Loss: 0.00002457
Iteration 194/1000 | Loss: 0.00002457
Iteration 195/1000 | Loss: 0.00002457
Iteration 196/1000 | Loss: 0.00002457
Iteration 197/1000 | Loss: 0.00002457
Iteration 198/1000 | Loss: 0.00002457
Iteration 199/1000 | Loss: 0.00002457
Iteration 200/1000 | Loss: 0.00002457
Iteration 201/1000 | Loss: 0.00002457
Iteration 202/1000 | Loss: 0.00002457
Iteration 203/1000 | Loss: 0.00002457
Iteration 204/1000 | Loss: 0.00002457
Iteration 205/1000 | Loss: 0.00002457
Iteration 206/1000 | Loss: 0.00002457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.457015580148436e-05, 2.457015580148436e-05, 2.457015580148436e-05, 2.457015580148436e-05, 2.457015580148436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.457015580148436e-05

Optimization complete. Final v2v error: 4.139520645141602 mm

Highest mean error: 4.77168083190918 mm for frame 67

Lowest mean error: 3.536832332611084 mm for frame 178

Saving results

Total time: 103.58092522621155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068988
Iteration 2/25 | Loss: 0.00354116
Iteration 3/25 | Loss: 0.00231587
Iteration 4/25 | Loss: 0.00205335
Iteration 5/25 | Loss: 0.00189943
Iteration 6/25 | Loss: 0.00188589
Iteration 7/25 | Loss: 0.00186336
Iteration 8/25 | Loss: 0.00181190
Iteration 9/25 | Loss: 0.00177601
Iteration 10/25 | Loss: 0.00174817
Iteration 11/25 | Loss: 0.00173179
Iteration 12/25 | Loss: 0.00171563
Iteration 13/25 | Loss: 0.00170874
Iteration 14/25 | Loss: 0.00171104
Iteration 15/25 | Loss: 0.00170055
Iteration 16/25 | Loss: 0.00169610
Iteration 17/25 | Loss: 0.00169621
Iteration 18/25 | Loss: 0.00169823
Iteration 19/25 | Loss: 0.00169288
Iteration 20/25 | Loss: 0.00169065
Iteration 21/25 | Loss: 0.00168623
Iteration 22/25 | Loss: 0.00168344
Iteration 23/25 | Loss: 0.00168288
Iteration 24/25 | Loss: 0.00168229
Iteration 25/25 | Loss: 0.00168008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.58687878
Iteration 2/25 | Loss: 0.01459233
Iteration 3/25 | Loss: 0.00804721
Iteration 4/25 | Loss: 0.00804721
Iteration 5/25 | Loss: 0.00804721
Iteration 6/25 | Loss: 0.00804721
Iteration 7/25 | Loss: 0.00804721
Iteration 8/25 | Loss: 0.00804721
Iteration 9/25 | Loss: 0.00804721
Iteration 10/25 | Loss: 0.00804721
Iteration 11/25 | Loss: 0.00804721
Iteration 12/25 | Loss: 0.00804721
Iteration 13/25 | Loss: 0.00804721
Iteration 14/25 | Loss: 0.00804721
Iteration 15/25 | Loss: 0.00804721
Iteration 16/25 | Loss: 0.00804721
Iteration 17/25 | Loss: 0.00804721
Iteration 18/25 | Loss: 0.00804721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.008047206327319145, 0.008047206327319145, 0.008047206327319145, 0.008047206327319145, 0.008047206327319145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008047206327319145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00804721
Iteration 2/1000 | Loss: 0.00502190
Iteration 3/1000 | Loss: 0.00169342
Iteration 4/1000 | Loss: 0.00385992
Iteration 5/1000 | Loss: 0.00045720
Iteration 6/1000 | Loss: 0.00040962
Iteration 7/1000 | Loss: 0.00417449
Iteration 8/1000 | Loss: 0.00678575
Iteration 9/1000 | Loss: 0.00789115
Iteration 10/1000 | Loss: 0.00247002
Iteration 11/1000 | Loss: 0.00047094
Iteration 12/1000 | Loss: 0.00093746
Iteration 13/1000 | Loss: 0.00149489
Iteration 14/1000 | Loss: 0.00162458
Iteration 15/1000 | Loss: 0.00503052
Iteration 16/1000 | Loss: 0.00071196
Iteration 17/1000 | Loss: 0.00169684
Iteration 18/1000 | Loss: 0.00085333
Iteration 19/1000 | Loss: 0.00099233
Iteration 20/1000 | Loss: 0.00323654
Iteration 21/1000 | Loss: 0.00036586
Iteration 22/1000 | Loss: 0.00010912
Iteration 23/1000 | Loss: 0.00006650
Iteration 24/1000 | Loss: 0.00128138
Iteration 25/1000 | Loss: 0.00058455
Iteration 26/1000 | Loss: 0.00004948
Iteration 27/1000 | Loss: 0.00089092
Iteration 28/1000 | Loss: 0.00006601
Iteration 29/1000 | Loss: 0.00004327
Iteration 30/1000 | Loss: 0.00005485
Iteration 31/1000 | Loss: 0.00003346
Iteration 32/1000 | Loss: 0.00004738
Iteration 33/1000 | Loss: 0.00003254
Iteration 34/1000 | Loss: 0.00003608
Iteration 35/1000 | Loss: 0.00004583
Iteration 36/1000 | Loss: 0.00004208
Iteration 37/1000 | Loss: 0.00004092
Iteration 38/1000 | Loss: 0.00003193
Iteration 39/1000 | Loss: 0.00011944
Iteration 40/1000 | Loss: 0.00004413
Iteration 41/1000 | Loss: 0.00024338
Iteration 42/1000 | Loss: 0.00003899
Iteration 43/1000 | Loss: 0.00017032
Iteration 44/1000 | Loss: 0.00004286
Iteration 45/1000 | Loss: 0.00004489
Iteration 46/1000 | Loss: 0.00003733
Iteration 47/1000 | Loss: 0.00004701
Iteration 48/1000 | Loss: 0.00003746
Iteration 49/1000 | Loss: 0.00003804
Iteration 50/1000 | Loss: 0.00004051
Iteration 51/1000 | Loss: 0.00003637
Iteration 52/1000 | Loss: 0.00003974
Iteration 53/1000 | Loss: 0.00054835
Iteration 54/1000 | Loss: 0.00180123
Iteration 55/1000 | Loss: 0.00003535
Iteration 56/1000 | Loss: 0.00003793
Iteration 57/1000 | Loss: 0.00003269
Iteration 58/1000 | Loss: 0.00003620
Iteration 59/1000 | Loss: 0.00003288
Iteration 60/1000 | Loss: 0.00003622
Iteration 61/1000 | Loss: 0.00003228
Iteration 62/1000 | Loss: 0.00003752
Iteration 63/1000 | Loss: 0.00003172
Iteration 64/1000 | Loss: 0.00003639
Iteration 65/1000 | Loss: 0.00003474
Iteration 66/1000 | Loss: 0.00004869
Iteration 67/1000 | Loss: 0.00002651
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00061739
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00001920
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001751
Iteration 78/1000 | Loss: 0.00001746
Iteration 79/1000 | Loss: 0.00001736
Iteration 80/1000 | Loss: 0.00001734
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001728
Iteration 89/1000 | Loss: 0.00001728
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001723
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001721
Iteration 134/1000 | Loss: 0.00001721
Iteration 135/1000 | Loss: 0.00001721
Iteration 136/1000 | Loss: 0.00001721
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001719
Iteration 152/1000 | Loss: 0.00001719
Iteration 153/1000 | Loss: 0.00001719
Iteration 154/1000 | Loss: 0.00001719
Iteration 155/1000 | Loss: 0.00001719
Iteration 156/1000 | Loss: 0.00001719
Iteration 157/1000 | Loss: 0.00001719
Iteration 158/1000 | Loss: 0.00001719
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001718
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001718
Iteration 165/1000 | Loss: 0.00001718
Iteration 166/1000 | Loss: 0.00001718
Iteration 167/1000 | Loss: 0.00001718
Iteration 168/1000 | Loss: 0.00001717
Iteration 169/1000 | Loss: 0.00001717
Iteration 170/1000 | Loss: 0.00001717
Iteration 171/1000 | Loss: 0.00001717
Iteration 172/1000 | Loss: 0.00001717
Iteration 173/1000 | Loss: 0.00001717
Iteration 174/1000 | Loss: 0.00001717
Iteration 175/1000 | Loss: 0.00001717
Iteration 176/1000 | Loss: 0.00001716
Iteration 177/1000 | Loss: 0.00001716
Iteration 178/1000 | Loss: 0.00001716
Iteration 179/1000 | Loss: 0.00001716
Iteration 180/1000 | Loss: 0.00001716
Iteration 181/1000 | Loss: 0.00001716
Iteration 182/1000 | Loss: 0.00001716
Iteration 183/1000 | Loss: 0.00001716
Iteration 184/1000 | Loss: 0.00001716
Iteration 185/1000 | Loss: 0.00001716
Iteration 186/1000 | Loss: 0.00001716
Iteration 187/1000 | Loss: 0.00001716
Iteration 188/1000 | Loss: 0.00001716
Iteration 189/1000 | Loss: 0.00001716
Iteration 190/1000 | Loss: 0.00001716
Iteration 191/1000 | Loss: 0.00001716
Iteration 192/1000 | Loss: 0.00001716
Iteration 193/1000 | Loss: 0.00001716
Iteration 194/1000 | Loss: 0.00001716
Iteration 195/1000 | Loss: 0.00001716
Iteration 196/1000 | Loss: 0.00001716
Iteration 197/1000 | Loss: 0.00001716
Iteration 198/1000 | Loss: 0.00001716
Iteration 199/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.7161773939733393e-05, 1.7161773939733393e-05, 1.7161773939733393e-05, 1.7161773939733393e-05, 1.7161773939733393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7161773939733393e-05

Optimization complete. Final v2v error: 3.4671790599823 mm

Highest mean error: 4.25523567199707 mm for frame 78

Lowest mean error: 3.1041014194488525 mm for frame 185

Saving results

Total time: 165.87592220306396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046543
Iteration 2/25 | Loss: 0.00318080
Iteration 3/25 | Loss: 0.00200844
Iteration 4/25 | Loss: 0.00181972
Iteration 5/25 | Loss: 0.00174303
Iteration 6/25 | Loss: 0.00154624
Iteration 7/25 | Loss: 0.00151982
Iteration 8/25 | Loss: 0.00142096
Iteration 9/25 | Loss: 0.00129617
Iteration 10/25 | Loss: 0.00125609
Iteration 11/25 | Loss: 0.00123620
Iteration 12/25 | Loss: 0.00122294
Iteration 13/25 | Loss: 0.00122143
Iteration 14/25 | Loss: 0.00121756
Iteration 15/25 | Loss: 0.00121226
Iteration 16/25 | Loss: 0.00121628
Iteration 17/25 | Loss: 0.00121428
Iteration 18/25 | Loss: 0.00120939
Iteration 19/25 | Loss: 0.00121187
Iteration 20/25 | Loss: 0.00121321
Iteration 21/25 | Loss: 0.00120888
Iteration 22/25 | Loss: 0.00120669
Iteration 23/25 | Loss: 0.00120282
Iteration 24/25 | Loss: 0.00120301
Iteration 25/25 | Loss: 0.00120135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49451375
Iteration 2/25 | Loss: 0.00322630
Iteration 3/25 | Loss: 0.00304014
Iteration 4/25 | Loss: 0.00304014
Iteration 5/25 | Loss: 0.00304014
Iteration 6/25 | Loss: 0.00304014
Iteration 7/25 | Loss: 0.00304014
Iteration 8/25 | Loss: 0.00304014
Iteration 9/25 | Loss: 0.00304014
Iteration 10/25 | Loss: 0.00304014
Iteration 11/25 | Loss: 0.00304014
Iteration 12/25 | Loss: 0.00304014
Iteration 13/25 | Loss: 0.00304014
Iteration 14/25 | Loss: 0.00304014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003040138864889741, 0.003040138864889741, 0.003040138864889741, 0.003040138864889741, 0.003040138864889741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003040138864889741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304014
Iteration 2/1000 | Loss: 0.00065070
Iteration 3/1000 | Loss: 0.00164657
Iteration 4/1000 | Loss: 0.00739420
Iteration 5/1000 | Loss: 0.00064486
Iteration 6/1000 | Loss: 0.00370616
Iteration 7/1000 | Loss: 0.00068402
Iteration 8/1000 | Loss: 0.00111672
Iteration 9/1000 | Loss: 0.00056949
Iteration 10/1000 | Loss: 0.00021228
Iteration 11/1000 | Loss: 0.00017254
Iteration 12/1000 | Loss: 0.00184961
Iteration 13/1000 | Loss: 0.00345690
Iteration 14/1000 | Loss: 0.00090773
Iteration 15/1000 | Loss: 0.00194683
Iteration 16/1000 | Loss: 0.00395708
Iteration 17/1000 | Loss: 0.00704080
Iteration 18/1000 | Loss: 0.00466224
Iteration 19/1000 | Loss: 0.00242004
Iteration 20/1000 | Loss: 0.00596163
Iteration 21/1000 | Loss: 0.00279616
Iteration 22/1000 | Loss: 0.00586878
Iteration 23/1000 | Loss: 0.00323041
Iteration 24/1000 | Loss: 0.00519308
Iteration 25/1000 | Loss: 0.00158787
Iteration 26/1000 | Loss: 0.00792632
Iteration 27/1000 | Loss: 0.00334358
Iteration 28/1000 | Loss: 0.00475951
Iteration 29/1000 | Loss: 0.00481823
Iteration 30/1000 | Loss: 0.00656216
Iteration 31/1000 | Loss: 0.00343833
Iteration 32/1000 | Loss: 0.00693594
Iteration 33/1000 | Loss: 0.00327756
Iteration 34/1000 | Loss: 0.00319150
Iteration 35/1000 | Loss: 0.00433616
Iteration 36/1000 | Loss: 0.00352833
Iteration 37/1000 | Loss: 0.00586058
Iteration 38/1000 | Loss: 0.00311073
Iteration 39/1000 | Loss: 0.00296757
Iteration 40/1000 | Loss: 0.00754944
Iteration 41/1000 | Loss: 0.00365522
Iteration 42/1000 | Loss: 0.00450393
Iteration 43/1000 | Loss: 0.00308238
Iteration 44/1000 | Loss: 0.00442086
Iteration 45/1000 | Loss: 0.00391643
Iteration 46/1000 | Loss: 0.00510497
Iteration 47/1000 | Loss: 0.00437100
Iteration 48/1000 | Loss: 0.00286687
Iteration 49/1000 | Loss: 0.00313716
Iteration 50/1000 | Loss: 0.00265991
Iteration 51/1000 | Loss: 0.00170549
Iteration 52/1000 | Loss: 0.00138786
Iteration 53/1000 | Loss: 0.00088116
Iteration 54/1000 | Loss: 0.00066495
Iteration 55/1000 | Loss: 0.00077682
Iteration 56/1000 | Loss: 0.00146803
Iteration 57/1000 | Loss: 0.00154151
Iteration 58/1000 | Loss: 0.00154943
Iteration 59/1000 | Loss: 0.00110880
Iteration 60/1000 | Loss: 0.00065969
Iteration 61/1000 | Loss: 0.00100243
Iteration 62/1000 | Loss: 0.00088183
Iteration 63/1000 | Loss: 0.00149366
Iteration 64/1000 | Loss: 0.00042170
Iteration 65/1000 | Loss: 0.00056082
Iteration 66/1000 | Loss: 0.00069974
Iteration 67/1000 | Loss: 0.00154593
Iteration 68/1000 | Loss: 0.00179593
Iteration 69/1000 | Loss: 0.00153948
Iteration 70/1000 | Loss: 0.00043376
Iteration 71/1000 | Loss: 0.00019566
Iteration 72/1000 | Loss: 0.00057195
Iteration 73/1000 | Loss: 0.00085396
Iteration 74/1000 | Loss: 0.00049413
Iteration 75/1000 | Loss: 0.00079564
Iteration 76/1000 | Loss: 0.00073567
Iteration 77/1000 | Loss: 0.00091751
Iteration 78/1000 | Loss: 0.00040859
Iteration 79/1000 | Loss: 0.00031290
Iteration 80/1000 | Loss: 0.00150944
Iteration 81/1000 | Loss: 0.00067236
Iteration 82/1000 | Loss: 0.00043832
Iteration 83/1000 | Loss: 0.00052390
Iteration 84/1000 | Loss: 0.00078338
Iteration 85/1000 | Loss: 0.00115333
Iteration 86/1000 | Loss: 0.00096731
Iteration 87/1000 | Loss: 0.00129695
Iteration 88/1000 | Loss: 0.00115125
Iteration 89/1000 | Loss: 0.00115285
Iteration 90/1000 | Loss: 0.00120669
Iteration 91/1000 | Loss: 0.00117567
Iteration 92/1000 | Loss: 0.00092147
Iteration 93/1000 | Loss: 0.00089759
Iteration 94/1000 | Loss: 0.00061743
Iteration 95/1000 | Loss: 0.00080044
Iteration 96/1000 | Loss: 0.00101533
Iteration 97/1000 | Loss: 0.00135107
Iteration 98/1000 | Loss: 0.00052667
Iteration 99/1000 | Loss: 0.00036614
Iteration 100/1000 | Loss: 0.00038309
Iteration 101/1000 | Loss: 0.00049489
Iteration 102/1000 | Loss: 0.00022480
Iteration 103/1000 | Loss: 0.00050362
Iteration 104/1000 | Loss: 0.00047036
Iteration 105/1000 | Loss: 0.00032010
Iteration 106/1000 | Loss: 0.00018117
Iteration 107/1000 | Loss: 0.00019628
Iteration 108/1000 | Loss: 0.00037908
Iteration 109/1000 | Loss: 0.00026509
Iteration 110/1000 | Loss: 0.00045341
Iteration 111/1000 | Loss: 0.00201551
Iteration 112/1000 | Loss: 0.00049309
Iteration 113/1000 | Loss: 0.00054700
Iteration 114/1000 | Loss: 0.00021496
Iteration 115/1000 | Loss: 0.00021300
Iteration 116/1000 | Loss: 0.00043817
Iteration 117/1000 | Loss: 0.00057878
Iteration 118/1000 | Loss: 0.00068393
Iteration 119/1000 | Loss: 0.00039223
Iteration 120/1000 | Loss: 0.00045476
Iteration 121/1000 | Loss: 0.00106535
Iteration 122/1000 | Loss: 0.00035630
Iteration 123/1000 | Loss: 0.00025031
Iteration 124/1000 | Loss: 0.00015430
Iteration 125/1000 | Loss: 0.00015640
Iteration 126/1000 | Loss: 0.00043279
Iteration 127/1000 | Loss: 0.00052464
Iteration 128/1000 | Loss: 0.00063726
Iteration 129/1000 | Loss: 0.00010154
Iteration 130/1000 | Loss: 0.00024488
Iteration 131/1000 | Loss: 0.00007257
Iteration 132/1000 | Loss: 0.00027782
Iteration 133/1000 | Loss: 0.00005027
Iteration 134/1000 | Loss: 0.00004116
Iteration 135/1000 | Loss: 0.00003933
Iteration 136/1000 | Loss: 0.00004027
Iteration 137/1000 | Loss: 0.00032642
Iteration 138/1000 | Loss: 0.00154147
Iteration 139/1000 | Loss: 0.00050792
Iteration 140/1000 | Loss: 0.00051086
Iteration 141/1000 | Loss: 0.00004120
Iteration 142/1000 | Loss: 0.00008854
Iteration 143/1000 | Loss: 0.00003120
Iteration 144/1000 | Loss: 0.00027126
Iteration 145/1000 | Loss: 0.00021117
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002710
Iteration 148/1000 | Loss: 0.00007562
Iteration 149/1000 | Loss: 0.00020483
Iteration 150/1000 | Loss: 0.00026216
Iteration 151/1000 | Loss: 0.00020714
Iteration 152/1000 | Loss: 0.00004368
Iteration 153/1000 | Loss: 0.00003971
Iteration 154/1000 | Loss: 0.00021914
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00002211
Iteration 157/1000 | Loss: 0.00004442
Iteration 158/1000 | Loss: 0.00002076
Iteration 159/1000 | Loss: 0.00005486
Iteration 160/1000 | Loss: 0.00002046
Iteration 161/1000 | Loss: 0.00001997
Iteration 162/1000 | Loss: 0.00001973
Iteration 163/1000 | Loss: 0.00001963
Iteration 164/1000 | Loss: 0.00001962
Iteration 165/1000 | Loss: 0.00001962
Iteration 166/1000 | Loss: 0.00001959
Iteration 167/1000 | Loss: 0.00001946
Iteration 168/1000 | Loss: 0.00005085
Iteration 169/1000 | Loss: 0.00006153
Iteration 170/1000 | Loss: 0.00002178
Iteration 171/1000 | Loss: 0.00002873
Iteration 172/1000 | Loss: 0.00001930
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00002453
Iteration 176/1000 | Loss: 0.00004755
Iteration 177/1000 | Loss: 0.00002740
Iteration 178/1000 | Loss: 0.00001927
Iteration 179/1000 | Loss: 0.00002510
Iteration 180/1000 | Loss: 0.00001913
Iteration 181/1000 | Loss: 0.00001905
Iteration 182/1000 | Loss: 0.00001905
Iteration 183/1000 | Loss: 0.00001903
Iteration 184/1000 | Loss: 0.00001902
Iteration 185/1000 | Loss: 0.00001901
Iteration 186/1000 | Loss: 0.00001901
Iteration 187/1000 | Loss: 0.00001901
Iteration 188/1000 | Loss: 0.00002612
Iteration 189/1000 | Loss: 0.00001904
Iteration 190/1000 | Loss: 0.00001900
Iteration 191/1000 | Loss: 0.00001899
Iteration 192/1000 | Loss: 0.00001899
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001897
Iteration 196/1000 | Loss: 0.00001897
Iteration 197/1000 | Loss: 0.00001897
Iteration 198/1000 | Loss: 0.00001897
Iteration 199/1000 | Loss: 0.00001897
Iteration 200/1000 | Loss: 0.00001897
Iteration 201/1000 | Loss: 0.00001897
Iteration 202/1000 | Loss: 0.00001897
Iteration 203/1000 | Loss: 0.00001896
Iteration 204/1000 | Loss: 0.00001896
Iteration 205/1000 | Loss: 0.00001896
Iteration 206/1000 | Loss: 0.00001896
Iteration 207/1000 | Loss: 0.00001896
Iteration 208/1000 | Loss: 0.00001896
Iteration 209/1000 | Loss: 0.00001896
Iteration 210/1000 | Loss: 0.00001896
Iteration 211/1000 | Loss: 0.00001895
Iteration 212/1000 | Loss: 0.00001895
Iteration 213/1000 | Loss: 0.00001895
Iteration 214/1000 | Loss: 0.00001895
Iteration 215/1000 | Loss: 0.00001895
Iteration 216/1000 | Loss: 0.00001894
Iteration 217/1000 | Loss: 0.00001894
Iteration 218/1000 | Loss: 0.00001894
Iteration 219/1000 | Loss: 0.00001894
Iteration 220/1000 | Loss: 0.00001894
Iteration 221/1000 | Loss: 0.00001893
Iteration 222/1000 | Loss: 0.00001893
Iteration 223/1000 | Loss: 0.00001893
Iteration 224/1000 | Loss: 0.00001892
Iteration 225/1000 | Loss: 0.00001892
Iteration 226/1000 | Loss: 0.00001892
Iteration 227/1000 | Loss: 0.00001892
Iteration 228/1000 | Loss: 0.00001892
Iteration 229/1000 | Loss: 0.00001891
Iteration 230/1000 | Loss: 0.00001891
Iteration 231/1000 | Loss: 0.00001891
Iteration 232/1000 | Loss: 0.00001891
Iteration 233/1000 | Loss: 0.00001891
Iteration 234/1000 | Loss: 0.00001891
Iteration 235/1000 | Loss: 0.00001891
Iteration 236/1000 | Loss: 0.00001891
Iteration 237/1000 | Loss: 0.00001891
Iteration 238/1000 | Loss: 0.00001891
Iteration 239/1000 | Loss: 0.00001890
Iteration 240/1000 | Loss: 0.00001890
Iteration 241/1000 | Loss: 0.00001890
Iteration 242/1000 | Loss: 0.00001890
Iteration 243/1000 | Loss: 0.00001890
Iteration 244/1000 | Loss: 0.00001889
Iteration 245/1000 | Loss: 0.00001889
Iteration 246/1000 | Loss: 0.00001889
Iteration 247/1000 | Loss: 0.00001889
Iteration 248/1000 | Loss: 0.00001889
Iteration 249/1000 | Loss: 0.00001889
Iteration 250/1000 | Loss: 0.00001889
Iteration 251/1000 | Loss: 0.00001889
Iteration 252/1000 | Loss: 0.00001889
Iteration 253/1000 | Loss: 0.00001889
Iteration 254/1000 | Loss: 0.00001889
Iteration 255/1000 | Loss: 0.00001889
Iteration 256/1000 | Loss: 0.00001888
Iteration 257/1000 | Loss: 0.00001888
Iteration 258/1000 | Loss: 0.00001888
Iteration 259/1000 | Loss: 0.00001888
Iteration 260/1000 | Loss: 0.00001888
Iteration 261/1000 | Loss: 0.00001888
Iteration 262/1000 | Loss: 0.00001888
Iteration 263/1000 | Loss: 0.00001888
Iteration 264/1000 | Loss: 0.00001888
Iteration 265/1000 | Loss: 0.00001888
Iteration 266/1000 | Loss: 0.00001888
Iteration 267/1000 | Loss: 0.00001888
Iteration 268/1000 | Loss: 0.00001888
Iteration 269/1000 | Loss: 0.00001888
Iteration 270/1000 | Loss: 0.00001888
Iteration 271/1000 | Loss: 0.00001888
Iteration 272/1000 | Loss: 0.00001888
Iteration 273/1000 | Loss: 0.00001888
Iteration 274/1000 | Loss: 0.00001888
Iteration 275/1000 | Loss: 0.00001888
Iteration 276/1000 | Loss: 0.00001888
Iteration 277/1000 | Loss: 0.00001888
Iteration 278/1000 | Loss: 0.00001888
Iteration 279/1000 | Loss: 0.00001888
Iteration 280/1000 | Loss: 0.00001888
Iteration 281/1000 | Loss: 0.00001888
Iteration 282/1000 | Loss: 0.00001888
Iteration 283/1000 | Loss: 0.00001888
Iteration 284/1000 | Loss: 0.00001888
Iteration 285/1000 | Loss: 0.00001888
Iteration 286/1000 | Loss: 0.00001888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [1.887799044197891e-05, 1.887799044197891e-05, 1.887799044197891e-05, 1.887799044197891e-05, 1.887799044197891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.887799044197891e-05

Optimization complete. Final v2v error: 3.5959489345550537 mm

Highest mean error: 9.072707176208496 mm for frame 104

Lowest mean error: 2.903331756591797 mm for frame 123

Saving results

Total time: 335.85105562210083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601209
Iteration 2/25 | Loss: 0.00139632
Iteration 3/25 | Loss: 0.00098445
Iteration 4/25 | Loss: 0.00091675
Iteration 5/25 | Loss: 0.00090316
Iteration 6/25 | Loss: 0.00089961
Iteration 7/25 | Loss: 0.00089886
Iteration 8/25 | Loss: 0.00089886
Iteration 9/25 | Loss: 0.00089886
Iteration 10/25 | Loss: 0.00089886
Iteration 11/25 | Loss: 0.00089886
Iteration 12/25 | Loss: 0.00089886
Iteration 13/25 | Loss: 0.00089886
Iteration 14/25 | Loss: 0.00089886
Iteration 15/25 | Loss: 0.00089886
Iteration 16/25 | Loss: 0.00089886
Iteration 17/25 | Loss: 0.00089886
Iteration 18/25 | Loss: 0.00089886
Iteration 19/25 | Loss: 0.00089886
Iteration 20/25 | Loss: 0.00089886
Iteration 21/25 | Loss: 0.00089886
Iteration 22/25 | Loss: 0.00089886
Iteration 23/25 | Loss: 0.00089886
Iteration 24/25 | Loss: 0.00089886
Iteration 25/25 | Loss: 0.00089886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51827192
Iteration 2/25 | Loss: 0.00053613
Iteration 3/25 | Loss: 0.00053610
Iteration 4/25 | Loss: 0.00053610
Iteration 5/25 | Loss: 0.00053610
Iteration 6/25 | Loss: 0.00053610
Iteration 7/25 | Loss: 0.00053610
Iteration 8/25 | Loss: 0.00053610
Iteration 9/25 | Loss: 0.00053610
Iteration 10/25 | Loss: 0.00053610
Iteration 11/25 | Loss: 0.00053610
Iteration 12/25 | Loss: 0.00053610
Iteration 13/25 | Loss: 0.00053610
Iteration 14/25 | Loss: 0.00053610
Iteration 15/25 | Loss: 0.00053610
Iteration 16/25 | Loss: 0.00053610
Iteration 17/25 | Loss: 0.00053610
Iteration 18/25 | Loss: 0.00053610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005361014045774937, 0.0005361014045774937, 0.0005361014045774937, 0.0005361014045774937, 0.0005361014045774937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005361014045774937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053610
Iteration 2/1000 | Loss: 0.00003385
Iteration 3/1000 | Loss: 0.00002607
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002039
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001960
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001870
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001869
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001868
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001867
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001866
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001863
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001857
Iteration 132/1000 | Loss: 0.00001857
Iteration 133/1000 | Loss: 0.00001857
Iteration 134/1000 | Loss: 0.00001857
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001857
Iteration 138/1000 | Loss: 0.00001857
Iteration 139/1000 | Loss: 0.00001857
Iteration 140/1000 | Loss: 0.00001856
Iteration 141/1000 | Loss: 0.00001856
Iteration 142/1000 | Loss: 0.00001856
Iteration 143/1000 | Loss: 0.00001856
Iteration 144/1000 | Loss: 0.00001856
Iteration 145/1000 | Loss: 0.00001856
Iteration 146/1000 | Loss: 0.00001856
Iteration 147/1000 | Loss: 0.00001856
Iteration 148/1000 | Loss: 0.00001856
Iteration 149/1000 | Loss: 0.00001856
Iteration 150/1000 | Loss: 0.00001856
Iteration 151/1000 | Loss: 0.00001856
Iteration 152/1000 | Loss: 0.00001856
Iteration 153/1000 | Loss: 0.00001856
Iteration 154/1000 | Loss: 0.00001856
Iteration 155/1000 | Loss: 0.00001856
Iteration 156/1000 | Loss: 0.00001856
Iteration 157/1000 | Loss: 0.00001856
Iteration 158/1000 | Loss: 0.00001856
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Iteration 170/1000 | Loss: 0.00001856
Iteration 171/1000 | Loss: 0.00001856
Iteration 172/1000 | Loss: 0.00001856
Iteration 173/1000 | Loss: 0.00001856
Iteration 174/1000 | Loss: 0.00001856
Iteration 175/1000 | Loss: 0.00001856
Iteration 176/1000 | Loss: 0.00001856
Iteration 177/1000 | Loss: 0.00001856
Iteration 178/1000 | Loss: 0.00001856
Iteration 179/1000 | Loss: 0.00001856
Iteration 180/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8560000171419233e-05, 1.8560000171419233e-05, 1.8560000171419233e-05, 1.8560000171419233e-05, 1.8560000171419233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8560000171419233e-05

Optimization complete. Final v2v error: 3.660111427307129 mm

Highest mean error: 4.323461532592773 mm for frame 57

Lowest mean error: 3.313337802886963 mm for frame 131

Saving results

Total time: 39.383758783340454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540683
Iteration 2/25 | Loss: 0.00115420
Iteration 3/25 | Loss: 0.00098551
Iteration 4/25 | Loss: 0.00094740
Iteration 5/25 | Loss: 0.00093604
Iteration 6/25 | Loss: 0.00093405
Iteration 7/25 | Loss: 0.00093329
Iteration 8/25 | Loss: 0.00093328
Iteration 9/25 | Loss: 0.00093328
Iteration 10/25 | Loss: 0.00093328
Iteration 11/25 | Loss: 0.00093328
Iteration 12/25 | Loss: 0.00093328
Iteration 13/25 | Loss: 0.00093328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009332815534435213, 0.0009332815534435213, 0.0009332815534435213, 0.0009332815534435213, 0.0009332815534435213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009332815534435213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53410792
Iteration 2/25 | Loss: 0.00072332
Iteration 3/25 | Loss: 0.00072332
Iteration 4/25 | Loss: 0.00072332
Iteration 5/25 | Loss: 0.00072332
Iteration 6/25 | Loss: 0.00072331
Iteration 7/25 | Loss: 0.00072331
Iteration 8/25 | Loss: 0.00072331
Iteration 9/25 | Loss: 0.00072331
Iteration 10/25 | Loss: 0.00072331
Iteration 11/25 | Loss: 0.00072331
Iteration 12/25 | Loss: 0.00072331
Iteration 13/25 | Loss: 0.00072331
Iteration 14/25 | Loss: 0.00072331
Iteration 15/25 | Loss: 0.00072331
Iteration 16/25 | Loss: 0.00072331
Iteration 17/25 | Loss: 0.00072331
Iteration 18/25 | Loss: 0.00072331
Iteration 19/25 | Loss: 0.00072331
Iteration 20/25 | Loss: 0.00072331
Iteration 21/25 | Loss: 0.00072331
Iteration 22/25 | Loss: 0.00072331
Iteration 23/25 | Loss: 0.00072331
Iteration 24/25 | Loss: 0.00072331
Iteration 25/25 | Loss: 0.00072331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072331
Iteration 2/1000 | Loss: 0.00005634
Iteration 3/1000 | Loss: 0.00004047
Iteration 4/1000 | Loss: 0.00003360
Iteration 5/1000 | Loss: 0.00003160
Iteration 6/1000 | Loss: 0.00003037
Iteration 7/1000 | Loss: 0.00002943
Iteration 8/1000 | Loss: 0.00002894
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00002802
Iteration 11/1000 | Loss: 0.00002772
Iteration 12/1000 | Loss: 0.00002748
Iteration 13/1000 | Loss: 0.00002730
Iteration 14/1000 | Loss: 0.00002722
Iteration 15/1000 | Loss: 0.00002717
Iteration 16/1000 | Loss: 0.00002714
Iteration 17/1000 | Loss: 0.00002713
Iteration 18/1000 | Loss: 0.00002709
Iteration 19/1000 | Loss: 0.00002708
Iteration 20/1000 | Loss: 0.00002703
Iteration 21/1000 | Loss: 0.00002698
Iteration 22/1000 | Loss: 0.00002695
Iteration 23/1000 | Loss: 0.00002694
Iteration 24/1000 | Loss: 0.00002687
Iteration 25/1000 | Loss: 0.00002686
Iteration 26/1000 | Loss: 0.00002683
Iteration 27/1000 | Loss: 0.00002683
Iteration 28/1000 | Loss: 0.00002679
Iteration 29/1000 | Loss: 0.00002679
Iteration 30/1000 | Loss: 0.00002678
Iteration 31/1000 | Loss: 0.00002678
Iteration 32/1000 | Loss: 0.00002678
Iteration 33/1000 | Loss: 0.00002678
Iteration 34/1000 | Loss: 0.00002678
Iteration 35/1000 | Loss: 0.00002678
Iteration 36/1000 | Loss: 0.00002678
Iteration 37/1000 | Loss: 0.00002678
Iteration 38/1000 | Loss: 0.00002678
Iteration 39/1000 | Loss: 0.00002678
Iteration 40/1000 | Loss: 0.00002678
Iteration 41/1000 | Loss: 0.00002677
Iteration 42/1000 | Loss: 0.00002677
Iteration 43/1000 | Loss: 0.00002677
Iteration 44/1000 | Loss: 0.00002677
Iteration 45/1000 | Loss: 0.00002676
Iteration 46/1000 | Loss: 0.00002676
Iteration 47/1000 | Loss: 0.00002676
Iteration 48/1000 | Loss: 0.00002675
Iteration 49/1000 | Loss: 0.00002675
Iteration 50/1000 | Loss: 0.00002675
Iteration 51/1000 | Loss: 0.00002675
Iteration 52/1000 | Loss: 0.00002674
Iteration 53/1000 | Loss: 0.00002674
Iteration 54/1000 | Loss: 0.00002673
Iteration 55/1000 | Loss: 0.00002673
Iteration 56/1000 | Loss: 0.00002673
Iteration 57/1000 | Loss: 0.00002673
Iteration 58/1000 | Loss: 0.00002672
Iteration 59/1000 | Loss: 0.00002672
Iteration 60/1000 | Loss: 0.00002672
Iteration 61/1000 | Loss: 0.00002672
Iteration 62/1000 | Loss: 0.00002671
Iteration 63/1000 | Loss: 0.00002671
Iteration 64/1000 | Loss: 0.00002671
Iteration 65/1000 | Loss: 0.00002670
Iteration 66/1000 | Loss: 0.00002670
Iteration 67/1000 | Loss: 0.00002670
Iteration 68/1000 | Loss: 0.00002669
Iteration 69/1000 | Loss: 0.00002669
Iteration 70/1000 | Loss: 0.00002669
Iteration 71/1000 | Loss: 0.00002668
Iteration 72/1000 | Loss: 0.00002668
Iteration 73/1000 | Loss: 0.00002667
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002666
Iteration 76/1000 | Loss: 0.00002666
Iteration 77/1000 | Loss: 0.00002665
Iteration 78/1000 | Loss: 0.00002665
Iteration 79/1000 | Loss: 0.00002664
Iteration 80/1000 | Loss: 0.00002664
Iteration 81/1000 | Loss: 0.00002663
Iteration 82/1000 | Loss: 0.00002662
Iteration 83/1000 | Loss: 0.00002662
Iteration 84/1000 | Loss: 0.00002662
Iteration 85/1000 | Loss: 0.00002662
Iteration 86/1000 | Loss: 0.00002662
Iteration 87/1000 | Loss: 0.00002662
Iteration 88/1000 | Loss: 0.00002662
Iteration 89/1000 | Loss: 0.00002662
Iteration 90/1000 | Loss: 0.00002662
Iteration 91/1000 | Loss: 0.00002662
Iteration 92/1000 | Loss: 0.00002662
Iteration 93/1000 | Loss: 0.00002661
Iteration 94/1000 | Loss: 0.00002661
Iteration 95/1000 | Loss: 0.00002660
Iteration 96/1000 | Loss: 0.00002660
Iteration 97/1000 | Loss: 0.00002660
Iteration 98/1000 | Loss: 0.00002659
Iteration 99/1000 | Loss: 0.00002659
Iteration 100/1000 | Loss: 0.00002659
Iteration 101/1000 | Loss: 0.00002658
Iteration 102/1000 | Loss: 0.00002658
Iteration 103/1000 | Loss: 0.00002658
Iteration 104/1000 | Loss: 0.00002657
Iteration 105/1000 | Loss: 0.00002657
Iteration 106/1000 | Loss: 0.00002657
Iteration 107/1000 | Loss: 0.00002657
Iteration 108/1000 | Loss: 0.00002657
Iteration 109/1000 | Loss: 0.00002656
Iteration 110/1000 | Loss: 0.00002656
Iteration 111/1000 | Loss: 0.00002656
Iteration 112/1000 | Loss: 0.00002656
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002655
Iteration 115/1000 | Loss: 0.00002655
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002654
Iteration 119/1000 | Loss: 0.00002654
Iteration 120/1000 | Loss: 0.00002653
Iteration 121/1000 | Loss: 0.00002653
Iteration 122/1000 | Loss: 0.00002653
Iteration 123/1000 | Loss: 0.00002653
Iteration 124/1000 | Loss: 0.00002652
Iteration 125/1000 | Loss: 0.00002652
Iteration 126/1000 | Loss: 0.00002652
Iteration 127/1000 | Loss: 0.00002652
Iteration 128/1000 | Loss: 0.00002651
Iteration 129/1000 | Loss: 0.00002651
Iteration 130/1000 | Loss: 0.00002651
Iteration 131/1000 | Loss: 0.00002651
Iteration 132/1000 | Loss: 0.00002651
Iteration 133/1000 | Loss: 0.00002651
Iteration 134/1000 | Loss: 0.00002651
Iteration 135/1000 | Loss: 0.00002651
Iteration 136/1000 | Loss: 0.00002651
Iteration 137/1000 | Loss: 0.00002651
Iteration 138/1000 | Loss: 0.00002651
Iteration 139/1000 | Loss: 0.00002650
Iteration 140/1000 | Loss: 0.00002650
Iteration 141/1000 | Loss: 0.00002650
Iteration 142/1000 | Loss: 0.00002650
Iteration 143/1000 | Loss: 0.00002650
Iteration 144/1000 | Loss: 0.00002650
Iteration 145/1000 | Loss: 0.00002650
Iteration 146/1000 | Loss: 0.00002650
Iteration 147/1000 | Loss: 0.00002650
Iteration 148/1000 | Loss: 0.00002650
Iteration 149/1000 | Loss: 0.00002650
Iteration 150/1000 | Loss: 0.00002650
Iteration 151/1000 | Loss: 0.00002650
Iteration 152/1000 | Loss: 0.00002650
Iteration 153/1000 | Loss: 0.00002650
Iteration 154/1000 | Loss: 0.00002650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.650309761520475e-05, 2.650309761520475e-05, 2.650309761520475e-05, 2.650309761520475e-05, 2.650309761520475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.650309761520475e-05

Optimization complete. Final v2v error: 4.229624271392822 mm

Highest mean error: 4.8352556228637695 mm for frame 82

Lowest mean error: 3.5325989723205566 mm for frame 120

Saving results

Total time: 41.69189953804016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414699
Iteration 2/25 | Loss: 0.00108409
Iteration 3/25 | Loss: 0.00089921
Iteration 4/25 | Loss: 0.00085864
Iteration 5/25 | Loss: 0.00084698
Iteration 6/25 | Loss: 0.00084379
Iteration 7/25 | Loss: 0.00084295
Iteration 8/25 | Loss: 0.00084277
Iteration 9/25 | Loss: 0.00084277
Iteration 10/25 | Loss: 0.00084277
Iteration 11/25 | Loss: 0.00084277
Iteration 12/25 | Loss: 0.00084277
Iteration 13/25 | Loss: 0.00084277
Iteration 14/25 | Loss: 0.00084277
Iteration 15/25 | Loss: 0.00084277
Iteration 16/25 | Loss: 0.00084277
Iteration 17/25 | Loss: 0.00084277
Iteration 18/25 | Loss: 0.00084277
Iteration 19/25 | Loss: 0.00084277
Iteration 20/25 | Loss: 0.00084277
Iteration 21/25 | Loss: 0.00084277
Iteration 22/25 | Loss: 0.00084277
Iteration 23/25 | Loss: 0.00084277
Iteration 24/25 | Loss: 0.00084277
Iteration 25/25 | Loss: 0.00084277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55955267
Iteration 2/25 | Loss: 0.00054614
Iteration 3/25 | Loss: 0.00054613
Iteration 4/25 | Loss: 0.00054613
Iteration 5/25 | Loss: 0.00054613
Iteration 6/25 | Loss: 0.00054613
Iteration 7/25 | Loss: 0.00054613
Iteration 8/25 | Loss: 0.00054613
Iteration 9/25 | Loss: 0.00054613
Iteration 10/25 | Loss: 0.00054613
Iteration 11/25 | Loss: 0.00054613
Iteration 12/25 | Loss: 0.00054613
Iteration 13/25 | Loss: 0.00054613
Iteration 14/25 | Loss: 0.00054613
Iteration 15/25 | Loss: 0.00054613
Iteration 16/25 | Loss: 0.00054613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005461310502141714, 0.0005461310502141714, 0.0005461310502141714, 0.0005461310502141714, 0.0005461310502141714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005461310502141714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054613
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001943
Iteration 6/1000 | Loss: 0.00001868
Iteration 7/1000 | Loss: 0.00001812
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001702
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001688
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001685
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001684
Iteration 22/1000 | Loss: 0.00001683
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001681
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001678
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001676
Iteration 40/1000 | Loss: 0.00001675
Iteration 41/1000 | Loss: 0.00001675
Iteration 42/1000 | Loss: 0.00001675
Iteration 43/1000 | Loss: 0.00001674
Iteration 44/1000 | Loss: 0.00001674
Iteration 45/1000 | Loss: 0.00001674
Iteration 46/1000 | Loss: 0.00001673
Iteration 47/1000 | Loss: 0.00001673
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001673
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00001673
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001666
Iteration 70/1000 | Loss: 0.00001666
Iteration 71/1000 | Loss: 0.00001666
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001664
Iteration 77/1000 | Loss: 0.00001664
Iteration 78/1000 | Loss: 0.00001663
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001662
Iteration 88/1000 | Loss: 0.00001662
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001661
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001655
Iteration 116/1000 | Loss: 0.00001655
Iteration 117/1000 | Loss: 0.00001655
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001654
Iteration 121/1000 | Loss: 0.00001654
Iteration 122/1000 | Loss: 0.00001654
Iteration 123/1000 | Loss: 0.00001654
Iteration 124/1000 | Loss: 0.00001654
Iteration 125/1000 | Loss: 0.00001654
Iteration 126/1000 | Loss: 0.00001654
Iteration 127/1000 | Loss: 0.00001654
Iteration 128/1000 | Loss: 0.00001654
Iteration 129/1000 | Loss: 0.00001654
Iteration 130/1000 | Loss: 0.00001653
Iteration 131/1000 | Loss: 0.00001653
Iteration 132/1000 | Loss: 0.00001653
Iteration 133/1000 | Loss: 0.00001653
Iteration 134/1000 | Loss: 0.00001653
Iteration 135/1000 | Loss: 0.00001653
Iteration 136/1000 | Loss: 0.00001653
Iteration 137/1000 | Loss: 0.00001653
Iteration 138/1000 | Loss: 0.00001653
Iteration 139/1000 | Loss: 0.00001653
Iteration 140/1000 | Loss: 0.00001653
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001652
Iteration 145/1000 | Loss: 0.00001652
Iteration 146/1000 | Loss: 0.00001652
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001652
Iteration 150/1000 | Loss: 0.00001652
Iteration 151/1000 | Loss: 0.00001652
Iteration 152/1000 | Loss: 0.00001652
Iteration 153/1000 | Loss: 0.00001652
Iteration 154/1000 | Loss: 0.00001652
Iteration 155/1000 | Loss: 0.00001651
Iteration 156/1000 | Loss: 0.00001651
Iteration 157/1000 | Loss: 0.00001651
Iteration 158/1000 | Loss: 0.00001651
Iteration 159/1000 | Loss: 0.00001651
Iteration 160/1000 | Loss: 0.00001651
Iteration 161/1000 | Loss: 0.00001651
Iteration 162/1000 | Loss: 0.00001651
Iteration 163/1000 | Loss: 0.00001651
Iteration 164/1000 | Loss: 0.00001650
Iteration 165/1000 | Loss: 0.00001650
Iteration 166/1000 | Loss: 0.00001650
Iteration 167/1000 | Loss: 0.00001650
Iteration 168/1000 | Loss: 0.00001650
Iteration 169/1000 | Loss: 0.00001650
Iteration 170/1000 | Loss: 0.00001650
Iteration 171/1000 | Loss: 0.00001650
Iteration 172/1000 | Loss: 0.00001650
Iteration 173/1000 | Loss: 0.00001650
Iteration 174/1000 | Loss: 0.00001650
Iteration 175/1000 | Loss: 0.00001650
Iteration 176/1000 | Loss: 0.00001650
Iteration 177/1000 | Loss: 0.00001650
Iteration 178/1000 | Loss: 0.00001650
Iteration 179/1000 | Loss: 0.00001650
Iteration 180/1000 | Loss: 0.00001650
Iteration 181/1000 | Loss: 0.00001650
Iteration 182/1000 | Loss: 0.00001650
Iteration 183/1000 | Loss: 0.00001650
Iteration 184/1000 | Loss: 0.00001649
Iteration 185/1000 | Loss: 0.00001649
Iteration 186/1000 | Loss: 0.00001649
Iteration 187/1000 | Loss: 0.00001649
Iteration 188/1000 | Loss: 0.00001649
Iteration 189/1000 | Loss: 0.00001649
Iteration 190/1000 | Loss: 0.00001649
Iteration 191/1000 | Loss: 0.00001649
Iteration 192/1000 | Loss: 0.00001649
Iteration 193/1000 | Loss: 0.00001649
Iteration 194/1000 | Loss: 0.00001649
Iteration 195/1000 | Loss: 0.00001649
Iteration 196/1000 | Loss: 0.00001649
Iteration 197/1000 | Loss: 0.00001649
Iteration 198/1000 | Loss: 0.00001649
Iteration 199/1000 | Loss: 0.00001649
Iteration 200/1000 | Loss: 0.00001649
Iteration 201/1000 | Loss: 0.00001649
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001648
Iteration 204/1000 | Loss: 0.00001648
Iteration 205/1000 | Loss: 0.00001648
Iteration 206/1000 | Loss: 0.00001648
Iteration 207/1000 | Loss: 0.00001648
Iteration 208/1000 | Loss: 0.00001648
Iteration 209/1000 | Loss: 0.00001648
Iteration 210/1000 | Loss: 0.00001648
Iteration 211/1000 | Loss: 0.00001648
Iteration 212/1000 | Loss: 0.00001648
Iteration 213/1000 | Loss: 0.00001648
Iteration 214/1000 | Loss: 0.00001648
Iteration 215/1000 | Loss: 0.00001648
Iteration 216/1000 | Loss: 0.00001648
Iteration 217/1000 | Loss: 0.00001648
Iteration 218/1000 | Loss: 0.00001648
Iteration 219/1000 | Loss: 0.00001648
Iteration 220/1000 | Loss: 0.00001647
Iteration 221/1000 | Loss: 0.00001647
Iteration 222/1000 | Loss: 0.00001647
Iteration 223/1000 | Loss: 0.00001647
Iteration 224/1000 | Loss: 0.00001647
Iteration 225/1000 | Loss: 0.00001647
Iteration 226/1000 | Loss: 0.00001647
Iteration 227/1000 | Loss: 0.00001647
Iteration 228/1000 | Loss: 0.00001647
Iteration 229/1000 | Loss: 0.00001647
Iteration 230/1000 | Loss: 0.00001647
Iteration 231/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.6470368791488e-05, 1.6470368791488e-05, 1.6470368791488e-05, 1.6470368791488e-05, 1.6470368791488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6470368791488e-05

Optimization complete. Final v2v error: 3.417161464691162 mm

Highest mean error: 4.411915302276611 mm for frame 46

Lowest mean error: 3.004502534866333 mm for frame 92

Saving results

Total time: 43.34633994102478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544258
Iteration 2/25 | Loss: 0.00141693
Iteration 3/25 | Loss: 0.00099705
Iteration 4/25 | Loss: 0.00094863
Iteration 5/25 | Loss: 0.00093615
Iteration 6/25 | Loss: 0.00093219
Iteration 7/25 | Loss: 0.00093146
Iteration 8/25 | Loss: 0.00093146
Iteration 9/25 | Loss: 0.00093146
Iteration 10/25 | Loss: 0.00093146
Iteration 11/25 | Loss: 0.00093146
Iteration 12/25 | Loss: 0.00093146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009314593626186252, 0.0009314593626186252, 0.0009314593626186252, 0.0009314593626186252, 0.0009314593626186252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009314593626186252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02598047
Iteration 2/25 | Loss: 0.00058230
Iteration 3/25 | Loss: 0.00058230
Iteration 4/25 | Loss: 0.00058230
Iteration 5/25 | Loss: 0.00058230
Iteration 6/25 | Loss: 0.00058230
Iteration 7/25 | Loss: 0.00058230
Iteration 8/25 | Loss: 0.00058230
Iteration 9/25 | Loss: 0.00058230
Iteration 10/25 | Loss: 0.00058230
Iteration 11/25 | Loss: 0.00058230
Iteration 12/25 | Loss: 0.00058230
Iteration 13/25 | Loss: 0.00058230
Iteration 14/25 | Loss: 0.00058230
Iteration 15/25 | Loss: 0.00058230
Iteration 16/25 | Loss: 0.00058230
Iteration 17/25 | Loss: 0.00058230
Iteration 18/25 | Loss: 0.00058230
Iteration 19/25 | Loss: 0.00058230
Iteration 20/25 | Loss: 0.00058230
Iteration 21/25 | Loss: 0.00058230
Iteration 22/25 | Loss: 0.00058230
Iteration 23/25 | Loss: 0.00058230
Iteration 24/25 | Loss: 0.00058230
Iteration 25/25 | Loss: 0.00058230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005822964012622833, 0.0005822964012622833, 0.0005822964012622833, 0.0005822964012622833, 0.0005822964012622833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005822964012622833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058230
Iteration 2/1000 | Loss: 0.00005204
Iteration 3/1000 | Loss: 0.00003777
Iteration 4/1000 | Loss: 0.00003289
Iteration 5/1000 | Loss: 0.00003110
Iteration 6/1000 | Loss: 0.00002932
Iteration 7/1000 | Loss: 0.00002854
Iteration 8/1000 | Loss: 0.00002777
Iteration 9/1000 | Loss: 0.00002738
Iteration 10/1000 | Loss: 0.00002713
Iteration 11/1000 | Loss: 0.00002688
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002635
Iteration 14/1000 | Loss: 0.00002607
Iteration 15/1000 | Loss: 0.00002587
Iteration 16/1000 | Loss: 0.00002576
Iteration 17/1000 | Loss: 0.00002575
Iteration 18/1000 | Loss: 0.00002574
Iteration 19/1000 | Loss: 0.00002572
Iteration 20/1000 | Loss: 0.00002559
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002534
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002533
Iteration 26/1000 | Loss: 0.00002532
Iteration 27/1000 | Loss: 0.00002531
Iteration 28/1000 | Loss: 0.00002531
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002529
Iteration 31/1000 | Loss: 0.00002529
Iteration 32/1000 | Loss: 0.00002529
Iteration 33/1000 | Loss: 0.00002529
Iteration 34/1000 | Loss: 0.00002529
Iteration 35/1000 | Loss: 0.00002528
Iteration 36/1000 | Loss: 0.00002528
Iteration 37/1000 | Loss: 0.00002528
Iteration 38/1000 | Loss: 0.00002527
Iteration 39/1000 | Loss: 0.00002527
Iteration 40/1000 | Loss: 0.00002527
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002525
Iteration 43/1000 | Loss: 0.00002525
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00002525
Iteration 46/1000 | Loss: 0.00002524
Iteration 47/1000 | Loss: 0.00002524
Iteration 48/1000 | Loss: 0.00002524
Iteration 49/1000 | Loss: 0.00002524
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002524
Iteration 54/1000 | Loss: 0.00002523
Iteration 55/1000 | Loss: 0.00002523
Iteration 56/1000 | Loss: 0.00002523
Iteration 57/1000 | Loss: 0.00002523
Iteration 58/1000 | Loss: 0.00002522
Iteration 59/1000 | Loss: 0.00002522
Iteration 60/1000 | Loss: 0.00002522
Iteration 61/1000 | Loss: 0.00002522
Iteration 62/1000 | Loss: 0.00002522
Iteration 63/1000 | Loss: 0.00002522
Iteration 64/1000 | Loss: 0.00002522
Iteration 65/1000 | Loss: 0.00002522
Iteration 66/1000 | Loss: 0.00002522
Iteration 67/1000 | Loss: 0.00002522
Iteration 68/1000 | Loss: 0.00002522
Iteration 69/1000 | Loss: 0.00002521
Iteration 70/1000 | Loss: 0.00002521
Iteration 71/1000 | Loss: 0.00002521
Iteration 72/1000 | Loss: 0.00002521
Iteration 73/1000 | Loss: 0.00002521
Iteration 74/1000 | Loss: 0.00002521
Iteration 75/1000 | Loss: 0.00002521
Iteration 76/1000 | Loss: 0.00002521
Iteration 77/1000 | Loss: 0.00002521
Iteration 78/1000 | Loss: 0.00002520
Iteration 79/1000 | Loss: 0.00002520
Iteration 80/1000 | Loss: 0.00002519
Iteration 81/1000 | Loss: 0.00002519
Iteration 82/1000 | Loss: 0.00002519
Iteration 83/1000 | Loss: 0.00002519
Iteration 84/1000 | Loss: 0.00002519
Iteration 85/1000 | Loss: 0.00002519
Iteration 86/1000 | Loss: 0.00002519
Iteration 87/1000 | Loss: 0.00002518
Iteration 88/1000 | Loss: 0.00002518
Iteration 89/1000 | Loss: 0.00002518
Iteration 90/1000 | Loss: 0.00002518
Iteration 91/1000 | Loss: 0.00002518
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002517
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002516
Iteration 102/1000 | Loss: 0.00002516
Iteration 103/1000 | Loss: 0.00002516
Iteration 104/1000 | Loss: 0.00002516
Iteration 105/1000 | Loss: 0.00002516
Iteration 106/1000 | Loss: 0.00002516
Iteration 107/1000 | Loss: 0.00002516
Iteration 108/1000 | Loss: 0.00002516
Iteration 109/1000 | Loss: 0.00002516
Iteration 110/1000 | Loss: 0.00002516
Iteration 111/1000 | Loss: 0.00002516
Iteration 112/1000 | Loss: 0.00002515
Iteration 113/1000 | Loss: 0.00002515
Iteration 114/1000 | Loss: 0.00002515
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002515
Iteration 117/1000 | Loss: 0.00002515
Iteration 118/1000 | Loss: 0.00002514
Iteration 119/1000 | Loss: 0.00002514
Iteration 120/1000 | Loss: 0.00002514
Iteration 121/1000 | Loss: 0.00002514
Iteration 122/1000 | Loss: 0.00002514
Iteration 123/1000 | Loss: 0.00002513
Iteration 124/1000 | Loss: 0.00002513
Iteration 125/1000 | Loss: 0.00002513
Iteration 126/1000 | Loss: 0.00002513
Iteration 127/1000 | Loss: 0.00002513
Iteration 128/1000 | Loss: 0.00002513
Iteration 129/1000 | Loss: 0.00002513
Iteration 130/1000 | Loss: 0.00002513
Iteration 131/1000 | Loss: 0.00002513
Iteration 132/1000 | Loss: 0.00002513
Iteration 133/1000 | Loss: 0.00002512
Iteration 134/1000 | Loss: 0.00002512
Iteration 135/1000 | Loss: 0.00002512
Iteration 136/1000 | Loss: 0.00002512
Iteration 137/1000 | Loss: 0.00002512
Iteration 138/1000 | Loss: 0.00002512
Iteration 139/1000 | Loss: 0.00002512
Iteration 140/1000 | Loss: 0.00002512
Iteration 141/1000 | Loss: 0.00002512
Iteration 142/1000 | Loss: 0.00002511
Iteration 143/1000 | Loss: 0.00002511
Iteration 144/1000 | Loss: 0.00002511
Iteration 145/1000 | Loss: 0.00002511
Iteration 146/1000 | Loss: 0.00002511
Iteration 147/1000 | Loss: 0.00002511
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002510
Iteration 150/1000 | Loss: 0.00002510
Iteration 151/1000 | Loss: 0.00002510
Iteration 152/1000 | Loss: 0.00002510
Iteration 153/1000 | Loss: 0.00002510
Iteration 154/1000 | Loss: 0.00002510
Iteration 155/1000 | Loss: 0.00002510
Iteration 156/1000 | Loss: 0.00002510
Iteration 157/1000 | Loss: 0.00002510
Iteration 158/1000 | Loss: 0.00002510
Iteration 159/1000 | Loss: 0.00002510
Iteration 160/1000 | Loss: 0.00002510
Iteration 161/1000 | Loss: 0.00002510
Iteration 162/1000 | Loss: 0.00002510
Iteration 163/1000 | Loss: 0.00002510
Iteration 164/1000 | Loss: 0.00002510
Iteration 165/1000 | Loss: 0.00002510
Iteration 166/1000 | Loss: 0.00002510
Iteration 167/1000 | Loss: 0.00002510
Iteration 168/1000 | Loss: 0.00002510
Iteration 169/1000 | Loss: 0.00002510
Iteration 170/1000 | Loss: 0.00002510
Iteration 171/1000 | Loss: 0.00002510
Iteration 172/1000 | Loss: 0.00002510
Iteration 173/1000 | Loss: 0.00002510
Iteration 174/1000 | Loss: 0.00002510
Iteration 175/1000 | Loss: 0.00002510
Iteration 176/1000 | Loss: 0.00002510
Iteration 177/1000 | Loss: 0.00002510
Iteration 178/1000 | Loss: 0.00002510
Iteration 179/1000 | Loss: 0.00002510
Iteration 180/1000 | Loss: 0.00002510
Iteration 181/1000 | Loss: 0.00002510
Iteration 182/1000 | Loss: 0.00002510
Iteration 183/1000 | Loss: 0.00002510
Iteration 184/1000 | Loss: 0.00002510
Iteration 185/1000 | Loss: 0.00002510
Iteration 186/1000 | Loss: 0.00002510
Iteration 187/1000 | Loss: 0.00002510
Iteration 188/1000 | Loss: 0.00002510
Iteration 189/1000 | Loss: 0.00002510
Iteration 190/1000 | Loss: 0.00002510
Iteration 191/1000 | Loss: 0.00002510
Iteration 192/1000 | Loss: 0.00002510
Iteration 193/1000 | Loss: 0.00002510
Iteration 194/1000 | Loss: 0.00002510
Iteration 195/1000 | Loss: 0.00002510
Iteration 196/1000 | Loss: 0.00002510
Iteration 197/1000 | Loss: 0.00002510
Iteration 198/1000 | Loss: 0.00002510
Iteration 199/1000 | Loss: 0.00002510
Iteration 200/1000 | Loss: 0.00002510
Iteration 201/1000 | Loss: 0.00002510
Iteration 202/1000 | Loss: 0.00002510
Iteration 203/1000 | Loss: 0.00002510
Iteration 204/1000 | Loss: 0.00002510
Iteration 205/1000 | Loss: 0.00002510
Iteration 206/1000 | Loss: 0.00002510
Iteration 207/1000 | Loss: 0.00002510
Iteration 208/1000 | Loss: 0.00002510
Iteration 209/1000 | Loss: 0.00002510
Iteration 210/1000 | Loss: 0.00002510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.5095325327129103e-05, 2.5095325327129103e-05, 2.5095325327129103e-05, 2.5095325327129103e-05, 2.5095325327129103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5095325327129103e-05

Optimization complete. Final v2v error: 4.080106735229492 mm

Highest mean error: 5.558300971984863 mm for frame 19

Lowest mean error: 3.201319456100464 mm for frame 90

Saving results

Total time: 47.57662534713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096231
Iteration 2/25 | Loss: 0.00199006
Iteration 3/25 | Loss: 0.00118227
Iteration 4/25 | Loss: 0.00108050
Iteration 5/25 | Loss: 0.00103647
Iteration 6/25 | Loss: 0.00104427
Iteration 7/25 | Loss: 0.00097765
Iteration 8/25 | Loss: 0.00094748
Iteration 9/25 | Loss: 0.00093885
Iteration 10/25 | Loss: 0.00092251
Iteration 11/25 | Loss: 0.00091704
Iteration 12/25 | Loss: 0.00091391
Iteration 13/25 | Loss: 0.00091371
Iteration 14/25 | Loss: 0.00090798
Iteration 15/25 | Loss: 0.00091082
Iteration 16/25 | Loss: 0.00090884
Iteration 17/25 | Loss: 0.00090774
Iteration 18/25 | Loss: 0.00090578
Iteration 19/25 | Loss: 0.00090552
Iteration 20/25 | Loss: 0.00090552
Iteration 21/25 | Loss: 0.00090539
Iteration 22/25 | Loss: 0.00090530
Iteration 23/25 | Loss: 0.00090963
Iteration 24/25 | Loss: 0.00090514
Iteration 25/25 | Loss: 0.00090660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30916083
Iteration 2/25 | Loss: 0.00054792
Iteration 3/25 | Loss: 0.00054792
Iteration 4/25 | Loss: 0.00054792
Iteration 5/25 | Loss: 0.00054792
Iteration 6/25 | Loss: 0.00054792
Iteration 7/25 | Loss: 0.00054792
Iteration 8/25 | Loss: 0.00054792
Iteration 9/25 | Loss: 0.00054792
Iteration 10/25 | Loss: 0.00054792
Iteration 11/25 | Loss: 0.00054792
Iteration 12/25 | Loss: 0.00054792
Iteration 13/25 | Loss: 0.00054792
Iteration 14/25 | Loss: 0.00054792
Iteration 15/25 | Loss: 0.00054792
Iteration 16/25 | Loss: 0.00054792
Iteration 17/25 | Loss: 0.00054792
Iteration 18/25 | Loss: 0.00054792
Iteration 19/25 | Loss: 0.00054792
Iteration 20/25 | Loss: 0.00054792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005479174433276057, 0.0005479174433276057, 0.0005479174433276057, 0.0005479174433276057, 0.0005479174433276057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005479174433276057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054792
Iteration 2/1000 | Loss: 0.00004525
Iteration 3/1000 | Loss: 0.00004689
Iteration 4/1000 | Loss: 0.00002795
Iteration 5/1000 | Loss: 0.00002655
Iteration 6/1000 | Loss: 0.00004841
Iteration 7/1000 | Loss: 0.00002972
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002364
Iteration 11/1000 | Loss: 0.00005200
Iteration 12/1000 | Loss: 0.00002560
Iteration 13/1000 | Loss: 0.00002331
Iteration 14/1000 | Loss: 0.00002300
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00002291
Iteration 17/1000 | Loss: 0.00002291
Iteration 18/1000 | Loss: 0.00002284
Iteration 19/1000 | Loss: 0.00002270
Iteration 20/1000 | Loss: 0.00002270
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002267
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002267
Iteration 27/1000 | Loss: 0.00002266
Iteration 28/1000 | Loss: 0.00002266
Iteration 29/1000 | Loss: 0.00002265
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002264
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002261
Iteration 40/1000 | Loss: 0.00002260
Iteration 41/1000 | Loss: 0.00002260
Iteration 42/1000 | Loss: 0.00002260
Iteration 43/1000 | Loss: 0.00002259
Iteration 44/1000 | Loss: 0.00002259
Iteration 45/1000 | Loss: 0.00002259
Iteration 46/1000 | Loss: 0.00002259
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002258
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002257
Iteration 54/1000 | Loss: 0.00002257
Iteration 55/1000 | Loss: 0.00002257
Iteration 56/1000 | Loss: 0.00002257
Iteration 57/1000 | Loss: 0.00002257
Iteration 58/1000 | Loss: 0.00002257
Iteration 59/1000 | Loss: 0.00002256
Iteration 60/1000 | Loss: 0.00002256
Iteration 61/1000 | Loss: 0.00002256
Iteration 62/1000 | Loss: 0.00002256
Iteration 63/1000 | Loss: 0.00002256
Iteration 64/1000 | Loss: 0.00002256
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002256
Iteration 67/1000 | Loss: 0.00002256
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002254
Iteration 73/1000 | Loss: 0.00002254
Iteration 74/1000 | Loss: 0.00002254
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002253
Iteration 77/1000 | Loss: 0.00002253
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002252
Iteration 87/1000 | Loss: 0.00002252
Iteration 88/1000 | Loss: 0.00002252
Iteration 89/1000 | Loss: 0.00004726
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002251
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002248
Iteration 94/1000 | Loss: 0.00002248
Iteration 95/1000 | Loss: 0.00002248
Iteration 96/1000 | Loss: 0.00002248
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002247
Iteration 101/1000 | Loss: 0.00002247
Iteration 102/1000 | Loss: 0.00002247
Iteration 103/1000 | Loss: 0.00002247
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002245
Iteration 110/1000 | Loss: 0.00002245
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002245
Iteration 121/1000 | Loss: 0.00002245
Iteration 122/1000 | Loss: 0.00002245
Iteration 123/1000 | Loss: 0.00002245
Iteration 124/1000 | Loss: 0.00002244
Iteration 125/1000 | Loss: 0.00002244
Iteration 126/1000 | Loss: 0.00002244
Iteration 127/1000 | Loss: 0.00002244
Iteration 128/1000 | Loss: 0.00002244
Iteration 129/1000 | Loss: 0.00002244
Iteration 130/1000 | Loss: 0.00002244
Iteration 131/1000 | Loss: 0.00002244
Iteration 132/1000 | Loss: 0.00002244
Iteration 133/1000 | Loss: 0.00002244
Iteration 134/1000 | Loss: 0.00002244
Iteration 135/1000 | Loss: 0.00002244
Iteration 136/1000 | Loss: 0.00002244
Iteration 137/1000 | Loss: 0.00002244
Iteration 138/1000 | Loss: 0.00002244
Iteration 139/1000 | Loss: 0.00002244
Iteration 140/1000 | Loss: 0.00002244
Iteration 141/1000 | Loss: 0.00002243
Iteration 142/1000 | Loss: 0.00002243
Iteration 143/1000 | Loss: 0.00002243
Iteration 144/1000 | Loss: 0.00002243
Iteration 145/1000 | Loss: 0.00002243
Iteration 146/1000 | Loss: 0.00002243
Iteration 147/1000 | Loss: 0.00002243
Iteration 148/1000 | Loss: 0.00002243
Iteration 149/1000 | Loss: 0.00002243
Iteration 150/1000 | Loss: 0.00002243
Iteration 151/1000 | Loss: 0.00002243
Iteration 152/1000 | Loss: 0.00002243
Iteration 153/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.2432146579376422e-05, 2.2432146579376422e-05, 2.2432146579376422e-05, 2.2432146579376422e-05, 2.2432146579376422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2432146579376422e-05

Optimization complete. Final v2v error: 3.7143731117248535 mm

Highest mean error: 6.158888816833496 mm for frame 74

Lowest mean error: 2.7688961029052734 mm for frame 164

Saving results

Total time: 74.80455613136292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473041
Iteration 2/25 | Loss: 0.00095730
Iteration 3/25 | Loss: 0.00086202
Iteration 4/25 | Loss: 0.00084660
Iteration 5/25 | Loss: 0.00084214
Iteration 6/25 | Loss: 0.00084164
Iteration 7/25 | Loss: 0.00084164
Iteration 8/25 | Loss: 0.00084164
Iteration 9/25 | Loss: 0.00084164
Iteration 10/25 | Loss: 0.00084164
Iteration 11/25 | Loss: 0.00084164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008416404598392546, 0.0008416404598392546, 0.0008416404598392546, 0.0008416404598392546, 0.0008416404598392546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008416404598392546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97915220
Iteration 2/25 | Loss: 0.00052145
Iteration 3/25 | Loss: 0.00052145
Iteration 4/25 | Loss: 0.00052145
Iteration 5/25 | Loss: 0.00052145
Iteration 6/25 | Loss: 0.00052145
Iteration 7/25 | Loss: 0.00052145
Iteration 8/25 | Loss: 0.00052145
Iteration 9/25 | Loss: 0.00052144
Iteration 10/25 | Loss: 0.00052144
Iteration 11/25 | Loss: 0.00052144
Iteration 12/25 | Loss: 0.00052144
Iteration 13/25 | Loss: 0.00052144
Iteration 14/25 | Loss: 0.00052144
Iteration 15/25 | Loss: 0.00052144
Iteration 16/25 | Loss: 0.00052144
Iteration 17/25 | Loss: 0.00052144
Iteration 18/25 | Loss: 0.00052144
Iteration 19/25 | Loss: 0.00052144
Iteration 20/25 | Loss: 0.00052144
Iteration 21/25 | Loss: 0.00052144
Iteration 22/25 | Loss: 0.00052144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000521444424521178, 0.000521444424521178, 0.000521444424521178, 0.000521444424521178, 0.000521444424521178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000521444424521178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052144
Iteration 2/1000 | Loss: 0.00002104
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001522
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001467
Iteration 11/1000 | Loss: 0.00001466
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001434
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001428
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001403
Iteration 41/1000 | Loss: 0.00001403
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001400
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001400
Iteration 67/1000 | Loss: 0.00001400
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001400
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.4000331248098519e-05, 1.4000331248098519e-05, 1.4000331248098519e-05, 1.4000331248098519e-05, 1.4000331248098519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4000331248098519e-05

Optimization complete. Final v2v error: 3.197197675704956 mm

Highest mean error: 3.412108898162842 mm for frame 91

Lowest mean error: 3.055251121520996 mm for frame 32

Saving results

Total time: 34.113516092300415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480445
Iteration 2/25 | Loss: 0.00100564
Iteration 3/25 | Loss: 0.00087563
Iteration 4/25 | Loss: 0.00084997
Iteration 5/25 | Loss: 0.00084237
Iteration 6/25 | Loss: 0.00084079
Iteration 7/25 | Loss: 0.00084079
Iteration 8/25 | Loss: 0.00084079
Iteration 9/25 | Loss: 0.00084079
Iteration 10/25 | Loss: 0.00084079
Iteration 11/25 | Loss: 0.00084079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008407887071371078, 0.0008407887071371078, 0.0008407887071371078, 0.0008407887071371078, 0.0008407887071371078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008407887071371078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17452478
Iteration 2/25 | Loss: 0.00051687
Iteration 3/25 | Loss: 0.00051687
Iteration 4/25 | Loss: 0.00051687
Iteration 5/25 | Loss: 0.00051687
Iteration 6/25 | Loss: 0.00051687
Iteration 7/25 | Loss: 0.00051687
Iteration 8/25 | Loss: 0.00051687
Iteration 9/25 | Loss: 0.00051687
Iteration 10/25 | Loss: 0.00051687
Iteration 11/25 | Loss: 0.00051687
Iteration 12/25 | Loss: 0.00051687
Iteration 13/25 | Loss: 0.00051687
Iteration 14/25 | Loss: 0.00051687
Iteration 15/25 | Loss: 0.00051687
Iteration 16/25 | Loss: 0.00051687
Iteration 17/25 | Loss: 0.00051687
Iteration 18/25 | Loss: 0.00051687
Iteration 19/25 | Loss: 0.00051687
Iteration 20/25 | Loss: 0.00051687
Iteration 21/25 | Loss: 0.00051687
Iteration 22/25 | Loss: 0.00051687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005168695352040231, 0.0005168695352040231, 0.0005168695352040231, 0.0005168695352040231, 0.0005168695352040231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005168695352040231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051687
Iteration 2/1000 | Loss: 0.00002124
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00001696
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001544
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001494
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001455
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001387
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001387
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001386
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.3846231013303623e-05, 1.3846231013303623e-05, 1.3846231013303623e-05, 1.3846231013303623e-05, 1.3846231013303623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3846231013303623e-05

Optimization complete. Final v2v error: 3.1801342964172363 mm

Highest mean error: 3.4816370010375977 mm for frame 229

Lowest mean error: 2.9891984462738037 mm for frame 171

Saving results

Total time: 36.09737205505371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499230
Iteration 2/25 | Loss: 0.00110471
Iteration 3/25 | Loss: 0.00094690
Iteration 4/25 | Loss: 0.00092944
Iteration 5/25 | Loss: 0.00092611
Iteration 6/25 | Loss: 0.00092523
Iteration 7/25 | Loss: 0.00092506
Iteration 8/25 | Loss: 0.00092506
Iteration 9/25 | Loss: 0.00092506
Iteration 10/25 | Loss: 0.00092506
Iteration 11/25 | Loss: 0.00092506
Iteration 12/25 | Loss: 0.00092506
Iteration 13/25 | Loss: 0.00092506
Iteration 14/25 | Loss: 0.00092506
Iteration 15/25 | Loss: 0.00092506
Iteration 16/25 | Loss: 0.00092506
Iteration 17/25 | Loss: 0.00092506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009250627481378615, 0.0009250627481378615, 0.0009250627481378615, 0.0009250627481378615, 0.0009250627481378615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009250627481378615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50477326
Iteration 2/25 | Loss: 0.00061486
Iteration 3/25 | Loss: 0.00061484
Iteration 4/25 | Loss: 0.00061484
Iteration 5/25 | Loss: 0.00061484
Iteration 6/25 | Loss: 0.00061484
Iteration 7/25 | Loss: 0.00061484
Iteration 8/25 | Loss: 0.00061484
Iteration 9/25 | Loss: 0.00061484
Iteration 10/25 | Loss: 0.00061484
Iteration 11/25 | Loss: 0.00061484
Iteration 12/25 | Loss: 0.00061484
Iteration 13/25 | Loss: 0.00061484
Iteration 14/25 | Loss: 0.00061484
Iteration 15/25 | Loss: 0.00061484
Iteration 16/25 | Loss: 0.00061484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006148376269266009, 0.0006148376269266009, 0.0006148376269266009, 0.0006148376269266009, 0.0006148376269266009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006148376269266009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061484
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00002761
Iteration 4/1000 | Loss: 0.00002522
Iteration 5/1000 | Loss: 0.00002381
Iteration 6/1000 | Loss: 0.00002304
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002221
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002194
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002186
Iteration 13/1000 | Loss: 0.00002169
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002163
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002158
Iteration 18/1000 | Loss: 0.00002157
Iteration 19/1000 | Loss: 0.00002155
Iteration 20/1000 | Loss: 0.00002155
Iteration 21/1000 | Loss: 0.00002152
Iteration 22/1000 | Loss: 0.00002151
Iteration 23/1000 | Loss: 0.00002148
Iteration 24/1000 | Loss: 0.00002147
Iteration 25/1000 | Loss: 0.00002146
Iteration 26/1000 | Loss: 0.00002146
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002143
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002140
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002140
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002139
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002138
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002131
Iteration 72/1000 | Loss: 0.00002131
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002131
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002130
Iteration 77/1000 | Loss: 0.00002130
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002130
Iteration 80/1000 | Loss: 0.00002130
Iteration 81/1000 | Loss: 0.00002130
Iteration 82/1000 | Loss: 0.00002129
Iteration 83/1000 | Loss: 0.00002129
Iteration 84/1000 | Loss: 0.00002129
Iteration 85/1000 | Loss: 0.00002129
Iteration 86/1000 | Loss: 0.00002129
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002129
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002128
Iteration 98/1000 | Loss: 0.00002128
Iteration 99/1000 | Loss: 0.00002128
Iteration 100/1000 | Loss: 0.00002128
Iteration 101/1000 | Loss: 0.00002128
Iteration 102/1000 | Loss: 0.00002128
Iteration 103/1000 | Loss: 0.00002127
Iteration 104/1000 | Loss: 0.00002127
Iteration 105/1000 | Loss: 0.00002127
Iteration 106/1000 | Loss: 0.00002127
Iteration 107/1000 | Loss: 0.00002127
Iteration 108/1000 | Loss: 0.00002126
Iteration 109/1000 | Loss: 0.00002126
Iteration 110/1000 | Loss: 0.00002126
Iteration 111/1000 | Loss: 0.00002126
Iteration 112/1000 | Loss: 0.00002126
Iteration 113/1000 | Loss: 0.00002126
Iteration 114/1000 | Loss: 0.00002126
Iteration 115/1000 | Loss: 0.00002126
Iteration 116/1000 | Loss: 0.00002126
Iteration 117/1000 | Loss: 0.00002125
Iteration 118/1000 | Loss: 0.00002125
Iteration 119/1000 | Loss: 0.00002125
Iteration 120/1000 | Loss: 0.00002125
Iteration 121/1000 | Loss: 0.00002125
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002122
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002122
Iteration 145/1000 | Loss: 0.00002122
Iteration 146/1000 | Loss: 0.00002122
Iteration 147/1000 | Loss: 0.00002122
Iteration 148/1000 | Loss: 0.00002122
Iteration 149/1000 | Loss: 0.00002122
Iteration 150/1000 | Loss: 0.00002122
Iteration 151/1000 | Loss: 0.00002122
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002121
Iteration 155/1000 | Loss: 0.00002121
Iteration 156/1000 | Loss: 0.00002121
Iteration 157/1000 | Loss: 0.00002121
Iteration 158/1000 | Loss: 0.00002121
Iteration 159/1000 | Loss: 0.00002121
Iteration 160/1000 | Loss: 0.00002121
Iteration 161/1000 | Loss: 0.00002121
Iteration 162/1000 | Loss: 0.00002121
Iteration 163/1000 | Loss: 0.00002121
Iteration 164/1000 | Loss: 0.00002121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.1209481928963214e-05, 2.1209481928963214e-05, 2.1209481928963214e-05, 2.1209481928963214e-05, 2.1209481928963214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1209481928963214e-05

Optimization complete. Final v2v error: 3.7360763549804688 mm

Highest mean error: 4.702789783477783 mm for frame 131

Lowest mean error: 3.016301155090332 mm for frame 4

Saving results

Total time: 36.35318660736084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379002
Iteration 2/25 | Loss: 0.00111811
Iteration 3/25 | Loss: 0.00091603
Iteration 4/25 | Loss: 0.00086566
Iteration 5/25 | Loss: 0.00085333
Iteration 6/25 | Loss: 0.00085100
Iteration 7/25 | Loss: 0.00085057
Iteration 8/25 | Loss: 0.00085057
Iteration 9/25 | Loss: 0.00085057
Iteration 10/25 | Loss: 0.00085057
Iteration 11/25 | Loss: 0.00085057
Iteration 12/25 | Loss: 0.00085057
Iteration 13/25 | Loss: 0.00085057
Iteration 14/25 | Loss: 0.00085057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008505743462592363, 0.0008505743462592363, 0.0008505743462592363, 0.0008505743462592363, 0.0008505743462592363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008505743462592363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50012624
Iteration 2/25 | Loss: 0.00049734
Iteration 3/25 | Loss: 0.00049733
Iteration 4/25 | Loss: 0.00049733
Iteration 5/25 | Loss: 0.00049733
Iteration 6/25 | Loss: 0.00049733
Iteration 7/25 | Loss: 0.00049733
Iteration 8/25 | Loss: 0.00049733
Iteration 9/25 | Loss: 0.00049733
Iteration 10/25 | Loss: 0.00049733
Iteration 11/25 | Loss: 0.00049733
Iteration 12/25 | Loss: 0.00049733
Iteration 13/25 | Loss: 0.00049733
Iteration 14/25 | Loss: 0.00049733
Iteration 15/25 | Loss: 0.00049733
Iteration 16/25 | Loss: 0.00049733
Iteration 17/25 | Loss: 0.00049733
Iteration 18/25 | Loss: 0.00049733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004973316681571305, 0.0004973316681571305, 0.0004973316681571305, 0.0004973316681571305, 0.0004973316681571305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004973316681571305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049733
Iteration 2/1000 | Loss: 0.00003522
Iteration 3/1000 | Loss: 0.00002479
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00001988
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001911
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001856
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001836
Iteration 16/1000 | Loss: 0.00001834
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001805
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001802
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001793
Iteration 46/1000 | Loss: 0.00001792
Iteration 47/1000 | Loss: 0.00001791
Iteration 48/1000 | Loss: 0.00001790
Iteration 49/1000 | Loss: 0.00001790
Iteration 50/1000 | Loss: 0.00001789
Iteration 51/1000 | Loss: 0.00001789
Iteration 52/1000 | Loss: 0.00001789
Iteration 53/1000 | Loss: 0.00001789
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001788
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001787
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001786
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001780
Iteration 102/1000 | Loss: 0.00001780
Iteration 103/1000 | Loss: 0.00001780
Iteration 104/1000 | Loss: 0.00001780
Iteration 105/1000 | Loss: 0.00001780
Iteration 106/1000 | Loss: 0.00001780
Iteration 107/1000 | Loss: 0.00001780
Iteration 108/1000 | Loss: 0.00001780
Iteration 109/1000 | Loss: 0.00001780
Iteration 110/1000 | Loss: 0.00001780
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001780
Iteration 115/1000 | Loss: 0.00001780
Iteration 116/1000 | Loss: 0.00001780
Iteration 117/1000 | Loss: 0.00001780
Iteration 118/1000 | Loss: 0.00001780
Iteration 119/1000 | Loss: 0.00001780
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.7798678527469747e-05, 1.7798678527469747e-05, 1.7798678527469747e-05, 1.7798678527469747e-05, 1.7798678527469747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7798678527469747e-05

Optimization complete. Final v2v error: 3.5150794982910156 mm

Highest mean error: 3.728544235229492 mm for frame 175

Lowest mean error: 3.298119068145752 mm for frame 145

Saving results

Total time: 39.16598057746887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001643
Iteration 2/25 | Loss: 0.00260957
Iteration 3/25 | Loss: 0.00190974
Iteration 4/25 | Loss: 0.00171346
Iteration 5/25 | Loss: 0.00172006
Iteration 6/25 | Loss: 0.00156285
Iteration 7/25 | Loss: 0.00143626
Iteration 8/25 | Loss: 0.00134463
Iteration 9/25 | Loss: 0.00128276
Iteration 10/25 | Loss: 0.00125215
Iteration 11/25 | Loss: 0.00126192
Iteration 12/25 | Loss: 0.00126498
Iteration 13/25 | Loss: 0.00122804
Iteration 14/25 | Loss: 0.00117079
Iteration 15/25 | Loss: 0.00112553
Iteration 16/25 | Loss: 0.00111575
Iteration 17/25 | Loss: 0.00110706
Iteration 18/25 | Loss: 0.00112149
Iteration 19/25 | Loss: 0.00108537
Iteration 20/25 | Loss: 0.00106444
Iteration 21/25 | Loss: 0.00104739
Iteration 22/25 | Loss: 0.00105209
Iteration 23/25 | Loss: 0.00105210
Iteration 24/25 | Loss: 0.00107177
Iteration 25/25 | Loss: 0.00102072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52880013
Iteration 2/25 | Loss: 0.00140764
Iteration 3/25 | Loss: 0.00140762
Iteration 4/25 | Loss: 0.00140762
Iteration 5/25 | Loss: 0.00140762
Iteration 6/25 | Loss: 0.00140762
Iteration 7/25 | Loss: 0.00140762
Iteration 8/25 | Loss: 0.00140762
Iteration 9/25 | Loss: 0.00140762
Iteration 10/25 | Loss: 0.00140762
Iteration 11/25 | Loss: 0.00140762
Iteration 12/25 | Loss: 0.00140762
Iteration 13/25 | Loss: 0.00140762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014076163060963154, 0.0014076163060963154, 0.0014076163060963154, 0.0014076163060963154, 0.0014076163060963154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014076163060963154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140762
Iteration 2/1000 | Loss: 0.00057794
Iteration 3/1000 | Loss: 0.00045625
Iteration 4/1000 | Loss: 0.00013304
Iteration 5/1000 | Loss: 0.00008247
Iteration 6/1000 | Loss: 0.00005474
Iteration 7/1000 | Loss: 0.00016943
Iteration 8/1000 | Loss: 0.00011824
Iteration 9/1000 | Loss: 0.00003738
Iteration 10/1000 | Loss: 0.00003424
Iteration 11/1000 | Loss: 0.00003189
Iteration 12/1000 | Loss: 0.00002963
Iteration 13/1000 | Loss: 0.00046184
Iteration 14/1000 | Loss: 0.00013757
Iteration 15/1000 | Loss: 0.00003719
Iteration 16/1000 | Loss: 0.00002679
Iteration 17/1000 | Loss: 0.00002445
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00002218
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00020316
Iteration 22/1000 | Loss: 0.00018113
Iteration 23/1000 | Loss: 0.00011337
Iteration 24/1000 | Loss: 0.00004719
Iteration 25/1000 | Loss: 0.00002943
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00001993
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001798
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001793
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00013886
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00017540
Iteration 41/1000 | Loss: 0.00002501
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001728
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001724
Iteration 95/1000 | Loss: 0.00001724
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001723
Iteration 98/1000 | Loss: 0.00001723
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001722
Iteration 103/1000 | Loss: 0.00001722
Iteration 104/1000 | Loss: 0.00001721
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001721
Iteration 108/1000 | Loss: 0.00001721
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001720
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001717
Iteration 122/1000 | Loss: 0.00001717
Iteration 123/1000 | Loss: 0.00001717
Iteration 124/1000 | Loss: 0.00001717
Iteration 125/1000 | Loss: 0.00001717
Iteration 126/1000 | Loss: 0.00001717
Iteration 127/1000 | Loss: 0.00001717
Iteration 128/1000 | Loss: 0.00001717
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00001717
Iteration 132/1000 | Loss: 0.00001716
Iteration 133/1000 | Loss: 0.00001716
Iteration 134/1000 | Loss: 0.00001716
Iteration 135/1000 | Loss: 0.00001716
Iteration 136/1000 | Loss: 0.00001716
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001714
Iteration 155/1000 | Loss: 0.00001714
Iteration 156/1000 | Loss: 0.00001714
Iteration 157/1000 | Loss: 0.00001714
Iteration 158/1000 | Loss: 0.00001714
Iteration 159/1000 | Loss: 0.00001714
Iteration 160/1000 | Loss: 0.00001714
Iteration 161/1000 | Loss: 0.00001714
Iteration 162/1000 | Loss: 0.00001714
Iteration 163/1000 | Loss: 0.00001714
Iteration 164/1000 | Loss: 0.00001714
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001714
Iteration 168/1000 | Loss: 0.00001713
Iteration 169/1000 | Loss: 0.00001713
Iteration 170/1000 | Loss: 0.00001713
Iteration 171/1000 | Loss: 0.00001713
Iteration 172/1000 | Loss: 0.00001713
Iteration 173/1000 | Loss: 0.00001713
Iteration 174/1000 | Loss: 0.00001713
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001713
Iteration 183/1000 | Loss: 0.00001713
Iteration 184/1000 | Loss: 0.00001713
Iteration 185/1000 | Loss: 0.00001713
Iteration 186/1000 | Loss: 0.00001713
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001712
Iteration 190/1000 | Loss: 0.00001712
Iteration 191/1000 | Loss: 0.00001712
Iteration 192/1000 | Loss: 0.00001712
Iteration 193/1000 | Loss: 0.00001712
Iteration 194/1000 | Loss: 0.00001712
Iteration 195/1000 | Loss: 0.00001712
Iteration 196/1000 | Loss: 0.00001712
Iteration 197/1000 | Loss: 0.00001712
Iteration 198/1000 | Loss: 0.00001712
Iteration 199/1000 | Loss: 0.00001712
Iteration 200/1000 | Loss: 0.00001712
Iteration 201/1000 | Loss: 0.00001712
Iteration 202/1000 | Loss: 0.00001712
Iteration 203/1000 | Loss: 0.00001712
Iteration 204/1000 | Loss: 0.00001712
Iteration 205/1000 | Loss: 0.00001712
Iteration 206/1000 | Loss: 0.00001712
Iteration 207/1000 | Loss: 0.00001712
Iteration 208/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.712234370643273e-05, 1.712234370643273e-05, 1.712234370643273e-05, 1.712234370643273e-05, 1.712234370643273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.712234370643273e-05

Optimization complete. Final v2v error: 3.446168899536133 mm

Highest mean error: 4.435539245605469 mm for frame 86

Lowest mean error: 3.0560054779052734 mm for frame 163

Saving results

Total time: 134.04706358909607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409922
Iteration 2/25 | Loss: 0.00099144
Iteration 3/25 | Loss: 0.00085262
Iteration 4/25 | Loss: 0.00083040
Iteration 5/25 | Loss: 0.00082019
Iteration 6/25 | Loss: 0.00081670
Iteration 7/25 | Loss: 0.00081561
Iteration 8/25 | Loss: 0.00081530
Iteration 9/25 | Loss: 0.00081525
Iteration 10/25 | Loss: 0.00081525
Iteration 11/25 | Loss: 0.00081525
Iteration 12/25 | Loss: 0.00081525
Iteration 13/25 | Loss: 0.00081525
Iteration 14/25 | Loss: 0.00081525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008152537629939616, 0.0008152537629939616, 0.0008152537629939616, 0.0008152537629939616, 0.0008152537629939616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008152537629939616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51060343
Iteration 2/25 | Loss: 0.00051602
Iteration 3/25 | Loss: 0.00051601
Iteration 4/25 | Loss: 0.00051601
Iteration 5/25 | Loss: 0.00051601
Iteration 6/25 | Loss: 0.00051601
Iteration 7/25 | Loss: 0.00051601
Iteration 8/25 | Loss: 0.00051601
Iteration 9/25 | Loss: 0.00051601
Iteration 10/25 | Loss: 0.00051601
Iteration 11/25 | Loss: 0.00051601
Iteration 12/25 | Loss: 0.00051601
Iteration 13/25 | Loss: 0.00051601
Iteration 14/25 | Loss: 0.00051601
Iteration 15/25 | Loss: 0.00051601
Iteration 16/25 | Loss: 0.00051601
Iteration 17/25 | Loss: 0.00051601
Iteration 18/25 | Loss: 0.00051601
Iteration 19/25 | Loss: 0.00051601
Iteration 20/25 | Loss: 0.00051601
Iteration 21/25 | Loss: 0.00051601
Iteration 22/25 | Loss: 0.00051601
Iteration 23/25 | Loss: 0.00051601
Iteration 24/25 | Loss: 0.00051601
Iteration 25/25 | Loss: 0.00051601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051601
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001199
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001195
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001162
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001148
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001145
Iteration 142/1000 | Loss: 0.00001145
Iteration 143/1000 | Loss: 0.00001145
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001141
Iteration 170/1000 | Loss: 0.00001141
Iteration 171/1000 | Loss: 0.00001141
Iteration 172/1000 | Loss: 0.00001141
Iteration 173/1000 | Loss: 0.00001141
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001140
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001138
Iteration 186/1000 | Loss: 0.00001138
Iteration 187/1000 | Loss: 0.00001138
Iteration 188/1000 | Loss: 0.00001138
Iteration 189/1000 | Loss: 0.00001138
Iteration 190/1000 | Loss: 0.00001138
Iteration 191/1000 | Loss: 0.00001138
Iteration 192/1000 | Loss: 0.00001138
Iteration 193/1000 | Loss: 0.00001138
Iteration 194/1000 | Loss: 0.00001137
Iteration 195/1000 | Loss: 0.00001137
Iteration 196/1000 | Loss: 0.00001137
Iteration 197/1000 | Loss: 0.00001137
Iteration 198/1000 | Loss: 0.00001137
Iteration 199/1000 | Loss: 0.00001137
Iteration 200/1000 | Loss: 0.00001136
Iteration 201/1000 | Loss: 0.00001136
Iteration 202/1000 | Loss: 0.00001136
Iteration 203/1000 | Loss: 0.00001136
Iteration 204/1000 | Loss: 0.00001136
Iteration 205/1000 | Loss: 0.00001136
Iteration 206/1000 | Loss: 0.00001136
Iteration 207/1000 | Loss: 0.00001136
Iteration 208/1000 | Loss: 0.00001136
Iteration 209/1000 | Loss: 0.00001136
Iteration 210/1000 | Loss: 0.00001136
Iteration 211/1000 | Loss: 0.00001136
Iteration 212/1000 | Loss: 0.00001136
Iteration 213/1000 | Loss: 0.00001136
Iteration 214/1000 | Loss: 0.00001136
Iteration 215/1000 | Loss: 0.00001136
Iteration 216/1000 | Loss: 0.00001136
Iteration 217/1000 | Loss: 0.00001136
Iteration 218/1000 | Loss: 0.00001136
Iteration 219/1000 | Loss: 0.00001136
Iteration 220/1000 | Loss: 0.00001136
Iteration 221/1000 | Loss: 0.00001136
Iteration 222/1000 | Loss: 0.00001135
Iteration 223/1000 | Loss: 0.00001135
Iteration 224/1000 | Loss: 0.00001135
Iteration 225/1000 | Loss: 0.00001135
Iteration 226/1000 | Loss: 0.00001135
Iteration 227/1000 | Loss: 0.00001135
Iteration 228/1000 | Loss: 0.00001135
Iteration 229/1000 | Loss: 0.00001135
Iteration 230/1000 | Loss: 0.00001135
Iteration 231/1000 | Loss: 0.00001135
Iteration 232/1000 | Loss: 0.00001135
Iteration 233/1000 | Loss: 0.00001134
Iteration 234/1000 | Loss: 0.00001134
Iteration 235/1000 | Loss: 0.00001134
Iteration 236/1000 | Loss: 0.00001134
Iteration 237/1000 | Loss: 0.00001134
Iteration 238/1000 | Loss: 0.00001134
Iteration 239/1000 | Loss: 0.00001134
Iteration 240/1000 | Loss: 0.00001134
Iteration 241/1000 | Loss: 0.00001134
Iteration 242/1000 | Loss: 0.00001134
Iteration 243/1000 | Loss: 0.00001134
Iteration 244/1000 | Loss: 0.00001134
Iteration 245/1000 | Loss: 0.00001134
Iteration 246/1000 | Loss: 0.00001134
Iteration 247/1000 | Loss: 0.00001134
Iteration 248/1000 | Loss: 0.00001134
Iteration 249/1000 | Loss: 0.00001134
Iteration 250/1000 | Loss: 0.00001134
Iteration 251/1000 | Loss: 0.00001134
Iteration 252/1000 | Loss: 0.00001134
Iteration 253/1000 | Loss: 0.00001134
Iteration 254/1000 | Loss: 0.00001134
Iteration 255/1000 | Loss: 0.00001134
Iteration 256/1000 | Loss: 0.00001134
Iteration 257/1000 | Loss: 0.00001134
Iteration 258/1000 | Loss: 0.00001134
Iteration 259/1000 | Loss: 0.00001134
Iteration 260/1000 | Loss: 0.00001134
Iteration 261/1000 | Loss: 0.00001134
Iteration 262/1000 | Loss: 0.00001134
Iteration 263/1000 | Loss: 0.00001134
Iteration 264/1000 | Loss: 0.00001134
Iteration 265/1000 | Loss: 0.00001134
Iteration 266/1000 | Loss: 0.00001134
Iteration 267/1000 | Loss: 0.00001134
Iteration 268/1000 | Loss: 0.00001134
Iteration 269/1000 | Loss: 0.00001134
Iteration 270/1000 | Loss: 0.00001134
Iteration 271/1000 | Loss: 0.00001134
Iteration 272/1000 | Loss: 0.00001134
Iteration 273/1000 | Loss: 0.00001134
Iteration 274/1000 | Loss: 0.00001134
Iteration 275/1000 | Loss: 0.00001134
Iteration 276/1000 | Loss: 0.00001134
Iteration 277/1000 | Loss: 0.00001134
Iteration 278/1000 | Loss: 0.00001134
Iteration 279/1000 | Loss: 0.00001134
Iteration 280/1000 | Loss: 0.00001134
Iteration 281/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.1342759535182267e-05, 1.1342759535182267e-05, 1.1342759535182267e-05, 1.1342759535182267e-05, 1.1342759535182267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1342759535182267e-05

Optimization complete. Final v2v error: 2.8602492809295654 mm

Highest mean error: 3.747870683670044 mm for frame 28

Lowest mean error: 2.5827789306640625 mm for frame 88

Saving results

Total time: 44.33794450759888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898673
Iteration 2/25 | Loss: 0.00144855
Iteration 3/25 | Loss: 0.00107919
Iteration 4/25 | Loss: 0.00102956
Iteration 5/25 | Loss: 0.00098339
Iteration 6/25 | Loss: 0.00098712
Iteration 7/25 | Loss: 0.00097212
Iteration 8/25 | Loss: 0.00096094
Iteration 9/25 | Loss: 0.00096130
Iteration 10/25 | Loss: 0.00095643
Iteration 11/25 | Loss: 0.00095367
Iteration 12/25 | Loss: 0.00095272
Iteration 13/25 | Loss: 0.00095254
Iteration 14/25 | Loss: 0.00095244
Iteration 15/25 | Loss: 0.00095243
Iteration 16/25 | Loss: 0.00095242
Iteration 17/25 | Loss: 0.00095242
Iteration 18/25 | Loss: 0.00095242
Iteration 19/25 | Loss: 0.00095242
Iteration 20/25 | Loss: 0.00095242
Iteration 21/25 | Loss: 0.00095242
Iteration 22/25 | Loss: 0.00095242
Iteration 23/25 | Loss: 0.00095242
Iteration 24/25 | Loss: 0.00095242
Iteration 25/25 | Loss: 0.00095241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56168652
Iteration 2/25 | Loss: 0.00054921
Iteration 3/25 | Loss: 0.00054918
Iteration 4/25 | Loss: 0.00054918
Iteration 5/25 | Loss: 0.00054918
Iteration 6/25 | Loss: 0.00054917
Iteration 7/25 | Loss: 0.00054917
Iteration 8/25 | Loss: 0.00054917
Iteration 9/25 | Loss: 0.00054917
Iteration 10/25 | Loss: 0.00054917
Iteration 11/25 | Loss: 0.00054917
Iteration 12/25 | Loss: 0.00054917
Iteration 13/25 | Loss: 0.00054917
Iteration 14/25 | Loss: 0.00054917
Iteration 15/25 | Loss: 0.00054917
Iteration 16/25 | Loss: 0.00054917
Iteration 17/25 | Loss: 0.00054917
Iteration 18/25 | Loss: 0.00054917
Iteration 19/25 | Loss: 0.00054917
Iteration 20/25 | Loss: 0.00054917
Iteration 21/25 | Loss: 0.00054917
Iteration 22/25 | Loss: 0.00054917
Iteration 23/25 | Loss: 0.00054917
Iteration 24/25 | Loss: 0.00054917
Iteration 25/25 | Loss: 0.00054917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054917
Iteration 2/1000 | Loss: 0.00004694
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002680
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002360
Iteration 8/1000 | Loss: 0.00002320
Iteration 9/1000 | Loss: 0.00002291
Iteration 10/1000 | Loss: 0.00002273
Iteration 11/1000 | Loss: 0.00002268
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002262
Iteration 14/1000 | Loss: 0.00002262
Iteration 15/1000 | Loss: 0.00002261
Iteration 16/1000 | Loss: 0.00002260
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002258
Iteration 19/1000 | Loss: 0.00002258
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002255
Iteration 22/1000 | Loss: 0.00002255
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002254
Iteration 25/1000 | Loss: 0.00002252
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002251
Iteration 28/1000 | Loss: 0.00002246
Iteration 29/1000 | Loss: 0.00002245
Iteration 30/1000 | Loss: 0.00002245
Iteration 31/1000 | Loss: 0.00002244
Iteration 32/1000 | Loss: 0.00002244
Iteration 33/1000 | Loss: 0.00002243
Iteration 34/1000 | Loss: 0.00002243
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002241
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002241
Iteration 41/1000 | Loss: 0.00002240
Iteration 42/1000 | Loss: 0.00002240
Iteration 43/1000 | Loss: 0.00002240
Iteration 44/1000 | Loss: 0.00002239
Iteration 45/1000 | Loss: 0.00002239
Iteration 46/1000 | Loss: 0.00002239
Iteration 47/1000 | Loss: 0.00002238
Iteration 48/1000 | Loss: 0.00002238
Iteration 49/1000 | Loss: 0.00002237
Iteration 50/1000 | Loss: 0.00002237
Iteration 51/1000 | Loss: 0.00002237
Iteration 52/1000 | Loss: 0.00002236
Iteration 53/1000 | Loss: 0.00002236
Iteration 54/1000 | Loss: 0.00002236
Iteration 55/1000 | Loss: 0.00002236
Iteration 56/1000 | Loss: 0.00002236
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002236
Iteration 59/1000 | Loss: 0.00002236
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002236
Iteration 63/1000 | Loss: 0.00002236
Iteration 64/1000 | Loss: 0.00002236
Iteration 65/1000 | Loss: 0.00002235
Iteration 66/1000 | Loss: 0.00002235
Iteration 67/1000 | Loss: 0.00002235
Iteration 68/1000 | Loss: 0.00002234
Iteration 69/1000 | Loss: 0.00002234
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002234
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002234
Iteration 76/1000 | Loss: 0.00002234
Iteration 77/1000 | Loss: 0.00002233
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00002233
Iteration 80/1000 | Loss: 0.00002233
Iteration 81/1000 | Loss: 0.00002233
Iteration 82/1000 | Loss: 0.00002233
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002232
Iteration 85/1000 | Loss: 0.00002232
Iteration 86/1000 | Loss: 0.00002232
Iteration 87/1000 | Loss: 0.00002232
Iteration 88/1000 | Loss: 0.00002232
Iteration 89/1000 | Loss: 0.00002232
Iteration 90/1000 | Loss: 0.00002232
Iteration 91/1000 | Loss: 0.00002232
Iteration 92/1000 | Loss: 0.00002231
Iteration 93/1000 | Loss: 0.00002231
Iteration 94/1000 | Loss: 0.00002231
Iteration 95/1000 | Loss: 0.00002231
Iteration 96/1000 | Loss: 0.00002231
Iteration 97/1000 | Loss: 0.00002231
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002231
Iteration 101/1000 | Loss: 0.00002231
Iteration 102/1000 | Loss: 0.00002230
Iteration 103/1000 | Loss: 0.00002230
Iteration 104/1000 | Loss: 0.00002230
Iteration 105/1000 | Loss: 0.00002230
Iteration 106/1000 | Loss: 0.00002230
Iteration 107/1000 | Loss: 0.00002230
Iteration 108/1000 | Loss: 0.00002230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.230336940556299e-05, 2.230336940556299e-05, 2.230336940556299e-05, 2.230336940556299e-05, 2.230336940556299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.230336940556299e-05

Optimization complete. Final v2v error: 3.9551007747650146 mm

Highest mean error: 4.763684272766113 mm for frame 12

Lowest mean error: 3.4590964317321777 mm for frame 201

Saving results

Total time: 53.96703767776489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477156
Iteration 2/25 | Loss: 0.00106447
Iteration 3/25 | Loss: 0.00089398
Iteration 4/25 | Loss: 0.00086333
Iteration 5/25 | Loss: 0.00085211
Iteration 6/25 | Loss: 0.00084996
Iteration 7/25 | Loss: 0.00084939
Iteration 8/25 | Loss: 0.00084939
Iteration 9/25 | Loss: 0.00084939
Iteration 10/25 | Loss: 0.00084939
Iteration 11/25 | Loss: 0.00084939
Iteration 12/25 | Loss: 0.00084939
Iteration 13/25 | Loss: 0.00084939
Iteration 14/25 | Loss: 0.00084939
Iteration 15/25 | Loss: 0.00084939
Iteration 16/25 | Loss: 0.00084939
Iteration 17/25 | Loss: 0.00084939
Iteration 18/25 | Loss: 0.00084939
Iteration 19/25 | Loss: 0.00084939
Iteration 20/25 | Loss: 0.00084939
Iteration 21/25 | Loss: 0.00084939
Iteration 22/25 | Loss: 0.00084939
Iteration 23/25 | Loss: 0.00084939
Iteration 24/25 | Loss: 0.00084939
Iteration 25/25 | Loss: 0.00084939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81646383
Iteration 2/25 | Loss: 0.00040969
Iteration 3/25 | Loss: 0.00040969
Iteration 4/25 | Loss: 0.00040969
Iteration 5/25 | Loss: 0.00040969
Iteration 6/25 | Loss: 0.00040969
Iteration 7/25 | Loss: 0.00040969
Iteration 8/25 | Loss: 0.00040969
Iteration 9/25 | Loss: 0.00040969
Iteration 10/25 | Loss: 0.00040969
Iteration 11/25 | Loss: 0.00040969
Iteration 12/25 | Loss: 0.00040969
Iteration 13/25 | Loss: 0.00040969
Iteration 14/25 | Loss: 0.00040969
Iteration 15/25 | Loss: 0.00040969
Iteration 16/25 | Loss: 0.00040969
Iteration 17/25 | Loss: 0.00040969
Iteration 18/25 | Loss: 0.00040969
Iteration 19/25 | Loss: 0.00040969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004096910124644637, 0.0004096910124644637, 0.0004096910124644637, 0.0004096910124644637, 0.0004096910124644637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004096910124644637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040969
Iteration 2/1000 | Loss: 0.00003394
Iteration 3/1000 | Loss: 0.00002609
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002390
Iteration 6/1000 | Loss: 0.00002321
Iteration 7/1000 | Loss: 0.00002269
Iteration 8/1000 | Loss: 0.00002228
Iteration 9/1000 | Loss: 0.00002195
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002137
Iteration 13/1000 | Loss: 0.00002119
Iteration 14/1000 | Loss: 0.00002117
Iteration 15/1000 | Loss: 0.00002102
Iteration 16/1000 | Loss: 0.00002099
Iteration 17/1000 | Loss: 0.00002099
Iteration 18/1000 | Loss: 0.00002094
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002092
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002080
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002075
Iteration 35/1000 | Loss: 0.00002075
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002075
Iteration 39/1000 | Loss: 0.00002075
Iteration 40/1000 | Loss: 0.00002075
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002074
Iteration 47/1000 | Loss: 0.00002074
Iteration 48/1000 | Loss: 0.00002074
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00002071
Iteration 52/1000 | Loss: 0.00002071
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00002070
Iteration 55/1000 | Loss: 0.00002070
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002067
Iteration 59/1000 | Loss: 0.00002067
Iteration 60/1000 | Loss: 0.00002067
Iteration 61/1000 | Loss: 0.00002067
Iteration 62/1000 | Loss: 0.00002067
Iteration 63/1000 | Loss: 0.00002066
Iteration 64/1000 | Loss: 0.00002066
Iteration 65/1000 | Loss: 0.00002066
Iteration 66/1000 | Loss: 0.00002065
Iteration 67/1000 | Loss: 0.00002065
Iteration 68/1000 | Loss: 0.00002064
Iteration 69/1000 | Loss: 0.00002063
Iteration 70/1000 | Loss: 0.00002063
Iteration 71/1000 | Loss: 0.00002063
Iteration 72/1000 | Loss: 0.00002063
Iteration 73/1000 | Loss: 0.00002062
Iteration 74/1000 | Loss: 0.00002062
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002061
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002059
Iteration 82/1000 | Loss: 0.00002059
Iteration 83/1000 | Loss: 0.00002059
Iteration 84/1000 | Loss: 0.00002059
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002058
Iteration 90/1000 | Loss: 0.00002058
Iteration 91/1000 | Loss: 0.00002058
Iteration 92/1000 | Loss: 0.00002058
Iteration 93/1000 | Loss: 0.00002058
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002057
Iteration 99/1000 | Loss: 0.00002057
Iteration 100/1000 | Loss: 0.00002057
Iteration 101/1000 | Loss: 0.00002057
Iteration 102/1000 | Loss: 0.00002057
Iteration 103/1000 | Loss: 0.00002057
Iteration 104/1000 | Loss: 0.00002057
Iteration 105/1000 | Loss: 0.00002057
Iteration 106/1000 | Loss: 0.00002057
Iteration 107/1000 | Loss: 0.00002057
Iteration 108/1000 | Loss: 0.00002057
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002056
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002056
Iteration 114/1000 | Loss: 0.00002056
Iteration 115/1000 | Loss: 0.00002056
Iteration 116/1000 | Loss: 0.00002056
Iteration 117/1000 | Loss: 0.00002055
Iteration 118/1000 | Loss: 0.00002055
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00002054
Iteration 121/1000 | Loss: 0.00002054
Iteration 122/1000 | Loss: 0.00002054
Iteration 123/1000 | Loss: 0.00002053
Iteration 124/1000 | Loss: 0.00002053
Iteration 125/1000 | Loss: 0.00002053
Iteration 126/1000 | Loss: 0.00002052
Iteration 127/1000 | Loss: 0.00002052
Iteration 128/1000 | Loss: 0.00002052
Iteration 129/1000 | Loss: 0.00002052
Iteration 130/1000 | Loss: 0.00002052
Iteration 131/1000 | Loss: 0.00002051
Iteration 132/1000 | Loss: 0.00002051
Iteration 133/1000 | Loss: 0.00002051
Iteration 134/1000 | Loss: 0.00002050
Iteration 135/1000 | Loss: 0.00002050
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00002050
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002050
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002050
Iteration 142/1000 | Loss: 0.00002050
Iteration 143/1000 | Loss: 0.00002050
Iteration 144/1000 | Loss: 0.00002050
Iteration 145/1000 | Loss: 0.00002049
Iteration 146/1000 | Loss: 0.00002049
Iteration 147/1000 | Loss: 0.00002049
Iteration 148/1000 | Loss: 0.00002049
Iteration 149/1000 | Loss: 0.00002049
Iteration 150/1000 | Loss: 0.00002049
Iteration 151/1000 | Loss: 0.00002049
Iteration 152/1000 | Loss: 0.00002048
Iteration 153/1000 | Loss: 0.00002048
Iteration 154/1000 | Loss: 0.00002048
Iteration 155/1000 | Loss: 0.00002048
Iteration 156/1000 | Loss: 0.00002048
Iteration 157/1000 | Loss: 0.00002048
Iteration 158/1000 | Loss: 0.00002048
Iteration 159/1000 | Loss: 0.00002048
Iteration 160/1000 | Loss: 0.00002048
Iteration 161/1000 | Loss: 0.00002047
Iteration 162/1000 | Loss: 0.00002047
Iteration 163/1000 | Loss: 0.00002047
Iteration 164/1000 | Loss: 0.00002046
Iteration 165/1000 | Loss: 0.00002046
Iteration 166/1000 | Loss: 0.00002046
Iteration 167/1000 | Loss: 0.00002046
Iteration 168/1000 | Loss: 0.00002046
Iteration 169/1000 | Loss: 0.00002046
Iteration 170/1000 | Loss: 0.00002045
Iteration 171/1000 | Loss: 0.00002045
Iteration 172/1000 | Loss: 0.00002045
Iteration 173/1000 | Loss: 0.00002045
Iteration 174/1000 | Loss: 0.00002045
Iteration 175/1000 | Loss: 0.00002045
Iteration 176/1000 | Loss: 0.00002045
Iteration 177/1000 | Loss: 0.00002045
Iteration 178/1000 | Loss: 0.00002044
Iteration 179/1000 | Loss: 0.00002044
Iteration 180/1000 | Loss: 0.00002044
Iteration 181/1000 | Loss: 0.00002044
Iteration 182/1000 | Loss: 0.00002044
Iteration 183/1000 | Loss: 0.00002044
Iteration 184/1000 | Loss: 0.00002044
Iteration 185/1000 | Loss: 0.00002043
Iteration 186/1000 | Loss: 0.00002043
Iteration 187/1000 | Loss: 0.00002043
Iteration 188/1000 | Loss: 0.00002043
Iteration 189/1000 | Loss: 0.00002043
Iteration 190/1000 | Loss: 0.00002043
Iteration 191/1000 | Loss: 0.00002043
Iteration 192/1000 | Loss: 0.00002043
Iteration 193/1000 | Loss: 0.00002043
Iteration 194/1000 | Loss: 0.00002043
Iteration 195/1000 | Loss: 0.00002043
Iteration 196/1000 | Loss: 0.00002043
Iteration 197/1000 | Loss: 0.00002043
Iteration 198/1000 | Loss: 0.00002043
Iteration 199/1000 | Loss: 0.00002043
Iteration 200/1000 | Loss: 0.00002043
Iteration 201/1000 | Loss: 0.00002043
Iteration 202/1000 | Loss: 0.00002043
Iteration 203/1000 | Loss: 0.00002043
Iteration 204/1000 | Loss: 0.00002042
Iteration 205/1000 | Loss: 0.00002042
Iteration 206/1000 | Loss: 0.00002042
Iteration 207/1000 | Loss: 0.00002042
Iteration 208/1000 | Loss: 0.00002042
Iteration 209/1000 | Loss: 0.00002042
Iteration 210/1000 | Loss: 0.00002042
Iteration 211/1000 | Loss: 0.00002042
Iteration 212/1000 | Loss: 0.00002042
Iteration 213/1000 | Loss: 0.00002042
Iteration 214/1000 | Loss: 0.00002042
Iteration 215/1000 | Loss: 0.00002042
Iteration 216/1000 | Loss: 0.00002042
Iteration 217/1000 | Loss: 0.00002042
Iteration 218/1000 | Loss: 0.00002042
Iteration 219/1000 | Loss: 0.00002042
Iteration 220/1000 | Loss: 0.00002042
Iteration 221/1000 | Loss: 0.00002042
Iteration 222/1000 | Loss: 0.00002042
Iteration 223/1000 | Loss: 0.00002042
Iteration 224/1000 | Loss: 0.00002042
Iteration 225/1000 | Loss: 0.00002042
Iteration 226/1000 | Loss: 0.00002042
Iteration 227/1000 | Loss: 0.00002042
Iteration 228/1000 | Loss: 0.00002042
Iteration 229/1000 | Loss: 0.00002042
Iteration 230/1000 | Loss: 0.00002042
Iteration 231/1000 | Loss: 0.00002042
Iteration 232/1000 | Loss: 0.00002042
Iteration 233/1000 | Loss: 0.00002042
Iteration 234/1000 | Loss: 0.00002042
Iteration 235/1000 | Loss: 0.00002042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.0424176909727976e-05, 2.0424176909727976e-05, 2.0424176909727976e-05, 2.0424176909727976e-05, 2.0424176909727976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0424176909727976e-05

Optimization complete. Final v2v error: 3.8582189083099365 mm

Highest mean error: 4.737558841705322 mm for frame 258

Lowest mean error: 3.6490445137023926 mm for frame 62

Saving results

Total time: 54.12225317955017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867241
Iteration 2/25 | Loss: 0.00184385
Iteration 3/25 | Loss: 0.00108176
Iteration 4/25 | Loss: 0.00100278
Iteration 5/25 | Loss: 0.00097158
Iteration 6/25 | Loss: 0.00095950
Iteration 7/25 | Loss: 0.00096509
Iteration 8/25 | Loss: 0.00096041
Iteration 9/25 | Loss: 0.00095684
Iteration 10/25 | Loss: 0.00095113
Iteration 11/25 | Loss: 0.00094580
Iteration 12/25 | Loss: 0.00094405
Iteration 13/25 | Loss: 0.00094329
Iteration 14/25 | Loss: 0.00094268
Iteration 15/25 | Loss: 0.00094676
Iteration 16/25 | Loss: 0.00094380
Iteration 17/25 | Loss: 0.00094301
Iteration 18/25 | Loss: 0.00094120
Iteration 19/25 | Loss: 0.00094095
Iteration 20/25 | Loss: 0.00094112
Iteration 21/25 | Loss: 0.00093839
Iteration 22/25 | Loss: 0.00093692
Iteration 23/25 | Loss: 0.00093674
Iteration 24/25 | Loss: 0.00093669
Iteration 25/25 | Loss: 0.00093669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76695299
Iteration 2/25 | Loss: 0.00087038
Iteration 3/25 | Loss: 0.00066189
Iteration 4/25 | Loss: 0.00066189
Iteration 5/25 | Loss: 0.00066189
Iteration 6/25 | Loss: 0.00066189
Iteration 7/25 | Loss: 0.00066189
Iteration 8/25 | Loss: 0.00066189
Iteration 9/25 | Loss: 0.00066189
Iteration 10/25 | Loss: 0.00066189
Iteration 11/25 | Loss: 0.00066189
Iteration 12/25 | Loss: 0.00066189
Iteration 13/25 | Loss: 0.00066189
Iteration 14/25 | Loss: 0.00066189
Iteration 15/25 | Loss: 0.00066189
Iteration 16/25 | Loss: 0.00066189
Iteration 17/25 | Loss: 0.00066189
Iteration 18/25 | Loss: 0.00066189
Iteration 19/25 | Loss: 0.00066189
Iteration 20/25 | Loss: 0.00066189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006618900806643069, 0.0006618900806643069, 0.0006618900806643069, 0.0006618900806643069, 0.0006618900806643069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006618900806643069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066189
Iteration 2/1000 | Loss: 0.00005102
Iteration 3/1000 | Loss: 0.00003943
Iteration 4/1000 | Loss: 0.00050878
Iteration 5/1000 | Loss: 0.00065758
Iteration 6/1000 | Loss: 0.00016355
Iteration 7/1000 | Loss: 0.00016530
Iteration 8/1000 | Loss: 0.00036855
Iteration 9/1000 | Loss: 0.00004070
Iteration 10/1000 | Loss: 0.00026734
Iteration 11/1000 | Loss: 0.00004165
Iteration 12/1000 | Loss: 0.00028257
Iteration 13/1000 | Loss: 0.00016964
Iteration 14/1000 | Loss: 0.00073466
Iteration 15/1000 | Loss: 0.00033356
Iteration 16/1000 | Loss: 0.00036512
Iteration 17/1000 | Loss: 0.00003425
Iteration 18/1000 | Loss: 0.00002918
Iteration 19/1000 | Loss: 0.00002597
Iteration 20/1000 | Loss: 0.00002401
Iteration 21/1000 | Loss: 0.00039099
Iteration 22/1000 | Loss: 0.00002313
Iteration 23/1000 | Loss: 0.00002246
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002179
Iteration 26/1000 | Loss: 0.00002175
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002120
Iteration 30/1000 | Loss: 0.00002101
Iteration 31/1000 | Loss: 0.00002100
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002096
Iteration 35/1000 | Loss: 0.00002096
Iteration 36/1000 | Loss: 0.00002095
Iteration 37/1000 | Loss: 0.00002095
Iteration 38/1000 | Loss: 0.00002095
Iteration 39/1000 | Loss: 0.00002095
Iteration 40/1000 | Loss: 0.00002094
Iteration 41/1000 | Loss: 0.00002094
Iteration 42/1000 | Loss: 0.00002093
Iteration 43/1000 | Loss: 0.00002093
Iteration 44/1000 | Loss: 0.00002093
Iteration 45/1000 | Loss: 0.00002092
Iteration 46/1000 | Loss: 0.00002092
Iteration 47/1000 | Loss: 0.00002092
Iteration 48/1000 | Loss: 0.00002092
Iteration 49/1000 | Loss: 0.00002091
Iteration 50/1000 | Loss: 0.00002091
Iteration 51/1000 | Loss: 0.00002090
Iteration 52/1000 | Loss: 0.00002090
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002089
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002084
Iteration 60/1000 | Loss: 0.00002083
Iteration 61/1000 | Loss: 0.00002083
Iteration 62/1000 | Loss: 0.00002082
Iteration 63/1000 | Loss: 0.00002082
Iteration 64/1000 | Loss: 0.00002082
Iteration 65/1000 | Loss: 0.00002082
Iteration 66/1000 | Loss: 0.00002082
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002081
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002080
Iteration 72/1000 | Loss: 0.00002080
Iteration 73/1000 | Loss: 0.00002080
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002079
Iteration 76/1000 | Loss: 0.00002079
Iteration 77/1000 | Loss: 0.00002078
Iteration 78/1000 | Loss: 0.00002078
Iteration 79/1000 | Loss: 0.00002078
Iteration 80/1000 | Loss: 0.00002078
Iteration 81/1000 | Loss: 0.00002078
Iteration 82/1000 | Loss: 0.00002077
Iteration 83/1000 | Loss: 0.00002077
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002076
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002076
Iteration 90/1000 | Loss: 0.00002076
Iteration 91/1000 | Loss: 0.00002076
Iteration 92/1000 | Loss: 0.00002075
Iteration 93/1000 | Loss: 0.00002075
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00002075
Iteration 96/1000 | Loss: 0.00002074
Iteration 97/1000 | Loss: 0.00002074
Iteration 98/1000 | Loss: 0.00002074
Iteration 99/1000 | Loss: 0.00002074
Iteration 100/1000 | Loss: 0.00002074
Iteration 101/1000 | Loss: 0.00002073
Iteration 102/1000 | Loss: 0.00002073
Iteration 103/1000 | Loss: 0.00002073
Iteration 104/1000 | Loss: 0.00002072
Iteration 105/1000 | Loss: 0.00002072
Iteration 106/1000 | Loss: 0.00002072
Iteration 107/1000 | Loss: 0.00002072
Iteration 108/1000 | Loss: 0.00002072
Iteration 109/1000 | Loss: 0.00002071
Iteration 110/1000 | Loss: 0.00002071
Iteration 111/1000 | Loss: 0.00002071
Iteration 112/1000 | Loss: 0.00002071
Iteration 113/1000 | Loss: 0.00002071
Iteration 114/1000 | Loss: 0.00002071
Iteration 115/1000 | Loss: 0.00002071
Iteration 116/1000 | Loss: 0.00002071
Iteration 117/1000 | Loss: 0.00002071
Iteration 118/1000 | Loss: 0.00002071
Iteration 119/1000 | Loss: 0.00002071
Iteration 120/1000 | Loss: 0.00002071
Iteration 121/1000 | Loss: 0.00002071
Iteration 122/1000 | Loss: 0.00002071
Iteration 123/1000 | Loss: 0.00002071
Iteration 124/1000 | Loss: 0.00002070
Iteration 125/1000 | Loss: 0.00002070
Iteration 126/1000 | Loss: 0.00002070
Iteration 127/1000 | Loss: 0.00002070
Iteration 128/1000 | Loss: 0.00002070
Iteration 129/1000 | Loss: 0.00002070
Iteration 130/1000 | Loss: 0.00002070
Iteration 131/1000 | Loss: 0.00002070
Iteration 132/1000 | Loss: 0.00002070
Iteration 133/1000 | Loss: 0.00002070
Iteration 134/1000 | Loss: 0.00002070
Iteration 135/1000 | Loss: 0.00002070
Iteration 136/1000 | Loss: 0.00002070
Iteration 137/1000 | Loss: 0.00002070
Iteration 138/1000 | Loss: 0.00002070
Iteration 139/1000 | Loss: 0.00002070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.070406117127277e-05, 2.070406117127277e-05, 2.070406117127277e-05, 2.070406117127277e-05, 2.070406117127277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.070406117127277e-05

Optimization complete. Final v2v error: 3.7268266677856445 mm

Highest mean error: 4.657526969909668 mm for frame 224

Lowest mean error: 2.9312562942504883 mm for frame 126

Saving results

Total time: 100.69823932647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429422
Iteration 2/25 | Loss: 0.00097511
Iteration 3/25 | Loss: 0.00088307
Iteration 4/25 | Loss: 0.00085459
Iteration 5/25 | Loss: 0.00084653
Iteration 6/25 | Loss: 0.00084494
Iteration 7/25 | Loss: 0.00084479
Iteration 8/25 | Loss: 0.00084479
Iteration 9/25 | Loss: 0.00084479
Iteration 10/25 | Loss: 0.00084479
Iteration 11/25 | Loss: 0.00084479
Iteration 12/25 | Loss: 0.00084479
Iteration 13/25 | Loss: 0.00084479
Iteration 14/25 | Loss: 0.00084479
Iteration 15/25 | Loss: 0.00084479
Iteration 16/25 | Loss: 0.00084479
Iteration 17/25 | Loss: 0.00084479
Iteration 18/25 | Loss: 0.00084479
Iteration 19/25 | Loss: 0.00084479
Iteration 20/25 | Loss: 0.00084479
Iteration 21/25 | Loss: 0.00084479
Iteration 22/25 | Loss: 0.00084479
Iteration 23/25 | Loss: 0.00084479
Iteration 24/25 | Loss: 0.00084479
Iteration 25/25 | Loss: 0.00084479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51844001
Iteration 2/25 | Loss: 0.00052499
Iteration 3/25 | Loss: 0.00052499
Iteration 4/25 | Loss: 0.00052499
Iteration 5/25 | Loss: 0.00052499
Iteration 6/25 | Loss: 0.00052499
Iteration 7/25 | Loss: 0.00052499
Iteration 8/25 | Loss: 0.00052499
Iteration 9/25 | Loss: 0.00052499
Iteration 10/25 | Loss: 0.00052499
Iteration 11/25 | Loss: 0.00052499
Iteration 12/25 | Loss: 0.00052499
Iteration 13/25 | Loss: 0.00052499
Iteration 14/25 | Loss: 0.00052499
Iteration 15/25 | Loss: 0.00052499
Iteration 16/25 | Loss: 0.00052499
Iteration 17/25 | Loss: 0.00052499
Iteration 18/25 | Loss: 0.00052499
Iteration 19/25 | Loss: 0.00052499
Iteration 20/25 | Loss: 0.00052499
Iteration 21/25 | Loss: 0.00052499
Iteration 22/25 | Loss: 0.00052499
Iteration 23/25 | Loss: 0.00052499
Iteration 24/25 | Loss: 0.00052499
Iteration 25/25 | Loss: 0.00052499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052499
Iteration 2/1000 | Loss: 0.00003193
Iteration 3/1000 | Loss: 0.00002188
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00001684
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001678
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001677
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001673
Iteration 28/1000 | Loss: 0.00001673
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001671
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001670
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001670
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001668
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001661
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001651
Iteration 67/1000 | Loss: 0.00001651
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001649
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001645
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001644
Iteration 92/1000 | Loss: 0.00001644
Iteration 93/1000 | Loss: 0.00001644
Iteration 94/1000 | Loss: 0.00001644
Iteration 95/1000 | Loss: 0.00001644
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001643
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001643
Iteration 128/1000 | Loss: 0.00001643
Iteration 129/1000 | Loss: 0.00001643
Iteration 130/1000 | Loss: 0.00001643
Iteration 131/1000 | Loss: 0.00001643
Iteration 132/1000 | Loss: 0.00001643
Iteration 133/1000 | Loss: 0.00001643
Iteration 134/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6426147340098396e-05, 1.6426147340098396e-05, 1.6426147340098396e-05, 1.6426147340098396e-05, 1.6426147340098396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6426147340098396e-05

Optimization complete. Final v2v error: 3.4456918239593506 mm

Highest mean error: 3.6320066452026367 mm for frame 90

Lowest mean error: 3.221914052963257 mm for frame 128

Saving results

Total time: 35.14414358139038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867951
Iteration 2/25 | Loss: 0.00126942
Iteration 3/25 | Loss: 0.00099959
Iteration 4/25 | Loss: 0.00096234
Iteration 5/25 | Loss: 0.00095504
Iteration 6/25 | Loss: 0.00095450
Iteration 7/25 | Loss: 0.00095450
Iteration 8/25 | Loss: 0.00095450
Iteration 9/25 | Loss: 0.00095450
Iteration 10/25 | Loss: 0.00095450
Iteration 11/25 | Loss: 0.00095450
Iteration 12/25 | Loss: 0.00095450
Iteration 13/25 | Loss: 0.00095450
Iteration 14/25 | Loss: 0.00095450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009544988279230893, 0.0009544988279230893, 0.0009544988279230893, 0.0009544988279230893, 0.0009544988279230893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009544988279230893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07628512
Iteration 2/25 | Loss: 0.00044660
Iteration 3/25 | Loss: 0.00044660
Iteration 4/25 | Loss: 0.00044660
Iteration 5/25 | Loss: 0.00044660
Iteration 6/25 | Loss: 0.00044660
Iteration 7/25 | Loss: 0.00044660
Iteration 8/25 | Loss: 0.00044659
Iteration 9/25 | Loss: 0.00044659
Iteration 10/25 | Loss: 0.00044659
Iteration 11/25 | Loss: 0.00044659
Iteration 12/25 | Loss: 0.00044659
Iteration 13/25 | Loss: 0.00044659
Iteration 14/25 | Loss: 0.00044659
Iteration 15/25 | Loss: 0.00044659
Iteration 16/25 | Loss: 0.00044659
Iteration 17/25 | Loss: 0.00044659
Iteration 18/25 | Loss: 0.00044659
Iteration 19/25 | Loss: 0.00044659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00044659440754912794, 0.00044659440754912794, 0.00044659440754912794, 0.00044659440754912794, 0.00044659440754912794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044659440754912794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044659
Iteration 2/1000 | Loss: 0.00003630
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00002878
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002626
Iteration 7/1000 | Loss: 0.00002583
Iteration 8/1000 | Loss: 0.00002545
Iteration 9/1000 | Loss: 0.00002518
Iteration 10/1000 | Loss: 0.00002488
Iteration 11/1000 | Loss: 0.00002467
Iteration 12/1000 | Loss: 0.00002452
Iteration 13/1000 | Loss: 0.00002450
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00002449
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002445
Iteration 18/1000 | Loss: 0.00002445
Iteration 19/1000 | Loss: 0.00002445
Iteration 20/1000 | Loss: 0.00002445
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002444
Iteration 23/1000 | Loss: 0.00002441
Iteration 24/1000 | Loss: 0.00002438
Iteration 25/1000 | Loss: 0.00002437
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002434
Iteration 36/1000 | Loss: 0.00002434
Iteration 37/1000 | Loss: 0.00002434
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002433
Iteration 40/1000 | Loss: 0.00002433
Iteration 41/1000 | Loss: 0.00002433
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002432
Iteration 47/1000 | Loss: 0.00002432
Iteration 48/1000 | Loss: 0.00002432
Iteration 49/1000 | Loss: 0.00002432
Iteration 50/1000 | Loss: 0.00002432
Iteration 51/1000 | Loss: 0.00002432
Iteration 52/1000 | Loss: 0.00002432
Iteration 53/1000 | Loss: 0.00002431
Iteration 54/1000 | Loss: 0.00002431
Iteration 55/1000 | Loss: 0.00002431
Iteration 56/1000 | Loss: 0.00002431
Iteration 57/1000 | Loss: 0.00002431
Iteration 58/1000 | Loss: 0.00002431
Iteration 59/1000 | Loss: 0.00002430
Iteration 60/1000 | Loss: 0.00002430
Iteration 61/1000 | Loss: 0.00002430
Iteration 62/1000 | Loss: 0.00002430
Iteration 63/1000 | Loss: 0.00002430
Iteration 64/1000 | Loss: 0.00002429
Iteration 65/1000 | Loss: 0.00002429
Iteration 66/1000 | Loss: 0.00002429
Iteration 67/1000 | Loss: 0.00002429
Iteration 68/1000 | Loss: 0.00002429
Iteration 69/1000 | Loss: 0.00002429
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002428
Iteration 72/1000 | Loss: 0.00002428
Iteration 73/1000 | Loss: 0.00002428
Iteration 74/1000 | Loss: 0.00002428
Iteration 75/1000 | Loss: 0.00002428
Iteration 76/1000 | Loss: 0.00002428
Iteration 77/1000 | Loss: 0.00002428
Iteration 78/1000 | Loss: 0.00002428
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002428
Iteration 81/1000 | Loss: 0.00002428
Iteration 82/1000 | Loss: 0.00002428
Iteration 83/1000 | Loss: 0.00002428
Iteration 84/1000 | Loss: 0.00002428
Iteration 85/1000 | Loss: 0.00002428
Iteration 86/1000 | Loss: 0.00002428
Iteration 87/1000 | Loss: 0.00002427
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002426
Iteration 91/1000 | Loss: 0.00002426
Iteration 92/1000 | Loss: 0.00002426
Iteration 93/1000 | Loss: 0.00002426
Iteration 94/1000 | Loss: 0.00002426
Iteration 95/1000 | Loss: 0.00002426
Iteration 96/1000 | Loss: 0.00002426
Iteration 97/1000 | Loss: 0.00002426
Iteration 98/1000 | Loss: 0.00002426
Iteration 99/1000 | Loss: 0.00002426
Iteration 100/1000 | Loss: 0.00002426
Iteration 101/1000 | Loss: 0.00002426
Iteration 102/1000 | Loss: 0.00002426
Iteration 103/1000 | Loss: 0.00002425
Iteration 104/1000 | Loss: 0.00002425
Iteration 105/1000 | Loss: 0.00002425
Iteration 106/1000 | Loss: 0.00002424
Iteration 107/1000 | Loss: 0.00002424
Iteration 108/1000 | Loss: 0.00002424
Iteration 109/1000 | Loss: 0.00002423
Iteration 110/1000 | Loss: 0.00002423
Iteration 111/1000 | Loss: 0.00002423
Iteration 112/1000 | Loss: 0.00002423
Iteration 113/1000 | Loss: 0.00002423
Iteration 114/1000 | Loss: 0.00002423
Iteration 115/1000 | Loss: 0.00002422
Iteration 116/1000 | Loss: 0.00002422
Iteration 117/1000 | Loss: 0.00002422
Iteration 118/1000 | Loss: 0.00002422
Iteration 119/1000 | Loss: 0.00002422
Iteration 120/1000 | Loss: 0.00002422
Iteration 121/1000 | Loss: 0.00002421
Iteration 122/1000 | Loss: 0.00002421
Iteration 123/1000 | Loss: 0.00002421
Iteration 124/1000 | Loss: 0.00002421
Iteration 125/1000 | Loss: 0.00002421
Iteration 126/1000 | Loss: 0.00002421
Iteration 127/1000 | Loss: 0.00002421
Iteration 128/1000 | Loss: 0.00002421
Iteration 129/1000 | Loss: 0.00002421
Iteration 130/1000 | Loss: 0.00002421
Iteration 131/1000 | Loss: 0.00002421
Iteration 132/1000 | Loss: 0.00002421
Iteration 133/1000 | Loss: 0.00002421
Iteration 134/1000 | Loss: 0.00002421
Iteration 135/1000 | Loss: 0.00002421
Iteration 136/1000 | Loss: 0.00002421
Iteration 137/1000 | Loss: 0.00002421
Iteration 138/1000 | Loss: 0.00002420
Iteration 139/1000 | Loss: 0.00002420
Iteration 140/1000 | Loss: 0.00002420
Iteration 141/1000 | Loss: 0.00002420
Iteration 142/1000 | Loss: 0.00002419
Iteration 143/1000 | Loss: 0.00002419
Iteration 144/1000 | Loss: 0.00002419
Iteration 145/1000 | Loss: 0.00002419
Iteration 146/1000 | Loss: 0.00002419
Iteration 147/1000 | Loss: 0.00002419
Iteration 148/1000 | Loss: 0.00002419
Iteration 149/1000 | Loss: 0.00002419
Iteration 150/1000 | Loss: 0.00002419
Iteration 151/1000 | Loss: 0.00002419
Iteration 152/1000 | Loss: 0.00002419
Iteration 153/1000 | Loss: 0.00002419
Iteration 154/1000 | Loss: 0.00002419
Iteration 155/1000 | Loss: 0.00002419
Iteration 156/1000 | Loss: 0.00002419
Iteration 157/1000 | Loss: 0.00002419
Iteration 158/1000 | Loss: 0.00002419
Iteration 159/1000 | Loss: 0.00002419
Iteration 160/1000 | Loss: 0.00002419
Iteration 161/1000 | Loss: 0.00002419
Iteration 162/1000 | Loss: 0.00002419
Iteration 163/1000 | Loss: 0.00002419
Iteration 164/1000 | Loss: 0.00002419
Iteration 165/1000 | Loss: 0.00002419
Iteration 166/1000 | Loss: 0.00002419
Iteration 167/1000 | Loss: 0.00002419
Iteration 168/1000 | Loss: 0.00002419
Iteration 169/1000 | Loss: 0.00002419
Iteration 170/1000 | Loss: 0.00002419
Iteration 171/1000 | Loss: 0.00002419
Iteration 172/1000 | Loss: 0.00002419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.4187462258851156e-05, 2.4187462258851156e-05, 2.4187462258851156e-05, 2.4187462258851156e-05, 2.4187462258851156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4187462258851156e-05

Optimization complete. Final v2v error: 4.0973429679870605 mm

Highest mean error: 4.149936199188232 mm for frame 78

Lowest mean error: 4.042580604553223 mm for frame 128

Saving results

Total time: 33.65696382522583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402812
Iteration 2/25 | Loss: 0.00095850
Iteration 3/25 | Loss: 0.00086791
Iteration 4/25 | Loss: 0.00083984
Iteration 5/25 | Loss: 0.00083117
Iteration 6/25 | Loss: 0.00082900
Iteration 7/25 | Loss: 0.00082836
Iteration 8/25 | Loss: 0.00082820
Iteration 9/25 | Loss: 0.00082820
Iteration 10/25 | Loss: 0.00082820
Iteration 11/25 | Loss: 0.00082820
Iteration 12/25 | Loss: 0.00082820
Iteration 13/25 | Loss: 0.00082818
Iteration 14/25 | Loss: 0.00082818
Iteration 15/25 | Loss: 0.00082818
Iteration 16/25 | Loss: 0.00082818
Iteration 17/25 | Loss: 0.00082818
Iteration 18/25 | Loss: 0.00082818
Iteration 19/25 | Loss: 0.00082818
Iteration 20/25 | Loss: 0.00082818
Iteration 21/25 | Loss: 0.00082818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008281759219244123, 0.0008281759219244123, 0.0008281759219244123, 0.0008281759219244123, 0.0008281759219244123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008281759219244123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10495996
Iteration 2/25 | Loss: 0.00051073
Iteration 3/25 | Loss: 0.00051072
Iteration 4/25 | Loss: 0.00051072
Iteration 5/25 | Loss: 0.00051072
Iteration 6/25 | Loss: 0.00051072
Iteration 7/25 | Loss: 0.00051072
Iteration 8/25 | Loss: 0.00051072
Iteration 9/25 | Loss: 0.00051072
Iteration 10/25 | Loss: 0.00051072
Iteration 11/25 | Loss: 0.00051072
Iteration 12/25 | Loss: 0.00051072
Iteration 13/25 | Loss: 0.00051072
Iteration 14/25 | Loss: 0.00051072
Iteration 15/25 | Loss: 0.00051072
Iteration 16/25 | Loss: 0.00051072
Iteration 17/25 | Loss: 0.00051072
Iteration 18/25 | Loss: 0.00051072
Iteration 19/25 | Loss: 0.00051072
Iteration 20/25 | Loss: 0.00051072
Iteration 21/25 | Loss: 0.00051072
Iteration 22/25 | Loss: 0.00051072
Iteration 23/25 | Loss: 0.00051072
Iteration 24/25 | Loss: 0.00051072
Iteration 25/25 | Loss: 0.00051072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051072
Iteration 2/1000 | Loss: 0.00004361
Iteration 3/1000 | Loss: 0.00002841
Iteration 4/1000 | Loss: 0.00002445
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002136
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002037
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00001988
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001917
Iteration 41/1000 | Loss: 0.00001917
Iteration 42/1000 | Loss: 0.00001917
Iteration 43/1000 | Loss: 0.00001917
Iteration 44/1000 | Loss: 0.00001917
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001916
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001913
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001909
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001896
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00001895
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001895
Iteration 106/1000 | Loss: 0.00001895
Iteration 107/1000 | Loss: 0.00001895
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001894
Iteration 113/1000 | Loss: 0.00001894
Iteration 114/1000 | Loss: 0.00001894
Iteration 115/1000 | Loss: 0.00001894
Iteration 116/1000 | Loss: 0.00001894
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001892
Iteration 133/1000 | Loss: 0.00001892
Iteration 134/1000 | Loss: 0.00001892
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001892
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001891
Iteration 147/1000 | Loss: 0.00001891
Iteration 148/1000 | Loss: 0.00001891
Iteration 149/1000 | Loss: 0.00001891
Iteration 150/1000 | Loss: 0.00001891
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001891
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001891
Iteration 165/1000 | Loss: 0.00001891
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001891
Iteration 170/1000 | Loss: 0.00001891
Iteration 171/1000 | Loss: 0.00001891
Iteration 172/1000 | Loss: 0.00001891
Iteration 173/1000 | Loss: 0.00001891
Iteration 174/1000 | Loss: 0.00001891
Iteration 175/1000 | Loss: 0.00001891
Iteration 176/1000 | Loss: 0.00001891
Iteration 177/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.89135880646063e-05, 1.89135880646063e-05, 1.89135880646063e-05, 1.89135880646063e-05, 1.89135880646063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.89135880646063e-05

Optimization complete. Final v2v error: 3.649101734161377 mm

Highest mean error: 4.686900615692139 mm for frame 62

Lowest mean error: 3.2644455432891846 mm for frame 91

Saving results

Total time: 43.10747456550598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378681
Iteration 2/25 | Loss: 0.00095846
Iteration 3/25 | Loss: 0.00086902
Iteration 4/25 | Loss: 0.00085955
Iteration 5/25 | Loss: 0.00085469
Iteration 6/25 | Loss: 0.00085380
Iteration 7/25 | Loss: 0.00085380
Iteration 8/25 | Loss: 0.00085380
Iteration 9/25 | Loss: 0.00085380
Iteration 10/25 | Loss: 0.00085380
Iteration 11/25 | Loss: 0.00085380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008537970716133714, 0.0008537970716133714, 0.0008537970716133714, 0.0008537970716133714, 0.0008537970716133714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008537970716133714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66492105
Iteration 2/25 | Loss: 0.00064274
Iteration 3/25 | Loss: 0.00064274
Iteration 4/25 | Loss: 0.00064274
Iteration 5/25 | Loss: 0.00064274
Iteration 6/25 | Loss: 0.00064274
Iteration 7/25 | Loss: 0.00064274
Iteration 8/25 | Loss: 0.00064274
Iteration 9/25 | Loss: 0.00064274
Iteration 10/25 | Loss: 0.00064274
Iteration 11/25 | Loss: 0.00064274
Iteration 12/25 | Loss: 0.00064274
Iteration 13/25 | Loss: 0.00064274
Iteration 14/25 | Loss: 0.00064274
Iteration 15/25 | Loss: 0.00064274
Iteration 16/25 | Loss: 0.00064274
Iteration 17/25 | Loss: 0.00064274
Iteration 18/25 | Loss: 0.00064274
Iteration 19/25 | Loss: 0.00064274
Iteration 20/25 | Loss: 0.00064274
Iteration 21/25 | Loss: 0.00064274
Iteration 22/25 | Loss: 0.00064274
Iteration 23/25 | Loss: 0.00064274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006427359185181558, 0.0006427359185181558, 0.0006427359185181558, 0.0006427359185181558, 0.0006427359185181558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006427359185181558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064274
Iteration 2/1000 | Loss: 0.00002665
Iteration 3/1000 | Loss: 0.00001817
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001349
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001329
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001329
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001323
Iteration 70/1000 | Loss: 0.00001323
Iteration 71/1000 | Loss: 0.00001323
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.3232181117928121e-05, 1.3232181117928121e-05, 1.3232181117928121e-05, 1.3232181117928121e-05, 1.3232181117928121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3232181117928121e-05

Optimization complete. Final v2v error: 3.107496976852417 mm

Highest mean error: 3.4486541748046875 mm for frame 193

Lowest mean error: 2.8700149059295654 mm for frame 220

Saving results

Total time: 34.97721314430237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063251
Iteration 2/25 | Loss: 0.00183358
Iteration 3/25 | Loss: 0.00127881
Iteration 4/25 | Loss: 0.00140317
Iteration 5/25 | Loss: 0.00120650
Iteration 6/25 | Loss: 0.00103956
Iteration 7/25 | Loss: 0.00101999
Iteration 8/25 | Loss: 0.00095500
Iteration 9/25 | Loss: 0.00093736
Iteration 10/25 | Loss: 0.00093529
Iteration 11/25 | Loss: 0.00095300
Iteration 12/25 | Loss: 0.00092152
Iteration 13/25 | Loss: 0.00091244
Iteration 14/25 | Loss: 0.00095318
Iteration 15/25 | Loss: 0.00091812
Iteration 16/25 | Loss: 0.00094260
Iteration 17/25 | Loss: 0.00100631
Iteration 18/25 | Loss: 0.00093142
Iteration 19/25 | Loss: 0.00089437
Iteration 20/25 | Loss: 0.00092829
Iteration 21/25 | Loss: 0.00091948
Iteration 22/25 | Loss: 0.00092269
Iteration 23/25 | Loss: 0.00091853
Iteration 24/25 | Loss: 0.00091516
Iteration 25/25 | Loss: 0.00091027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66067910
Iteration 2/25 | Loss: 0.00124121
Iteration 3/25 | Loss: 0.00124121
Iteration 4/25 | Loss: 0.00124120
Iteration 5/25 | Loss: 0.00124120
Iteration 6/25 | Loss: 0.00124120
Iteration 7/25 | Loss: 0.00124120
Iteration 8/25 | Loss: 0.00124120
Iteration 9/25 | Loss: 0.00124120
Iteration 10/25 | Loss: 0.00124120
Iteration 11/25 | Loss: 0.00124120
Iteration 12/25 | Loss: 0.00124120
Iteration 13/25 | Loss: 0.00124120
Iteration 14/25 | Loss: 0.00124120
Iteration 15/25 | Loss: 0.00124120
Iteration 16/25 | Loss: 0.00124120
Iteration 17/25 | Loss: 0.00124120
Iteration 18/25 | Loss: 0.00124120
Iteration 19/25 | Loss: 0.00124120
Iteration 20/25 | Loss: 0.00124120
Iteration 21/25 | Loss: 0.00124120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012412030482664704, 0.0012412030482664704, 0.0012412030482664704, 0.0012412030482664704, 0.0012412030482664704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012412030482664704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124120
Iteration 2/1000 | Loss: 0.00061541
Iteration 3/1000 | Loss: 0.00089065
Iteration 4/1000 | Loss: 0.00187142
Iteration 5/1000 | Loss: 0.00147734
Iteration 6/1000 | Loss: 0.00007046
Iteration 7/1000 | Loss: 0.00005731
Iteration 8/1000 | Loss: 0.00121980
Iteration 9/1000 | Loss: 0.00007067
Iteration 10/1000 | Loss: 0.00257622
Iteration 11/1000 | Loss: 0.00398156
Iteration 12/1000 | Loss: 0.00052738
Iteration 13/1000 | Loss: 0.00412629
Iteration 14/1000 | Loss: 0.00715352
Iteration 15/1000 | Loss: 0.00380845
Iteration 16/1000 | Loss: 0.00154741
Iteration 17/1000 | Loss: 0.00402005
Iteration 18/1000 | Loss: 0.00380064
Iteration 19/1000 | Loss: 0.00259609
Iteration 20/1000 | Loss: 0.00176843
Iteration 21/1000 | Loss: 0.00412436
Iteration 22/1000 | Loss: 0.00488785
Iteration 23/1000 | Loss: 0.00313710
Iteration 24/1000 | Loss: 0.00354669
Iteration 25/1000 | Loss: 0.00365255
Iteration 26/1000 | Loss: 0.00326914
Iteration 27/1000 | Loss: 0.00372218
Iteration 28/1000 | Loss: 0.00229049
Iteration 29/1000 | Loss: 0.00445078
Iteration 30/1000 | Loss: 0.00356708
Iteration 31/1000 | Loss: 0.00281085
Iteration 32/1000 | Loss: 0.00232639
Iteration 33/1000 | Loss: 0.00254164
Iteration 34/1000 | Loss: 0.00226407
Iteration 35/1000 | Loss: 0.00195339
Iteration 36/1000 | Loss: 0.00273270
Iteration 37/1000 | Loss: 0.00275449
Iteration 38/1000 | Loss: 0.00263747
Iteration 39/1000 | Loss: 0.00295551
Iteration 40/1000 | Loss: 0.00210848
Iteration 41/1000 | Loss: 0.00124286
Iteration 42/1000 | Loss: 0.00246271
Iteration 43/1000 | Loss: 0.00263089
Iteration 44/1000 | Loss: 0.00026583
Iteration 45/1000 | Loss: 0.00090692
Iteration 46/1000 | Loss: 0.00043970
Iteration 47/1000 | Loss: 0.00196018
Iteration 48/1000 | Loss: 0.00383369
Iteration 49/1000 | Loss: 0.00233875
Iteration 50/1000 | Loss: 0.00291661
Iteration 51/1000 | Loss: 0.00211388
Iteration 52/1000 | Loss: 0.00144717
Iteration 53/1000 | Loss: 0.00228356
Iteration 54/1000 | Loss: 0.00400854
Iteration 55/1000 | Loss: 0.00372927
Iteration 56/1000 | Loss: 0.00209130
Iteration 57/1000 | Loss: 0.00090870
Iteration 58/1000 | Loss: 0.00127207
Iteration 59/1000 | Loss: 0.00284956
Iteration 60/1000 | Loss: 0.00407246
Iteration 61/1000 | Loss: 0.00237901
Iteration 62/1000 | Loss: 0.00309541
Iteration 63/1000 | Loss: 0.00202133
Iteration 64/1000 | Loss: 0.00257755
Iteration 65/1000 | Loss: 0.00249369
Iteration 66/1000 | Loss: 0.00406305
Iteration 67/1000 | Loss: 0.00275123
Iteration 68/1000 | Loss: 0.00346307
Iteration 69/1000 | Loss: 0.00257666
Iteration 70/1000 | Loss: 0.00331821
Iteration 71/1000 | Loss: 0.00206411
Iteration 72/1000 | Loss: 0.00218559
Iteration 73/1000 | Loss: 0.00201910
Iteration 74/1000 | Loss: 0.00232112
Iteration 75/1000 | Loss: 0.00286222
Iteration 76/1000 | Loss: 0.00179388
Iteration 77/1000 | Loss: 0.00270275
Iteration 78/1000 | Loss: 0.00282349
Iteration 79/1000 | Loss: 0.00176881
Iteration 80/1000 | Loss: 0.00324754
Iteration 81/1000 | Loss: 0.00240229
Iteration 82/1000 | Loss: 0.00139252
Iteration 83/1000 | Loss: 0.00206638
Iteration 84/1000 | Loss: 0.00214510
Iteration 85/1000 | Loss: 0.00141963
Iteration 86/1000 | Loss: 0.00266771
Iteration 87/1000 | Loss: 0.00299685
Iteration 88/1000 | Loss: 0.00392770
Iteration 89/1000 | Loss: 0.00234753
Iteration 90/1000 | Loss: 0.00228231
Iteration 91/1000 | Loss: 0.00236559
Iteration 92/1000 | Loss: 0.00271941
Iteration 93/1000 | Loss: 0.00274925
Iteration 94/1000 | Loss: 0.00314338
Iteration 95/1000 | Loss: 0.00314171
Iteration 96/1000 | Loss: 0.00033868
Iteration 97/1000 | Loss: 0.00157678
Iteration 98/1000 | Loss: 0.00162193
Iteration 99/1000 | Loss: 0.00112727
Iteration 100/1000 | Loss: 0.00108093
Iteration 101/1000 | Loss: 0.00163097
Iteration 102/1000 | Loss: 0.00162163
Iteration 103/1000 | Loss: 0.00155891
Iteration 104/1000 | Loss: 0.00149582
Iteration 105/1000 | Loss: 0.00091870
Iteration 106/1000 | Loss: 0.00192927
Iteration 107/1000 | Loss: 0.00263737
Iteration 108/1000 | Loss: 0.00094613
Iteration 109/1000 | Loss: 0.00106793
Iteration 110/1000 | Loss: 0.00134897
Iteration 111/1000 | Loss: 0.00236477
Iteration 112/1000 | Loss: 0.00267511
Iteration 113/1000 | Loss: 0.00216959
Iteration 114/1000 | Loss: 0.00268852
Iteration 115/1000 | Loss: 0.00023305
Iteration 116/1000 | Loss: 0.00095395
Iteration 117/1000 | Loss: 0.00069652
Iteration 118/1000 | Loss: 0.00007827
Iteration 119/1000 | Loss: 0.00103415
Iteration 120/1000 | Loss: 0.00021281
Iteration 121/1000 | Loss: 0.00075394
Iteration 122/1000 | Loss: 0.00040850
Iteration 123/1000 | Loss: 0.00004317
Iteration 124/1000 | Loss: 0.00003188
Iteration 125/1000 | Loss: 0.00012083
Iteration 126/1000 | Loss: 0.00008923
Iteration 127/1000 | Loss: 0.00004893
Iteration 128/1000 | Loss: 0.00002502
Iteration 129/1000 | Loss: 0.00027517
Iteration 130/1000 | Loss: 0.00003340
Iteration 131/1000 | Loss: 0.00002508
Iteration 132/1000 | Loss: 0.00002238
Iteration 133/1000 | Loss: 0.00002096
Iteration 134/1000 | Loss: 0.00026482
Iteration 135/1000 | Loss: 0.00024063
Iteration 136/1000 | Loss: 0.00002046
Iteration 137/1000 | Loss: 0.00031846
Iteration 138/1000 | Loss: 0.00015169
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00028666
Iteration 141/1000 | Loss: 0.00020628
Iteration 142/1000 | Loss: 0.00016513
Iteration 143/1000 | Loss: 0.00028109
Iteration 144/1000 | Loss: 0.00034538
Iteration 145/1000 | Loss: 0.00013686
Iteration 146/1000 | Loss: 0.00023595
Iteration 147/1000 | Loss: 0.00002930
Iteration 148/1000 | Loss: 0.00002009
Iteration 149/1000 | Loss: 0.00001733
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001575
Iteration 152/1000 | Loss: 0.00001518
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001443
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001414
Iteration 157/1000 | Loss: 0.00001413
Iteration 158/1000 | Loss: 0.00001413
Iteration 159/1000 | Loss: 0.00001413
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001385
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001381
Iteration 165/1000 | Loss: 0.00001380
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001371
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001355
Iteration 170/1000 | Loss: 0.00001352
Iteration 171/1000 | Loss: 0.00001352
Iteration 172/1000 | Loss: 0.00001351
Iteration 173/1000 | Loss: 0.00001350
Iteration 174/1000 | Loss: 0.00001350
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00001349
Iteration 177/1000 | Loss: 0.00001349
Iteration 178/1000 | Loss: 0.00001348
Iteration 179/1000 | Loss: 0.00001348
Iteration 180/1000 | Loss: 0.00001348
Iteration 181/1000 | Loss: 0.00001347
Iteration 182/1000 | Loss: 0.00001347
Iteration 183/1000 | Loss: 0.00001347
Iteration 184/1000 | Loss: 0.00001347
Iteration 185/1000 | Loss: 0.00001347
Iteration 186/1000 | Loss: 0.00001347
Iteration 187/1000 | Loss: 0.00001347
Iteration 188/1000 | Loss: 0.00001347
Iteration 189/1000 | Loss: 0.00001347
Iteration 190/1000 | Loss: 0.00001347
Iteration 191/1000 | Loss: 0.00001347
Iteration 192/1000 | Loss: 0.00001347
Iteration 193/1000 | Loss: 0.00001347
Iteration 194/1000 | Loss: 0.00001346
Iteration 195/1000 | Loss: 0.00001346
Iteration 196/1000 | Loss: 0.00001346
Iteration 197/1000 | Loss: 0.00001346
Iteration 198/1000 | Loss: 0.00001346
Iteration 199/1000 | Loss: 0.00001345
Iteration 200/1000 | Loss: 0.00001345
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001343
Iteration 204/1000 | Loss: 0.00001343
Iteration 205/1000 | Loss: 0.00001343
Iteration 206/1000 | Loss: 0.00001343
Iteration 207/1000 | Loss: 0.00001343
Iteration 208/1000 | Loss: 0.00001342
Iteration 209/1000 | Loss: 0.00001342
Iteration 210/1000 | Loss: 0.00001342
Iteration 211/1000 | Loss: 0.00001342
Iteration 212/1000 | Loss: 0.00001341
Iteration 213/1000 | Loss: 0.00001341
Iteration 214/1000 | Loss: 0.00001341
Iteration 215/1000 | Loss: 0.00001340
Iteration 216/1000 | Loss: 0.00001340
Iteration 217/1000 | Loss: 0.00001340
Iteration 218/1000 | Loss: 0.00001340
Iteration 219/1000 | Loss: 0.00001340
Iteration 220/1000 | Loss: 0.00001339
Iteration 221/1000 | Loss: 0.00001339
Iteration 222/1000 | Loss: 0.00001339
Iteration 223/1000 | Loss: 0.00001339
Iteration 224/1000 | Loss: 0.00001339
Iteration 225/1000 | Loss: 0.00001339
Iteration 226/1000 | Loss: 0.00001339
Iteration 227/1000 | Loss: 0.00001339
Iteration 228/1000 | Loss: 0.00001339
Iteration 229/1000 | Loss: 0.00001339
Iteration 230/1000 | Loss: 0.00001339
Iteration 231/1000 | Loss: 0.00001338
Iteration 232/1000 | Loss: 0.00001338
Iteration 233/1000 | Loss: 0.00001338
Iteration 234/1000 | Loss: 0.00001338
Iteration 235/1000 | Loss: 0.00001338
Iteration 236/1000 | Loss: 0.00001338
Iteration 237/1000 | Loss: 0.00001338
Iteration 238/1000 | Loss: 0.00001338
Iteration 239/1000 | Loss: 0.00001338
Iteration 240/1000 | Loss: 0.00001337
Iteration 241/1000 | Loss: 0.00001337
Iteration 242/1000 | Loss: 0.00001337
Iteration 243/1000 | Loss: 0.00001337
Iteration 244/1000 | Loss: 0.00001337
Iteration 245/1000 | Loss: 0.00001337
Iteration 246/1000 | Loss: 0.00001337
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001336
Iteration 253/1000 | Loss: 0.00001336
Iteration 254/1000 | Loss: 0.00001335
Iteration 255/1000 | Loss: 0.00001335
Iteration 256/1000 | Loss: 0.00001335
Iteration 257/1000 | Loss: 0.00001335
Iteration 258/1000 | Loss: 0.00001335
Iteration 259/1000 | Loss: 0.00001335
Iteration 260/1000 | Loss: 0.00001335
Iteration 261/1000 | Loss: 0.00001335
Iteration 262/1000 | Loss: 0.00001335
Iteration 263/1000 | Loss: 0.00001335
Iteration 264/1000 | Loss: 0.00001335
Iteration 265/1000 | Loss: 0.00001334
Iteration 266/1000 | Loss: 0.00001334
Iteration 267/1000 | Loss: 0.00001334
Iteration 268/1000 | Loss: 0.00001334
Iteration 269/1000 | Loss: 0.00001334
Iteration 270/1000 | Loss: 0.00001334
Iteration 271/1000 | Loss: 0.00001334
Iteration 272/1000 | Loss: 0.00001334
Iteration 273/1000 | Loss: 0.00001333
Iteration 274/1000 | Loss: 0.00001333
Iteration 275/1000 | Loss: 0.00001333
Iteration 276/1000 | Loss: 0.00001333
Iteration 277/1000 | Loss: 0.00001333
Iteration 278/1000 | Loss: 0.00001333
Iteration 279/1000 | Loss: 0.00001333
Iteration 280/1000 | Loss: 0.00001333
Iteration 281/1000 | Loss: 0.00001333
Iteration 282/1000 | Loss: 0.00001333
Iteration 283/1000 | Loss: 0.00001333
Iteration 284/1000 | Loss: 0.00001333
Iteration 285/1000 | Loss: 0.00001333
Iteration 286/1000 | Loss: 0.00001333
Iteration 287/1000 | Loss: 0.00001333
Iteration 288/1000 | Loss: 0.00001333
Iteration 289/1000 | Loss: 0.00001332
Iteration 290/1000 | Loss: 0.00001332
Iteration 291/1000 | Loss: 0.00001332
Iteration 292/1000 | Loss: 0.00001332
Iteration 293/1000 | Loss: 0.00001332
Iteration 294/1000 | Loss: 0.00001332
Iteration 295/1000 | Loss: 0.00001332
Iteration 296/1000 | Loss: 0.00001332
Iteration 297/1000 | Loss: 0.00001332
Iteration 298/1000 | Loss: 0.00001332
Iteration 299/1000 | Loss: 0.00001332
Iteration 300/1000 | Loss: 0.00001332
Iteration 301/1000 | Loss: 0.00001332
Iteration 302/1000 | Loss: 0.00001332
Iteration 303/1000 | Loss: 0.00001331
Iteration 304/1000 | Loss: 0.00001331
Iteration 305/1000 | Loss: 0.00001331
Iteration 306/1000 | Loss: 0.00001331
Iteration 307/1000 | Loss: 0.00001331
Iteration 308/1000 | Loss: 0.00001331
Iteration 309/1000 | Loss: 0.00001331
Iteration 310/1000 | Loss: 0.00001331
Iteration 311/1000 | Loss: 0.00001331
Iteration 312/1000 | Loss: 0.00001331
Iteration 313/1000 | Loss: 0.00001331
Iteration 314/1000 | Loss: 0.00001331
Iteration 315/1000 | Loss: 0.00001331
Iteration 316/1000 | Loss: 0.00001331
Iteration 317/1000 | Loss: 0.00001330
Iteration 318/1000 | Loss: 0.00001330
Iteration 319/1000 | Loss: 0.00001330
Iteration 320/1000 | Loss: 0.00001330
Iteration 321/1000 | Loss: 0.00001330
Iteration 322/1000 | Loss: 0.00001330
Iteration 323/1000 | Loss: 0.00001330
Iteration 324/1000 | Loss: 0.00001330
Iteration 325/1000 | Loss: 0.00001330
Iteration 326/1000 | Loss: 0.00001330
Iteration 327/1000 | Loss: 0.00001330
Iteration 328/1000 | Loss: 0.00001330
Iteration 329/1000 | Loss: 0.00001330
Iteration 330/1000 | Loss: 0.00001330
Iteration 331/1000 | Loss: 0.00001330
Iteration 332/1000 | Loss: 0.00001330
Iteration 333/1000 | Loss: 0.00001330
Iteration 334/1000 | Loss: 0.00001330
Iteration 335/1000 | Loss: 0.00001330
Iteration 336/1000 | Loss: 0.00001330
Iteration 337/1000 | Loss: 0.00001330
Iteration 338/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [1.3301643775776029e-05, 1.3301643775776029e-05, 1.3301643775776029e-05, 1.3301643775776029e-05, 1.3301643775776029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3301643775776029e-05

Optimization complete. Final v2v error: 3.016571521759033 mm

Highest mean error: 5.471577167510986 mm for frame 47

Lowest mean error: 2.806044578552246 mm for frame 1

Saving results

Total time: 275.1718940734863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790212
Iteration 2/25 | Loss: 0.00118492
Iteration 3/25 | Loss: 0.00087907
Iteration 4/25 | Loss: 0.00085196
Iteration 5/25 | Loss: 0.00083674
Iteration 6/25 | Loss: 0.00083627
Iteration 7/25 | Loss: 0.00083396
Iteration 8/25 | Loss: 0.00083178
Iteration 9/25 | Loss: 0.00083095
Iteration 10/25 | Loss: 0.00083028
Iteration 11/25 | Loss: 0.00082972
Iteration 12/25 | Loss: 0.00082944
Iteration 13/25 | Loss: 0.00082931
Iteration 14/25 | Loss: 0.00082925
Iteration 15/25 | Loss: 0.00082925
Iteration 16/25 | Loss: 0.00082924
Iteration 17/25 | Loss: 0.00082924
Iteration 18/25 | Loss: 0.00082924
Iteration 19/25 | Loss: 0.00082924
Iteration 20/25 | Loss: 0.00082924
Iteration 21/25 | Loss: 0.00082924
Iteration 22/25 | Loss: 0.00082924
Iteration 23/25 | Loss: 0.00082924
Iteration 24/25 | Loss: 0.00082924
Iteration 25/25 | Loss: 0.00082924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56699324
Iteration 2/25 | Loss: 0.00056583
Iteration 3/25 | Loss: 0.00056192
Iteration 4/25 | Loss: 0.00056192
Iteration 5/25 | Loss: 0.00056192
Iteration 6/25 | Loss: 0.00056192
Iteration 7/25 | Loss: 0.00056192
Iteration 8/25 | Loss: 0.00056192
Iteration 9/25 | Loss: 0.00056192
Iteration 10/25 | Loss: 0.00056192
Iteration 11/25 | Loss: 0.00056192
Iteration 12/25 | Loss: 0.00056192
Iteration 13/25 | Loss: 0.00056192
Iteration 14/25 | Loss: 0.00056192
Iteration 15/25 | Loss: 0.00056192
Iteration 16/25 | Loss: 0.00056192
Iteration 17/25 | Loss: 0.00056192
Iteration 18/25 | Loss: 0.00056192
Iteration 19/25 | Loss: 0.00056192
Iteration 20/25 | Loss: 0.00056192
Iteration 21/25 | Loss: 0.00056192
Iteration 22/25 | Loss: 0.00056192
Iteration 23/25 | Loss: 0.00056192
Iteration 24/25 | Loss: 0.00056192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005619204021058977, 0.0005619204021058977, 0.0005619204021058977, 0.0005619204021058977, 0.0005619204021058977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005619204021058977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056192
Iteration 2/1000 | Loss: 0.00003352
Iteration 3/1000 | Loss: 0.00004573
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001543
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001477
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001366
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001360
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001351
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001350
Iteration 61/1000 | Loss: 0.00001350
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001348
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001348
Iteration 87/1000 | Loss: 0.00001348
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001345
Iteration 114/1000 | Loss: 0.00001345
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001344
Iteration 128/1000 | Loss: 0.00001344
Iteration 129/1000 | Loss: 0.00001344
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3428911188384518e-05, 1.3428911188384518e-05, 1.3428911188384518e-05, 1.3428911188384518e-05, 1.3428911188384518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3428911188384518e-05

Optimization complete. Final v2v error: 3.1331892013549805 mm

Highest mean error: 3.358055830001831 mm for frame 116

Lowest mean error: 2.8556177616119385 mm for frame 22

Saving results

Total time: 56.42086410522461
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975451
Iteration 2/25 | Loss: 0.00219540
Iteration 3/25 | Loss: 0.00158848
Iteration 4/25 | Loss: 0.00145080
Iteration 5/25 | Loss: 0.00121783
Iteration 6/25 | Loss: 0.00118009
Iteration 7/25 | Loss: 0.00115370
Iteration 8/25 | Loss: 0.00113933
Iteration 9/25 | Loss: 0.00113753
Iteration 10/25 | Loss: 0.00117968
Iteration 11/25 | Loss: 0.00118195
Iteration 12/25 | Loss: 0.00113073
Iteration 13/25 | Loss: 0.00108714
Iteration 14/25 | Loss: 0.00102871
Iteration 15/25 | Loss: 0.00101141
Iteration 16/25 | Loss: 0.00100846
Iteration 17/25 | Loss: 0.00100401
Iteration 18/25 | Loss: 0.00098170
Iteration 19/25 | Loss: 0.00097287
Iteration 20/25 | Loss: 0.00096949
Iteration 21/25 | Loss: 0.00096388
Iteration 22/25 | Loss: 0.00095904
Iteration 23/25 | Loss: 0.00095537
Iteration 24/25 | Loss: 0.00095624
Iteration 25/25 | Loss: 0.00095459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55643594
Iteration 2/25 | Loss: 0.00082823
Iteration 3/25 | Loss: 0.00082247
Iteration 4/25 | Loss: 0.00082247
Iteration 5/25 | Loss: 0.00082247
Iteration 6/25 | Loss: 0.00082247
Iteration 7/25 | Loss: 0.00082247
Iteration 8/25 | Loss: 0.00082247
Iteration 9/25 | Loss: 0.00082247
Iteration 10/25 | Loss: 0.00082247
Iteration 11/25 | Loss: 0.00082247
Iteration 12/25 | Loss: 0.00082247
Iteration 13/25 | Loss: 0.00082247
Iteration 14/25 | Loss: 0.00082247
Iteration 15/25 | Loss: 0.00082247
Iteration 16/25 | Loss: 0.00082247
Iteration 17/25 | Loss: 0.00082247
Iteration 18/25 | Loss: 0.00082247
Iteration 19/25 | Loss: 0.00082247
Iteration 20/25 | Loss: 0.00082247
Iteration 21/25 | Loss: 0.00082247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008224711054936051, 0.0008224711054936051, 0.0008224711054936051, 0.0008224711054936051, 0.0008224711054936051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008224711054936051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082247
Iteration 2/1000 | Loss: 0.00011193
Iteration 3/1000 | Loss: 0.00008095
Iteration 4/1000 | Loss: 0.00008401
Iteration 5/1000 | Loss: 0.00005988
Iteration 6/1000 | Loss: 0.00023071
Iteration 7/1000 | Loss: 0.00006564
Iteration 8/1000 | Loss: 0.00006892
Iteration 9/1000 | Loss: 0.00006547
Iteration 10/1000 | Loss: 0.00005768
Iteration 11/1000 | Loss: 0.00005365
Iteration 12/1000 | Loss: 0.00025411
Iteration 13/1000 | Loss: 0.00005191
Iteration 14/1000 | Loss: 0.00004655
Iteration 15/1000 | Loss: 0.00005209
Iteration 16/1000 | Loss: 0.00005192
Iteration 17/1000 | Loss: 0.00005861
Iteration 18/1000 | Loss: 0.00005448
Iteration 19/1000 | Loss: 0.00005833
Iteration 20/1000 | Loss: 0.00005503
Iteration 21/1000 | Loss: 0.00005829
Iteration 22/1000 | Loss: 0.00004293
Iteration 23/1000 | Loss: 0.00005204
Iteration 24/1000 | Loss: 0.00005863
Iteration 25/1000 | Loss: 0.00005110
Iteration 26/1000 | Loss: 0.00005811
Iteration 27/1000 | Loss: 0.00005151
Iteration 28/1000 | Loss: 0.00005635
Iteration 29/1000 | Loss: 0.00006344
Iteration 30/1000 | Loss: 0.00005896
Iteration 31/1000 | Loss: 0.00005801
Iteration 32/1000 | Loss: 0.00005646
Iteration 33/1000 | Loss: 0.00005887
Iteration 34/1000 | Loss: 0.00005492
Iteration 35/1000 | Loss: 0.00005657
Iteration 36/1000 | Loss: 0.00006017
Iteration 37/1000 | Loss: 0.00005555
Iteration 38/1000 | Loss: 0.00005950
Iteration 39/1000 | Loss: 0.00007904
Iteration 40/1000 | Loss: 0.00005050
Iteration 41/1000 | Loss: 0.00004325
Iteration 42/1000 | Loss: 0.00003895
Iteration 43/1000 | Loss: 0.00003785
Iteration 44/1000 | Loss: 0.00003545
Iteration 45/1000 | Loss: 0.00003416
Iteration 46/1000 | Loss: 0.00003332
Iteration 47/1000 | Loss: 0.00003261
Iteration 48/1000 | Loss: 0.00003219
Iteration 49/1000 | Loss: 0.00003184
Iteration 50/1000 | Loss: 0.00086129
Iteration 51/1000 | Loss: 0.00061435
Iteration 52/1000 | Loss: 0.00069881
Iteration 53/1000 | Loss: 0.00008494
Iteration 54/1000 | Loss: 0.00005690
Iteration 55/1000 | Loss: 0.00003980
Iteration 56/1000 | Loss: 0.00003091
Iteration 57/1000 | Loss: 0.00002995
Iteration 58/1000 | Loss: 0.00002938
Iteration 59/1000 | Loss: 0.00002882
Iteration 60/1000 | Loss: 0.00002841
Iteration 61/1000 | Loss: 0.00002809
Iteration 62/1000 | Loss: 0.00002789
Iteration 63/1000 | Loss: 0.00002781
Iteration 64/1000 | Loss: 0.00002775
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002773
Iteration 67/1000 | Loss: 0.00002768
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00002765
Iteration 70/1000 | Loss: 0.00002764
Iteration 71/1000 | Loss: 0.00002764
Iteration 72/1000 | Loss: 0.00002764
Iteration 73/1000 | Loss: 0.00002763
Iteration 74/1000 | Loss: 0.00002763
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00002758
Iteration 77/1000 | Loss: 0.00002758
Iteration 78/1000 | Loss: 0.00002758
Iteration 79/1000 | Loss: 0.00002757
Iteration 80/1000 | Loss: 0.00002757
Iteration 81/1000 | Loss: 0.00002756
Iteration 82/1000 | Loss: 0.00002756
Iteration 83/1000 | Loss: 0.00002756
Iteration 84/1000 | Loss: 0.00002756
Iteration 85/1000 | Loss: 0.00002756
Iteration 86/1000 | Loss: 0.00002756
Iteration 87/1000 | Loss: 0.00002756
Iteration 88/1000 | Loss: 0.00002755
Iteration 89/1000 | Loss: 0.00002755
Iteration 90/1000 | Loss: 0.00002754
Iteration 91/1000 | Loss: 0.00002753
Iteration 92/1000 | Loss: 0.00002753
Iteration 93/1000 | Loss: 0.00002753
Iteration 94/1000 | Loss: 0.00002753
Iteration 95/1000 | Loss: 0.00002753
Iteration 96/1000 | Loss: 0.00002752
Iteration 97/1000 | Loss: 0.00002750
Iteration 98/1000 | Loss: 0.00002750
Iteration 99/1000 | Loss: 0.00002749
Iteration 100/1000 | Loss: 0.00002748
Iteration 101/1000 | Loss: 0.00002748
Iteration 102/1000 | Loss: 0.00002748
Iteration 103/1000 | Loss: 0.00002747
Iteration 104/1000 | Loss: 0.00002747
Iteration 105/1000 | Loss: 0.00002747
Iteration 106/1000 | Loss: 0.00002746
Iteration 107/1000 | Loss: 0.00002746
Iteration 108/1000 | Loss: 0.00002746
Iteration 109/1000 | Loss: 0.00002746
Iteration 110/1000 | Loss: 0.00002745
Iteration 111/1000 | Loss: 0.00002745
Iteration 112/1000 | Loss: 0.00002745
Iteration 113/1000 | Loss: 0.00002745
Iteration 114/1000 | Loss: 0.00002745
Iteration 115/1000 | Loss: 0.00002744
Iteration 116/1000 | Loss: 0.00002744
Iteration 117/1000 | Loss: 0.00002744
Iteration 118/1000 | Loss: 0.00002744
Iteration 119/1000 | Loss: 0.00002744
Iteration 120/1000 | Loss: 0.00002743
Iteration 121/1000 | Loss: 0.00002743
Iteration 122/1000 | Loss: 0.00002743
Iteration 123/1000 | Loss: 0.00002742
Iteration 124/1000 | Loss: 0.00002742
Iteration 125/1000 | Loss: 0.00002742
Iteration 126/1000 | Loss: 0.00002742
Iteration 127/1000 | Loss: 0.00002742
Iteration 128/1000 | Loss: 0.00002742
Iteration 129/1000 | Loss: 0.00002742
Iteration 130/1000 | Loss: 0.00002742
Iteration 131/1000 | Loss: 0.00002742
Iteration 132/1000 | Loss: 0.00002741
Iteration 133/1000 | Loss: 0.00002741
Iteration 134/1000 | Loss: 0.00002740
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002740
Iteration 137/1000 | Loss: 0.00002740
Iteration 138/1000 | Loss: 0.00002740
Iteration 139/1000 | Loss: 0.00002739
Iteration 140/1000 | Loss: 0.00002739
Iteration 141/1000 | Loss: 0.00002739
Iteration 142/1000 | Loss: 0.00002739
Iteration 143/1000 | Loss: 0.00002739
Iteration 144/1000 | Loss: 0.00002739
Iteration 145/1000 | Loss: 0.00002739
Iteration 146/1000 | Loss: 0.00002739
Iteration 147/1000 | Loss: 0.00002739
Iteration 148/1000 | Loss: 0.00002739
Iteration 149/1000 | Loss: 0.00002739
Iteration 150/1000 | Loss: 0.00002739
Iteration 151/1000 | Loss: 0.00002738
Iteration 152/1000 | Loss: 0.00002738
Iteration 153/1000 | Loss: 0.00002738
Iteration 154/1000 | Loss: 0.00002738
Iteration 155/1000 | Loss: 0.00002738
Iteration 156/1000 | Loss: 0.00002738
Iteration 157/1000 | Loss: 0.00002738
Iteration 158/1000 | Loss: 0.00002738
Iteration 159/1000 | Loss: 0.00002738
Iteration 160/1000 | Loss: 0.00002738
Iteration 161/1000 | Loss: 0.00002738
Iteration 162/1000 | Loss: 0.00002738
Iteration 163/1000 | Loss: 0.00002738
Iteration 164/1000 | Loss: 0.00002738
Iteration 165/1000 | Loss: 0.00002738
Iteration 166/1000 | Loss: 0.00002738
Iteration 167/1000 | Loss: 0.00002738
Iteration 168/1000 | Loss: 0.00002738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.7378657250665128e-05, 2.7378657250665128e-05, 2.7378657250665128e-05, 2.7378657250665128e-05, 2.7378657250665128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7378657250665128e-05

Optimization complete. Final v2v error: 4.253427028656006 mm

Highest mean error: 5.493414402008057 mm for frame 84

Lowest mean error: 3.1285648345947266 mm for frame 20

Saving results

Total time: 140.88958859443665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087083
Iteration 2/25 | Loss: 0.01087082
Iteration 3/25 | Loss: 0.01087082
Iteration 4/25 | Loss: 0.01087082
Iteration 5/25 | Loss: 0.01087082
Iteration 6/25 | Loss: 0.01087082
Iteration 7/25 | Loss: 0.01087082
Iteration 8/25 | Loss: 0.01087082
Iteration 9/25 | Loss: 0.01087082
Iteration 10/25 | Loss: 0.01087082
Iteration 11/25 | Loss: 0.01087082
Iteration 12/25 | Loss: 0.01087081
Iteration 13/25 | Loss: 0.01087081
Iteration 14/25 | Loss: 0.01087081
Iteration 15/25 | Loss: 0.01087081
Iteration 16/25 | Loss: 0.01087081
Iteration 17/25 | Loss: 0.01087081
Iteration 18/25 | Loss: 0.01087081
Iteration 19/25 | Loss: 0.01087081
Iteration 20/25 | Loss: 0.01087081
Iteration 21/25 | Loss: 0.01087081
Iteration 22/25 | Loss: 0.01087081
Iteration 23/25 | Loss: 0.01087081
Iteration 24/25 | Loss: 0.01087080
Iteration 25/25 | Loss: 0.01087080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80936253
Iteration 2/25 | Loss: 0.07707290
Iteration 3/25 | Loss: 0.07706033
Iteration 4/25 | Loss: 0.07706033
Iteration 5/25 | Loss: 0.07706032
Iteration 6/25 | Loss: 0.07706032
Iteration 7/25 | Loss: 0.07706031
Iteration 8/25 | Loss: 0.07706031
Iteration 9/25 | Loss: 0.07706031
Iteration 10/25 | Loss: 0.07706031
Iteration 11/25 | Loss: 0.07706031
Iteration 12/25 | Loss: 0.07706031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.07706031203269958, 0.07706031203269958, 0.07706031203269958, 0.07706031203269958, 0.07706031203269958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07706031203269958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07706031
Iteration 2/1000 | Loss: 0.00051261
Iteration 3/1000 | Loss: 0.00012381
Iteration 4/1000 | Loss: 0.00005487
Iteration 5/1000 | Loss: 0.00003479
Iteration 6/1000 | Loss: 0.00003049
Iteration 7/1000 | Loss: 0.00002754
Iteration 8/1000 | Loss: 0.00002512
Iteration 9/1000 | Loss: 0.00002367
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001222
Iteration 39/1000 | Loss: 0.00001221
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2044728464388754e-05, 1.2044728464388754e-05, 1.2044728464388754e-05, 1.2044728464388754e-05, 1.2044728464388754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2044728464388754e-05

Optimization complete. Final v2v error: 2.9679737091064453 mm

Highest mean error: 3.150552272796631 mm for frame 3

Lowest mean error: 2.628892183303833 mm for frame 185

Saving results

Total time: 49.11670684814453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468329
Iteration 2/25 | Loss: 0.00136780
Iteration 3/25 | Loss: 0.00106101
Iteration 4/25 | Loss: 0.00098608
Iteration 5/25 | Loss: 0.00096549
Iteration 6/25 | Loss: 0.00095304
Iteration 7/25 | Loss: 0.00095654
Iteration 8/25 | Loss: 0.00092002
Iteration 9/25 | Loss: 0.00089851
Iteration 10/25 | Loss: 0.00089551
Iteration 11/25 | Loss: 0.00088772
Iteration 12/25 | Loss: 0.00087721
Iteration 13/25 | Loss: 0.00087485
Iteration 14/25 | Loss: 0.00086800
Iteration 15/25 | Loss: 0.00086747
Iteration 16/25 | Loss: 0.00086372
Iteration 17/25 | Loss: 0.00086099
Iteration 18/25 | Loss: 0.00086228
Iteration 19/25 | Loss: 0.00086041
Iteration 20/25 | Loss: 0.00085687
Iteration 21/25 | Loss: 0.00085533
Iteration 22/25 | Loss: 0.00085435
Iteration 23/25 | Loss: 0.00085391
Iteration 24/25 | Loss: 0.00085383
Iteration 25/25 | Loss: 0.00085383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52577519
Iteration 2/25 | Loss: 0.00050483
Iteration 3/25 | Loss: 0.00050482
Iteration 4/25 | Loss: 0.00050482
Iteration 5/25 | Loss: 0.00050482
Iteration 6/25 | Loss: 0.00050482
Iteration 7/25 | Loss: 0.00050482
Iteration 8/25 | Loss: 0.00050482
Iteration 9/25 | Loss: 0.00050482
Iteration 10/25 | Loss: 0.00050482
Iteration 11/25 | Loss: 0.00050482
Iteration 12/25 | Loss: 0.00050482
Iteration 13/25 | Loss: 0.00050482
Iteration 14/25 | Loss: 0.00050482
Iteration 15/25 | Loss: 0.00050482
Iteration 16/25 | Loss: 0.00050482
Iteration 17/25 | Loss: 0.00050482
Iteration 18/25 | Loss: 0.00050482
Iteration 19/25 | Loss: 0.00050482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005048206076025963, 0.0005048206076025963, 0.0005048206076025963, 0.0005048206076025963, 0.0005048206076025963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005048206076025963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050482
Iteration 2/1000 | Loss: 0.00002807
Iteration 3/1000 | Loss: 0.00002209
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001694
Iteration 10/1000 | Loss: 0.00001678
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001636
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001628
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001627
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001624
Iteration 59/1000 | Loss: 0.00001624
Iteration 60/1000 | Loss: 0.00001624
Iteration 61/1000 | Loss: 0.00001624
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001623
Iteration 64/1000 | Loss: 0.00001623
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001622
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001607
Iteration 111/1000 | Loss: 0.00001607
Iteration 112/1000 | Loss: 0.00001607
Iteration 113/1000 | Loss: 0.00001607
Iteration 114/1000 | Loss: 0.00001606
Iteration 115/1000 | Loss: 0.00001606
Iteration 116/1000 | Loss: 0.00001606
Iteration 117/1000 | Loss: 0.00001606
Iteration 118/1000 | Loss: 0.00001606
Iteration 119/1000 | Loss: 0.00001605
Iteration 120/1000 | Loss: 0.00001605
Iteration 121/1000 | Loss: 0.00001605
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001605
Iteration 124/1000 | Loss: 0.00001605
Iteration 125/1000 | Loss: 0.00001605
Iteration 126/1000 | Loss: 0.00001605
Iteration 127/1000 | Loss: 0.00001605
Iteration 128/1000 | Loss: 0.00001605
Iteration 129/1000 | Loss: 0.00001605
Iteration 130/1000 | Loss: 0.00001605
Iteration 131/1000 | Loss: 0.00001605
Iteration 132/1000 | Loss: 0.00001605
Iteration 133/1000 | Loss: 0.00001605
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001604
Iteration 139/1000 | Loss: 0.00001604
Iteration 140/1000 | Loss: 0.00001604
Iteration 141/1000 | Loss: 0.00001604
Iteration 142/1000 | Loss: 0.00001604
Iteration 143/1000 | Loss: 0.00001604
Iteration 144/1000 | Loss: 0.00001604
Iteration 145/1000 | Loss: 0.00001604
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001604
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001603
Iteration 162/1000 | Loss: 0.00001603
Iteration 163/1000 | Loss: 0.00001603
Iteration 164/1000 | Loss: 0.00001602
Iteration 165/1000 | Loss: 0.00001602
Iteration 166/1000 | Loss: 0.00001602
Iteration 167/1000 | Loss: 0.00001602
Iteration 168/1000 | Loss: 0.00001602
Iteration 169/1000 | Loss: 0.00001602
Iteration 170/1000 | Loss: 0.00001602
Iteration 171/1000 | Loss: 0.00001602
Iteration 172/1000 | Loss: 0.00001602
Iteration 173/1000 | Loss: 0.00001602
Iteration 174/1000 | Loss: 0.00001602
Iteration 175/1000 | Loss: 0.00001602
Iteration 176/1000 | Loss: 0.00001602
Iteration 177/1000 | Loss: 0.00001602
Iteration 178/1000 | Loss: 0.00001602
Iteration 179/1000 | Loss: 0.00001602
Iteration 180/1000 | Loss: 0.00001601
Iteration 181/1000 | Loss: 0.00001601
Iteration 182/1000 | Loss: 0.00001601
Iteration 183/1000 | Loss: 0.00001601
Iteration 184/1000 | Loss: 0.00001601
Iteration 185/1000 | Loss: 0.00001601
Iteration 186/1000 | Loss: 0.00001601
Iteration 187/1000 | Loss: 0.00001601
Iteration 188/1000 | Loss: 0.00001601
Iteration 189/1000 | Loss: 0.00001601
Iteration 190/1000 | Loss: 0.00001601
Iteration 191/1000 | Loss: 0.00001601
Iteration 192/1000 | Loss: 0.00001601
Iteration 193/1000 | Loss: 0.00001601
Iteration 194/1000 | Loss: 0.00001601
Iteration 195/1000 | Loss: 0.00001601
Iteration 196/1000 | Loss: 0.00001601
Iteration 197/1000 | Loss: 0.00001601
Iteration 198/1000 | Loss: 0.00001600
Iteration 199/1000 | Loss: 0.00001600
Iteration 200/1000 | Loss: 0.00001600
Iteration 201/1000 | Loss: 0.00001600
Iteration 202/1000 | Loss: 0.00001600
Iteration 203/1000 | Loss: 0.00001600
Iteration 204/1000 | Loss: 0.00001600
Iteration 205/1000 | Loss: 0.00001600
Iteration 206/1000 | Loss: 0.00001600
Iteration 207/1000 | Loss: 0.00001600
Iteration 208/1000 | Loss: 0.00001600
Iteration 209/1000 | Loss: 0.00001600
Iteration 210/1000 | Loss: 0.00001600
Iteration 211/1000 | Loss: 0.00001600
Iteration 212/1000 | Loss: 0.00001600
Iteration 213/1000 | Loss: 0.00001600
Iteration 214/1000 | Loss: 0.00001600
Iteration 215/1000 | Loss: 0.00001600
Iteration 216/1000 | Loss: 0.00001600
Iteration 217/1000 | Loss: 0.00001600
Iteration 218/1000 | Loss: 0.00001600
Iteration 219/1000 | Loss: 0.00001600
Iteration 220/1000 | Loss: 0.00001600
Iteration 221/1000 | Loss: 0.00001600
Iteration 222/1000 | Loss: 0.00001600
Iteration 223/1000 | Loss: 0.00001600
Iteration 224/1000 | Loss: 0.00001600
Iteration 225/1000 | Loss: 0.00001600
Iteration 226/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.599613460712135e-05, 1.599613460712135e-05, 1.599613460712135e-05, 1.599613460712135e-05, 1.599613460712135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599613460712135e-05

Optimization complete. Final v2v error: 3.3939430713653564 mm

Highest mean error: 3.6957905292510986 mm for frame 122

Lowest mean error: 3.2087888717651367 mm for frame 67

Saving results

Total time: 69.62918782234192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083519
Iteration 2/25 | Loss: 0.00341176
Iteration 3/25 | Loss: 0.00217278
Iteration 4/25 | Loss: 0.00204741
Iteration 5/25 | Loss: 0.00211189
Iteration 6/25 | Loss: 0.00230358
Iteration 7/25 | Loss: 0.00173429
Iteration 8/25 | Loss: 0.00136395
Iteration 9/25 | Loss: 0.00122834
Iteration 10/25 | Loss: 0.00118012
Iteration 11/25 | Loss: 0.00113904
Iteration 12/25 | Loss: 0.00112719
Iteration 13/25 | Loss: 0.00113764
Iteration 14/25 | Loss: 0.00110463
Iteration 15/25 | Loss: 0.00109106
Iteration 16/25 | Loss: 0.00108827
Iteration 17/25 | Loss: 0.00108447
Iteration 18/25 | Loss: 0.00108484
Iteration 19/25 | Loss: 0.00107413
Iteration 20/25 | Loss: 0.00107157
Iteration 21/25 | Loss: 0.00106639
Iteration 22/25 | Loss: 0.00106729
Iteration 23/25 | Loss: 0.00106563
Iteration 24/25 | Loss: 0.00106600
Iteration 25/25 | Loss: 0.00105528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68015027
Iteration 2/25 | Loss: 0.00077509
Iteration 3/25 | Loss: 0.00077509
Iteration 4/25 | Loss: 0.00077509
Iteration 5/25 | Loss: 0.00077509
Iteration 6/25 | Loss: 0.00077509
Iteration 7/25 | Loss: 0.00077509
Iteration 8/25 | Loss: 0.00077509
Iteration 9/25 | Loss: 0.00077509
Iteration 10/25 | Loss: 0.00077509
Iteration 11/25 | Loss: 0.00077509
Iteration 12/25 | Loss: 0.00077509
Iteration 13/25 | Loss: 0.00077509
Iteration 14/25 | Loss: 0.00077509
Iteration 15/25 | Loss: 0.00077509
Iteration 16/25 | Loss: 0.00077509
Iteration 17/25 | Loss: 0.00077509
Iteration 18/25 | Loss: 0.00077509
Iteration 19/25 | Loss: 0.00077509
Iteration 20/25 | Loss: 0.00077509
Iteration 21/25 | Loss: 0.00077509
Iteration 22/25 | Loss: 0.00077508
Iteration 23/25 | Loss: 0.00077509
Iteration 24/25 | Loss: 0.00077509
Iteration 25/25 | Loss: 0.00077509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077508
Iteration 2/1000 | Loss: 0.00015810
Iteration 3/1000 | Loss: 0.00042776
Iteration 4/1000 | Loss: 0.00024776
Iteration 5/1000 | Loss: 0.00017649
Iteration 6/1000 | Loss: 0.00022222
Iteration 7/1000 | Loss: 0.00018363
Iteration 8/1000 | Loss: 0.00007329
Iteration 9/1000 | Loss: 0.00005714
Iteration 10/1000 | Loss: 0.00088515
Iteration 11/1000 | Loss: 0.00119060
Iteration 12/1000 | Loss: 0.00031170
Iteration 13/1000 | Loss: 0.00025065
Iteration 14/1000 | Loss: 0.00022519
Iteration 15/1000 | Loss: 0.00011895
Iteration 16/1000 | Loss: 0.00005101
Iteration 17/1000 | Loss: 0.00004432
Iteration 18/1000 | Loss: 0.00004025
Iteration 19/1000 | Loss: 0.00003773
Iteration 20/1000 | Loss: 0.00003527
Iteration 21/1000 | Loss: 0.00003398
Iteration 22/1000 | Loss: 0.00028834
Iteration 23/1000 | Loss: 0.00010005
Iteration 24/1000 | Loss: 0.00013364
Iteration 25/1000 | Loss: 0.00015965
Iteration 26/1000 | Loss: 0.00011167
Iteration 27/1000 | Loss: 0.00003642
Iteration 28/1000 | Loss: 0.00003224
Iteration 29/1000 | Loss: 0.00003105
Iteration 30/1000 | Loss: 0.00003007
Iteration 31/1000 | Loss: 0.00002958
Iteration 32/1000 | Loss: 0.00002927
Iteration 33/1000 | Loss: 0.00002907
Iteration 34/1000 | Loss: 0.00002900
Iteration 35/1000 | Loss: 0.00002890
Iteration 36/1000 | Loss: 0.00002889
Iteration 37/1000 | Loss: 0.00002887
Iteration 38/1000 | Loss: 0.00002883
Iteration 39/1000 | Loss: 0.00002881
Iteration 40/1000 | Loss: 0.00002881
Iteration 41/1000 | Loss: 0.00002881
Iteration 42/1000 | Loss: 0.00002880
Iteration 43/1000 | Loss: 0.00002880
Iteration 44/1000 | Loss: 0.00002875
Iteration 45/1000 | Loss: 0.00002869
Iteration 46/1000 | Loss: 0.00002868
Iteration 47/1000 | Loss: 0.00002867
Iteration 48/1000 | Loss: 0.00002867
Iteration 49/1000 | Loss: 0.00002866
Iteration 50/1000 | Loss: 0.00002866
Iteration 51/1000 | Loss: 0.00002865
Iteration 52/1000 | Loss: 0.00002864
Iteration 53/1000 | Loss: 0.00002864
Iteration 54/1000 | Loss: 0.00002861
Iteration 55/1000 | Loss: 0.00002861
Iteration 56/1000 | Loss: 0.00002861
Iteration 57/1000 | Loss: 0.00002861
Iteration 58/1000 | Loss: 0.00002861
Iteration 59/1000 | Loss: 0.00002860
Iteration 60/1000 | Loss: 0.00002860
Iteration 61/1000 | Loss: 0.00002860
Iteration 62/1000 | Loss: 0.00002860
Iteration 63/1000 | Loss: 0.00002858
Iteration 64/1000 | Loss: 0.00002858
Iteration 65/1000 | Loss: 0.00002858
Iteration 66/1000 | Loss: 0.00002857
Iteration 67/1000 | Loss: 0.00002857
Iteration 68/1000 | Loss: 0.00002857
Iteration 69/1000 | Loss: 0.00002857
Iteration 70/1000 | Loss: 0.00002857
Iteration 71/1000 | Loss: 0.00002857
Iteration 72/1000 | Loss: 0.00002857
Iteration 73/1000 | Loss: 0.00002857
Iteration 74/1000 | Loss: 0.00002857
Iteration 75/1000 | Loss: 0.00002856
Iteration 76/1000 | Loss: 0.00002856
Iteration 77/1000 | Loss: 0.00002856
Iteration 78/1000 | Loss: 0.00002856
Iteration 79/1000 | Loss: 0.00002856
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002854
Iteration 85/1000 | Loss: 0.00002854
Iteration 86/1000 | Loss: 0.00002854
Iteration 87/1000 | Loss: 0.00002854
Iteration 88/1000 | Loss: 0.00002854
Iteration 89/1000 | Loss: 0.00002854
Iteration 90/1000 | Loss: 0.00002854
Iteration 91/1000 | Loss: 0.00002854
Iteration 92/1000 | Loss: 0.00002854
Iteration 93/1000 | Loss: 0.00002853
Iteration 94/1000 | Loss: 0.00002853
Iteration 95/1000 | Loss: 0.00002853
Iteration 96/1000 | Loss: 0.00002853
Iteration 97/1000 | Loss: 0.00002853
Iteration 98/1000 | Loss: 0.00002853
Iteration 99/1000 | Loss: 0.00002853
Iteration 100/1000 | Loss: 0.00002853
Iteration 101/1000 | Loss: 0.00002853
Iteration 102/1000 | Loss: 0.00002853
Iteration 103/1000 | Loss: 0.00002853
Iteration 104/1000 | Loss: 0.00002852
Iteration 105/1000 | Loss: 0.00002852
Iteration 106/1000 | Loss: 0.00002852
Iteration 107/1000 | Loss: 0.00002852
Iteration 108/1000 | Loss: 0.00002852
Iteration 109/1000 | Loss: 0.00002852
Iteration 110/1000 | Loss: 0.00002852
Iteration 111/1000 | Loss: 0.00002852
Iteration 112/1000 | Loss: 0.00002852
Iteration 113/1000 | Loss: 0.00002852
Iteration 114/1000 | Loss: 0.00002852
Iteration 115/1000 | Loss: 0.00002851
Iteration 116/1000 | Loss: 0.00002851
Iteration 117/1000 | Loss: 0.00002851
Iteration 118/1000 | Loss: 0.00002851
Iteration 119/1000 | Loss: 0.00002851
Iteration 120/1000 | Loss: 0.00002851
Iteration 121/1000 | Loss: 0.00002851
Iteration 122/1000 | Loss: 0.00002851
Iteration 123/1000 | Loss: 0.00002851
Iteration 124/1000 | Loss: 0.00002851
Iteration 125/1000 | Loss: 0.00002850
Iteration 126/1000 | Loss: 0.00002850
Iteration 127/1000 | Loss: 0.00002850
Iteration 128/1000 | Loss: 0.00002850
Iteration 129/1000 | Loss: 0.00002850
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002850
Iteration 132/1000 | Loss: 0.00002850
Iteration 133/1000 | Loss: 0.00002850
Iteration 134/1000 | Loss: 0.00002850
Iteration 135/1000 | Loss: 0.00002850
Iteration 136/1000 | Loss: 0.00002850
Iteration 137/1000 | Loss: 0.00002850
Iteration 138/1000 | Loss: 0.00002849
Iteration 139/1000 | Loss: 0.00002849
Iteration 140/1000 | Loss: 0.00002849
Iteration 141/1000 | Loss: 0.00002849
Iteration 142/1000 | Loss: 0.00002849
Iteration 143/1000 | Loss: 0.00002849
Iteration 144/1000 | Loss: 0.00002849
Iteration 145/1000 | Loss: 0.00002849
Iteration 146/1000 | Loss: 0.00002849
Iteration 147/1000 | Loss: 0.00002849
Iteration 148/1000 | Loss: 0.00002849
Iteration 149/1000 | Loss: 0.00002849
Iteration 150/1000 | Loss: 0.00002849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.8492417186498642e-05, 2.8492417186498642e-05, 2.8492417186498642e-05, 2.8492417186498642e-05, 2.8492417186498642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8492417186498642e-05

Optimization complete. Final v2v error: 4.468165397644043 mm

Highest mean error: 4.929709434509277 mm for frame 23

Lowest mean error: 3.967834711074829 mm for frame 40

Saving results

Total time: 105.09215331077576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377992
Iteration 2/25 | Loss: 0.00102967
Iteration 3/25 | Loss: 0.00087276
Iteration 4/25 | Loss: 0.00084275
Iteration 5/25 | Loss: 0.00083474
Iteration 6/25 | Loss: 0.00083211
Iteration 7/25 | Loss: 0.00083146
Iteration 8/25 | Loss: 0.00083146
Iteration 9/25 | Loss: 0.00083146
Iteration 10/25 | Loss: 0.00083146
Iteration 11/25 | Loss: 0.00083146
Iteration 12/25 | Loss: 0.00083146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008314591250382364, 0.0008314591250382364, 0.0008314591250382364, 0.0008314591250382364, 0.0008314591250382364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008314591250382364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61075222
Iteration 2/25 | Loss: 0.00052020
Iteration 3/25 | Loss: 0.00052020
Iteration 4/25 | Loss: 0.00052020
Iteration 5/25 | Loss: 0.00052020
Iteration 6/25 | Loss: 0.00052020
Iteration 7/25 | Loss: 0.00052020
Iteration 8/25 | Loss: 0.00052020
Iteration 9/25 | Loss: 0.00052020
Iteration 10/25 | Loss: 0.00052020
Iteration 11/25 | Loss: 0.00052020
Iteration 12/25 | Loss: 0.00052020
Iteration 13/25 | Loss: 0.00052020
Iteration 14/25 | Loss: 0.00052020
Iteration 15/25 | Loss: 0.00052020
Iteration 16/25 | Loss: 0.00052020
Iteration 17/25 | Loss: 0.00052020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005201951717026532, 0.0005201951717026532, 0.0005201951717026532, 0.0005201951717026532, 0.0005201951717026532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005201951717026532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052020
Iteration 2/1000 | Loss: 0.00002771
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001620
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001593
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001559
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001524
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001502
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00001501
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001486
Iteration 122/1000 | Loss: 0.00001486
Iteration 123/1000 | Loss: 0.00001486
Iteration 124/1000 | Loss: 0.00001486
Iteration 125/1000 | Loss: 0.00001486
Iteration 126/1000 | Loss: 0.00001486
Iteration 127/1000 | Loss: 0.00001486
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001486
Iteration 130/1000 | Loss: 0.00001486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4862912394164596e-05, 1.4862912394164596e-05, 1.4862912394164596e-05, 1.4862912394164596e-05, 1.4862912394164596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4862912394164596e-05

Optimization complete. Final v2v error: 3.2534728050231934 mm

Highest mean error: 3.9442553520202637 mm for frame 143

Lowest mean error: 2.869565725326538 mm for frame 14

Saving results

Total time: 37.359734535217285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00283114
Iteration 2/25 | Loss: 0.00119126
Iteration 3/25 | Loss: 0.00093692
Iteration 4/25 | Loss: 0.00085722
Iteration 5/25 | Loss: 0.00084203
Iteration 6/25 | Loss: 0.00083848
Iteration 7/25 | Loss: 0.00083746
Iteration 8/25 | Loss: 0.00083709
Iteration 9/25 | Loss: 0.00083695
Iteration 10/25 | Loss: 0.00083689
Iteration 11/25 | Loss: 0.00083689
Iteration 12/25 | Loss: 0.00083688
Iteration 13/25 | Loss: 0.00083688
Iteration 14/25 | Loss: 0.00083688
Iteration 15/25 | Loss: 0.00083688
Iteration 16/25 | Loss: 0.00083688
Iteration 17/25 | Loss: 0.00083688
Iteration 18/25 | Loss: 0.00083687
Iteration 19/25 | Loss: 0.00083687
Iteration 20/25 | Loss: 0.00083687
Iteration 21/25 | Loss: 0.00083687
Iteration 22/25 | Loss: 0.00083686
Iteration 23/25 | Loss: 0.00083686
Iteration 24/25 | Loss: 0.00083686
Iteration 25/25 | Loss: 0.00083686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54668093
Iteration 2/25 | Loss: 0.00082371
Iteration 3/25 | Loss: 0.00082371
Iteration 4/25 | Loss: 0.00082371
Iteration 5/25 | Loss: 0.00082371
Iteration 6/25 | Loss: 0.00082371
Iteration 7/25 | Loss: 0.00082371
Iteration 8/25 | Loss: 0.00082371
Iteration 9/25 | Loss: 0.00082371
Iteration 10/25 | Loss: 0.00082371
Iteration 11/25 | Loss: 0.00082371
Iteration 12/25 | Loss: 0.00082371
Iteration 13/25 | Loss: 0.00082371
Iteration 14/25 | Loss: 0.00082371
Iteration 15/25 | Loss: 0.00082371
Iteration 16/25 | Loss: 0.00082371
Iteration 17/25 | Loss: 0.00082371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008237102883867919, 0.0008237102883867919, 0.0008237102883867919, 0.0008237102883867919, 0.0008237102883867919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008237102883867919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082371
Iteration 2/1000 | Loss: 0.00011161
Iteration 3/1000 | Loss: 0.00007635
Iteration 4/1000 | Loss: 0.00003682
Iteration 5/1000 | Loss: 0.00002805
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002104
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001744
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001739
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001685
Iteration 21/1000 | Loss: 0.00001685
Iteration 22/1000 | Loss: 0.00001684
Iteration 23/1000 | Loss: 0.00001684
Iteration 24/1000 | Loss: 0.00001683
Iteration 25/1000 | Loss: 0.00001683
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001679
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001674
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001667
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001654
Iteration 88/1000 | Loss: 0.00001654
Iteration 89/1000 | Loss: 0.00001654
Iteration 90/1000 | Loss: 0.00001653
Iteration 91/1000 | Loss: 0.00001653
Iteration 92/1000 | Loss: 0.00001653
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001652
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001649
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001648
Iteration 114/1000 | Loss: 0.00001648
Iteration 115/1000 | Loss: 0.00001648
Iteration 116/1000 | Loss: 0.00001648
Iteration 117/1000 | Loss: 0.00001648
Iteration 118/1000 | Loss: 0.00001648
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00001647
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001647
Iteration 129/1000 | Loss: 0.00001647
Iteration 130/1000 | Loss: 0.00001647
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001647
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001647
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001646
Iteration 144/1000 | Loss: 0.00001646
Iteration 145/1000 | Loss: 0.00001646
Iteration 146/1000 | Loss: 0.00001646
Iteration 147/1000 | Loss: 0.00001646
Iteration 148/1000 | Loss: 0.00001646
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001646
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001646
Iteration 155/1000 | Loss: 0.00001646
Iteration 156/1000 | Loss: 0.00001646
Iteration 157/1000 | Loss: 0.00001646
Iteration 158/1000 | Loss: 0.00001646
Iteration 159/1000 | Loss: 0.00001646
Iteration 160/1000 | Loss: 0.00001646
Iteration 161/1000 | Loss: 0.00001645
Iteration 162/1000 | Loss: 0.00001645
Iteration 163/1000 | Loss: 0.00001645
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001645
Iteration 166/1000 | Loss: 0.00001645
Iteration 167/1000 | Loss: 0.00001645
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.6454525393783115e-05, 1.6454525393783115e-05, 1.6454525393783115e-05, 1.6454525393783115e-05, 1.6454525393783115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6454525393783115e-05

Optimization complete. Final v2v error: 3.4066946506500244 mm

Highest mean error: 3.6843788623809814 mm for frame 97

Lowest mean error: 3.2386701107025146 mm for frame 112

Saving results

Total time: 52.37959384918213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872128
Iteration 2/25 | Loss: 0.00120439
Iteration 3/25 | Loss: 0.00093364
Iteration 4/25 | Loss: 0.00089660
Iteration 5/25 | Loss: 0.00088120
Iteration 6/25 | Loss: 0.00087639
Iteration 7/25 | Loss: 0.00087453
Iteration 8/25 | Loss: 0.00087374
Iteration 9/25 | Loss: 0.00087372
Iteration 10/25 | Loss: 0.00087372
Iteration 11/25 | Loss: 0.00087372
Iteration 12/25 | Loss: 0.00087372
Iteration 13/25 | Loss: 0.00087372
Iteration 14/25 | Loss: 0.00087372
Iteration 15/25 | Loss: 0.00087372
Iteration 16/25 | Loss: 0.00087372
Iteration 17/25 | Loss: 0.00087372
Iteration 18/25 | Loss: 0.00087372
Iteration 19/25 | Loss: 0.00087372
Iteration 20/25 | Loss: 0.00087372
Iteration 21/25 | Loss: 0.00087372
Iteration 22/25 | Loss: 0.00087372
Iteration 23/25 | Loss: 0.00087372
Iteration 24/25 | Loss: 0.00087372
Iteration 25/25 | Loss: 0.00087372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54013395
Iteration 2/25 | Loss: 0.00051708
Iteration 3/25 | Loss: 0.00051708
Iteration 4/25 | Loss: 0.00051708
Iteration 5/25 | Loss: 0.00051708
Iteration 6/25 | Loss: 0.00051708
Iteration 7/25 | Loss: 0.00051708
Iteration 8/25 | Loss: 0.00051708
Iteration 9/25 | Loss: 0.00051708
Iteration 10/25 | Loss: 0.00051708
Iteration 11/25 | Loss: 0.00051708
Iteration 12/25 | Loss: 0.00051708
Iteration 13/25 | Loss: 0.00051708
Iteration 14/25 | Loss: 0.00051708
Iteration 15/25 | Loss: 0.00051708
Iteration 16/25 | Loss: 0.00051708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005170804797671735, 0.0005170804797671735, 0.0005170804797671735, 0.0005170804797671735, 0.0005170804797671735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005170804797671735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051708
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00002643
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001968
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001940
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001923
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001882
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001877
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001874
Iteration 46/1000 | Loss: 0.00001874
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001872
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001871
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001867
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001865
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001865
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001864
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001863
Iteration 115/1000 | Loss: 0.00001863
Iteration 116/1000 | Loss: 0.00001863
Iteration 117/1000 | Loss: 0.00001863
Iteration 118/1000 | Loss: 0.00001863
Iteration 119/1000 | Loss: 0.00001863
Iteration 120/1000 | Loss: 0.00001863
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001863
Iteration 127/1000 | Loss: 0.00001863
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001863
Iteration 132/1000 | Loss: 0.00001863
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001863
Iteration 135/1000 | Loss: 0.00001863
Iteration 136/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.862602039182093e-05, 1.862602039182093e-05, 1.862602039182093e-05, 1.862602039182093e-05, 1.862602039182093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.862602039182093e-05

Optimization complete. Final v2v error: 3.5616815090179443 mm

Highest mean error: 6.0632734298706055 mm for frame 70

Lowest mean error: 2.713691473007202 mm for frame 128

Saving results

Total time: 40.34051728248596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907455
Iteration 2/25 | Loss: 0.00126050
Iteration 3/25 | Loss: 0.00092617
Iteration 4/25 | Loss: 0.00090613
Iteration 5/25 | Loss: 0.00090323
Iteration 6/25 | Loss: 0.00088036
Iteration 7/25 | Loss: 0.00087464
Iteration 8/25 | Loss: 0.00086781
Iteration 9/25 | Loss: 0.00086752
Iteration 10/25 | Loss: 0.00086747
Iteration 11/25 | Loss: 0.00086747
Iteration 12/25 | Loss: 0.00086747
Iteration 13/25 | Loss: 0.00086747
Iteration 14/25 | Loss: 0.00086746
Iteration 15/25 | Loss: 0.00086746
Iteration 16/25 | Loss: 0.00086746
Iteration 17/25 | Loss: 0.00086744
Iteration 18/25 | Loss: 0.00086744
Iteration 19/25 | Loss: 0.00086744
Iteration 20/25 | Loss: 0.00086743
Iteration 21/25 | Loss: 0.00086743
Iteration 22/25 | Loss: 0.00086741
Iteration 23/25 | Loss: 0.00086741
Iteration 24/25 | Loss: 0.00086741
Iteration 25/25 | Loss: 0.00086741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46794796
Iteration 2/25 | Loss: 0.00048492
Iteration 3/25 | Loss: 0.00048491
Iteration 4/25 | Loss: 0.00048491
Iteration 5/25 | Loss: 0.00048491
Iteration 6/25 | Loss: 0.00048491
Iteration 7/25 | Loss: 0.00048491
Iteration 8/25 | Loss: 0.00048491
Iteration 9/25 | Loss: 0.00048491
Iteration 10/25 | Loss: 0.00048491
Iteration 11/25 | Loss: 0.00048491
Iteration 12/25 | Loss: 0.00048491
Iteration 13/25 | Loss: 0.00048491
Iteration 14/25 | Loss: 0.00048491
Iteration 15/25 | Loss: 0.00048491
Iteration 16/25 | Loss: 0.00048491
Iteration 17/25 | Loss: 0.00048491
Iteration 18/25 | Loss: 0.00048491
Iteration 19/25 | Loss: 0.00048491
Iteration 20/25 | Loss: 0.00048491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004849083488807082, 0.0004849083488807082, 0.0004849083488807082, 0.0004849083488807082, 0.0004849083488807082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004849083488807082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048491
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00001989
Iteration 4/1000 | Loss: 0.00001830
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001524
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001503
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001500
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001485
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001479
Iteration 121/1000 | Loss: 0.00001479
Iteration 122/1000 | Loss: 0.00001479
Iteration 123/1000 | Loss: 0.00001479
Iteration 124/1000 | Loss: 0.00001479
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001478
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001477
Iteration 133/1000 | Loss: 0.00001477
Iteration 134/1000 | Loss: 0.00001477
Iteration 135/1000 | Loss: 0.00001477
Iteration 136/1000 | Loss: 0.00001476
Iteration 137/1000 | Loss: 0.00001476
Iteration 138/1000 | Loss: 0.00001476
Iteration 139/1000 | Loss: 0.00001476
Iteration 140/1000 | Loss: 0.00001476
Iteration 141/1000 | Loss: 0.00001476
Iteration 142/1000 | Loss: 0.00001476
Iteration 143/1000 | Loss: 0.00001476
Iteration 144/1000 | Loss: 0.00001476
Iteration 145/1000 | Loss: 0.00001476
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001475
Iteration 148/1000 | Loss: 0.00001475
Iteration 149/1000 | Loss: 0.00001475
Iteration 150/1000 | Loss: 0.00001475
Iteration 151/1000 | Loss: 0.00001475
Iteration 152/1000 | Loss: 0.00001475
Iteration 153/1000 | Loss: 0.00001475
Iteration 154/1000 | Loss: 0.00001475
Iteration 155/1000 | Loss: 0.00001475
Iteration 156/1000 | Loss: 0.00001475
Iteration 157/1000 | Loss: 0.00001474
Iteration 158/1000 | Loss: 0.00001474
Iteration 159/1000 | Loss: 0.00001474
Iteration 160/1000 | Loss: 0.00001474
Iteration 161/1000 | Loss: 0.00001474
Iteration 162/1000 | Loss: 0.00001474
Iteration 163/1000 | Loss: 0.00001474
Iteration 164/1000 | Loss: 0.00001474
Iteration 165/1000 | Loss: 0.00001474
Iteration 166/1000 | Loss: 0.00001474
Iteration 167/1000 | Loss: 0.00001474
Iteration 168/1000 | Loss: 0.00001474
Iteration 169/1000 | Loss: 0.00001474
Iteration 170/1000 | Loss: 0.00001474
Iteration 171/1000 | Loss: 0.00001474
Iteration 172/1000 | Loss: 0.00001473
Iteration 173/1000 | Loss: 0.00001473
Iteration 174/1000 | Loss: 0.00001473
Iteration 175/1000 | Loss: 0.00001473
Iteration 176/1000 | Loss: 0.00001473
Iteration 177/1000 | Loss: 0.00001473
Iteration 178/1000 | Loss: 0.00001473
Iteration 179/1000 | Loss: 0.00001473
Iteration 180/1000 | Loss: 0.00001473
Iteration 181/1000 | Loss: 0.00001473
Iteration 182/1000 | Loss: 0.00001473
Iteration 183/1000 | Loss: 0.00001473
Iteration 184/1000 | Loss: 0.00001473
Iteration 185/1000 | Loss: 0.00001473
Iteration 186/1000 | Loss: 0.00001473
Iteration 187/1000 | Loss: 0.00001472
Iteration 188/1000 | Loss: 0.00001472
Iteration 189/1000 | Loss: 0.00001472
Iteration 190/1000 | Loss: 0.00001472
Iteration 191/1000 | Loss: 0.00001472
Iteration 192/1000 | Loss: 0.00001472
Iteration 193/1000 | Loss: 0.00001472
Iteration 194/1000 | Loss: 0.00001472
Iteration 195/1000 | Loss: 0.00001472
Iteration 196/1000 | Loss: 0.00001472
Iteration 197/1000 | Loss: 0.00001472
Iteration 198/1000 | Loss: 0.00001472
Iteration 199/1000 | Loss: 0.00001472
Iteration 200/1000 | Loss: 0.00001472
Iteration 201/1000 | Loss: 0.00001472
Iteration 202/1000 | Loss: 0.00001472
Iteration 203/1000 | Loss: 0.00001472
Iteration 204/1000 | Loss: 0.00001472
Iteration 205/1000 | Loss: 0.00001472
Iteration 206/1000 | Loss: 0.00001472
Iteration 207/1000 | Loss: 0.00001472
Iteration 208/1000 | Loss: 0.00001472
Iteration 209/1000 | Loss: 0.00001472
Iteration 210/1000 | Loss: 0.00001472
Iteration 211/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.4717455087520648e-05, 1.4717455087520648e-05, 1.4717455087520648e-05, 1.4717455087520648e-05, 1.4717455087520648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4717455087520648e-05

Optimization complete. Final v2v error: 3.2176527976989746 mm

Highest mean error: 3.9403610229492188 mm for frame 113

Lowest mean error: 2.7605466842651367 mm for frame 8

Saving results

Total time: 56.81071877479553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044464
Iteration 2/25 | Loss: 0.00181721
Iteration 3/25 | Loss: 0.00122151
Iteration 4/25 | Loss: 0.00119694
Iteration 5/25 | Loss: 0.00111871
Iteration 6/25 | Loss: 0.00104642
Iteration 7/25 | Loss: 0.00104587
Iteration 8/25 | Loss: 0.00101605
Iteration 9/25 | Loss: 0.00101045
Iteration 10/25 | Loss: 0.00100712
Iteration 11/25 | Loss: 0.00100111
Iteration 12/25 | Loss: 0.00100012
Iteration 13/25 | Loss: 0.00100495
Iteration 14/25 | Loss: 0.00100625
Iteration 15/25 | Loss: 0.00100551
Iteration 16/25 | Loss: 0.00100769
Iteration 17/25 | Loss: 0.00100828
Iteration 18/25 | Loss: 0.00100843
Iteration 19/25 | Loss: 0.00100752
Iteration 20/25 | Loss: 0.00100631
Iteration 21/25 | Loss: 0.00101056
Iteration 22/25 | Loss: 0.00100588
Iteration 23/25 | Loss: 0.00100804
Iteration 24/25 | Loss: 0.00100401
Iteration 25/25 | Loss: 0.00100419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.94702911
Iteration 2/25 | Loss: 0.00146701
Iteration 3/25 | Loss: 0.00146545
Iteration 4/25 | Loss: 0.00146545
Iteration 5/25 | Loss: 0.00146545
Iteration 6/25 | Loss: 0.00146545
Iteration 7/25 | Loss: 0.00146545
Iteration 8/25 | Loss: 0.00146545
Iteration 9/25 | Loss: 0.00146545
Iteration 10/25 | Loss: 0.00146545
Iteration 11/25 | Loss: 0.00146545
Iteration 12/25 | Loss: 0.00146545
Iteration 13/25 | Loss: 0.00146545
Iteration 14/25 | Loss: 0.00146545
Iteration 15/25 | Loss: 0.00146545
Iteration 16/25 | Loss: 0.00146545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001465448527596891, 0.001465448527596891, 0.001465448527596891, 0.001465448527596891, 0.001465448527596891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001465448527596891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146545
Iteration 2/1000 | Loss: 0.00097682
Iteration 3/1000 | Loss: 0.00155581
Iteration 4/1000 | Loss: 0.00878489
Iteration 5/1000 | Loss: 0.00122445
Iteration 6/1000 | Loss: 0.00838505
Iteration 7/1000 | Loss: 0.00666816
Iteration 8/1000 | Loss: 0.00085619
Iteration 9/1000 | Loss: 0.00113680
Iteration 10/1000 | Loss: 0.00120913
Iteration 11/1000 | Loss: 0.00675470
Iteration 12/1000 | Loss: 0.00625985
Iteration 13/1000 | Loss: 0.00397940
Iteration 14/1000 | Loss: 0.00117668
Iteration 15/1000 | Loss: 0.00266073
Iteration 16/1000 | Loss: 0.00122569
Iteration 17/1000 | Loss: 0.00078667
Iteration 18/1000 | Loss: 0.00100353
Iteration 19/1000 | Loss: 0.00097887
Iteration 20/1000 | Loss: 0.00769609
Iteration 21/1000 | Loss: 0.00129966
Iteration 22/1000 | Loss: 0.00125013
Iteration 23/1000 | Loss: 0.00067614
Iteration 24/1000 | Loss: 0.00030685
Iteration 25/1000 | Loss: 0.00009947
Iteration 26/1000 | Loss: 0.00008902
Iteration 27/1000 | Loss: 0.00007954
Iteration 28/1000 | Loss: 0.00415452
Iteration 29/1000 | Loss: 0.00160031
Iteration 30/1000 | Loss: 0.00024106
Iteration 31/1000 | Loss: 0.00007533
Iteration 32/1000 | Loss: 0.00005959
Iteration 33/1000 | Loss: 0.00337080
Iteration 34/1000 | Loss: 0.00007858
Iteration 35/1000 | Loss: 0.00101772
Iteration 36/1000 | Loss: 0.00005774
Iteration 37/1000 | Loss: 0.00013191
Iteration 38/1000 | Loss: 0.00005339
Iteration 39/1000 | Loss: 0.00004655
Iteration 40/1000 | Loss: 0.00004007
Iteration 41/1000 | Loss: 0.00003573
Iteration 42/1000 | Loss: 0.00003398
Iteration 43/1000 | Loss: 0.00003278
Iteration 44/1000 | Loss: 0.00003192
Iteration 45/1000 | Loss: 0.00003145
Iteration 46/1000 | Loss: 0.00003097
Iteration 47/1000 | Loss: 0.00003056
Iteration 48/1000 | Loss: 0.00003023
Iteration 49/1000 | Loss: 0.00003003
Iteration 50/1000 | Loss: 0.00002981
Iteration 51/1000 | Loss: 0.00002973
Iteration 52/1000 | Loss: 0.00002968
Iteration 53/1000 | Loss: 0.00002957
Iteration 54/1000 | Loss: 0.00002950
Iteration 55/1000 | Loss: 0.00002950
Iteration 56/1000 | Loss: 0.00002949
Iteration 57/1000 | Loss: 0.00002948
Iteration 58/1000 | Loss: 0.00002948
Iteration 59/1000 | Loss: 0.00002948
Iteration 60/1000 | Loss: 0.00002947
Iteration 61/1000 | Loss: 0.00002947
Iteration 62/1000 | Loss: 0.00002946
Iteration 63/1000 | Loss: 0.00002946
Iteration 64/1000 | Loss: 0.00002945
Iteration 65/1000 | Loss: 0.00002945
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002944
Iteration 68/1000 | Loss: 0.00002943
Iteration 69/1000 | Loss: 0.00002940
Iteration 70/1000 | Loss: 0.00002939
Iteration 71/1000 | Loss: 0.00002939
Iteration 72/1000 | Loss: 0.00002939
Iteration 73/1000 | Loss: 0.00002938
Iteration 74/1000 | Loss: 0.00002938
Iteration 75/1000 | Loss: 0.00002938
Iteration 76/1000 | Loss: 0.00002938
Iteration 77/1000 | Loss: 0.00002938
Iteration 78/1000 | Loss: 0.00002938
Iteration 79/1000 | Loss: 0.00002938
Iteration 80/1000 | Loss: 0.00002938
Iteration 81/1000 | Loss: 0.00002938
Iteration 82/1000 | Loss: 0.00002938
Iteration 83/1000 | Loss: 0.00002938
Iteration 84/1000 | Loss: 0.00002937
Iteration 85/1000 | Loss: 0.00002936
Iteration 86/1000 | Loss: 0.00002936
Iteration 87/1000 | Loss: 0.00002936
Iteration 88/1000 | Loss: 0.00002936
Iteration 89/1000 | Loss: 0.00002935
Iteration 90/1000 | Loss: 0.00002935
Iteration 91/1000 | Loss: 0.00002935
Iteration 92/1000 | Loss: 0.00002935
Iteration 93/1000 | Loss: 0.00002935
Iteration 94/1000 | Loss: 0.00002935
Iteration 95/1000 | Loss: 0.00002935
Iteration 96/1000 | Loss: 0.00002935
Iteration 97/1000 | Loss: 0.00002934
Iteration 98/1000 | Loss: 0.00002934
Iteration 99/1000 | Loss: 0.00002934
Iteration 100/1000 | Loss: 0.00002934
Iteration 101/1000 | Loss: 0.00002934
Iteration 102/1000 | Loss: 0.00002934
Iteration 103/1000 | Loss: 0.00002934
Iteration 104/1000 | Loss: 0.00002934
Iteration 105/1000 | Loss: 0.00002934
Iteration 106/1000 | Loss: 0.00002933
Iteration 107/1000 | Loss: 0.00002933
Iteration 108/1000 | Loss: 0.00002933
Iteration 109/1000 | Loss: 0.00002933
Iteration 110/1000 | Loss: 0.00002933
Iteration 111/1000 | Loss: 0.00002933
Iteration 112/1000 | Loss: 0.00002933
Iteration 113/1000 | Loss: 0.00002933
Iteration 114/1000 | Loss: 0.00002932
Iteration 115/1000 | Loss: 0.00002932
Iteration 116/1000 | Loss: 0.00002932
Iteration 117/1000 | Loss: 0.00002932
Iteration 118/1000 | Loss: 0.00002932
Iteration 119/1000 | Loss: 0.00002932
Iteration 120/1000 | Loss: 0.00002932
Iteration 121/1000 | Loss: 0.00002932
Iteration 122/1000 | Loss: 0.00002932
Iteration 123/1000 | Loss: 0.00002932
Iteration 124/1000 | Loss: 0.00002931
Iteration 125/1000 | Loss: 0.00002931
Iteration 126/1000 | Loss: 0.00002931
Iteration 127/1000 | Loss: 0.00002931
Iteration 128/1000 | Loss: 0.00002931
Iteration 129/1000 | Loss: 0.00002931
Iteration 130/1000 | Loss: 0.00002931
Iteration 131/1000 | Loss: 0.00002930
Iteration 132/1000 | Loss: 0.00002930
Iteration 133/1000 | Loss: 0.00002930
Iteration 134/1000 | Loss: 0.00002930
Iteration 135/1000 | Loss: 0.00002930
Iteration 136/1000 | Loss: 0.00002930
Iteration 137/1000 | Loss: 0.00002930
Iteration 138/1000 | Loss: 0.00002930
Iteration 139/1000 | Loss: 0.00002930
Iteration 140/1000 | Loss: 0.00002930
Iteration 141/1000 | Loss: 0.00002930
Iteration 142/1000 | Loss: 0.00002930
Iteration 143/1000 | Loss: 0.00002930
Iteration 144/1000 | Loss: 0.00002930
Iteration 145/1000 | Loss: 0.00002930
Iteration 146/1000 | Loss: 0.00002930
Iteration 147/1000 | Loss: 0.00002930
Iteration 148/1000 | Loss: 0.00002930
Iteration 149/1000 | Loss: 0.00002930
Iteration 150/1000 | Loss: 0.00002930
Iteration 151/1000 | Loss: 0.00002930
Iteration 152/1000 | Loss: 0.00002930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.9303420888027176e-05, 2.9303420888027176e-05, 2.9303420888027176e-05, 2.9303420888027176e-05, 2.9303420888027176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9303420888027176e-05

Optimization complete. Final v2v error: 4.453679084777832 mm

Highest mean error: 6.475290775299072 mm for frame 124

Lowest mean error: 3.244532823562622 mm for frame 98

Saving results

Total time: 129.0078239440918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360470
Iteration 2/25 | Loss: 0.00103086
Iteration 3/25 | Loss: 0.00089872
Iteration 4/25 | Loss: 0.00086572
Iteration 5/25 | Loss: 0.00085749
Iteration 6/25 | Loss: 0.00085511
Iteration 7/25 | Loss: 0.00085418
Iteration 8/25 | Loss: 0.00085410
Iteration 9/25 | Loss: 0.00085410
Iteration 10/25 | Loss: 0.00085410
Iteration 11/25 | Loss: 0.00085410
Iteration 12/25 | Loss: 0.00085410
Iteration 13/25 | Loss: 0.00085410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008540997514501214, 0.0008540997514501214, 0.0008540997514501214, 0.0008540997514501214, 0.0008540997514501214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008540997514501214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55791986
Iteration 2/25 | Loss: 0.00065565
Iteration 3/25 | Loss: 0.00065565
Iteration 4/25 | Loss: 0.00065565
Iteration 5/25 | Loss: 0.00065565
Iteration 6/25 | Loss: 0.00065565
Iteration 7/25 | Loss: 0.00065565
Iteration 8/25 | Loss: 0.00065565
Iteration 9/25 | Loss: 0.00065565
Iteration 10/25 | Loss: 0.00065565
Iteration 11/25 | Loss: 0.00065565
Iteration 12/25 | Loss: 0.00065565
Iteration 13/25 | Loss: 0.00065565
Iteration 14/25 | Loss: 0.00065565
Iteration 15/25 | Loss: 0.00065565
Iteration 16/25 | Loss: 0.00065565
Iteration 17/25 | Loss: 0.00065565
Iteration 18/25 | Loss: 0.00065565
Iteration 19/25 | Loss: 0.00065565
Iteration 20/25 | Loss: 0.00065565
Iteration 21/25 | Loss: 0.00065565
Iteration 22/25 | Loss: 0.00065565
Iteration 23/25 | Loss: 0.00065565
Iteration 24/25 | Loss: 0.00065565
Iteration 25/25 | Loss: 0.00065565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065565
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002011
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001739
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001719
Iteration 43/1000 | Loss: 0.00001719
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001707
Iteration 132/1000 | Loss: 0.00001707
Iteration 133/1000 | Loss: 0.00001707
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001706
Iteration 148/1000 | Loss: 0.00001706
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.7062604456441477e-05, 1.7062604456441477e-05, 1.7062604456441477e-05, 1.7062604456441477e-05, 1.7062604456441477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7062604456441477e-05

Optimization complete. Final v2v error: 3.4331109523773193 mm

Highest mean error: 4.795656204223633 mm for frame 151

Lowest mean error: 2.606332302093506 mm for frame 166

Saving results

Total time: 41.35841679573059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041038
Iteration 2/25 | Loss: 0.00249193
Iteration 3/25 | Loss: 0.00170941
Iteration 4/25 | Loss: 0.00144206
Iteration 5/25 | Loss: 0.00122792
Iteration 6/25 | Loss: 0.00115627
Iteration 7/25 | Loss: 0.00111836
Iteration 8/25 | Loss: 0.00109251
Iteration 9/25 | Loss: 0.00107984
Iteration 10/25 | Loss: 0.00107274
Iteration 11/25 | Loss: 0.00105858
Iteration 12/25 | Loss: 0.00104237
Iteration 13/25 | Loss: 0.00104105
Iteration 14/25 | Loss: 0.00102257
Iteration 15/25 | Loss: 0.00102778
Iteration 16/25 | Loss: 0.00102051
Iteration 17/25 | Loss: 0.00100571
Iteration 18/25 | Loss: 0.00100771
Iteration 19/25 | Loss: 0.00101314
Iteration 20/25 | Loss: 0.00100251
Iteration 21/25 | Loss: 0.00099881
Iteration 22/25 | Loss: 0.00100357
Iteration 23/25 | Loss: 0.00100277
Iteration 24/25 | Loss: 0.00099248
Iteration 25/25 | Loss: 0.00098932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55216074
Iteration 2/25 | Loss: 0.00114671
Iteration 3/25 | Loss: 0.00111877
Iteration 4/25 | Loss: 0.00110097
Iteration 5/25 | Loss: 0.00110097
Iteration 6/25 | Loss: 0.00110097
Iteration 7/25 | Loss: 0.00110097
Iteration 8/25 | Loss: 0.00110097
Iteration 9/25 | Loss: 0.00110097
Iteration 10/25 | Loss: 0.00110097
Iteration 11/25 | Loss: 0.00110097
Iteration 12/25 | Loss: 0.00110097
Iteration 13/25 | Loss: 0.00110097
Iteration 14/25 | Loss: 0.00110097
Iteration 15/25 | Loss: 0.00110097
Iteration 16/25 | Loss: 0.00110097
Iteration 17/25 | Loss: 0.00110097
Iteration 18/25 | Loss: 0.00110097
Iteration 19/25 | Loss: 0.00110097
Iteration 20/25 | Loss: 0.00110097
Iteration 21/25 | Loss: 0.00110097
Iteration 22/25 | Loss: 0.00110097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00110096693970263, 0.00110096693970263, 0.00110096693970263, 0.00110096693970263, 0.00110096693970263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00110096693970263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110097
Iteration 2/1000 | Loss: 0.00017008
Iteration 3/1000 | Loss: 0.00015382
Iteration 4/1000 | Loss: 0.00022629
Iteration 5/1000 | Loss: 0.00012978
Iteration 6/1000 | Loss: 0.00013752
Iteration 7/1000 | Loss: 0.00019117
Iteration 8/1000 | Loss: 0.00012770
Iteration 9/1000 | Loss: 0.00014751
Iteration 10/1000 | Loss: 0.00009649
Iteration 11/1000 | Loss: 0.00010904
Iteration 12/1000 | Loss: 0.00011974
Iteration 13/1000 | Loss: 0.00011860
Iteration 14/1000 | Loss: 0.00011339
Iteration 15/1000 | Loss: 0.00020683
Iteration 16/1000 | Loss: 0.00012262
Iteration 17/1000 | Loss: 0.00011711
Iteration 18/1000 | Loss: 0.00011249
Iteration 19/1000 | Loss: 0.00007176
Iteration 20/1000 | Loss: 0.00008793
Iteration 21/1000 | Loss: 0.00007484
Iteration 22/1000 | Loss: 0.00009694
Iteration 23/1000 | Loss: 0.00008614
Iteration 24/1000 | Loss: 0.00009736
Iteration 25/1000 | Loss: 0.00009557
Iteration 26/1000 | Loss: 0.00010415
Iteration 27/1000 | Loss: 0.00013359
Iteration 28/1000 | Loss: 0.00011851
Iteration 29/1000 | Loss: 0.00013428
Iteration 30/1000 | Loss: 0.00012032
Iteration 31/1000 | Loss: 0.00010146
Iteration 32/1000 | Loss: 0.00013531
Iteration 33/1000 | Loss: 0.00011398
Iteration 34/1000 | Loss: 0.00017031
Iteration 35/1000 | Loss: 0.00008349
Iteration 36/1000 | Loss: 0.00011312
Iteration 37/1000 | Loss: 0.00009169
Iteration 38/1000 | Loss: 0.00008602
Iteration 39/1000 | Loss: 0.00014629
Iteration 40/1000 | Loss: 0.00011534
Iteration 41/1000 | Loss: 0.00010417
Iteration 42/1000 | Loss: 0.00015897
Iteration 43/1000 | Loss: 0.00010129
Iteration 44/1000 | Loss: 0.00008917
Iteration 45/1000 | Loss: 0.00009199
Iteration 46/1000 | Loss: 0.00008862
Iteration 47/1000 | Loss: 0.00008992
Iteration 48/1000 | Loss: 0.00008885
Iteration 49/1000 | Loss: 0.00012251
Iteration 50/1000 | Loss: 0.00013206
Iteration 51/1000 | Loss: 0.00006741
Iteration 52/1000 | Loss: 0.00008131
Iteration 53/1000 | Loss: 0.00010911
Iteration 54/1000 | Loss: 0.00008664
Iteration 55/1000 | Loss: 0.00009735
Iteration 56/1000 | Loss: 0.00009795
Iteration 57/1000 | Loss: 0.00008655
Iteration 58/1000 | Loss: 0.00008621
Iteration 59/1000 | Loss: 0.00007389
Iteration 60/1000 | Loss: 0.00007988
Iteration 61/1000 | Loss: 0.00008411
Iteration 62/1000 | Loss: 0.00009892
Iteration 63/1000 | Loss: 0.00008335
Iteration 64/1000 | Loss: 0.00010186
Iteration 65/1000 | Loss: 0.00009414
Iteration 66/1000 | Loss: 0.00008808
Iteration 67/1000 | Loss: 0.00009766
Iteration 68/1000 | Loss: 0.00008557
Iteration 69/1000 | Loss: 0.00007937
Iteration 70/1000 | Loss: 0.00008728
Iteration 71/1000 | Loss: 0.00006636
Iteration 72/1000 | Loss: 0.00007186
Iteration 73/1000 | Loss: 0.00007695
Iteration 74/1000 | Loss: 0.00007912
Iteration 75/1000 | Loss: 0.00006763
Iteration 76/1000 | Loss: 0.00007720
Iteration 77/1000 | Loss: 0.00007266
Iteration 78/1000 | Loss: 0.00008479
Iteration 79/1000 | Loss: 0.00008399
Iteration 80/1000 | Loss: 0.00007852
Iteration 81/1000 | Loss: 0.00010297
Iteration 82/1000 | Loss: 0.00008806
Iteration 83/1000 | Loss: 0.00009630
Iteration 84/1000 | Loss: 0.00008479
Iteration 85/1000 | Loss: 0.00445244
Iteration 86/1000 | Loss: 0.00108802
Iteration 87/1000 | Loss: 0.00043166
Iteration 88/1000 | Loss: 0.00108973
Iteration 89/1000 | Loss: 0.00023036
Iteration 90/1000 | Loss: 0.00029526
Iteration 91/1000 | Loss: 0.00010822
Iteration 92/1000 | Loss: 0.00013291
Iteration 93/1000 | Loss: 0.00027255
Iteration 94/1000 | Loss: 0.00004725
Iteration 95/1000 | Loss: 0.00010495
Iteration 96/1000 | Loss: 0.00004298
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00008260
Iteration 99/1000 | Loss: 0.00010481
Iteration 100/1000 | Loss: 0.00059938
Iteration 101/1000 | Loss: 0.00003513
Iteration 102/1000 | Loss: 0.00002965
Iteration 103/1000 | Loss: 0.00003571
Iteration 104/1000 | Loss: 0.00002020
Iteration 105/1000 | Loss: 0.00005407
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001832
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001778
Iteration 114/1000 | Loss: 0.00001776
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001765
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001765
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001764
Iteration 136/1000 | Loss: 0.00001764
Iteration 137/1000 | Loss: 0.00001764
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001762
Iteration 142/1000 | Loss: 0.00001762
Iteration 143/1000 | Loss: 0.00001762
Iteration 144/1000 | Loss: 0.00001761
Iteration 145/1000 | Loss: 0.00001761
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001757
Iteration 154/1000 | Loss: 0.00001756
Iteration 155/1000 | Loss: 0.00001755
Iteration 156/1000 | Loss: 0.00001755
Iteration 157/1000 | Loss: 0.00001755
Iteration 158/1000 | Loss: 0.00001755
Iteration 159/1000 | Loss: 0.00001755
Iteration 160/1000 | Loss: 0.00001754
Iteration 161/1000 | Loss: 0.00001754
Iteration 162/1000 | Loss: 0.00001753
Iteration 163/1000 | Loss: 0.00001753
Iteration 164/1000 | Loss: 0.00001752
Iteration 165/1000 | Loss: 0.00001751
Iteration 166/1000 | Loss: 0.00001750
Iteration 167/1000 | Loss: 0.00001749
Iteration 168/1000 | Loss: 0.00001749
Iteration 169/1000 | Loss: 0.00001748
Iteration 170/1000 | Loss: 0.00001747
Iteration 171/1000 | Loss: 0.00001747
Iteration 172/1000 | Loss: 0.00001747
Iteration 173/1000 | Loss: 0.00001747
Iteration 174/1000 | Loss: 0.00001747
Iteration 175/1000 | Loss: 0.00001747
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001746
Iteration 181/1000 | Loss: 0.00001746
Iteration 182/1000 | Loss: 0.00001746
Iteration 183/1000 | Loss: 0.00001746
Iteration 184/1000 | Loss: 0.00001745
Iteration 185/1000 | Loss: 0.00001745
Iteration 186/1000 | Loss: 0.00001745
Iteration 187/1000 | Loss: 0.00001745
Iteration 188/1000 | Loss: 0.00001745
Iteration 189/1000 | Loss: 0.00001745
Iteration 190/1000 | Loss: 0.00001745
Iteration 191/1000 | Loss: 0.00001745
Iteration 192/1000 | Loss: 0.00001745
Iteration 193/1000 | Loss: 0.00001745
Iteration 194/1000 | Loss: 0.00001745
Iteration 195/1000 | Loss: 0.00001745
Iteration 196/1000 | Loss: 0.00001744
Iteration 197/1000 | Loss: 0.00001744
Iteration 198/1000 | Loss: 0.00001744
Iteration 199/1000 | Loss: 0.00001744
Iteration 200/1000 | Loss: 0.00001744
Iteration 201/1000 | Loss: 0.00001744
Iteration 202/1000 | Loss: 0.00001744
Iteration 203/1000 | Loss: 0.00001744
Iteration 204/1000 | Loss: 0.00001744
Iteration 205/1000 | Loss: 0.00001744
Iteration 206/1000 | Loss: 0.00001744
Iteration 207/1000 | Loss: 0.00001743
Iteration 208/1000 | Loss: 0.00001743
Iteration 209/1000 | Loss: 0.00001743
Iteration 210/1000 | Loss: 0.00001742
Iteration 211/1000 | Loss: 0.00001742
Iteration 212/1000 | Loss: 0.00001742
Iteration 213/1000 | Loss: 0.00001742
Iteration 214/1000 | Loss: 0.00001741
Iteration 215/1000 | Loss: 0.00001741
Iteration 216/1000 | Loss: 0.00001741
Iteration 217/1000 | Loss: 0.00001741
Iteration 218/1000 | Loss: 0.00001741
Iteration 219/1000 | Loss: 0.00001741
Iteration 220/1000 | Loss: 0.00001741
Iteration 221/1000 | Loss: 0.00001741
Iteration 222/1000 | Loss: 0.00001741
Iteration 223/1000 | Loss: 0.00001740
Iteration 224/1000 | Loss: 0.00001740
Iteration 225/1000 | Loss: 0.00001740
Iteration 226/1000 | Loss: 0.00001740
Iteration 227/1000 | Loss: 0.00001740
Iteration 228/1000 | Loss: 0.00001740
Iteration 229/1000 | Loss: 0.00001740
Iteration 230/1000 | Loss: 0.00001739
Iteration 231/1000 | Loss: 0.00001739
Iteration 232/1000 | Loss: 0.00001739
Iteration 233/1000 | Loss: 0.00001739
Iteration 234/1000 | Loss: 0.00001739
Iteration 235/1000 | Loss: 0.00001739
Iteration 236/1000 | Loss: 0.00001739
Iteration 237/1000 | Loss: 0.00001739
Iteration 238/1000 | Loss: 0.00001739
Iteration 239/1000 | Loss: 0.00001739
Iteration 240/1000 | Loss: 0.00001739
Iteration 241/1000 | Loss: 0.00001739
Iteration 242/1000 | Loss: 0.00001739
Iteration 243/1000 | Loss: 0.00001739
Iteration 244/1000 | Loss: 0.00001739
Iteration 245/1000 | Loss: 0.00001739
Iteration 246/1000 | Loss: 0.00001739
Iteration 247/1000 | Loss: 0.00001739
Iteration 248/1000 | Loss: 0.00001739
Iteration 249/1000 | Loss: 0.00001739
Iteration 250/1000 | Loss: 0.00001739
Iteration 251/1000 | Loss: 0.00001739
Iteration 252/1000 | Loss: 0.00001739
Iteration 253/1000 | Loss: 0.00001739
Iteration 254/1000 | Loss: 0.00001739
Iteration 255/1000 | Loss: 0.00001739
Iteration 256/1000 | Loss: 0.00001739
Iteration 257/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.7393800590070896e-05, 1.7393800590070896e-05, 1.7393800590070896e-05, 1.7393800590070896e-05, 1.7393800590070896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7393800590070896e-05

Optimization complete. Final v2v error: 3.519225835800171 mm

Highest mean error: 3.8271262645721436 mm for frame 86

Lowest mean error: 3.1760571002960205 mm for frame 101

Saving results

Total time: 205.1615710258484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795453
Iteration 2/25 | Loss: 0.00139071
Iteration 3/25 | Loss: 0.00105172
Iteration 4/25 | Loss: 0.00096913
Iteration 5/25 | Loss: 0.00094402
Iteration 6/25 | Loss: 0.00093856
Iteration 7/25 | Loss: 0.00093281
Iteration 8/25 | Loss: 0.00093136
Iteration 9/25 | Loss: 0.00093113
Iteration 10/25 | Loss: 0.00093112
Iteration 11/25 | Loss: 0.00093112
Iteration 12/25 | Loss: 0.00093112
Iteration 13/25 | Loss: 0.00093112
Iteration 14/25 | Loss: 0.00093112
Iteration 15/25 | Loss: 0.00093111
Iteration 16/25 | Loss: 0.00093111
Iteration 17/25 | Loss: 0.00093111
Iteration 18/25 | Loss: 0.00093109
Iteration 19/25 | Loss: 0.00093109
Iteration 20/25 | Loss: 0.00093109
Iteration 21/25 | Loss: 0.00093109
Iteration 22/25 | Loss: 0.00093108
Iteration 23/25 | Loss: 0.00093108
Iteration 24/25 | Loss: 0.00093108
Iteration 25/25 | Loss: 0.00093108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52634799
Iteration 2/25 | Loss: 0.00068007
Iteration 3/25 | Loss: 0.00068002
Iteration 4/25 | Loss: 0.00068002
Iteration 5/25 | Loss: 0.00068002
Iteration 6/25 | Loss: 0.00068002
Iteration 7/25 | Loss: 0.00068002
Iteration 8/25 | Loss: 0.00068002
Iteration 9/25 | Loss: 0.00068002
Iteration 10/25 | Loss: 0.00068002
Iteration 11/25 | Loss: 0.00068002
Iteration 12/25 | Loss: 0.00068002
Iteration 13/25 | Loss: 0.00068002
Iteration 14/25 | Loss: 0.00068002
Iteration 15/25 | Loss: 0.00068002
Iteration 16/25 | Loss: 0.00068002
Iteration 17/25 | Loss: 0.00068002
Iteration 18/25 | Loss: 0.00068002
Iteration 19/25 | Loss: 0.00068002
Iteration 20/25 | Loss: 0.00068002
Iteration 21/25 | Loss: 0.00068002
Iteration 22/25 | Loss: 0.00068002
Iteration 23/25 | Loss: 0.00068002
Iteration 24/25 | Loss: 0.00068002
Iteration 25/25 | Loss: 0.00068002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068002
Iteration 2/1000 | Loss: 0.00006883
Iteration 3/1000 | Loss: 0.00005069
Iteration 4/1000 | Loss: 0.00004502
Iteration 5/1000 | Loss: 0.00004177
Iteration 6/1000 | Loss: 0.00003924
Iteration 7/1000 | Loss: 0.00003760
Iteration 8/1000 | Loss: 0.00003651
Iteration 9/1000 | Loss: 0.00003551
Iteration 10/1000 | Loss: 0.00003485
Iteration 11/1000 | Loss: 0.00003435
Iteration 12/1000 | Loss: 0.00003392
Iteration 13/1000 | Loss: 0.00003360
Iteration 14/1000 | Loss: 0.00003327
Iteration 15/1000 | Loss: 0.00003300
Iteration 16/1000 | Loss: 0.00003279
Iteration 17/1000 | Loss: 0.00003263
Iteration 18/1000 | Loss: 0.00003258
Iteration 19/1000 | Loss: 0.00003250
Iteration 20/1000 | Loss: 0.00003240
Iteration 21/1000 | Loss: 0.00003240
Iteration 22/1000 | Loss: 0.00003239
Iteration 23/1000 | Loss: 0.00003239
Iteration 24/1000 | Loss: 0.00003235
Iteration 25/1000 | Loss: 0.00003235
Iteration 26/1000 | Loss: 0.00003232
Iteration 27/1000 | Loss: 0.00003231
Iteration 28/1000 | Loss: 0.00003231
Iteration 29/1000 | Loss: 0.00003230
Iteration 30/1000 | Loss: 0.00003230
Iteration 31/1000 | Loss: 0.00003229
Iteration 32/1000 | Loss: 0.00003229
Iteration 33/1000 | Loss: 0.00003229
Iteration 34/1000 | Loss: 0.00003228
Iteration 35/1000 | Loss: 0.00003228
Iteration 36/1000 | Loss: 0.00003228
Iteration 37/1000 | Loss: 0.00003228
Iteration 38/1000 | Loss: 0.00003228
Iteration 39/1000 | Loss: 0.00003228
Iteration 40/1000 | Loss: 0.00003228
Iteration 41/1000 | Loss: 0.00003228
Iteration 42/1000 | Loss: 0.00003228
Iteration 43/1000 | Loss: 0.00003228
Iteration 44/1000 | Loss: 0.00003228
Iteration 45/1000 | Loss: 0.00003228
Iteration 46/1000 | Loss: 0.00003227
Iteration 47/1000 | Loss: 0.00003227
Iteration 48/1000 | Loss: 0.00003226
Iteration 49/1000 | Loss: 0.00003226
Iteration 50/1000 | Loss: 0.00003226
Iteration 51/1000 | Loss: 0.00003225
Iteration 52/1000 | Loss: 0.00003225
Iteration 53/1000 | Loss: 0.00003225
Iteration 54/1000 | Loss: 0.00003225
Iteration 55/1000 | Loss: 0.00003224
Iteration 56/1000 | Loss: 0.00003224
Iteration 57/1000 | Loss: 0.00003224
Iteration 58/1000 | Loss: 0.00003223
Iteration 59/1000 | Loss: 0.00003223
Iteration 60/1000 | Loss: 0.00003223
Iteration 61/1000 | Loss: 0.00003222
Iteration 62/1000 | Loss: 0.00003222
Iteration 63/1000 | Loss: 0.00003222
Iteration 64/1000 | Loss: 0.00003221
Iteration 65/1000 | Loss: 0.00003221
Iteration 66/1000 | Loss: 0.00003221
Iteration 67/1000 | Loss: 0.00003220
Iteration 68/1000 | Loss: 0.00003220
Iteration 69/1000 | Loss: 0.00003220
Iteration 70/1000 | Loss: 0.00003219
Iteration 71/1000 | Loss: 0.00003219
Iteration 72/1000 | Loss: 0.00003219
Iteration 73/1000 | Loss: 0.00003218
Iteration 74/1000 | Loss: 0.00003218
Iteration 75/1000 | Loss: 0.00003218
Iteration 76/1000 | Loss: 0.00003217
Iteration 77/1000 | Loss: 0.00003217
Iteration 78/1000 | Loss: 0.00003217
Iteration 79/1000 | Loss: 0.00003217
Iteration 80/1000 | Loss: 0.00003217
Iteration 81/1000 | Loss: 0.00003216
Iteration 82/1000 | Loss: 0.00003216
Iteration 83/1000 | Loss: 0.00003216
Iteration 84/1000 | Loss: 0.00003216
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Iteration 89/1000 | Loss: 0.00003215
Iteration 90/1000 | Loss: 0.00003215
Iteration 91/1000 | Loss: 0.00003215
Iteration 92/1000 | Loss: 0.00003215
Iteration 93/1000 | Loss: 0.00003215
Iteration 94/1000 | Loss: 0.00003214
Iteration 95/1000 | Loss: 0.00003214
Iteration 96/1000 | Loss: 0.00003214
Iteration 97/1000 | Loss: 0.00003214
Iteration 98/1000 | Loss: 0.00003213
Iteration 99/1000 | Loss: 0.00003213
Iteration 100/1000 | Loss: 0.00003213
Iteration 101/1000 | Loss: 0.00003213
Iteration 102/1000 | Loss: 0.00003213
Iteration 103/1000 | Loss: 0.00003213
Iteration 104/1000 | Loss: 0.00003213
Iteration 105/1000 | Loss: 0.00003213
Iteration 106/1000 | Loss: 0.00003213
Iteration 107/1000 | Loss: 0.00003213
Iteration 108/1000 | Loss: 0.00003213
Iteration 109/1000 | Loss: 0.00003212
Iteration 110/1000 | Loss: 0.00003212
Iteration 111/1000 | Loss: 0.00003212
Iteration 112/1000 | Loss: 0.00003212
Iteration 113/1000 | Loss: 0.00003211
Iteration 114/1000 | Loss: 0.00003211
Iteration 115/1000 | Loss: 0.00003211
Iteration 116/1000 | Loss: 0.00003211
Iteration 117/1000 | Loss: 0.00003211
Iteration 118/1000 | Loss: 0.00003210
Iteration 119/1000 | Loss: 0.00003210
Iteration 120/1000 | Loss: 0.00003210
Iteration 121/1000 | Loss: 0.00003210
Iteration 122/1000 | Loss: 0.00003209
Iteration 123/1000 | Loss: 0.00003209
Iteration 124/1000 | Loss: 0.00003209
Iteration 125/1000 | Loss: 0.00003209
Iteration 126/1000 | Loss: 0.00003209
Iteration 127/1000 | Loss: 0.00003209
Iteration 128/1000 | Loss: 0.00003209
Iteration 129/1000 | Loss: 0.00003209
Iteration 130/1000 | Loss: 0.00003209
Iteration 131/1000 | Loss: 0.00003209
Iteration 132/1000 | Loss: 0.00003209
Iteration 133/1000 | Loss: 0.00003209
Iteration 134/1000 | Loss: 0.00003209
Iteration 135/1000 | Loss: 0.00003209
Iteration 136/1000 | Loss: 0.00003209
Iteration 137/1000 | Loss: 0.00003208
Iteration 138/1000 | Loss: 0.00003208
Iteration 139/1000 | Loss: 0.00003208
Iteration 140/1000 | Loss: 0.00003208
Iteration 141/1000 | Loss: 0.00003208
Iteration 142/1000 | Loss: 0.00003208
Iteration 143/1000 | Loss: 0.00003208
Iteration 144/1000 | Loss: 0.00003208
Iteration 145/1000 | Loss: 0.00003208
Iteration 146/1000 | Loss: 0.00003208
Iteration 147/1000 | Loss: 0.00003208
Iteration 148/1000 | Loss: 0.00003208
Iteration 149/1000 | Loss: 0.00003208
Iteration 150/1000 | Loss: 0.00003208
Iteration 151/1000 | Loss: 0.00003208
Iteration 152/1000 | Loss: 0.00003207
Iteration 153/1000 | Loss: 0.00003207
Iteration 154/1000 | Loss: 0.00003207
Iteration 155/1000 | Loss: 0.00003207
Iteration 156/1000 | Loss: 0.00003207
Iteration 157/1000 | Loss: 0.00003207
Iteration 158/1000 | Loss: 0.00003207
Iteration 159/1000 | Loss: 0.00003207
Iteration 160/1000 | Loss: 0.00003207
Iteration 161/1000 | Loss: 0.00003207
Iteration 162/1000 | Loss: 0.00003207
Iteration 163/1000 | Loss: 0.00003207
Iteration 164/1000 | Loss: 0.00003207
Iteration 165/1000 | Loss: 0.00003207
Iteration 166/1000 | Loss: 0.00003206
Iteration 167/1000 | Loss: 0.00003206
Iteration 168/1000 | Loss: 0.00003206
Iteration 169/1000 | Loss: 0.00003206
Iteration 170/1000 | Loss: 0.00003206
Iteration 171/1000 | Loss: 0.00003206
Iteration 172/1000 | Loss: 0.00003206
Iteration 173/1000 | Loss: 0.00003206
Iteration 174/1000 | Loss: 0.00003206
Iteration 175/1000 | Loss: 0.00003206
Iteration 176/1000 | Loss: 0.00003206
Iteration 177/1000 | Loss: 0.00003206
Iteration 178/1000 | Loss: 0.00003206
Iteration 179/1000 | Loss: 0.00003206
Iteration 180/1000 | Loss: 0.00003206
Iteration 181/1000 | Loss: 0.00003206
Iteration 182/1000 | Loss: 0.00003205
Iteration 183/1000 | Loss: 0.00003205
Iteration 184/1000 | Loss: 0.00003205
Iteration 185/1000 | Loss: 0.00003205
Iteration 186/1000 | Loss: 0.00003205
Iteration 187/1000 | Loss: 0.00003205
Iteration 188/1000 | Loss: 0.00003205
Iteration 189/1000 | Loss: 0.00003205
Iteration 190/1000 | Loss: 0.00003205
Iteration 191/1000 | Loss: 0.00003205
Iteration 192/1000 | Loss: 0.00003205
Iteration 193/1000 | Loss: 0.00003205
Iteration 194/1000 | Loss: 0.00003205
Iteration 195/1000 | Loss: 0.00003205
Iteration 196/1000 | Loss: 0.00003205
Iteration 197/1000 | Loss: 0.00003205
Iteration 198/1000 | Loss: 0.00003204
Iteration 199/1000 | Loss: 0.00003204
Iteration 200/1000 | Loss: 0.00003204
Iteration 201/1000 | Loss: 0.00003204
Iteration 202/1000 | Loss: 0.00003204
Iteration 203/1000 | Loss: 0.00003204
Iteration 204/1000 | Loss: 0.00003204
Iteration 205/1000 | Loss: 0.00003204
Iteration 206/1000 | Loss: 0.00003204
Iteration 207/1000 | Loss: 0.00003204
Iteration 208/1000 | Loss: 0.00003204
Iteration 209/1000 | Loss: 0.00003204
Iteration 210/1000 | Loss: 0.00003204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [3.204305176041089e-05, 3.204305176041089e-05, 3.204305176041089e-05, 3.204305176041089e-05, 3.204305176041089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.204305176041089e-05

Optimization complete. Final v2v error: 4.614983081817627 mm

Highest mean error: 6.21381950378418 mm for frame 13

Lowest mean error: 3.6068055629730225 mm for frame 4

Saving results

Total time: 62.17221641540527
