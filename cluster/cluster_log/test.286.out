Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=286, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16016-16071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051323
Iteration 2/25 | Loss: 0.00232666
Iteration 3/25 | Loss: 0.00185368
Iteration 4/25 | Loss: 0.00175751
Iteration 5/25 | Loss: 0.00172200
Iteration 6/25 | Loss: 0.00158818
Iteration 7/25 | Loss: 0.00153822
Iteration 8/25 | Loss: 0.00150197
Iteration 9/25 | Loss: 0.00148229
Iteration 10/25 | Loss: 0.00146222
Iteration 11/25 | Loss: 0.00146573
Iteration 12/25 | Loss: 0.00141234
Iteration 13/25 | Loss: 0.00138705
Iteration 14/25 | Loss: 0.00138190
Iteration 15/25 | Loss: 0.00136776
Iteration 16/25 | Loss: 0.00137154
Iteration 17/25 | Loss: 0.00135849
Iteration 18/25 | Loss: 0.00136475
Iteration 19/25 | Loss: 0.00136189
Iteration 20/25 | Loss: 0.00135132
Iteration 21/25 | Loss: 0.00134814
Iteration 22/25 | Loss: 0.00134775
Iteration 23/25 | Loss: 0.00134763
Iteration 24/25 | Loss: 0.00134757
Iteration 25/25 | Loss: 0.00134757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71810949
Iteration 2/25 | Loss: 0.00553005
Iteration 3/25 | Loss: 0.00553005
Iteration 4/25 | Loss: 0.00553004
Iteration 5/25 | Loss: 0.00553004
Iteration 6/25 | Loss: 0.00553004
Iteration 7/25 | Loss: 0.00553004
Iteration 8/25 | Loss: 0.00553004
Iteration 9/25 | Loss: 0.00553004
Iteration 10/25 | Loss: 0.00553004
Iteration 11/25 | Loss: 0.00553004
Iteration 12/25 | Loss: 0.00553004
Iteration 13/25 | Loss: 0.00553004
Iteration 14/25 | Loss: 0.00553004
Iteration 15/25 | Loss: 0.00553004
Iteration 16/25 | Loss: 0.00553004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005530042108148336, 0.005530042108148336, 0.005530042108148336, 0.005530042108148336, 0.005530042108148336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005530042108148336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00553004
Iteration 2/1000 | Loss: 0.00050773
Iteration 3/1000 | Loss: 0.00038631
Iteration 4/1000 | Loss: 0.00030414
Iteration 5/1000 | Loss: 0.00026677
Iteration 6/1000 | Loss: 0.00024686
Iteration 7/1000 | Loss: 0.00023102
Iteration 8/1000 | Loss: 0.00946031
Iteration 9/1000 | Loss: 0.00536254
Iteration 10/1000 | Loss: 0.00832960
Iteration 11/1000 | Loss: 0.00132212
Iteration 12/1000 | Loss: 0.00377789
Iteration 13/1000 | Loss: 0.00080364
Iteration 14/1000 | Loss: 0.00022675
Iteration 15/1000 | Loss: 0.00012723
Iteration 16/1000 | Loss: 0.00007907
Iteration 17/1000 | Loss: 0.00005349
Iteration 18/1000 | Loss: 0.00004073
Iteration 19/1000 | Loss: 0.00003203
Iteration 20/1000 | Loss: 0.00002716
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00001793
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001420
Iteration 29/1000 | Loss: 0.00001399
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001355
Iteration 80/1000 | Loss: 0.00001355
Iteration 81/1000 | Loss: 0.00001355
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001354
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001354
Iteration 92/1000 | Loss: 0.00001354
Iteration 93/1000 | Loss: 0.00001354
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001353
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3520120774046518e-05, 1.3520120774046518e-05, 1.3520120774046518e-05, 1.3520120774046518e-05, 1.3520120774046518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3520120774046518e-05

Optimization complete. Final v2v error: 3.1469898223876953 mm

Highest mean error: 3.3225741386413574 mm for frame 12

Lowest mean error: 2.9333646297454834 mm for frame 122

Saving results

Total time: 91.9934434890747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412660
Iteration 2/25 | Loss: 0.00103760
Iteration 3/25 | Loss: 0.00089871
Iteration 4/25 | Loss: 0.00086318
Iteration 5/25 | Loss: 0.00085750
Iteration 6/25 | Loss: 0.00085575
Iteration 7/25 | Loss: 0.00085507
Iteration 8/25 | Loss: 0.00085500
Iteration 9/25 | Loss: 0.00085500
Iteration 10/25 | Loss: 0.00085500
Iteration 11/25 | Loss: 0.00085500
Iteration 12/25 | Loss: 0.00085500
Iteration 13/25 | Loss: 0.00085500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008549974299967289, 0.0008549974299967289, 0.0008549974299967289, 0.0008549974299967289, 0.0008549974299967289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008549974299967289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76031733
Iteration 2/25 | Loss: 0.00383386
Iteration 3/25 | Loss: 0.00383386
Iteration 4/25 | Loss: 0.00383386
Iteration 5/25 | Loss: 0.00383386
Iteration 6/25 | Loss: 0.00383386
Iteration 7/25 | Loss: 0.00383386
Iteration 8/25 | Loss: 0.00383386
Iteration 9/25 | Loss: 0.00383386
Iteration 10/25 | Loss: 0.00383386
Iteration 11/25 | Loss: 0.00383386
Iteration 12/25 | Loss: 0.00383386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0038338552694767714, 0.0038338552694767714, 0.0038338552694767714, 0.0038338552694767714, 0.0038338552694767714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038338552694767714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00383386
Iteration 2/1000 | Loss: 0.00002508
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001266
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001237
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001236
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001235
Iteration 50/1000 | Loss: 0.00001235
Iteration 51/1000 | Loss: 0.00001235
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001231
Iteration 69/1000 | Loss: 0.00001231
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001229
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001224
Iteration 103/1000 | Loss: 0.00001224
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001218
Iteration 123/1000 | Loss: 0.00001218
Iteration 124/1000 | Loss: 0.00001218
Iteration 125/1000 | Loss: 0.00001218
Iteration 126/1000 | Loss: 0.00001217
Iteration 127/1000 | Loss: 0.00001217
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001210
Iteration 148/1000 | Loss: 0.00001210
Iteration 149/1000 | Loss: 0.00001210
Iteration 150/1000 | Loss: 0.00001210
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001209
Iteration 155/1000 | Loss: 0.00001209
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001209
Iteration 160/1000 | Loss: 0.00001209
Iteration 161/1000 | Loss: 0.00001209
Iteration 162/1000 | Loss: 0.00001208
Iteration 163/1000 | Loss: 0.00001208
Iteration 164/1000 | Loss: 0.00001208
Iteration 165/1000 | Loss: 0.00001208
Iteration 166/1000 | Loss: 0.00001208
Iteration 167/1000 | Loss: 0.00001208
Iteration 168/1000 | Loss: 0.00001208
Iteration 169/1000 | Loss: 0.00001208
Iteration 170/1000 | Loss: 0.00001208
Iteration 171/1000 | Loss: 0.00001208
Iteration 172/1000 | Loss: 0.00001208
Iteration 173/1000 | Loss: 0.00001208
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001207
Iteration 176/1000 | Loss: 0.00001207
Iteration 177/1000 | Loss: 0.00001207
Iteration 178/1000 | Loss: 0.00001207
Iteration 179/1000 | Loss: 0.00001207
Iteration 180/1000 | Loss: 0.00001207
Iteration 181/1000 | Loss: 0.00001207
Iteration 182/1000 | Loss: 0.00001207
Iteration 183/1000 | Loss: 0.00001207
Iteration 184/1000 | Loss: 0.00001207
Iteration 185/1000 | Loss: 0.00001207
Iteration 186/1000 | Loss: 0.00001207
Iteration 187/1000 | Loss: 0.00001207
Iteration 188/1000 | Loss: 0.00001207
Iteration 189/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2070933735230938e-05, 1.2070933735230938e-05, 1.2070933735230938e-05, 1.2070933735230938e-05, 1.2070933735230938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2070933735230938e-05

Optimization complete. Final v2v error: 2.8735883235931396 mm

Highest mean error: 3.3021316528320312 mm for frame 0

Lowest mean error: 2.719135284423828 mm for frame 68

Saving results

Total time: 40.78318214416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446728
Iteration 2/25 | Loss: 0.00103767
Iteration 3/25 | Loss: 0.00085814
Iteration 4/25 | Loss: 0.00083648
Iteration 5/25 | Loss: 0.00082988
Iteration 6/25 | Loss: 0.00082815
Iteration 7/25 | Loss: 0.00082771
Iteration 8/25 | Loss: 0.00082761
Iteration 9/25 | Loss: 0.00082761
Iteration 10/25 | Loss: 0.00082761
Iteration 11/25 | Loss: 0.00082761
Iteration 12/25 | Loss: 0.00082761
Iteration 13/25 | Loss: 0.00082761
Iteration 14/25 | Loss: 0.00082761
Iteration 15/25 | Loss: 0.00082761
Iteration 16/25 | Loss: 0.00082761
Iteration 17/25 | Loss: 0.00082761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008276075823232532, 0.0008276075823232532, 0.0008276075823232532, 0.0008276075823232532, 0.0008276075823232532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008276075823232532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73589480
Iteration 2/25 | Loss: 0.00303716
Iteration 3/25 | Loss: 0.00303714
Iteration 4/25 | Loss: 0.00303714
Iteration 5/25 | Loss: 0.00303714
Iteration 6/25 | Loss: 0.00303713
Iteration 7/25 | Loss: 0.00303713
Iteration 8/25 | Loss: 0.00303713
Iteration 9/25 | Loss: 0.00303713
Iteration 10/25 | Loss: 0.00303713
Iteration 11/25 | Loss: 0.00303713
Iteration 12/25 | Loss: 0.00303713
Iteration 13/25 | Loss: 0.00303713
Iteration 14/25 | Loss: 0.00303713
Iteration 15/25 | Loss: 0.00303713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0030371330212801695, 0.0030371330212801695, 0.0030371330212801695, 0.0030371330212801695, 0.0030371330212801695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030371330212801695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00303713
Iteration 2/1000 | Loss: 0.00002985
Iteration 3/1000 | Loss: 0.00001795
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001128
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001105
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001104
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001094
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001092
Iteration 58/1000 | Loss: 0.00001092
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001091
Iteration 61/1000 | Loss: 0.00001091
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001091
Iteration 65/1000 | Loss: 0.00001091
Iteration 66/1000 | Loss: 0.00001091
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001090
Iteration 72/1000 | Loss: 0.00001090
Iteration 73/1000 | Loss: 0.00001090
Iteration 74/1000 | Loss: 0.00001090
Iteration 75/1000 | Loss: 0.00001090
Iteration 76/1000 | Loss: 0.00001090
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001089
Iteration 79/1000 | Loss: 0.00001089
Iteration 80/1000 | Loss: 0.00001089
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001088
Iteration 93/1000 | Loss: 0.00001088
Iteration 94/1000 | Loss: 0.00001088
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001088
Iteration 99/1000 | Loss: 0.00001088
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001088
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001088
Iteration 137/1000 | Loss: 0.00001088
Iteration 138/1000 | Loss: 0.00001088
Iteration 139/1000 | Loss: 0.00001088
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001088
Iteration 142/1000 | Loss: 0.00001088
Iteration 143/1000 | Loss: 0.00001088
Iteration 144/1000 | Loss: 0.00001088
Iteration 145/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0880826266657095e-05, 1.0880826266657095e-05, 1.0880826266657095e-05, 1.0880826266657095e-05, 1.0880826266657095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0880826266657095e-05

Optimization complete. Final v2v error: 2.7502360343933105 mm

Highest mean error: 3.40511155128479 mm for frame 37

Lowest mean error: 2.235678195953369 mm for frame 118

Saving results

Total time: 34.967143297195435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164919
Iteration 2/25 | Loss: 0.01164918
Iteration 3/25 | Loss: 0.01164918
Iteration 4/25 | Loss: 0.01164918
Iteration 5/25 | Loss: 0.01164918
Iteration 6/25 | Loss: 0.01164918
Iteration 7/25 | Loss: 0.00303117
Iteration 8/25 | Loss: 0.00290468
Iteration 9/25 | Loss: 0.00216618
Iteration 10/25 | Loss: 0.00206931
Iteration 11/25 | Loss: 0.00196176
Iteration 12/25 | Loss: 0.00182142
Iteration 13/25 | Loss: 0.00180459
Iteration 14/25 | Loss: 0.00161820
Iteration 15/25 | Loss: 0.00153036
Iteration 16/25 | Loss: 0.00143352
Iteration 17/25 | Loss: 0.00135172
Iteration 18/25 | Loss: 0.00134127
Iteration 19/25 | Loss: 0.00127166
Iteration 20/25 | Loss: 0.00125789
Iteration 21/25 | Loss: 0.00121962
Iteration 22/25 | Loss: 0.00122408
Iteration 23/25 | Loss: 0.00124987
Iteration 24/25 | Loss: 0.00124718
Iteration 25/25 | Loss: 0.00116646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05151343
Iteration 2/25 | Loss: 0.00475751
Iteration 3/25 | Loss: 0.00459916
Iteration 4/25 | Loss: 0.00459916
Iteration 5/25 | Loss: 0.00459915
Iteration 6/25 | Loss: 0.00459915
Iteration 7/25 | Loss: 0.00459915
Iteration 8/25 | Loss: 0.00459915
Iteration 9/25 | Loss: 0.00459915
Iteration 10/25 | Loss: 0.00459915
Iteration 11/25 | Loss: 0.00459915
Iteration 12/25 | Loss: 0.00459915
Iteration 13/25 | Loss: 0.00459915
Iteration 14/25 | Loss: 0.00459915
Iteration 15/25 | Loss: 0.00459915
Iteration 16/25 | Loss: 0.00459915
Iteration 17/25 | Loss: 0.00459915
Iteration 18/25 | Loss: 0.00459915
Iteration 19/25 | Loss: 0.00459915
Iteration 20/25 | Loss: 0.00459915
Iteration 21/25 | Loss: 0.00459915
Iteration 22/25 | Loss: 0.00459915
Iteration 23/25 | Loss: 0.00459915
Iteration 24/25 | Loss: 0.00459915
Iteration 25/25 | Loss: 0.00459915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00459915
Iteration 2/1000 | Loss: 0.00373226
Iteration 3/1000 | Loss: 0.00372912
Iteration 4/1000 | Loss: 0.00374417
Iteration 5/1000 | Loss: 0.00152861
Iteration 6/1000 | Loss: 0.00463038
Iteration 7/1000 | Loss: 0.00485466
Iteration 8/1000 | Loss: 0.00308649
Iteration 9/1000 | Loss: 0.00537845
Iteration 10/1000 | Loss: 0.00388512
Iteration 11/1000 | Loss: 0.00200846
Iteration 12/1000 | Loss: 0.00257114
Iteration 13/1000 | Loss: 0.00181223
Iteration 14/1000 | Loss: 0.00115214
Iteration 15/1000 | Loss: 0.00208768
Iteration 16/1000 | Loss: 0.00094562
Iteration 17/1000 | Loss: 0.00096745
Iteration 18/1000 | Loss: 0.00076973
Iteration 19/1000 | Loss: 0.00059422
Iteration 20/1000 | Loss: 0.00138606
Iteration 21/1000 | Loss: 0.00075097
Iteration 22/1000 | Loss: 0.00050958
Iteration 23/1000 | Loss: 0.00060519
Iteration 24/1000 | Loss: 0.00049218
Iteration 25/1000 | Loss: 0.00044314
Iteration 26/1000 | Loss: 0.00096966
Iteration 27/1000 | Loss: 0.00055657
Iteration 28/1000 | Loss: 0.00068386
Iteration 29/1000 | Loss: 0.00076545
Iteration 30/1000 | Loss: 0.00061037
Iteration 31/1000 | Loss: 0.00064360
Iteration 32/1000 | Loss: 0.00210202
Iteration 33/1000 | Loss: 0.00134818
Iteration 34/1000 | Loss: 0.00068168
Iteration 35/1000 | Loss: 0.00041070
Iteration 36/1000 | Loss: 0.00044198
Iteration 37/1000 | Loss: 0.00057197
Iteration 38/1000 | Loss: 0.00048419
Iteration 39/1000 | Loss: 0.00101767
Iteration 40/1000 | Loss: 0.00045708
Iteration 41/1000 | Loss: 0.00031739
Iteration 42/1000 | Loss: 0.00040108
Iteration 43/1000 | Loss: 0.00060295
Iteration 44/1000 | Loss: 0.00051523
Iteration 45/1000 | Loss: 0.00033055
Iteration 46/1000 | Loss: 0.00044908
Iteration 47/1000 | Loss: 0.00045750
Iteration 48/1000 | Loss: 0.00079387
Iteration 49/1000 | Loss: 0.00075403
Iteration 50/1000 | Loss: 0.00101224
Iteration 51/1000 | Loss: 0.00166820
Iteration 52/1000 | Loss: 0.00091209
Iteration 53/1000 | Loss: 0.00098647
Iteration 54/1000 | Loss: 0.00215886
Iteration 55/1000 | Loss: 0.00187070
Iteration 56/1000 | Loss: 0.00153884
Iteration 57/1000 | Loss: 0.00092662
Iteration 58/1000 | Loss: 0.00028711
Iteration 59/1000 | Loss: 0.00079785
Iteration 60/1000 | Loss: 0.00156172
Iteration 61/1000 | Loss: 0.00156772
Iteration 62/1000 | Loss: 0.00103471
Iteration 63/1000 | Loss: 0.00097574
Iteration 64/1000 | Loss: 0.00092795
Iteration 65/1000 | Loss: 0.00059610
Iteration 66/1000 | Loss: 0.00073137
Iteration 67/1000 | Loss: 0.00114205
Iteration 68/1000 | Loss: 0.00059450
Iteration 69/1000 | Loss: 0.00065281
Iteration 70/1000 | Loss: 0.00036278
Iteration 71/1000 | Loss: 0.00060403
Iteration 72/1000 | Loss: 0.00049278
Iteration 73/1000 | Loss: 0.00040347
Iteration 74/1000 | Loss: 0.00064633
Iteration 75/1000 | Loss: 0.00071365
Iteration 76/1000 | Loss: 0.00093977
Iteration 77/1000 | Loss: 0.00155753
Iteration 78/1000 | Loss: 0.00102575
Iteration 79/1000 | Loss: 0.00048594
Iteration 80/1000 | Loss: 0.00051103
Iteration 81/1000 | Loss: 0.00104097
Iteration 82/1000 | Loss: 0.00105438
Iteration 83/1000 | Loss: 0.00074121
Iteration 84/1000 | Loss: 0.00058442
Iteration 85/1000 | Loss: 0.00103623
Iteration 86/1000 | Loss: 0.00104960
Iteration 87/1000 | Loss: 0.00168240
Iteration 88/1000 | Loss: 0.00113431
Iteration 89/1000 | Loss: 0.00107242
Iteration 90/1000 | Loss: 0.00032641
Iteration 91/1000 | Loss: 0.00037630
Iteration 92/1000 | Loss: 0.00026030
Iteration 93/1000 | Loss: 0.00043785
Iteration 94/1000 | Loss: 0.00052374
Iteration 95/1000 | Loss: 0.00040668
Iteration 96/1000 | Loss: 0.00035431
Iteration 97/1000 | Loss: 0.00044873
Iteration 98/1000 | Loss: 0.00047084
Iteration 99/1000 | Loss: 0.00038254
Iteration 100/1000 | Loss: 0.00033498
Iteration 101/1000 | Loss: 0.00029157
Iteration 102/1000 | Loss: 0.00019789
Iteration 103/1000 | Loss: 0.00028985
Iteration 104/1000 | Loss: 0.00051038
Iteration 105/1000 | Loss: 0.00045050
Iteration 106/1000 | Loss: 0.00028997
Iteration 107/1000 | Loss: 0.00031236
Iteration 108/1000 | Loss: 0.00034875
Iteration 109/1000 | Loss: 0.00060173
Iteration 110/1000 | Loss: 0.00097185
Iteration 111/1000 | Loss: 0.00059251
Iteration 112/1000 | Loss: 0.00036568
Iteration 113/1000 | Loss: 0.00032903
Iteration 114/1000 | Loss: 0.00094430
Iteration 115/1000 | Loss: 0.00118479
Iteration 116/1000 | Loss: 0.00064953
Iteration 117/1000 | Loss: 0.00047262
Iteration 118/1000 | Loss: 0.00051361
Iteration 119/1000 | Loss: 0.00031385
Iteration 120/1000 | Loss: 0.00044263
Iteration 121/1000 | Loss: 0.00041598
Iteration 122/1000 | Loss: 0.00043336
Iteration 123/1000 | Loss: 0.00049616
Iteration 124/1000 | Loss: 0.00067846
Iteration 125/1000 | Loss: 0.00068687
Iteration 126/1000 | Loss: 0.00062791
Iteration 127/1000 | Loss: 0.00035913
Iteration 128/1000 | Loss: 0.00051451
Iteration 129/1000 | Loss: 0.00061427
Iteration 130/1000 | Loss: 0.00038132
Iteration 131/1000 | Loss: 0.00051111
Iteration 132/1000 | Loss: 0.00049577
Iteration 133/1000 | Loss: 0.00043975
Iteration 134/1000 | Loss: 0.00040868
Iteration 135/1000 | Loss: 0.00030185
Iteration 136/1000 | Loss: 0.00055340
Iteration 137/1000 | Loss: 0.00025451
Iteration 138/1000 | Loss: 0.00020069
Iteration 139/1000 | Loss: 0.00031868
Iteration 140/1000 | Loss: 0.00034292
Iteration 141/1000 | Loss: 0.00076298
Iteration 142/1000 | Loss: 0.00028370
Iteration 143/1000 | Loss: 0.00020077
Iteration 144/1000 | Loss: 0.00026872
Iteration 145/1000 | Loss: 0.00042971
Iteration 146/1000 | Loss: 0.00025148
Iteration 147/1000 | Loss: 0.00037010
Iteration 148/1000 | Loss: 0.00037892
Iteration 149/1000 | Loss: 0.00037062
Iteration 150/1000 | Loss: 0.00028373
Iteration 151/1000 | Loss: 0.00039842
Iteration 152/1000 | Loss: 0.00088458
Iteration 153/1000 | Loss: 0.00064589
Iteration 154/1000 | Loss: 0.00060739
Iteration 155/1000 | Loss: 0.00100269
Iteration 156/1000 | Loss: 0.00021555
Iteration 157/1000 | Loss: 0.00011270
Iteration 158/1000 | Loss: 0.00028480
Iteration 159/1000 | Loss: 0.00015837
Iteration 160/1000 | Loss: 0.00022279
Iteration 161/1000 | Loss: 0.00018476
Iteration 162/1000 | Loss: 0.00027485
Iteration 163/1000 | Loss: 0.00007525
Iteration 164/1000 | Loss: 0.00035481
Iteration 165/1000 | Loss: 0.00127458
Iteration 166/1000 | Loss: 0.00127000
Iteration 167/1000 | Loss: 0.00080621
Iteration 168/1000 | Loss: 0.00040760
Iteration 169/1000 | Loss: 0.00047084
Iteration 170/1000 | Loss: 0.00071884
Iteration 171/1000 | Loss: 0.00068975
Iteration 172/1000 | Loss: 0.00057165
Iteration 173/1000 | Loss: 0.00011080
Iteration 174/1000 | Loss: 0.00010161
Iteration 175/1000 | Loss: 0.00012219
Iteration 176/1000 | Loss: 0.00008281
Iteration 177/1000 | Loss: 0.00093931
Iteration 178/1000 | Loss: 0.00024377
Iteration 179/1000 | Loss: 0.00024983
Iteration 180/1000 | Loss: 0.00032341
Iteration 181/1000 | Loss: 0.00021163
Iteration 182/1000 | Loss: 0.00033521
Iteration 183/1000 | Loss: 0.00008069
Iteration 184/1000 | Loss: 0.00009115
Iteration 185/1000 | Loss: 0.00063091
Iteration 186/1000 | Loss: 0.00222365
Iteration 187/1000 | Loss: 0.00253033
Iteration 188/1000 | Loss: 0.00338325
Iteration 189/1000 | Loss: 0.00353900
Iteration 190/1000 | Loss: 0.00370721
Iteration 191/1000 | Loss: 0.00306337
Iteration 192/1000 | Loss: 0.00340971
Iteration 193/1000 | Loss: 0.00331956
Iteration 194/1000 | Loss: 0.00188147
Iteration 195/1000 | Loss: 0.00022176
Iteration 196/1000 | Loss: 0.00104206
Iteration 197/1000 | Loss: 0.00187440
Iteration 198/1000 | Loss: 0.00097979
Iteration 199/1000 | Loss: 0.00203235
Iteration 200/1000 | Loss: 0.00073492
Iteration 201/1000 | Loss: 0.00011931
Iteration 202/1000 | Loss: 0.00124389
Iteration 203/1000 | Loss: 0.00074722
Iteration 204/1000 | Loss: 0.00083069
Iteration 205/1000 | Loss: 0.00075900
Iteration 206/1000 | Loss: 0.00082526
Iteration 207/1000 | Loss: 0.00031964
Iteration 208/1000 | Loss: 0.00005758
Iteration 209/1000 | Loss: 0.00067816
Iteration 210/1000 | Loss: 0.00064492
Iteration 211/1000 | Loss: 0.00108495
Iteration 212/1000 | Loss: 0.00151426
Iteration 213/1000 | Loss: 0.00106653
Iteration 214/1000 | Loss: 0.00072300
Iteration 215/1000 | Loss: 0.00137627
Iteration 216/1000 | Loss: 0.00092121
Iteration 217/1000 | Loss: 0.00039076
Iteration 218/1000 | Loss: 0.00140264
Iteration 219/1000 | Loss: 0.00081417
Iteration 220/1000 | Loss: 0.00083231
Iteration 221/1000 | Loss: 0.00161115
Iteration 222/1000 | Loss: 0.00115804
Iteration 223/1000 | Loss: 0.00178274
Iteration 224/1000 | Loss: 0.00185321
Iteration 225/1000 | Loss: 0.00005657
Iteration 226/1000 | Loss: 0.00111035
Iteration 227/1000 | Loss: 0.00089333
Iteration 228/1000 | Loss: 0.00076184
Iteration 229/1000 | Loss: 0.00147943
Iteration 230/1000 | Loss: 0.00005433
Iteration 231/1000 | Loss: 0.00223577
Iteration 232/1000 | Loss: 0.00012779
Iteration 233/1000 | Loss: 0.00046768
Iteration 234/1000 | Loss: 0.00110895
Iteration 235/1000 | Loss: 0.00008068
Iteration 236/1000 | Loss: 0.00005231
Iteration 237/1000 | Loss: 0.00003858
Iteration 238/1000 | Loss: 0.00123001
Iteration 239/1000 | Loss: 0.00064726
Iteration 240/1000 | Loss: 0.00202437
Iteration 241/1000 | Loss: 0.00117552
Iteration 242/1000 | Loss: 0.00119663
Iteration 243/1000 | Loss: 0.00005304
Iteration 244/1000 | Loss: 0.00003579
Iteration 245/1000 | Loss: 0.00003390
Iteration 246/1000 | Loss: 0.00003280
Iteration 247/1000 | Loss: 0.00003211
Iteration 248/1000 | Loss: 0.00114171
Iteration 249/1000 | Loss: 0.00123348
Iteration 250/1000 | Loss: 0.00086193
Iteration 251/1000 | Loss: 0.00097281
Iteration 252/1000 | Loss: 0.00090465
Iteration 253/1000 | Loss: 0.00073006
Iteration 254/1000 | Loss: 0.00066359
Iteration 255/1000 | Loss: 0.00086751
Iteration 256/1000 | Loss: 0.00080366
Iteration 257/1000 | Loss: 0.00085086
Iteration 258/1000 | Loss: 0.00159732
Iteration 259/1000 | Loss: 0.00102344
Iteration 260/1000 | Loss: 0.00100550
Iteration 261/1000 | Loss: 0.00103039
Iteration 262/1000 | Loss: 0.00181649
Iteration 263/1000 | Loss: 0.00058562
Iteration 264/1000 | Loss: 0.00104300
Iteration 265/1000 | Loss: 0.00065421
Iteration 266/1000 | Loss: 0.00102732
Iteration 267/1000 | Loss: 0.00119660
Iteration 268/1000 | Loss: 0.00070154
Iteration 269/1000 | Loss: 0.00095907
Iteration 270/1000 | Loss: 0.00156363
Iteration 271/1000 | Loss: 0.00008741
Iteration 272/1000 | Loss: 0.00078734
Iteration 273/1000 | Loss: 0.00003201
Iteration 274/1000 | Loss: 0.00002828
Iteration 275/1000 | Loss: 0.00002730
Iteration 276/1000 | Loss: 0.00002672
Iteration 277/1000 | Loss: 0.00002628
Iteration 278/1000 | Loss: 0.00002580
Iteration 279/1000 | Loss: 0.00002567
Iteration 280/1000 | Loss: 0.00002565
Iteration 281/1000 | Loss: 0.00035691
Iteration 282/1000 | Loss: 0.00031444
Iteration 283/1000 | Loss: 0.00005598
Iteration 284/1000 | Loss: 0.00002606
Iteration 285/1000 | Loss: 0.00038636
Iteration 286/1000 | Loss: 0.00047961
Iteration 287/1000 | Loss: 0.00015726
Iteration 288/1000 | Loss: 0.00010271
Iteration 289/1000 | Loss: 0.00004585
Iteration 290/1000 | Loss: 0.00005558
Iteration 291/1000 | Loss: 0.00006415
Iteration 292/1000 | Loss: 0.00018691
Iteration 293/1000 | Loss: 0.00025890
Iteration 294/1000 | Loss: 0.00012548
Iteration 295/1000 | Loss: 0.00008812
Iteration 296/1000 | Loss: 0.00013229
Iteration 297/1000 | Loss: 0.00025454
Iteration 298/1000 | Loss: 0.00012813
Iteration 299/1000 | Loss: 0.00009633
Iteration 300/1000 | Loss: 0.00010101
Iteration 301/1000 | Loss: 0.00008437
Iteration 302/1000 | Loss: 0.00007278
Iteration 303/1000 | Loss: 0.00009074
Iteration 304/1000 | Loss: 0.00011127
Iteration 305/1000 | Loss: 0.00016410
Iteration 306/1000 | Loss: 0.00012744
Iteration 307/1000 | Loss: 0.00011160
Iteration 308/1000 | Loss: 0.00013375
Iteration 309/1000 | Loss: 0.00011009
Iteration 310/1000 | Loss: 0.00010625
Iteration 311/1000 | Loss: 0.00013481
Iteration 312/1000 | Loss: 0.00027206
Iteration 313/1000 | Loss: 0.00016188
Iteration 314/1000 | Loss: 0.00009461
Iteration 315/1000 | Loss: 0.00009725
Iteration 316/1000 | Loss: 0.00012644
Iteration 317/1000 | Loss: 0.00012539
Iteration 318/1000 | Loss: 0.00041365
Iteration 319/1000 | Loss: 0.00021644
Iteration 320/1000 | Loss: 0.00016853
Iteration 321/1000 | Loss: 0.00007243
Iteration 322/1000 | Loss: 0.00004428
Iteration 323/1000 | Loss: 0.00014567
Iteration 324/1000 | Loss: 0.00003504
Iteration 325/1000 | Loss: 0.00003028
Iteration 326/1000 | Loss: 0.00018235
Iteration 327/1000 | Loss: 0.00006193
Iteration 328/1000 | Loss: 0.00002987
Iteration 329/1000 | Loss: 0.00015341
Iteration 330/1000 | Loss: 0.00005074
Iteration 331/1000 | Loss: 0.00009770
Iteration 332/1000 | Loss: 0.00003023
Iteration 333/1000 | Loss: 0.00002848
Iteration 334/1000 | Loss: 0.00002746
Iteration 335/1000 | Loss: 0.00002683
Iteration 336/1000 | Loss: 0.00002640
Iteration 337/1000 | Loss: 0.00002608
Iteration 338/1000 | Loss: 0.00002570
Iteration 339/1000 | Loss: 0.00010086
Iteration 340/1000 | Loss: 0.00012796
Iteration 341/1000 | Loss: 0.00025138
Iteration 342/1000 | Loss: 0.00022080
Iteration 343/1000 | Loss: 0.00023023
Iteration 344/1000 | Loss: 0.00040978
Iteration 345/1000 | Loss: 0.00028044
Iteration 346/1000 | Loss: 0.00037486
Iteration 347/1000 | Loss: 0.00031519
Iteration 348/1000 | Loss: 0.00004665
Iteration 349/1000 | Loss: 0.00003559
Iteration 350/1000 | Loss: 0.00023596
Iteration 351/1000 | Loss: 0.00006625
Iteration 352/1000 | Loss: 0.00023401
Iteration 353/1000 | Loss: 0.00027324
Iteration 354/1000 | Loss: 0.00023929
Iteration 355/1000 | Loss: 0.00013452
Iteration 356/1000 | Loss: 0.00009198
Iteration 357/1000 | Loss: 0.00003665
Iteration 358/1000 | Loss: 0.00017535
Iteration 359/1000 | Loss: 0.00012052
Iteration 360/1000 | Loss: 0.00009549
Iteration 361/1000 | Loss: 0.00008605
Iteration 362/1000 | Loss: 0.00011069
Iteration 363/1000 | Loss: 0.00004814
Iteration 364/1000 | Loss: 0.00003279
Iteration 365/1000 | Loss: 0.00003054
Iteration 366/1000 | Loss: 0.00002896
Iteration 367/1000 | Loss: 0.00021862
Iteration 368/1000 | Loss: 0.00016660
Iteration 369/1000 | Loss: 0.00003456
Iteration 370/1000 | Loss: 0.00009891
Iteration 371/1000 | Loss: 0.00016031
Iteration 372/1000 | Loss: 0.00023764
Iteration 373/1000 | Loss: 0.00037167
Iteration 374/1000 | Loss: 0.00047863
Iteration 375/1000 | Loss: 0.00004915
Iteration 376/1000 | Loss: 0.00007219
Iteration 377/1000 | Loss: 0.00016732
Iteration 378/1000 | Loss: 0.00008582
Iteration 379/1000 | Loss: 0.00014897
Iteration 380/1000 | Loss: 0.00005206
Iteration 381/1000 | Loss: 0.00010836
Iteration 382/1000 | Loss: 0.00006045
Iteration 383/1000 | Loss: 0.00006171
Iteration 384/1000 | Loss: 0.00015346
Iteration 385/1000 | Loss: 0.00011766
Iteration 386/1000 | Loss: 0.00002868
Iteration 387/1000 | Loss: 0.00002702
Iteration 388/1000 | Loss: 0.00002600
Iteration 389/1000 | Loss: 0.00002481
Iteration 390/1000 | Loss: 0.00002381
Iteration 391/1000 | Loss: 0.00002310
Iteration 392/1000 | Loss: 0.00002272
Iteration 393/1000 | Loss: 0.00002237
Iteration 394/1000 | Loss: 0.00009593
Iteration 395/1000 | Loss: 0.00006158
Iteration 396/1000 | Loss: 0.00013367
Iteration 397/1000 | Loss: 0.00016180
Iteration 398/1000 | Loss: 0.00011213
Iteration 399/1000 | Loss: 0.00024023
Iteration 400/1000 | Loss: 0.00004143
Iteration 401/1000 | Loss: 0.00002636
Iteration 402/1000 | Loss: 0.00002436
Iteration 403/1000 | Loss: 0.00002278
Iteration 404/1000 | Loss: 0.00002214
Iteration 405/1000 | Loss: 0.00002190
Iteration 406/1000 | Loss: 0.00002170
Iteration 407/1000 | Loss: 0.00002154
Iteration 408/1000 | Loss: 0.00002153
Iteration 409/1000 | Loss: 0.00002153
Iteration 410/1000 | Loss: 0.00002153
Iteration 411/1000 | Loss: 0.00002152
Iteration 412/1000 | Loss: 0.00002152
Iteration 413/1000 | Loss: 0.00002151
Iteration 414/1000 | Loss: 0.00002151
Iteration 415/1000 | Loss: 0.00002151
Iteration 416/1000 | Loss: 0.00002151
Iteration 417/1000 | Loss: 0.00002151
Iteration 418/1000 | Loss: 0.00002150
Iteration 419/1000 | Loss: 0.00002150
Iteration 420/1000 | Loss: 0.00002150
Iteration 421/1000 | Loss: 0.00002149
Iteration 422/1000 | Loss: 0.00002149
Iteration 423/1000 | Loss: 0.00002149
Iteration 424/1000 | Loss: 0.00002149
Iteration 425/1000 | Loss: 0.00002149
Iteration 426/1000 | Loss: 0.00002148
Iteration 427/1000 | Loss: 0.00002148
Iteration 428/1000 | Loss: 0.00002147
Iteration 429/1000 | Loss: 0.00002147
Iteration 430/1000 | Loss: 0.00002147
Iteration 431/1000 | Loss: 0.00002146
Iteration 432/1000 | Loss: 0.00002146
Iteration 433/1000 | Loss: 0.00002146
Iteration 434/1000 | Loss: 0.00002145
Iteration 435/1000 | Loss: 0.00002145
Iteration 436/1000 | Loss: 0.00002145
Iteration 437/1000 | Loss: 0.00002144
Iteration 438/1000 | Loss: 0.00002144
Iteration 439/1000 | Loss: 0.00002144
Iteration 440/1000 | Loss: 0.00002143
Iteration 441/1000 | Loss: 0.00002143
Iteration 442/1000 | Loss: 0.00002143
Iteration 443/1000 | Loss: 0.00002142
Iteration 444/1000 | Loss: 0.00002142
Iteration 445/1000 | Loss: 0.00002142
Iteration 446/1000 | Loss: 0.00002142
Iteration 447/1000 | Loss: 0.00002142
Iteration 448/1000 | Loss: 0.00002142
Iteration 449/1000 | Loss: 0.00002142
Iteration 450/1000 | Loss: 0.00002142
Iteration 451/1000 | Loss: 0.00002142
Iteration 452/1000 | Loss: 0.00002142
Iteration 453/1000 | Loss: 0.00002142
Iteration 454/1000 | Loss: 0.00002142
Iteration 455/1000 | Loss: 0.00002142
Iteration 456/1000 | Loss: 0.00002142
Iteration 457/1000 | Loss: 0.00002141
Iteration 458/1000 | Loss: 0.00002141
Iteration 459/1000 | Loss: 0.00002141
Iteration 460/1000 | Loss: 0.00002141
Iteration 461/1000 | Loss: 0.00002140
Iteration 462/1000 | Loss: 0.00002140
Iteration 463/1000 | Loss: 0.00002140
Iteration 464/1000 | Loss: 0.00002140
Iteration 465/1000 | Loss: 0.00002140
Iteration 466/1000 | Loss: 0.00002140
Iteration 467/1000 | Loss: 0.00002140
Iteration 468/1000 | Loss: 0.00002140
Iteration 469/1000 | Loss: 0.00002140
Iteration 470/1000 | Loss: 0.00002140
Iteration 471/1000 | Loss: 0.00002140
Iteration 472/1000 | Loss: 0.00002140
Iteration 473/1000 | Loss: 0.00002140
Iteration 474/1000 | Loss: 0.00002140
Iteration 475/1000 | Loss: 0.00002140
Iteration 476/1000 | Loss: 0.00002140
Iteration 477/1000 | Loss: 0.00002140
Iteration 478/1000 | Loss: 0.00002140
Iteration 479/1000 | Loss: 0.00002140
Iteration 480/1000 | Loss: 0.00002140
Iteration 481/1000 | Loss: 0.00002140
Iteration 482/1000 | Loss: 0.00002140
Iteration 483/1000 | Loss: 0.00002140
Iteration 484/1000 | Loss: 0.00002140
Iteration 485/1000 | Loss: 0.00002140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 485. Stopping optimization.
Last 5 losses: [2.1397139789769426e-05, 2.1397139789769426e-05, 2.1397139789769426e-05, 2.1397139789769426e-05, 2.1397139789769426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1397139789769426e-05

Optimization complete. Final v2v error: 3.8906283378601074 mm

Highest mean error: 4.956564903259277 mm for frame 64

Lowest mean error: 3.604919195175171 mm for frame 15

Saving results

Total time: 700.7713806629181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074008
Iteration 2/25 | Loss: 0.00260684
Iteration 3/25 | Loss: 0.00172811
Iteration 4/25 | Loss: 0.00163764
Iteration 5/25 | Loss: 0.00157837
Iteration 6/25 | Loss: 0.00154469
Iteration 7/25 | Loss: 0.00152315
Iteration 8/25 | Loss: 0.00153253
Iteration 9/25 | Loss: 0.00156963
Iteration 10/25 | Loss: 0.00133915
Iteration 11/25 | Loss: 0.00124428
Iteration 12/25 | Loss: 0.00120616
Iteration 13/25 | Loss: 0.00119960
Iteration 14/25 | Loss: 0.00117383
Iteration 15/25 | Loss: 0.00114545
Iteration 16/25 | Loss: 0.00112889
Iteration 17/25 | Loss: 0.00112724
Iteration 18/25 | Loss: 0.00113473
Iteration 19/25 | Loss: 0.00113389
Iteration 20/25 | Loss: 0.00114179
Iteration 21/25 | Loss: 0.00113350
Iteration 22/25 | Loss: 0.00113232
Iteration 23/25 | Loss: 0.00115753
Iteration 24/25 | Loss: 0.00114763
Iteration 25/25 | Loss: 0.00114148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73344994
Iteration 2/25 | Loss: 0.00559010
Iteration 3/25 | Loss: 0.00559010
Iteration 4/25 | Loss: 0.00559010
Iteration 5/25 | Loss: 0.00559010
Iteration 6/25 | Loss: 0.00559010
Iteration 7/25 | Loss: 0.00559010
Iteration 8/25 | Loss: 0.00559010
Iteration 9/25 | Loss: 0.00559010
Iteration 10/25 | Loss: 0.00559010
Iteration 11/25 | Loss: 0.00559010
Iteration 12/25 | Loss: 0.00559010
Iteration 13/25 | Loss: 0.00559010
Iteration 14/25 | Loss: 0.00559010
Iteration 15/25 | Loss: 0.00559010
Iteration 16/25 | Loss: 0.00559010
Iteration 17/25 | Loss: 0.00559010
Iteration 18/25 | Loss: 0.00559010
Iteration 19/25 | Loss: 0.00559010
Iteration 20/25 | Loss: 0.00559010
Iteration 21/25 | Loss: 0.00559010
Iteration 22/25 | Loss: 0.00559010
Iteration 23/25 | Loss: 0.00559010
Iteration 24/25 | Loss: 0.00559010
Iteration 25/25 | Loss: 0.00559010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00559010
Iteration 2/1000 | Loss: 0.00078274
Iteration 3/1000 | Loss: 0.00059448
Iteration 4/1000 | Loss: 0.00081434
Iteration 5/1000 | Loss: 0.00063349
Iteration 6/1000 | Loss: 0.00111589
Iteration 7/1000 | Loss: 0.00077633
Iteration 8/1000 | Loss: 0.00042340
Iteration 9/1000 | Loss: 0.00070773
Iteration 10/1000 | Loss: 0.00063987
Iteration 11/1000 | Loss: 0.00078835
Iteration 12/1000 | Loss: 0.00108285
Iteration 13/1000 | Loss: 0.00123477
Iteration 14/1000 | Loss: 0.00067520
Iteration 15/1000 | Loss: 0.00079808
Iteration 16/1000 | Loss: 0.00195194
Iteration 17/1000 | Loss: 0.00101147
Iteration 18/1000 | Loss: 0.00053112
Iteration 19/1000 | Loss: 0.00043915
Iteration 20/1000 | Loss: 0.00059530
Iteration 21/1000 | Loss: 0.00065358
Iteration 22/1000 | Loss: 0.00074423
Iteration 23/1000 | Loss: 0.00056866
Iteration 24/1000 | Loss: 0.00055792
Iteration 25/1000 | Loss: 0.00078554
Iteration 26/1000 | Loss: 0.00048451
Iteration 27/1000 | Loss: 0.00059823
Iteration 28/1000 | Loss: 0.00090182
Iteration 29/1000 | Loss: 0.00033476
Iteration 30/1000 | Loss: 0.00108078
Iteration 31/1000 | Loss: 0.00015236
Iteration 32/1000 | Loss: 0.00033552
Iteration 33/1000 | Loss: 0.00050771
Iteration 34/1000 | Loss: 0.00055640
Iteration 35/1000 | Loss: 0.00052178
Iteration 36/1000 | Loss: 0.00160019
Iteration 37/1000 | Loss: 0.00042636
Iteration 38/1000 | Loss: 0.00086986
Iteration 39/1000 | Loss: 0.00074243
Iteration 40/1000 | Loss: 0.00056251
Iteration 41/1000 | Loss: 0.00024778
Iteration 42/1000 | Loss: 0.00032245
Iteration 43/1000 | Loss: 0.00241659
Iteration 44/1000 | Loss: 0.00141927
Iteration 45/1000 | Loss: 0.00046356
Iteration 46/1000 | Loss: 0.00013999
Iteration 47/1000 | Loss: 0.00101851
Iteration 48/1000 | Loss: 0.00019722
Iteration 49/1000 | Loss: 0.00014633
Iteration 50/1000 | Loss: 0.00125921
Iteration 51/1000 | Loss: 0.00011261
Iteration 52/1000 | Loss: 0.00113141
Iteration 53/1000 | Loss: 0.00152606
Iteration 54/1000 | Loss: 0.00018894
Iteration 55/1000 | Loss: 0.00137289
Iteration 56/1000 | Loss: 0.00224358
Iteration 57/1000 | Loss: 0.00078945
Iteration 58/1000 | Loss: 0.00145771
Iteration 59/1000 | Loss: 0.00062509
Iteration 60/1000 | Loss: 0.00064785
Iteration 61/1000 | Loss: 0.00045438
Iteration 62/1000 | Loss: 0.00081067
Iteration 63/1000 | Loss: 0.00046321
Iteration 64/1000 | Loss: 0.00030900
Iteration 65/1000 | Loss: 0.00033564
Iteration 66/1000 | Loss: 0.00028080
Iteration 67/1000 | Loss: 0.00025404
Iteration 68/1000 | Loss: 0.00014592
Iteration 69/1000 | Loss: 0.00028888
Iteration 70/1000 | Loss: 0.00038539
Iteration 71/1000 | Loss: 0.00022169
Iteration 72/1000 | Loss: 0.00019811
Iteration 73/1000 | Loss: 0.00019568
Iteration 74/1000 | Loss: 0.00016021
Iteration 75/1000 | Loss: 0.00007385
Iteration 76/1000 | Loss: 0.00007775
Iteration 77/1000 | Loss: 0.00006058
Iteration 78/1000 | Loss: 0.00121800
Iteration 79/1000 | Loss: 0.00066884
Iteration 80/1000 | Loss: 0.00032329
Iteration 81/1000 | Loss: 0.00065012
Iteration 82/1000 | Loss: 0.00044068
Iteration 83/1000 | Loss: 0.00062455
Iteration 84/1000 | Loss: 0.00061541
Iteration 85/1000 | Loss: 0.00067437
Iteration 86/1000 | Loss: 0.00018160
Iteration 87/1000 | Loss: 0.00008460
Iteration 88/1000 | Loss: 0.00008710
Iteration 89/1000 | Loss: 0.00008003
Iteration 90/1000 | Loss: 0.00009091
Iteration 91/1000 | Loss: 0.00007480
Iteration 92/1000 | Loss: 0.00007891
Iteration 93/1000 | Loss: 0.00008481
Iteration 94/1000 | Loss: 0.00008820
Iteration 95/1000 | Loss: 0.00008211
Iteration 96/1000 | Loss: 0.00008151
Iteration 97/1000 | Loss: 0.00007014
Iteration 98/1000 | Loss: 0.00104790
Iteration 99/1000 | Loss: 0.00090864
Iteration 100/1000 | Loss: 0.00098823
Iteration 101/1000 | Loss: 0.00071759
Iteration 102/1000 | Loss: 0.00092672
Iteration 103/1000 | Loss: 0.00050071
Iteration 104/1000 | Loss: 0.00091942
Iteration 105/1000 | Loss: 0.00068486
Iteration 106/1000 | Loss: 0.00049508
Iteration 107/1000 | Loss: 0.00046996
Iteration 108/1000 | Loss: 0.00009448
Iteration 109/1000 | Loss: 0.00060318
Iteration 110/1000 | Loss: 0.00055847
Iteration 111/1000 | Loss: 0.00046959
Iteration 112/1000 | Loss: 0.00045273
Iteration 113/1000 | Loss: 0.00011792
Iteration 114/1000 | Loss: 0.00006596
Iteration 115/1000 | Loss: 0.00007975
Iteration 116/1000 | Loss: 0.00005889
Iteration 117/1000 | Loss: 0.00005900
Iteration 118/1000 | Loss: 0.00005681
Iteration 119/1000 | Loss: 0.00004631
Iteration 120/1000 | Loss: 0.00006244
Iteration 121/1000 | Loss: 0.00008838
Iteration 122/1000 | Loss: 0.00005662
Iteration 123/1000 | Loss: 0.00005739
Iteration 124/1000 | Loss: 0.00006403
Iteration 125/1000 | Loss: 0.00007003
Iteration 126/1000 | Loss: 0.00040888
Iteration 127/1000 | Loss: 0.00029574
Iteration 128/1000 | Loss: 0.00067711
Iteration 129/1000 | Loss: 0.00061623
Iteration 130/1000 | Loss: 0.00007219
Iteration 131/1000 | Loss: 0.00005681
Iteration 132/1000 | Loss: 0.00004613
Iteration 133/1000 | Loss: 0.00004144
Iteration 134/1000 | Loss: 0.00004543
Iteration 135/1000 | Loss: 0.00003913
Iteration 136/1000 | Loss: 0.00003490
Iteration 137/1000 | Loss: 0.00003314
Iteration 138/1000 | Loss: 0.00003202
Iteration 139/1000 | Loss: 0.00003104
Iteration 140/1000 | Loss: 0.00003037
Iteration 141/1000 | Loss: 0.00003001
Iteration 142/1000 | Loss: 0.00002971
Iteration 143/1000 | Loss: 0.00028750
Iteration 144/1000 | Loss: 0.00045329
Iteration 145/1000 | Loss: 0.00004786
Iteration 146/1000 | Loss: 0.00004176
Iteration 147/1000 | Loss: 0.00003799
Iteration 148/1000 | Loss: 0.00003538
Iteration 149/1000 | Loss: 0.00079074
Iteration 150/1000 | Loss: 0.00004119
Iteration 151/1000 | Loss: 0.00003514
Iteration 152/1000 | Loss: 0.00003270
Iteration 153/1000 | Loss: 0.00003146
Iteration 154/1000 | Loss: 0.00002991
Iteration 155/1000 | Loss: 0.00002894
Iteration 156/1000 | Loss: 0.00002845
Iteration 157/1000 | Loss: 0.00002817
Iteration 158/1000 | Loss: 0.00002781
Iteration 159/1000 | Loss: 0.00002767
Iteration 160/1000 | Loss: 0.00002742
Iteration 161/1000 | Loss: 0.00002725
Iteration 162/1000 | Loss: 0.00002717
Iteration 163/1000 | Loss: 0.00002713
Iteration 164/1000 | Loss: 0.00002711
Iteration 165/1000 | Loss: 0.00002708
Iteration 166/1000 | Loss: 0.00002707
Iteration 167/1000 | Loss: 0.00002701
Iteration 168/1000 | Loss: 0.00002701
Iteration 169/1000 | Loss: 0.00002701
Iteration 170/1000 | Loss: 0.00002701
Iteration 171/1000 | Loss: 0.00002700
Iteration 172/1000 | Loss: 0.00002700
Iteration 173/1000 | Loss: 0.00002699
Iteration 174/1000 | Loss: 0.00002698
Iteration 175/1000 | Loss: 0.00002698
Iteration 176/1000 | Loss: 0.00002698
Iteration 177/1000 | Loss: 0.00002698
Iteration 178/1000 | Loss: 0.00002697
Iteration 179/1000 | Loss: 0.00002697
Iteration 180/1000 | Loss: 0.00002697
Iteration 181/1000 | Loss: 0.00002697
Iteration 182/1000 | Loss: 0.00002697
Iteration 183/1000 | Loss: 0.00002697
Iteration 184/1000 | Loss: 0.00002697
Iteration 185/1000 | Loss: 0.00002696
Iteration 186/1000 | Loss: 0.00002696
Iteration 187/1000 | Loss: 0.00002696
Iteration 188/1000 | Loss: 0.00002696
Iteration 189/1000 | Loss: 0.00002696
Iteration 190/1000 | Loss: 0.00026547
Iteration 191/1000 | Loss: 0.00030587
Iteration 192/1000 | Loss: 0.00003345
Iteration 193/1000 | Loss: 0.00002786
Iteration 194/1000 | Loss: 0.00002703
Iteration 195/1000 | Loss: 0.00002691
Iteration 196/1000 | Loss: 0.00002690
Iteration 197/1000 | Loss: 0.00002690
Iteration 198/1000 | Loss: 0.00002689
Iteration 199/1000 | Loss: 0.00002687
Iteration 200/1000 | Loss: 0.00002687
Iteration 201/1000 | Loss: 0.00002687
Iteration 202/1000 | Loss: 0.00002687
Iteration 203/1000 | Loss: 0.00002687
Iteration 204/1000 | Loss: 0.00002687
Iteration 205/1000 | Loss: 0.00002687
Iteration 206/1000 | Loss: 0.00002687
Iteration 207/1000 | Loss: 0.00002687
Iteration 208/1000 | Loss: 0.00002687
Iteration 209/1000 | Loss: 0.00002686
Iteration 210/1000 | Loss: 0.00002686
Iteration 211/1000 | Loss: 0.00002686
Iteration 212/1000 | Loss: 0.00002686
Iteration 213/1000 | Loss: 0.00002686
Iteration 214/1000 | Loss: 0.00002685
Iteration 215/1000 | Loss: 0.00002685
Iteration 216/1000 | Loss: 0.00002685
Iteration 217/1000 | Loss: 0.00002685
Iteration 218/1000 | Loss: 0.00002685
Iteration 219/1000 | Loss: 0.00002685
Iteration 220/1000 | Loss: 0.00002685
Iteration 221/1000 | Loss: 0.00002684
Iteration 222/1000 | Loss: 0.00002684
Iteration 223/1000 | Loss: 0.00002684
Iteration 224/1000 | Loss: 0.00002684
Iteration 225/1000 | Loss: 0.00002684
Iteration 226/1000 | Loss: 0.00002684
Iteration 227/1000 | Loss: 0.00002684
Iteration 228/1000 | Loss: 0.00002684
Iteration 229/1000 | Loss: 0.00002684
Iteration 230/1000 | Loss: 0.00002684
Iteration 231/1000 | Loss: 0.00002684
Iteration 232/1000 | Loss: 0.00002684
Iteration 233/1000 | Loss: 0.00002684
Iteration 234/1000 | Loss: 0.00002684
Iteration 235/1000 | Loss: 0.00002684
Iteration 236/1000 | Loss: 0.00002684
Iteration 237/1000 | Loss: 0.00002684
Iteration 238/1000 | Loss: 0.00002684
Iteration 239/1000 | Loss: 0.00002684
Iteration 240/1000 | Loss: 0.00002684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [2.6836023607756943e-05, 2.6836023607756943e-05, 2.6836023607756943e-05, 2.6836023607756943e-05, 2.6836023607756943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6836023607756943e-05

Optimization complete. Final v2v error: 4.0670270919799805 mm

Highest mean error: 7.882512092590332 mm for frame 117

Lowest mean error: 2.9544308185577393 mm for frame 36

Saving results

Total time: 281.695166349411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966142
Iteration 2/25 | Loss: 0.00115606
Iteration 3/25 | Loss: 0.00098801
Iteration 4/25 | Loss: 0.00094889
Iteration 5/25 | Loss: 0.00093298
Iteration 6/25 | Loss: 0.00092965
Iteration 7/25 | Loss: 0.00092881
Iteration 8/25 | Loss: 0.00092877
Iteration 9/25 | Loss: 0.00092877
Iteration 10/25 | Loss: 0.00092877
Iteration 11/25 | Loss: 0.00092877
Iteration 12/25 | Loss: 0.00092877
Iteration 13/25 | Loss: 0.00092877
Iteration 14/25 | Loss: 0.00092877
Iteration 15/25 | Loss: 0.00092877
Iteration 16/25 | Loss: 0.00092877
Iteration 17/25 | Loss: 0.00092877
Iteration 18/25 | Loss: 0.00092877
Iteration 19/25 | Loss: 0.00092877
Iteration 20/25 | Loss: 0.00092877
Iteration 21/25 | Loss: 0.00092877
Iteration 22/25 | Loss: 0.00092877
Iteration 23/25 | Loss: 0.00092877
Iteration 24/25 | Loss: 0.00092877
Iteration 25/25 | Loss: 0.00092877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67214453
Iteration 2/25 | Loss: 0.00295341
Iteration 3/25 | Loss: 0.00295338
Iteration 4/25 | Loss: 0.00295338
Iteration 5/25 | Loss: 0.00295338
Iteration 6/25 | Loss: 0.00295338
Iteration 7/25 | Loss: 0.00295338
Iteration 8/25 | Loss: 0.00295338
Iteration 9/25 | Loss: 0.00295338
Iteration 10/25 | Loss: 0.00295338
Iteration 11/25 | Loss: 0.00295338
Iteration 12/25 | Loss: 0.00295338
Iteration 13/25 | Loss: 0.00295338
Iteration 14/25 | Loss: 0.00295338
Iteration 15/25 | Loss: 0.00295338
Iteration 16/25 | Loss: 0.00295338
Iteration 17/25 | Loss: 0.00295338
Iteration 18/25 | Loss: 0.00295338
Iteration 19/25 | Loss: 0.00295338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029533780179917812, 0.0029533780179917812, 0.0029533780179917812, 0.0029533780179917812, 0.0029533780179917812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029533780179917812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295338
Iteration 2/1000 | Loss: 0.00004414
Iteration 3/1000 | Loss: 0.00003204
Iteration 4/1000 | Loss: 0.00002856
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002417
Iteration 10/1000 | Loss: 0.00002387
Iteration 11/1000 | Loss: 0.00002365
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002344
Iteration 14/1000 | Loss: 0.00002342
Iteration 15/1000 | Loss: 0.00002327
Iteration 16/1000 | Loss: 0.00002319
Iteration 17/1000 | Loss: 0.00002315
Iteration 18/1000 | Loss: 0.00002315
Iteration 19/1000 | Loss: 0.00002315
Iteration 20/1000 | Loss: 0.00002314
Iteration 21/1000 | Loss: 0.00002314
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00002312
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002311
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002309
Iteration 30/1000 | Loss: 0.00002309
Iteration 31/1000 | Loss: 0.00002308
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002307
Iteration 34/1000 | Loss: 0.00002307
Iteration 35/1000 | Loss: 0.00002307
Iteration 36/1000 | Loss: 0.00002307
Iteration 37/1000 | Loss: 0.00002306
Iteration 38/1000 | Loss: 0.00002305
Iteration 39/1000 | Loss: 0.00002305
Iteration 40/1000 | Loss: 0.00002304
Iteration 41/1000 | Loss: 0.00002304
Iteration 42/1000 | Loss: 0.00002304
Iteration 43/1000 | Loss: 0.00002303
Iteration 44/1000 | Loss: 0.00002303
Iteration 45/1000 | Loss: 0.00002303
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002300
Iteration 50/1000 | Loss: 0.00002300
Iteration 51/1000 | Loss: 0.00002300
Iteration 52/1000 | Loss: 0.00002298
Iteration 53/1000 | Loss: 0.00002298
Iteration 54/1000 | Loss: 0.00002298
Iteration 55/1000 | Loss: 0.00002298
Iteration 56/1000 | Loss: 0.00002298
Iteration 57/1000 | Loss: 0.00002298
Iteration 58/1000 | Loss: 0.00002297
Iteration 59/1000 | Loss: 0.00002297
Iteration 60/1000 | Loss: 0.00002297
Iteration 61/1000 | Loss: 0.00002297
Iteration 62/1000 | Loss: 0.00002297
Iteration 63/1000 | Loss: 0.00002297
Iteration 64/1000 | Loss: 0.00002297
Iteration 65/1000 | Loss: 0.00002297
Iteration 66/1000 | Loss: 0.00002297
Iteration 67/1000 | Loss: 0.00002297
Iteration 68/1000 | Loss: 0.00002297
Iteration 69/1000 | Loss: 0.00002297
Iteration 70/1000 | Loss: 0.00002297
Iteration 71/1000 | Loss: 0.00002297
Iteration 72/1000 | Loss: 0.00002297
Iteration 73/1000 | Loss: 0.00002297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.296843013027683e-05, 2.296843013027683e-05, 2.296843013027683e-05, 2.296843013027683e-05, 2.296843013027683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.296843013027683e-05

Optimization complete. Final v2v error: 4.037841320037842 mm

Highest mean error: 5.704000949859619 mm for frame 69

Lowest mean error: 3.5015625953674316 mm for frame 128

Saving results

Total time: 35.77451205253601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860414
Iteration 2/25 | Loss: 0.00163104
Iteration 3/25 | Loss: 0.00111304
Iteration 4/25 | Loss: 0.00099375
Iteration 5/25 | Loss: 0.00100245
Iteration 6/25 | Loss: 0.00095744
Iteration 7/25 | Loss: 0.00094828
Iteration 8/25 | Loss: 0.00094438
Iteration 9/25 | Loss: 0.00094133
Iteration 10/25 | Loss: 0.00093602
Iteration 11/25 | Loss: 0.00093507
Iteration 12/25 | Loss: 0.00093343
Iteration 13/25 | Loss: 0.00093269
Iteration 14/25 | Loss: 0.00093249
Iteration 15/25 | Loss: 0.00093241
Iteration 16/25 | Loss: 0.00093241
Iteration 17/25 | Loss: 0.00093241
Iteration 18/25 | Loss: 0.00093241
Iteration 19/25 | Loss: 0.00093241
Iteration 20/25 | Loss: 0.00093240
Iteration 21/25 | Loss: 0.00093240
Iteration 22/25 | Loss: 0.00093240
Iteration 23/25 | Loss: 0.00093240
Iteration 24/25 | Loss: 0.00093240
Iteration 25/25 | Loss: 0.00093240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14058471
Iteration 2/25 | Loss: 0.00331802
Iteration 3/25 | Loss: 0.00312479
Iteration 4/25 | Loss: 0.00316219
Iteration 5/25 | Loss: 0.00316219
Iteration 6/25 | Loss: 0.00316219
Iteration 7/25 | Loss: 0.00316219
Iteration 8/25 | Loss: 0.00312479
Iteration 9/25 | Loss: 0.00312479
Iteration 10/25 | Loss: 0.00312479
Iteration 11/25 | Loss: 0.00312478
Iteration 12/25 | Loss: 0.00312478
Iteration 13/25 | Loss: 0.00312478
Iteration 14/25 | Loss: 0.00312478
Iteration 15/25 | Loss: 0.00312478
Iteration 16/25 | Loss: 0.00312478
Iteration 17/25 | Loss: 0.00312478
Iteration 18/25 | Loss: 0.00312478
Iteration 19/25 | Loss: 0.00312478
Iteration 20/25 | Loss: 0.00312478
Iteration 21/25 | Loss: 0.00312478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0031247837468981743, 0.0031247837468981743, 0.0031247837468981743, 0.0031247837468981743, 0.0031247837468981743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031247837468981743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312478
Iteration 2/1000 | Loss: 0.00013402
Iteration 3/1000 | Loss: 0.00019973
Iteration 4/1000 | Loss: 0.00005486
Iteration 5/1000 | Loss: 0.00002831
Iteration 6/1000 | Loss: 0.00006188
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00002633
Iteration 9/1000 | Loss: 0.00006551
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002509
Iteration 13/1000 | Loss: 0.00024152
Iteration 14/1000 | Loss: 0.00002511
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002485
Iteration 17/1000 | Loss: 0.00006208
Iteration 18/1000 | Loss: 0.00002569
Iteration 19/1000 | Loss: 0.00022136
Iteration 20/1000 | Loss: 0.00003985
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00010803
Iteration 23/1000 | Loss: 0.00002812
Iteration 24/1000 | Loss: 0.00007060
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002458
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002453
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002453
Iteration 39/1000 | Loss: 0.00002453
Iteration 40/1000 | Loss: 0.00002452
Iteration 41/1000 | Loss: 0.00002452
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002451
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002451
Iteration 52/1000 | Loss: 0.00002451
Iteration 53/1000 | Loss: 0.00002451
Iteration 54/1000 | Loss: 0.00002451
Iteration 55/1000 | Loss: 0.00002451
Iteration 56/1000 | Loss: 0.00002451
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002450
Iteration 61/1000 | Loss: 0.00002450
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002450
Iteration 65/1000 | Loss: 0.00002449
Iteration 66/1000 | Loss: 0.00011228
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002452
Iteration 70/1000 | Loss: 0.00002452
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002446
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002446
Iteration 78/1000 | Loss: 0.00002446
Iteration 79/1000 | Loss: 0.00002446
Iteration 80/1000 | Loss: 0.00002446
Iteration 81/1000 | Loss: 0.00002446
Iteration 82/1000 | Loss: 0.00002446
Iteration 83/1000 | Loss: 0.00002446
Iteration 84/1000 | Loss: 0.00002445
Iteration 85/1000 | Loss: 0.00002445
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002444
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002444
Iteration 92/1000 | Loss: 0.00002443
Iteration 93/1000 | Loss: 0.00002443
Iteration 94/1000 | Loss: 0.00002443
Iteration 95/1000 | Loss: 0.00002443
Iteration 96/1000 | Loss: 0.00002443
Iteration 97/1000 | Loss: 0.00002443
Iteration 98/1000 | Loss: 0.00002441
Iteration 99/1000 | Loss: 0.00002440
Iteration 100/1000 | Loss: 0.00002440
Iteration 101/1000 | Loss: 0.00006346
Iteration 102/1000 | Loss: 0.00003620
Iteration 103/1000 | Loss: 0.00002468
Iteration 104/1000 | Loss: 0.00003761
Iteration 105/1000 | Loss: 0.00005057
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002870
Iteration 108/1000 | Loss: 0.00002445
Iteration 109/1000 | Loss: 0.00009827
Iteration 110/1000 | Loss: 0.00003045
Iteration 111/1000 | Loss: 0.00002471
Iteration 112/1000 | Loss: 0.00003101
Iteration 113/1000 | Loss: 0.00004651
Iteration 114/1000 | Loss: 0.00002532
Iteration 115/1000 | Loss: 0.00002527
Iteration 116/1000 | Loss: 0.00002442
Iteration 117/1000 | Loss: 0.00002441
Iteration 118/1000 | Loss: 0.00002441
Iteration 119/1000 | Loss: 0.00002441
Iteration 120/1000 | Loss: 0.00002441
Iteration 121/1000 | Loss: 0.00002440
Iteration 122/1000 | Loss: 0.00002439
Iteration 123/1000 | Loss: 0.00002611
Iteration 124/1000 | Loss: 0.00002437
Iteration 125/1000 | Loss: 0.00002437
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002437
Iteration 130/1000 | Loss: 0.00002437
Iteration 131/1000 | Loss: 0.00002437
Iteration 132/1000 | Loss: 0.00002437
Iteration 133/1000 | Loss: 0.00002437
Iteration 134/1000 | Loss: 0.00002436
Iteration 135/1000 | Loss: 0.00002652
Iteration 136/1000 | Loss: 0.00004459
Iteration 137/1000 | Loss: 0.00002636
Iteration 138/1000 | Loss: 0.00008458
Iteration 139/1000 | Loss: 0.00017502
Iteration 140/1000 | Loss: 0.00018415
Iteration 141/1000 | Loss: 0.00002716
Iteration 142/1000 | Loss: 0.00005551
Iteration 143/1000 | Loss: 0.00002868
Iteration 144/1000 | Loss: 0.00007907
Iteration 145/1000 | Loss: 0.00002591
Iteration 146/1000 | Loss: 0.00002709
Iteration 147/1000 | Loss: 0.00002456
Iteration 148/1000 | Loss: 0.00002447
Iteration 149/1000 | Loss: 0.00002439
Iteration 150/1000 | Loss: 0.00002438
Iteration 151/1000 | Loss: 0.00002438
Iteration 152/1000 | Loss: 0.00002438
Iteration 153/1000 | Loss: 0.00002438
Iteration 154/1000 | Loss: 0.00002437
Iteration 155/1000 | Loss: 0.00002437
Iteration 156/1000 | Loss: 0.00002437
Iteration 157/1000 | Loss: 0.00002437
Iteration 158/1000 | Loss: 0.00002437
Iteration 159/1000 | Loss: 0.00002437
Iteration 160/1000 | Loss: 0.00009883
Iteration 161/1000 | Loss: 0.00003058
Iteration 162/1000 | Loss: 0.00004135
Iteration 163/1000 | Loss: 0.00002456
Iteration 164/1000 | Loss: 0.00005127
Iteration 165/1000 | Loss: 0.00005649
Iteration 166/1000 | Loss: 0.00002460
Iteration 167/1000 | Loss: 0.00003527
Iteration 168/1000 | Loss: 0.00002437
Iteration 169/1000 | Loss: 0.00002435
Iteration 170/1000 | Loss: 0.00002434
Iteration 171/1000 | Loss: 0.00002434
Iteration 172/1000 | Loss: 0.00002434
Iteration 173/1000 | Loss: 0.00002434
Iteration 174/1000 | Loss: 0.00002433
Iteration 175/1000 | Loss: 0.00002433
Iteration 176/1000 | Loss: 0.00002433
Iteration 177/1000 | Loss: 0.00002433
Iteration 178/1000 | Loss: 0.00002433
Iteration 179/1000 | Loss: 0.00002433
Iteration 180/1000 | Loss: 0.00002433
Iteration 181/1000 | Loss: 0.00002433
Iteration 182/1000 | Loss: 0.00002432
Iteration 183/1000 | Loss: 0.00002432
Iteration 184/1000 | Loss: 0.00002432
Iteration 185/1000 | Loss: 0.00002432
Iteration 186/1000 | Loss: 0.00002432
Iteration 187/1000 | Loss: 0.00002432
Iteration 188/1000 | Loss: 0.00002432
Iteration 189/1000 | Loss: 0.00002432
Iteration 190/1000 | Loss: 0.00002432
Iteration 191/1000 | Loss: 0.00002432
Iteration 192/1000 | Loss: 0.00002432
Iteration 193/1000 | Loss: 0.00002432
Iteration 194/1000 | Loss: 0.00002432
Iteration 195/1000 | Loss: 0.00002432
Iteration 196/1000 | Loss: 0.00002432
Iteration 197/1000 | Loss: 0.00002432
Iteration 198/1000 | Loss: 0.00002432
Iteration 199/1000 | Loss: 0.00002432
Iteration 200/1000 | Loss: 0.00002432
Iteration 201/1000 | Loss: 0.00002432
Iteration 202/1000 | Loss: 0.00002432
Iteration 203/1000 | Loss: 0.00002432
Iteration 204/1000 | Loss: 0.00002432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.4316621420439333e-05, 2.4316621420439333e-05, 2.4316621420439333e-05, 2.4316621420439333e-05, 2.4316621420439333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4316621420439333e-05

Optimization complete. Final v2v error: 4.1003522872924805 mm

Highest mean error: 4.526094436645508 mm for frame 76

Lowest mean error: 3.55012845993042 mm for frame 136

Saving results

Total time: 125.7668719291687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702339
Iteration 2/25 | Loss: 0.00102300
Iteration 3/25 | Loss: 0.00090664
Iteration 4/25 | Loss: 0.00089088
Iteration 5/25 | Loss: 0.00088668
Iteration 6/25 | Loss: 0.00088546
Iteration 7/25 | Loss: 0.00088515
Iteration 8/25 | Loss: 0.00088515
Iteration 9/25 | Loss: 0.00088515
Iteration 10/25 | Loss: 0.00088515
Iteration 11/25 | Loss: 0.00088515
Iteration 12/25 | Loss: 0.00088515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008851453312672675, 0.0008851453312672675, 0.0008851453312672675, 0.0008851453312672675, 0.0008851453312672675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008851453312672675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62207937
Iteration 2/25 | Loss: 0.00332309
Iteration 3/25 | Loss: 0.00332308
Iteration 4/25 | Loss: 0.00332308
Iteration 5/25 | Loss: 0.00332308
Iteration 6/25 | Loss: 0.00332308
Iteration 7/25 | Loss: 0.00332308
Iteration 8/25 | Loss: 0.00332308
Iteration 9/25 | Loss: 0.00332308
Iteration 10/25 | Loss: 0.00332308
Iteration 11/25 | Loss: 0.00332308
Iteration 12/25 | Loss: 0.00332308
Iteration 13/25 | Loss: 0.00332308
Iteration 14/25 | Loss: 0.00332308
Iteration 15/25 | Loss: 0.00332308
Iteration 16/25 | Loss: 0.00332308
Iteration 17/25 | Loss: 0.00332308
Iteration 18/25 | Loss: 0.00332308
Iteration 19/25 | Loss: 0.00332308
Iteration 20/25 | Loss: 0.00332308
Iteration 21/25 | Loss: 0.00332308
Iteration 22/25 | Loss: 0.00332308
Iteration 23/25 | Loss: 0.00332308
Iteration 24/25 | Loss: 0.00332308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0033230786211788654, 0.0033230786211788654, 0.0033230786211788654, 0.0033230786211788654, 0.0033230786211788654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033230786211788654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332308
Iteration 2/1000 | Loss: 0.00003799
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002122
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001977
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001967
Iteration 15/1000 | Loss: 0.00001966
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001936
Iteration 42/1000 | Loss: 0.00001936
Iteration 43/1000 | Loss: 0.00001936
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001935
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001934
Iteration 49/1000 | Loss: 0.00001934
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001932
Iteration 76/1000 | Loss: 0.00001932
Iteration 77/1000 | Loss: 0.00001932
Iteration 78/1000 | Loss: 0.00001932
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001932
Iteration 94/1000 | Loss: 0.00001932
Iteration 95/1000 | Loss: 0.00001932
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001932
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9322837033541873e-05, 1.9322837033541873e-05, 1.9322837033541873e-05, 1.9322837033541873e-05, 1.9322837033541873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9322837033541873e-05

Optimization complete. Final v2v error: 3.7663867473602295 mm

Highest mean error: 4.071521759033203 mm for frame 74

Lowest mean error: 3.208127498626709 mm for frame 1

Saving results

Total time: 30.581631898880005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446858
Iteration 2/25 | Loss: 0.00118883
Iteration 3/25 | Loss: 0.00091863
Iteration 4/25 | Loss: 0.00089691
Iteration 5/25 | Loss: 0.00089363
Iteration 6/25 | Loss: 0.00089240
Iteration 7/25 | Loss: 0.00089213
Iteration 8/25 | Loss: 0.00089213
Iteration 9/25 | Loss: 0.00089213
Iteration 10/25 | Loss: 0.00089213
Iteration 11/25 | Loss: 0.00089213
Iteration 12/25 | Loss: 0.00089213
Iteration 13/25 | Loss: 0.00089213
Iteration 14/25 | Loss: 0.00089213
Iteration 15/25 | Loss: 0.00089213
Iteration 16/25 | Loss: 0.00089213
Iteration 17/25 | Loss: 0.00089213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000892125244718045, 0.000892125244718045, 0.000892125244718045, 0.000892125244718045, 0.000892125244718045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000892125244718045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.61236095
Iteration 2/25 | Loss: 0.00311174
Iteration 3/25 | Loss: 0.00311174
Iteration 4/25 | Loss: 0.00311174
Iteration 5/25 | Loss: 0.00311174
Iteration 6/25 | Loss: 0.00311174
Iteration 7/25 | Loss: 0.00311174
Iteration 8/25 | Loss: 0.00311174
Iteration 9/25 | Loss: 0.00311174
Iteration 10/25 | Loss: 0.00311174
Iteration 11/25 | Loss: 0.00311174
Iteration 12/25 | Loss: 0.00311174
Iteration 13/25 | Loss: 0.00311174
Iteration 14/25 | Loss: 0.00311174
Iteration 15/25 | Loss: 0.00311174
Iteration 16/25 | Loss: 0.00311174
Iteration 17/25 | Loss: 0.00311174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031117352191358805, 0.0031117352191358805, 0.0031117352191358805, 0.0031117352191358805, 0.0031117352191358805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031117352191358805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311174
Iteration 2/1000 | Loss: 0.00003707
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00002162
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001894
Iteration 9/1000 | Loss: 0.00001860
Iteration 10/1000 | Loss: 0.00001851
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001819
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001816
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001814
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001812
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001809
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00001809
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001803
Iteration 75/1000 | Loss: 0.00001803
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001801
Iteration 89/1000 | Loss: 0.00001800
Iteration 90/1000 | Loss: 0.00001800
Iteration 91/1000 | Loss: 0.00001800
Iteration 92/1000 | Loss: 0.00001800
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001800
Iteration 99/1000 | Loss: 0.00001800
Iteration 100/1000 | Loss: 0.00001800
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001799
Iteration 103/1000 | Loss: 0.00001799
Iteration 104/1000 | Loss: 0.00001799
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001799
Iteration 113/1000 | Loss: 0.00001799
Iteration 114/1000 | Loss: 0.00001799
Iteration 115/1000 | Loss: 0.00001799
Iteration 116/1000 | Loss: 0.00001799
Iteration 117/1000 | Loss: 0.00001799
Iteration 118/1000 | Loss: 0.00001799
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001799
Iteration 122/1000 | Loss: 0.00001799
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001799
Iteration 125/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.798558332666289e-05, 1.798558332666289e-05, 1.798558332666289e-05, 1.798558332666289e-05, 1.798558332666289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.798558332666289e-05

Optimization complete. Final v2v error: 3.647289991378784 mm

Highest mean error: 4.074351787567139 mm for frame 68

Lowest mean error: 3.221808433532715 mm for frame 0

Saving results

Total time: 33.49569296836853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893401
Iteration 2/25 | Loss: 0.00143954
Iteration 3/25 | Loss: 0.00115245
Iteration 4/25 | Loss: 0.00109853
Iteration 5/25 | Loss: 0.00108419
Iteration 6/25 | Loss: 0.00108055
Iteration 7/25 | Loss: 0.00108296
Iteration 8/25 | Loss: 0.00108025
Iteration 9/25 | Loss: 0.00107940
Iteration 10/25 | Loss: 0.00108040
Iteration 11/25 | Loss: 0.00107851
Iteration 12/25 | Loss: 0.00107609
Iteration 13/25 | Loss: 0.00107516
Iteration 14/25 | Loss: 0.00107491
Iteration 15/25 | Loss: 0.00107479
Iteration 16/25 | Loss: 0.00107478
Iteration 17/25 | Loss: 0.00107478
Iteration 18/25 | Loss: 0.00107478
Iteration 19/25 | Loss: 0.00107478
Iteration 20/25 | Loss: 0.00107477
Iteration 21/25 | Loss: 0.00107477
Iteration 22/25 | Loss: 0.00107477
Iteration 23/25 | Loss: 0.00107477
Iteration 24/25 | Loss: 0.00107477
Iteration 25/25 | Loss: 0.00107477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28202248
Iteration 2/25 | Loss: 0.00070094
Iteration 3/25 | Loss: 0.00070093
Iteration 4/25 | Loss: 0.00070093
Iteration 5/25 | Loss: 0.00070093
Iteration 6/25 | Loss: 0.00070093
Iteration 7/25 | Loss: 0.00070093
Iteration 8/25 | Loss: 0.00070092
Iteration 9/25 | Loss: 0.00070092
Iteration 10/25 | Loss: 0.00070092
Iteration 11/25 | Loss: 0.00070092
Iteration 12/25 | Loss: 0.00070092
Iteration 13/25 | Loss: 0.00070092
Iteration 14/25 | Loss: 0.00070092
Iteration 15/25 | Loss: 0.00070092
Iteration 16/25 | Loss: 0.00070092
Iteration 17/25 | Loss: 0.00070092
Iteration 18/25 | Loss: 0.00070092
Iteration 19/25 | Loss: 0.00070092
Iteration 20/25 | Loss: 0.00070092
Iteration 21/25 | Loss: 0.00070092
Iteration 22/25 | Loss: 0.00070092
Iteration 23/25 | Loss: 0.00070092
Iteration 24/25 | Loss: 0.00070092
Iteration 25/25 | Loss: 0.00070092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070092
Iteration 2/1000 | Loss: 0.00069420
Iteration 3/1000 | Loss: 0.00028486
Iteration 4/1000 | Loss: 0.00065819
Iteration 5/1000 | Loss: 0.00036434
Iteration 6/1000 | Loss: 0.00006928
Iteration 7/1000 | Loss: 0.00020822
Iteration 8/1000 | Loss: 0.00006532
Iteration 9/1000 | Loss: 0.00005980
Iteration 10/1000 | Loss: 0.00052869
Iteration 11/1000 | Loss: 0.00006900
Iteration 12/1000 | Loss: 0.00021760
Iteration 13/1000 | Loss: 0.00006257
Iteration 14/1000 | Loss: 0.00006263
Iteration 15/1000 | Loss: 0.00006145
Iteration 16/1000 | Loss: 0.00006741
Iteration 17/1000 | Loss: 0.00005615
Iteration 18/1000 | Loss: 0.00004985
Iteration 19/1000 | Loss: 0.00004726
Iteration 20/1000 | Loss: 0.00004413
Iteration 21/1000 | Loss: 0.00004261
Iteration 22/1000 | Loss: 0.00004135
Iteration 23/1000 | Loss: 0.00004039
Iteration 24/1000 | Loss: 0.00003975
Iteration 25/1000 | Loss: 0.00003912
Iteration 26/1000 | Loss: 0.00003864
Iteration 27/1000 | Loss: 0.00003827
Iteration 28/1000 | Loss: 0.00003793
Iteration 29/1000 | Loss: 0.00003767
Iteration 30/1000 | Loss: 0.00003746
Iteration 31/1000 | Loss: 0.00003731
Iteration 32/1000 | Loss: 0.00003728
Iteration 33/1000 | Loss: 0.00003725
Iteration 34/1000 | Loss: 0.00003724
Iteration 35/1000 | Loss: 0.00003719
Iteration 36/1000 | Loss: 0.00003719
Iteration 37/1000 | Loss: 0.00003715
Iteration 38/1000 | Loss: 0.00003714
Iteration 39/1000 | Loss: 0.00003706
Iteration 40/1000 | Loss: 0.00003703
Iteration 41/1000 | Loss: 0.00003702
Iteration 42/1000 | Loss: 0.00003702
Iteration 43/1000 | Loss: 0.00003701
Iteration 44/1000 | Loss: 0.00003701
Iteration 45/1000 | Loss: 0.00003700
Iteration 46/1000 | Loss: 0.00003700
Iteration 47/1000 | Loss: 0.00003695
Iteration 48/1000 | Loss: 0.00003695
Iteration 49/1000 | Loss: 0.00003691
Iteration 50/1000 | Loss: 0.00055283
Iteration 51/1000 | Loss: 0.00222412
Iteration 52/1000 | Loss: 0.00209388
Iteration 53/1000 | Loss: 0.00006665
Iteration 54/1000 | Loss: 0.00004440
Iteration 55/1000 | Loss: 0.00003883
Iteration 56/1000 | Loss: 0.00003516
Iteration 57/1000 | Loss: 0.00003188
Iteration 58/1000 | Loss: 0.00003030
Iteration 59/1000 | Loss: 0.00002924
Iteration 60/1000 | Loss: 0.00002836
Iteration 61/1000 | Loss: 0.00002788
Iteration 62/1000 | Loss: 0.00002753
Iteration 63/1000 | Loss: 0.00002721
Iteration 64/1000 | Loss: 0.00002700
Iteration 65/1000 | Loss: 0.00002693
Iteration 66/1000 | Loss: 0.00002677
Iteration 67/1000 | Loss: 0.00002673
Iteration 68/1000 | Loss: 0.00002671
Iteration 69/1000 | Loss: 0.00002666
Iteration 70/1000 | Loss: 0.00002665
Iteration 71/1000 | Loss: 0.00002664
Iteration 72/1000 | Loss: 0.00002663
Iteration 73/1000 | Loss: 0.00002659
Iteration 74/1000 | Loss: 0.00002659
Iteration 75/1000 | Loss: 0.00002658
Iteration 76/1000 | Loss: 0.00002658
Iteration 77/1000 | Loss: 0.00002657
Iteration 78/1000 | Loss: 0.00002657
Iteration 79/1000 | Loss: 0.00002656
Iteration 80/1000 | Loss: 0.00002656
Iteration 81/1000 | Loss: 0.00002654
Iteration 82/1000 | Loss: 0.00002651
Iteration 83/1000 | Loss: 0.00002651
Iteration 84/1000 | Loss: 0.00002651
Iteration 85/1000 | Loss: 0.00002651
Iteration 86/1000 | Loss: 0.00002651
Iteration 87/1000 | Loss: 0.00002650
Iteration 88/1000 | Loss: 0.00002649
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002646
Iteration 97/1000 | Loss: 0.00002646
Iteration 98/1000 | Loss: 0.00002646
Iteration 99/1000 | Loss: 0.00002646
Iteration 100/1000 | Loss: 0.00002645
Iteration 101/1000 | Loss: 0.00002645
Iteration 102/1000 | Loss: 0.00002645
Iteration 103/1000 | Loss: 0.00002645
Iteration 104/1000 | Loss: 0.00002645
Iteration 105/1000 | Loss: 0.00002644
Iteration 106/1000 | Loss: 0.00002644
Iteration 107/1000 | Loss: 0.00002644
Iteration 108/1000 | Loss: 0.00002644
Iteration 109/1000 | Loss: 0.00002644
Iteration 110/1000 | Loss: 0.00002644
Iteration 111/1000 | Loss: 0.00002644
Iteration 112/1000 | Loss: 0.00002644
Iteration 113/1000 | Loss: 0.00002644
Iteration 114/1000 | Loss: 0.00002643
Iteration 115/1000 | Loss: 0.00002643
Iteration 116/1000 | Loss: 0.00002643
Iteration 117/1000 | Loss: 0.00002643
Iteration 118/1000 | Loss: 0.00002643
Iteration 119/1000 | Loss: 0.00002642
Iteration 120/1000 | Loss: 0.00002642
Iteration 121/1000 | Loss: 0.00002642
Iteration 122/1000 | Loss: 0.00002642
Iteration 123/1000 | Loss: 0.00002642
Iteration 124/1000 | Loss: 0.00002642
Iteration 125/1000 | Loss: 0.00002642
Iteration 126/1000 | Loss: 0.00002641
Iteration 127/1000 | Loss: 0.00002641
Iteration 128/1000 | Loss: 0.00002641
Iteration 129/1000 | Loss: 0.00002641
Iteration 130/1000 | Loss: 0.00002641
Iteration 131/1000 | Loss: 0.00002641
Iteration 132/1000 | Loss: 0.00002641
Iteration 133/1000 | Loss: 0.00002641
Iteration 134/1000 | Loss: 0.00002640
Iteration 135/1000 | Loss: 0.00002640
Iteration 136/1000 | Loss: 0.00002640
Iteration 137/1000 | Loss: 0.00002640
Iteration 138/1000 | Loss: 0.00002640
Iteration 139/1000 | Loss: 0.00002640
Iteration 140/1000 | Loss: 0.00002640
Iteration 141/1000 | Loss: 0.00002640
Iteration 142/1000 | Loss: 0.00002640
Iteration 143/1000 | Loss: 0.00002640
Iteration 144/1000 | Loss: 0.00002640
Iteration 145/1000 | Loss: 0.00002640
Iteration 146/1000 | Loss: 0.00002640
Iteration 147/1000 | Loss: 0.00002639
Iteration 148/1000 | Loss: 0.00002639
Iteration 149/1000 | Loss: 0.00002639
Iteration 150/1000 | Loss: 0.00002639
Iteration 151/1000 | Loss: 0.00002639
Iteration 152/1000 | Loss: 0.00002639
Iteration 153/1000 | Loss: 0.00002639
Iteration 154/1000 | Loss: 0.00002639
Iteration 155/1000 | Loss: 0.00002639
Iteration 156/1000 | Loss: 0.00002638
Iteration 157/1000 | Loss: 0.00002638
Iteration 158/1000 | Loss: 0.00002638
Iteration 159/1000 | Loss: 0.00002638
Iteration 160/1000 | Loss: 0.00002638
Iteration 161/1000 | Loss: 0.00002638
Iteration 162/1000 | Loss: 0.00002638
Iteration 163/1000 | Loss: 0.00002637
Iteration 164/1000 | Loss: 0.00002637
Iteration 165/1000 | Loss: 0.00002637
Iteration 166/1000 | Loss: 0.00002637
Iteration 167/1000 | Loss: 0.00002637
Iteration 168/1000 | Loss: 0.00002637
Iteration 169/1000 | Loss: 0.00002637
Iteration 170/1000 | Loss: 0.00002637
Iteration 171/1000 | Loss: 0.00002637
Iteration 172/1000 | Loss: 0.00002637
Iteration 173/1000 | Loss: 0.00002637
Iteration 174/1000 | Loss: 0.00002637
Iteration 175/1000 | Loss: 0.00002637
Iteration 176/1000 | Loss: 0.00002637
Iteration 177/1000 | Loss: 0.00002637
Iteration 178/1000 | Loss: 0.00002637
Iteration 179/1000 | Loss: 0.00002637
Iteration 180/1000 | Loss: 0.00002637
Iteration 181/1000 | Loss: 0.00002636
Iteration 182/1000 | Loss: 0.00002636
Iteration 183/1000 | Loss: 0.00002636
Iteration 184/1000 | Loss: 0.00002636
Iteration 185/1000 | Loss: 0.00002636
Iteration 186/1000 | Loss: 0.00002636
Iteration 187/1000 | Loss: 0.00002636
Iteration 188/1000 | Loss: 0.00002636
Iteration 189/1000 | Loss: 0.00002636
Iteration 190/1000 | Loss: 0.00002636
Iteration 191/1000 | Loss: 0.00002636
Iteration 192/1000 | Loss: 0.00002636
Iteration 193/1000 | Loss: 0.00002636
Iteration 194/1000 | Loss: 0.00002635
Iteration 195/1000 | Loss: 0.00002635
Iteration 196/1000 | Loss: 0.00002635
Iteration 197/1000 | Loss: 0.00002635
Iteration 198/1000 | Loss: 0.00002635
Iteration 199/1000 | Loss: 0.00002635
Iteration 200/1000 | Loss: 0.00002635
Iteration 201/1000 | Loss: 0.00002635
Iteration 202/1000 | Loss: 0.00002635
Iteration 203/1000 | Loss: 0.00002635
Iteration 204/1000 | Loss: 0.00002634
Iteration 205/1000 | Loss: 0.00002634
Iteration 206/1000 | Loss: 0.00002634
Iteration 207/1000 | Loss: 0.00002634
Iteration 208/1000 | Loss: 0.00002634
Iteration 209/1000 | Loss: 0.00002634
Iteration 210/1000 | Loss: 0.00002634
Iteration 211/1000 | Loss: 0.00002634
Iteration 212/1000 | Loss: 0.00002634
Iteration 213/1000 | Loss: 0.00002634
Iteration 214/1000 | Loss: 0.00002634
Iteration 215/1000 | Loss: 0.00002634
Iteration 216/1000 | Loss: 0.00002634
Iteration 217/1000 | Loss: 0.00002634
Iteration 218/1000 | Loss: 0.00002634
Iteration 219/1000 | Loss: 0.00002633
Iteration 220/1000 | Loss: 0.00002633
Iteration 221/1000 | Loss: 0.00002633
Iteration 222/1000 | Loss: 0.00002633
Iteration 223/1000 | Loss: 0.00002633
Iteration 224/1000 | Loss: 0.00002633
Iteration 225/1000 | Loss: 0.00002633
Iteration 226/1000 | Loss: 0.00002633
Iteration 227/1000 | Loss: 0.00002633
Iteration 228/1000 | Loss: 0.00002633
Iteration 229/1000 | Loss: 0.00002633
Iteration 230/1000 | Loss: 0.00002633
Iteration 231/1000 | Loss: 0.00002633
Iteration 232/1000 | Loss: 0.00002633
Iteration 233/1000 | Loss: 0.00002633
Iteration 234/1000 | Loss: 0.00002633
Iteration 235/1000 | Loss: 0.00002633
Iteration 236/1000 | Loss: 0.00002632
Iteration 237/1000 | Loss: 0.00002632
Iteration 238/1000 | Loss: 0.00002632
Iteration 239/1000 | Loss: 0.00002632
Iteration 240/1000 | Loss: 0.00002632
Iteration 241/1000 | Loss: 0.00002632
Iteration 242/1000 | Loss: 0.00002632
Iteration 243/1000 | Loss: 0.00002632
Iteration 244/1000 | Loss: 0.00002632
Iteration 245/1000 | Loss: 0.00002632
Iteration 246/1000 | Loss: 0.00002632
Iteration 247/1000 | Loss: 0.00002632
Iteration 248/1000 | Loss: 0.00002632
Iteration 249/1000 | Loss: 0.00002632
Iteration 250/1000 | Loss: 0.00002632
Iteration 251/1000 | Loss: 0.00002632
Iteration 252/1000 | Loss: 0.00002631
Iteration 253/1000 | Loss: 0.00002631
Iteration 254/1000 | Loss: 0.00002631
Iteration 255/1000 | Loss: 0.00002631
Iteration 256/1000 | Loss: 0.00002631
Iteration 257/1000 | Loss: 0.00002631
Iteration 258/1000 | Loss: 0.00002631
Iteration 259/1000 | Loss: 0.00002631
Iteration 260/1000 | Loss: 0.00002631
Iteration 261/1000 | Loss: 0.00002631
Iteration 262/1000 | Loss: 0.00002631
Iteration 263/1000 | Loss: 0.00002631
Iteration 264/1000 | Loss: 0.00002631
Iteration 265/1000 | Loss: 0.00002631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.6311832698411308e-05, 2.6311832698411308e-05, 2.6311832698411308e-05, 2.6311832698411308e-05, 2.6311832698411308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6311832698411308e-05

Optimization complete. Final v2v error: 4.05784797668457 mm

Highest mean error: 6.292237281799316 mm for frame 60

Lowest mean error: 3.6519899368286133 mm for frame 19

Saving results

Total time: 127.07064437866211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677912
Iteration 2/25 | Loss: 0.00110182
Iteration 3/25 | Loss: 0.00093355
Iteration 4/25 | Loss: 0.00092451
Iteration 5/25 | Loss: 0.00092223
Iteration 6/25 | Loss: 0.00092204
Iteration 7/25 | Loss: 0.00092204
Iteration 8/25 | Loss: 0.00092204
Iteration 9/25 | Loss: 0.00092204
Iteration 10/25 | Loss: 0.00092204
Iteration 11/25 | Loss: 0.00092204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009220385691151023, 0.0009220385691151023, 0.0009220385691151023, 0.0009220385691151023, 0.0009220385691151023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009220385691151023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07046914
Iteration 2/25 | Loss: 0.00048384
Iteration 3/25 | Loss: 0.00048381
Iteration 4/25 | Loss: 0.00048381
Iteration 5/25 | Loss: 0.00048381
Iteration 6/25 | Loss: 0.00048380
Iteration 7/25 | Loss: 0.00048380
Iteration 8/25 | Loss: 0.00048380
Iteration 9/25 | Loss: 0.00048380
Iteration 10/25 | Loss: 0.00048380
Iteration 11/25 | Loss: 0.00048380
Iteration 12/25 | Loss: 0.00048380
Iteration 13/25 | Loss: 0.00048380
Iteration 14/25 | Loss: 0.00048380
Iteration 15/25 | Loss: 0.00048380
Iteration 16/25 | Loss: 0.00048380
Iteration 17/25 | Loss: 0.00048380
Iteration 18/25 | Loss: 0.00048380
Iteration 19/25 | Loss: 0.00048380
Iteration 20/25 | Loss: 0.00048380
Iteration 21/25 | Loss: 0.00048380
Iteration 22/25 | Loss: 0.00048380
Iteration 23/25 | Loss: 0.00048380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004838032182306051, 0.0004838032182306051, 0.0004838032182306051, 0.0004838032182306051, 0.0004838032182306051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004838032182306051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048380
Iteration 2/1000 | Loss: 0.00002807
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001620
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001562
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001534
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001530
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001528
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001526
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001522
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001518
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001518
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001518
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001518
Iteration 91/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.5175959561020136e-05, 1.5175959561020136e-05, 1.5175959561020136e-05, 1.5175959561020136e-05, 1.5175959561020136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5175959561020136e-05

Optimization complete. Final v2v error: 3.210817813873291 mm

Highest mean error: 3.8383350372314453 mm for frame 59

Lowest mean error: 2.704901695251465 mm for frame 22

Saving results

Total time: 28.828716039657593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449218
Iteration 2/25 | Loss: 0.00096842
Iteration 3/25 | Loss: 0.00087567
Iteration 4/25 | Loss: 0.00086215
Iteration 5/25 | Loss: 0.00085808
Iteration 6/25 | Loss: 0.00085706
Iteration 7/25 | Loss: 0.00085704
Iteration 8/25 | Loss: 0.00085704
Iteration 9/25 | Loss: 0.00085704
Iteration 10/25 | Loss: 0.00085704
Iteration 11/25 | Loss: 0.00085704
Iteration 12/25 | Loss: 0.00085704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008570387726649642, 0.0008570387726649642, 0.0008570387726649642, 0.0008570387726649642, 0.0008570387726649642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008570387726649642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34441602
Iteration 2/25 | Loss: 0.00048585
Iteration 3/25 | Loss: 0.00048584
Iteration 4/25 | Loss: 0.00048584
Iteration 5/25 | Loss: 0.00048584
Iteration 6/25 | Loss: 0.00048584
Iteration 7/25 | Loss: 0.00048584
Iteration 8/25 | Loss: 0.00048584
Iteration 9/25 | Loss: 0.00048584
Iteration 10/25 | Loss: 0.00048584
Iteration 11/25 | Loss: 0.00048584
Iteration 12/25 | Loss: 0.00048584
Iteration 13/25 | Loss: 0.00048584
Iteration 14/25 | Loss: 0.00048584
Iteration 15/25 | Loss: 0.00048584
Iteration 16/25 | Loss: 0.00048584
Iteration 17/25 | Loss: 0.00048584
Iteration 18/25 | Loss: 0.00048584
Iteration 19/25 | Loss: 0.00048584
Iteration 20/25 | Loss: 0.00048584
Iteration 21/25 | Loss: 0.00048584
Iteration 22/25 | Loss: 0.00048584
Iteration 23/25 | Loss: 0.00048584
Iteration 24/25 | Loss: 0.00048584
Iteration 25/25 | Loss: 0.00048584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048584
Iteration 2/1000 | Loss: 0.00002405
Iteration 3/1000 | Loss: 0.00001936
Iteration 4/1000 | Loss: 0.00001740
Iteration 5/1000 | Loss: 0.00001639
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001499
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001447
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001444
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001443
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001430
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001426
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001421
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001417
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001416
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001416
Iteration 68/1000 | Loss: 0.00001416
Iteration 69/1000 | Loss: 0.00001416
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001416
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001414
Iteration 100/1000 | Loss: 0.00001414
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001413
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001413
Iteration 121/1000 | Loss: 0.00001413
Iteration 122/1000 | Loss: 0.00001413
Iteration 123/1000 | Loss: 0.00001413
Iteration 124/1000 | Loss: 0.00001413
Iteration 125/1000 | Loss: 0.00001413
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001412
Iteration 135/1000 | Loss: 0.00001412
Iteration 136/1000 | Loss: 0.00001412
Iteration 137/1000 | Loss: 0.00001412
Iteration 138/1000 | Loss: 0.00001412
Iteration 139/1000 | Loss: 0.00001412
Iteration 140/1000 | Loss: 0.00001412
Iteration 141/1000 | Loss: 0.00001412
Iteration 142/1000 | Loss: 0.00001412
Iteration 143/1000 | Loss: 0.00001412
Iteration 144/1000 | Loss: 0.00001412
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.411920220562024e-05, 1.411920220562024e-05, 1.411920220562024e-05, 1.411920220562024e-05, 1.411920220562024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.411920220562024e-05

Optimization complete. Final v2v error: 3.0962436199188232 mm

Highest mean error: 3.6373093128204346 mm for frame 135

Lowest mean error: 2.75216007232666 mm for frame 48

Saving results

Total time: 31.480695486068726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818946
Iteration 2/25 | Loss: 0.00130811
Iteration 3/25 | Loss: 0.00102722
Iteration 4/25 | Loss: 0.00097452
Iteration 5/25 | Loss: 0.00096496
Iteration 6/25 | Loss: 0.00096269
Iteration 7/25 | Loss: 0.00096260
Iteration 8/25 | Loss: 0.00096260
Iteration 9/25 | Loss: 0.00096260
Iteration 10/25 | Loss: 0.00096260
Iteration 11/25 | Loss: 0.00096260
Iteration 12/25 | Loss: 0.00096260
Iteration 13/25 | Loss: 0.00096260
Iteration 14/25 | Loss: 0.00096260
Iteration 15/25 | Loss: 0.00096260
Iteration 16/25 | Loss: 0.00096260
Iteration 17/25 | Loss: 0.00096260
Iteration 18/25 | Loss: 0.00096260
Iteration 19/25 | Loss: 0.00096260
Iteration 20/25 | Loss: 0.00096260
Iteration 21/25 | Loss: 0.00096260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009626008686609566, 0.0009626008686609566, 0.0009626008686609566, 0.0009626008686609566, 0.0009626008686609566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009626008686609566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34435928
Iteration 2/25 | Loss: 0.00056956
Iteration 3/25 | Loss: 0.00056955
Iteration 4/25 | Loss: 0.00056955
Iteration 5/25 | Loss: 0.00056955
Iteration 6/25 | Loss: 0.00056955
Iteration 7/25 | Loss: 0.00056955
Iteration 8/25 | Loss: 0.00056955
Iteration 9/25 | Loss: 0.00056955
Iteration 10/25 | Loss: 0.00056955
Iteration 11/25 | Loss: 0.00056955
Iteration 12/25 | Loss: 0.00056955
Iteration 13/25 | Loss: 0.00056955
Iteration 14/25 | Loss: 0.00056955
Iteration 15/25 | Loss: 0.00056955
Iteration 16/25 | Loss: 0.00056955
Iteration 17/25 | Loss: 0.00056955
Iteration 18/25 | Loss: 0.00056955
Iteration 19/25 | Loss: 0.00056955
Iteration 20/25 | Loss: 0.00056955
Iteration 21/25 | Loss: 0.00056955
Iteration 22/25 | Loss: 0.00056955
Iteration 23/25 | Loss: 0.00056955
Iteration 24/25 | Loss: 0.00056955
Iteration 25/25 | Loss: 0.00056955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056955
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001541
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001431
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001427
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001426
Iteration 17/1000 | Loss: 0.00001425
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001412
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001410
Iteration 46/1000 | Loss: 0.00001409
Iteration 47/1000 | Loss: 0.00001409
Iteration 48/1000 | Loss: 0.00001408
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001408
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001405
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001404
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001397
Iteration 102/1000 | Loss: 0.00001397
Iteration 103/1000 | Loss: 0.00001397
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.3972858141642064e-05, 1.3972858141642064e-05, 1.3972858141642064e-05, 1.3972858141642064e-05, 1.3972858141642064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3972858141642064e-05

Optimization complete. Final v2v error: 3.1761631965637207 mm

Highest mean error: 3.7090249061584473 mm for frame 185

Lowest mean error: 2.816650867462158 mm for frame 85

Saving results

Total time: 37.269381284713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014672
Iteration 2/25 | Loss: 0.00355203
Iteration 3/25 | Loss: 0.00210942
Iteration 4/25 | Loss: 0.00177953
Iteration 5/25 | Loss: 0.00161072
Iteration 6/25 | Loss: 0.00120128
Iteration 7/25 | Loss: 0.00101719
Iteration 8/25 | Loss: 0.00097720
Iteration 9/25 | Loss: 0.00096142
Iteration 10/25 | Loss: 0.00095728
Iteration 11/25 | Loss: 0.00095694
Iteration 12/25 | Loss: 0.00095309
Iteration 13/25 | Loss: 0.00095418
Iteration 14/25 | Loss: 0.00094035
Iteration 15/25 | Loss: 0.00093581
Iteration 16/25 | Loss: 0.00093451
Iteration 17/25 | Loss: 0.00093407
Iteration 18/25 | Loss: 0.00093391
Iteration 19/25 | Loss: 0.00093381
Iteration 20/25 | Loss: 0.00093380
Iteration 21/25 | Loss: 0.00093380
Iteration 22/25 | Loss: 0.00093379
Iteration 23/25 | Loss: 0.00093379
Iteration 24/25 | Loss: 0.00093379
Iteration 25/25 | Loss: 0.00093379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35688198
Iteration 2/25 | Loss: 0.00023211
Iteration 3/25 | Loss: 0.00023211
Iteration 4/25 | Loss: 0.00023211
Iteration 5/25 | Loss: 0.00023211
Iteration 6/25 | Loss: 0.00023211
Iteration 7/25 | Loss: 0.00023211
Iteration 8/25 | Loss: 0.00023211
Iteration 9/25 | Loss: 0.00023211
Iteration 10/25 | Loss: 0.00023211
Iteration 11/25 | Loss: 0.00023211
Iteration 12/25 | Loss: 0.00023211
Iteration 13/25 | Loss: 0.00023211
Iteration 14/25 | Loss: 0.00023211
Iteration 15/25 | Loss: 0.00023211
Iteration 16/25 | Loss: 0.00023211
Iteration 17/25 | Loss: 0.00023211
Iteration 18/25 | Loss: 0.00023211
Iteration 19/25 | Loss: 0.00023211
Iteration 20/25 | Loss: 0.00023211
Iteration 21/25 | Loss: 0.00023211
Iteration 22/25 | Loss: 0.00023211
Iteration 23/25 | Loss: 0.00023211
Iteration 24/25 | Loss: 0.00023211
Iteration 25/25 | Loss: 0.00023211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023211
Iteration 2/1000 | Loss: 0.00003077
Iteration 3/1000 | Loss: 0.00002299
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001553
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001536
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001528
Iteration 78/1000 | Loss: 0.00001528
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001528
Iteration 81/1000 | Loss: 0.00001528
Iteration 82/1000 | Loss: 0.00001528
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001527
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001526
Iteration 99/1000 | Loss: 0.00001526
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001524
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001524
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001524
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001524
Iteration 137/1000 | Loss: 0.00001524
Iteration 138/1000 | Loss: 0.00001524
Iteration 139/1000 | Loss: 0.00001524
Iteration 140/1000 | Loss: 0.00001524
Iteration 141/1000 | Loss: 0.00001524
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001523
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001523
Iteration 147/1000 | Loss: 0.00001523
Iteration 148/1000 | Loss: 0.00001523
Iteration 149/1000 | Loss: 0.00001523
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001523
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001523
Iteration 158/1000 | Loss: 0.00001523
Iteration 159/1000 | Loss: 0.00001522
Iteration 160/1000 | Loss: 0.00001522
Iteration 161/1000 | Loss: 0.00001522
Iteration 162/1000 | Loss: 0.00001522
Iteration 163/1000 | Loss: 0.00001522
Iteration 164/1000 | Loss: 0.00001522
Iteration 165/1000 | Loss: 0.00001522
Iteration 166/1000 | Loss: 0.00001522
Iteration 167/1000 | Loss: 0.00001522
Iteration 168/1000 | Loss: 0.00001522
Iteration 169/1000 | Loss: 0.00001522
Iteration 170/1000 | Loss: 0.00001522
Iteration 171/1000 | Loss: 0.00001522
Iteration 172/1000 | Loss: 0.00001522
Iteration 173/1000 | Loss: 0.00001522
Iteration 174/1000 | Loss: 0.00001522
Iteration 175/1000 | Loss: 0.00001522
Iteration 176/1000 | Loss: 0.00001522
Iteration 177/1000 | Loss: 0.00001522
Iteration 178/1000 | Loss: 0.00001522
Iteration 179/1000 | Loss: 0.00001522
Iteration 180/1000 | Loss: 0.00001522
Iteration 181/1000 | Loss: 0.00001522
Iteration 182/1000 | Loss: 0.00001522
Iteration 183/1000 | Loss: 0.00001522
Iteration 184/1000 | Loss: 0.00001522
Iteration 185/1000 | Loss: 0.00001522
Iteration 186/1000 | Loss: 0.00001522
Iteration 187/1000 | Loss: 0.00001522
Iteration 188/1000 | Loss: 0.00001522
Iteration 189/1000 | Loss: 0.00001522
Iteration 190/1000 | Loss: 0.00001522
Iteration 191/1000 | Loss: 0.00001522
Iteration 192/1000 | Loss: 0.00001522
Iteration 193/1000 | Loss: 0.00001522
Iteration 194/1000 | Loss: 0.00001522
Iteration 195/1000 | Loss: 0.00001522
Iteration 196/1000 | Loss: 0.00001522
Iteration 197/1000 | Loss: 0.00001522
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001522
Iteration 208/1000 | Loss: 0.00001522
Iteration 209/1000 | Loss: 0.00001522
Iteration 210/1000 | Loss: 0.00001522
Iteration 211/1000 | Loss: 0.00001522
Iteration 212/1000 | Loss: 0.00001522
Iteration 213/1000 | Loss: 0.00001522
Iteration 214/1000 | Loss: 0.00001522
Iteration 215/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.5215924577205442e-05, 1.5215924577205442e-05, 1.5215924577205442e-05, 1.5215924577205442e-05, 1.5215924577205442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5215924577205442e-05

Optimization complete. Final v2v error: 3.3064146041870117 mm

Highest mean error: 3.586801528930664 mm for frame 0

Lowest mean error: 3.134592294692993 mm for frame 121

Saving results

Total time: 57.08259129524231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408406
Iteration 2/25 | Loss: 0.00112547
Iteration 3/25 | Loss: 0.00092714
Iteration 4/25 | Loss: 0.00090631
Iteration 5/25 | Loss: 0.00089906
Iteration 6/25 | Loss: 0.00089715
Iteration 7/25 | Loss: 0.00089713
Iteration 8/25 | Loss: 0.00089713
Iteration 9/25 | Loss: 0.00089713
Iteration 10/25 | Loss: 0.00089713
Iteration 11/25 | Loss: 0.00089713
Iteration 12/25 | Loss: 0.00089713
Iteration 13/25 | Loss: 0.00089713
Iteration 14/25 | Loss: 0.00089713
Iteration 15/25 | Loss: 0.00089713
Iteration 16/25 | Loss: 0.00089713
Iteration 17/25 | Loss: 0.00089713
Iteration 18/25 | Loss: 0.00089713
Iteration 19/25 | Loss: 0.00089713
Iteration 20/25 | Loss: 0.00089713
Iteration 21/25 | Loss: 0.00089713
Iteration 22/25 | Loss: 0.00089713
Iteration 23/25 | Loss: 0.00089713
Iteration 24/25 | Loss: 0.00089713
Iteration 25/25 | Loss: 0.00089713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008971330244094133, 0.0008971330244094133, 0.0008971330244094133, 0.0008971330244094133, 0.0008971330244094133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008971330244094133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42261326
Iteration 2/25 | Loss: 0.00079092
Iteration 3/25 | Loss: 0.00079092
Iteration 4/25 | Loss: 0.00079092
Iteration 5/25 | Loss: 0.00079092
Iteration 6/25 | Loss: 0.00079092
Iteration 7/25 | Loss: 0.00079092
Iteration 8/25 | Loss: 0.00079092
Iteration 9/25 | Loss: 0.00079092
Iteration 10/25 | Loss: 0.00079092
Iteration 11/25 | Loss: 0.00079092
Iteration 12/25 | Loss: 0.00079092
Iteration 13/25 | Loss: 0.00079092
Iteration 14/25 | Loss: 0.00079092
Iteration 15/25 | Loss: 0.00079092
Iteration 16/25 | Loss: 0.00079092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000790920399595052, 0.000790920399595052, 0.000790920399595052, 0.000790920399595052, 0.000790920399595052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000790920399595052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079092
Iteration 2/1000 | Loss: 0.00002940
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001970
Iteration 6/1000 | Loss: 0.00001922
Iteration 7/1000 | Loss: 0.00001880
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001820
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001819
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001817
Iteration 34/1000 | Loss: 0.00001817
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001816
Iteration 37/1000 | Loss: 0.00001816
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001815
Iteration 41/1000 | Loss: 0.00001815
Iteration 42/1000 | Loss: 0.00001814
Iteration 43/1000 | Loss: 0.00001814
Iteration 44/1000 | Loss: 0.00001814
Iteration 45/1000 | Loss: 0.00001814
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001813
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001811
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001811
Iteration 62/1000 | Loss: 0.00001810
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001806
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001799
Iteration 74/1000 | Loss: 0.00001799
Iteration 75/1000 | Loss: 0.00001798
Iteration 76/1000 | Loss: 0.00001798
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001798
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001798
Iteration 81/1000 | Loss: 0.00001797
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001796
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001795
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001794
Iteration 100/1000 | Loss: 0.00001794
Iteration 101/1000 | Loss: 0.00001794
Iteration 102/1000 | Loss: 0.00001794
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001791
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001790
Iteration 111/1000 | Loss: 0.00001790
Iteration 112/1000 | Loss: 0.00001790
Iteration 113/1000 | Loss: 0.00001790
Iteration 114/1000 | Loss: 0.00001790
Iteration 115/1000 | Loss: 0.00001790
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00001788
Iteration 118/1000 | Loss: 0.00001788
Iteration 119/1000 | Loss: 0.00001787
Iteration 120/1000 | Loss: 0.00001787
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Iteration 123/1000 | Loss: 0.00001786
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001785
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001783
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001779
Iteration 141/1000 | Loss: 0.00001779
Iteration 142/1000 | Loss: 0.00001779
Iteration 143/1000 | Loss: 0.00001778
Iteration 144/1000 | Loss: 0.00001778
Iteration 145/1000 | Loss: 0.00001778
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001777
Iteration 148/1000 | Loss: 0.00001777
Iteration 149/1000 | Loss: 0.00001777
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001777
Iteration 153/1000 | Loss: 0.00001777
Iteration 154/1000 | Loss: 0.00001777
Iteration 155/1000 | Loss: 0.00001777
Iteration 156/1000 | Loss: 0.00001777
Iteration 157/1000 | Loss: 0.00001777
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00001777
Iteration 163/1000 | Loss: 0.00001777
Iteration 164/1000 | Loss: 0.00001777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7771662896848284e-05, 1.7771662896848284e-05, 1.7771662896848284e-05, 1.7771662896848284e-05, 1.7771662896848284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7771662896848284e-05

Optimization complete. Final v2v error: 3.4375505447387695 mm

Highest mean error: 4.095175743103027 mm for frame 136

Lowest mean error: 2.726799726486206 mm for frame 12

Saving results

Total time: 39.52488708496094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797390
Iteration 2/25 | Loss: 0.00132662
Iteration 3/25 | Loss: 0.00107009
Iteration 4/25 | Loss: 0.00101756
Iteration 5/25 | Loss: 0.00099531
Iteration 6/25 | Loss: 0.00099808
Iteration 7/25 | Loss: 0.00099051
Iteration 8/25 | Loss: 0.00097475
Iteration 9/25 | Loss: 0.00097039
Iteration 10/25 | Loss: 0.00096981
Iteration 11/25 | Loss: 0.00095663
Iteration 12/25 | Loss: 0.00095699
Iteration 13/25 | Loss: 0.00095574
Iteration 14/25 | Loss: 0.00095508
Iteration 15/25 | Loss: 0.00095507
Iteration 16/25 | Loss: 0.00095507
Iteration 17/25 | Loss: 0.00095507
Iteration 18/25 | Loss: 0.00095507
Iteration 19/25 | Loss: 0.00095615
Iteration 20/25 | Loss: 0.00095498
Iteration 21/25 | Loss: 0.00095497
Iteration 22/25 | Loss: 0.00095497
Iteration 23/25 | Loss: 0.00095497
Iteration 24/25 | Loss: 0.00095497
Iteration 25/25 | Loss: 0.00095497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79247093
Iteration 2/25 | Loss: 0.00063297
Iteration 3/25 | Loss: 0.00063296
Iteration 4/25 | Loss: 0.00063296
Iteration 5/25 | Loss: 0.00063296
Iteration 6/25 | Loss: 0.00063296
Iteration 7/25 | Loss: 0.00063296
Iteration 8/25 | Loss: 0.00063296
Iteration 9/25 | Loss: 0.00063296
Iteration 10/25 | Loss: 0.00063296
Iteration 11/25 | Loss: 0.00063296
Iteration 12/25 | Loss: 0.00063296
Iteration 13/25 | Loss: 0.00063296
Iteration 14/25 | Loss: 0.00063296
Iteration 15/25 | Loss: 0.00063296
Iteration 16/25 | Loss: 0.00063296
Iteration 17/25 | Loss: 0.00063296
Iteration 18/25 | Loss: 0.00063296
Iteration 19/25 | Loss: 0.00063296
Iteration 20/25 | Loss: 0.00063296
Iteration 21/25 | Loss: 0.00063296
Iteration 22/25 | Loss: 0.00063296
Iteration 23/25 | Loss: 0.00063296
Iteration 24/25 | Loss: 0.00063296
Iteration 25/25 | Loss: 0.00063296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063296
Iteration 2/1000 | Loss: 0.00004018
Iteration 3/1000 | Loss: 0.00002644
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00064310
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001790
Iteration 13/1000 | Loss: 0.00001723
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001652
Iteration 19/1000 | Loss: 0.00001649
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001637
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001619
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001603
Iteration 69/1000 | Loss: 0.00001603
Iteration 70/1000 | Loss: 0.00001603
Iteration 71/1000 | Loss: 0.00001603
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00001602
Iteration 74/1000 | Loss: 0.00001602
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001601
Iteration 77/1000 | Loss: 0.00001601
Iteration 78/1000 | Loss: 0.00001601
Iteration 79/1000 | Loss: 0.00001601
Iteration 80/1000 | Loss: 0.00001601
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001600
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001600
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Iteration 89/1000 | Loss: 0.00001600
Iteration 90/1000 | Loss: 0.00001600
Iteration 91/1000 | Loss: 0.00001600
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001599
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001598
Iteration 106/1000 | Loss: 0.00001598
Iteration 107/1000 | Loss: 0.00001598
Iteration 108/1000 | Loss: 0.00001598
Iteration 109/1000 | Loss: 0.00001598
Iteration 110/1000 | Loss: 0.00001598
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001597
Iteration 113/1000 | Loss: 0.00001597
Iteration 114/1000 | Loss: 0.00001597
Iteration 115/1000 | Loss: 0.00001597
Iteration 116/1000 | Loss: 0.00001597
Iteration 117/1000 | Loss: 0.00001597
Iteration 118/1000 | Loss: 0.00001597
Iteration 119/1000 | Loss: 0.00001597
Iteration 120/1000 | Loss: 0.00001597
Iteration 121/1000 | Loss: 0.00001597
Iteration 122/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.5972449546097778e-05, 1.5972449546097778e-05, 1.5972449546097778e-05, 1.5972449546097778e-05, 1.5972449546097778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5972449546097778e-05

Optimization complete. Final v2v error: 3.3174006938934326 mm

Highest mean error: 4.094709396362305 mm for frame 115

Lowest mean error: 2.7574474811553955 mm for frame 131

Saving results

Total time: 59.14985775947571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842608
Iteration 2/25 | Loss: 0.00131689
Iteration 3/25 | Loss: 0.00105813
Iteration 4/25 | Loss: 0.00102663
Iteration 5/25 | Loss: 0.00101812
Iteration 6/25 | Loss: 0.00101641
Iteration 7/25 | Loss: 0.00101641
Iteration 8/25 | Loss: 0.00101641
Iteration 9/25 | Loss: 0.00101641
Iteration 10/25 | Loss: 0.00101641
Iteration 11/25 | Loss: 0.00101641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010164071572944522, 0.0010164071572944522, 0.0010164071572944522, 0.0010164071572944522, 0.0010164071572944522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010164071572944522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07462132
Iteration 2/25 | Loss: 0.00061827
Iteration 3/25 | Loss: 0.00061827
Iteration 4/25 | Loss: 0.00061827
Iteration 5/25 | Loss: 0.00061827
Iteration 6/25 | Loss: 0.00061827
Iteration 7/25 | Loss: 0.00061827
Iteration 8/25 | Loss: 0.00061827
Iteration 9/25 | Loss: 0.00061827
Iteration 10/25 | Loss: 0.00061827
Iteration 11/25 | Loss: 0.00061827
Iteration 12/25 | Loss: 0.00061827
Iteration 13/25 | Loss: 0.00061827
Iteration 14/25 | Loss: 0.00061827
Iteration 15/25 | Loss: 0.00061827
Iteration 16/25 | Loss: 0.00061827
Iteration 17/25 | Loss: 0.00061827
Iteration 18/25 | Loss: 0.00061827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006182687939144671, 0.0006182687939144671, 0.0006182687939144671, 0.0006182687939144671, 0.0006182687939144671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006182687939144671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061827
Iteration 2/1000 | Loss: 0.00006977
Iteration 3/1000 | Loss: 0.00004613
Iteration 4/1000 | Loss: 0.00003727
Iteration 5/1000 | Loss: 0.00003549
Iteration 6/1000 | Loss: 0.00003417
Iteration 7/1000 | Loss: 0.00003304
Iteration 8/1000 | Loss: 0.00003234
Iteration 9/1000 | Loss: 0.00003167
Iteration 10/1000 | Loss: 0.00003106
Iteration 11/1000 | Loss: 0.00003071
Iteration 12/1000 | Loss: 0.00003036
Iteration 13/1000 | Loss: 0.00003005
Iteration 14/1000 | Loss: 0.00002980
Iteration 15/1000 | Loss: 0.00002970
Iteration 16/1000 | Loss: 0.00002951
Iteration 17/1000 | Loss: 0.00002936
Iteration 18/1000 | Loss: 0.00002928
Iteration 19/1000 | Loss: 0.00002927
Iteration 20/1000 | Loss: 0.00002927
Iteration 21/1000 | Loss: 0.00002925
Iteration 22/1000 | Loss: 0.00002923
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002897
Iteration 26/1000 | Loss: 0.00002885
Iteration 27/1000 | Loss: 0.00002885
Iteration 28/1000 | Loss: 0.00002885
Iteration 29/1000 | Loss: 0.00002884
Iteration 30/1000 | Loss: 0.00002884
Iteration 31/1000 | Loss: 0.00002884
Iteration 32/1000 | Loss: 0.00002884
Iteration 33/1000 | Loss: 0.00002884
Iteration 34/1000 | Loss: 0.00002884
Iteration 35/1000 | Loss: 0.00002884
Iteration 36/1000 | Loss: 0.00002882
Iteration 37/1000 | Loss: 0.00002882
Iteration 38/1000 | Loss: 0.00002879
Iteration 39/1000 | Loss: 0.00002878
Iteration 40/1000 | Loss: 0.00002878
Iteration 41/1000 | Loss: 0.00002874
Iteration 42/1000 | Loss: 0.00002873
Iteration 43/1000 | Loss: 0.00002873
Iteration 44/1000 | Loss: 0.00002872
Iteration 45/1000 | Loss: 0.00002872
Iteration 46/1000 | Loss: 0.00002872
Iteration 47/1000 | Loss: 0.00002871
Iteration 48/1000 | Loss: 0.00002869
Iteration 49/1000 | Loss: 0.00002868
Iteration 50/1000 | Loss: 0.00002868
Iteration 51/1000 | Loss: 0.00002868
Iteration 52/1000 | Loss: 0.00002867
Iteration 53/1000 | Loss: 0.00002867
Iteration 54/1000 | Loss: 0.00002867
Iteration 55/1000 | Loss: 0.00002867
Iteration 56/1000 | Loss: 0.00002867
Iteration 57/1000 | Loss: 0.00002867
Iteration 58/1000 | Loss: 0.00002867
Iteration 59/1000 | Loss: 0.00002867
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002866
Iteration 62/1000 | Loss: 0.00002866
Iteration 63/1000 | Loss: 0.00002866
Iteration 64/1000 | Loss: 0.00002866
Iteration 65/1000 | Loss: 0.00002866
Iteration 66/1000 | Loss: 0.00002865
Iteration 67/1000 | Loss: 0.00002865
Iteration 68/1000 | Loss: 0.00002865
Iteration 69/1000 | Loss: 0.00002865
Iteration 70/1000 | Loss: 0.00002865
Iteration 71/1000 | Loss: 0.00002865
Iteration 72/1000 | Loss: 0.00002864
Iteration 73/1000 | Loss: 0.00002864
Iteration 74/1000 | Loss: 0.00002864
Iteration 75/1000 | Loss: 0.00002864
Iteration 76/1000 | Loss: 0.00002863
Iteration 77/1000 | Loss: 0.00002863
Iteration 78/1000 | Loss: 0.00002863
Iteration 79/1000 | Loss: 0.00002863
Iteration 80/1000 | Loss: 0.00002863
Iteration 81/1000 | Loss: 0.00002863
Iteration 82/1000 | Loss: 0.00002862
Iteration 83/1000 | Loss: 0.00002862
Iteration 84/1000 | Loss: 0.00002862
Iteration 85/1000 | Loss: 0.00002862
Iteration 86/1000 | Loss: 0.00002862
Iteration 87/1000 | Loss: 0.00002862
Iteration 88/1000 | Loss: 0.00002862
Iteration 89/1000 | Loss: 0.00002862
Iteration 90/1000 | Loss: 0.00002861
Iteration 91/1000 | Loss: 0.00002861
Iteration 92/1000 | Loss: 0.00002861
Iteration 93/1000 | Loss: 0.00002860
Iteration 94/1000 | Loss: 0.00002860
Iteration 95/1000 | Loss: 0.00002860
Iteration 96/1000 | Loss: 0.00002860
Iteration 97/1000 | Loss: 0.00002859
Iteration 98/1000 | Loss: 0.00002859
Iteration 99/1000 | Loss: 0.00002859
Iteration 100/1000 | Loss: 0.00002859
Iteration 101/1000 | Loss: 0.00002859
Iteration 102/1000 | Loss: 0.00002859
Iteration 103/1000 | Loss: 0.00002859
Iteration 104/1000 | Loss: 0.00002859
Iteration 105/1000 | Loss: 0.00002858
Iteration 106/1000 | Loss: 0.00002858
Iteration 107/1000 | Loss: 0.00002858
Iteration 108/1000 | Loss: 0.00002858
Iteration 109/1000 | Loss: 0.00002858
Iteration 110/1000 | Loss: 0.00002858
Iteration 111/1000 | Loss: 0.00002858
Iteration 112/1000 | Loss: 0.00002858
Iteration 113/1000 | Loss: 0.00002858
Iteration 114/1000 | Loss: 0.00002857
Iteration 115/1000 | Loss: 0.00002857
Iteration 116/1000 | Loss: 0.00002857
Iteration 117/1000 | Loss: 0.00002857
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00002856
Iteration 120/1000 | Loss: 0.00002856
Iteration 121/1000 | Loss: 0.00002856
Iteration 122/1000 | Loss: 0.00002856
Iteration 123/1000 | Loss: 0.00002856
Iteration 124/1000 | Loss: 0.00002856
Iteration 125/1000 | Loss: 0.00002856
Iteration 126/1000 | Loss: 0.00002856
Iteration 127/1000 | Loss: 0.00002856
Iteration 128/1000 | Loss: 0.00002856
Iteration 129/1000 | Loss: 0.00002856
Iteration 130/1000 | Loss: 0.00002856
Iteration 131/1000 | Loss: 0.00002856
Iteration 132/1000 | Loss: 0.00002856
Iteration 133/1000 | Loss: 0.00002856
Iteration 134/1000 | Loss: 0.00002856
Iteration 135/1000 | Loss: 0.00002856
Iteration 136/1000 | Loss: 0.00002856
Iteration 137/1000 | Loss: 0.00002856
Iteration 138/1000 | Loss: 0.00002856
Iteration 139/1000 | Loss: 0.00002856
Iteration 140/1000 | Loss: 0.00002856
Iteration 141/1000 | Loss: 0.00002856
Iteration 142/1000 | Loss: 0.00002856
Iteration 143/1000 | Loss: 0.00002856
Iteration 144/1000 | Loss: 0.00002856
Iteration 145/1000 | Loss: 0.00002856
Iteration 146/1000 | Loss: 0.00002856
Iteration 147/1000 | Loss: 0.00002856
Iteration 148/1000 | Loss: 0.00002856
Iteration 149/1000 | Loss: 0.00002856
Iteration 150/1000 | Loss: 0.00002856
Iteration 151/1000 | Loss: 0.00002856
Iteration 152/1000 | Loss: 0.00002856
Iteration 153/1000 | Loss: 0.00002856
Iteration 154/1000 | Loss: 0.00002856
Iteration 155/1000 | Loss: 0.00002856
Iteration 156/1000 | Loss: 0.00002856
Iteration 157/1000 | Loss: 0.00002856
Iteration 158/1000 | Loss: 0.00002856
Iteration 159/1000 | Loss: 0.00002856
Iteration 160/1000 | Loss: 0.00002856
Iteration 161/1000 | Loss: 0.00002856
Iteration 162/1000 | Loss: 0.00002856
Iteration 163/1000 | Loss: 0.00002856
Iteration 164/1000 | Loss: 0.00002856
Iteration 165/1000 | Loss: 0.00002856
Iteration 166/1000 | Loss: 0.00002856
Iteration 167/1000 | Loss: 0.00002856
Iteration 168/1000 | Loss: 0.00002856
Iteration 169/1000 | Loss: 0.00002856
Iteration 170/1000 | Loss: 0.00002856
Iteration 171/1000 | Loss: 0.00002856
Iteration 172/1000 | Loss: 0.00002856
Iteration 173/1000 | Loss: 0.00002856
Iteration 174/1000 | Loss: 0.00002856
Iteration 175/1000 | Loss: 0.00002856
Iteration 176/1000 | Loss: 0.00002856
Iteration 177/1000 | Loss: 0.00002856
Iteration 178/1000 | Loss: 0.00002856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.856120772776194e-05, 2.856120772776194e-05, 2.856120772776194e-05, 2.856120772776194e-05, 2.856120772776194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.856120772776194e-05

Optimization complete. Final v2v error: 4.223575592041016 mm

Highest mean error: 5.878705024719238 mm for frame 36

Lowest mean error: 3.114685535430908 mm for frame 0

Saving results

Total time: 53.761674880981445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101625
Iteration 2/25 | Loss: 0.01101625
Iteration 3/25 | Loss: 0.00196788
Iteration 4/25 | Loss: 0.00145289
Iteration 5/25 | Loss: 0.00124853
Iteration 6/25 | Loss: 0.00115311
Iteration 7/25 | Loss: 0.00110828
Iteration 8/25 | Loss: 0.00112037
Iteration 9/25 | Loss: 0.00107574
Iteration 10/25 | Loss: 0.00105975
Iteration 11/25 | Loss: 0.00097793
Iteration 12/25 | Loss: 0.00095113
Iteration 13/25 | Loss: 0.00093440
Iteration 14/25 | Loss: 0.00092206
Iteration 15/25 | Loss: 0.00091825
Iteration 16/25 | Loss: 0.00091732
Iteration 17/25 | Loss: 0.00091780
Iteration 18/25 | Loss: 0.00091807
Iteration 19/25 | Loss: 0.00091776
Iteration 20/25 | Loss: 0.00091718
Iteration 21/25 | Loss: 0.00091704
Iteration 22/25 | Loss: 0.00091778
Iteration 23/25 | Loss: 0.00091692
Iteration 24/25 | Loss: 0.00091769
Iteration 25/25 | Loss: 0.00091735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47930205
Iteration 2/25 | Loss: 0.00075438
Iteration 3/25 | Loss: 0.00073516
Iteration 4/25 | Loss: 0.00073516
Iteration 5/25 | Loss: 0.00073516
Iteration 6/25 | Loss: 0.00073516
Iteration 7/25 | Loss: 0.00073516
Iteration 8/25 | Loss: 0.00073516
Iteration 9/25 | Loss: 0.00073516
Iteration 10/25 | Loss: 0.00073516
Iteration 11/25 | Loss: 0.00073516
Iteration 12/25 | Loss: 0.00073516
Iteration 13/25 | Loss: 0.00073516
Iteration 14/25 | Loss: 0.00073516
Iteration 15/25 | Loss: 0.00073516
Iteration 16/25 | Loss: 0.00073516
Iteration 17/25 | Loss: 0.00073516
Iteration 18/25 | Loss: 0.00073516
Iteration 19/25 | Loss: 0.00073516
Iteration 20/25 | Loss: 0.00073516
Iteration 21/25 | Loss: 0.00073516
Iteration 22/25 | Loss: 0.00073516
Iteration 23/25 | Loss: 0.00073516
Iteration 24/25 | Loss: 0.00073516
Iteration 25/25 | Loss: 0.00073516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073516
Iteration 2/1000 | Loss: 0.00017864
Iteration 3/1000 | Loss: 0.00020812
Iteration 4/1000 | Loss: 0.00012961
Iteration 5/1000 | Loss: 0.00002240
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00005953
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00003361
Iteration 10/1000 | Loss: 0.00002822
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00003171
Iteration 13/1000 | Loss: 0.00002603
Iteration 14/1000 | Loss: 0.00002574
Iteration 15/1000 | Loss: 0.00043293
Iteration 16/1000 | Loss: 0.00024430
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00029095
Iteration 20/1000 | Loss: 0.00014277
Iteration 21/1000 | Loss: 0.00002496
Iteration 22/1000 | Loss: 0.00003509
Iteration 23/1000 | Loss: 0.00002993
Iteration 24/1000 | Loss: 0.00018241
Iteration 25/1000 | Loss: 0.00028103
Iteration 26/1000 | Loss: 0.00027269
Iteration 27/1000 | Loss: 0.00024463
Iteration 28/1000 | Loss: 0.00013851
Iteration 29/1000 | Loss: 0.00006542
Iteration 30/1000 | Loss: 0.00002799
Iteration 31/1000 | Loss: 0.00002638
Iteration 32/1000 | Loss: 0.00004911
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00004404
Iteration 36/1000 | Loss: 0.00002630
Iteration 37/1000 | Loss: 0.00003252
Iteration 38/1000 | Loss: 0.00004443
Iteration 39/1000 | Loss: 0.00002359
Iteration 40/1000 | Loss: 0.00002104
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00004242
Iteration 43/1000 | Loss: 0.00002064
Iteration 44/1000 | Loss: 0.00002458
Iteration 45/1000 | Loss: 0.00004891
Iteration 46/1000 | Loss: 0.00003700
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00006334
Iteration 49/1000 | Loss: 0.00046851
Iteration 50/1000 | Loss: 0.00004335
Iteration 51/1000 | Loss: 0.00003917
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00006035
Iteration 56/1000 | Loss: 0.00003861
Iteration 57/1000 | Loss: 0.00002332
Iteration 58/1000 | Loss: 0.00002355
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001325
Iteration 74/1000 | Loss: 0.00001325
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.3243793546280358e-05, 1.3243793546280358e-05, 1.3243793546280358e-05, 1.3243793546280358e-05, 1.3243793546280358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3243793546280358e-05

Optimization complete. Final v2v error: 3.0315017700195312 mm

Highest mean error: 5.030189514160156 mm for frame 198

Lowest mean error: 2.643364429473877 mm for frame 72

Saving results

Total time: 145.24239826202393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915536
Iteration 2/25 | Loss: 0.00133176
Iteration 3/25 | Loss: 0.00101474
Iteration 4/25 | Loss: 0.00097935
Iteration 5/25 | Loss: 0.00097470
Iteration 6/25 | Loss: 0.00097363
Iteration 7/25 | Loss: 0.00097363
Iteration 8/25 | Loss: 0.00097363
Iteration 9/25 | Loss: 0.00097363
Iteration 10/25 | Loss: 0.00097363
Iteration 11/25 | Loss: 0.00097363
Iteration 12/25 | Loss: 0.00097363
Iteration 13/25 | Loss: 0.00097363
Iteration 14/25 | Loss: 0.00097363
Iteration 15/25 | Loss: 0.00097363
Iteration 16/25 | Loss: 0.00097363
Iteration 17/25 | Loss: 0.00097363
Iteration 18/25 | Loss: 0.00097363
Iteration 19/25 | Loss: 0.00097363
Iteration 20/25 | Loss: 0.00097363
Iteration 21/25 | Loss: 0.00097363
Iteration 22/25 | Loss: 0.00097363
Iteration 23/25 | Loss: 0.00097363
Iteration 24/25 | Loss: 0.00097363
Iteration 25/25 | Loss: 0.00097363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90260631
Iteration 2/25 | Loss: 0.00021548
Iteration 3/25 | Loss: 0.00021548
Iteration 4/25 | Loss: 0.00021548
Iteration 5/25 | Loss: 0.00021548
Iteration 6/25 | Loss: 0.00021548
Iteration 7/25 | Loss: 0.00021548
Iteration 8/25 | Loss: 0.00021548
Iteration 9/25 | Loss: 0.00021548
Iteration 10/25 | Loss: 0.00021548
Iteration 11/25 | Loss: 0.00021548
Iteration 12/25 | Loss: 0.00021548
Iteration 13/25 | Loss: 0.00021548
Iteration 14/25 | Loss: 0.00021548
Iteration 15/25 | Loss: 0.00021548
Iteration 16/25 | Loss: 0.00021548
Iteration 17/25 | Loss: 0.00021548
Iteration 18/25 | Loss: 0.00021548
Iteration 19/25 | Loss: 0.00021548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00021548169024754316, 0.00021548169024754316, 0.00021548169024754316, 0.00021548169024754316, 0.00021548169024754316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021548169024754316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021548
Iteration 2/1000 | Loss: 0.00004695
Iteration 3/1000 | Loss: 0.00003754
Iteration 4/1000 | Loss: 0.00003437
Iteration 5/1000 | Loss: 0.00003273
Iteration 6/1000 | Loss: 0.00003116
Iteration 7/1000 | Loss: 0.00003027
Iteration 8/1000 | Loss: 0.00002990
Iteration 9/1000 | Loss: 0.00002949
Iteration 10/1000 | Loss: 0.00002917
Iteration 11/1000 | Loss: 0.00002895
Iteration 12/1000 | Loss: 0.00002879
Iteration 13/1000 | Loss: 0.00002869
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002865
Iteration 16/1000 | Loss: 0.00002865
Iteration 17/1000 | Loss: 0.00002865
Iteration 18/1000 | Loss: 0.00002865
Iteration 19/1000 | Loss: 0.00002865
Iteration 20/1000 | Loss: 0.00002865
Iteration 21/1000 | Loss: 0.00002865
Iteration 22/1000 | Loss: 0.00002864
Iteration 23/1000 | Loss: 0.00002864
Iteration 24/1000 | Loss: 0.00002864
Iteration 25/1000 | Loss: 0.00002864
Iteration 26/1000 | Loss: 0.00002862
Iteration 27/1000 | Loss: 0.00002861
Iteration 28/1000 | Loss: 0.00002861
Iteration 29/1000 | Loss: 0.00002860
Iteration 30/1000 | Loss: 0.00002858
Iteration 31/1000 | Loss: 0.00002858
Iteration 32/1000 | Loss: 0.00002858
Iteration 33/1000 | Loss: 0.00002858
Iteration 34/1000 | Loss: 0.00002858
Iteration 35/1000 | Loss: 0.00002858
Iteration 36/1000 | Loss: 0.00002858
Iteration 37/1000 | Loss: 0.00002857
Iteration 38/1000 | Loss: 0.00002857
Iteration 39/1000 | Loss: 0.00002857
Iteration 40/1000 | Loss: 0.00002857
Iteration 41/1000 | Loss: 0.00002857
Iteration 42/1000 | Loss: 0.00002857
Iteration 43/1000 | Loss: 0.00002857
Iteration 44/1000 | Loss: 0.00002857
Iteration 45/1000 | Loss: 0.00002857
Iteration 46/1000 | Loss: 0.00002857
Iteration 47/1000 | Loss: 0.00002856
Iteration 48/1000 | Loss: 0.00002856
Iteration 49/1000 | Loss: 0.00002856
Iteration 50/1000 | Loss: 0.00002856
Iteration 51/1000 | Loss: 0.00002856
Iteration 52/1000 | Loss: 0.00002856
Iteration 53/1000 | Loss: 0.00002856
Iteration 54/1000 | Loss: 0.00002856
Iteration 55/1000 | Loss: 0.00002856
Iteration 56/1000 | Loss: 0.00002856
Iteration 57/1000 | Loss: 0.00002856
Iteration 58/1000 | Loss: 0.00002856
Iteration 59/1000 | Loss: 0.00002856
Iteration 60/1000 | Loss: 0.00002856
Iteration 61/1000 | Loss: 0.00002856
Iteration 62/1000 | Loss: 0.00002856
Iteration 63/1000 | Loss: 0.00002856
Iteration 64/1000 | Loss: 0.00002856
Iteration 65/1000 | Loss: 0.00002856
Iteration 66/1000 | Loss: 0.00002856
Iteration 67/1000 | Loss: 0.00002856
Iteration 68/1000 | Loss: 0.00002856
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002856
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002856
Iteration 73/1000 | Loss: 0.00002856
Iteration 74/1000 | Loss: 0.00002856
Iteration 75/1000 | Loss: 0.00002856
Iteration 76/1000 | Loss: 0.00002856
Iteration 77/1000 | Loss: 0.00002856
Iteration 78/1000 | Loss: 0.00002856
Iteration 79/1000 | Loss: 0.00002856
Iteration 80/1000 | Loss: 0.00002856
Iteration 81/1000 | Loss: 0.00002856
Iteration 82/1000 | Loss: 0.00002856
Iteration 83/1000 | Loss: 0.00002856
Iteration 84/1000 | Loss: 0.00002856
Iteration 85/1000 | Loss: 0.00002856
Iteration 86/1000 | Loss: 0.00002856
Iteration 87/1000 | Loss: 0.00002856
Iteration 88/1000 | Loss: 0.00002856
Iteration 89/1000 | Loss: 0.00002856
Iteration 90/1000 | Loss: 0.00002856
Iteration 91/1000 | Loss: 0.00002856
Iteration 92/1000 | Loss: 0.00002856
Iteration 93/1000 | Loss: 0.00002856
Iteration 94/1000 | Loss: 0.00002856
Iteration 95/1000 | Loss: 0.00002856
Iteration 96/1000 | Loss: 0.00002856
Iteration 97/1000 | Loss: 0.00002856
Iteration 98/1000 | Loss: 0.00002856
Iteration 99/1000 | Loss: 0.00002856
Iteration 100/1000 | Loss: 0.00002856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.856145874829963e-05, 2.856145874829963e-05, 2.856145874829963e-05, 2.856145874829963e-05, 2.856145874829963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.856145874829963e-05

Optimization complete. Final v2v error: 4.5164361000061035 mm

Highest mean error: 4.768281936645508 mm for frame 122

Lowest mean error: 4.278011798858643 mm for frame 38

Saving results

Total time: 30.758728981018066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422459
Iteration 2/25 | Loss: 0.00105935
Iteration 3/25 | Loss: 0.00090270
Iteration 4/25 | Loss: 0.00088781
Iteration 5/25 | Loss: 0.00088280
Iteration 6/25 | Loss: 0.00088212
Iteration 7/25 | Loss: 0.00088212
Iteration 8/25 | Loss: 0.00088212
Iteration 9/25 | Loss: 0.00088212
Iteration 10/25 | Loss: 0.00088212
Iteration 11/25 | Loss: 0.00088212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000882115971762687, 0.000882115971762687, 0.000882115971762687, 0.000882115971762687, 0.000882115971762687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000882115971762687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45135617
Iteration 2/25 | Loss: 0.00043458
Iteration 3/25 | Loss: 0.00043458
Iteration 4/25 | Loss: 0.00043458
Iteration 5/25 | Loss: 0.00043458
Iteration 6/25 | Loss: 0.00043458
Iteration 7/25 | Loss: 0.00043458
Iteration 8/25 | Loss: 0.00043458
Iteration 9/25 | Loss: 0.00043458
Iteration 10/25 | Loss: 0.00043458
Iteration 11/25 | Loss: 0.00043458
Iteration 12/25 | Loss: 0.00043458
Iteration 13/25 | Loss: 0.00043458
Iteration 14/25 | Loss: 0.00043458
Iteration 15/25 | Loss: 0.00043458
Iteration 16/25 | Loss: 0.00043458
Iteration 17/25 | Loss: 0.00043458
Iteration 18/25 | Loss: 0.00043458
Iteration 19/25 | Loss: 0.00043458
Iteration 20/25 | Loss: 0.00043458
Iteration 21/25 | Loss: 0.00043458
Iteration 22/25 | Loss: 0.00043458
Iteration 23/25 | Loss: 0.00043458
Iteration 24/25 | Loss: 0.00043458
Iteration 25/25 | Loss: 0.00043458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00043457598076201975, 0.00043457598076201975, 0.00043457598076201975, 0.00043457598076201975, 0.00043457598076201975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043457598076201975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043458
Iteration 2/1000 | Loss: 0.00002033
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001114
Iteration 6/1000 | Loss: 0.00001082
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001029
Iteration 12/1000 | Loss: 0.00001029
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001023
Iteration 15/1000 | Loss: 0.00001022
Iteration 16/1000 | Loss: 0.00001022
Iteration 17/1000 | Loss: 0.00001022
Iteration 18/1000 | Loss: 0.00001022
Iteration 19/1000 | Loss: 0.00001021
Iteration 20/1000 | Loss: 0.00001021
Iteration 21/1000 | Loss: 0.00001020
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001019
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001017
Iteration 27/1000 | Loss: 0.00001017
Iteration 28/1000 | Loss: 0.00001017
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001017
Iteration 31/1000 | Loss: 0.00001016
Iteration 32/1000 | Loss: 0.00001016
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001016
Iteration 35/1000 | Loss: 0.00001016
Iteration 36/1000 | Loss: 0.00001016
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001007
Iteration 65/1000 | Loss: 0.00001007
Iteration 66/1000 | Loss: 0.00001007
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001006
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001005
Iteration 77/1000 | Loss: 0.00001005
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001002
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001002
Iteration 89/1000 | Loss: 0.00001002
Iteration 90/1000 | Loss: 0.00001002
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00001001
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00000999
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000994
Iteration 148/1000 | Loss: 0.00000994
Iteration 149/1000 | Loss: 0.00000994
Iteration 150/1000 | Loss: 0.00000994
Iteration 151/1000 | Loss: 0.00000994
Iteration 152/1000 | Loss: 0.00000994
Iteration 153/1000 | Loss: 0.00000994
Iteration 154/1000 | Loss: 0.00000994
Iteration 155/1000 | Loss: 0.00000994
Iteration 156/1000 | Loss: 0.00000994
Iteration 157/1000 | Loss: 0.00000994
Iteration 158/1000 | Loss: 0.00000994
Iteration 159/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [9.935569323715754e-06, 9.935569323715754e-06, 9.935569323715754e-06, 9.935569323715754e-06, 9.935569323715754e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.935569323715754e-06

Optimization complete. Final v2v error: 2.7282092571258545 mm

Highest mean error: 3.100393056869507 mm for frame 4

Lowest mean error: 2.392094135284424 mm for frame 222

Saving results

Total time: 36.30057096481323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865046
Iteration 2/25 | Loss: 0.00147465
Iteration 3/25 | Loss: 0.00108333
Iteration 4/25 | Loss: 0.00103022
Iteration 5/25 | Loss: 0.00102636
Iteration 6/25 | Loss: 0.00102471
Iteration 7/25 | Loss: 0.00102471
Iteration 8/25 | Loss: 0.00102471
Iteration 9/25 | Loss: 0.00102471
Iteration 10/25 | Loss: 0.00102471
Iteration 11/25 | Loss: 0.00102471
Iteration 12/25 | Loss: 0.00102471
Iteration 13/25 | Loss: 0.00102471
Iteration 14/25 | Loss: 0.00102471
Iteration 15/25 | Loss: 0.00102471
Iteration 16/25 | Loss: 0.00102471
Iteration 17/25 | Loss: 0.00102471
Iteration 18/25 | Loss: 0.00102471
Iteration 19/25 | Loss: 0.00102471
Iteration 20/25 | Loss: 0.00102471
Iteration 21/25 | Loss: 0.00102471
Iteration 22/25 | Loss: 0.00102471
Iteration 23/25 | Loss: 0.00102471
Iteration 24/25 | Loss: 0.00102471
Iteration 25/25 | Loss: 0.00102471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33031237
Iteration 2/25 | Loss: 0.00052530
Iteration 3/25 | Loss: 0.00052530
Iteration 4/25 | Loss: 0.00052530
Iteration 5/25 | Loss: 0.00052530
Iteration 6/25 | Loss: 0.00052530
Iteration 7/25 | Loss: 0.00052530
Iteration 8/25 | Loss: 0.00052530
Iteration 9/25 | Loss: 0.00052530
Iteration 10/25 | Loss: 0.00052530
Iteration 11/25 | Loss: 0.00052530
Iteration 12/25 | Loss: 0.00052530
Iteration 13/25 | Loss: 0.00052530
Iteration 14/25 | Loss: 0.00052530
Iteration 15/25 | Loss: 0.00052530
Iteration 16/25 | Loss: 0.00052530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005252984701655805, 0.0005252984701655805, 0.0005252984701655805, 0.0005252984701655805, 0.0005252984701655805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005252984701655805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052530
Iteration 2/1000 | Loss: 0.00003562
Iteration 3/1000 | Loss: 0.00002710
Iteration 4/1000 | Loss: 0.00002361
Iteration 5/1000 | Loss: 0.00002198
Iteration 6/1000 | Loss: 0.00002123
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001949
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001887
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001844
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001823
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001793
Iteration 37/1000 | Loss: 0.00001793
Iteration 38/1000 | Loss: 0.00001793
Iteration 39/1000 | Loss: 0.00001793
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001792
Iteration 43/1000 | Loss: 0.00001792
Iteration 44/1000 | Loss: 0.00001792
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001792
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001792
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001792
Iteration 54/1000 | Loss: 0.00001792
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001792
Iteration 58/1000 | Loss: 0.00001792
Iteration 59/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.792070543160662e-05, 1.792070543160662e-05, 1.792070543160662e-05, 1.792070543160662e-05, 1.792070543160662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.792070543160662e-05

Optimization complete. Final v2v error: 3.6350438594818115 mm

Highest mean error: 3.9806697368621826 mm for frame 65

Lowest mean error: 3.119863271713257 mm for frame 14

Saving results

Total time: 34.08232021331787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780122
Iteration 2/25 | Loss: 0.00142644
Iteration 3/25 | Loss: 0.00099297
Iteration 4/25 | Loss: 0.00094824
Iteration 5/25 | Loss: 0.00093807
Iteration 6/25 | Loss: 0.00092591
Iteration 7/25 | Loss: 0.00093345
Iteration 8/25 | Loss: 0.00092440
Iteration 9/25 | Loss: 0.00091755
Iteration 10/25 | Loss: 0.00091177
Iteration 11/25 | Loss: 0.00091291
Iteration 12/25 | Loss: 0.00091020
Iteration 13/25 | Loss: 0.00090555
Iteration 14/25 | Loss: 0.00090584
Iteration 15/25 | Loss: 0.00090385
Iteration 16/25 | Loss: 0.00090328
Iteration 17/25 | Loss: 0.00090602
Iteration 18/25 | Loss: 0.00090561
Iteration 19/25 | Loss: 0.00090229
Iteration 20/25 | Loss: 0.00090095
Iteration 21/25 | Loss: 0.00090074
Iteration 22/25 | Loss: 0.00090060
Iteration 23/25 | Loss: 0.00090058
Iteration 24/25 | Loss: 0.00090057
Iteration 25/25 | Loss: 0.00090057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58376956
Iteration 2/25 | Loss: 0.00048883
Iteration 3/25 | Loss: 0.00048879
Iteration 4/25 | Loss: 0.00048879
Iteration 5/25 | Loss: 0.00048879
Iteration 6/25 | Loss: 0.00048879
Iteration 7/25 | Loss: 0.00048879
Iteration 8/25 | Loss: 0.00048879
Iteration 9/25 | Loss: 0.00048879
Iteration 10/25 | Loss: 0.00048879
Iteration 11/25 | Loss: 0.00048879
Iteration 12/25 | Loss: 0.00048879
Iteration 13/25 | Loss: 0.00048879
Iteration 14/25 | Loss: 0.00048879
Iteration 15/25 | Loss: 0.00048879
Iteration 16/25 | Loss: 0.00048879
Iteration 17/25 | Loss: 0.00048879
Iteration 18/25 | Loss: 0.00048879
Iteration 19/25 | Loss: 0.00048879
Iteration 20/25 | Loss: 0.00048879
Iteration 21/25 | Loss: 0.00048879
Iteration 22/25 | Loss: 0.00048879
Iteration 23/25 | Loss: 0.00048879
Iteration 24/25 | Loss: 0.00048879
Iteration 25/25 | Loss: 0.00048879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048879
Iteration 2/1000 | Loss: 0.00002281
Iteration 3/1000 | Loss: 0.00001550
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001257
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001157
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001156
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001140
Iteration 79/1000 | Loss: 0.00001140
Iteration 80/1000 | Loss: 0.00001140
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.1404246833990328e-05, 1.1404246833990328e-05, 1.1404246833990328e-05, 1.1404246833990328e-05, 1.1404246833990328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1404246833990328e-05

Optimization complete. Final v2v error: 2.8842215538024902 mm

Highest mean error: 3.5171427726745605 mm for frame 70

Lowest mean error: 2.3472070693969727 mm for frame 17

Saving results

Total time: 66.10242438316345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425859
Iteration 2/25 | Loss: 0.00107970
Iteration 3/25 | Loss: 0.00091501
Iteration 4/25 | Loss: 0.00090319
Iteration 5/25 | Loss: 0.00089889
Iteration 6/25 | Loss: 0.00089726
Iteration 7/25 | Loss: 0.00089726
Iteration 8/25 | Loss: 0.00089726
Iteration 9/25 | Loss: 0.00089726
Iteration 10/25 | Loss: 0.00089726
Iteration 11/25 | Loss: 0.00089726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008972595678642392, 0.0008972595678642392, 0.0008972595678642392, 0.0008972595678642392, 0.0008972595678642392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008972595678642392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38421309
Iteration 2/25 | Loss: 0.00073581
Iteration 3/25 | Loss: 0.00073581
Iteration 4/25 | Loss: 0.00073580
Iteration 5/25 | Loss: 0.00073580
Iteration 6/25 | Loss: 0.00073580
Iteration 7/25 | Loss: 0.00073580
Iteration 8/25 | Loss: 0.00073580
Iteration 9/25 | Loss: 0.00073580
Iteration 10/25 | Loss: 0.00073580
Iteration 11/25 | Loss: 0.00073580
Iteration 12/25 | Loss: 0.00073580
Iteration 13/25 | Loss: 0.00073580
Iteration 14/25 | Loss: 0.00073580
Iteration 15/25 | Loss: 0.00073580
Iteration 16/25 | Loss: 0.00073580
Iteration 17/25 | Loss: 0.00073580
Iteration 18/25 | Loss: 0.00073580
Iteration 19/25 | Loss: 0.00073580
Iteration 20/25 | Loss: 0.00073580
Iteration 21/25 | Loss: 0.00073580
Iteration 22/25 | Loss: 0.00073580
Iteration 23/25 | Loss: 0.00073580
Iteration 24/25 | Loss: 0.00073580
Iteration 25/25 | Loss: 0.00073580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073580
Iteration 2/1000 | Loss: 0.00002093
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001326
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001311
Iteration 56/1000 | Loss: 0.00001311
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001311
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001290
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.290447744395351e-05, 1.290447744395351e-05, 1.290447744395351e-05, 1.290447744395351e-05, 1.290447744395351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.290447744395351e-05

Optimization complete. Final v2v error: 2.9727344512939453 mm

Highest mean error: 3.1671648025512695 mm for frame 40

Lowest mean error: 2.7825098037719727 mm for frame 81

Saving results

Total time: 37.37685298919678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499016
Iteration 2/25 | Loss: 0.00107380
Iteration 3/25 | Loss: 0.00098430
Iteration 4/25 | Loss: 0.00097273
Iteration 5/25 | Loss: 0.00096950
Iteration 6/25 | Loss: 0.00096908
Iteration 7/25 | Loss: 0.00096908
Iteration 8/25 | Loss: 0.00096908
Iteration 9/25 | Loss: 0.00096908
Iteration 10/25 | Loss: 0.00096908
Iteration 11/25 | Loss: 0.00096908
Iteration 12/25 | Loss: 0.00096908
Iteration 13/25 | Loss: 0.00096908
Iteration 14/25 | Loss: 0.00096908
Iteration 15/25 | Loss: 0.00096908
Iteration 16/25 | Loss: 0.00096908
Iteration 17/25 | Loss: 0.00096908
Iteration 18/25 | Loss: 0.00096908
Iteration 19/25 | Loss: 0.00096908
Iteration 20/25 | Loss: 0.00096908
Iteration 21/25 | Loss: 0.00096908
Iteration 22/25 | Loss: 0.00096908
Iteration 23/25 | Loss: 0.00096908
Iteration 24/25 | Loss: 0.00096908
Iteration 25/25 | Loss: 0.00096908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35152459
Iteration 2/25 | Loss: 0.00065098
Iteration 3/25 | Loss: 0.00065098
Iteration 4/25 | Loss: 0.00065098
Iteration 5/25 | Loss: 0.00065098
Iteration 6/25 | Loss: 0.00065098
Iteration 7/25 | Loss: 0.00065097
Iteration 8/25 | Loss: 0.00065097
Iteration 9/25 | Loss: 0.00065097
Iteration 10/25 | Loss: 0.00065097
Iteration 11/25 | Loss: 0.00065097
Iteration 12/25 | Loss: 0.00065097
Iteration 13/25 | Loss: 0.00065097
Iteration 14/25 | Loss: 0.00065097
Iteration 15/25 | Loss: 0.00065097
Iteration 16/25 | Loss: 0.00065097
Iteration 17/25 | Loss: 0.00065097
Iteration 18/25 | Loss: 0.00065097
Iteration 19/25 | Loss: 0.00065097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006509730010293424, 0.0006509730010293424, 0.0006509730010293424, 0.0006509730010293424, 0.0006509730010293424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006509730010293424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065097
Iteration 2/1000 | Loss: 0.00003800
Iteration 3/1000 | Loss: 0.00002947
Iteration 4/1000 | Loss: 0.00002734
Iteration 5/1000 | Loss: 0.00002617
Iteration 6/1000 | Loss: 0.00002537
Iteration 7/1000 | Loss: 0.00002478
Iteration 8/1000 | Loss: 0.00002444
Iteration 9/1000 | Loss: 0.00002420
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002399
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002391
Iteration 19/1000 | Loss: 0.00002391
Iteration 20/1000 | Loss: 0.00002391
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00002390
Iteration 23/1000 | Loss: 0.00002390
Iteration 24/1000 | Loss: 0.00002390
Iteration 25/1000 | Loss: 0.00002390
Iteration 26/1000 | Loss: 0.00002389
Iteration 27/1000 | Loss: 0.00002389
Iteration 28/1000 | Loss: 0.00002388
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002386
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002385
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00002384
Iteration 40/1000 | Loss: 0.00002383
Iteration 41/1000 | Loss: 0.00002383
Iteration 42/1000 | Loss: 0.00002383
Iteration 43/1000 | Loss: 0.00002383
Iteration 44/1000 | Loss: 0.00002383
Iteration 45/1000 | Loss: 0.00002383
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002382
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00002382
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002382
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002379
Iteration 57/1000 | Loss: 0.00002379
Iteration 58/1000 | Loss: 0.00002379
Iteration 59/1000 | Loss: 0.00002378
Iteration 60/1000 | Loss: 0.00002378
Iteration 61/1000 | Loss: 0.00002378
Iteration 62/1000 | Loss: 0.00002378
Iteration 63/1000 | Loss: 0.00002377
Iteration 64/1000 | Loss: 0.00002377
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002376
Iteration 67/1000 | Loss: 0.00002375
Iteration 68/1000 | Loss: 0.00002375
Iteration 69/1000 | Loss: 0.00002375
Iteration 70/1000 | Loss: 0.00002375
Iteration 71/1000 | Loss: 0.00002375
Iteration 72/1000 | Loss: 0.00002374
Iteration 73/1000 | Loss: 0.00002374
Iteration 74/1000 | Loss: 0.00002374
Iteration 75/1000 | Loss: 0.00002374
Iteration 76/1000 | Loss: 0.00002374
Iteration 77/1000 | Loss: 0.00002374
Iteration 78/1000 | Loss: 0.00002374
Iteration 79/1000 | Loss: 0.00002374
Iteration 80/1000 | Loss: 0.00002373
Iteration 81/1000 | Loss: 0.00002373
Iteration 82/1000 | Loss: 0.00002373
Iteration 83/1000 | Loss: 0.00002373
Iteration 84/1000 | Loss: 0.00002373
Iteration 85/1000 | Loss: 0.00002372
Iteration 86/1000 | Loss: 0.00002372
Iteration 87/1000 | Loss: 0.00002372
Iteration 88/1000 | Loss: 0.00002372
Iteration 89/1000 | Loss: 0.00002372
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002371
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002371
Iteration 96/1000 | Loss: 0.00002371
Iteration 97/1000 | Loss: 0.00002371
Iteration 98/1000 | Loss: 0.00002370
Iteration 99/1000 | Loss: 0.00002370
Iteration 100/1000 | Loss: 0.00002370
Iteration 101/1000 | Loss: 0.00002370
Iteration 102/1000 | Loss: 0.00002370
Iteration 103/1000 | Loss: 0.00002370
Iteration 104/1000 | Loss: 0.00002370
Iteration 105/1000 | Loss: 0.00002370
Iteration 106/1000 | Loss: 0.00002370
Iteration 107/1000 | Loss: 0.00002370
Iteration 108/1000 | Loss: 0.00002370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.3700664314674214e-05, 2.3700664314674214e-05, 2.3700664314674214e-05, 2.3700664314674214e-05, 2.3700664314674214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3700664314674214e-05

Optimization complete. Final v2v error: 3.7236063480377197 mm

Highest mean error: 4.8237199783325195 mm for frame 93

Lowest mean error: 3.1244122982025146 mm for frame 56

Saving results

Total time: 32.35508751869202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053682
Iteration 2/25 | Loss: 0.00233898
Iteration 3/25 | Loss: 0.00193081
Iteration 4/25 | Loss: 0.00189428
Iteration 5/25 | Loss: 0.00163792
Iteration 6/25 | Loss: 0.00157923
Iteration 7/25 | Loss: 0.00135797
Iteration 8/25 | Loss: 0.00108213
Iteration 9/25 | Loss: 0.00102575
Iteration 10/25 | Loss: 0.00101497
Iteration 11/25 | Loss: 0.00101044
Iteration 12/25 | Loss: 0.00100679
Iteration 13/25 | Loss: 0.00100093
Iteration 14/25 | Loss: 0.00099754
Iteration 15/25 | Loss: 0.00099676
Iteration 16/25 | Loss: 0.00099655
Iteration 17/25 | Loss: 0.00099648
Iteration 18/25 | Loss: 0.00099648
Iteration 19/25 | Loss: 0.00099647
Iteration 20/25 | Loss: 0.00099647
Iteration 21/25 | Loss: 0.00099647
Iteration 22/25 | Loss: 0.00099647
Iteration 23/25 | Loss: 0.00099647
Iteration 24/25 | Loss: 0.00099647
Iteration 25/25 | Loss: 0.00099646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34566283
Iteration 2/25 | Loss: 0.00049070
Iteration 3/25 | Loss: 0.00049070
Iteration 4/25 | Loss: 0.00049070
Iteration 5/25 | Loss: 0.00049070
Iteration 6/25 | Loss: 0.00049070
Iteration 7/25 | Loss: 0.00049070
Iteration 8/25 | Loss: 0.00049070
Iteration 9/25 | Loss: 0.00049070
Iteration 10/25 | Loss: 0.00049070
Iteration 11/25 | Loss: 0.00049070
Iteration 12/25 | Loss: 0.00049070
Iteration 13/25 | Loss: 0.00049070
Iteration 14/25 | Loss: 0.00049070
Iteration 15/25 | Loss: 0.00049070
Iteration 16/25 | Loss: 0.00049070
Iteration 17/25 | Loss: 0.00049070
Iteration 18/25 | Loss: 0.00049070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004906960530206561, 0.0004906960530206561, 0.0004906960530206561, 0.0004906960530206561, 0.0004906960530206561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004906960530206561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049070
Iteration 2/1000 | Loss: 0.00006453
Iteration 3/1000 | Loss: 0.00004614
Iteration 4/1000 | Loss: 0.00004027
Iteration 5/1000 | Loss: 0.00003737
Iteration 6/1000 | Loss: 0.00003591
Iteration 7/1000 | Loss: 0.00003438
Iteration 8/1000 | Loss: 0.00003344
Iteration 9/1000 | Loss: 0.00003261
Iteration 10/1000 | Loss: 0.00003188
Iteration 11/1000 | Loss: 0.00003082
Iteration 12/1000 | Loss: 0.00002995
Iteration 13/1000 | Loss: 0.00002916
Iteration 14/1000 | Loss: 0.00002837
Iteration 15/1000 | Loss: 0.00002805
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002744
Iteration 18/1000 | Loss: 0.00017258
Iteration 19/1000 | Loss: 0.00061652
Iteration 20/1000 | Loss: 0.00005762
Iteration 21/1000 | Loss: 0.00003677
Iteration 22/1000 | Loss: 0.00003250
Iteration 23/1000 | Loss: 0.00002842
Iteration 24/1000 | Loss: 0.00002365
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001845
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001804
Iteration 35/1000 | Loss: 0.00001803
Iteration 36/1000 | Loss: 0.00001802
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001794
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001793
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001791
Iteration 75/1000 | Loss: 0.00001791
Iteration 76/1000 | Loss: 0.00001791
Iteration 77/1000 | Loss: 0.00001791
Iteration 78/1000 | Loss: 0.00001791
Iteration 79/1000 | Loss: 0.00001791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.790521855582483e-05, 1.790521855582483e-05, 1.790521855582483e-05, 1.790521855582483e-05, 1.790521855582483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790521855582483e-05

Optimization complete. Final v2v error: 3.605552911758423 mm

Highest mean error: 3.900782346725464 mm for frame 70

Lowest mean error: 3.271152973175049 mm for frame 237

Saving results

Total time: 88.55609440803528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374483
Iteration 2/25 | Loss: 0.00108139
Iteration 3/25 | Loss: 0.00090510
Iteration 4/25 | Loss: 0.00087845
Iteration 5/25 | Loss: 0.00087246
Iteration 6/25 | Loss: 0.00087110
Iteration 7/25 | Loss: 0.00087110
Iteration 8/25 | Loss: 0.00087110
Iteration 9/25 | Loss: 0.00087110
Iteration 10/25 | Loss: 0.00087110
Iteration 11/25 | Loss: 0.00087110
Iteration 12/25 | Loss: 0.00087110
Iteration 13/25 | Loss: 0.00087110
Iteration 14/25 | Loss: 0.00087110
Iteration 15/25 | Loss: 0.00087110
Iteration 16/25 | Loss: 0.00087110
Iteration 17/25 | Loss: 0.00087110
Iteration 18/25 | Loss: 0.00087110
Iteration 19/25 | Loss: 0.00087110
Iteration 20/25 | Loss: 0.00087110
Iteration 21/25 | Loss: 0.00087110
Iteration 22/25 | Loss: 0.00087110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008711013942956924, 0.0008711013942956924, 0.0008711013942956924, 0.0008711013942956924, 0.0008711013942956924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008711013942956924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33366954
Iteration 2/25 | Loss: 0.00062623
Iteration 3/25 | Loss: 0.00062622
Iteration 4/25 | Loss: 0.00062622
Iteration 5/25 | Loss: 0.00062622
Iteration 6/25 | Loss: 0.00062622
Iteration 7/25 | Loss: 0.00062622
Iteration 8/25 | Loss: 0.00062622
Iteration 9/25 | Loss: 0.00062622
Iteration 10/25 | Loss: 0.00062622
Iteration 11/25 | Loss: 0.00062622
Iteration 12/25 | Loss: 0.00062622
Iteration 13/25 | Loss: 0.00062622
Iteration 14/25 | Loss: 0.00062622
Iteration 15/25 | Loss: 0.00062622
Iteration 16/25 | Loss: 0.00062622
Iteration 17/25 | Loss: 0.00062622
Iteration 18/25 | Loss: 0.00062622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006262172828428447, 0.0006262172828428447, 0.0006262172828428447, 0.0006262172828428447, 0.0006262172828428447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006262172828428447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062622
Iteration 2/1000 | Loss: 0.00002661
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001240
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001169
Iteration 12/1000 | Loss: 0.00001161
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001117
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001115
Iteration 94/1000 | Loss: 0.00001115
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001112
Iteration 119/1000 | Loss: 0.00001112
Iteration 120/1000 | Loss: 0.00001112
Iteration 121/1000 | Loss: 0.00001112
Iteration 122/1000 | Loss: 0.00001112
Iteration 123/1000 | Loss: 0.00001112
Iteration 124/1000 | Loss: 0.00001111
Iteration 125/1000 | Loss: 0.00001111
Iteration 126/1000 | Loss: 0.00001111
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001108
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001104
Iteration 160/1000 | Loss: 0.00001104
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001103
Iteration 171/1000 | Loss: 0.00001103
Iteration 172/1000 | Loss: 0.00001103
Iteration 173/1000 | Loss: 0.00001103
Iteration 174/1000 | Loss: 0.00001103
Iteration 175/1000 | Loss: 0.00001103
Iteration 176/1000 | Loss: 0.00001103
Iteration 177/1000 | Loss: 0.00001103
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001103
Iteration 181/1000 | Loss: 0.00001103
Iteration 182/1000 | Loss: 0.00001103
Iteration 183/1000 | Loss: 0.00001103
Iteration 184/1000 | Loss: 0.00001103
Iteration 185/1000 | Loss: 0.00001103
Iteration 186/1000 | Loss: 0.00001103
Iteration 187/1000 | Loss: 0.00001103
Iteration 188/1000 | Loss: 0.00001103
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.1030419045710005e-05, 1.1030419045710005e-05, 1.1030419045710005e-05, 1.1030419045710005e-05, 1.1030419045710005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1030419045710005e-05

Optimization complete. Final v2v error: 2.734866142272949 mm

Highest mean error: 3.3700203895568848 mm for frame 170

Lowest mean error: 2.2456753253936768 mm for frame 15

Saving results

Total time: 42.63323903083801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410521
Iteration 2/25 | Loss: 0.00096876
Iteration 3/25 | Loss: 0.00087924
Iteration 4/25 | Loss: 0.00085773
Iteration 5/25 | Loss: 0.00085132
Iteration 6/25 | Loss: 0.00085054
Iteration 7/25 | Loss: 0.00085054
Iteration 8/25 | Loss: 0.00085054
Iteration 9/25 | Loss: 0.00085054
Iteration 10/25 | Loss: 0.00085054
Iteration 11/25 | Loss: 0.00085054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008505434379912913, 0.0008505434379912913, 0.0008505434379912913, 0.0008505434379912913, 0.0008505434379912913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008505434379912913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37582922
Iteration 2/25 | Loss: 0.00058491
Iteration 3/25 | Loss: 0.00058490
Iteration 4/25 | Loss: 0.00058490
Iteration 5/25 | Loss: 0.00058490
Iteration 6/25 | Loss: 0.00058490
Iteration 7/25 | Loss: 0.00058490
Iteration 8/25 | Loss: 0.00058490
Iteration 9/25 | Loss: 0.00058490
Iteration 10/25 | Loss: 0.00058490
Iteration 11/25 | Loss: 0.00058490
Iteration 12/25 | Loss: 0.00058490
Iteration 13/25 | Loss: 0.00058490
Iteration 14/25 | Loss: 0.00058490
Iteration 15/25 | Loss: 0.00058490
Iteration 16/25 | Loss: 0.00058490
Iteration 17/25 | Loss: 0.00058490
Iteration 18/25 | Loss: 0.00058490
Iteration 19/25 | Loss: 0.00058490
Iteration 20/25 | Loss: 0.00058490
Iteration 21/25 | Loss: 0.00058490
Iteration 22/25 | Loss: 0.00058490
Iteration 23/25 | Loss: 0.00058490
Iteration 24/25 | Loss: 0.00058490
Iteration 25/25 | Loss: 0.00058490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058490
Iteration 2/1000 | Loss: 0.00002104
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001166
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001161
Iteration 38/1000 | Loss: 0.00001161
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001161
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1534580153238494e-05, 1.1534580153238494e-05, 1.1534580153238494e-05, 1.1534580153238494e-05, 1.1534580153238494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1534580153238494e-05

Optimization complete. Final v2v error: 2.873882293701172 mm

Highest mean error: 3.266272783279419 mm for frame 98

Lowest mean error: 2.449780225753784 mm for frame 0

Saving results

Total time: 28.333090782165527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952708
Iteration 2/25 | Loss: 0.00239792
Iteration 3/25 | Loss: 0.00174412
Iteration 4/25 | Loss: 0.00168248
Iteration 5/25 | Loss: 0.00148056
Iteration 6/25 | Loss: 0.00141347
Iteration 7/25 | Loss: 0.00136101
Iteration 8/25 | Loss: 0.00133124
Iteration 9/25 | Loss: 0.00130356
Iteration 10/25 | Loss: 0.00129365
Iteration 11/25 | Loss: 0.00126845
Iteration 12/25 | Loss: 0.00125996
Iteration 13/25 | Loss: 0.00125991
Iteration 14/25 | Loss: 0.00125888
Iteration 15/25 | Loss: 0.00125677
Iteration 16/25 | Loss: 0.00125420
Iteration 17/25 | Loss: 0.00125723
Iteration 18/25 | Loss: 0.00125613
Iteration 19/25 | Loss: 0.00125261
Iteration 20/25 | Loss: 0.00125164
Iteration 21/25 | Loss: 0.00125093
Iteration 22/25 | Loss: 0.00125061
Iteration 23/25 | Loss: 0.00125043
Iteration 24/25 | Loss: 0.00125033
Iteration 25/25 | Loss: 0.00125007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30894256
Iteration 2/25 | Loss: 0.00253559
Iteration 3/25 | Loss: 0.00242357
Iteration 4/25 | Loss: 0.00242357
Iteration 5/25 | Loss: 0.00242357
Iteration 6/25 | Loss: 0.00242357
Iteration 7/25 | Loss: 0.00242357
Iteration 8/25 | Loss: 0.00242357
Iteration 9/25 | Loss: 0.00242357
Iteration 10/25 | Loss: 0.00242357
Iteration 11/25 | Loss: 0.00242357
Iteration 12/25 | Loss: 0.00242357
Iteration 13/25 | Loss: 0.00242357
Iteration 14/25 | Loss: 0.00242357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002423566300421953, 0.002423566300421953, 0.002423566300421953, 0.002423566300421953, 0.002423566300421953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002423566300421953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242357
Iteration 2/1000 | Loss: 0.01068443
Iteration 3/1000 | Loss: 0.00246861
Iteration 4/1000 | Loss: 0.00610045
Iteration 5/1000 | Loss: 0.00299835
Iteration 6/1000 | Loss: 0.00244719
Iteration 7/1000 | Loss: 0.00083641
Iteration 8/1000 | Loss: 0.00077160
Iteration 9/1000 | Loss: 0.00191652
Iteration 10/1000 | Loss: 0.00183258
Iteration 11/1000 | Loss: 0.00136657
Iteration 12/1000 | Loss: 0.00037000
Iteration 13/1000 | Loss: 0.00067806
Iteration 14/1000 | Loss: 0.00043521
Iteration 15/1000 | Loss: 0.00041712
Iteration 16/1000 | Loss: 0.00022099
Iteration 17/1000 | Loss: 0.00042513
Iteration 18/1000 | Loss: 0.00044151
Iteration 19/1000 | Loss: 0.00038282
Iteration 20/1000 | Loss: 0.00021318
Iteration 21/1000 | Loss: 0.00021980
Iteration 22/1000 | Loss: 0.00015689
Iteration 23/1000 | Loss: 0.00008897
Iteration 24/1000 | Loss: 0.00024377
Iteration 25/1000 | Loss: 0.00020507
Iteration 26/1000 | Loss: 0.00021704
Iteration 27/1000 | Loss: 0.00007847
Iteration 28/1000 | Loss: 0.00031324
Iteration 29/1000 | Loss: 0.00007520
Iteration 30/1000 | Loss: 0.00007087
Iteration 31/1000 | Loss: 0.00006708
Iteration 32/1000 | Loss: 0.00017636
Iteration 33/1000 | Loss: 0.00021839
Iteration 34/1000 | Loss: 0.00008167
Iteration 35/1000 | Loss: 0.00007383
Iteration 36/1000 | Loss: 0.00006615
Iteration 37/1000 | Loss: 0.00021159
Iteration 38/1000 | Loss: 0.00015082
Iteration 39/1000 | Loss: 0.00006097
Iteration 40/1000 | Loss: 0.00005785
Iteration 41/1000 | Loss: 0.00019938
Iteration 42/1000 | Loss: 0.00017414
Iteration 43/1000 | Loss: 0.00014026
Iteration 44/1000 | Loss: 0.00006956
Iteration 45/1000 | Loss: 0.00012836
Iteration 46/1000 | Loss: 0.00012757
Iteration 47/1000 | Loss: 0.00014760
Iteration 48/1000 | Loss: 0.00006038
Iteration 49/1000 | Loss: 0.00005683
Iteration 50/1000 | Loss: 0.00005523
Iteration 51/1000 | Loss: 0.00006051
Iteration 52/1000 | Loss: 0.00005166
Iteration 53/1000 | Loss: 0.00005021
Iteration 54/1000 | Loss: 0.00004921
Iteration 55/1000 | Loss: 0.00004810
Iteration 56/1000 | Loss: 0.00004691
Iteration 57/1000 | Loss: 0.00004626
Iteration 58/1000 | Loss: 0.00004589
Iteration 59/1000 | Loss: 0.00004555
Iteration 60/1000 | Loss: 0.00004532
Iteration 61/1000 | Loss: 0.00004522
Iteration 62/1000 | Loss: 0.00004510
Iteration 63/1000 | Loss: 0.00004506
Iteration 64/1000 | Loss: 0.00004505
Iteration 65/1000 | Loss: 0.00004505
Iteration 66/1000 | Loss: 0.00047360
Iteration 67/1000 | Loss: 0.00005048
Iteration 68/1000 | Loss: 0.00004484
Iteration 69/1000 | Loss: 0.00004406
Iteration 70/1000 | Loss: 0.00004344
Iteration 71/1000 | Loss: 0.00004335
Iteration 72/1000 | Loss: 0.00004317
Iteration 73/1000 | Loss: 0.00004295
Iteration 74/1000 | Loss: 0.00004285
Iteration 75/1000 | Loss: 0.00004283
Iteration 76/1000 | Loss: 0.00004282
Iteration 77/1000 | Loss: 0.00004279
Iteration 78/1000 | Loss: 0.00004278
Iteration 79/1000 | Loss: 0.00004273
Iteration 80/1000 | Loss: 0.00004269
Iteration 81/1000 | Loss: 0.00004268
Iteration 82/1000 | Loss: 0.00004267
Iteration 83/1000 | Loss: 0.00004265
Iteration 84/1000 | Loss: 0.00004262
Iteration 85/1000 | Loss: 0.00004262
Iteration 86/1000 | Loss: 0.00004262
Iteration 87/1000 | Loss: 0.00004261
Iteration 88/1000 | Loss: 0.00004260
Iteration 89/1000 | Loss: 0.00004259
Iteration 90/1000 | Loss: 0.00004259
Iteration 91/1000 | Loss: 0.00004258
Iteration 92/1000 | Loss: 0.00004257
Iteration 93/1000 | Loss: 0.00004257
Iteration 94/1000 | Loss: 0.00004257
Iteration 95/1000 | Loss: 0.00004257
Iteration 96/1000 | Loss: 0.00004257
Iteration 97/1000 | Loss: 0.00004257
Iteration 98/1000 | Loss: 0.00004257
Iteration 99/1000 | Loss: 0.00004257
Iteration 100/1000 | Loss: 0.00004257
Iteration 101/1000 | Loss: 0.00004254
Iteration 102/1000 | Loss: 0.00004253
Iteration 103/1000 | Loss: 0.00004253
Iteration 104/1000 | Loss: 0.00004252
Iteration 105/1000 | Loss: 0.00004251
Iteration 106/1000 | Loss: 0.00004250
Iteration 107/1000 | Loss: 0.00004250
Iteration 108/1000 | Loss: 0.00004249
Iteration 109/1000 | Loss: 0.00004249
Iteration 110/1000 | Loss: 0.00004249
Iteration 111/1000 | Loss: 0.00004249
Iteration 112/1000 | Loss: 0.00004248
Iteration 113/1000 | Loss: 0.00004248
Iteration 114/1000 | Loss: 0.00004248
Iteration 115/1000 | Loss: 0.00004247
Iteration 116/1000 | Loss: 0.00004247
Iteration 117/1000 | Loss: 0.00004247
Iteration 118/1000 | Loss: 0.00004247
Iteration 119/1000 | Loss: 0.00004247
Iteration 120/1000 | Loss: 0.00004246
Iteration 121/1000 | Loss: 0.00004246
Iteration 122/1000 | Loss: 0.00004246
Iteration 123/1000 | Loss: 0.00004246
Iteration 124/1000 | Loss: 0.00004246
Iteration 125/1000 | Loss: 0.00004246
Iteration 126/1000 | Loss: 0.00004245
Iteration 127/1000 | Loss: 0.00004245
Iteration 128/1000 | Loss: 0.00004245
Iteration 129/1000 | Loss: 0.00004245
Iteration 130/1000 | Loss: 0.00004245
Iteration 131/1000 | Loss: 0.00004245
Iteration 132/1000 | Loss: 0.00004245
Iteration 133/1000 | Loss: 0.00004245
Iteration 134/1000 | Loss: 0.00004245
Iteration 135/1000 | Loss: 0.00004244
Iteration 136/1000 | Loss: 0.00004244
Iteration 137/1000 | Loss: 0.00004244
Iteration 138/1000 | Loss: 0.00004244
Iteration 139/1000 | Loss: 0.00004244
Iteration 140/1000 | Loss: 0.00004244
Iteration 141/1000 | Loss: 0.00004244
Iteration 142/1000 | Loss: 0.00004244
Iteration 143/1000 | Loss: 0.00004244
Iteration 144/1000 | Loss: 0.00004244
Iteration 145/1000 | Loss: 0.00004244
Iteration 146/1000 | Loss: 0.00004244
Iteration 147/1000 | Loss: 0.00004244
Iteration 148/1000 | Loss: 0.00004244
Iteration 149/1000 | Loss: 0.00004244
Iteration 150/1000 | Loss: 0.00004244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [4.244031515554525e-05, 4.244031515554525e-05, 4.244031515554525e-05, 4.244031515554525e-05, 4.244031515554525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.244031515554525e-05

Optimization complete. Final v2v error: 3.733778953552246 mm

Highest mean error: 11.117694854736328 mm for frame 145

Lowest mean error: 3.0167548656463623 mm for frame 90

Saving results

Total time: 171.29635334014893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975611
Iteration 2/25 | Loss: 0.00186778
Iteration 3/25 | Loss: 0.00116575
Iteration 4/25 | Loss: 0.00105663
Iteration 5/25 | Loss: 0.00105292
Iteration 6/25 | Loss: 0.00104879
Iteration 7/25 | Loss: 0.00103737
Iteration 8/25 | Loss: 0.00102081
Iteration 9/25 | Loss: 0.00101445
Iteration 10/25 | Loss: 0.00100950
Iteration 11/25 | Loss: 0.00100483
Iteration 12/25 | Loss: 0.00100618
Iteration 13/25 | Loss: 0.00101907
Iteration 14/25 | Loss: 0.00101362
Iteration 15/25 | Loss: 0.00100731
Iteration 16/25 | Loss: 0.00100632
Iteration 17/25 | Loss: 0.00101170
Iteration 18/25 | Loss: 0.00100543
Iteration 19/25 | Loss: 0.00100485
Iteration 20/25 | Loss: 0.00100305
Iteration 21/25 | Loss: 0.00100022
Iteration 22/25 | Loss: 0.00100074
Iteration 23/25 | Loss: 0.00099935
Iteration 24/25 | Loss: 0.00099835
Iteration 25/25 | Loss: 0.00099713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27250719
Iteration 2/25 | Loss: 0.00057107
Iteration 3/25 | Loss: 0.00057107
Iteration 4/25 | Loss: 0.00057107
Iteration 5/25 | Loss: 0.00057107
Iteration 6/25 | Loss: 0.00057107
Iteration 7/25 | Loss: 0.00057107
Iteration 8/25 | Loss: 0.00057107
Iteration 9/25 | Loss: 0.00057107
Iteration 10/25 | Loss: 0.00057107
Iteration 11/25 | Loss: 0.00057107
Iteration 12/25 | Loss: 0.00057107
Iteration 13/25 | Loss: 0.00057107
Iteration 14/25 | Loss: 0.00057107
Iteration 15/25 | Loss: 0.00057107
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005710693658329546, 0.0005710693658329546, 0.0005710693658329546, 0.0005710693658329546, 0.0005710693658329546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005710693658329546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057107
Iteration 2/1000 | Loss: 0.00004435
Iteration 3/1000 | Loss: 0.00003136
Iteration 4/1000 | Loss: 0.00003546
Iteration 5/1000 | Loss: 0.00002621
Iteration 6/1000 | Loss: 0.00003974
Iteration 7/1000 | Loss: 0.00004037
Iteration 8/1000 | Loss: 0.00002837
Iteration 9/1000 | Loss: 0.00002682
Iteration 10/1000 | Loss: 0.00003223
Iteration 11/1000 | Loss: 0.00002875
Iteration 12/1000 | Loss: 0.00003465
Iteration 13/1000 | Loss: 0.00003866
Iteration 14/1000 | Loss: 0.00003143
Iteration 15/1000 | Loss: 0.00002822
Iteration 16/1000 | Loss: 0.00002238
Iteration 17/1000 | Loss: 0.00002728
Iteration 18/1000 | Loss: 0.00003752
Iteration 19/1000 | Loss: 0.00003545
Iteration 20/1000 | Loss: 0.00005741
Iteration 21/1000 | Loss: 0.00004542
Iteration 22/1000 | Loss: 0.00003441
Iteration 23/1000 | Loss: 0.00005167
Iteration 24/1000 | Loss: 0.00003221
Iteration 25/1000 | Loss: 0.00003680
Iteration 26/1000 | Loss: 0.00003639
Iteration 27/1000 | Loss: 0.00004639
Iteration 28/1000 | Loss: 0.00003789
Iteration 29/1000 | Loss: 0.00003161
Iteration 30/1000 | Loss: 0.00004303
Iteration 31/1000 | Loss: 0.00004189
Iteration 32/1000 | Loss: 0.00033771
Iteration 33/1000 | Loss: 0.00016510
Iteration 34/1000 | Loss: 0.00004210
Iteration 35/1000 | Loss: 0.00005108
Iteration 36/1000 | Loss: 0.00003572
Iteration 37/1000 | Loss: 0.00005003
Iteration 38/1000 | Loss: 0.00004893
Iteration 39/1000 | Loss: 0.00004098
Iteration 40/1000 | Loss: 0.00004450
Iteration 41/1000 | Loss: 0.00004082
Iteration 42/1000 | Loss: 0.00004655
Iteration 43/1000 | Loss: 0.00003720
Iteration 44/1000 | Loss: 0.00003306
Iteration 45/1000 | Loss: 0.00004484
Iteration 46/1000 | Loss: 0.00003899
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00004558
Iteration 49/1000 | Loss: 0.00004174
Iteration 50/1000 | Loss: 0.00004613
Iteration 51/1000 | Loss: 0.00004442
Iteration 52/1000 | Loss: 0.00004570
Iteration 53/1000 | Loss: 0.00003526
Iteration 54/1000 | Loss: 0.00004263
Iteration 55/1000 | Loss: 0.00004593
Iteration 56/1000 | Loss: 0.00004743
Iteration 57/1000 | Loss: 0.00004702
Iteration 58/1000 | Loss: 0.00004242
Iteration 59/1000 | Loss: 0.00005651
Iteration 60/1000 | Loss: 0.00003649
Iteration 61/1000 | Loss: 0.00003953
Iteration 62/1000 | Loss: 0.00003527
Iteration 63/1000 | Loss: 0.00005796
Iteration 64/1000 | Loss: 0.00003953
Iteration 65/1000 | Loss: 0.00004708
Iteration 66/1000 | Loss: 0.00004313
Iteration 67/1000 | Loss: 0.00005540
Iteration 68/1000 | Loss: 0.00004324
Iteration 69/1000 | Loss: 0.00005645
Iteration 70/1000 | Loss: 0.00042218
Iteration 71/1000 | Loss: 0.00002575
Iteration 72/1000 | Loss: 0.00002190
Iteration 73/1000 | Loss: 0.00020863
Iteration 74/1000 | Loss: 0.00002136
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001785
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001740
Iteration 82/1000 | Loss: 0.00001725
Iteration 83/1000 | Loss: 0.00001725
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001698
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001695
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001692
Iteration 134/1000 | Loss: 0.00001692
Iteration 135/1000 | Loss: 0.00001712
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001691
Iteration 144/1000 | Loss: 0.00001690
Iteration 145/1000 | Loss: 0.00001690
Iteration 146/1000 | Loss: 0.00001690
Iteration 147/1000 | Loss: 0.00001690
Iteration 148/1000 | Loss: 0.00001689
Iteration 149/1000 | Loss: 0.00001689
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Iteration 154/1000 | Loss: 0.00001688
Iteration 155/1000 | Loss: 0.00001688
Iteration 156/1000 | Loss: 0.00001688
Iteration 157/1000 | Loss: 0.00001687
Iteration 158/1000 | Loss: 0.00001687
Iteration 159/1000 | Loss: 0.00001687
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001687
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001687
Iteration 166/1000 | Loss: 0.00001687
Iteration 167/1000 | Loss: 0.00001687
Iteration 168/1000 | Loss: 0.00001687
Iteration 169/1000 | Loss: 0.00001687
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001686
Iteration 173/1000 | Loss: 0.00001686
Iteration 174/1000 | Loss: 0.00001686
Iteration 175/1000 | Loss: 0.00001686
Iteration 176/1000 | Loss: 0.00001686
Iteration 177/1000 | Loss: 0.00001686
Iteration 178/1000 | Loss: 0.00001686
Iteration 179/1000 | Loss: 0.00001686
Iteration 180/1000 | Loss: 0.00001686
Iteration 181/1000 | Loss: 0.00001686
Iteration 182/1000 | Loss: 0.00001686
Iteration 183/1000 | Loss: 0.00001685
Iteration 184/1000 | Loss: 0.00001685
Iteration 185/1000 | Loss: 0.00001685
Iteration 186/1000 | Loss: 0.00001685
Iteration 187/1000 | Loss: 0.00001685
Iteration 188/1000 | Loss: 0.00001685
Iteration 189/1000 | Loss: 0.00001685
Iteration 190/1000 | Loss: 0.00001685
Iteration 191/1000 | Loss: 0.00001685
Iteration 192/1000 | Loss: 0.00001685
Iteration 193/1000 | Loss: 0.00001685
Iteration 194/1000 | Loss: 0.00001685
Iteration 195/1000 | Loss: 0.00001685
Iteration 196/1000 | Loss: 0.00001685
Iteration 197/1000 | Loss: 0.00001685
Iteration 198/1000 | Loss: 0.00001685
Iteration 199/1000 | Loss: 0.00001685
Iteration 200/1000 | Loss: 0.00001685
Iteration 201/1000 | Loss: 0.00001685
Iteration 202/1000 | Loss: 0.00001685
Iteration 203/1000 | Loss: 0.00001685
Iteration 204/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.6846543076098897e-05, 1.6846543076098897e-05, 1.6846543076098897e-05, 1.6846543076098897e-05, 1.6846543076098897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6846543076098897e-05

Optimization complete. Final v2v error: 3.354140043258667 mm

Highest mean error: 11.04218578338623 mm for frame 99

Lowest mean error: 2.884232521057129 mm for frame 203

Saving results

Total time: 176.1205611228943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900103
Iteration 2/25 | Loss: 0.00131887
Iteration 3/25 | Loss: 0.00104435
Iteration 4/25 | Loss: 0.00101690
Iteration 5/25 | Loss: 0.00101282
Iteration 6/25 | Loss: 0.00101525
Iteration 7/25 | Loss: 0.00100815
Iteration 8/25 | Loss: 0.00100683
Iteration 9/25 | Loss: 0.00100035
Iteration 10/25 | Loss: 0.00099357
Iteration 11/25 | Loss: 0.00099048
Iteration 12/25 | Loss: 0.00098860
Iteration 13/25 | Loss: 0.00098803
Iteration 14/25 | Loss: 0.00098788
Iteration 15/25 | Loss: 0.00098787
Iteration 16/25 | Loss: 0.00098786
Iteration 17/25 | Loss: 0.00098786
Iteration 18/25 | Loss: 0.00098786
Iteration 19/25 | Loss: 0.00098786
Iteration 20/25 | Loss: 0.00098786
Iteration 21/25 | Loss: 0.00098786
Iteration 22/25 | Loss: 0.00098786
Iteration 23/25 | Loss: 0.00098786
Iteration 24/25 | Loss: 0.00098786
Iteration 25/25 | Loss: 0.00098786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90214819
Iteration 2/25 | Loss: 0.00021991
Iteration 3/25 | Loss: 0.00021991
Iteration 4/25 | Loss: 0.00021991
Iteration 5/25 | Loss: 0.00021990
Iteration 6/25 | Loss: 0.00021990
Iteration 7/25 | Loss: 0.00021990
Iteration 8/25 | Loss: 0.00021990
Iteration 9/25 | Loss: 0.00021990
Iteration 10/25 | Loss: 0.00021990
Iteration 11/25 | Loss: 0.00021990
Iteration 12/25 | Loss: 0.00021990
Iteration 13/25 | Loss: 0.00021990
Iteration 14/25 | Loss: 0.00021990
Iteration 15/25 | Loss: 0.00021990
Iteration 16/25 | Loss: 0.00021990
Iteration 17/25 | Loss: 0.00021990
Iteration 18/25 | Loss: 0.00021990
Iteration 19/25 | Loss: 0.00021990
Iteration 20/25 | Loss: 0.00021990
Iteration 21/25 | Loss: 0.00021990
Iteration 22/25 | Loss: 0.00021990
Iteration 23/25 | Loss: 0.00021990
Iteration 24/25 | Loss: 0.00021990
Iteration 25/25 | Loss: 0.00021990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021990
Iteration 2/1000 | Loss: 0.00004680
Iteration 3/1000 | Loss: 0.00003892
Iteration 4/1000 | Loss: 0.00003511
Iteration 5/1000 | Loss: 0.00003315
Iteration 6/1000 | Loss: 0.00003177
Iteration 7/1000 | Loss: 0.00003084
Iteration 8/1000 | Loss: 0.00003021
Iteration 9/1000 | Loss: 0.00002980
Iteration 10/1000 | Loss: 0.00002954
Iteration 11/1000 | Loss: 0.00002926
Iteration 12/1000 | Loss: 0.00002908
Iteration 13/1000 | Loss: 0.00002906
Iteration 14/1000 | Loss: 0.00002905
Iteration 15/1000 | Loss: 0.00002905
Iteration 16/1000 | Loss: 0.00002905
Iteration 17/1000 | Loss: 0.00002904
Iteration 18/1000 | Loss: 0.00002904
Iteration 19/1000 | Loss: 0.00002904
Iteration 20/1000 | Loss: 0.00002904
Iteration 21/1000 | Loss: 0.00002903
Iteration 22/1000 | Loss: 0.00002901
Iteration 23/1000 | Loss: 0.00002897
Iteration 24/1000 | Loss: 0.00002896
Iteration 25/1000 | Loss: 0.00002896
Iteration 26/1000 | Loss: 0.00002896
Iteration 27/1000 | Loss: 0.00002896
Iteration 28/1000 | Loss: 0.00002896
Iteration 29/1000 | Loss: 0.00002896
Iteration 30/1000 | Loss: 0.00002896
Iteration 31/1000 | Loss: 0.00002896
Iteration 32/1000 | Loss: 0.00002896
Iteration 33/1000 | Loss: 0.00002896
Iteration 34/1000 | Loss: 0.00002896
Iteration 35/1000 | Loss: 0.00002895
Iteration 36/1000 | Loss: 0.00002894
Iteration 37/1000 | Loss: 0.00002894
Iteration 38/1000 | Loss: 0.00002894
Iteration 39/1000 | Loss: 0.00002894
Iteration 40/1000 | Loss: 0.00002894
Iteration 41/1000 | Loss: 0.00002894
Iteration 42/1000 | Loss: 0.00002894
Iteration 43/1000 | Loss: 0.00002894
Iteration 44/1000 | Loss: 0.00002894
Iteration 45/1000 | Loss: 0.00002894
Iteration 46/1000 | Loss: 0.00002893
Iteration 47/1000 | Loss: 0.00002893
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002893
Iteration 50/1000 | Loss: 0.00002893
Iteration 51/1000 | Loss: 0.00002893
Iteration 52/1000 | Loss: 0.00002893
Iteration 53/1000 | Loss: 0.00002893
Iteration 54/1000 | Loss: 0.00002892
Iteration 55/1000 | Loss: 0.00002892
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00002892
Iteration 58/1000 | Loss: 0.00002892
Iteration 59/1000 | Loss: 0.00002892
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002892
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002892
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002892
Iteration 67/1000 | Loss: 0.00002892
Iteration 68/1000 | Loss: 0.00002892
Iteration 69/1000 | Loss: 0.00002892
Iteration 70/1000 | Loss: 0.00002892
Iteration 71/1000 | Loss: 0.00002892
Iteration 72/1000 | Loss: 0.00002892
Iteration 73/1000 | Loss: 0.00002892
Iteration 74/1000 | Loss: 0.00002892
Iteration 75/1000 | Loss: 0.00002892
Iteration 76/1000 | Loss: 0.00002892
Iteration 77/1000 | Loss: 0.00002892
Iteration 78/1000 | Loss: 0.00002892
Iteration 79/1000 | Loss: 0.00002892
Iteration 80/1000 | Loss: 0.00002892
Iteration 81/1000 | Loss: 0.00002892
Iteration 82/1000 | Loss: 0.00002892
Iteration 83/1000 | Loss: 0.00002892
Iteration 84/1000 | Loss: 0.00002892
Iteration 85/1000 | Loss: 0.00002892
Iteration 86/1000 | Loss: 0.00002892
Iteration 87/1000 | Loss: 0.00002892
Iteration 88/1000 | Loss: 0.00002892
Iteration 89/1000 | Loss: 0.00002892
Iteration 90/1000 | Loss: 0.00002892
Iteration 91/1000 | Loss: 0.00002892
Iteration 92/1000 | Loss: 0.00002892
Iteration 93/1000 | Loss: 0.00002892
Iteration 94/1000 | Loss: 0.00002892
Iteration 95/1000 | Loss: 0.00002892
Iteration 96/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.891623262257781e-05, 2.891623262257781e-05, 2.891623262257781e-05, 2.891623262257781e-05, 2.891623262257781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.891623262257781e-05

Optimization complete. Final v2v error: 4.490477085113525 mm

Highest mean error: 4.62075138092041 mm for frame 41

Lowest mean error: 4.404429912567139 mm for frame 86

Saving results

Total time: 45.60418128967285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820204
Iteration 2/25 | Loss: 0.00122719
Iteration 3/25 | Loss: 0.00092778
Iteration 4/25 | Loss: 0.00090638
Iteration 5/25 | Loss: 0.00090444
Iteration 6/25 | Loss: 0.00090438
Iteration 7/25 | Loss: 0.00090438
Iteration 8/25 | Loss: 0.00090438
Iteration 9/25 | Loss: 0.00090438
Iteration 10/25 | Loss: 0.00090438
Iteration 11/25 | Loss: 0.00090438
Iteration 12/25 | Loss: 0.00090438
Iteration 13/25 | Loss: 0.00090438
Iteration 14/25 | Loss: 0.00090438
Iteration 15/25 | Loss: 0.00090438
Iteration 16/25 | Loss: 0.00090438
Iteration 17/25 | Loss: 0.00090438
Iteration 18/25 | Loss: 0.00090438
Iteration 19/25 | Loss: 0.00090438
Iteration 20/25 | Loss: 0.00090438
Iteration 21/25 | Loss: 0.00090438
Iteration 22/25 | Loss: 0.00090438
Iteration 23/25 | Loss: 0.00090438
Iteration 24/25 | Loss: 0.00090438
Iteration 25/25 | Loss: 0.00090438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35536039
Iteration 2/25 | Loss: 0.00041602
Iteration 3/25 | Loss: 0.00041601
Iteration 4/25 | Loss: 0.00041601
Iteration 5/25 | Loss: 0.00041601
Iteration 6/25 | Loss: 0.00041601
Iteration 7/25 | Loss: 0.00041601
Iteration 8/25 | Loss: 0.00041601
Iteration 9/25 | Loss: 0.00041601
Iteration 10/25 | Loss: 0.00041601
Iteration 11/25 | Loss: 0.00041601
Iteration 12/25 | Loss: 0.00041601
Iteration 13/25 | Loss: 0.00041601
Iteration 14/25 | Loss: 0.00041601
Iteration 15/25 | Loss: 0.00041601
Iteration 16/25 | Loss: 0.00041601
Iteration 17/25 | Loss: 0.00041601
Iteration 18/25 | Loss: 0.00041601
Iteration 19/25 | Loss: 0.00041601
Iteration 20/25 | Loss: 0.00041601
Iteration 21/25 | Loss: 0.00041601
Iteration 22/25 | Loss: 0.00041601
Iteration 23/25 | Loss: 0.00041601
Iteration 24/25 | Loss: 0.00041601
Iteration 25/25 | Loss: 0.00041601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041601
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001644
Iteration 5/1000 | Loss: 0.00001442
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001308
Iteration 8/1000 | Loss: 0.00001269
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001209
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001203
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001199
Iteration 69/1000 | Loss: 0.00001199
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001199
Iteration 76/1000 | Loss: 0.00001199
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.1992885447398294e-05, 1.1992885447398294e-05, 1.1992885447398294e-05, 1.1992885447398294e-05, 1.1992885447398294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1992885447398294e-05

Optimization complete. Final v2v error: 2.9077181816101074 mm

Highest mean error: 3.20664119720459 mm for frame 123

Lowest mean error: 2.561366081237793 mm for frame 5

Saving results

Total time: 29.342265844345093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737728
Iteration 2/25 | Loss: 0.00107115
Iteration 3/25 | Loss: 0.00092240
Iteration 4/25 | Loss: 0.00089728
Iteration 5/25 | Loss: 0.00088711
Iteration 6/25 | Loss: 0.00088423
Iteration 7/25 | Loss: 0.00088362
Iteration 8/25 | Loss: 0.00088362
Iteration 9/25 | Loss: 0.00088362
Iteration 10/25 | Loss: 0.00088362
Iteration 11/25 | Loss: 0.00088362
Iteration 12/25 | Loss: 0.00088362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008836154593154788, 0.0008836154593154788, 0.0008836154593154788, 0.0008836154593154788, 0.0008836154593154788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008836154593154788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22618604
Iteration 2/25 | Loss: 0.00059329
Iteration 3/25 | Loss: 0.00059329
Iteration 4/25 | Loss: 0.00059329
Iteration 5/25 | Loss: 0.00059329
Iteration 6/25 | Loss: 0.00059329
Iteration 7/25 | Loss: 0.00059329
Iteration 8/25 | Loss: 0.00059329
Iteration 9/25 | Loss: 0.00059329
Iteration 10/25 | Loss: 0.00059329
Iteration 11/25 | Loss: 0.00059329
Iteration 12/25 | Loss: 0.00059329
Iteration 13/25 | Loss: 0.00059329
Iteration 14/25 | Loss: 0.00059329
Iteration 15/25 | Loss: 0.00059329
Iteration 16/25 | Loss: 0.00059329
Iteration 17/25 | Loss: 0.00059329
Iteration 18/25 | Loss: 0.00059329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005932853673584759, 0.0005932853673584759, 0.0005932853673584759, 0.0005932853673584759, 0.0005932853673584759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005932853673584759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059329
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001322
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001285
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001283
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001279
Iteration 87/1000 | Loss: 0.00001279
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001278
Iteration 91/1000 | Loss: 0.00001278
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001277
Iteration 95/1000 | Loss: 0.00001277
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001276
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001272
Iteration 106/1000 | Loss: 0.00001272
Iteration 107/1000 | Loss: 0.00001272
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001271
Iteration 112/1000 | Loss: 0.00001271
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001268
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001267
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001266
Iteration 143/1000 | Loss: 0.00001266
Iteration 144/1000 | Loss: 0.00001266
Iteration 145/1000 | Loss: 0.00001266
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Iteration 151/1000 | Loss: 0.00001265
Iteration 152/1000 | Loss: 0.00001265
Iteration 153/1000 | Loss: 0.00001265
Iteration 154/1000 | Loss: 0.00001265
Iteration 155/1000 | Loss: 0.00001265
Iteration 156/1000 | Loss: 0.00001265
Iteration 157/1000 | Loss: 0.00001265
Iteration 158/1000 | Loss: 0.00001265
Iteration 159/1000 | Loss: 0.00001265
Iteration 160/1000 | Loss: 0.00001265
Iteration 161/1000 | Loss: 0.00001265
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001264
Iteration 166/1000 | Loss: 0.00001264
Iteration 167/1000 | Loss: 0.00001264
Iteration 168/1000 | Loss: 0.00001263
Iteration 169/1000 | Loss: 0.00001263
Iteration 170/1000 | Loss: 0.00001263
Iteration 171/1000 | Loss: 0.00001263
Iteration 172/1000 | Loss: 0.00001263
Iteration 173/1000 | Loss: 0.00001263
Iteration 174/1000 | Loss: 0.00001262
Iteration 175/1000 | Loss: 0.00001262
Iteration 176/1000 | Loss: 0.00001262
Iteration 177/1000 | Loss: 0.00001262
Iteration 178/1000 | Loss: 0.00001262
Iteration 179/1000 | Loss: 0.00001262
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001262
Iteration 182/1000 | Loss: 0.00001262
Iteration 183/1000 | Loss: 0.00001262
Iteration 184/1000 | Loss: 0.00001262
Iteration 185/1000 | Loss: 0.00001261
Iteration 186/1000 | Loss: 0.00001261
Iteration 187/1000 | Loss: 0.00001261
Iteration 188/1000 | Loss: 0.00001261
Iteration 189/1000 | Loss: 0.00001261
Iteration 190/1000 | Loss: 0.00001261
Iteration 191/1000 | Loss: 0.00001261
Iteration 192/1000 | Loss: 0.00001261
Iteration 193/1000 | Loss: 0.00001261
Iteration 194/1000 | Loss: 0.00001261
Iteration 195/1000 | Loss: 0.00001261
Iteration 196/1000 | Loss: 0.00001261
Iteration 197/1000 | Loss: 0.00001261
Iteration 198/1000 | Loss: 0.00001261
Iteration 199/1000 | Loss: 0.00001261
Iteration 200/1000 | Loss: 0.00001261
Iteration 201/1000 | Loss: 0.00001261
Iteration 202/1000 | Loss: 0.00001261
Iteration 203/1000 | Loss: 0.00001261
Iteration 204/1000 | Loss: 0.00001261
Iteration 205/1000 | Loss: 0.00001261
Iteration 206/1000 | Loss: 0.00001261
Iteration 207/1000 | Loss: 0.00001261
Iteration 208/1000 | Loss: 0.00001261
Iteration 209/1000 | Loss: 0.00001261
Iteration 210/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.2607504686457105e-05, 1.2607504686457105e-05, 1.2607504686457105e-05, 1.2607504686457105e-05, 1.2607504686457105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2607504686457105e-05

Optimization complete. Final v2v error: 3.0559468269348145 mm

Highest mean error: 3.414309501647949 mm for frame 161

Lowest mean error: 2.651066780090332 mm for frame 0

Saving results

Total time: 40.34840703010559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984078
Iteration 2/25 | Loss: 0.00224896
Iteration 3/25 | Loss: 0.00140748
Iteration 4/25 | Loss: 0.00128988
Iteration 5/25 | Loss: 0.00124048
Iteration 6/25 | Loss: 0.00120181
Iteration 7/25 | Loss: 0.00114971
Iteration 8/25 | Loss: 0.00112408
Iteration 9/25 | Loss: 0.00110090
Iteration 10/25 | Loss: 0.00110217
Iteration 11/25 | Loss: 0.00110062
Iteration 12/25 | Loss: 0.00109609
Iteration 13/25 | Loss: 0.00109114
Iteration 14/25 | Loss: 0.00108948
Iteration 15/25 | Loss: 0.00108872
Iteration 16/25 | Loss: 0.00108776
Iteration 17/25 | Loss: 0.00108659
Iteration 18/25 | Loss: 0.00108630
Iteration 19/25 | Loss: 0.00108627
Iteration 20/25 | Loss: 0.00108627
Iteration 21/25 | Loss: 0.00108627
Iteration 22/25 | Loss: 0.00108626
Iteration 23/25 | Loss: 0.00108626
Iteration 24/25 | Loss: 0.00108626
Iteration 25/25 | Loss: 0.00108626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87322128
Iteration 2/25 | Loss: 0.00043806
Iteration 3/25 | Loss: 0.00043802
Iteration 4/25 | Loss: 0.00043802
Iteration 5/25 | Loss: 0.00043802
Iteration 6/25 | Loss: 0.00043802
Iteration 7/25 | Loss: 0.00043802
Iteration 8/25 | Loss: 0.00043802
Iteration 9/25 | Loss: 0.00043802
Iteration 10/25 | Loss: 0.00043802
Iteration 11/25 | Loss: 0.00043802
Iteration 12/25 | Loss: 0.00043802
Iteration 13/25 | Loss: 0.00043802
Iteration 14/25 | Loss: 0.00043802
Iteration 15/25 | Loss: 0.00043802
Iteration 16/25 | Loss: 0.00043802
Iteration 17/25 | Loss: 0.00043802
Iteration 18/25 | Loss: 0.00043802
Iteration 19/25 | Loss: 0.00043802
Iteration 20/25 | Loss: 0.00043802
Iteration 21/25 | Loss: 0.00043802
Iteration 22/25 | Loss: 0.00043802
Iteration 23/25 | Loss: 0.00043802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004380171885713935, 0.0004380171885713935, 0.0004380171885713935, 0.0004380171885713935, 0.0004380171885713935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004380171885713935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043802
Iteration 2/1000 | Loss: 0.00004875
Iteration 3/1000 | Loss: 0.00006696
Iteration 4/1000 | Loss: 0.00003401
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00002928
Iteration 7/1000 | Loss: 0.00002766
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002551
Iteration 10/1000 | Loss: 0.00002492
Iteration 11/1000 | Loss: 0.00002452
Iteration 12/1000 | Loss: 0.00002426
Iteration 13/1000 | Loss: 0.00002410
Iteration 14/1000 | Loss: 0.00002409
Iteration 15/1000 | Loss: 0.00002402
Iteration 16/1000 | Loss: 0.00002389
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00002382
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00002381
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002366
Iteration 24/1000 | Loss: 0.00002366
Iteration 25/1000 | Loss: 0.00002356
Iteration 26/1000 | Loss: 0.00002354
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002353
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002348
Iteration 34/1000 | Loss: 0.00002347
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002346
Iteration 37/1000 | Loss: 0.00002345
Iteration 38/1000 | Loss: 0.00002345
Iteration 39/1000 | Loss: 0.00002345
Iteration 40/1000 | Loss: 0.00002344
Iteration 41/1000 | Loss: 0.00002344
Iteration 42/1000 | Loss: 0.00002344
Iteration 43/1000 | Loss: 0.00002343
Iteration 44/1000 | Loss: 0.00002343
Iteration 45/1000 | Loss: 0.00002343
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002343
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002342
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002341
Iteration 60/1000 | Loss: 0.00002340
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002340
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002339
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002338
Iteration 75/1000 | Loss: 0.00002338
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002337
Iteration 79/1000 | Loss: 0.00002337
Iteration 80/1000 | Loss: 0.00002336
Iteration 81/1000 | Loss: 0.00002336
Iteration 82/1000 | Loss: 0.00002336
Iteration 83/1000 | Loss: 0.00002336
Iteration 84/1000 | Loss: 0.00002336
Iteration 85/1000 | Loss: 0.00002336
Iteration 86/1000 | Loss: 0.00002336
Iteration 87/1000 | Loss: 0.00002336
Iteration 88/1000 | Loss: 0.00002336
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002335
Iteration 91/1000 | Loss: 0.00002335
Iteration 92/1000 | Loss: 0.00002335
Iteration 93/1000 | Loss: 0.00002335
Iteration 94/1000 | Loss: 0.00002335
Iteration 95/1000 | Loss: 0.00002335
Iteration 96/1000 | Loss: 0.00002335
Iteration 97/1000 | Loss: 0.00002334
Iteration 98/1000 | Loss: 0.00002334
Iteration 99/1000 | Loss: 0.00002334
Iteration 100/1000 | Loss: 0.00002333
Iteration 101/1000 | Loss: 0.00002333
Iteration 102/1000 | Loss: 0.00002333
Iteration 103/1000 | Loss: 0.00002333
Iteration 104/1000 | Loss: 0.00002333
Iteration 105/1000 | Loss: 0.00002333
Iteration 106/1000 | Loss: 0.00002333
Iteration 107/1000 | Loss: 0.00002333
Iteration 108/1000 | Loss: 0.00002333
Iteration 109/1000 | Loss: 0.00002333
Iteration 110/1000 | Loss: 0.00002333
Iteration 111/1000 | Loss: 0.00002333
Iteration 112/1000 | Loss: 0.00002333
Iteration 113/1000 | Loss: 0.00002333
Iteration 114/1000 | Loss: 0.00002333
Iteration 115/1000 | Loss: 0.00002333
Iteration 116/1000 | Loss: 0.00002332
Iteration 117/1000 | Loss: 0.00002332
Iteration 118/1000 | Loss: 0.00002332
Iteration 119/1000 | Loss: 0.00002332
Iteration 120/1000 | Loss: 0.00002332
Iteration 121/1000 | Loss: 0.00002332
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002332
Iteration 126/1000 | Loss: 0.00002332
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00002332
Iteration 131/1000 | Loss: 0.00002332
Iteration 132/1000 | Loss: 0.00002332
Iteration 133/1000 | Loss: 0.00002332
Iteration 134/1000 | Loss: 0.00002332
Iteration 135/1000 | Loss: 0.00002331
Iteration 136/1000 | Loss: 0.00002331
Iteration 137/1000 | Loss: 0.00002331
Iteration 138/1000 | Loss: 0.00002331
Iteration 139/1000 | Loss: 0.00002331
Iteration 140/1000 | Loss: 0.00002331
Iteration 141/1000 | Loss: 0.00002331
Iteration 142/1000 | Loss: 0.00002331
Iteration 143/1000 | Loss: 0.00002331
Iteration 144/1000 | Loss: 0.00002331
Iteration 145/1000 | Loss: 0.00002331
Iteration 146/1000 | Loss: 0.00002331
Iteration 147/1000 | Loss: 0.00002331
Iteration 148/1000 | Loss: 0.00002331
Iteration 149/1000 | Loss: 0.00002331
Iteration 150/1000 | Loss: 0.00002331
Iteration 151/1000 | Loss: 0.00002330
Iteration 152/1000 | Loss: 0.00002330
Iteration 153/1000 | Loss: 0.00002330
Iteration 154/1000 | Loss: 0.00002330
Iteration 155/1000 | Loss: 0.00002330
Iteration 156/1000 | Loss: 0.00002330
Iteration 157/1000 | Loss: 0.00002330
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002330
Iteration 160/1000 | Loss: 0.00002330
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002330
Iteration 163/1000 | Loss: 0.00002330
Iteration 164/1000 | Loss: 0.00002330
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002330
Iteration 167/1000 | Loss: 0.00002330
Iteration 168/1000 | Loss: 0.00002330
Iteration 169/1000 | Loss: 0.00002330
Iteration 170/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.330157622054685e-05, 2.330157622054685e-05, 2.330157622054685e-05, 2.330157622054685e-05, 2.330157622054685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.330157622054685e-05

Optimization complete. Final v2v error: 3.592637777328491 mm

Highest mean error: 12.189516067504883 mm for frame 6

Lowest mean error: 3.1762936115264893 mm for frame 151

Saving results

Total time: 74.58097219467163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5960/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5960/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368183
Iteration 2/25 | Loss: 0.00093456
Iteration 3/25 | Loss: 0.00083092
Iteration 4/25 | Loss: 0.00082308
Iteration 5/25 | Loss: 0.00082130
Iteration 6/25 | Loss: 0.00082070
Iteration 7/25 | Loss: 0.00082070
Iteration 8/25 | Loss: 0.00082070
Iteration 9/25 | Loss: 0.00082070
Iteration 10/25 | Loss: 0.00082070
Iteration 11/25 | Loss: 0.00082070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008206989732570946, 0.0008206989732570946, 0.0008206989732570946, 0.0008206989732570946, 0.0008206989732570946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008206989732570946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44362342
Iteration 2/25 | Loss: 0.00060556
Iteration 3/25 | Loss: 0.00060556
Iteration 4/25 | Loss: 0.00060556
Iteration 5/25 | Loss: 0.00060556
Iteration 6/25 | Loss: 0.00060556
Iteration 7/25 | Loss: 0.00060556
Iteration 8/25 | Loss: 0.00060556
Iteration 9/25 | Loss: 0.00060556
Iteration 10/25 | Loss: 0.00060556
Iteration 11/25 | Loss: 0.00060556
Iteration 12/25 | Loss: 0.00060556
Iteration 13/25 | Loss: 0.00060556
Iteration 14/25 | Loss: 0.00060556
Iteration 15/25 | Loss: 0.00060556
Iteration 16/25 | Loss: 0.00060556
Iteration 17/25 | Loss: 0.00060556
Iteration 18/25 | Loss: 0.00060556
Iteration 19/25 | Loss: 0.00060556
Iteration 20/25 | Loss: 0.00060556
Iteration 21/25 | Loss: 0.00060556
Iteration 22/25 | Loss: 0.00060556
Iteration 23/25 | Loss: 0.00060556
Iteration 24/25 | Loss: 0.00060556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006055589765310287, 0.0006055589765310287, 0.0006055589765310287, 0.0006055589765310287, 0.0006055589765310287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006055589765310287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060556
Iteration 2/1000 | Loss: 0.00001737
Iteration 3/1000 | Loss: 0.00001190
Iteration 4/1000 | Loss: 0.00000997
Iteration 5/1000 | Loss: 0.00000930
Iteration 6/1000 | Loss: 0.00000895
Iteration 7/1000 | Loss: 0.00000848
Iteration 8/1000 | Loss: 0.00000829
Iteration 9/1000 | Loss: 0.00000828
Iteration 10/1000 | Loss: 0.00000826
Iteration 11/1000 | Loss: 0.00000822
Iteration 12/1000 | Loss: 0.00000821
Iteration 13/1000 | Loss: 0.00000815
Iteration 14/1000 | Loss: 0.00000815
Iteration 15/1000 | Loss: 0.00000813
Iteration 16/1000 | Loss: 0.00000813
Iteration 17/1000 | Loss: 0.00000812
Iteration 18/1000 | Loss: 0.00000811
Iteration 19/1000 | Loss: 0.00000810
Iteration 20/1000 | Loss: 0.00000809
Iteration 21/1000 | Loss: 0.00000808
Iteration 22/1000 | Loss: 0.00000808
Iteration 23/1000 | Loss: 0.00000808
Iteration 24/1000 | Loss: 0.00000807
Iteration 25/1000 | Loss: 0.00000807
Iteration 26/1000 | Loss: 0.00000806
Iteration 27/1000 | Loss: 0.00000806
Iteration 28/1000 | Loss: 0.00000806
Iteration 29/1000 | Loss: 0.00000806
Iteration 30/1000 | Loss: 0.00000805
Iteration 31/1000 | Loss: 0.00000805
Iteration 32/1000 | Loss: 0.00000805
Iteration 33/1000 | Loss: 0.00000804
Iteration 34/1000 | Loss: 0.00000804
Iteration 35/1000 | Loss: 0.00000803
Iteration 36/1000 | Loss: 0.00000803
Iteration 37/1000 | Loss: 0.00000803
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000802
Iteration 40/1000 | Loss: 0.00000802
Iteration 41/1000 | Loss: 0.00000801
Iteration 42/1000 | Loss: 0.00000801
Iteration 43/1000 | Loss: 0.00000801
Iteration 44/1000 | Loss: 0.00000800
Iteration 45/1000 | Loss: 0.00000800
Iteration 46/1000 | Loss: 0.00000800
Iteration 47/1000 | Loss: 0.00000799
Iteration 48/1000 | Loss: 0.00000799
Iteration 49/1000 | Loss: 0.00000799
Iteration 50/1000 | Loss: 0.00000798
Iteration 51/1000 | Loss: 0.00000798
Iteration 52/1000 | Loss: 0.00000798
Iteration 53/1000 | Loss: 0.00000798
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000797
Iteration 56/1000 | Loss: 0.00000797
Iteration 57/1000 | Loss: 0.00000797
Iteration 58/1000 | Loss: 0.00000797
Iteration 59/1000 | Loss: 0.00000796
Iteration 60/1000 | Loss: 0.00000796
Iteration 61/1000 | Loss: 0.00000796
Iteration 62/1000 | Loss: 0.00000796
Iteration 63/1000 | Loss: 0.00000796
Iteration 64/1000 | Loss: 0.00000795
Iteration 65/1000 | Loss: 0.00000795
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000795
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000795
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000795
Iteration 72/1000 | Loss: 0.00000795
Iteration 73/1000 | Loss: 0.00000795
Iteration 74/1000 | Loss: 0.00000795
Iteration 75/1000 | Loss: 0.00000795
Iteration 76/1000 | Loss: 0.00000795
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000794
Iteration 79/1000 | Loss: 0.00000794
Iteration 80/1000 | Loss: 0.00000794
Iteration 81/1000 | Loss: 0.00000794
Iteration 82/1000 | Loss: 0.00000794
Iteration 83/1000 | Loss: 0.00000794
Iteration 84/1000 | Loss: 0.00000794
Iteration 85/1000 | Loss: 0.00000794
Iteration 86/1000 | Loss: 0.00000794
Iteration 87/1000 | Loss: 0.00000794
Iteration 88/1000 | Loss: 0.00000794
Iteration 89/1000 | Loss: 0.00000793
Iteration 90/1000 | Loss: 0.00000793
Iteration 91/1000 | Loss: 0.00000793
Iteration 92/1000 | Loss: 0.00000793
Iteration 93/1000 | Loss: 0.00000793
Iteration 94/1000 | Loss: 0.00000793
Iteration 95/1000 | Loss: 0.00000793
Iteration 96/1000 | Loss: 0.00000793
Iteration 97/1000 | Loss: 0.00000793
Iteration 98/1000 | Loss: 0.00000793
Iteration 99/1000 | Loss: 0.00000793
Iteration 100/1000 | Loss: 0.00000793
Iteration 101/1000 | Loss: 0.00000793
Iteration 102/1000 | Loss: 0.00000793
Iteration 103/1000 | Loss: 0.00000792
Iteration 104/1000 | Loss: 0.00000792
Iteration 105/1000 | Loss: 0.00000792
Iteration 106/1000 | Loss: 0.00000792
Iteration 107/1000 | Loss: 0.00000792
Iteration 108/1000 | Loss: 0.00000792
Iteration 109/1000 | Loss: 0.00000792
Iteration 110/1000 | Loss: 0.00000792
Iteration 111/1000 | Loss: 0.00000792
Iteration 112/1000 | Loss: 0.00000792
Iteration 113/1000 | Loss: 0.00000792
Iteration 114/1000 | Loss: 0.00000791
Iteration 115/1000 | Loss: 0.00000791
Iteration 116/1000 | Loss: 0.00000791
Iteration 117/1000 | Loss: 0.00000791
Iteration 118/1000 | Loss: 0.00000791
Iteration 119/1000 | Loss: 0.00000791
Iteration 120/1000 | Loss: 0.00000791
Iteration 121/1000 | Loss: 0.00000791
Iteration 122/1000 | Loss: 0.00000791
Iteration 123/1000 | Loss: 0.00000791
Iteration 124/1000 | Loss: 0.00000791
Iteration 125/1000 | Loss: 0.00000791
Iteration 126/1000 | Loss: 0.00000791
Iteration 127/1000 | Loss: 0.00000791
Iteration 128/1000 | Loss: 0.00000791
Iteration 129/1000 | Loss: 0.00000790
Iteration 130/1000 | Loss: 0.00000790
Iteration 131/1000 | Loss: 0.00000790
Iteration 132/1000 | Loss: 0.00000790
Iteration 133/1000 | Loss: 0.00000790
Iteration 134/1000 | Loss: 0.00000790
Iteration 135/1000 | Loss: 0.00000790
Iteration 136/1000 | Loss: 0.00000790
Iteration 137/1000 | Loss: 0.00000790
Iteration 138/1000 | Loss: 0.00000790
Iteration 139/1000 | Loss: 0.00000789
Iteration 140/1000 | Loss: 0.00000789
Iteration 141/1000 | Loss: 0.00000789
Iteration 142/1000 | Loss: 0.00000789
Iteration 143/1000 | Loss: 0.00000789
Iteration 144/1000 | Loss: 0.00000789
Iteration 145/1000 | Loss: 0.00000789
Iteration 146/1000 | Loss: 0.00000789
Iteration 147/1000 | Loss: 0.00000789
Iteration 148/1000 | Loss: 0.00000789
Iteration 149/1000 | Loss: 0.00000789
Iteration 150/1000 | Loss: 0.00000789
Iteration 151/1000 | Loss: 0.00000789
Iteration 152/1000 | Loss: 0.00000789
Iteration 153/1000 | Loss: 0.00000789
Iteration 154/1000 | Loss: 0.00000789
Iteration 155/1000 | Loss: 0.00000789
Iteration 156/1000 | Loss: 0.00000789
Iteration 157/1000 | Loss: 0.00000789
Iteration 158/1000 | Loss: 0.00000789
Iteration 159/1000 | Loss: 0.00000789
Iteration 160/1000 | Loss: 0.00000789
Iteration 161/1000 | Loss: 0.00000789
Iteration 162/1000 | Loss: 0.00000789
Iteration 163/1000 | Loss: 0.00000789
Iteration 164/1000 | Loss: 0.00000789
Iteration 165/1000 | Loss: 0.00000789
Iteration 166/1000 | Loss: 0.00000789
Iteration 167/1000 | Loss: 0.00000789
Iteration 168/1000 | Loss: 0.00000789
Iteration 169/1000 | Loss: 0.00000789
Iteration 170/1000 | Loss: 0.00000789
Iteration 171/1000 | Loss: 0.00000789
Iteration 172/1000 | Loss: 0.00000789
Iteration 173/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [7.89155819802545e-06, 7.89155819802545e-06, 7.89155819802545e-06, 7.89155819802545e-06, 7.89155819802545e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.89155819802545e-06

Optimization complete. Final v2v error: 2.35284423828125 mm

Highest mean error: 2.8829612731933594 mm for frame 96

Lowest mean error: 2.0557689666748047 mm for frame 194

Saving results

Total time: 33.547863721847534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874001
Iteration 2/25 | Loss: 0.00178618
Iteration 3/25 | Loss: 0.00169341
Iteration 4/25 | Loss: 0.00167343
Iteration 5/25 | Loss: 0.00166880
Iteration 6/25 | Loss: 0.00166833
Iteration 7/25 | Loss: 0.00166833
Iteration 8/25 | Loss: 0.00166833
Iteration 9/25 | Loss: 0.00166833
Iteration 10/25 | Loss: 0.00166833
Iteration 11/25 | Loss: 0.00166833
Iteration 12/25 | Loss: 0.00166833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001668328302912414, 0.001668328302912414, 0.001668328302912414, 0.001668328302912414, 0.001668328302912414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001668328302912414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39749467
Iteration 2/25 | Loss: 0.00171950
Iteration 3/25 | Loss: 0.00171940
Iteration 4/25 | Loss: 0.00171940
Iteration 5/25 | Loss: 0.00171940
Iteration 6/25 | Loss: 0.00171940
Iteration 7/25 | Loss: 0.00171940
Iteration 8/25 | Loss: 0.00171940
Iteration 9/25 | Loss: 0.00171940
Iteration 10/25 | Loss: 0.00171940
Iteration 11/25 | Loss: 0.00171940
Iteration 12/25 | Loss: 0.00171940
Iteration 13/25 | Loss: 0.00171940
Iteration 14/25 | Loss: 0.00171940
Iteration 15/25 | Loss: 0.00171940
Iteration 16/25 | Loss: 0.00171940
Iteration 17/25 | Loss: 0.00171940
Iteration 18/25 | Loss: 0.00171940
Iteration 19/25 | Loss: 0.00171940
Iteration 20/25 | Loss: 0.00171940
Iteration 21/25 | Loss: 0.00171940
Iteration 22/25 | Loss: 0.00171940
Iteration 23/25 | Loss: 0.00171940
Iteration 24/25 | Loss: 0.00171940
Iteration 25/25 | Loss: 0.00171940

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171940
Iteration 2/1000 | Loss: 0.00005397
Iteration 3/1000 | Loss: 0.00004230
Iteration 4/1000 | Loss: 0.00003786
Iteration 5/1000 | Loss: 0.00003572
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00003301
Iteration 8/1000 | Loss: 0.00003205
Iteration 9/1000 | Loss: 0.00003153
Iteration 10/1000 | Loss: 0.00003119
Iteration 11/1000 | Loss: 0.00003090
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003070
Iteration 14/1000 | Loss: 0.00003068
Iteration 15/1000 | Loss: 0.00003066
Iteration 16/1000 | Loss: 0.00003063
Iteration 17/1000 | Loss: 0.00003062
Iteration 18/1000 | Loss: 0.00003056
Iteration 19/1000 | Loss: 0.00003055
Iteration 20/1000 | Loss: 0.00003054
Iteration 21/1000 | Loss: 0.00003053
Iteration 22/1000 | Loss: 0.00003053
Iteration 23/1000 | Loss: 0.00003048
Iteration 24/1000 | Loss: 0.00003045
Iteration 25/1000 | Loss: 0.00003044
Iteration 26/1000 | Loss: 0.00003044
Iteration 27/1000 | Loss: 0.00003044
Iteration 28/1000 | Loss: 0.00003043
Iteration 29/1000 | Loss: 0.00003043
Iteration 30/1000 | Loss: 0.00003042
Iteration 31/1000 | Loss: 0.00003042
Iteration 32/1000 | Loss: 0.00003040
Iteration 33/1000 | Loss: 0.00003038
Iteration 34/1000 | Loss: 0.00003037
Iteration 35/1000 | Loss: 0.00003037
Iteration 36/1000 | Loss: 0.00003037
Iteration 37/1000 | Loss: 0.00003037
Iteration 38/1000 | Loss: 0.00003037
Iteration 39/1000 | Loss: 0.00003037
Iteration 40/1000 | Loss: 0.00003037
Iteration 41/1000 | Loss: 0.00003037
Iteration 42/1000 | Loss: 0.00003037
Iteration 43/1000 | Loss: 0.00003035
Iteration 44/1000 | Loss: 0.00003035
Iteration 45/1000 | Loss: 0.00003034
Iteration 46/1000 | Loss: 0.00003034
Iteration 47/1000 | Loss: 0.00003033
Iteration 48/1000 | Loss: 0.00003032
Iteration 49/1000 | Loss: 0.00003031
Iteration 50/1000 | Loss: 0.00003031
Iteration 51/1000 | Loss: 0.00003031
Iteration 52/1000 | Loss: 0.00003031
Iteration 53/1000 | Loss: 0.00003031
Iteration 54/1000 | Loss: 0.00003031
Iteration 55/1000 | Loss: 0.00003031
Iteration 56/1000 | Loss: 0.00003031
Iteration 57/1000 | Loss: 0.00003031
Iteration 58/1000 | Loss: 0.00003031
Iteration 59/1000 | Loss: 0.00003031
Iteration 60/1000 | Loss: 0.00003031
Iteration 61/1000 | Loss: 0.00003031
Iteration 62/1000 | Loss: 0.00003031
Iteration 63/1000 | Loss: 0.00003031
Iteration 64/1000 | Loss: 0.00003031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [3.030874540854711e-05, 3.030874540854711e-05, 3.030874540854711e-05, 3.030874540854711e-05, 3.030874540854711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.030874540854711e-05

Optimization complete. Final v2v error: 4.809398651123047 mm

Highest mean error: 5.278637886047363 mm for frame 153

Lowest mean error: 4.456244468688965 mm for frame 240

Saving results

Total time: 36.59056234359741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980415
Iteration 2/25 | Loss: 0.00172948
Iteration 3/25 | Loss: 0.00161415
Iteration 4/25 | Loss: 0.00159718
Iteration 5/25 | Loss: 0.00159138
Iteration 6/25 | Loss: 0.00159004
Iteration 7/25 | Loss: 0.00159004
Iteration 8/25 | Loss: 0.00159004
Iteration 9/25 | Loss: 0.00159004
Iteration 10/25 | Loss: 0.00159004
Iteration 11/25 | Loss: 0.00159004
Iteration 12/25 | Loss: 0.00159004
Iteration 13/25 | Loss: 0.00159004
Iteration 14/25 | Loss: 0.00159004
Iteration 15/25 | Loss: 0.00159004
Iteration 16/25 | Loss: 0.00159004
Iteration 17/25 | Loss: 0.00159004
Iteration 18/25 | Loss: 0.00159004
Iteration 19/25 | Loss: 0.00159004
Iteration 20/25 | Loss: 0.00159004
Iteration 21/25 | Loss: 0.00159004
Iteration 22/25 | Loss: 0.00159004
Iteration 23/25 | Loss: 0.00159004
Iteration 24/25 | Loss: 0.00159004
Iteration 25/25 | Loss: 0.00159004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.90063953
Iteration 2/25 | Loss: 0.00154124
Iteration 3/25 | Loss: 0.00154124
Iteration 4/25 | Loss: 0.00154124
Iteration 5/25 | Loss: 0.00154124
Iteration 6/25 | Loss: 0.00154124
Iteration 7/25 | Loss: 0.00154124
Iteration 8/25 | Loss: 0.00154124
Iteration 9/25 | Loss: 0.00154124
Iteration 10/25 | Loss: 0.00154124
Iteration 11/25 | Loss: 0.00154124
Iteration 12/25 | Loss: 0.00154124
Iteration 13/25 | Loss: 0.00154124
Iteration 14/25 | Loss: 0.00154124
Iteration 15/25 | Loss: 0.00154124
Iteration 16/25 | Loss: 0.00154124
Iteration 17/25 | Loss: 0.00154124
Iteration 18/25 | Loss: 0.00154124
Iteration 19/25 | Loss: 0.00154124
Iteration 20/25 | Loss: 0.00154124
Iteration 21/25 | Loss: 0.00154124
Iteration 22/25 | Loss: 0.00154124
Iteration 23/25 | Loss: 0.00154124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015412380453199148, 0.0015412380453199148, 0.0015412380453199148, 0.0015412380453199148, 0.0015412380453199148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015412380453199148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154124
Iteration 2/1000 | Loss: 0.00005005
Iteration 3/1000 | Loss: 0.00003743
Iteration 4/1000 | Loss: 0.00003527
Iteration 5/1000 | Loss: 0.00003376
Iteration 6/1000 | Loss: 0.00003296
Iteration 7/1000 | Loss: 0.00003232
Iteration 8/1000 | Loss: 0.00003191
Iteration 9/1000 | Loss: 0.00003156
Iteration 10/1000 | Loss: 0.00003134
Iteration 11/1000 | Loss: 0.00003118
Iteration 12/1000 | Loss: 0.00003104
Iteration 13/1000 | Loss: 0.00003100
Iteration 14/1000 | Loss: 0.00003090
Iteration 15/1000 | Loss: 0.00003084
Iteration 16/1000 | Loss: 0.00003081
Iteration 17/1000 | Loss: 0.00003077
Iteration 18/1000 | Loss: 0.00003077
Iteration 19/1000 | Loss: 0.00003077
Iteration 20/1000 | Loss: 0.00003077
Iteration 21/1000 | Loss: 0.00003077
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00003076
Iteration 24/1000 | Loss: 0.00003076
Iteration 25/1000 | Loss: 0.00003075
Iteration 26/1000 | Loss: 0.00003074
Iteration 27/1000 | Loss: 0.00003074
Iteration 28/1000 | Loss: 0.00003073
Iteration 29/1000 | Loss: 0.00003073
Iteration 30/1000 | Loss: 0.00003073
Iteration 31/1000 | Loss: 0.00003073
Iteration 32/1000 | Loss: 0.00003072
Iteration 33/1000 | Loss: 0.00003072
Iteration 34/1000 | Loss: 0.00003072
Iteration 35/1000 | Loss: 0.00003071
Iteration 36/1000 | Loss: 0.00003071
Iteration 37/1000 | Loss: 0.00003071
Iteration 38/1000 | Loss: 0.00003071
Iteration 39/1000 | Loss: 0.00003071
Iteration 40/1000 | Loss: 0.00003071
Iteration 41/1000 | Loss: 0.00003071
Iteration 42/1000 | Loss: 0.00003071
Iteration 43/1000 | Loss: 0.00003071
Iteration 44/1000 | Loss: 0.00003071
Iteration 45/1000 | Loss: 0.00003070
Iteration 46/1000 | Loss: 0.00003070
Iteration 47/1000 | Loss: 0.00003070
Iteration 48/1000 | Loss: 0.00003070
Iteration 49/1000 | Loss: 0.00003069
Iteration 50/1000 | Loss: 0.00003069
Iteration 51/1000 | Loss: 0.00003069
Iteration 52/1000 | Loss: 0.00003069
Iteration 53/1000 | Loss: 0.00003069
Iteration 54/1000 | Loss: 0.00003069
Iteration 55/1000 | Loss: 0.00003069
Iteration 56/1000 | Loss: 0.00003069
Iteration 57/1000 | Loss: 0.00003068
Iteration 58/1000 | Loss: 0.00003068
Iteration 59/1000 | Loss: 0.00003068
Iteration 60/1000 | Loss: 0.00003068
Iteration 61/1000 | Loss: 0.00003068
Iteration 62/1000 | Loss: 0.00003068
Iteration 63/1000 | Loss: 0.00003068
Iteration 64/1000 | Loss: 0.00003068
Iteration 65/1000 | Loss: 0.00003068
Iteration 66/1000 | Loss: 0.00003068
Iteration 67/1000 | Loss: 0.00003068
Iteration 68/1000 | Loss: 0.00003068
Iteration 69/1000 | Loss: 0.00003068
Iteration 70/1000 | Loss: 0.00003068
Iteration 71/1000 | Loss: 0.00003068
Iteration 72/1000 | Loss: 0.00003068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [3.0683881050208583e-05, 3.0683881050208583e-05, 3.0683881050208583e-05, 3.0683881050208583e-05, 3.0683881050208583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0683881050208583e-05

Optimization complete. Final v2v error: 4.917121887207031 mm

Highest mean error: 5.460229396820068 mm for frame 237

Lowest mean error: 4.480629920959473 mm for frame 35

Saving results

Total time: 35.64402651786804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460557
Iteration 2/25 | Loss: 0.00176134
Iteration 3/25 | Loss: 0.00163820
Iteration 4/25 | Loss: 0.00161401
Iteration 5/25 | Loss: 0.00160362
Iteration 6/25 | Loss: 0.00160068
Iteration 7/25 | Loss: 0.00159974
Iteration 8/25 | Loss: 0.00159974
Iteration 9/25 | Loss: 0.00159974
Iteration 10/25 | Loss: 0.00159974
Iteration 11/25 | Loss: 0.00159974
Iteration 12/25 | Loss: 0.00159974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015997444279491901, 0.0015997444279491901, 0.0015997444279491901, 0.0015997444279491901, 0.0015997444279491901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015997444279491901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38307035
Iteration 2/25 | Loss: 0.00160553
Iteration 3/25 | Loss: 0.00160553
Iteration 4/25 | Loss: 0.00160553
Iteration 5/25 | Loss: 0.00160553
Iteration 6/25 | Loss: 0.00160553
Iteration 7/25 | Loss: 0.00160553
Iteration 8/25 | Loss: 0.00160553
Iteration 9/25 | Loss: 0.00160553
Iteration 10/25 | Loss: 0.00160553
Iteration 11/25 | Loss: 0.00160553
Iteration 12/25 | Loss: 0.00160553
Iteration 13/25 | Loss: 0.00160553
Iteration 14/25 | Loss: 0.00160553
Iteration 15/25 | Loss: 0.00160553
Iteration 16/25 | Loss: 0.00160553
Iteration 17/25 | Loss: 0.00160553
Iteration 18/25 | Loss: 0.00160553
Iteration 19/25 | Loss: 0.00160553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016055316664278507, 0.0016055316664278507, 0.0016055316664278507, 0.0016055316664278507, 0.0016055316664278507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016055316664278507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160553
Iteration 2/1000 | Loss: 0.00006875
Iteration 3/1000 | Loss: 0.00005078
Iteration 4/1000 | Loss: 0.00004323
Iteration 5/1000 | Loss: 0.00003924
Iteration 6/1000 | Loss: 0.00003706
Iteration 7/1000 | Loss: 0.00003537
Iteration 8/1000 | Loss: 0.00003441
Iteration 9/1000 | Loss: 0.00003361
Iteration 10/1000 | Loss: 0.00003304
Iteration 11/1000 | Loss: 0.00003261
Iteration 12/1000 | Loss: 0.00003230
Iteration 13/1000 | Loss: 0.00003198
Iteration 14/1000 | Loss: 0.00003186
Iteration 15/1000 | Loss: 0.00003174
Iteration 16/1000 | Loss: 0.00003169
Iteration 17/1000 | Loss: 0.00003162
Iteration 18/1000 | Loss: 0.00003160
Iteration 19/1000 | Loss: 0.00003160
Iteration 20/1000 | Loss: 0.00003159
Iteration 21/1000 | Loss: 0.00003159
Iteration 22/1000 | Loss: 0.00003158
Iteration 23/1000 | Loss: 0.00003158
Iteration 24/1000 | Loss: 0.00003158
Iteration 25/1000 | Loss: 0.00003157
Iteration 26/1000 | Loss: 0.00003157
Iteration 27/1000 | Loss: 0.00003156
Iteration 28/1000 | Loss: 0.00003155
Iteration 29/1000 | Loss: 0.00003155
Iteration 30/1000 | Loss: 0.00003154
Iteration 31/1000 | Loss: 0.00003153
Iteration 32/1000 | Loss: 0.00003153
Iteration 33/1000 | Loss: 0.00003153
Iteration 34/1000 | Loss: 0.00003152
Iteration 35/1000 | Loss: 0.00003152
Iteration 36/1000 | Loss: 0.00003151
Iteration 37/1000 | Loss: 0.00003151
Iteration 38/1000 | Loss: 0.00003151
Iteration 39/1000 | Loss: 0.00003150
Iteration 40/1000 | Loss: 0.00003150
Iteration 41/1000 | Loss: 0.00003149
Iteration 42/1000 | Loss: 0.00003149
Iteration 43/1000 | Loss: 0.00003149
Iteration 44/1000 | Loss: 0.00003149
Iteration 45/1000 | Loss: 0.00003148
Iteration 46/1000 | Loss: 0.00003148
Iteration 47/1000 | Loss: 0.00003148
Iteration 48/1000 | Loss: 0.00003148
Iteration 49/1000 | Loss: 0.00003148
Iteration 50/1000 | Loss: 0.00003147
Iteration 51/1000 | Loss: 0.00003147
Iteration 52/1000 | Loss: 0.00003147
Iteration 53/1000 | Loss: 0.00003147
Iteration 54/1000 | Loss: 0.00003146
Iteration 55/1000 | Loss: 0.00003146
Iteration 56/1000 | Loss: 0.00003146
Iteration 57/1000 | Loss: 0.00003146
Iteration 58/1000 | Loss: 0.00003145
Iteration 59/1000 | Loss: 0.00003145
Iteration 60/1000 | Loss: 0.00003145
Iteration 61/1000 | Loss: 0.00003145
Iteration 62/1000 | Loss: 0.00003145
Iteration 63/1000 | Loss: 0.00003144
Iteration 64/1000 | Loss: 0.00003144
Iteration 65/1000 | Loss: 0.00003144
Iteration 66/1000 | Loss: 0.00003144
Iteration 67/1000 | Loss: 0.00003144
Iteration 68/1000 | Loss: 0.00003144
Iteration 69/1000 | Loss: 0.00003143
Iteration 70/1000 | Loss: 0.00003143
Iteration 71/1000 | Loss: 0.00003143
Iteration 72/1000 | Loss: 0.00003142
Iteration 73/1000 | Loss: 0.00003142
Iteration 74/1000 | Loss: 0.00003142
Iteration 75/1000 | Loss: 0.00003142
Iteration 76/1000 | Loss: 0.00003142
Iteration 77/1000 | Loss: 0.00003142
Iteration 78/1000 | Loss: 0.00003141
Iteration 79/1000 | Loss: 0.00003141
Iteration 80/1000 | Loss: 0.00003141
Iteration 81/1000 | Loss: 0.00003141
Iteration 82/1000 | Loss: 0.00003141
Iteration 83/1000 | Loss: 0.00003141
Iteration 84/1000 | Loss: 0.00003141
Iteration 85/1000 | Loss: 0.00003141
Iteration 86/1000 | Loss: 0.00003140
Iteration 87/1000 | Loss: 0.00003140
Iteration 88/1000 | Loss: 0.00003140
Iteration 89/1000 | Loss: 0.00003140
Iteration 90/1000 | Loss: 0.00003140
Iteration 91/1000 | Loss: 0.00003140
Iteration 92/1000 | Loss: 0.00003140
Iteration 93/1000 | Loss: 0.00003140
Iteration 94/1000 | Loss: 0.00003140
Iteration 95/1000 | Loss: 0.00003140
Iteration 96/1000 | Loss: 0.00003139
Iteration 97/1000 | Loss: 0.00003139
Iteration 98/1000 | Loss: 0.00003139
Iteration 99/1000 | Loss: 0.00003139
Iteration 100/1000 | Loss: 0.00003139
Iteration 101/1000 | Loss: 0.00003139
Iteration 102/1000 | Loss: 0.00003139
Iteration 103/1000 | Loss: 0.00003139
Iteration 104/1000 | Loss: 0.00003139
Iteration 105/1000 | Loss: 0.00003138
Iteration 106/1000 | Loss: 0.00003138
Iteration 107/1000 | Loss: 0.00003138
Iteration 108/1000 | Loss: 0.00003138
Iteration 109/1000 | Loss: 0.00003138
Iteration 110/1000 | Loss: 0.00003138
Iteration 111/1000 | Loss: 0.00003138
Iteration 112/1000 | Loss: 0.00003138
Iteration 113/1000 | Loss: 0.00003138
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003137
Iteration 116/1000 | Loss: 0.00003137
Iteration 117/1000 | Loss: 0.00003137
Iteration 118/1000 | Loss: 0.00003137
Iteration 119/1000 | Loss: 0.00003137
Iteration 120/1000 | Loss: 0.00003137
Iteration 121/1000 | Loss: 0.00003137
Iteration 122/1000 | Loss: 0.00003137
Iteration 123/1000 | Loss: 0.00003137
Iteration 124/1000 | Loss: 0.00003137
Iteration 125/1000 | Loss: 0.00003136
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00003136
Iteration 129/1000 | Loss: 0.00003136
Iteration 130/1000 | Loss: 0.00003136
Iteration 131/1000 | Loss: 0.00003136
Iteration 132/1000 | Loss: 0.00003136
Iteration 133/1000 | Loss: 0.00003136
Iteration 134/1000 | Loss: 0.00003136
Iteration 135/1000 | Loss: 0.00003136
Iteration 136/1000 | Loss: 0.00003136
Iteration 137/1000 | Loss: 0.00003136
Iteration 138/1000 | Loss: 0.00003136
Iteration 139/1000 | Loss: 0.00003136
Iteration 140/1000 | Loss: 0.00003136
Iteration 141/1000 | Loss: 0.00003136
Iteration 142/1000 | Loss: 0.00003136
Iteration 143/1000 | Loss: 0.00003136
Iteration 144/1000 | Loss: 0.00003136
Iteration 145/1000 | Loss: 0.00003136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.1356237741420045e-05, 3.1356237741420045e-05, 3.1356237741420045e-05, 3.1356237741420045e-05, 3.1356237741420045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1356237741420045e-05

Optimization complete. Final v2v error: 4.750895023345947 mm

Highest mean error: 5.360301494598389 mm for frame 89

Lowest mean error: 4.073021411895752 mm for frame 69

Saving results

Total time: 40.07460427284241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01190264
Iteration 2/25 | Loss: 0.00721853
Iteration 3/25 | Loss: 0.00472491
Iteration 4/25 | Loss: 0.00391750
Iteration 5/25 | Loss: 0.00366032
Iteration 6/25 | Loss: 0.00357576
Iteration 7/25 | Loss: 0.00306339
Iteration 8/25 | Loss: 0.00277800
Iteration 9/25 | Loss: 0.00267502
Iteration 10/25 | Loss: 0.00260417
Iteration 11/25 | Loss: 0.00252717
Iteration 12/25 | Loss: 0.00254265
Iteration 13/25 | Loss: 0.00248156
Iteration 14/25 | Loss: 0.00244179
Iteration 15/25 | Loss: 0.00242029
Iteration 16/25 | Loss: 0.00242596
Iteration 17/25 | Loss: 0.00241346
Iteration 18/25 | Loss: 0.00243910
Iteration 19/25 | Loss: 0.00240569
Iteration 20/25 | Loss: 0.00241227
Iteration 21/25 | Loss: 0.00239839
Iteration 22/25 | Loss: 0.00240952
Iteration 23/25 | Loss: 0.00239652
Iteration 24/25 | Loss: 0.00238935
Iteration 25/25 | Loss: 0.00238946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43203250
Iteration 2/25 | Loss: 0.00921689
Iteration 3/25 | Loss: 0.00837674
Iteration 4/25 | Loss: 0.00837674
Iteration 5/25 | Loss: 0.00837674
Iteration 6/25 | Loss: 0.00837674
Iteration 7/25 | Loss: 0.00837674
Iteration 8/25 | Loss: 0.00837674
Iteration 9/25 | Loss: 0.00837674
Iteration 10/25 | Loss: 0.00837674
Iteration 11/25 | Loss: 0.00837674
Iteration 12/25 | Loss: 0.00837674
Iteration 13/25 | Loss: 0.00837674
Iteration 14/25 | Loss: 0.00837674
Iteration 15/25 | Loss: 0.00837674
Iteration 16/25 | Loss: 0.00837674
Iteration 17/25 | Loss: 0.00837674
Iteration 18/25 | Loss: 0.00837674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00837673619389534, 0.00837673619389534, 0.00837673619389534, 0.00837673619389534, 0.00837673619389534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00837673619389534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00837674
Iteration 2/1000 | Loss: 0.00364539
Iteration 3/1000 | Loss: 0.00150400
Iteration 4/1000 | Loss: 0.01947440
Iteration 5/1000 | Loss: 0.00176543
Iteration 6/1000 | Loss: 0.00749136
Iteration 7/1000 | Loss: 0.00196455
Iteration 8/1000 | Loss: 0.00501186
Iteration 9/1000 | Loss: 0.00456622
Iteration 10/1000 | Loss: 0.00397398
Iteration 11/1000 | Loss: 0.00264276
Iteration 12/1000 | Loss: 0.00115717
Iteration 13/1000 | Loss: 0.00347826
Iteration 14/1000 | Loss: 0.00682364
Iteration 15/1000 | Loss: 0.00272277
Iteration 16/1000 | Loss: 0.00467591
Iteration 17/1000 | Loss: 0.00686655
Iteration 18/1000 | Loss: 0.00376859
Iteration 19/1000 | Loss: 0.00267931
Iteration 20/1000 | Loss: 0.00357464
Iteration 21/1000 | Loss: 0.00337739
Iteration 22/1000 | Loss: 0.00304774
Iteration 23/1000 | Loss: 0.00408532
Iteration 24/1000 | Loss: 0.00144416
Iteration 25/1000 | Loss: 0.00112546
Iteration 26/1000 | Loss: 0.00077384
Iteration 27/1000 | Loss: 0.00126151
Iteration 28/1000 | Loss: 0.00090591
Iteration 29/1000 | Loss: 0.00063515
Iteration 30/1000 | Loss: 0.00068950
Iteration 31/1000 | Loss: 0.00066952
Iteration 32/1000 | Loss: 0.00113233
Iteration 33/1000 | Loss: 0.00094285
Iteration 34/1000 | Loss: 0.00108609
Iteration 35/1000 | Loss: 0.00092374
Iteration 36/1000 | Loss: 0.00119105
Iteration 37/1000 | Loss: 0.00071968
Iteration 38/1000 | Loss: 0.00044176
Iteration 39/1000 | Loss: 0.00094377
Iteration 40/1000 | Loss: 0.00045624
Iteration 41/1000 | Loss: 0.00144066
Iteration 42/1000 | Loss: 0.00358147
Iteration 43/1000 | Loss: 0.00077783
Iteration 44/1000 | Loss: 0.00077172
Iteration 45/1000 | Loss: 0.00127212
Iteration 46/1000 | Loss: 0.00070347
Iteration 47/1000 | Loss: 0.00065314
Iteration 48/1000 | Loss: 0.00083063
Iteration 49/1000 | Loss: 0.00146820
Iteration 50/1000 | Loss: 0.00114097
Iteration 51/1000 | Loss: 0.00061983
Iteration 52/1000 | Loss: 0.00151600
Iteration 53/1000 | Loss: 0.00074923
Iteration 54/1000 | Loss: 0.00115622
Iteration 55/1000 | Loss: 0.00087552
Iteration 56/1000 | Loss: 0.00052056
Iteration 57/1000 | Loss: 0.00096245
Iteration 58/1000 | Loss: 0.00075192
Iteration 59/1000 | Loss: 0.00060328
Iteration 60/1000 | Loss: 0.00048543
Iteration 61/1000 | Loss: 0.00045761
Iteration 62/1000 | Loss: 0.00050046
Iteration 63/1000 | Loss: 0.00042796
Iteration 64/1000 | Loss: 0.00053467
Iteration 65/1000 | Loss: 0.00028241
Iteration 66/1000 | Loss: 0.00097352
Iteration 67/1000 | Loss: 0.00141329
Iteration 68/1000 | Loss: 0.00097913
Iteration 69/1000 | Loss: 0.00025508
Iteration 70/1000 | Loss: 0.00052950
Iteration 71/1000 | Loss: 0.00117934
Iteration 72/1000 | Loss: 0.00045028
Iteration 73/1000 | Loss: 0.00040489
Iteration 74/1000 | Loss: 0.00050181
Iteration 75/1000 | Loss: 0.00026298
Iteration 76/1000 | Loss: 0.00024766
Iteration 77/1000 | Loss: 0.00023651
Iteration 78/1000 | Loss: 0.00023194
Iteration 79/1000 | Loss: 0.00022731
Iteration 80/1000 | Loss: 0.00069300
Iteration 81/1000 | Loss: 0.00028924
Iteration 82/1000 | Loss: 0.00022812
Iteration 83/1000 | Loss: 0.00046762
Iteration 84/1000 | Loss: 0.00040313
Iteration 85/1000 | Loss: 0.00023937
Iteration 86/1000 | Loss: 0.00059669
Iteration 87/1000 | Loss: 0.00024126
Iteration 88/1000 | Loss: 0.00022150
Iteration 89/1000 | Loss: 0.00027619
Iteration 90/1000 | Loss: 0.00022019
Iteration 91/1000 | Loss: 0.00059996
Iteration 92/1000 | Loss: 0.00077403
Iteration 93/1000 | Loss: 0.00055912
Iteration 94/1000 | Loss: 0.00056111
Iteration 95/1000 | Loss: 0.00053469
Iteration 96/1000 | Loss: 0.00021279
Iteration 97/1000 | Loss: 0.00021051
Iteration 98/1000 | Loss: 0.00209852
Iteration 99/1000 | Loss: 0.01615963
Iteration 100/1000 | Loss: 0.00290998
Iteration 101/1000 | Loss: 0.00123189
Iteration 102/1000 | Loss: 0.00049425
Iteration 103/1000 | Loss: 0.00031019
Iteration 104/1000 | Loss: 0.00036516
Iteration 105/1000 | Loss: 0.00095171
Iteration 106/1000 | Loss: 0.00061266
Iteration 107/1000 | Loss: 0.00034432
Iteration 108/1000 | Loss: 0.00142298
Iteration 109/1000 | Loss: 0.00108530
Iteration 110/1000 | Loss: 0.00111883
Iteration 111/1000 | Loss: 0.00050940
Iteration 112/1000 | Loss: 0.00014154
Iteration 113/1000 | Loss: 0.00079700
Iteration 114/1000 | Loss: 0.00031985
Iteration 115/1000 | Loss: 0.00073220
Iteration 116/1000 | Loss: 0.00029434
Iteration 117/1000 | Loss: 0.00043354
Iteration 118/1000 | Loss: 0.00057984
Iteration 119/1000 | Loss: 0.00053826
Iteration 120/1000 | Loss: 0.00010637
Iteration 121/1000 | Loss: 0.00013191
Iteration 122/1000 | Loss: 0.00040932
Iteration 123/1000 | Loss: 0.00015538
Iteration 124/1000 | Loss: 0.00024586
Iteration 125/1000 | Loss: 0.00042755
Iteration 126/1000 | Loss: 0.00044883
Iteration 127/1000 | Loss: 0.00040721
Iteration 128/1000 | Loss: 0.00009842
Iteration 129/1000 | Loss: 0.00008819
Iteration 130/1000 | Loss: 0.00008214
Iteration 131/1000 | Loss: 0.00007930
Iteration 132/1000 | Loss: 0.00007741
Iteration 133/1000 | Loss: 0.00007628
Iteration 134/1000 | Loss: 0.00007527
Iteration 135/1000 | Loss: 0.00007450
Iteration 136/1000 | Loss: 0.00007389
Iteration 137/1000 | Loss: 0.00007332
Iteration 138/1000 | Loss: 0.00007283
Iteration 139/1000 | Loss: 0.00007234
Iteration 140/1000 | Loss: 0.00007203
Iteration 141/1000 | Loss: 0.00009009
Iteration 142/1000 | Loss: 0.00008590
Iteration 143/1000 | Loss: 0.00009394
Iteration 144/1000 | Loss: 0.00008353
Iteration 145/1000 | Loss: 0.00008277
Iteration 146/1000 | Loss: 0.00009168
Iteration 147/1000 | Loss: 0.00009980
Iteration 148/1000 | Loss: 0.00009395
Iteration 149/1000 | Loss: 0.00009189
Iteration 150/1000 | Loss: 0.00008658
Iteration 151/1000 | Loss: 0.00010171
Iteration 152/1000 | Loss: 0.00009074
Iteration 153/1000 | Loss: 0.00009996
Iteration 154/1000 | Loss: 0.00009516
Iteration 155/1000 | Loss: 0.00009261
Iteration 156/1000 | Loss: 0.00009246
Iteration 157/1000 | Loss: 0.00009545
Iteration 158/1000 | Loss: 0.00009396
Iteration 159/1000 | Loss: 0.00009345
Iteration 160/1000 | Loss: 0.00008825
Iteration 161/1000 | Loss: 0.00008907
Iteration 162/1000 | Loss: 0.00009251
Iteration 163/1000 | Loss: 0.00009491
Iteration 164/1000 | Loss: 0.00009494
Iteration 165/1000 | Loss: 0.00009060
Iteration 166/1000 | Loss: 0.00009091
Iteration 167/1000 | Loss: 0.00008949
Iteration 168/1000 | Loss: 0.00009510
Iteration 169/1000 | Loss: 0.00008537
Iteration 170/1000 | Loss: 0.00009385
Iteration 171/1000 | Loss: 0.00008964
Iteration 172/1000 | Loss: 0.00009022
Iteration 173/1000 | Loss: 0.00009823
Iteration 174/1000 | Loss: 0.00011942
Iteration 175/1000 | Loss: 0.00010452
Iteration 176/1000 | Loss: 0.00009817
Iteration 177/1000 | Loss: 0.00009250
Iteration 178/1000 | Loss: 0.00009108
Iteration 179/1000 | Loss: 0.00009061
Iteration 180/1000 | Loss: 0.00008658
Iteration 181/1000 | Loss: 0.00009050
Iteration 182/1000 | Loss: 0.00008510
Iteration 183/1000 | Loss: 0.00008125
Iteration 184/1000 | Loss: 0.00008877
Iteration 185/1000 | Loss: 0.00008339
Iteration 186/1000 | Loss: 0.00008776
Iteration 187/1000 | Loss: 0.00009286
Iteration 188/1000 | Loss: 0.00009238
Iteration 189/1000 | Loss: 0.00010395
Iteration 190/1000 | Loss: 0.00008713
Iteration 191/1000 | Loss: 0.00009493
Iteration 192/1000 | Loss: 0.00009243
Iteration 193/1000 | Loss: 0.00011098
Iteration 194/1000 | Loss: 0.00007808
Iteration 195/1000 | Loss: 0.00007239
Iteration 196/1000 | Loss: 0.00007191
Iteration 197/1000 | Loss: 0.00007167
Iteration 198/1000 | Loss: 0.00007144
Iteration 199/1000 | Loss: 0.00007128
Iteration 200/1000 | Loss: 0.00007125
Iteration 201/1000 | Loss: 0.00007124
Iteration 202/1000 | Loss: 0.00007124
Iteration 203/1000 | Loss: 0.00007124
Iteration 204/1000 | Loss: 0.00007124
Iteration 205/1000 | Loss: 0.00007124
Iteration 206/1000 | Loss: 0.00007124
Iteration 207/1000 | Loss: 0.00007123
Iteration 208/1000 | Loss: 0.00007122
Iteration 209/1000 | Loss: 0.00007120
Iteration 210/1000 | Loss: 0.00007120
Iteration 211/1000 | Loss: 0.00007119
Iteration 212/1000 | Loss: 0.00007119
Iteration 213/1000 | Loss: 0.00007119
Iteration 214/1000 | Loss: 0.00007118
Iteration 215/1000 | Loss: 0.00007118
Iteration 216/1000 | Loss: 0.00007118
Iteration 217/1000 | Loss: 0.00007118
Iteration 218/1000 | Loss: 0.00007118
Iteration 219/1000 | Loss: 0.00007118
Iteration 220/1000 | Loss: 0.00007118
Iteration 221/1000 | Loss: 0.00007118
Iteration 222/1000 | Loss: 0.00007117
Iteration 223/1000 | Loss: 0.00007117
Iteration 224/1000 | Loss: 0.00007117
Iteration 225/1000 | Loss: 0.00007117
Iteration 226/1000 | Loss: 0.00007117
Iteration 227/1000 | Loss: 0.00007117
Iteration 228/1000 | Loss: 0.00007117
Iteration 229/1000 | Loss: 0.00007117
Iteration 230/1000 | Loss: 0.00007117
Iteration 231/1000 | Loss: 0.00007117
Iteration 232/1000 | Loss: 0.00007116
Iteration 233/1000 | Loss: 0.00007116
Iteration 234/1000 | Loss: 0.00007116
Iteration 235/1000 | Loss: 0.00007116
Iteration 236/1000 | Loss: 0.00007116
Iteration 237/1000 | Loss: 0.00007116
Iteration 238/1000 | Loss: 0.00007116
Iteration 239/1000 | Loss: 0.00007116
Iteration 240/1000 | Loss: 0.00007115
Iteration 241/1000 | Loss: 0.00007115
Iteration 242/1000 | Loss: 0.00007115
Iteration 243/1000 | Loss: 0.00007115
Iteration 244/1000 | Loss: 0.00007114
Iteration 245/1000 | Loss: 0.00007114
Iteration 246/1000 | Loss: 0.00007114
Iteration 247/1000 | Loss: 0.00007114
Iteration 248/1000 | Loss: 0.00007114
Iteration 249/1000 | Loss: 0.00007113
Iteration 250/1000 | Loss: 0.00007113
Iteration 251/1000 | Loss: 0.00007113
Iteration 252/1000 | Loss: 0.00007112
Iteration 253/1000 | Loss: 0.00007112
Iteration 254/1000 | Loss: 0.00007112
Iteration 255/1000 | Loss: 0.00007112
Iteration 256/1000 | Loss: 0.00007112
Iteration 257/1000 | Loss: 0.00007112
Iteration 258/1000 | Loss: 0.00007112
Iteration 259/1000 | Loss: 0.00007112
Iteration 260/1000 | Loss: 0.00007112
Iteration 261/1000 | Loss: 0.00007112
Iteration 262/1000 | Loss: 0.00007112
Iteration 263/1000 | Loss: 0.00007112
Iteration 264/1000 | Loss: 0.00007111
Iteration 265/1000 | Loss: 0.00007111
Iteration 266/1000 | Loss: 0.00007111
Iteration 267/1000 | Loss: 0.00007111
Iteration 268/1000 | Loss: 0.00007111
Iteration 269/1000 | Loss: 0.00007111
Iteration 270/1000 | Loss: 0.00007111
Iteration 271/1000 | Loss: 0.00007111
Iteration 272/1000 | Loss: 0.00007111
Iteration 273/1000 | Loss: 0.00007111
Iteration 274/1000 | Loss: 0.00007111
Iteration 275/1000 | Loss: 0.00007111
Iteration 276/1000 | Loss: 0.00007110
Iteration 277/1000 | Loss: 0.00007110
Iteration 278/1000 | Loss: 0.00007110
Iteration 279/1000 | Loss: 0.00007110
Iteration 280/1000 | Loss: 0.00007110
Iteration 281/1000 | Loss: 0.00007110
Iteration 282/1000 | Loss: 0.00007110
Iteration 283/1000 | Loss: 0.00007110
Iteration 284/1000 | Loss: 0.00007110
Iteration 285/1000 | Loss: 0.00007110
Iteration 286/1000 | Loss: 0.00007110
Iteration 287/1000 | Loss: 0.00007110
Iteration 288/1000 | Loss: 0.00007110
Iteration 289/1000 | Loss: 0.00007110
Iteration 290/1000 | Loss: 0.00007110
Iteration 291/1000 | Loss: 0.00007110
Iteration 292/1000 | Loss: 0.00007110
Iteration 293/1000 | Loss: 0.00007110
Iteration 294/1000 | Loss: 0.00007109
Iteration 295/1000 | Loss: 0.00007109
Iteration 296/1000 | Loss: 0.00007109
Iteration 297/1000 | Loss: 0.00007109
Iteration 298/1000 | Loss: 0.00007109
Iteration 299/1000 | Loss: 0.00007109
Iteration 300/1000 | Loss: 0.00007109
Iteration 301/1000 | Loss: 0.00007109
Iteration 302/1000 | Loss: 0.00007109
Iteration 303/1000 | Loss: 0.00007108
Iteration 304/1000 | Loss: 0.00007108
Iteration 305/1000 | Loss: 0.00007108
Iteration 306/1000 | Loss: 0.00007108
Iteration 307/1000 | Loss: 0.00007108
Iteration 308/1000 | Loss: 0.00007108
Iteration 309/1000 | Loss: 0.00007108
Iteration 310/1000 | Loss: 0.00007108
Iteration 311/1000 | Loss: 0.00007108
Iteration 312/1000 | Loss: 0.00007108
Iteration 313/1000 | Loss: 0.00007107
Iteration 314/1000 | Loss: 0.00007107
Iteration 315/1000 | Loss: 0.00007107
Iteration 316/1000 | Loss: 0.00007107
Iteration 317/1000 | Loss: 0.00007107
Iteration 318/1000 | Loss: 0.00007107
Iteration 319/1000 | Loss: 0.00007107
Iteration 320/1000 | Loss: 0.00007107
Iteration 321/1000 | Loss: 0.00007107
Iteration 322/1000 | Loss: 0.00007107
Iteration 323/1000 | Loss: 0.00007107
Iteration 324/1000 | Loss: 0.00007107
Iteration 325/1000 | Loss: 0.00007107
Iteration 326/1000 | Loss: 0.00007107
Iteration 327/1000 | Loss: 0.00007107
Iteration 328/1000 | Loss: 0.00007106
Iteration 329/1000 | Loss: 0.00007106
Iteration 330/1000 | Loss: 0.00007106
Iteration 331/1000 | Loss: 0.00007106
Iteration 332/1000 | Loss: 0.00007106
Iteration 333/1000 | Loss: 0.00007106
Iteration 334/1000 | Loss: 0.00007106
Iteration 335/1000 | Loss: 0.00007106
Iteration 336/1000 | Loss: 0.00007106
Iteration 337/1000 | Loss: 0.00007106
Iteration 338/1000 | Loss: 0.00007106
Iteration 339/1000 | Loss: 0.00007106
Iteration 340/1000 | Loss: 0.00007106
Iteration 341/1000 | Loss: 0.00007105
Iteration 342/1000 | Loss: 0.00007105
Iteration 343/1000 | Loss: 0.00007105
Iteration 344/1000 | Loss: 0.00007105
Iteration 345/1000 | Loss: 0.00007105
Iteration 346/1000 | Loss: 0.00007105
Iteration 347/1000 | Loss: 0.00007105
Iteration 348/1000 | Loss: 0.00007105
Iteration 349/1000 | Loss: 0.00007104
Iteration 350/1000 | Loss: 0.00007104
Iteration 351/1000 | Loss: 0.00007104
Iteration 352/1000 | Loss: 0.00007104
Iteration 353/1000 | Loss: 0.00007104
Iteration 354/1000 | Loss: 0.00007104
Iteration 355/1000 | Loss: 0.00007104
Iteration 356/1000 | Loss: 0.00007104
Iteration 357/1000 | Loss: 0.00007104
Iteration 358/1000 | Loss: 0.00007104
Iteration 359/1000 | Loss: 0.00007104
Iteration 360/1000 | Loss: 0.00007104
Iteration 361/1000 | Loss: 0.00007104
Iteration 362/1000 | Loss: 0.00007104
Iteration 363/1000 | Loss: 0.00007104
Iteration 364/1000 | Loss: 0.00007104
Iteration 365/1000 | Loss: 0.00007104
Iteration 366/1000 | Loss: 0.00007104
Iteration 367/1000 | Loss: 0.00007104
Iteration 368/1000 | Loss: 0.00007104
Iteration 369/1000 | Loss: 0.00007104
Iteration 370/1000 | Loss: 0.00007104
Iteration 371/1000 | Loss: 0.00007104
Iteration 372/1000 | Loss: 0.00007104
Iteration 373/1000 | Loss: 0.00007104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [7.103843381628394e-05, 7.103843381628394e-05, 7.103843381628394e-05, 7.103843381628394e-05, 7.103843381628394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.103843381628394e-05

Optimization complete. Final v2v error: 6.623845100402832 mm

Highest mean error: 14.03895378112793 mm for frame 41

Lowest mean error: 5.626344680786133 mm for frame 17

Saving results

Total time: 359.56574177742004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130235
Iteration 2/25 | Loss: 0.00282685
Iteration 3/25 | Loss: 0.00198503
Iteration 4/25 | Loss: 0.00197918
Iteration 5/25 | Loss: 0.00172894
Iteration 6/25 | Loss: 0.00167695
Iteration 7/25 | Loss: 0.00148670
Iteration 8/25 | Loss: 0.00147606
Iteration 9/25 | Loss: 0.00143400
Iteration 10/25 | Loss: 0.00143731
Iteration 11/25 | Loss: 0.00142205
Iteration 12/25 | Loss: 0.00143038
Iteration 13/25 | Loss: 0.00142501
Iteration 14/25 | Loss: 0.00142201
Iteration 15/25 | Loss: 0.00141686
Iteration 16/25 | Loss: 0.00141683
Iteration 17/25 | Loss: 0.00141659
Iteration 18/25 | Loss: 0.00141558
Iteration 19/25 | Loss: 0.00141542
Iteration 20/25 | Loss: 0.00141515
Iteration 21/25 | Loss: 0.00141562
Iteration 22/25 | Loss: 0.00141543
Iteration 23/25 | Loss: 0.00141468
Iteration 24/25 | Loss: 0.00141450
Iteration 25/25 | Loss: 0.00141450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42626524
Iteration 2/25 | Loss: 0.00140377
Iteration 3/25 | Loss: 0.00140376
Iteration 4/25 | Loss: 0.00140376
Iteration 5/25 | Loss: 0.00140376
Iteration 6/25 | Loss: 0.00140376
Iteration 7/25 | Loss: 0.00140376
Iteration 8/25 | Loss: 0.00140376
Iteration 9/25 | Loss: 0.00140376
Iteration 10/25 | Loss: 0.00140376
Iteration 11/25 | Loss: 0.00140376
Iteration 12/25 | Loss: 0.00140376
Iteration 13/25 | Loss: 0.00140376
Iteration 14/25 | Loss: 0.00140376
Iteration 15/25 | Loss: 0.00140376
Iteration 16/25 | Loss: 0.00140376
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014037595828995109, 0.0014037595828995109, 0.0014037595828995109, 0.0014037595828995109, 0.0014037595828995109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014037595828995109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140376
Iteration 2/1000 | Loss: 0.00012866
Iteration 3/1000 | Loss: 0.00015971
Iteration 4/1000 | Loss: 0.00008245
Iteration 5/1000 | Loss: 0.00009263
Iteration 6/1000 | Loss: 0.00010123
Iteration 7/1000 | Loss: 0.00006010
Iteration 8/1000 | Loss: 0.00009362
Iteration 9/1000 | Loss: 0.00004599
Iteration 10/1000 | Loss: 0.00008554
Iteration 11/1000 | Loss: 0.00008301
Iteration 12/1000 | Loss: 0.00007072
Iteration 13/1000 | Loss: 0.00005504
Iteration 14/1000 | Loss: 0.00004453
Iteration 15/1000 | Loss: 0.00004486
Iteration 16/1000 | Loss: 0.00004837
Iteration 17/1000 | Loss: 0.00032017
Iteration 18/1000 | Loss: 0.00006076
Iteration 19/1000 | Loss: 0.00004433
Iteration 20/1000 | Loss: 0.00008149
Iteration 21/1000 | Loss: 0.00005650
Iteration 22/1000 | Loss: 0.00004423
Iteration 23/1000 | Loss: 0.00004410
Iteration 24/1000 | Loss: 0.00004410
Iteration 25/1000 | Loss: 0.00004410
Iteration 26/1000 | Loss: 0.00004409
Iteration 27/1000 | Loss: 0.00004409
Iteration 28/1000 | Loss: 0.00004409
Iteration 29/1000 | Loss: 0.00005098
Iteration 30/1000 | Loss: 0.00006786
Iteration 31/1000 | Loss: 0.00004411
Iteration 32/1000 | Loss: 0.00005739
Iteration 33/1000 | Loss: 0.00005663
Iteration 34/1000 | Loss: 0.00012621
Iteration 35/1000 | Loss: 0.00006252
Iteration 36/1000 | Loss: 0.00007520
Iteration 37/1000 | Loss: 0.00005400
Iteration 38/1000 | Loss: 0.00004397
Iteration 39/1000 | Loss: 0.00004397
Iteration 40/1000 | Loss: 0.00004397
Iteration 41/1000 | Loss: 0.00004397
Iteration 42/1000 | Loss: 0.00004397
Iteration 43/1000 | Loss: 0.00004397
Iteration 44/1000 | Loss: 0.00004397
Iteration 45/1000 | Loss: 0.00004397
Iteration 46/1000 | Loss: 0.00004397
Iteration 47/1000 | Loss: 0.00004397
Iteration 48/1000 | Loss: 0.00004397
Iteration 49/1000 | Loss: 0.00005429
Iteration 50/1000 | Loss: 0.00004404
Iteration 51/1000 | Loss: 0.00004964
Iteration 52/1000 | Loss: 0.00007065
Iteration 53/1000 | Loss: 0.00004562
Iteration 54/1000 | Loss: 0.00005620
Iteration 55/1000 | Loss: 0.00004843
Iteration 56/1000 | Loss: 0.00005057
Iteration 57/1000 | Loss: 0.00006647
Iteration 58/1000 | Loss: 0.00004960
Iteration 59/1000 | Loss: 0.00004747
Iteration 60/1000 | Loss: 0.00004635
Iteration 61/1000 | Loss: 0.00004430
Iteration 62/1000 | Loss: 0.00004430
Iteration 63/1000 | Loss: 0.00004397
Iteration 64/1000 | Loss: 0.00004396
Iteration 65/1000 | Loss: 0.00004396
Iteration 66/1000 | Loss: 0.00004396
Iteration 67/1000 | Loss: 0.00004396
Iteration 68/1000 | Loss: 0.00004396
Iteration 69/1000 | Loss: 0.00004396
Iteration 70/1000 | Loss: 0.00004563
Iteration 71/1000 | Loss: 0.00004416
Iteration 72/1000 | Loss: 0.00005549
Iteration 73/1000 | Loss: 0.00010095
Iteration 74/1000 | Loss: 0.00005434
Iteration 75/1000 | Loss: 0.00004482
Iteration 76/1000 | Loss: 0.00004542
Iteration 77/1000 | Loss: 0.00004404
Iteration 78/1000 | Loss: 0.00004397
Iteration 79/1000 | Loss: 0.00004396
Iteration 80/1000 | Loss: 0.00004394
Iteration 81/1000 | Loss: 0.00004394
Iteration 82/1000 | Loss: 0.00004394
Iteration 83/1000 | Loss: 0.00004394
Iteration 84/1000 | Loss: 0.00004394
Iteration 85/1000 | Loss: 0.00004394
Iteration 86/1000 | Loss: 0.00004394
Iteration 87/1000 | Loss: 0.00004394
Iteration 88/1000 | Loss: 0.00004394
Iteration 89/1000 | Loss: 0.00004394
Iteration 90/1000 | Loss: 0.00004394
Iteration 91/1000 | Loss: 0.00004394
Iteration 92/1000 | Loss: 0.00004394
Iteration 93/1000 | Loss: 0.00004394
Iteration 94/1000 | Loss: 0.00004394
Iteration 95/1000 | Loss: 0.00004394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [4.3937725422438234e-05, 4.3937725422438234e-05, 4.3937725422438234e-05, 4.3937725422438234e-05, 4.3937725422438234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3937725422438234e-05

Optimization complete. Final v2v error: 5.287795066833496 mm

Highest mean error: 16.372325897216797 mm for frame 35

Lowest mean error: 4.638906478881836 mm for frame 209

Saving results

Total time: 118.85445523262024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106266
Iteration 2/25 | Loss: 0.01106266
Iteration 3/25 | Loss: 0.01106266
Iteration 4/25 | Loss: 0.01106266
Iteration 5/25 | Loss: 0.01106266
Iteration 6/25 | Loss: 0.01106266
Iteration 7/25 | Loss: 0.01106266
Iteration 8/25 | Loss: 0.01106266
Iteration 9/25 | Loss: 0.01106266
Iteration 10/25 | Loss: 0.01106266
Iteration 11/25 | Loss: 0.01106266
Iteration 12/25 | Loss: 0.01106266
Iteration 13/25 | Loss: 0.01106265
Iteration 14/25 | Loss: 0.01106265
Iteration 15/25 | Loss: 0.01106265
Iteration 16/25 | Loss: 0.01106265
Iteration 17/25 | Loss: 0.01106265
Iteration 18/25 | Loss: 0.01106265
Iteration 19/25 | Loss: 0.01106265
Iteration 20/25 | Loss: 0.01106265
Iteration 21/25 | Loss: 0.01106265
Iteration 22/25 | Loss: 0.01106265
Iteration 23/25 | Loss: 0.01106265
Iteration 24/25 | Loss: 0.01106264
Iteration 25/25 | Loss: 0.01106264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81218612
Iteration 2/25 | Loss: 0.13241200
Iteration 3/25 | Loss: 0.12973884
Iteration 4/25 | Loss: 0.12646787
Iteration 5/25 | Loss: 0.12638739
Iteration 6/25 | Loss: 0.12638737
Iteration 7/25 | Loss: 0.12638737
Iteration 8/25 | Loss: 0.12638736
Iteration 9/25 | Loss: 0.12638734
Iteration 10/25 | Loss: 0.12638734
Iteration 11/25 | Loss: 0.12638734
Iteration 12/25 | Loss: 0.12638734
Iteration 13/25 | Loss: 0.12638734
Iteration 14/25 | Loss: 0.12638734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.1263873428106308, 0.1263873428106308, 0.1263873428106308, 0.1263873428106308, 0.1263873428106308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1263873428106308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12638734
Iteration 2/1000 | Loss: 0.01164310
Iteration 3/1000 | Loss: 0.00492464
Iteration 4/1000 | Loss: 0.00294140
Iteration 5/1000 | Loss: 0.00112228
Iteration 6/1000 | Loss: 0.00079496
Iteration 7/1000 | Loss: 0.00141514
Iteration 8/1000 | Loss: 0.00014192
Iteration 9/1000 | Loss: 0.00067218
Iteration 10/1000 | Loss: 0.00039841
Iteration 11/1000 | Loss: 0.00065100
Iteration 12/1000 | Loss: 0.00049562
Iteration 13/1000 | Loss: 0.00051976
Iteration 14/1000 | Loss: 0.00040322
Iteration 15/1000 | Loss: 0.00293853
Iteration 16/1000 | Loss: 0.00067033
Iteration 17/1000 | Loss: 0.00015305
Iteration 18/1000 | Loss: 0.00033587
Iteration 19/1000 | Loss: 0.00076467
Iteration 20/1000 | Loss: 0.00079032
Iteration 21/1000 | Loss: 0.00057181
Iteration 22/1000 | Loss: 0.00040111
Iteration 23/1000 | Loss: 0.00005557
Iteration 24/1000 | Loss: 0.00028867
Iteration 25/1000 | Loss: 0.00029205
Iteration 26/1000 | Loss: 0.00013074
Iteration 27/1000 | Loss: 0.00004495
Iteration 28/1000 | Loss: 0.00004367
Iteration 29/1000 | Loss: 0.00007943
Iteration 30/1000 | Loss: 0.00004643
Iteration 31/1000 | Loss: 0.00050804
Iteration 32/1000 | Loss: 0.00004130
Iteration 33/1000 | Loss: 0.00004003
Iteration 34/1000 | Loss: 0.00007242
Iteration 35/1000 | Loss: 0.00003920
Iteration 36/1000 | Loss: 0.00003869
Iteration 37/1000 | Loss: 0.00003869
Iteration 38/1000 | Loss: 0.00003825
Iteration 39/1000 | Loss: 0.00003866
Iteration 40/1000 | Loss: 0.00011321
Iteration 41/1000 | Loss: 0.00003777
Iteration 42/1000 | Loss: 0.00005282
Iteration 43/1000 | Loss: 0.00003739
Iteration 44/1000 | Loss: 0.00008093
Iteration 45/1000 | Loss: 0.00005634
Iteration 46/1000 | Loss: 0.00003695
Iteration 47/1000 | Loss: 0.00003685
Iteration 48/1000 | Loss: 0.00003677
Iteration 49/1000 | Loss: 0.00009262
Iteration 50/1000 | Loss: 0.00007071
Iteration 51/1000 | Loss: 0.00004365
Iteration 52/1000 | Loss: 0.00004398
Iteration 53/1000 | Loss: 0.00003664
Iteration 54/1000 | Loss: 0.00003661
Iteration 55/1000 | Loss: 0.00003660
Iteration 56/1000 | Loss: 0.00003660
Iteration 57/1000 | Loss: 0.00003660
Iteration 58/1000 | Loss: 0.00003660
Iteration 59/1000 | Loss: 0.00003660
Iteration 60/1000 | Loss: 0.00003660
Iteration 61/1000 | Loss: 0.00003660
Iteration 62/1000 | Loss: 0.00003660
Iteration 63/1000 | Loss: 0.00003660
Iteration 64/1000 | Loss: 0.00003660
Iteration 65/1000 | Loss: 0.00003659
Iteration 66/1000 | Loss: 0.00003659
Iteration 67/1000 | Loss: 0.00003659
Iteration 68/1000 | Loss: 0.00003659
Iteration 69/1000 | Loss: 0.00003659
Iteration 70/1000 | Loss: 0.00003659
Iteration 71/1000 | Loss: 0.00003659
Iteration 72/1000 | Loss: 0.00003659
Iteration 73/1000 | Loss: 0.00004746
Iteration 74/1000 | Loss: 0.00003656
Iteration 75/1000 | Loss: 0.00003652
Iteration 76/1000 | Loss: 0.00003652
Iteration 77/1000 | Loss: 0.00003652
Iteration 78/1000 | Loss: 0.00003652
Iteration 79/1000 | Loss: 0.00003651
Iteration 80/1000 | Loss: 0.00003651
Iteration 81/1000 | Loss: 0.00003651
Iteration 82/1000 | Loss: 0.00003651
Iteration 83/1000 | Loss: 0.00003650
Iteration 84/1000 | Loss: 0.00003650
Iteration 85/1000 | Loss: 0.00003650
Iteration 86/1000 | Loss: 0.00003650
Iteration 87/1000 | Loss: 0.00003650
Iteration 88/1000 | Loss: 0.00003650
Iteration 89/1000 | Loss: 0.00003650
Iteration 90/1000 | Loss: 0.00003650
Iteration 91/1000 | Loss: 0.00003650
Iteration 92/1000 | Loss: 0.00003650
Iteration 93/1000 | Loss: 0.00003650
Iteration 94/1000 | Loss: 0.00003649
Iteration 95/1000 | Loss: 0.00003649
Iteration 96/1000 | Loss: 0.00003649
Iteration 97/1000 | Loss: 0.00003649
Iteration 98/1000 | Loss: 0.00003649
Iteration 99/1000 | Loss: 0.00003649
Iteration 100/1000 | Loss: 0.00003649
Iteration 101/1000 | Loss: 0.00003649
Iteration 102/1000 | Loss: 0.00003649
Iteration 103/1000 | Loss: 0.00003649
Iteration 104/1000 | Loss: 0.00003649
Iteration 105/1000 | Loss: 0.00003649
Iteration 106/1000 | Loss: 0.00003649
Iteration 107/1000 | Loss: 0.00003649
Iteration 108/1000 | Loss: 0.00003649
Iteration 109/1000 | Loss: 0.00003649
Iteration 110/1000 | Loss: 0.00003649
Iteration 111/1000 | Loss: 0.00003649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.648676283773966e-05, 3.648676283773966e-05, 3.648676283773966e-05, 3.648676283773966e-05, 3.648676283773966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.648676283773966e-05

Optimization complete. Final v2v error: 5.163999080657959 mm

Highest mean error: 11.774933815002441 mm for frame 224

Lowest mean error: 4.852715492248535 mm for frame 189

Saving results

Total time: 97.00181150436401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593310
Iteration 2/25 | Loss: 0.00178454
Iteration 3/25 | Loss: 0.00167574
Iteration 4/25 | Loss: 0.00164961
Iteration 5/25 | Loss: 0.00164128
Iteration 6/25 | Loss: 0.00163952
Iteration 7/25 | Loss: 0.00163894
Iteration 8/25 | Loss: 0.00163894
Iteration 9/25 | Loss: 0.00163894
Iteration 10/25 | Loss: 0.00163894
Iteration 11/25 | Loss: 0.00163894
Iteration 12/25 | Loss: 0.00163894
Iteration 13/25 | Loss: 0.00163894
Iteration 14/25 | Loss: 0.00163894
Iteration 15/25 | Loss: 0.00163894
Iteration 16/25 | Loss: 0.00163894
Iteration 17/25 | Loss: 0.00163894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016389377415180206, 0.0016389377415180206, 0.0016389377415180206, 0.0016389377415180206, 0.0016389377415180206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016389377415180206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44349098
Iteration 2/25 | Loss: 0.00186037
Iteration 3/25 | Loss: 0.00186037
Iteration 4/25 | Loss: 0.00186037
Iteration 5/25 | Loss: 0.00186037
Iteration 6/25 | Loss: 0.00186037
Iteration 7/25 | Loss: 0.00186037
Iteration 8/25 | Loss: 0.00186037
Iteration 9/25 | Loss: 0.00186037
Iteration 10/25 | Loss: 0.00186037
Iteration 11/25 | Loss: 0.00186037
Iteration 12/25 | Loss: 0.00186037
Iteration 13/25 | Loss: 0.00186037
Iteration 14/25 | Loss: 0.00186037
Iteration 15/25 | Loss: 0.00186037
Iteration 16/25 | Loss: 0.00186037
Iteration 17/25 | Loss: 0.00186037
Iteration 18/25 | Loss: 0.00186037
Iteration 19/25 | Loss: 0.00186037
Iteration 20/25 | Loss: 0.00186037
Iteration 21/25 | Loss: 0.00186037
Iteration 22/25 | Loss: 0.00186037
Iteration 23/25 | Loss: 0.00186037
Iteration 24/25 | Loss: 0.00186037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018603657372295856, 0.0018603657372295856, 0.0018603657372295856, 0.0018603657372295856, 0.0018603657372295856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018603657372295856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186037
Iteration 2/1000 | Loss: 0.00008812
Iteration 3/1000 | Loss: 0.00006145
Iteration 4/1000 | Loss: 0.00005245
Iteration 5/1000 | Loss: 0.00004695
Iteration 6/1000 | Loss: 0.00004423
Iteration 7/1000 | Loss: 0.00004238
Iteration 8/1000 | Loss: 0.00004067
Iteration 9/1000 | Loss: 0.00003965
Iteration 10/1000 | Loss: 0.00003874
Iteration 11/1000 | Loss: 0.00003818
Iteration 12/1000 | Loss: 0.00003777
Iteration 13/1000 | Loss: 0.00003750
Iteration 14/1000 | Loss: 0.00003721
Iteration 15/1000 | Loss: 0.00003696
Iteration 16/1000 | Loss: 0.00003678
Iteration 17/1000 | Loss: 0.00003678
Iteration 18/1000 | Loss: 0.00003667
Iteration 19/1000 | Loss: 0.00003663
Iteration 20/1000 | Loss: 0.00003660
Iteration 21/1000 | Loss: 0.00003660
Iteration 22/1000 | Loss: 0.00003659
Iteration 23/1000 | Loss: 0.00003659
Iteration 24/1000 | Loss: 0.00003658
Iteration 25/1000 | Loss: 0.00003658
Iteration 26/1000 | Loss: 0.00003656
Iteration 27/1000 | Loss: 0.00003656
Iteration 28/1000 | Loss: 0.00003655
Iteration 29/1000 | Loss: 0.00003655
Iteration 30/1000 | Loss: 0.00003655
Iteration 31/1000 | Loss: 0.00003654
Iteration 32/1000 | Loss: 0.00003654
Iteration 33/1000 | Loss: 0.00003653
Iteration 34/1000 | Loss: 0.00003653
Iteration 35/1000 | Loss: 0.00003652
Iteration 36/1000 | Loss: 0.00003651
Iteration 37/1000 | Loss: 0.00003651
Iteration 38/1000 | Loss: 0.00003650
Iteration 39/1000 | Loss: 0.00003649
Iteration 40/1000 | Loss: 0.00003649
Iteration 41/1000 | Loss: 0.00003649
Iteration 42/1000 | Loss: 0.00003648
Iteration 43/1000 | Loss: 0.00003648
Iteration 44/1000 | Loss: 0.00003647
Iteration 45/1000 | Loss: 0.00003647
Iteration 46/1000 | Loss: 0.00003646
Iteration 47/1000 | Loss: 0.00003646
Iteration 48/1000 | Loss: 0.00003646
Iteration 49/1000 | Loss: 0.00003645
Iteration 50/1000 | Loss: 0.00003644
Iteration 51/1000 | Loss: 0.00003644
Iteration 52/1000 | Loss: 0.00003643
Iteration 53/1000 | Loss: 0.00003643
Iteration 54/1000 | Loss: 0.00003642
Iteration 55/1000 | Loss: 0.00003642
Iteration 56/1000 | Loss: 0.00003642
Iteration 57/1000 | Loss: 0.00003641
Iteration 58/1000 | Loss: 0.00003641
Iteration 59/1000 | Loss: 0.00003640
Iteration 60/1000 | Loss: 0.00003640
Iteration 61/1000 | Loss: 0.00003639
Iteration 62/1000 | Loss: 0.00003638
Iteration 63/1000 | Loss: 0.00003638
Iteration 64/1000 | Loss: 0.00003636
Iteration 65/1000 | Loss: 0.00003636
Iteration 66/1000 | Loss: 0.00003636
Iteration 67/1000 | Loss: 0.00003636
Iteration 68/1000 | Loss: 0.00003635
Iteration 69/1000 | Loss: 0.00003634
Iteration 70/1000 | Loss: 0.00003634
Iteration 71/1000 | Loss: 0.00003633
Iteration 72/1000 | Loss: 0.00003633
Iteration 73/1000 | Loss: 0.00003633
Iteration 74/1000 | Loss: 0.00003632
Iteration 75/1000 | Loss: 0.00003632
Iteration 76/1000 | Loss: 0.00003632
Iteration 77/1000 | Loss: 0.00003631
Iteration 78/1000 | Loss: 0.00003631
Iteration 79/1000 | Loss: 0.00003631
Iteration 80/1000 | Loss: 0.00003630
Iteration 81/1000 | Loss: 0.00003630
Iteration 82/1000 | Loss: 0.00003630
Iteration 83/1000 | Loss: 0.00003630
Iteration 84/1000 | Loss: 0.00003629
Iteration 85/1000 | Loss: 0.00003629
Iteration 86/1000 | Loss: 0.00003629
Iteration 87/1000 | Loss: 0.00003629
Iteration 88/1000 | Loss: 0.00003629
Iteration 89/1000 | Loss: 0.00003629
Iteration 90/1000 | Loss: 0.00003628
Iteration 91/1000 | Loss: 0.00003628
Iteration 92/1000 | Loss: 0.00003628
Iteration 93/1000 | Loss: 0.00003627
Iteration 94/1000 | Loss: 0.00003627
Iteration 95/1000 | Loss: 0.00003627
Iteration 96/1000 | Loss: 0.00003626
Iteration 97/1000 | Loss: 0.00003626
Iteration 98/1000 | Loss: 0.00003626
Iteration 99/1000 | Loss: 0.00003626
Iteration 100/1000 | Loss: 0.00003625
Iteration 101/1000 | Loss: 0.00003625
Iteration 102/1000 | Loss: 0.00003625
Iteration 103/1000 | Loss: 0.00003625
Iteration 104/1000 | Loss: 0.00003624
Iteration 105/1000 | Loss: 0.00003624
Iteration 106/1000 | Loss: 0.00003624
Iteration 107/1000 | Loss: 0.00003624
Iteration 108/1000 | Loss: 0.00003624
Iteration 109/1000 | Loss: 0.00003624
Iteration 110/1000 | Loss: 0.00003624
Iteration 111/1000 | Loss: 0.00003624
Iteration 112/1000 | Loss: 0.00003624
Iteration 113/1000 | Loss: 0.00003624
Iteration 114/1000 | Loss: 0.00003623
Iteration 115/1000 | Loss: 0.00003623
Iteration 116/1000 | Loss: 0.00003623
Iteration 117/1000 | Loss: 0.00003623
Iteration 118/1000 | Loss: 0.00003622
Iteration 119/1000 | Loss: 0.00003622
Iteration 120/1000 | Loss: 0.00003622
Iteration 121/1000 | Loss: 0.00003622
Iteration 122/1000 | Loss: 0.00003622
Iteration 123/1000 | Loss: 0.00003621
Iteration 124/1000 | Loss: 0.00003621
Iteration 125/1000 | Loss: 0.00003621
Iteration 126/1000 | Loss: 0.00003621
Iteration 127/1000 | Loss: 0.00003621
Iteration 128/1000 | Loss: 0.00003620
Iteration 129/1000 | Loss: 0.00003620
Iteration 130/1000 | Loss: 0.00003620
Iteration 131/1000 | Loss: 0.00003620
Iteration 132/1000 | Loss: 0.00003620
Iteration 133/1000 | Loss: 0.00003619
Iteration 134/1000 | Loss: 0.00003619
Iteration 135/1000 | Loss: 0.00003619
Iteration 136/1000 | Loss: 0.00003619
Iteration 137/1000 | Loss: 0.00003619
Iteration 138/1000 | Loss: 0.00003619
Iteration 139/1000 | Loss: 0.00003619
Iteration 140/1000 | Loss: 0.00003619
Iteration 141/1000 | Loss: 0.00003619
Iteration 142/1000 | Loss: 0.00003619
Iteration 143/1000 | Loss: 0.00003619
Iteration 144/1000 | Loss: 0.00003619
Iteration 145/1000 | Loss: 0.00003618
Iteration 146/1000 | Loss: 0.00003618
Iteration 147/1000 | Loss: 0.00003618
Iteration 148/1000 | Loss: 0.00003618
Iteration 149/1000 | Loss: 0.00003618
Iteration 150/1000 | Loss: 0.00003618
Iteration 151/1000 | Loss: 0.00003618
Iteration 152/1000 | Loss: 0.00003617
Iteration 153/1000 | Loss: 0.00003617
Iteration 154/1000 | Loss: 0.00003617
Iteration 155/1000 | Loss: 0.00003617
Iteration 156/1000 | Loss: 0.00003617
Iteration 157/1000 | Loss: 0.00003617
Iteration 158/1000 | Loss: 0.00003617
Iteration 159/1000 | Loss: 0.00003616
Iteration 160/1000 | Loss: 0.00003616
Iteration 161/1000 | Loss: 0.00003616
Iteration 162/1000 | Loss: 0.00003616
Iteration 163/1000 | Loss: 0.00003616
Iteration 164/1000 | Loss: 0.00003616
Iteration 165/1000 | Loss: 0.00003616
Iteration 166/1000 | Loss: 0.00003616
Iteration 167/1000 | Loss: 0.00003616
Iteration 168/1000 | Loss: 0.00003616
Iteration 169/1000 | Loss: 0.00003616
Iteration 170/1000 | Loss: 0.00003616
Iteration 171/1000 | Loss: 0.00003615
Iteration 172/1000 | Loss: 0.00003615
Iteration 173/1000 | Loss: 0.00003615
Iteration 174/1000 | Loss: 0.00003615
Iteration 175/1000 | Loss: 0.00003615
Iteration 176/1000 | Loss: 0.00003615
Iteration 177/1000 | Loss: 0.00003615
Iteration 178/1000 | Loss: 0.00003615
Iteration 179/1000 | Loss: 0.00003615
Iteration 180/1000 | Loss: 0.00003615
Iteration 181/1000 | Loss: 0.00003615
Iteration 182/1000 | Loss: 0.00003615
Iteration 183/1000 | Loss: 0.00003615
Iteration 184/1000 | Loss: 0.00003615
Iteration 185/1000 | Loss: 0.00003615
Iteration 186/1000 | Loss: 0.00003615
Iteration 187/1000 | Loss: 0.00003615
Iteration 188/1000 | Loss: 0.00003615
Iteration 189/1000 | Loss: 0.00003615
Iteration 190/1000 | Loss: 0.00003615
Iteration 191/1000 | Loss: 0.00003615
Iteration 192/1000 | Loss: 0.00003615
Iteration 193/1000 | Loss: 0.00003615
Iteration 194/1000 | Loss: 0.00003615
Iteration 195/1000 | Loss: 0.00003615
Iteration 196/1000 | Loss: 0.00003615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [3.614994420786388e-05, 3.614994420786388e-05, 3.614994420786388e-05, 3.614994420786388e-05, 3.614994420786388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.614994420786388e-05

Optimization complete. Final v2v error: 5.265134811401367 mm

Highest mean error: 5.703614234924316 mm for frame 84

Lowest mean error: 4.759948253631592 mm for frame 118

Saving results

Total time: 47.08891463279724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122498
Iteration 2/25 | Loss: 0.00198715
Iteration 3/25 | Loss: 0.00165729
Iteration 4/25 | Loss: 0.00161436
Iteration 5/25 | Loss: 0.00160866
Iteration 6/25 | Loss: 0.00160669
Iteration 7/25 | Loss: 0.00160658
Iteration 8/25 | Loss: 0.00160658
Iteration 9/25 | Loss: 0.00160658
Iteration 10/25 | Loss: 0.00160658
Iteration 11/25 | Loss: 0.00160658
Iteration 12/25 | Loss: 0.00160658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016065820818766952, 0.0016065820818766952, 0.0016065820818766952, 0.0016065820818766952, 0.0016065820818766952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016065820818766952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42215884
Iteration 2/25 | Loss: 0.00160895
Iteration 3/25 | Loss: 0.00160895
Iteration 4/25 | Loss: 0.00160895
Iteration 5/25 | Loss: 0.00160895
Iteration 6/25 | Loss: 0.00160895
Iteration 7/25 | Loss: 0.00160895
Iteration 8/25 | Loss: 0.00160895
Iteration 9/25 | Loss: 0.00160895
Iteration 10/25 | Loss: 0.00160895
Iteration 11/25 | Loss: 0.00160895
Iteration 12/25 | Loss: 0.00160895
Iteration 13/25 | Loss: 0.00160895
Iteration 14/25 | Loss: 0.00160895
Iteration 15/25 | Loss: 0.00160895
Iteration 16/25 | Loss: 0.00160895
Iteration 17/25 | Loss: 0.00160895
Iteration 18/25 | Loss: 0.00160895
Iteration 19/25 | Loss: 0.00160895
Iteration 20/25 | Loss: 0.00160895
Iteration 21/25 | Loss: 0.00160895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016089524142444134, 0.0016089524142444134, 0.0016089524142444134, 0.0016089524142444134, 0.0016089524142444134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016089524142444134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160895
Iteration 2/1000 | Loss: 0.00005691
Iteration 3/1000 | Loss: 0.00004467
Iteration 4/1000 | Loss: 0.00003897
Iteration 5/1000 | Loss: 0.00003651
Iteration 6/1000 | Loss: 0.00003543
Iteration 7/1000 | Loss: 0.00003473
Iteration 8/1000 | Loss: 0.00003424
Iteration 9/1000 | Loss: 0.00003396
Iteration 10/1000 | Loss: 0.00003354
Iteration 11/1000 | Loss: 0.00003326
Iteration 12/1000 | Loss: 0.00003310
Iteration 13/1000 | Loss: 0.00003306
Iteration 14/1000 | Loss: 0.00003303
Iteration 15/1000 | Loss: 0.00003302
Iteration 16/1000 | Loss: 0.00003302
Iteration 17/1000 | Loss: 0.00003302
Iteration 18/1000 | Loss: 0.00003301
Iteration 19/1000 | Loss: 0.00003300
Iteration 20/1000 | Loss: 0.00003299
Iteration 21/1000 | Loss: 0.00003299
Iteration 22/1000 | Loss: 0.00003299
Iteration 23/1000 | Loss: 0.00003299
Iteration 24/1000 | Loss: 0.00003299
Iteration 25/1000 | Loss: 0.00003299
Iteration 26/1000 | Loss: 0.00003299
Iteration 27/1000 | Loss: 0.00003299
Iteration 28/1000 | Loss: 0.00003299
Iteration 29/1000 | Loss: 0.00003299
Iteration 30/1000 | Loss: 0.00003299
Iteration 31/1000 | Loss: 0.00003299
Iteration 32/1000 | Loss: 0.00003299
Iteration 33/1000 | Loss: 0.00003299
Iteration 34/1000 | Loss: 0.00003299
Iteration 35/1000 | Loss: 0.00003299
Iteration 36/1000 | Loss: 0.00003299
Iteration 37/1000 | Loss: 0.00003299
Iteration 38/1000 | Loss: 0.00003299
Iteration 39/1000 | Loss: 0.00003299
Iteration 40/1000 | Loss: 0.00003299
Iteration 41/1000 | Loss: 0.00003299
Iteration 42/1000 | Loss: 0.00003299
Iteration 43/1000 | Loss: 0.00003299
Iteration 44/1000 | Loss: 0.00003299
Iteration 45/1000 | Loss: 0.00003299
Iteration 46/1000 | Loss: 0.00003299
Iteration 47/1000 | Loss: 0.00003299
Iteration 48/1000 | Loss: 0.00003299
Iteration 49/1000 | Loss: 0.00003299
Iteration 50/1000 | Loss: 0.00003299
Iteration 51/1000 | Loss: 0.00003299
Iteration 52/1000 | Loss: 0.00003299
Iteration 53/1000 | Loss: 0.00003299
Iteration 54/1000 | Loss: 0.00003299
Iteration 55/1000 | Loss: 0.00003299
Iteration 56/1000 | Loss: 0.00003299
Iteration 57/1000 | Loss: 0.00003299
Iteration 58/1000 | Loss: 0.00003299
Iteration 59/1000 | Loss: 0.00003299
Iteration 60/1000 | Loss: 0.00003299
Iteration 61/1000 | Loss: 0.00003299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [3.2988438761094585e-05, 3.2988438761094585e-05, 3.2988438761094585e-05, 3.2988438761094585e-05, 3.2988438761094585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2988438761094585e-05

Optimization complete. Final v2v error: 5.027898788452148 mm

Highest mean error: 5.276878356933594 mm for frame 30

Lowest mean error: 4.592344760894775 mm for frame 10

Saving results

Total time: 27.803276777267456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510916
Iteration 2/25 | Loss: 0.00173677
Iteration 3/25 | Loss: 0.00165336
Iteration 4/25 | Loss: 0.00163522
Iteration 5/25 | Loss: 0.00162791
Iteration 6/25 | Loss: 0.00162560
Iteration 7/25 | Loss: 0.00162554
Iteration 8/25 | Loss: 0.00162554
Iteration 9/25 | Loss: 0.00162554
Iteration 10/25 | Loss: 0.00162554
Iteration 11/25 | Loss: 0.00162554
Iteration 12/25 | Loss: 0.00162554
Iteration 13/25 | Loss: 0.00162554
Iteration 14/25 | Loss: 0.00162553
Iteration 15/25 | Loss: 0.00162554
Iteration 16/25 | Loss: 0.00162553
Iteration 17/25 | Loss: 0.00162553
Iteration 18/25 | Loss: 0.00162553
Iteration 19/25 | Loss: 0.00162554
Iteration 20/25 | Loss: 0.00162554
Iteration 21/25 | Loss: 0.00162553
Iteration 22/25 | Loss: 0.00162554
Iteration 23/25 | Loss: 0.00162553
Iteration 24/25 | Loss: 0.00162554
Iteration 25/25 | Loss: 0.00162553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60715103
Iteration 2/25 | Loss: 0.00191821
Iteration 3/25 | Loss: 0.00191821
Iteration 4/25 | Loss: 0.00191821
Iteration 5/25 | Loss: 0.00191821
Iteration 6/25 | Loss: 0.00191821
Iteration 7/25 | Loss: 0.00191820
Iteration 8/25 | Loss: 0.00191820
Iteration 9/25 | Loss: 0.00191820
Iteration 10/25 | Loss: 0.00191820
Iteration 11/25 | Loss: 0.00191820
Iteration 12/25 | Loss: 0.00191820
Iteration 13/25 | Loss: 0.00191820
Iteration 14/25 | Loss: 0.00191820
Iteration 15/25 | Loss: 0.00191820
Iteration 16/25 | Loss: 0.00191820
Iteration 17/25 | Loss: 0.00191820
Iteration 18/25 | Loss: 0.00191820
Iteration 19/25 | Loss: 0.00191820
Iteration 20/25 | Loss: 0.00191820
Iteration 21/25 | Loss: 0.00191820
Iteration 22/25 | Loss: 0.00191820
Iteration 23/25 | Loss: 0.00191820
Iteration 24/25 | Loss: 0.00191820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019182040123268962, 0.0019182040123268962, 0.0019182040123268962, 0.0019182040123268962, 0.0019182040123268962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019182040123268962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191820
Iteration 2/1000 | Loss: 0.00006985
Iteration 3/1000 | Loss: 0.00005152
Iteration 4/1000 | Loss: 0.00004552
Iteration 5/1000 | Loss: 0.00004211
Iteration 6/1000 | Loss: 0.00004026
Iteration 7/1000 | Loss: 0.00003847
Iteration 8/1000 | Loss: 0.00003727
Iteration 9/1000 | Loss: 0.00003637
Iteration 10/1000 | Loss: 0.00003581
Iteration 11/1000 | Loss: 0.00003542
Iteration 12/1000 | Loss: 0.00003499
Iteration 13/1000 | Loss: 0.00003465
Iteration 14/1000 | Loss: 0.00003436
Iteration 15/1000 | Loss: 0.00003413
Iteration 16/1000 | Loss: 0.00003404
Iteration 17/1000 | Loss: 0.00003400
Iteration 18/1000 | Loss: 0.00003393
Iteration 19/1000 | Loss: 0.00003386
Iteration 20/1000 | Loss: 0.00003386
Iteration 21/1000 | Loss: 0.00003385
Iteration 22/1000 | Loss: 0.00003381
Iteration 23/1000 | Loss: 0.00003379
Iteration 24/1000 | Loss: 0.00003375
Iteration 25/1000 | Loss: 0.00003375
Iteration 26/1000 | Loss: 0.00003373
Iteration 27/1000 | Loss: 0.00003373
Iteration 28/1000 | Loss: 0.00003368
Iteration 29/1000 | Loss: 0.00003366
Iteration 30/1000 | Loss: 0.00003366
Iteration 31/1000 | Loss: 0.00003366
Iteration 32/1000 | Loss: 0.00003365
Iteration 33/1000 | Loss: 0.00003365
Iteration 34/1000 | Loss: 0.00003365
Iteration 35/1000 | Loss: 0.00003364
Iteration 36/1000 | Loss: 0.00003364
Iteration 37/1000 | Loss: 0.00003363
Iteration 38/1000 | Loss: 0.00003363
Iteration 39/1000 | Loss: 0.00003362
Iteration 40/1000 | Loss: 0.00003362
Iteration 41/1000 | Loss: 0.00003361
Iteration 42/1000 | Loss: 0.00003361
Iteration 43/1000 | Loss: 0.00003361
Iteration 44/1000 | Loss: 0.00003360
Iteration 45/1000 | Loss: 0.00003360
Iteration 46/1000 | Loss: 0.00003360
Iteration 47/1000 | Loss: 0.00003360
Iteration 48/1000 | Loss: 0.00003359
Iteration 49/1000 | Loss: 0.00003359
Iteration 50/1000 | Loss: 0.00003359
Iteration 51/1000 | Loss: 0.00003358
Iteration 52/1000 | Loss: 0.00003358
Iteration 53/1000 | Loss: 0.00003358
Iteration 54/1000 | Loss: 0.00003358
Iteration 55/1000 | Loss: 0.00003357
Iteration 56/1000 | Loss: 0.00003357
Iteration 57/1000 | Loss: 0.00003357
Iteration 58/1000 | Loss: 0.00003357
Iteration 59/1000 | Loss: 0.00003357
Iteration 60/1000 | Loss: 0.00003356
Iteration 61/1000 | Loss: 0.00003356
Iteration 62/1000 | Loss: 0.00003356
Iteration 63/1000 | Loss: 0.00003356
Iteration 64/1000 | Loss: 0.00003356
Iteration 65/1000 | Loss: 0.00003355
Iteration 66/1000 | Loss: 0.00003355
Iteration 67/1000 | Loss: 0.00003355
Iteration 68/1000 | Loss: 0.00003355
Iteration 69/1000 | Loss: 0.00003355
Iteration 70/1000 | Loss: 0.00003355
Iteration 71/1000 | Loss: 0.00003355
Iteration 72/1000 | Loss: 0.00003355
Iteration 73/1000 | Loss: 0.00003355
Iteration 74/1000 | Loss: 0.00003355
Iteration 75/1000 | Loss: 0.00003355
Iteration 76/1000 | Loss: 0.00003354
Iteration 77/1000 | Loss: 0.00003354
Iteration 78/1000 | Loss: 0.00003354
Iteration 79/1000 | Loss: 0.00003354
Iteration 80/1000 | Loss: 0.00003354
Iteration 81/1000 | Loss: 0.00003354
Iteration 82/1000 | Loss: 0.00003353
Iteration 83/1000 | Loss: 0.00003353
Iteration 84/1000 | Loss: 0.00003353
Iteration 85/1000 | Loss: 0.00003353
Iteration 86/1000 | Loss: 0.00003353
Iteration 87/1000 | Loss: 0.00003353
Iteration 88/1000 | Loss: 0.00003352
Iteration 89/1000 | Loss: 0.00003352
Iteration 90/1000 | Loss: 0.00003352
Iteration 91/1000 | Loss: 0.00003352
Iteration 92/1000 | Loss: 0.00003352
Iteration 93/1000 | Loss: 0.00003352
Iteration 94/1000 | Loss: 0.00003352
Iteration 95/1000 | Loss: 0.00003351
Iteration 96/1000 | Loss: 0.00003351
Iteration 97/1000 | Loss: 0.00003351
Iteration 98/1000 | Loss: 0.00003351
Iteration 99/1000 | Loss: 0.00003351
Iteration 100/1000 | Loss: 0.00003351
Iteration 101/1000 | Loss: 0.00003351
Iteration 102/1000 | Loss: 0.00003351
Iteration 103/1000 | Loss: 0.00003351
Iteration 104/1000 | Loss: 0.00003351
Iteration 105/1000 | Loss: 0.00003351
Iteration 106/1000 | Loss: 0.00003351
Iteration 107/1000 | Loss: 0.00003350
Iteration 108/1000 | Loss: 0.00003350
Iteration 109/1000 | Loss: 0.00003350
Iteration 110/1000 | Loss: 0.00003350
Iteration 111/1000 | Loss: 0.00003350
Iteration 112/1000 | Loss: 0.00003350
Iteration 113/1000 | Loss: 0.00003349
Iteration 114/1000 | Loss: 0.00003349
Iteration 115/1000 | Loss: 0.00003349
Iteration 116/1000 | Loss: 0.00003349
Iteration 117/1000 | Loss: 0.00003349
Iteration 118/1000 | Loss: 0.00003349
Iteration 119/1000 | Loss: 0.00003349
Iteration 120/1000 | Loss: 0.00003349
Iteration 121/1000 | Loss: 0.00003349
Iteration 122/1000 | Loss: 0.00003349
Iteration 123/1000 | Loss: 0.00003348
Iteration 124/1000 | Loss: 0.00003348
Iteration 125/1000 | Loss: 0.00003348
Iteration 126/1000 | Loss: 0.00003348
Iteration 127/1000 | Loss: 0.00003348
Iteration 128/1000 | Loss: 0.00003348
Iteration 129/1000 | Loss: 0.00003348
Iteration 130/1000 | Loss: 0.00003348
Iteration 131/1000 | Loss: 0.00003348
Iteration 132/1000 | Loss: 0.00003348
Iteration 133/1000 | Loss: 0.00003347
Iteration 134/1000 | Loss: 0.00003347
Iteration 135/1000 | Loss: 0.00003347
Iteration 136/1000 | Loss: 0.00003347
Iteration 137/1000 | Loss: 0.00003347
Iteration 138/1000 | Loss: 0.00003347
Iteration 139/1000 | Loss: 0.00003347
Iteration 140/1000 | Loss: 0.00003347
Iteration 141/1000 | Loss: 0.00003347
Iteration 142/1000 | Loss: 0.00003347
Iteration 143/1000 | Loss: 0.00003347
Iteration 144/1000 | Loss: 0.00003347
Iteration 145/1000 | Loss: 0.00003347
Iteration 146/1000 | Loss: 0.00003347
Iteration 147/1000 | Loss: 0.00003347
Iteration 148/1000 | Loss: 0.00003347
Iteration 149/1000 | Loss: 0.00003347
Iteration 150/1000 | Loss: 0.00003347
Iteration 151/1000 | Loss: 0.00003347
Iteration 152/1000 | Loss: 0.00003347
Iteration 153/1000 | Loss: 0.00003347
Iteration 154/1000 | Loss: 0.00003347
Iteration 155/1000 | Loss: 0.00003347
Iteration 156/1000 | Loss: 0.00003347
Iteration 157/1000 | Loss: 0.00003347
Iteration 158/1000 | Loss: 0.00003347
Iteration 159/1000 | Loss: 0.00003347
Iteration 160/1000 | Loss: 0.00003347
Iteration 161/1000 | Loss: 0.00003347
Iteration 162/1000 | Loss: 0.00003347
Iteration 163/1000 | Loss: 0.00003347
Iteration 164/1000 | Loss: 0.00003347
Iteration 165/1000 | Loss: 0.00003347
Iteration 166/1000 | Loss: 0.00003347
Iteration 167/1000 | Loss: 0.00003347
Iteration 168/1000 | Loss: 0.00003347
Iteration 169/1000 | Loss: 0.00003347
Iteration 170/1000 | Loss: 0.00003347
Iteration 171/1000 | Loss: 0.00003347
Iteration 172/1000 | Loss: 0.00003347
Iteration 173/1000 | Loss: 0.00003347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.346523953950964e-05, 3.346523953950964e-05, 3.346523953950964e-05, 3.346523953950964e-05, 3.346523953950964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.346523953950964e-05

Optimization complete. Final v2v error: 5.044278621673584 mm

Highest mean error: 7.201197147369385 mm for frame 66

Lowest mean error: 4.308076858520508 mm for frame 52

Saving results

Total time: 51.27333116531372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959557
Iteration 2/25 | Loss: 0.00223414
Iteration 3/25 | Loss: 0.00167274
Iteration 4/25 | Loss: 0.00160884
Iteration 5/25 | Loss: 0.00159393
Iteration 6/25 | Loss: 0.00159589
Iteration 7/25 | Loss: 0.00157320
Iteration 8/25 | Loss: 0.00156194
Iteration 9/25 | Loss: 0.00156617
Iteration 10/25 | Loss: 0.00155741
Iteration 11/25 | Loss: 0.00155991
Iteration 12/25 | Loss: 0.00155652
Iteration 13/25 | Loss: 0.00156075
Iteration 14/25 | Loss: 0.00155570
Iteration 15/25 | Loss: 0.00155527
Iteration 16/25 | Loss: 0.00155521
Iteration 17/25 | Loss: 0.00155521
Iteration 18/25 | Loss: 0.00155521
Iteration 19/25 | Loss: 0.00155521
Iteration 20/25 | Loss: 0.00155521
Iteration 21/25 | Loss: 0.00155521
Iteration 22/25 | Loss: 0.00155521
Iteration 23/25 | Loss: 0.00155521
Iteration 24/25 | Loss: 0.00155521
Iteration 25/25 | Loss: 0.00155521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48789430
Iteration 2/25 | Loss: 0.00155617
Iteration 3/25 | Loss: 0.00155614
Iteration 4/25 | Loss: 0.00155614
Iteration 5/25 | Loss: 0.00155614
Iteration 6/25 | Loss: 0.00155614
Iteration 7/25 | Loss: 0.00155614
Iteration 8/25 | Loss: 0.00155614
Iteration 9/25 | Loss: 0.00155614
Iteration 10/25 | Loss: 0.00155613
Iteration 11/25 | Loss: 0.00155613
Iteration 12/25 | Loss: 0.00155613
Iteration 13/25 | Loss: 0.00155613
Iteration 14/25 | Loss: 0.00155613
Iteration 15/25 | Loss: 0.00155613
Iteration 16/25 | Loss: 0.00155613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015561345499008894, 0.0015561345499008894, 0.0015561345499008894, 0.0015561345499008894, 0.0015561345499008894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015561345499008894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155613
Iteration 2/1000 | Loss: 0.00005159
Iteration 3/1000 | Loss: 0.00003969
Iteration 4/1000 | Loss: 0.00003633
Iteration 5/1000 | Loss: 0.00003436
Iteration 6/1000 | Loss: 0.00003374
Iteration 7/1000 | Loss: 0.00003330
Iteration 8/1000 | Loss: 0.00003287
Iteration 9/1000 | Loss: 0.00003242
Iteration 10/1000 | Loss: 0.00003222
Iteration 11/1000 | Loss: 0.00003221
Iteration 12/1000 | Loss: 0.00003204
Iteration 13/1000 | Loss: 0.00003198
Iteration 14/1000 | Loss: 0.00003194
Iteration 15/1000 | Loss: 0.00003192
Iteration 16/1000 | Loss: 0.00003191
Iteration 17/1000 | Loss: 0.00003187
Iteration 18/1000 | Loss: 0.00003187
Iteration 19/1000 | Loss: 0.00003187
Iteration 20/1000 | Loss: 0.00003186
Iteration 21/1000 | Loss: 0.00003186
Iteration 22/1000 | Loss: 0.00003186
Iteration 23/1000 | Loss: 0.00003185
Iteration 24/1000 | Loss: 0.00003184
Iteration 25/1000 | Loss: 0.00003184
Iteration 26/1000 | Loss: 0.00003183
Iteration 27/1000 | Loss: 0.00003182
Iteration 28/1000 | Loss: 0.00003180
Iteration 29/1000 | Loss: 0.00003180
Iteration 30/1000 | Loss: 0.00003179
Iteration 31/1000 | Loss: 0.00003179
Iteration 32/1000 | Loss: 0.00003179
Iteration 33/1000 | Loss: 0.00003179
Iteration 34/1000 | Loss: 0.00003179
Iteration 35/1000 | Loss: 0.00003179
Iteration 36/1000 | Loss: 0.00003179
Iteration 37/1000 | Loss: 0.00003179
Iteration 38/1000 | Loss: 0.00003179
Iteration 39/1000 | Loss: 0.00003179
Iteration 40/1000 | Loss: 0.00003179
Iteration 41/1000 | Loss: 0.00003179
Iteration 42/1000 | Loss: 0.00003178
Iteration 43/1000 | Loss: 0.00003177
Iteration 44/1000 | Loss: 0.00003177
Iteration 45/1000 | Loss: 0.00003176
Iteration 46/1000 | Loss: 0.00003176
Iteration 47/1000 | Loss: 0.00003176
Iteration 48/1000 | Loss: 0.00003176
Iteration 49/1000 | Loss: 0.00003176
Iteration 50/1000 | Loss: 0.00003176
Iteration 51/1000 | Loss: 0.00003176
Iteration 52/1000 | Loss: 0.00003176
Iteration 53/1000 | Loss: 0.00003176
Iteration 54/1000 | Loss: 0.00003176
Iteration 55/1000 | Loss: 0.00003176
Iteration 56/1000 | Loss: 0.00003176
Iteration 57/1000 | Loss: 0.00003176
Iteration 58/1000 | Loss: 0.00003176
Iteration 59/1000 | Loss: 0.00003176
Iteration 60/1000 | Loss: 0.00003176
Iteration 61/1000 | Loss: 0.00003176
Iteration 62/1000 | Loss: 0.00003176
Iteration 63/1000 | Loss: 0.00003176
Iteration 64/1000 | Loss: 0.00003176
Iteration 65/1000 | Loss: 0.00003176
Iteration 66/1000 | Loss: 0.00003176
Iteration 67/1000 | Loss: 0.00003176
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003176
Iteration 73/1000 | Loss: 0.00003176
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003176
Iteration 76/1000 | Loss: 0.00003176
Iteration 77/1000 | Loss: 0.00003176
Iteration 78/1000 | Loss: 0.00003176
Iteration 79/1000 | Loss: 0.00003176
Iteration 80/1000 | Loss: 0.00003176
Iteration 81/1000 | Loss: 0.00003176
Iteration 82/1000 | Loss: 0.00003176
Iteration 83/1000 | Loss: 0.00003176
Iteration 84/1000 | Loss: 0.00003176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [3.1759667763253674e-05, 3.1759667763253674e-05, 3.1759667763253674e-05, 3.1759667763253674e-05, 3.1759667763253674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1759667763253674e-05

Optimization complete. Final v2v error: 4.94038724899292 mm

Highest mean error: 5.20659875869751 mm for frame 105

Lowest mean error: 4.596512317657471 mm for frame 207

Saving results

Total time: 54.88904333114624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521114
Iteration 2/25 | Loss: 0.00165344
Iteration 3/25 | Loss: 0.00157767
Iteration 4/25 | Loss: 0.00156338
Iteration 5/25 | Loss: 0.00155750
Iteration 6/25 | Loss: 0.00155668
Iteration 7/25 | Loss: 0.00155668
Iteration 8/25 | Loss: 0.00155668
Iteration 9/25 | Loss: 0.00155668
Iteration 10/25 | Loss: 0.00155668
Iteration 11/25 | Loss: 0.00155668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001556683680973947, 0.001556683680973947, 0.001556683680973947, 0.001556683680973947, 0.001556683680973947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001556683680973947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35716426
Iteration 2/25 | Loss: 0.00149225
Iteration 3/25 | Loss: 0.00149225
Iteration 4/25 | Loss: 0.00149225
Iteration 5/25 | Loss: 0.00149225
Iteration 6/25 | Loss: 0.00149225
Iteration 7/25 | Loss: 0.00149225
Iteration 8/25 | Loss: 0.00149225
Iteration 9/25 | Loss: 0.00149225
Iteration 10/25 | Loss: 0.00149225
Iteration 11/25 | Loss: 0.00149225
Iteration 12/25 | Loss: 0.00149225
Iteration 13/25 | Loss: 0.00149225
Iteration 14/25 | Loss: 0.00149225
Iteration 15/25 | Loss: 0.00149225
Iteration 16/25 | Loss: 0.00149225
Iteration 17/25 | Loss: 0.00149225
Iteration 18/25 | Loss: 0.00149225
Iteration 19/25 | Loss: 0.00149225
Iteration 20/25 | Loss: 0.00149225
Iteration 21/25 | Loss: 0.00149225
Iteration 22/25 | Loss: 0.00149225
Iteration 23/25 | Loss: 0.00149225
Iteration 24/25 | Loss: 0.00149225
Iteration 25/25 | Loss: 0.00149225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149225
Iteration 2/1000 | Loss: 0.00005615
Iteration 3/1000 | Loss: 0.00004093
Iteration 4/1000 | Loss: 0.00003757
Iteration 5/1000 | Loss: 0.00003532
Iteration 6/1000 | Loss: 0.00003403
Iteration 7/1000 | Loss: 0.00003351
Iteration 8/1000 | Loss: 0.00003294
Iteration 9/1000 | Loss: 0.00003247
Iteration 10/1000 | Loss: 0.00003203
Iteration 11/1000 | Loss: 0.00003176
Iteration 12/1000 | Loss: 0.00003165
Iteration 13/1000 | Loss: 0.00003164
Iteration 14/1000 | Loss: 0.00003159
Iteration 15/1000 | Loss: 0.00003159
Iteration 16/1000 | Loss: 0.00003159
Iteration 17/1000 | Loss: 0.00003159
Iteration 18/1000 | Loss: 0.00003159
Iteration 19/1000 | Loss: 0.00003159
Iteration 20/1000 | Loss: 0.00003159
Iteration 21/1000 | Loss: 0.00003159
Iteration 22/1000 | Loss: 0.00003158
Iteration 23/1000 | Loss: 0.00003157
Iteration 24/1000 | Loss: 0.00003157
Iteration 25/1000 | Loss: 0.00003155
Iteration 26/1000 | Loss: 0.00003155
Iteration 27/1000 | Loss: 0.00003155
Iteration 28/1000 | Loss: 0.00003155
Iteration 29/1000 | Loss: 0.00003155
Iteration 30/1000 | Loss: 0.00003155
Iteration 31/1000 | Loss: 0.00003154
Iteration 32/1000 | Loss: 0.00003154
Iteration 33/1000 | Loss: 0.00003154
Iteration 34/1000 | Loss: 0.00003154
Iteration 35/1000 | Loss: 0.00003154
Iteration 36/1000 | Loss: 0.00003154
Iteration 37/1000 | Loss: 0.00003154
Iteration 38/1000 | Loss: 0.00003153
Iteration 39/1000 | Loss: 0.00003153
Iteration 40/1000 | Loss: 0.00003153
Iteration 41/1000 | Loss: 0.00003152
Iteration 42/1000 | Loss: 0.00003152
Iteration 43/1000 | Loss: 0.00003152
Iteration 44/1000 | Loss: 0.00003152
Iteration 45/1000 | Loss: 0.00003152
Iteration 46/1000 | Loss: 0.00003152
Iteration 47/1000 | Loss: 0.00003151
Iteration 48/1000 | Loss: 0.00003151
Iteration 49/1000 | Loss: 0.00003151
Iteration 50/1000 | Loss: 0.00003151
Iteration 51/1000 | Loss: 0.00003150
Iteration 52/1000 | Loss: 0.00003150
Iteration 53/1000 | Loss: 0.00003150
Iteration 54/1000 | Loss: 0.00003150
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003150
Iteration 57/1000 | Loss: 0.00003150
Iteration 58/1000 | Loss: 0.00003150
Iteration 59/1000 | Loss: 0.00003150
Iteration 60/1000 | Loss: 0.00003149
Iteration 61/1000 | Loss: 0.00003149
Iteration 62/1000 | Loss: 0.00003149
Iteration 63/1000 | Loss: 0.00003149
Iteration 64/1000 | Loss: 0.00003149
Iteration 65/1000 | Loss: 0.00003148
Iteration 66/1000 | Loss: 0.00003148
Iteration 67/1000 | Loss: 0.00003148
Iteration 68/1000 | Loss: 0.00003147
Iteration 69/1000 | Loss: 0.00003146
Iteration 70/1000 | Loss: 0.00003146
Iteration 71/1000 | Loss: 0.00003146
Iteration 72/1000 | Loss: 0.00003146
Iteration 73/1000 | Loss: 0.00003146
Iteration 74/1000 | Loss: 0.00003144
Iteration 75/1000 | Loss: 0.00003144
Iteration 76/1000 | Loss: 0.00003143
Iteration 77/1000 | Loss: 0.00003143
Iteration 78/1000 | Loss: 0.00003143
Iteration 79/1000 | Loss: 0.00003143
Iteration 80/1000 | Loss: 0.00003142
Iteration 81/1000 | Loss: 0.00003142
Iteration 82/1000 | Loss: 0.00003142
Iteration 83/1000 | Loss: 0.00003142
Iteration 84/1000 | Loss: 0.00003142
Iteration 85/1000 | Loss: 0.00003142
Iteration 86/1000 | Loss: 0.00003141
Iteration 87/1000 | Loss: 0.00003141
Iteration 88/1000 | Loss: 0.00003141
Iteration 89/1000 | Loss: 0.00003141
Iteration 90/1000 | Loss: 0.00003141
Iteration 91/1000 | Loss: 0.00003141
Iteration 92/1000 | Loss: 0.00003141
Iteration 93/1000 | Loss: 0.00003141
Iteration 94/1000 | Loss: 0.00003141
Iteration 95/1000 | Loss: 0.00003141
Iteration 96/1000 | Loss: 0.00003141
Iteration 97/1000 | Loss: 0.00003141
Iteration 98/1000 | Loss: 0.00003141
Iteration 99/1000 | Loss: 0.00003141
Iteration 100/1000 | Loss: 0.00003141
Iteration 101/1000 | Loss: 0.00003140
Iteration 102/1000 | Loss: 0.00003140
Iteration 103/1000 | Loss: 0.00003140
Iteration 104/1000 | Loss: 0.00003140
Iteration 105/1000 | Loss: 0.00003140
Iteration 106/1000 | Loss: 0.00003140
Iteration 107/1000 | Loss: 0.00003140
Iteration 108/1000 | Loss: 0.00003140
Iteration 109/1000 | Loss: 0.00003140
Iteration 110/1000 | Loss: 0.00003140
Iteration 111/1000 | Loss: 0.00003140
Iteration 112/1000 | Loss: 0.00003140
Iteration 113/1000 | Loss: 0.00003140
Iteration 114/1000 | Loss: 0.00003140
Iteration 115/1000 | Loss: 0.00003139
Iteration 116/1000 | Loss: 0.00003139
Iteration 117/1000 | Loss: 0.00003139
Iteration 118/1000 | Loss: 0.00003139
Iteration 119/1000 | Loss: 0.00003139
Iteration 120/1000 | Loss: 0.00003139
Iteration 121/1000 | Loss: 0.00003139
Iteration 122/1000 | Loss: 0.00003139
Iteration 123/1000 | Loss: 0.00003138
Iteration 124/1000 | Loss: 0.00003138
Iteration 125/1000 | Loss: 0.00003138
Iteration 126/1000 | Loss: 0.00003138
Iteration 127/1000 | Loss: 0.00003138
Iteration 128/1000 | Loss: 0.00003138
Iteration 129/1000 | Loss: 0.00003138
Iteration 130/1000 | Loss: 0.00003138
Iteration 131/1000 | Loss: 0.00003138
Iteration 132/1000 | Loss: 0.00003138
Iteration 133/1000 | Loss: 0.00003138
Iteration 134/1000 | Loss: 0.00003138
Iteration 135/1000 | Loss: 0.00003138
Iteration 136/1000 | Loss: 0.00003138
Iteration 137/1000 | Loss: 0.00003138
Iteration 138/1000 | Loss: 0.00003138
Iteration 139/1000 | Loss: 0.00003138
Iteration 140/1000 | Loss: 0.00003138
Iteration 141/1000 | Loss: 0.00003138
Iteration 142/1000 | Loss: 0.00003138
Iteration 143/1000 | Loss: 0.00003138
Iteration 144/1000 | Loss: 0.00003138
Iteration 145/1000 | Loss: 0.00003138
Iteration 146/1000 | Loss: 0.00003138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.137982639600523e-05, 3.137982639600523e-05, 3.137982639600523e-05, 3.137982639600523e-05, 3.137982639600523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.137982639600523e-05

Optimization complete. Final v2v error: 4.956846237182617 mm

Highest mean error: 5.308224678039551 mm for frame 121

Lowest mean error: 4.610219955444336 mm for frame 82

Saving results

Total time: 34.47853994369507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117392
Iteration 2/25 | Loss: 0.00250772
Iteration 3/25 | Loss: 0.00192450
Iteration 4/25 | Loss: 0.00197838
Iteration 5/25 | Loss: 0.00172892
Iteration 6/25 | Loss: 0.00165271
Iteration 7/25 | Loss: 0.00160670
Iteration 8/25 | Loss: 0.00159491
Iteration 9/25 | Loss: 0.00158754
Iteration 10/25 | Loss: 0.00158153
Iteration 11/25 | Loss: 0.00157987
Iteration 12/25 | Loss: 0.00157943
Iteration 13/25 | Loss: 0.00157904
Iteration 14/25 | Loss: 0.00157878
Iteration 15/25 | Loss: 0.00157827
Iteration 16/25 | Loss: 0.00157775
Iteration 17/25 | Loss: 0.00157761
Iteration 18/25 | Loss: 0.00157748
Iteration 19/25 | Loss: 0.00157728
Iteration 20/25 | Loss: 0.00157711
Iteration 21/25 | Loss: 0.00158069
Iteration 22/25 | Loss: 0.00157672
Iteration 23/25 | Loss: 0.00157617
Iteration 24/25 | Loss: 0.00157606
Iteration 25/25 | Loss: 0.00157606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39213896
Iteration 2/25 | Loss: 0.00191546
Iteration 3/25 | Loss: 0.00184609
Iteration 4/25 | Loss: 0.00184609
Iteration 5/25 | Loss: 0.00184609
Iteration 6/25 | Loss: 0.00184609
Iteration 7/25 | Loss: 0.00184609
Iteration 8/25 | Loss: 0.00184609
Iteration 9/25 | Loss: 0.00184609
Iteration 10/25 | Loss: 0.00184609
Iteration 11/25 | Loss: 0.00184609
Iteration 12/25 | Loss: 0.00184609
Iteration 13/25 | Loss: 0.00184609
Iteration 14/25 | Loss: 0.00184609
Iteration 15/25 | Loss: 0.00184609
Iteration 16/25 | Loss: 0.00184609
Iteration 17/25 | Loss: 0.00184609
Iteration 18/25 | Loss: 0.00184609
Iteration 19/25 | Loss: 0.00184609
Iteration 20/25 | Loss: 0.00184609
Iteration 21/25 | Loss: 0.00184609
Iteration 22/25 | Loss: 0.00184609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018460925202816725, 0.0018460925202816725, 0.0018460925202816725, 0.0018460925202816725, 0.0018460925202816725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018460925202816725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184609
Iteration 2/1000 | Loss: 0.00017908
Iteration 3/1000 | Loss: 0.00007560
Iteration 4/1000 | Loss: 0.00013673
Iteration 5/1000 | Loss: 0.00007987
Iteration 6/1000 | Loss: 0.00007692
Iteration 7/1000 | Loss: 0.00005579
Iteration 8/1000 | Loss: 0.00122021
Iteration 9/1000 | Loss: 0.00064741
Iteration 10/1000 | Loss: 0.00071065
Iteration 11/1000 | Loss: 0.00077036
Iteration 12/1000 | Loss: 0.00009028
Iteration 13/1000 | Loss: 0.00006195
Iteration 14/1000 | Loss: 0.00007118
Iteration 15/1000 | Loss: 0.00006713
Iteration 16/1000 | Loss: 0.00004334
Iteration 17/1000 | Loss: 0.00066156
Iteration 18/1000 | Loss: 0.00006326
Iteration 19/1000 | Loss: 0.00004931
Iteration 20/1000 | Loss: 0.00062978
Iteration 21/1000 | Loss: 0.00006050
Iteration 22/1000 | Loss: 0.00004615
Iteration 23/1000 | Loss: 0.00003944
Iteration 24/1000 | Loss: 0.00003637
Iteration 25/1000 | Loss: 0.00003498
Iteration 26/1000 | Loss: 0.00059165
Iteration 27/1000 | Loss: 0.00006515
Iteration 28/1000 | Loss: 0.00004481
Iteration 29/1000 | Loss: 0.00003792
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003250
Iteration 32/1000 | Loss: 0.00003130
Iteration 33/1000 | Loss: 0.00003066
Iteration 34/1000 | Loss: 0.00003033
Iteration 35/1000 | Loss: 0.00002999
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002954
Iteration 38/1000 | Loss: 0.00002954
Iteration 39/1000 | Loss: 0.00002950
Iteration 40/1000 | Loss: 0.00002944
Iteration 41/1000 | Loss: 0.00002944
Iteration 42/1000 | Loss: 0.00002938
Iteration 43/1000 | Loss: 0.00002937
Iteration 44/1000 | Loss: 0.00002937
Iteration 45/1000 | Loss: 0.00002937
Iteration 46/1000 | Loss: 0.00002936
Iteration 47/1000 | Loss: 0.00002935
Iteration 48/1000 | Loss: 0.00002934
Iteration 49/1000 | Loss: 0.00002934
Iteration 50/1000 | Loss: 0.00002931
Iteration 51/1000 | Loss: 0.00002931
Iteration 52/1000 | Loss: 0.00002931
Iteration 53/1000 | Loss: 0.00002931
Iteration 54/1000 | Loss: 0.00002931
Iteration 55/1000 | Loss: 0.00002931
Iteration 56/1000 | Loss: 0.00002931
Iteration 57/1000 | Loss: 0.00002931
Iteration 58/1000 | Loss: 0.00002931
Iteration 59/1000 | Loss: 0.00002930
Iteration 60/1000 | Loss: 0.00002930
Iteration 61/1000 | Loss: 0.00002930
Iteration 62/1000 | Loss: 0.00002930
Iteration 63/1000 | Loss: 0.00002930
Iteration 64/1000 | Loss: 0.00002930
Iteration 65/1000 | Loss: 0.00002930
Iteration 66/1000 | Loss: 0.00002930
Iteration 67/1000 | Loss: 0.00002930
Iteration 68/1000 | Loss: 0.00002929
Iteration 69/1000 | Loss: 0.00002929
Iteration 70/1000 | Loss: 0.00002929
Iteration 71/1000 | Loss: 0.00002928
Iteration 72/1000 | Loss: 0.00002928
Iteration 73/1000 | Loss: 0.00002928
Iteration 74/1000 | Loss: 0.00002927
Iteration 75/1000 | Loss: 0.00002927
Iteration 76/1000 | Loss: 0.00002927
Iteration 77/1000 | Loss: 0.00002927
Iteration 78/1000 | Loss: 0.00002927
Iteration 79/1000 | Loss: 0.00002926
Iteration 80/1000 | Loss: 0.00002926
Iteration 81/1000 | Loss: 0.00002926
Iteration 82/1000 | Loss: 0.00002926
Iteration 83/1000 | Loss: 0.00002926
Iteration 84/1000 | Loss: 0.00002926
Iteration 85/1000 | Loss: 0.00002926
Iteration 86/1000 | Loss: 0.00002925
Iteration 87/1000 | Loss: 0.00002925
Iteration 88/1000 | Loss: 0.00002925
Iteration 89/1000 | Loss: 0.00002925
Iteration 90/1000 | Loss: 0.00002925
Iteration 91/1000 | Loss: 0.00002925
Iteration 92/1000 | Loss: 0.00002925
Iteration 93/1000 | Loss: 0.00002925
Iteration 94/1000 | Loss: 0.00002925
Iteration 95/1000 | Loss: 0.00002925
Iteration 96/1000 | Loss: 0.00002925
Iteration 97/1000 | Loss: 0.00002925
Iteration 98/1000 | Loss: 0.00002925
Iteration 99/1000 | Loss: 0.00002925
Iteration 100/1000 | Loss: 0.00002924
Iteration 101/1000 | Loss: 0.00002924
Iteration 102/1000 | Loss: 0.00002924
Iteration 103/1000 | Loss: 0.00002924
Iteration 104/1000 | Loss: 0.00002924
Iteration 105/1000 | Loss: 0.00002924
Iteration 106/1000 | Loss: 0.00002924
Iteration 107/1000 | Loss: 0.00002924
Iteration 108/1000 | Loss: 0.00002924
Iteration 109/1000 | Loss: 0.00002924
Iteration 110/1000 | Loss: 0.00002924
Iteration 111/1000 | Loss: 0.00002924
Iteration 112/1000 | Loss: 0.00002924
Iteration 113/1000 | Loss: 0.00002924
Iteration 114/1000 | Loss: 0.00002923
Iteration 115/1000 | Loss: 0.00002923
Iteration 116/1000 | Loss: 0.00002923
Iteration 117/1000 | Loss: 0.00002923
Iteration 118/1000 | Loss: 0.00002923
Iteration 119/1000 | Loss: 0.00002923
Iteration 120/1000 | Loss: 0.00002923
Iteration 121/1000 | Loss: 0.00002923
Iteration 122/1000 | Loss: 0.00002923
Iteration 123/1000 | Loss: 0.00002923
Iteration 124/1000 | Loss: 0.00002923
Iteration 125/1000 | Loss: 0.00002922
Iteration 126/1000 | Loss: 0.00002922
Iteration 127/1000 | Loss: 0.00002922
Iteration 128/1000 | Loss: 0.00002922
Iteration 129/1000 | Loss: 0.00002922
Iteration 130/1000 | Loss: 0.00002922
Iteration 131/1000 | Loss: 0.00002922
Iteration 132/1000 | Loss: 0.00002922
Iteration 133/1000 | Loss: 0.00002922
Iteration 134/1000 | Loss: 0.00002922
Iteration 135/1000 | Loss: 0.00002921
Iteration 136/1000 | Loss: 0.00002921
Iteration 137/1000 | Loss: 0.00002921
Iteration 138/1000 | Loss: 0.00002921
Iteration 139/1000 | Loss: 0.00002921
Iteration 140/1000 | Loss: 0.00002921
Iteration 141/1000 | Loss: 0.00002921
Iteration 142/1000 | Loss: 0.00002921
Iteration 143/1000 | Loss: 0.00002921
Iteration 144/1000 | Loss: 0.00002921
Iteration 145/1000 | Loss: 0.00002921
Iteration 146/1000 | Loss: 0.00002921
Iteration 147/1000 | Loss: 0.00002921
Iteration 148/1000 | Loss: 0.00002921
Iteration 149/1000 | Loss: 0.00002921
Iteration 150/1000 | Loss: 0.00002921
Iteration 151/1000 | Loss: 0.00002921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.921361920016352e-05, 2.921361920016352e-05, 2.921361920016352e-05, 2.921361920016352e-05, 2.921361920016352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.921361920016352e-05

Optimization complete. Final v2v error: 4.5618486404418945 mm

Highest mean error: 13.121968269348145 mm for frame 91

Lowest mean error: 4.038684844970703 mm for frame 94

Saving results

Total time: 119.41710042953491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899486
Iteration 2/25 | Loss: 0.00207483
Iteration 3/25 | Loss: 0.00189562
Iteration 4/25 | Loss: 0.00184270
Iteration 5/25 | Loss: 0.00181798
Iteration 6/25 | Loss: 0.00180515
Iteration 7/25 | Loss: 0.00179864
Iteration 8/25 | Loss: 0.00179270
Iteration 9/25 | Loss: 0.00179451
Iteration 10/25 | Loss: 0.00179025
Iteration 11/25 | Loss: 0.00178895
Iteration 12/25 | Loss: 0.00178868
Iteration 13/25 | Loss: 0.00178828
Iteration 14/25 | Loss: 0.00178806
Iteration 15/25 | Loss: 0.00178795
Iteration 16/25 | Loss: 0.00178787
Iteration 17/25 | Loss: 0.00178784
Iteration 18/25 | Loss: 0.00178784
Iteration 19/25 | Loss: 0.00178784
Iteration 20/25 | Loss: 0.00178784
Iteration 21/25 | Loss: 0.00178783
Iteration 22/25 | Loss: 0.00178783
Iteration 23/25 | Loss: 0.00178783
Iteration 24/25 | Loss: 0.00178782
Iteration 25/25 | Loss: 0.00178782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.35568213
Iteration 2/25 | Loss: 0.00242270
Iteration 3/25 | Loss: 0.00242263
Iteration 4/25 | Loss: 0.00242263
Iteration 5/25 | Loss: 0.00242263
Iteration 6/25 | Loss: 0.00242263
Iteration 7/25 | Loss: 0.00242263
Iteration 8/25 | Loss: 0.00242263
Iteration 9/25 | Loss: 0.00242263
Iteration 10/25 | Loss: 0.00242263
Iteration 11/25 | Loss: 0.00242263
Iteration 12/25 | Loss: 0.00242263
Iteration 13/25 | Loss: 0.00242263
Iteration 14/25 | Loss: 0.00242263
Iteration 15/25 | Loss: 0.00242263
Iteration 16/25 | Loss: 0.00242263
Iteration 17/25 | Loss: 0.00242263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024226312525570393, 0.0024226312525570393, 0.0024226312525570393, 0.0024226312525570393, 0.0024226312525570393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024226312525570393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242263
Iteration 2/1000 | Loss: 0.00016083
Iteration 3/1000 | Loss: 0.00049304
Iteration 4/1000 | Loss: 0.00043541
Iteration 5/1000 | Loss: 0.00046758
Iteration 6/1000 | Loss: 0.00040056
Iteration 7/1000 | Loss: 0.00033323
Iteration 8/1000 | Loss: 0.00080490
Iteration 9/1000 | Loss: 0.00096070
Iteration 10/1000 | Loss: 0.00127932
Iteration 11/1000 | Loss: 0.00090366
Iteration 12/1000 | Loss: 0.00010514
Iteration 13/1000 | Loss: 0.00110186
Iteration 14/1000 | Loss: 0.00074730
Iteration 15/1000 | Loss: 0.00090745
Iteration 16/1000 | Loss: 0.00092662
Iteration 17/1000 | Loss: 0.00082364
Iteration 18/1000 | Loss: 0.00022652
Iteration 19/1000 | Loss: 0.00038936
Iteration 20/1000 | Loss: 0.00057245
Iteration 21/1000 | Loss: 0.00090465
Iteration 22/1000 | Loss: 0.00074714
Iteration 23/1000 | Loss: 0.00062881
Iteration 24/1000 | Loss: 0.00087409
Iteration 25/1000 | Loss: 0.00070680
Iteration 26/1000 | Loss: 0.00056739
Iteration 27/1000 | Loss: 0.00031664
Iteration 28/1000 | Loss: 0.00037567
Iteration 29/1000 | Loss: 0.00035143
Iteration 30/1000 | Loss: 0.00077205
Iteration 31/1000 | Loss: 0.00090820
Iteration 32/1000 | Loss: 0.00059148
Iteration 33/1000 | Loss: 0.00034607
Iteration 34/1000 | Loss: 0.00021494
Iteration 35/1000 | Loss: 0.00060317
Iteration 36/1000 | Loss: 0.00051888
Iteration 37/1000 | Loss: 0.00083821
Iteration 38/1000 | Loss: 0.00040939
Iteration 39/1000 | Loss: 0.00008999
Iteration 40/1000 | Loss: 0.00008758
Iteration 41/1000 | Loss: 0.00029470
Iteration 42/1000 | Loss: 0.00040145
Iteration 43/1000 | Loss: 0.00037094
Iteration 44/1000 | Loss: 0.00018066
Iteration 45/1000 | Loss: 0.00041653
Iteration 46/1000 | Loss: 0.00007840
Iteration 47/1000 | Loss: 0.00036264
Iteration 48/1000 | Loss: 0.00006691
Iteration 49/1000 | Loss: 0.00007872
Iteration 50/1000 | Loss: 0.00014629
Iteration 51/1000 | Loss: 0.00006510
Iteration 52/1000 | Loss: 0.00021622
Iteration 53/1000 | Loss: 0.00005452
Iteration 54/1000 | Loss: 0.00005110
Iteration 55/1000 | Loss: 0.00032855
Iteration 56/1000 | Loss: 0.00025529
Iteration 57/1000 | Loss: 0.00021749
Iteration 58/1000 | Loss: 0.00030759
Iteration 59/1000 | Loss: 0.00020215
Iteration 60/1000 | Loss: 0.00026336
Iteration 61/1000 | Loss: 0.00037799
Iteration 62/1000 | Loss: 0.00022827
Iteration 63/1000 | Loss: 0.00016041
Iteration 64/1000 | Loss: 0.00006661
Iteration 65/1000 | Loss: 0.00018955
Iteration 66/1000 | Loss: 0.00040775
Iteration 67/1000 | Loss: 0.00037881
Iteration 68/1000 | Loss: 0.00037206
Iteration 69/1000 | Loss: 0.00041247
Iteration 70/1000 | Loss: 0.00045664
Iteration 71/1000 | Loss: 0.00029555
Iteration 72/1000 | Loss: 0.00005716
Iteration 73/1000 | Loss: 0.00005139
Iteration 74/1000 | Loss: 0.00050654
Iteration 75/1000 | Loss: 0.00014248
Iteration 76/1000 | Loss: 0.00024315
Iteration 77/1000 | Loss: 0.00061084
Iteration 78/1000 | Loss: 0.00009206
Iteration 79/1000 | Loss: 0.00004890
Iteration 80/1000 | Loss: 0.00004243
Iteration 81/1000 | Loss: 0.00004056
Iteration 82/1000 | Loss: 0.00003976
Iteration 83/1000 | Loss: 0.00003916
Iteration 84/1000 | Loss: 0.00003878
Iteration 85/1000 | Loss: 0.00003860
Iteration 86/1000 | Loss: 0.00003849
Iteration 87/1000 | Loss: 0.00003829
Iteration 88/1000 | Loss: 0.00003829
Iteration 89/1000 | Loss: 0.00003829
Iteration 90/1000 | Loss: 0.00003814
Iteration 91/1000 | Loss: 0.00003808
Iteration 92/1000 | Loss: 0.00003802
Iteration 93/1000 | Loss: 0.00003802
Iteration 94/1000 | Loss: 0.00003801
Iteration 95/1000 | Loss: 0.00003792
Iteration 96/1000 | Loss: 0.00003791
Iteration 97/1000 | Loss: 0.00003791
Iteration 98/1000 | Loss: 0.00003790
Iteration 99/1000 | Loss: 0.00003790
Iteration 100/1000 | Loss: 0.00003789
Iteration 101/1000 | Loss: 0.00003788
Iteration 102/1000 | Loss: 0.00003787
Iteration 103/1000 | Loss: 0.00003786
Iteration 104/1000 | Loss: 0.00003786
Iteration 105/1000 | Loss: 0.00003785
Iteration 106/1000 | Loss: 0.00003785
Iteration 107/1000 | Loss: 0.00003781
Iteration 108/1000 | Loss: 0.00003781
Iteration 109/1000 | Loss: 0.00003780
Iteration 110/1000 | Loss: 0.00003780
Iteration 111/1000 | Loss: 0.00003780
Iteration 112/1000 | Loss: 0.00003780
Iteration 113/1000 | Loss: 0.00003780
Iteration 114/1000 | Loss: 0.00003779
Iteration 115/1000 | Loss: 0.00003779
Iteration 116/1000 | Loss: 0.00003778
Iteration 117/1000 | Loss: 0.00003776
Iteration 118/1000 | Loss: 0.00003776
Iteration 119/1000 | Loss: 0.00003776
Iteration 120/1000 | Loss: 0.00003776
Iteration 121/1000 | Loss: 0.00003776
Iteration 122/1000 | Loss: 0.00003776
Iteration 123/1000 | Loss: 0.00003775
Iteration 124/1000 | Loss: 0.00003775
Iteration 125/1000 | Loss: 0.00003775
Iteration 126/1000 | Loss: 0.00003774
Iteration 127/1000 | Loss: 0.00003773
Iteration 128/1000 | Loss: 0.00003773
Iteration 129/1000 | Loss: 0.00003772
Iteration 130/1000 | Loss: 0.00003772
Iteration 131/1000 | Loss: 0.00003771
Iteration 132/1000 | Loss: 0.00003771
Iteration 133/1000 | Loss: 0.00003771
Iteration 134/1000 | Loss: 0.00003771
Iteration 135/1000 | Loss: 0.00003771
Iteration 136/1000 | Loss: 0.00003771
Iteration 137/1000 | Loss: 0.00003770
Iteration 138/1000 | Loss: 0.00003770
Iteration 139/1000 | Loss: 0.00003770
Iteration 140/1000 | Loss: 0.00003770
Iteration 141/1000 | Loss: 0.00003770
Iteration 142/1000 | Loss: 0.00003769
Iteration 143/1000 | Loss: 0.00003769
Iteration 144/1000 | Loss: 0.00003769
Iteration 145/1000 | Loss: 0.00003769
Iteration 146/1000 | Loss: 0.00003769
Iteration 147/1000 | Loss: 0.00003769
Iteration 148/1000 | Loss: 0.00003768
Iteration 149/1000 | Loss: 0.00003767
Iteration 150/1000 | Loss: 0.00003767
Iteration 151/1000 | Loss: 0.00003767
Iteration 152/1000 | Loss: 0.00003766
Iteration 153/1000 | Loss: 0.00003766
Iteration 154/1000 | Loss: 0.00003766
Iteration 155/1000 | Loss: 0.00003766
Iteration 156/1000 | Loss: 0.00003765
Iteration 157/1000 | Loss: 0.00003765
Iteration 158/1000 | Loss: 0.00003765
Iteration 159/1000 | Loss: 0.00003765
Iteration 160/1000 | Loss: 0.00003764
Iteration 161/1000 | Loss: 0.00003764
Iteration 162/1000 | Loss: 0.00003764
Iteration 163/1000 | Loss: 0.00003764
Iteration 164/1000 | Loss: 0.00003764
Iteration 165/1000 | Loss: 0.00003764
Iteration 166/1000 | Loss: 0.00003764
Iteration 167/1000 | Loss: 0.00003764
Iteration 168/1000 | Loss: 0.00003764
Iteration 169/1000 | Loss: 0.00003764
Iteration 170/1000 | Loss: 0.00003764
Iteration 171/1000 | Loss: 0.00003763
Iteration 172/1000 | Loss: 0.00003763
Iteration 173/1000 | Loss: 0.00003762
Iteration 174/1000 | Loss: 0.00003762
Iteration 175/1000 | Loss: 0.00003762
Iteration 176/1000 | Loss: 0.00003762
Iteration 177/1000 | Loss: 0.00003762
Iteration 178/1000 | Loss: 0.00003762
Iteration 179/1000 | Loss: 0.00003761
Iteration 180/1000 | Loss: 0.00003761
Iteration 181/1000 | Loss: 0.00003761
Iteration 182/1000 | Loss: 0.00003761
Iteration 183/1000 | Loss: 0.00003761
Iteration 184/1000 | Loss: 0.00003761
Iteration 185/1000 | Loss: 0.00003760
Iteration 186/1000 | Loss: 0.00003760
Iteration 187/1000 | Loss: 0.00003760
Iteration 188/1000 | Loss: 0.00003760
Iteration 189/1000 | Loss: 0.00003759
Iteration 190/1000 | Loss: 0.00003759
Iteration 191/1000 | Loss: 0.00003759
Iteration 192/1000 | Loss: 0.00003759
Iteration 193/1000 | Loss: 0.00003759
Iteration 194/1000 | Loss: 0.00003758
Iteration 195/1000 | Loss: 0.00003758
Iteration 196/1000 | Loss: 0.00003758
Iteration 197/1000 | Loss: 0.00003758
Iteration 198/1000 | Loss: 0.00003758
Iteration 199/1000 | Loss: 0.00003758
Iteration 200/1000 | Loss: 0.00003758
Iteration 201/1000 | Loss: 0.00003758
Iteration 202/1000 | Loss: 0.00003758
Iteration 203/1000 | Loss: 0.00003757
Iteration 204/1000 | Loss: 0.00003757
Iteration 205/1000 | Loss: 0.00003757
Iteration 206/1000 | Loss: 0.00003757
Iteration 207/1000 | Loss: 0.00003757
Iteration 208/1000 | Loss: 0.00003757
Iteration 209/1000 | Loss: 0.00003757
Iteration 210/1000 | Loss: 0.00003757
Iteration 211/1000 | Loss: 0.00003757
Iteration 212/1000 | Loss: 0.00003757
Iteration 213/1000 | Loss: 0.00003756
Iteration 214/1000 | Loss: 0.00003756
Iteration 215/1000 | Loss: 0.00003756
Iteration 216/1000 | Loss: 0.00003756
Iteration 217/1000 | Loss: 0.00003756
Iteration 218/1000 | Loss: 0.00003756
Iteration 219/1000 | Loss: 0.00003756
Iteration 220/1000 | Loss: 0.00003755
Iteration 221/1000 | Loss: 0.00003755
Iteration 222/1000 | Loss: 0.00003755
Iteration 223/1000 | Loss: 0.00003755
Iteration 224/1000 | Loss: 0.00003755
Iteration 225/1000 | Loss: 0.00003755
Iteration 226/1000 | Loss: 0.00003755
Iteration 227/1000 | Loss: 0.00003755
Iteration 228/1000 | Loss: 0.00003755
Iteration 229/1000 | Loss: 0.00003755
Iteration 230/1000 | Loss: 0.00003754
Iteration 231/1000 | Loss: 0.00003754
Iteration 232/1000 | Loss: 0.00003754
Iteration 233/1000 | Loss: 0.00003754
Iteration 234/1000 | Loss: 0.00003754
Iteration 235/1000 | Loss: 0.00003754
Iteration 236/1000 | Loss: 0.00003753
Iteration 237/1000 | Loss: 0.00003753
Iteration 238/1000 | Loss: 0.00003753
Iteration 239/1000 | Loss: 0.00003753
Iteration 240/1000 | Loss: 0.00003753
Iteration 241/1000 | Loss: 0.00003753
Iteration 242/1000 | Loss: 0.00003753
Iteration 243/1000 | Loss: 0.00003753
Iteration 244/1000 | Loss: 0.00003753
Iteration 245/1000 | Loss: 0.00003753
Iteration 246/1000 | Loss: 0.00003753
Iteration 247/1000 | Loss: 0.00003752
Iteration 248/1000 | Loss: 0.00003752
Iteration 249/1000 | Loss: 0.00003752
Iteration 250/1000 | Loss: 0.00003752
Iteration 251/1000 | Loss: 0.00003752
Iteration 252/1000 | Loss: 0.00003752
Iteration 253/1000 | Loss: 0.00003752
Iteration 254/1000 | Loss: 0.00003752
Iteration 255/1000 | Loss: 0.00003752
Iteration 256/1000 | Loss: 0.00003752
Iteration 257/1000 | Loss: 0.00003752
Iteration 258/1000 | Loss: 0.00003752
Iteration 259/1000 | Loss: 0.00003752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [3.75237395928707e-05, 3.75237395928707e-05, 3.75237395928707e-05, 3.75237395928707e-05, 3.75237395928707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.75237395928707e-05

Optimization complete. Final v2v error: 5.344937324523926 mm

Highest mean error: 5.791341781616211 mm for frame 42

Lowest mean error: 4.829213619232178 mm for frame 190

Saving results

Total time: 191.052081823349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021787
Iteration 2/25 | Loss: 0.00189762
Iteration 3/25 | Loss: 0.00175617
Iteration 4/25 | Loss: 0.00172729
Iteration 5/25 | Loss: 0.00172005
Iteration 6/25 | Loss: 0.00171666
Iteration 7/25 | Loss: 0.00171656
Iteration 8/25 | Loss: 0.00171656
Iteration 9/25 | Loss: 0.00171656
Iteration 10/25 | Loss: 0.00171656
Iteration 11/25 | Loss: 0.00171656
Iteration 12/25 | Loss: 0.00171656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017165560275316238, 0.0017165560275316238, 0.0017165560275316238, 0.0017165560275316238, 0.0017165560275316238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017165560275316238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37406480
Iteration 2/25 | Loss: 0.00184129
Iteration 3/25 | Loss: 0.00184119
Iteration 4/25 | Loss: 0.00184119
Iteration 5/25 | Loss: 0.00184119
Iteration 6/25 | Loss: 0.00184119
Iteration 7/25 | Loss: 0.00184119
Iteration 8/25 | Loss: 0.00184118
Iteration 9/25 | Loss: 0.00184118
Iteration 10/25 | Loss: 0.00184118
Iteration 11/25 | Loss: 0.00184118
Iteration 12/25 | Loss: 0.00184118
Iteration 13/25 | Loss: 0.00184118
Iteration 14/25 | Loss: 0.00184118
Iteration 15/25 | Loss: 0.00184118
Iteration 16/25 | Loss: 0.00184118
Iteration 17/25 | Loss: 0.00184118
Iteration 18/25 | Loss: 0.00184118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018411849159747362, 0.0018411849159747362, 0.0018411849159747362, 0.0018411849159747362, 0.0018411849159747362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018411849159747362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184118
Iteration 2/1000 | Loss: 0.00008382
Iteration 3/1000 | Loss: 0.00006069
Iteration 4/1000 | Loss: 0.00005357
Iteration 5/1000 | Loss: 0.00004994
Iteration 6/1000 | Loss: 0.00004746
Iteration 7/1000 | Loss: 0.00004588
Iteration 8/1000 | Loss: 0.00004379
Iteration 9/1000 | Loss: 0.00004261
Iteration 10/1000 | Loss: 0.00004183
Iteration 11/1000 | Loss: 0.00004114
Iteration 12/1000 | Loss: 0.00004062
Iteration 13/1000 | Loss: 0.00003999
Iteration 14/1000 | Loss: 0.00003967
Iteration 15/1000 | Loss: 0.00003966
Iteration 16/1000 | Loss: 0.00003966
Iteration 17/1000 | Loss: 0.00003950
Iteration 18/1000 | Loss: 0.00003950
Iteration 19/1000 | Loss: 0.00003941
Iteration 20/1000 | Loss: 0.00003932
Iteration 21/1000 | Loss: 0.00003932
Iteration 22/1000 | Loss: 0.00003928
Iteration 23/1000 | Loss: 0.00003927
Iteration 24/1000 | Loss: 0.00003926
Iteration 25/1000 | Loss: 0.00003925
Iteration 26/1000 | Loss: 0.00003922
Iteration 27/1000 | Loss: 0.00003922
Iteration 28/1000 | Loss: 0.00003921
Iteration 29/1000 | Loss: 0.00003921
Iteration 30/1000 | Loss: 0.00003921
Iteration 31/1000 | Loss: 0.00003921
Iteration 32/1000 | Loss: 0.00003920
Iteration 33/1000 | Loss: 0.00003919
Iteration 34/1000 | Loss: 0.00003919
Iteration 35/1000 | Loss: 0.00003918
Iteration 36/1000 | Loss: 0.00003918
Iteration 37/1000 | Loss: 0.00003918
Iteration 38/1000 | Loss: 0.00003917
Iteration 39/1000 | Loss: 0.00003917
Iteration 40/1000 | Loss: 0.00003917
Iteration 41/1000 | Loss: 0.00003914
Iteration 42/1000 | Loss: 0.00003913
Iteration 43/1000 | Loss: 0.00003913
Iteration 44/1000 | Loss: 0.00003913
Iteration 45/1000 | Loss: 0.00003912
Iteration 46/1000 | Loss: 0.00003912
Iteration 47/1000 | Loss: 0.00003910
Iteration 48/1000 | Loss: 0.00003910
Iteration 49/1000 | Loss: 0.00003910
Iteration 50/1000 | Loss: 0.00003910
Iteration 51/1000 | Loss: 0.00003910
Iteration 52/1000 | Loss: 0.00003909
Iteration 53/1000 | Loss: 0.00003909
Iteration 54/1000 | Loss: 0.00003909
Iteration 55/1000 | Loss: 0.00003909
Iteration 56/1000 | Loss: 0.00003909
Iteration 57/1000 | Loss: 0.00003908
Iteration 58/1000 | Loss: 0.00003908
Iteration 59/1000 | Loss: 0.00003908
Iteration 60/1000 | Loss: 0.00003908
Iteration 61/1000 | Loss: 0.00003907
Iteration 62/1000 | Loss: 0.00003907
Iteration 63/1000 | Loss: 0.00003907
Iteration 64/1000 | Loss: 0.00003907
Iteration 65/1000 | Loss: 0.00003907
Iteration 66/1000 | Loss: 0.00003907
Iteration 67/1000 | Loss: 0.00003907
Iteration 68/1000 | Loss: 0.00003907
Iteration 69/1000 | Loss: 0.00003907
Iteration 70/1000 | Loss: 0.00003906
Iteration 71/1000 | Loss: 0.00003906
Iteration 72/1000 | Loss: 0.00003906
Iteration 73/1000 | Loss: 0.00003906
Iteration 74/1000 | Loss: 0.00003905
Iteration 75/1000 | Loss: 0.00003905
Iteration 76/1000 | Loss: 0.00003905
Iteration 77/1000 | Loss: 0.00003904
Iteration 78/1000 | Loss: 0.00003904
Iteration 79/1000 | Loss: 0.00003904
Iteration 80/1000 | Loss: 0.00003904
Iteration 81/1000 | Loss: 0.00003904
Iteration 82/1000 | Loss: 0.00003904
Iteration 83/1000 | Loss: 0.00003903
Iteration 84/1000 | Loss: 0.00003903
Iteration 85/1000 | Loss: 0.00003903
Iteration 86/1000 | Loss: 0.00003903
Iteration 87/1000 | Loss: 0.00003903
Iteration 88/1000 | Loss: 0.00003902
Iteration 89/1000 | Loss: 0.00003902
Iteration 90/1000 | Loss: 0.00003902
Iteration 91/1000 | Loss: 0.00003902
Iteration 92/1000 | Loss: 0.00003901
Iteration 93/1000 | Loss: 0.00003901
Iteration 94/1000 | Loss: 0.00003901
Iteration 95/1000 | Loss: 0.00003901
Iteration 96/1000 | Loss: 0.00003901
Iteration 97/1000 | Loss: 0.00003900
Iteration 98/1000 | Loss: 0.00003900
Iteration 99/1000 | Loss: 0.00003900
Iteration 100/1000 | Loss: 0.00003900
Iteration 101/1000 | Loss: 0.00003900
Iteration 102/1000 | Loss: 0.00003900
Iteration 103/1000 | Loss: 0.00003900
Iteration 104/1000 | Loss: 0.00003900
Iteration 105/1000 | Loss: 0.00003900
Iteration 106/1000 | Loss: 0.00003900
Iteration 107/1000 | Loss: 0.00003900
Iteration 108/1000 | Loss: 0.00003900
Iteration 109/1000 | Loss: 0.00003900
Iteration 110/1000 | Loss: 0.00003900
Iteration 111/1000 | Loss: 0.00003900
Iteration 112/1000 | Loss: 0.00003900
Iteration 113/1000 | Loss: 0.00003900
Iteration 114/1000 | Loss: 0.00003900
Iteration 115/1000 | Loss: 0.00003900
Iteration 116/1000 | Loss: 0.00003900
Iteration 117/1000 | Loss: 0.00003900
Iteration 118/1000 | Loss: 0.00003900
Iteration 119/1000 | Loss: 0.00003900
Iteration 120/1000 | Loss: 0.00003900
Iteration 121/1000 | Loss: 0.00003900
Iteration 122/1000 | Loss: 0.00003900
Iteration 123/1000 | Loss: 0.00003900
Iteration 124/1000 | Loss: 0.00003900
Iteration 125/1000 | Loss: 0.00003900
Iteration 126/1000 | Loss: 0.00003900
Iteration 127/1000 | Loss: 0.00003900
Iteration 128/1000 | Loss: 0.00003900
Iteration 129/1000 | Loss: 0.00003900
Iteration 130/1000 | Loss: 0.00003900
Iteration 131/1000 | Loss: 0.00003900
Iteration 132/1000 | Loss: 0.00003900
Iteration 133/1000 | Loss: 0.00003900
Iteration 134/1000 | Loss: 0.00003900
Iteration 135/1000 | Loss: 0.00003900
Iteration 136/1000 | Loss: 0.00003900
Iteration 137/1000 | Loss: 0.00003900
Iteration 138/1000 | Loss: 0.00003900
Iteration 139/1000 | Loss: 0.00003900
Iteration 140/1000 | Loss: 0.00003900
Iteration 141/1000 | Loss: 0.00003900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.8999405660433695e-05, 3.8999405660433695e-05, 3.8999405660433695e-05, 3.8999405660433695e-05, 3.8999405660433695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8999405660433695e-05

Optimization complete. Final v2v error: 5.554031848907471 mm

Highest mean error: 5.831179618835449 mm for frame 22

Lowest mean error: 4.993058204650879 mm for frame 72

Saving results

Total time: 46.81155848503113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959206
Iteration 2/25 | Loss: 0.00169010
Iteration 3/25 | Loss: 0.00157247
Iteration 4/25 | Loss: 0.00155314
Iteration 5/25 | Loss: 0.00154361
Iteration 6/25 | Loss: 0.00154157
Iteration 7/25 | Loss: 0.00154120
Iteration 8/25 | Loss: 0.00154120
Iteration 9/25 | Loss: 0.00154120
Iteration 10/25 | Loss: 0.00154120
Iteration 11/25 | Loss: 0.00154120
Iteration 12/25 | Loss: 0.00154120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015411964850500226, 0.0015411964850500226, 0.0015411964850500226, 0.0015411964850500226, 0.0015411964850500226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015411964850500226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27287817
Iteration 2/25 | Loss: 0.00136802
Iteration 3/25 | Loss: 0.00136802
Iteration 4/25 | Loss: 0.00136802
Iteration 5/25 | Loss: 0.00136802
Iteration 6/25 | Loss: 0.00136802
Iteration 7/25 | Loss: 0.00136802
Iteration 8/25 | Loss: 0.00136802
Iteration 9/25 | Loss: 0.00136802
Iteration 10/25 | Loss: 0.00136802
Iteration 11/25 | Loss: 0.00136802
Iteration 12/25 | Loss: 0.00136802
Iteration 13/25 | Loss: 0.00136802
Iteration 14/25 | Loss: 0.00136802
Iteration 15/25 | Loss: 0.00136802
Iteration 16/25 | Loss: 0.00136802
Iteration 17/25 | Loss: 0.00136802
Iteration 18/25 | Loss: 0.00136802
Iteration 19/25 | Loss: 0.00136802
Iteration 20/25 | Loss: 0.00136802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001368015888147056, 0.001368015888147056, 0.001368015888147056, 0.001368015888147056, 0.001368015888147056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001368015888147056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136802
Iteration 2/1000 | Loss: 0.00005199
Iteration 3/1000 | Loss: 0.00003664
Iteration 4/1000 | Loss: 0.00003387
Iteration 5/1000 | Loss: 0.00003230
Iteration 6/1000 | Loss: 0.00003115
Iteration 7/1000 | Loss: 0.00003061
Iteration 8/1000 | Loss: 0.00003004
Iteration 9/1000 | Loss: 0.00002978
Iteration 10/1000 | Loss: 0.00002944
Iteration 11/1000 | Loss: 0.00002934
Iteration 12/1000 | Loss: 0.00002922
Iteration 13/1000 | Loss: 0.00002919
Iteration 14/1000 | Loss: 0.00002911
Iteration 15/1000 | Loss: 0.00002911
Iteration 16/1000 | Loss: 0.00002910
Iteration 17/1000 | Loss: 0.00002910
Iteration 18/1000 | Loss: 0.00002909
Iteration 19/1000 | Loss: 0.00002908
Iteration 20/1000 | Loss: 0.00002906
Iteration 21/1000 | Loss: 0.00002906
Iteration 22/1000 | Loss: 0.00002906
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002905
Iteration 25/1000 | Loss: 0.00002905
Iteration 26/1000 | Loss: 0.00002904
Iteration 27/1000 | Loss: 0.00002903
Iteration 28/1000 | Loss: 0.00002903
Iteration 29/1000 | Loss: 0.00002903
Iteration 30/1000 | Loss: 0.00002903
Iteration 31/1000 | Loss: 0.00002903
Iteration 32/1000 | Loss: 0.00002903
Iteration 33/1000 | Loss: 0.00002902
Iteration 34/1000 | Loss: 0.00002902
Iteration 35/1000 | Loss: 0.00002902
Iteration 36/1000 | Loss: 0.00002902
Iteration 37/1000 | Loss: 0.00002902
Iteration 38/1000 | Loss: 0.00002901
Iteration 39/1000 | Loss: 0.00002901
Iteration 40/1000 | Loss: 0.00002901
Iteration 41/1000 | Loss: 0.00002900
Iteration 42/1000 | Loss: 0.00002900
Iteration 43/1000 | Loss: 0.00002899
Iteration 44/1000 | Loss: 0.00002897
Iteration 45/1000 | Loss: 0.00002897
Iteration 46/1000 | Loss: 0.00002897
Iteration 47/1000 | Loss: 0.00002897
Iteration 48/1000 | Loss: 0.00002897
Iteration 49/1000 | Loss: 0.00002897
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002896
Iteration 52/1000 | Loss: 0.00002896
Iteration 53/1000 | Loss: 0.00002895
Iteration 54/1000 | Loss: 0.00002895
Iteration 55/1000 | Loss: 0.00002894
Iteration 56/1000 | Loss: 0.00002894
Iteration 57/1000 | Loss: 0.00002893
Iteration 58/1000 | Loss: 0.00002893
Iteration 59/1000 | Loss: 0.00002893
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002892
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002892
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002891
Iteration 67/1000 | Loss: 0.00002891
Iteration 68/1000 | Loss: 0.00002891
Iteration 69/1000 | Loss: 0.00002890
Iteration 70/1000 | Loss: 0.00002890
Iteration 71/1000 | Loss: 0.00002890
Iteration 72/1000 | Loss: 0.00002889
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002889
Iteration 75/1000 | Loss: 0.00002889
Iteration 76/1000 | Loss: 0.00002888
Iteration 77/1000 | Loss: 0.00002888
Iteration 78/1000 | Loss: 0.00002888
Iteration 79/1000 | Loss: 0.00002888
Iteration 80/1000 | Loss: 0.00002888
Iteration 81/1000 | Loss: 0.00002888
Iteration 82/1000 | Loss: 0.00002887
Iteration 83/1000 | Loss: 0.00002887
Iteration 84/1000 | Loss: 0.00002887
Iteration 85/1000 | Loss: 0.00002887
Iteration 86/1000 | Loss: 0.00002887
Iteration 87/1000 | Loss: 0.00002886
Iteration 88/1000 | Loss: 0.00002886
Iteration 89/1000 | Loss: 0.00002886
Iteration 90/1000 | Loss: 0.00002886
Iteration 91/1000 | Loss: 0.00002886
Iteration 92/1000 | Loss: 0.00002886
Iteration 93/1000 | Loss: 0.00002886
Iteration 94/1000 | Loss: 0.00002886
Iteration 95/1000 | Loss: 0.00002886
Iteration 96/1000 | Loss: 0.00002886
Iteration 97/1000 | Loss: 0.00002886
Iteration 98/1000 | Loss: 0.00002886
Iteration 99/1000 | Loss: 0.00002886
Iteration 100/1000 | Loss: 0.00002886
Iteration 101/1000 | Loss: 0.00002886
Iteration 102/1000 | Loss: 0.00002886
Iteration 103/1000 | Loss: 0.00002886
Iteration 104/1000 | Loss: 0.00002886
Iteration 105/1000 | Loss: 0.00002886
Iteration 106/1000 | Loss: 0.00002886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.8857088182121515e-05, 2.8857088182121515e-05, 2.8857088182121515e-05, 2.8857088182121515e-05, 2.8857088182121515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8857088182121515e-05

Optimization complete. Final v2v error: 4.722944259643555 mm

Highest mean error: 5.319586753845215 mm for frame 89

Lowest mean error: 4.271857738494873 mm for frame 31

Saving results

Total time: 33.599541425704956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492458
Iteration 2/25 | Loss: 0.00164107
Iteration 3/25 | Loss: 0.00156644
Iteration 4/25 | Loss: 0.00155293
Iteration 5/25 | Loss: 0.00154753
Iteration 6/25 | Loss: 0.00154649
Iteration 7/25 | Loss: 0.00154649
Iteration 8/25 | Loss: 0.00154649
Iteration 9/25 | Loss: 0.00154649
Iteration 10/25 | Loss: 0.00154649
Iteration 11/25 | Loss: 0.00154649
Iteration 12/25 | Loss: 0.00154649
Iteration 13/25 | Loss: 0.00154649
Iteration 14/25 | Loss: 0.00154649
Iteration 15/25 | Loss: 0.00154649
Iteration 16/25 | Loss: 0.00154649
Iteration 17/25 | Loss: 0.00154649
Iteration 18/25 | Loss: 0.00154649
Iteration 19/25 | Loss: 0.00154649
Iteration 20/25 | Loss: 0.00154649
Iteration 21/25 | Loss: 0.00154649
Iteration 22/25 | Loss: 0.00154649
Iteration 23/25 | Loss: 0.00154649
Iteration 24/25 | Loss: 0.00154649
Iteration 25/25 | Loss: 0.00154649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41289926
Iteration 2/25 | Loss: 0.00162657
Iteration 3/25 | Loss: 0.00162657
Iteration 4/25 | Loss: 0.00162657
Iteration 5/25 | Loss: 0.00162657
Iteration 6/25 | Loss: 0.00162657
Iteration 7/25 | Loss: 0.00162657
Iteration 8/25 | Loss: 0.00162657
Iteration 9/25 | Loss: 0.00162657
Iteration 10/25 | Loss: 0.00162657
Iteration 11/25 | Loss: 0.00162657
Iteration 12/25 | Loss: 0.00162657
Iteration 13/25 | Loss: 0.00162657
Iteration 14/25 | Loss: 0.00162657
Iteration 15/25 | Loss: 0.00162657
Iteration 16/25 | Loss: 0.00162657
Iteration 17/25 | Loss: 0.00162657
Iteration 18/25 | Loss: 0.00162657
Iteration 19/25 | Loss: 0.00162657
Iteration 20/25 | Loss: 0.00162657
Iteration 21/25 | Loss: 0.00162657
Iteration 22/25 | Loss: 0.00162657
Iteration 23/25 | Loss: 0.00162657
Iteration 24/25 | Loss: 0.00162657
Iteration 25/25 | Loss: 0.00162657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162657
Iteration 2/1000 | Loss: 0.00005977
Iteration 3/1000 | Loss: 0.00004348
Iteration 4/1000 | Loss: 0.00003762
Iteration 5/1000 | Loss: 0.00003467
Iteration 6/1000 | Loss: 0.00003346
Iteration 7/1000 | Loss: 0.00003265
Iteration 8/1000 | Loss: 0.00003218
Iteration 9/1000 | Loss: 0.00003217
Iteration 10/1000 | Loss: 0.00003207
Iteration 11/1000 | Loss: 0.00003176
Iteration 12/1000 | Loss: 0.00003141
Iteration 13/1000 | Loss: 0.00003123
Iteration 14/1000 | Loss: 0.00003110
Iteration 15/1000 | Loss: 0.00003099
Iteration 16/1000 | Loss: 0.00003094
Iteration 17/1000 | Loss: 0.00003094
Iteration 18/1000 | Loss: 0.00003086
Iteration 19/1000 | Loss: 0.00003086
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003086
Iteration 22/1000 | Loss: 0.00003086
Iteration 23/1000 | Loss: 0.00003086
Iteration 24/1000 | Loss: 0.00003085
Iteration 25/1000 | Loss: 0.00003085
Iteration 26/1000 | Loss: 0.00003083
Iteration 27/1000 | Loss: 0.00003083
Iteration 28/1000 | Loss: 0.00003081
Iteration 29/1000 | Loss: 0.00003081
Iteration 30/1000 | Loss: 0.00003081
Iteration 31/1000 | Loss: 0.00003081
Iteration 32/1000 | Loss: 0.00003081
Iteration 33/1000 | Loss: 0.00003081
Iteration 34/1000 | Loss: 0.00003081
Iteration 35/1000 | Loss: 0.00003081
Iteration 36/1000 | Loss: 0.00003081
Iteration 37/1000 | Loss: 0.00003080
Iteration 38/1000 | Loss: 0.00003080
Iteration 39/1000 | Loss: 0.00003078
Iteration 40/1000 | Loss: 0.00003078
Iteration 41/1000 | Loss: 0.00003077
Iteration 42/1000 | Loss: 0.00003077
Iteration 43/1000 | Loss: 0.00003077
Iteration 44/1000 | Loss: 0.00003077
Iteration 45/1000 | Loss: 0.00003077
Iteration 46/1000 | Loss: 0.00003077
Iteration 47/1000 | Loss: 0.00003077
Iteration 48/1000 | Loss: 0.00003077
Iteration 49/1000 | Loss: 0.00003076
Iteration 50/1000 | Loss: 0.00003075
Iteration 51/1000 | Loss: 0.00003073
Iteration 52/1000 | Loss: 0.00003073
Iteration 53/1000 | Loss: 0.00003072
Iteration 54/1000 | Loss: 0.00003072
Iteration 55/1000 | Loss: 0.00003071
Iteration 56/1000 | Loss: 0.00003071
Iteration 57/1000 | Loss: 0.00003071
Iteration 58/1000 | Loss: 0.00003071
Iteration 59/1000 | Loss: 0.00003071
Iteration 60/1000 | Loss: 0.00003070
Iteration 61/1000 | Loss: 0.00003070
Iteration 62/1000 | Loss: 0.00003070
Iteration 63/1000 | Loss: 0.00003070
Iteration 64/1000 | Loss: 0.00003070
Iteration 65/1000 | Loss: 0.00003069
Iteration 66/1000 | Loss: 0.00003069
Iteration 67/1000 | Loss: 0.00003069
Iteration 68/1000 | Loss: 0.00003069
Iteration 69/1000 | Loss: 0.00003069
Iteration 70/1000 | Loss: 0.00003068
Iteration 71/1000 | Loss: 0.00003068
Iteration 72/1000 | Loss: 0.00003068
Iteration 73/1000 | Loss: 0.00003068
Iteration 74/1000 | Loss: 0.00003067
Iteration 75/1000 | Loss: 0.00003067
Iteration 76/1000 | Loss: 0.00003067
Iteration 77/1000 | Loss: 0.00003067
Iteration 78/1000 | Loss: 0.00003067
Iteration 79/1000 | Loss: 0.00003067
Iteration 80/1000 | Loss: 0.00003067
Iteration 81/1000 | Loss: 0.00003066
Iteration 82/1000 | Loss: 0.00003066
Iteration 83/1000 | Loss: 0.00003066
Iteration 84/1000 | Loss: 0.00003066
Iteration 85/1000 | Loss: 0.00003066
Iteration 86/1000 | Loss: 0.00003066
Iteration 87/1000 | Loss: 0.00003066
Iteration 88/1000 | Loss: 0.00003066
Iteration 89/1000 | Loss: 0.00003066
Iteration 90/1000 | Loss: 0.00003066
Iteration 91/1000 | Loss: 0.00003066
Iteration 92/1000 | Loss: 0.00003066
Iteration 93/1000 | Loss: 0.00003066
Iteration 94/1000 | Loss: 0.00003066
Iteration 95/1000 | Loss: 0.00003066
Iteration 96/1000 | Loss: 0.00003066
Iteration 97/1000 | Loss: 0.00003065
Iteration 98/1000 | Loss: 0.00003065
Iteration 99/1000 | Loss: 0.00003065
Iteration 100/1000 | Loss: 0.00003065
Iteration 101/1000 | Loss: 0.00003065
Iteration 102/1000 | Loss: 0.00003065
Iteration 103/1000 | Loss: 0.00003065
Iteration 104/1000 | Loss: 0.00003065
Iteration 105/1000 | Loss: 0.00003065
Iteration 106/1000 | Loss: 0.00003065
Iteration 107/1000 | Loss: 0.00003064
Iteration 108/1000 | Loss: 0.00003064
Iteration 109/1000 | Loss: 0.00003064
Iteration 110/1000 | Loss: 0.00003064
Iteration 111/1000 | Loss: 0.00003064
Iteration 112/1000 | Loss: 0.00003064
Iteration 113/1000 | Loss: 0.00003064
Iteration 114/1000 | Loss: 0.00003063
Iteration 115/1000 | Loss: 0.00003063
Iteration 116/1000 | Loss: 0.00003063
Iteration 117/1000 | Loss: 0.00003063
Iteration 118/1000 | Loss: 0.00003063
Iteration 119/1000 | Loss: 0.00003063
Iteration 120/1000 | Loss: 0.00003062
Iteration 121/1000 | Loss: 0.00003062
Iteration 122/1000 | Loss: 0.00003062
Iteration 123/1000 | Loss: 0.00003062
Iteration 124/1000 | Loss: 0.00003062
Iteration 125/1000 | Loss: 0.00003062
Iteration 126/1000 | Loss: 0.00003062
Iteration 127/1000 | Loss: 0.00003062
Iteration 128/1000 | Loss: 0.00003062
Iteration 129/1000 | Loss: 0.00003062
Iteration 130/1000 | Loss: 0.00003062
Iteration 131/1000 | Loss: 0.00003062
Iteration 132/1000 | Loss: 0.00003062
Iteration 133/1000 | Loss: 0.00003062
Iteration 134/1000 | Loss: 0.00003062
Iteration 135/1000 | Loss: 0.00003062
Iteration 136/1000 | Loss: 0.00003062
Iteration 137/1000 | Loss: 0.00003062
Iteration 138/1000 | Loss: 0.00003062
Iteration 139/1000 | Loss: 0.00003062
Iteration 140/1000 | Loss: 0.00003062
Iteration 141/1000 | Loss: 0.00003062
Iteration 142/1000 | Loss: 0.00003062
Iteration 143/1000 | Loss: 0.00003062
Iteration 144/1000 | Loss: 0.00003062
Iteration 145/1000 | Loss: 0.00003062
Iteration 146/1000 | Loss: 0.00003062
Iteration 147/1000 | Loss: 0.00003062
Iteration 148/1000 | Loss: 0.00003062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [3.061877214349806e-05, 3.061877214349806e-05, 3.061877214349806e-05, 3.061877214349806e-05, 3.061877214349806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.061877214349806e-05

Optimization complete. Final v2v error: 4.918859481811523 mm

Highest mean error: 5.141371250152588 mm for frame 68

Lowest mean error: 4.597783088684082 mm for frame 139

Saving results

Total time: 39.6987566947937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01249987
Iteration 2/25 | Loss: 0.01249987
Iteration 3/25 | Loss: 0.01249986
Iteration 4/25 | Loss: 0.00229608
Iteration 5/25 | Loss: 0.00163468
Iteration 6/25 | Loss: 0.00158067
Iteration 7/25 | Loss: 0.00161149
Iteration 8/25 | Loss: 0.00156281
Iteration 9/25 | Loss: 0.00155812
Iteration 10/25 | Loss: 0.00154988
Iteration 11/25 | Loss: 0.00154946
Iteration 12/25 | Loss: 0.00154931
Iteration 13/25 | Loss: 0.00155245
Iteration 14/25 | Loss: 0.00154717
Iteration 15/25 | Loss: 0.00154653
Iteration 16/25 | Loss: 0.00154650
Iteration 17/25 | Loss: 0.00154650
Iteration 18/25 | Loss: 0.00154649
Iteration 19/25 | Loss: 0.00154649
Iteration 20/25 | Loss: 0.00154649
Iteration 21/25 | Loss: 0.00154649
Iteration 22/25 | Loss: 0.00154649
Iteration 23/25 | Loss: 0.00154649
Iteration 24/25 | Loss: 0.00154649
Iteration 25/25 | Loss: 0.00154649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42231190
Iteration 2/25 | Loss: 0.00132099
Iteration 3/25 | Loss: 0.00132099
Iteration 4/25 | Loss: 0.00132099
Iteration 5/25 | Loss: 0.00132099
Iteration 6/25 | Loss: 0.00132099
Iteration 7/25 | Loss: 0.00132099
Iteration 8/25 | Loss: 0.00132099
Iteration 9/25 | Loss: 0.00132099
Iteration 10/25 | Loss: 0.00132099
Iteration 11/25 | Loss: 0.00132099
Iteration 12/25 | Loss: 0.00132099
Iteration 13/25 | Loss: 0.00132099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013209861936047673, 0.0013209861936047673, 0.0013209861936047673, 0.0013209861936047673, 0.0013209861936047673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013209861936047673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132099
Iteration 2/1000 | Loss: 0.00007259
Iteration 3/1000 | Loss: 0.00004710
Iteration 4/1000 | Loss: 0.00004302
Iteration 5/1000 | Loss: 0.00004091
Iteration 6/1000 | Loss: 0.00003948
Iteration 7/1000 | Loss: 0.00003847
Iteration 8/1000 | Loss: 0.00003787
Iteration 9/1000 | Loss: 0.00003732
Iteration 10/1000 | Loss: 0.00003691
Iteration 11/1000 | Loss: 0.00003663
Iteration 12/1000 | Loss: 0.00003645
Iteration 13/1000 | Loss: 0.00003637
Iteration 14/1000 | Loss: 0.00003635
Iteration 15/1000 | Loss: 0.00003634
Iteration 16/1000 | Loss: 0.00003629
Iteration 17/1000 | Loss: 0.00003628
Iteration 18/1000 | Loss: 0.00003625
Iteration 19/1000 | Loss: 0.00003625
Iteration 20/1000 | Loss: 0.00003625
Iteration 21/1000 | Loss: 0.00003625
Iteration 22/1000 | Loss: 0.00003624
Iteration 23/1000 | Loss: 0.00003624
Iteration 24/1000 | Loss: 0.00003622
Iteration 25/1000 | Loss: 0.00003621
Iteration 26/1000 | Loss: 0.00003621
Iteration 27/1000 | Loss: 0.00003621
Iteration 28/1000 | Loss: 0.00003620
Iteration 29/1000 | Loss: 0.00003620
Iteration 30/1000 | Loss: 0.00003619
Iteration 31/1000 | Loss: 0.00003618
Iteration 32/1000 | Loss: 0.00003618
Iteration 33/1000 | Loss: 0.00003618
Iteration 34/1000 | Loss: 0.00003617
Iteration 35/1000 | Loss: 0.00003617
Iteration 36/1000 | Loss: 0.00003616
Iteration 37/1000 | Loss: 0.00003616
Iteration 38/1000 | Loss: 0.00003616
Iteration 39/1000 | Loss: 0.00003615
Iteration 40/1000 | Loss: 0.00003615
Iteration 41/1000 | Loss: 0.00003615
Iteration 42/1000 | Loss: 0.00003614
Iteration 43/1000 | Loss: 0.00003614
Iteration 44/1000 | Loss: 0.00003614
Iteration 45/1000 | Loss: 0.00003614
Iteration 46/1000 | Loss: 0.00003613
Iteration 47/1000 | Loss: 0.00003613
Iteration 48/1000 | Loss: 0.00003613
Iteration 49/1000 | Loss: 0.00003613
Iteration 50/1000 | Loss: 0.00003613
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003612
Iteration 53/1000 | Loss: 0.00003612
Iteration 54/1000 | Loss: 0.00003612
Iteration 55/1000 | Loss: 0.00003612
Iteration 56/1000 | Loss: 0.00003611
Iteration 57/1000 | Loss: 0.00003611
Iteration 58/1000 | Loss: 0.00003611
Iteration 59/1000 | Loss: 0.00003611
Iteration 60/1000 | Loss: 0.00003610
Iteration 61/1000 | Loss: 0.00003610
Iteration 62/1000 | Loss: 0.00003610
Iteration 63/1000 | Loss: 0.00003610
Iteration 64/1000 | Loss: 0.00003610
Iteration 65/1000 | Loss: 0.00003610
Iteration 66/1000 | Loss: 0.00003610
Iteration 67/1000 | Loss: 0.00003610
Iteration 68/1000 | Loss: 0.00003610
Iteration 69/1000 | Loss: 0.00003610
Iteration 70/1000 | Loss: 0.00003610
Iteration 71/1000 | Loss: 0.00003610
Iteration 72/1000 | Loss: 0.00003610
Iteration 73/1000 | Loss: 0.00003610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [3.6098328564548865e-05, 3.6098328564548865e-05, 3.6098328564548865e-05, 3.6098328564548865e-05, 3.6098328564548865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6098328564548865e-05

Optimization complete. Final v2v error: 5.158716678619385 mm

Highest mean error: 6.1432623863220215 mm for frame 75

Lowest mean error: 4.612656593322754 mm for frame 204

Saving results

Total time: 53.347474098205566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938315
Iteration 2/25 | Loss: 0.00194152
Iteration 3/25 | Loss: 0.00171068
Iteration 4/25 | Loss: 0.00168053
Iteration 5/25 | Loss: 0.00167403
Iteration 6/25 | Loss: 0.00167193
Iteration 7/25 | Loss: 0.00167166
Iteration 8/25 | Loss: 0.00167166
Iteration 9/25 | Loss: 0.00167166
Iteration 10/25 | Loss: 0.00167166
Iteration 11/25 | Loss: 0.00167166
Iteration 12/25 | Loss: 0.00167166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001671664067544043, 0.001671664067544043, 0.001671664067544043, 0.001671664067544043, 0.001671664067544043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001671664067544043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00042105
Iteration 2/25 | Loss: 0.00162195
Iteration 3/25 | Loss: 0.00162195
Iteration 4/25 | Loss: 0.00162194
Iteration 5/25 | Loss: 0.00162194
Iteration 6/25 | Loss: 0.00162194
Iteration 7/25 | Loss: 0.00162194
Iteration 8/25 | Loss: 0.00162194
Iteration 9/25 | Loss: 0.00162194
Iteration 10/25 | Loss: 0.00162194
Iteration 11/25 | Loss: 0.00162194
Iteration 12/25 | Loss: 0.00162194
Iteration 13/25 | Loss: 0.00162194
Iteration 14/25 | Loss: 0.00162194
Iteration 15/25 | Loss: 0.00162194
Iteration 16/25 | Loss: 0.00162194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016219433164224029, 0.0016219433164224029, 0.0016219433164224029, 0.0016219433164224029, 0.0016219433164224029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016219433164224029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162194
Iteration 2/1000 | Loss: 0.00007242
Iteration 3/1000 | Loss: 0.00005893
Iteration 4/1000 | Loss: 0.00005300
Iteration 5/1000 | Loss: 0.00004954
Iteration 6/1000 | Loss: 0.00004810
Iteration 7/1000 | Loss: 0.00004681
Iteration 8/1000 | Loss: 0.00004626
Iteration 9/1000 | Loss: 0.00004587
Iteration 10/1000 | Loss: 0.00004546
Iteration 11/1000 | Loss: 0.00004533
Iteration 12/1000 | Loss: 0.00004531
Iteration 13/1000 | Loss: 0.00004519
Iteration 14/1000 | Loss: 0.00004518
Iteration 15/1000 | Loss: 0.00004510
Iteration 16/1000 | Loss: 0.00004510
Iteration 17/1000 | Loss: 0.00004510
Iteration 18/1000 | Loss: 0.00004510
Iteration 19/1000 | Loss: 0.00004510
Iteration 20/1000 | Loss: 0.00004510
Iteration 21/1000 | Loss: 0.00004509
Iteration 22/1000 | Loss: 0.00004509
Iteration 23/1000 | Loss: 0.00004509
Iteration 24/1000 | Loss: 0.00004509
Iteration 25/1000 | Loss: 0.00004508
Iteration 26/1000 | Loss: 0.00004507
Iteration 27/1000 | Loss: 0.00004504
Iteration 28/1000 | Loss: 0.00004503
Iteration 29/1000 | Loss: 0.00004503
Iteration 30/1000 | Loss: 0.00004502
Iteration 31/1000 | Loss: 0.00004501
Iteration 32/1000 | Loss: 0.00004501
Iteration 33/1000 | Loss: 0.00004500
Iteration 34/1000 | Loss: 0.00004500
Iteration 35/1000 | Loss: 0.00004500
Iteration 36/1000 | Loss: 0.00004500
Iteration 37/1000 | Loss: 0.00004499
Iteration 38/1000 | Loss: 0.00004499
Iteration 39/1000 | Loss: 0.00004499
Iteration 40/1000 | Loss: 0.00004499
Iteration 41/1000 | Loss: 0.00004499
Iteration 42/1000 | Loss: 0.00004499
Iteration 43/1000 | Loss: 0.00004499
Iteration 44/1000 | Loss: 0.00004499
Iteration 45/1000 | Loss: 0.00004498
Iteration 46/1000 | Loss: 0.00004498
Iteration 47/1000 | Loss: 0.00004498
Iteration 48/1000 | Loss: 0.00004497
Iteration 49/1000 | Loss: 0.00004497
Iteration 50/1000 | Loss: 0.00004497
Iteration 51/1000 | Loss: 0.00004497
Iteration 52/1000 | Loss: 0.00004497
Iteration 53/1000 | Loss: 0.00004497
Iteration 54/1000 | Loss: 0.00004497
Iteration 55/1000 | Loss: 0.00004497
Iteration 56/1000 | Loss: 0.00004497
Iteration 57/1000 | Loss: 0.00004496
Iteration 58/1000 | Loss: 0.00004496
Iteration 59/1000 | Loss: 0.00004496
Iteration 60/1000 | Loss: 0.00004496
Iteration 61/1000 | Loss: 0.00004496
Iteration 62/1000 | Loss: 0.00004495
Iteration 63/1000 | Loss: 0.00004495
Iteration 64/1000 | Loss: 0.00004494
Iteration 65/1000 | Loss: 0.00004494
Iteration 66/1000 | Loss: 0.00004494
Iteration 67/1000 | Loss: 0.00004494
Iteration 68/1000 | Loss: 0.00004494
Iteration 69/1000 | Loss: 0.00004494
Iteration 70/1000 | Loss: 0.00004494
Iteration 71/1000 | Loss: 0.00004494
Iteration 72/1000 | Loss: 0.00004494
Iteration 73/1000 | Loss: 0.00004493
Iteration 74/1000 | Loss: 0.00004493
Iteration 75/1000 | Loss: 0.00004493
Iteration 76/1000 | Loss: 0.00004493
Iteration 77/1000 | Loss: 0.00004493
Iteration 78/1000 | Loss: 0.00004493
Iteration 79/1000 | Loss: 0.00004493
Iteration 80/1000 | Loss: 0.00004493
Iteration 81/1000 | Loss: 0.00004493
Iteration 82/1000 | Loss: 0.00004492
Iteration 83/1000 | Loss: 0.00004492
Iteration 84/1000 | Loss: 0.00004492
Iteration 85/1000 | Loss: 0.00004492
Iteration 86/1000 | Loss: 0.00004492
Iteration 87/1000 | Loss: 0.00004492
Iteration 88/1000 | Loss: 0.00004492
Iteration 89/1000 | Loss: 0.00004492
Iteration 90/1000 | Loss: 0.00004492
Iteration 91/1000 | Loss: 0.00004492
Iteration 92/1000 | Loss: 0.00004492
Iteration 93/1000 | Loss: 0.00004492
Iteration 94/1000 | Loss: 0.00004492
Iteration 95/1000 | Loss: 0.00004492
Iteration 96/1000 | Loss: 0.00004492
Iteration 97/1000 | Loss: 0.00004492
Iteration 98/1000 | Loss: 0.00004492
Iteration 99/1000 | Loss: 0.00004492
Iteration 100/1000 | Loss: 0.00004492
Iteration 101/1000 | Loss: 0.00004492
Iteration 102/1000 | Loss: 0.00004492
Iteration 103/1000 | Loss: 0.00004492
Iteration 104/1000 | Loss: 0.00004492
Iteration 105/1000 | Loss: 0.00004492
Iteration 106/1000 | Loss: 0.00004492
Iteration 107/1000 | Loss: 0.00004492
Iteration 108/1000 | Loss: 0.00004492
Iteration 109/1000 | Loss: 0.00004492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [4.492027437663637e-05, 4.492027437663637e-05, 4.492027437663637e-05, 4.492027437663637e-05, 4.492027437663637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.492027437663637e-05

Optimization complete. Final v2v error: 5.71558952331543 mm

Highest mean error: 5.765145778656006 mm for frame 35

Lowest mean error: 5.667243480682373 mm for frame 72

Saving results

Total time: 30.798418045043945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421715
Iteration 2/25 | Loss: 0.00176165
Iteration 3/25 | Loss: 0.00163192
Iteration 4/25 | Loss: 0.00160000
Iteration 5/25 | Loss: 0.00159471
Iteration 6/25 | Loss: 0.00159468
Iteration 7/25 | Loss: 0.00159468
Iteration 8/25 | Loss: 0.00159468
Iteration 9/25 | Loss: 0.00159468
Iteration 10/25 | Loss: 0.00159468
Iteration 11/25 | Loss: 0.00159468
Iteration 12/25 | Loss: 0.00159468
Iteration 13/25 | Loss: 0.00159468
Iteration 14/25 | Loss: 0.00159468
Iteration 15/25 | Loss: 0.00159468
Iteration 16/25 | Loss: 0.00159468
Iteration 17/25 | Loss: 0.00159468
Iteration 18/25 | Loss: 0.00159468
Iteration 19/25 | Loss: 0.00159468
Iteration 20/25 | Loss: 0.00159468
Iteration 21/25 | Loss: 0.00159468
Iteration 22/25 | Loss: 0.00159468
Iteration 23/25 | Loss: 0.00159468
Iteration 24/25 | Loss: 0.00159468
Iteration 25/25 | Loss: 0.00159468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41608536
Iteration 2/25 | Loss: 0.00172678
Iteration 3/25 | Loss: 0.00172677
Iteration 4/25 | Loss: 0.00172677
Iteration 5/25 | Loss: 0.00172677
Iteration 6/25 | Loss: 0.00172677
Iteration 7/25 | Loss: 0.00172677
Iteration 8/25 | Loss: 0.00172677
Iteration 9/25 | Loss: 0.00172677
Iteration 10/25 | Loss: 0.00172677
Iteration 11/25 | Loss: 0.00172677
Iteration 12/25 | Loss: 0.00172677
Iteration 13/25 | Loss: 0.00172677
Iteration 14/25 | Loss: 0.00172677
Iteration 15/25 | Loss: 0.00172677
Iteration 16/25 | Loss: 0.00172677
Iteration 17/25 | Loss: 0.00172677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017267722869291902, 0.0017267722869291902, 0.0017267722869291902, 0.0017267722869291902, 0.0017267722869291902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017267722869291902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172677
Iteration 2/1000 | Loss: 0.00006142
Iteration 3/1000 | Loss: 0.00004395
Iteration 4/1000 | Loss: 0.00003848
Iteration 5/1000 | Loss: 0.00003520
Iteration 6/1000 | Loss: 0.00003309
Iteration 7/1000 | Loss: 0.00003184
Iteration 8/1000 | Loss: 0.00003101
Iteration 9/1000 | Loss: 0.00003024
Iteration 10/1000 | Loss: 0.00002981
Iteration 11/1000 | Loss: 0.00002938
Iteration 12/1000 | Loss: 0.00002903
Iteration 13/1000 | Loss: 0.00002879
Iteration 14/1000 | Loss: 0.00002878
Iteration 15/1000 | Loss: 0.00002877
Iteration 16/1000 | Loss: 0.00002873
Iteration 17/1000 | Loss: 0.00002870
Iteration 18/1000 | Loss: 0.00002870
Iteration 19/1000 | Loss: 0.00002870
Iteration 20/1000 | Loss: 0.00002869
Iteration 21/1000 | Loss: 0.00002858
Iteration 22/1000 | Loss: 0.00002856
Iteration 23/1000 | Loss: 0.00002853
Iteration 24/1000 | Loss: 0.00002853
Iteration 25/1000 | Loss: 0.00002851
Iteration 26/1000 | Loss: 0.00002848
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002843
Iteration 29/1000 | Loss: 0.00002843
Iteration 30/1000 | Loss: 0.00002842
Iteration 31/1000 | Loss: 0.00002841
Iteration 32/1000 | Loss: 0.00002840
Iteration 33/1000 | Loss: 0.00002840
Iteration 34/1000 | Loss: 0.00002840
Iteration 35/1000 | Loss: 0.00002839
Iteration 36/1000 | Loss: 0.00002839
Iteration 37/1000 | Loss: 0.00002836
Iteration 38/1000 | Loss: 0.00002835
Iteration 39/1000 | Loss: 0.00002835
Iteration 40/1000 | Loss: 0.00002835
Iteration 41/1000 | Loss: 0.00002834
Iteration 42/1000 | Loss: 0.00002834
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002833
Iteration 46/1000 | Loss: 0.00002833
Iteration 47/1000 | Loss: 0.00002832
Iteration 48/1000 | Loss: 0.00002832
Iteration 49/1000 | Loss: 0.00002832
Iteration 50/1000 | Loss: 0.00002832
Iteration 51/1000 | Loss: 0.00002831
Iteration 52/1000 | Loss: 0.00002831
Iteration 53/1000 | Loss: 0.00002831
Iteration 54/1000 | Loss: 0.00002831
Iteration 55/1000 | Loss: 0.00002831
Iteration 56/1000 | Loss: 0.00002831
Iteration 57/1000 | Loss: 0.00002830
Iteration 58/1000 | Loss: 0.00002830
Iteration 59/1000 | Loss: 0.00002830
Iteration 60/1000 | Loss: 0.00002830
Iteration 61/1000 | Loss: 0.00002830
Iteration 62/1000 | Loss: 0.00002829
Iteration 63/1000 | Loss: 0.00002829
Iteration 64/1000 | Loss: 0.00002829
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00002829
Iteration 67/1000 | Loss: 0.00002829
Iteration 68/1000 | Loss: 0.00002829
Iteration 69/1000 | Loss: 0.00002829
Iteration 70/1000 | Loss: 0.00002828
Iteration 71/1000 | Loss: 0.00002828
Iteration 72/1000 | Loss: 0.00002828
Iteration 73/1000 | Loss: 0.00002828
Iteration 74/1000 | Loss: 0.00002828
Iteration 75/1000 | Loss: 0.00002828
Iteration 76/1000 | Loss: 0.00002828
Iteration 77/1000 | Loss: 0.00002827
Iteration 78/1000 | Loss: 0.00002827
Iteration 79/1000 | Loss: 0.00002827
Iteration 80/1000 | Loss: 0.00002827
Iteration 81/1000 | Loss: 0.00002826
Iteration 82/1000 | Loss: 0.00002826
Iteration 83/1000 | Loss: 0.00002826
Iteration 84/1000 | Loss: 0.00002826
Iteration 85/1000 | Loss: 0.00002826
Iteration 86/1000 | Loss: 0.00002825
Iteration 87/1000 | Loss: 0.00002825
Iteration 88/1000 | Loss: 0.00002825
Iteration 89/1000 | Loss: 0.00002825
Iteration 90/1000 | Loss: 0.00002825
Iteration 91/1000 | Loss: 0.00002825
Iteration 92/1000 | Loss: 0.00002824
Iteration 93/1000 | Loss: 0.00002824
Iteration 94/1000 | Loss: 0.00002824
Iteration 95/1000 | Loss: 0.00002824
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00002824
Iteration 98/1000 | Loss: 0.00002824
Iteration 99/1000 | Loss: 0.00002823
Iteration 100/1000 | Loss: 0.00002823
Iteration 101/1000 | Loss: 0.00002823
Iteration 102/1000 | Loss: 0.00002823
Iteration 103/1000 | Loss: 0.00002823
Iteration 104/1000 | Loss: 0.00002823
Iteration 105/1000 | Loss: 0.00002823
Iteration 106/1000 | Loss: 0.00002822
Iteration 107/1000 | Loss: 0.00002822
Iteration 108/1000 | Loss: 0.00002822
Iteration 109/1000 | Loss: 0.00002822
Iteration 110/1000 | Loss: 0.00002822
Iteration 111/1000 | Loss: 0.00002821
Iteration 112/1000 | Loss: 0.00002821
Iteration 113/1000 | Loss: 0.00002821
Iteration 114/1000 | Loss: 0.00002821
Iteration 115/1000 | Loss: 0.00002821
Iteration 116/1000 | Loss: 0.00002821
Iteration 117/1000 | Loss: 0.00002820
Iteration 118/1000 | Loss: 0.00002820
Iteration 119/1000 | Loss: 0.00002820
Iteration 120/1000 | Loss: 0.00002820
Iteration 121/1000 | Loss: 0.00002820
Iteration 122/1000 | Loss: 0.00002820
Iteration 123/1000 | Loss: 0.00002819
Iteration 124/1000 | Loss: 0.00002819
Iteration 125/1000 | Loss: 0.00002819
Iteration 126/1000 | Loss: 0.00002819
Iteration 127/1000 | Loss: 0.00002819
Iteration 128/1000 | Loss: 0.00002819
Iteration 129/1000 | Loss: 0.00002819
Iteration 130/1000 | Loss: 0.00002819
Iteration 131/1000 | Loss: 0.00002819
Iteration 132/1000 | Loss: 0.00002819
Iteration 133/1000 | Loss: 0.00002818
Iteration 134/1000 | Loss: 0.00002818
Iteration 135/1000 | Loss: 0.00002818
Iteration 136/1000 | Loss: 0.00002818
Iteration 137/1000 | Loss: 0.00002818
Iteration 138/1000 | Loss: 0.00002818
Iteration 139/1000 | Loss: 0.00002818
Iteration 140/1000 | Loss: 0.00002818
Iteration 141/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.8184091206640005e-05, 2.8184091206640005e-05, 2.8184091206640005e-05, 2.8184091206640005e-05, 2.8184091206640005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8184091206640005e-05

Optimization complete. Final v2v error: 4.677654266357422 mm

Highest mean error: 5.156173229217529 mm for frame 28

Lowest mean error: 4.314576148986816 mm for frame 105

Saving results

Total time: 44.0189905166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702583
Iteration 2/25 | Loss: 0.00195660
Iteration 3/25 | Loss: 0.00177178
Iteration 4/25 | Loss: 0.00175738
Iteration 5/25 | Loss: 0.00169582
Iteration 6/25 | Loss: 0.00166705
Iteration 7/25 | Loss: 0.00165372
Iteration 8/25 | Loss: 0.00164638
Iteration 9/25 | Loss: 0.00164446
Iteration 10/25 | Loss: 0.00164405
Iteration 11/25 | Loss: 0.00164387
Iteration 12/25 | Loss: 0.00164374
Iteration 13/25 | Loss: 0.00164421
Iteration 14/25 | Loss: 0.00164016
Iteration 15/25 | Loss: 0.00163898
Iteration 16/25 | Loss: 0.00163859
Iteration 17/25 | Loss: 0.00163849
Iteration 18/25 | Loss: 0.00163849
Iteration 19/25 | Loss: 0.00163849
Iteration 20/25 | Loss: 0.00163849
Iteration 21/25 | Loss: 0.00163848
Iteration 22/25 | Loss: 0.00163848
Iteration 23/25 | Loss: 0.00163848
Iteration 24/25 | Loss: 0.00163848
Iteration 25/25 | Loss: 0.00163848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30593121
Iteration 2/25 | Loss: 0.00137743
Iteration 3/25 | Loss: 0.00137735
Iteration 4/25 | Loss: 0.00137735
Iteration 5/25 | Loss: 0.00137735
Iteration 6/25 | Loss: 0.00137735
Iteration 7/25 | Loss: 0.00137735
Iteration 8/25 | Loss: 0.00137735
Iteration 9/25 | Loss: 0.00137735
Iteration 10/25 | Loss: 0.00137735
Iteration 11/25 | Loss: 0.00137735
Iteration 12/25 | Loss: 0.00137735
Iteration 13/25 | Loss: 0.00137735
Iteration 14/25 | Loss: 0.00137735
Iteration 15/25 | Loss: 0.00137735
Iteration 16/25 | Loss: 0.00137735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013773459941148758, 0.0013773459941148758, 0.0013773459941148758, 0.0013773459941148758, 0.0013773459941148758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013773459941148758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137735
Iteration 2/1000 | Loss: 0.00009387
Iteration 3/1000 | Loss: 0.00006735
Iteration 4/1000 | Loss: 0.00006046
Iteration 5/1000 | Loss: 0.00005688
Iteration 6/1000 | Loss: 0.00005430
Iteration 7/1000 | Loss: 0.00005205
Iteration 8/1000 | Loss: 0.00005042
Iteration 9/1000 | Loss: 0.00004909
Iteration 10/1000 | Loss: 0.00004786
Iteration 11/1000 | Loss: 0.00004699
Iteration 12/1000 | Loss: 0.00004634
Iteration 13/1000 | Loss: 0.00004578
Iteration 14/1000 | Loss: 0.00004533
Iteration 15/1000 | Loss: 0.00004501
Iteration 16/1000 | Loss: 0.00004482
Iteration 17/1000 | Loss: 0.00004475
Iteration 18/1000 | Loss: 0.00004469
Iteration 19/1000 | Loss: 0.00004468
Iteration 20/1000 | Loss: 0.00004467
Iteration 21/1000 | Loss: 0.00004462
Iteration 22/1000 | Loss: 0.00004459
Iteration 23/1000 | Loss: 0.00004459
Iteration 24/1000 | Loss: 0.00004458
Iteration 25/1000 | Loss: 0.00004450
Iteration 26/1000 | Loss: 0.00004446
Iteration 27/1000 | Loss: 0.00004446
Iteration 28/1000 | Loss: 0.00004444
Iteration 29/1000 | Loss: 0.00004444
Iteration 30/1000 | Loss: 0.00004443
Iteration 31/1000 | Loss: 0.00004443
Iteration 32/1000 | Loss: 0.00004442
Iteration 33/1000 | Loss: 0.00004442
Iteration 34/1000 | Loss: 0.00004439
Iteration 35/1000 | Loss: 0.00004437
Iteration 36/1000 | Loss: 0.00004437
Iteration 37/1000 | Loss: 0.00004436
Iteration 38/1000 | Loss: 0.00004436
Iteration 39/1000 | Loss: 0.00004436
Iteration 40/1000 | Loss: 0.00004435
Iteration 41/1000 | Loss: 0.00004435
Iteration 42/1000 | Loss: 0.00004435
Iteration 43/1000 | Loss: 0.00004434
Iteration 44/1000 | Loss: 0.00004434
Iteration 45/1000 | Loss: 0.00004433
Iteration 46/1000 | Loss: 0.00004433
Iteration 47/1000 | Loss: 0.00004433
Iteration 48/1000 | Loss: 0.00004433
Iteration 49/1000 | Loss: 0.00004433
Iteration 50/1000 | Loss: 0.00004432
Iteration 51/1000 | Loss: 0.00004432
Iteration 52/1000 | Loss: 0.00004431
Iteration 53/1000 | Loss: 0.00004431
Iteration 54/1000 | Loss: 0.00004431
Iteration 55/1000 | Loss: 0.00004430
Iteration 56/1000 | Loss: 0.00004430
Iteration 57/1000 | Loss: 0.00004430
Iteration 58/1000 | Loss: 0.00004430
Iteration 59/1000 | Loss: 0.00004430
Iteration 60/1000 | Loss: 0.00004429
Iteration 61/1000 | Loss: 0.00004429
Iteration 62/1000 | Loss: 0.00004429
Iteration 63/1000 | Loss: 0.00004429
Iteration 64/1000 | Loss: 0.00004429
Iteration 65/1000 | Loss: 0.00004429
Iteration 66/1000 | Loss: 0.00004428
Iteration 67/1000 | Loss: 0.00004428
Iteration 68/1000 | Loss: 0.00004428
Iteration 69/1000 | Loss: 0.00004428
Iteration 70/1000 | Loss: 0.00004427
Iteration 71/1000 | Loss: 0.00004427
Iteration 72/1000 | Loss: 0.00004427
Iteration 73/1000 | Loss: 0.00004427
Iteration 74/1000 | Loss: 0.00004427
Iteration 75/1000 | Loss: 0.00004427
Iteration 76/1000 | Loss: 0.00004427
Iteration 77/1000 | Loss: 0.00004427
Iteration 78/1000 | Loss: 0.00004426
Iteration 79/1000 | Loss: 0.00004426
Iteration 80/1000 | Loss: 0.00004426
Iteration 81/1000 | Loss: 0.00004426
Iteration 82/1000 | Loss: 0.00004426
Iteration 83/1000 | Loss: 0.00004426
Iteration 84/1000 | Loss: 0.00004426
Iteration 85/1000 | Loss: 0.00004426
Iteration 86/1000 | Loss: 0.00004425
Iteration 87/1000 | Loss: 0.00004425
Iteration 88/1000 | Loss: 0.00004425
Iteration 89/1000 | Loss: 0.00004425
Iteration 90/1000 | Loss: 0.00004425
Iteration 91/1000 | Loss: 0.00004425
Iteration 92/1000 | Loss: 0.00004424
Iteration 93/1000 | Loss: 0.00004424
Iteration 94/1000 | Loss: 0.00004424
Iteration 95/1000 | Loss: 0.00004424
Iteration 96/1000 | Loss: 0.00004424
Iteration 97/1000 | Loss: 0.00004424
Iteration 98/1000 | Loss: 0.00004424
Iteration 99/1000 | Loss: 0.00004424
Iteration 100/1000 | Loss: 0.00004424
Iteration 101/1000 | Loss: 0.00004424
Iteration 102/1000 | Loss: 0.00004424
Iteration 103/1000 | Loss: 0.00004424
Iteration 104/1000 | Loss: 0.00004424
Iteration 105/1000 | Loss: 0.00004424
Iteration 106/1000 | Loss: 0.00004424
Iteration 107/1000 | Loss: 0.00004424
Iteration 108/1000 | Loss: 0.00004424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [4.423794962349348e-05, 4.423794962349348e-05, 4.423794962349348e-05, 4.423794962349348e-05, 4.423794962349348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.423794962349348e-05

Optimization complete. Final v2v error: 5.5330424308776855 mm

Highest mean error: 7.067370414733887 mm for frame 119

Lowest mean error: 4.788139820098877 mm for frame 96

Saving results

Total time: 64.51910734176636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660247
Iteration 2/25 | Loss: 0.00190712
Iteration 3/25 | Loss: 0.00163618
Iteration 4/25 | Loss: 0.00159801
Iteration 5/25 | Loss: 0.00158730
Iteration 6/25 | Loss: 0.00158475
Iteration 7/25 | Loss: 0.00158395
Iteration 8/25 | Loss: 0.00158395
Iteration 9/25 | Loss: 0.00158395
Iteration 10/25 | Loss: 0.00158395
Iteration 11/25 | Loss: 0.00158395
Iteration 12/25 | Loss: 0.00158395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015839546686038375, 0.0015839546686038375, 0.0015839546686038375, 0.0015839546686038375, 0.0015839546686038375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015839546686038375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11807299
Iteration 2/25 | Loss: 0.00157510
Iteration 3/25 | Loss: 0.00157507
Iteration 4/25 | Loss: 0.00157507
Iteration 5/25 | Loss: 0.00157507
Iteration 6/25 | Loss: 0.00157507
Iteration 7/25 | Loss: 0.00157507
Iteration 8/25 | Loss: 0.00157507
Iteration 9/25 | Loss: 0.00157507
Iteration 10/25 | Loss: 0.00157507
Iteration 11/25 | Loss: 0.00157507
Iteration 12/25 | Loss: 0.00157507
Iteration 13/25 | Loss: 0.00157507
Iteration 14/25 | Loss: 0.00157507
Iteration 15/25 | Loss: 0.00157507
Iteration 16/25 | Loss: 0.00157507
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015750675229355693, 0.0015750675229355693, 0.0015750675229355693, 0.0015750675229355693, 0.0015750675229355693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015750675229355693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157507
Iteration 2/1000 | Loss: 0.00005371
Iteration 3/1000 | Loss: 0.00004079
Iteration 4/1000 | Loss: 0.00003624
Iteration 5/1000 | Loss: 0.00003403
Iteration 6/1000 | Loss: 0.00003242
Iteration 7/1000 | Loss: 0.00003146
Iteration 8/1000 | Loss: 0.00003075
Iteration 9/1000 | Loss: 0.00003025
Iteration 10/1000 | Loss: 0.00002983
Iteration 11/1000 | Loss: 0.00002952
Iteration 12/1000 | Loss: 0.00002923
Iteration 13/1000 | Loss: 0.00002895
Iteration 14/1000 | Loss: 0.00002880
Iteration 15/1000 | Loss: 0.00002875
Iteration 16/1000 | Loss: 0.00002874
Iteration 17/1000 | Loss: 0.00002874
Iteration 18/1000 | Loss: 0.00002872
Iteration 19/1000 | Loss: 0.00002871
Iteration 20/1000 | Loss: 0.00002871
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00002868
Iteration 25/1000 | Loss: 0.00002868
Iteration 26/1000 | Loss: 0.00002867
Iteration 27/1000 | Loss: 0.00002866
Iteration 28/1000 | Loss: 0.00002866
Iteration 29/1000 | Loss: 0.00002866
Iteration 30/1000 | Loss: 0.00002865
Iteration 31/1000 | Loss: 0.00002865
Iteration 32/1000 | Loss: 0.00002865
Iteration 33/1000 | Loss: 0.00002865
Iteration 34/1000 | Loss: 0.00002865
Iteration 35/1000 | Loss: 0.00002864
Iteration 36/1000 | Loss: 0.00002864
Iteration 37/1000 | Loss: 0.00002864
Iteration 38/1000 | Loss: 0.00002863
Iteration 39/1000 | Loss: 0.00002862
Iteration 40/1000 | Loss: 0.00002861
Iteration 41/1000 | Loss: 0.00002861
Iteration 42/1000 | Loss: 0.00002861
Iteration 43/1000 | Loss: 0.00002861
Iteration 44/1000 | Loss: 0.00002860
Iteration 45/1000 | Loss: 0.00002860
Iteration 46/1000 | Loss: 0.00002860
Iteration 47/1000 | Loss: 0.00002859
Iteration 48/1000 | Loss: 0.00002859
Iteration 49/1000 | Loss: 0.00002859
Iteration 50/1000 | Loss: 0.00002858
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002858
Iteration 55/1000 | Loss: 0.00002857
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002856
Iteration 58/1000 | Loss: 0.00002856
Iteration 59/1000 | Loss: 0.00002856
Iteration 60/1000 | Loss: 0.00002856
Iteration 61/1000 | Loss: 0.00002856
Iteration 62/1000 | Loss: 0.00002856
Iteration 63/1000 | Loss: 0.00002856
Iteration 64/1000 | Loss: 0.00002856
Iteration 65/1000 | Loss: 0.00002856
Iteration 66/1000 | Loss: 0.00002856
Iteration 67/1000 | Loss: 0.00002856
Iteration 68/1000 | Loss: 0.00002856
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002856
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002856
Iteration 73/1000 | Loss: 0.00002856
Iteration 74/1000 | Loss: 0.00002856
Iteration 75/1000 | Loss: 0.00002856
Iteration 76/1000 | Loss: 0.00002856
Iteration 77/1000 | Loss: 0.00002856
Iteration 78/1000 | Loss: 0.00002856
Iteration 79/1000 | Loss: 0.00002855
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002855
Iteration 85/1000 | Loss: 0.00002855
Iteration 86/1000 | Loss: 0.00002855
Iteration 87/1000 | Loss: 0.00002855
Iteration 88/1000 | Loss: 0.00002855
Iteration 89/1000 | Loss: 0.00002855
Iteration 90/1000 | Loss: 0.00002855
Iteration 91/1000 | Loss: 0.00002855
Iteration 92/1000 | Loss: 0.00002855
Iteration 93/1000 | Loss: 0.00002855
Iteration 94/1000 | Loss: 0.00002855
Iteration 95/1000 | Loss: 0.00002855
Iteration 96/1000 | Loss: 0.00002855
Iteration 97/1000 | Loss: 0.00002855
Iteration 98/1000 | Loss: 0.00002855
Iteration 99/1000 | Loss: 0.00002855
Iteration 100/1000 | Loss: 0.00002855
Iteration 101/1000 | Loss: 0.00002855
Iteration 102/1000 | Loss: 0.00002855
Iteration 103/1000 | Loss: 0.00002855
Iteration 104/1000 | Loss: 0.00002855
Iteration 105/1000 | Loss: 0.00002855
Iteration 106/1000 | Loss: 0.00002855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.8554608434205875e-05, 2.8554608434205875e-05, 2.8554608434205875e-05, 2.8554608434205875e-05, 2.8554608434205875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8554608434205875e-05

Optimization complete. Final v2v error: 4.717126846313477 mm

Highest mean error: 5.245400905609131 mm for frame 88

Lowest mean error: 4.245912551879883 mm for frame 0

Saving results

Total time: 35.86754393577576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938503
Iteration 2/25 | Loss: 0.00164168
Iteration 3/25 | Loss: 0.00153478
Iteration 4/25 | Loss: 0.00152132
Iteration 5/25 | Loss: 0.00151662
Iteration 6/25 | Loss: 0.00151521
Iteration 7/25 | Loss: 0.00151521
Iteration 8/25 | Loss: 0.00151521
Iteration 9/25 | Loss: 0.00151521
Iteration 10/25 | Loss: 0.00151521
Iteration 11/25 | Loss: 0.00151521
Iteration 12/25 | Loss: 0.00151521
Iteration 13/25 | Loss: 0.00151521
Iteration 14/25 | Loss: 0.00151521
Iteration 15/25 | Loss: 0.00151521
Iteration 16/25 | Loss: 0.00151521
Iteration 17/25 | Loss: 0.00151521
Iteration 18/25 | Loss: 0.00151521
Iteration 19/25 | Loss: 0.00151521
Iteration 20/25 | Loss: 0.00151521
Iteration 21/25 | Loss: 0.00151521
Iteration 22/25 | Loss: 0.00151521
Iteration 23/25 | Loss: 0.00151521
Iteration 24/25 | Loss: 0.00151521
Iteration 25/25 | Loss: 0.00151521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42315459
Iteration 2/25 | Loss: 0.00150642
Iteration 3/25 | Loss: 0.00150642
Iteration 4/25 | Loss: 0.00150642
Iteration 5/25 | Loss: 0.00150642
Iteration 6/25 | Loss: 0.00150642
Iteration 7/25 | Loss: 0.00150642
Iteration 8/25 | Loss: 0.00150642
Iteration 9/25 | Loss: 0.00150642
Iteration 10/25 | Loss: 0.00150642
Iteration 11/25 | Loss: 0.00150642
Iteration 12/25 | Loss: 0.00150642
Iteration 13/25 | Loss: 0.00150642
Iteration 14/25 | Loss: 0.00150642
Iteration 15/25 | Loss: 0.00150642
Iteration 16/25 | Loss: 0.00150642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001506421365775168, 0.001506421365775168, 0.001506421365775168, 0.001506421365775168, 0.001506421365775168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001506421365775168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150642
Iteration 2/1000 | Loss: 0.00005260
Iteration 3/1000 | Loss: 0.00003585
Iteration 4/1000 | Loss: 0.00003166
Iteration 5/1000 | Loss: 0.00002993
Iteration 6/1000 | Loss: 0.00002821
Iteration 7/1000 | Loss: 0.00002746
Iteration 8/1000 | Loss: 0.00002694
Iteration 9/1000 | Loss: 0.00002660
Iteration 10/1000 | Loss: 0.00002624
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002602
Iteration 13/1000 | Loss: 0.00002602
Iteration 14/1000 | Loss: 0.00002602
Iteration 15/1000 | Loss: 0.00002601
Iteration 16/1000 | Loss: 0.00002601
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002599
Iteration 19/1000 | Loss: 0.00002598
Iteration 20/1000 | Loss: 0.00002597
Iteration 21/1000 | Loss: 0.00002593
Iteration 22/1000 | Loss: 0.00002592
Iteration 23/1000 | Loss: 0.00002592
Iteration 24/1000 | Loss: 0.00002592
Iteration 25/1000 | Loss: 0.00002592
Iteration 26/1000 | Loss: 0.00002591
Iteration 27/1000 | Loss: 0.00002591
Iteration 28/1000 | Loss: 0.00002585
Iteration 29/1000 | Loss: 0.00002585
Iteration 30/1000 | Loss: 0.00002582
Iteration 31/1000 | Loss: 0.00002582
Iteration 32/1000 | Loss: 0.00002581
Iteration 33/1000 | Loss: 0.00002580
Iteration 34/1000 | Loss: 0.00002580
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002580
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00002579
Iteration 39/1000 | Loss: 0.00002579
Iteration 40/1000 | Loss: 0.00002579
Iteration 41/1000 | Loss: 0.00002579
Iteration 42/1000 | Loss: 0.00002579
Iteration 43/1000 | Loss: 0.00002579
Iteration 44/1000 | Loss: 0.00002579
Iteration 45/1000 | Loss: 0.00002579
Iteration 46/1000 | Loss: 0.00002578
Iteration 47/1000 | Loss: 0.00002578
Iteration 48/1000 | Loss: 0.00002578
Iteration 49/1000 | Loss: 0.00002578
Iteration 50/1000 | Loss: 0.00002577
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00002577
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00002576
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002575
Iteration 58/1000 | Loss: 0.00002575
Iteration 59/1000 | Loss: 0.00002575
Iteration 60/1000 | Loss: 0.00002575
Iteration 61/1000 | Loss: 0.00002575
Iteration 62/1000 | Loss: 0.00002575
Iteration 63/1000 | Loss: 0.00002575
Iteration 64/1000 | Loss: 0.00002574
Iteration 65/1000 | Loss: 0.00002574
Iteration 66/1000 | Loss: 0.00002574
Iteration 67/1000 | Loss: 0.00002574
Iteration 68/1000 | Loss: 0.00002574
Iteration 69/1000 | Loss: 0.00002574
Iteration 70/1000 | Loss: 0.00002574
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002574
Iteration 77/1000 | Loss: 0.00002574
Iteration 78/1000 | Loss: 0.00002574
Iteration 79/1000 | Loss: 0.00002574
Iteration 80/1000 | Loss: 0.00002574
Iteration 81/1000 | Loss: 0.00002573
Iteration 82/1000 | Loss: 0.00002573
Iteration 83/1000 | Loss: 0.00002573
Iteration 84/1000 | Loss: 0.00002573
Iteration 85/1000 | Loss: 0.00002573
Iteration 86/1000 | Loss: 0.00002573
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002573
Iteration 91/1000 | Loss: 0.00002573
Iteration 92/1000 | Loss: 0.00002573
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002572
Iteration 97/1000 | Loss: 0.00002572
Iteration 98/1000 | Loss: 0.00002572
Iteration 99/1000 | Loss: 0.00002572
Iteration 100/1000 | Loss: 0.00002572
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002571
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00002571
Iteration 106/1000 | Loss: 0.00002571
Iteration 107/1000 | Loss: 0.00002571
Iteration 108/1000 | Loss: 0.00002571
Iteration 109/1000 | Loss: 0.00002571
Iteration 110/1000 | Loss: 0.00002570
Iteration 111/1000 | Loss: 0.00002570
Iteration 112/1000 | Loss: 0.00002570
Iteration 113/1000 | Loss: 0.00002570
Iteration 114/1000 | Loss: 0.00002570
Iteration 115/1000 | Loss: 0.00002570
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002570
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00002570
Iteration 121/1000 | Loss: 0.00002570
Iteration 122/1000 | Loss: 0.00002569
Iteration 123/1000 | Loss: 0.00002569
Iteration 124/1000 | Loss: 0.00002569
Iteration 125/1000 | Loss: 0.00002569
Iteration 126/1000 | Loss: 0.00002569
Iteration 127/1000 | Loss: 0.00002569
Iteration 128/1000 | Loss: 0.00002569
Iteration 129/1000 | Loss: 0.00002569
Iteration 130/1000 | Loss: 0.00002569
Iteration 131/1000 | Loss: 0.00002569
Iteration 132/1000 | Loss: 0.00002569
Iteration 133/1000 | Loss: 0.00002569
Iteration 134/1000 | Loss: 0.00002569
Iteration 135/1000 | Loss: 0.00002569
Iteration 136/1000 | Loss: 0.00002568
Iteration 137/1000 | Loss: 0.00002568
Iteration 138/1000 | Loss: 0.00002568
Iteration 139/1000 | Loss: 0.00002568
Iteration 140/1000 | Loss: 0.00002568
Iteration 141/1000 | Loss: 0.00002568
Iteration 142/1000 | Loss: 0.00002568
Iteration 143/1000 | Loss: 0.00002568
Iteration 144/1000 | Loss: 0.00002568
Iteration 145/1000 | Loss: 0.00002568
Iteration 146/1000 | Loss: 0.00002567
Iteration 147/1000 | Loss: 0.00002567
Iteration 148/1000 | Loss: 0.00002567
Iteration 149/1000 | Loss: 0.00002567
Iteration 150/1000 | Loss: 0.00002567
Iteration 151/1000 | Loss: 0.00002567
Iteration 152/1000 | Loss: 0.00002567
Iteration 153/1000 | Loss: 0.00002567
Iteration 154/1000 | Loss: 0.00002567
Iteration 155/1000 | Loss: 0.00002567
Iteration 156/1000 | Loss: 0.00002567
Iteration 157/1000 | Loss: 0.00002566
Iteration 158/1000 | Loss: 0.00002566
Iteration 159/1000 | Loss: 0.00002566
Iteration 160/1000 | Loss: 0.00002566
Iteration 161/1000 | Loss: 0.00002566
Iteration 162/1000 | Loss: 0.00002566
Iteration 163/1000 | Loss: 0.00002566
Iteration 164/1000 | Loss: 0.00002566
Iteration 165/1000 | Loss: 0.00002566
Iteration 166/1000 | Loss: 0.00002566
Iteration 167/1000 | Loss: 0.00002566
Iteration 168/1000 | Loss: 0.00002566
Iteration 169/1000 | Loss: 0.00002566
Iteration 170/1000 | Loss: 0.00002566
Iteration 171/1000 | Loss: 0.00002566
Iteration 172/1000 | Loss: 0.00002566
Iteration 173/1000 | Loss: 0.00002566
Iteration 174/1000 | Loss: 0.00002566
Iteration 175/1000 | Loss: 0.00002566
Iteration 176/1000 | Loss: 0.00002566
Iteration 177/1000 | Loss: 0.00002566
Iteration 178/1000 | Loss: 0.00002566
Iteration 179/1000 | Loss: 0.00002566
Iteration 180/1000 | Loss: 0.00002566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.565767317719292e-05, 2.565767317719292e-05, 2.565767317719292e-05, 2.565767317719292e-05, 2.565767317719292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.565767317719292e-05

Optimization complete. Final v2v error: 4.4818949699401855 mm

Highest mean error: 4.736412525177002 mm for frame 109

Lowest mean error: 4.254833221435547 mm for frame 86

Saving results

Total time: 37.010860443115234
