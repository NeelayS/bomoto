Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=289, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16184-16239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523328
Iteration 2/25 | Loss: 0.00124578
Iteration 3/25 | Loss: 0.00103957
Iteration 4/25 | Loss: 0.00102944
Iteration 5/25 | Loss: 0.00102669
Iteration 6/25 | Loss: 0.00102654
Iteration 7/25 | Loss: 0.00102654
Iteration 8/25 | Loss: 0.00102654
Iteration 9/25 | Loss: 0.00102654
Iteration 10/25 | Loss: 0.00102654
Iteration 11/25 | Loss: 0.00102654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010265392484143376, 0.0010265392484143376, 0.0010265392484143376, 0.0010265392484143376, 0.0010265392484143376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010265392484143376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26070213
Iteration 2/25 | Loss: 0.00080411
Iteration 3/25 | Loss: 0.00080409
Iteration 4/25 | Loss: 0.00080409
Iteration 5/25 | Loss: 0.00080409
Iteration 6/25 | Loss: 0.00080409
Iteration 7/25 | Loss: 0.00080409
Iteration 8/25 | Loss: 0.00080409
Iteration 9/25 | Loss: 0.00080409
Iteration 10/25 | Loss: 0.00080409
Iteration 11/25 | Loss: 0.00080409
Iteration 12/25 | Loss: 0.00080409
Iteration 13/25 | Loss: 0.00080409
Iteration 14/25 | Loss: 0.00080409
Iteration 15/25 | Loss: 0.00080409
Iteration 16/25 | Loss: 0.00080409
Iteration 17/25 | Loss: 0.00080409
Iteration 18/25 | Loss: 0.00080409
Iteration 19/25 | Loss: 0.00080409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008040890097618103, 0.0008040890097618103, 0.0008040890097618103, 0.0008040890097618103, 0.0008040890097618103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008040890097618103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080409
Iteration 2/1000 | Loss: 0.00004127
Iteration 3/1000 | Loss: 0.00003107
Iteration 4/1000 | Loss: 0.00002597
Iteration 5/1000 | Loss: 0.00002320
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001954
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001882
Iteration 12/1000 | Loss: 0.00001856
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001822
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001816
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001813
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001813
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001813
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001812
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001809
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001798
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001796
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001789
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001783
Iteration 68/1000 | Loss: 0.00001783
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001783
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001781
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001779
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Iteration 89/1000 | Loss: 0.00001778
Iteration 90/1000 | Loss: 0.00001778
Iteration 91/1000 | Loss: 0.00001778
Iteration 92/1000 | Loss: 0.00001778
Iteration 93/1000 | Loss: 0.00001778
Iteration 94/1000 | Loss: 0.00001778
Iteration 95/1000 | Loss: 0.00001778
Iteration 96/1000 | Loss: 0.00001778
Iteration 97/1000 | Loss: 0.00001777
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001776
Iteration 100/1000 | Loss: 0.00001776
Iteration 101/1000 | Loss: 0.00001776
Iteration 102/1000 | Loss: 0.00001776
Iteration 103/1000 | Loss: 0.00001776
Iteration 104/1000 | Loss: 0.00001776
Iteration 105/1000 | Loss: 0.00001776
Iteration 106/1000 | Loss: 0.00001776
Iteration 107/1000 | Loss: 0.00001776
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001776
Iteration 110/1000 | Loss: 0.00001776
Iteration 111/1000 | Loss: 0.00001776
Iteration 112/1000 | Loss: 0.00001776
Iteration 113/1000 | Loss: 0.00001776
Iteration 114/1000 | Loss: 0.00001776
Iteration 115/1000 | Loss: 0.00001776
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7762238712748513e-05, 1.7762238712748513e-05, 1.7762238712748513e-05, 1.7762238712748513e-05, 1.7762238712748513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7762238712748513e-05

Optimization complete. Final v2v error: 3.6632840633392334 mm

Highest mean error: 3.9419779777526855 mm for frame 115

Lowest mean error: 3.1919116973876953 mm for frame 137

Saving results

Total time: 37.79715371131897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088984
Iteration 2/25 | Loss: 0.01088984
Iteration 3/25 | Loss: 0.01088984
Iteration 4/25 | Loss: 0.01088984
Iteration 5/25 | Loss: 0.01088984
Iteration 6/25 | Loss: 0.01088984
Iteration 7/25 | Loss: 0.01088983
Iteration 8/25 | Loss: 0.01088983
Iteration 9/25 | Loss: 0.01088983
Iteration 10/25 | Loss: 0.01088983
Iteration 11/25 | Loss: 0.01088983
Iteration 12/25 | Loss: 0.01088983
Iteration 13/25 | Loss: 0.01088983
Iteration 14/25 | Loss: 0.01088983
Iteration 15/25 | Loss: 0.01088983
Iteration 16/25 | Loss: 0.01088983
Iteration 17/25 | Loss: 0.01088983
Iteration 18/25 | Loss: 0.01088983
Iteration 19/25 | Loss: 0.01088983
Iteration 20/25 | Loss: 0.01088983
Iteration 21/25 | Loss: 0.01088982
Iteration 22/25 | Loss: 0.01088982
Iteration 23/25 | Loss: 0.01088982
Iteration 24/25 | Loss: 0.01088982
Iteration 25/25 | Loss: 0.01088982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51228380
Iteration 2/25 | Loss: 0.06567790
Iteration 3/25 | Loss: 0.06481495
Iteration 4/25 | Loss: 0.06428598
Iteration 5/25 | Loss: 0.06428597
Iteration 6/25 | Loss: 0.06428596
Iteration 7/25 | Loss: 0.06428596
Iteration 8/25 | Loss: 0.06428596
Iteration 9/25 | Loss: 0.06428596
Iteration 10/25 | Loss: 0.06428596
Iteration 11/25 | Loss: 0.06428596
Iteration 12/25 | Loss: 0.06428596
Iteration 13/25 | Loss: 0.06428596
Iteration 14/25 | Loss: 0.06428596
Iteration 15/25 | Loss: 0.06428596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.06428595632314682, 0.06428595632314682, 0.06428595632314682, 0.06428595632314682, 0.06428595632314682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06428595632314682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06428596
Iteration 2/1000 | Loss: 0.00383522
Iteration 3/1000 | Loss: 0.00142624
Iteration 4/1000 | Loss: 0.00348627
Iteration 5/1000 | Loss: 0.00411829
Iteration 6/1000 | Loss: 0.00277955
Iteration 7/1000 | Loss: 0.00088583
Iteration 8/1000 | Loss: 0.00034276
Iteration 9/1000 | Loss: 0.00018013
Iteration 10/1000 | Loss: 0.00060264
Iteration 11/1000 | Loss: 0.00206443
Iteration 12/1000 | Loss: 0.00019223
Iteration 13/1000 | Loss: 0.00016030
Iteration 14/1000 | Loss: 0.00004418
Iteration 15/1000 | Loss: 0.00044385
Iteration 16/1000 | Loss: 0.00004992
Iteration 17/1000 | Loss: 0.00004697
Iteration 18/1000 | Loss: 0.00003955
Iteration 19/1000 | Loss: 0.00003387
Iteration 20/1000 | Loss: 0.00055344
Iteration 21/1000 | Loss: 0.00155909
Iteration 22/1000 | Loss: 0.00304094
Iteration 23/1000 | Loss: 0.00284353
Iteration 24/1000 | Loss: 0.00120421
Iteration 25/1000 | Loss: 0.00005719
Iteration 26/1000 | Loss: 0.00013594
Iteration 27/1000 | Loss: 0.00037695
Iteration 28/1000 | Loss: 0.00162776
Iteration 29/1000 | Loss: 0.00056897
Iteration 30/1000 | Loss: 0.00005911
Iteration 31/1000 | Loss: 0.00003705
Iteration 32/1000 | Loss: 0.00131262
Iteration 33/1000 | Loss: 0.00019477
Iteration 34/1000 | Loss: 0.00003979
Iteration 35/1000 | Loss: 0.00086853
Iteration 36/1000 | Loss: 0.00005184
Iteration 37/1000 | Loss: 0.00018821
Iteration 38/1000 | Loss: 0.00026648
Iteration 39/1000 | Loss: 0.00003008
Iteration 40/1000 | Loss: 0.00002702
Iteration 41/1000 | Loss: 0.00002568
Iteration 42/1000 | Loss: 0.00002495
Iteration 43/1000 | Loss: 0.00043926
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002345
Iteration 46/1000 | Loss: 0.00002305
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002038
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001985
Iteration 55/1000 | Loss: 0.00001964
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001945
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001942
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001928
Iteration 68/1000 | Loss: 0.00001928
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001923
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001918
Iteration 94/1000 | Loss: 0.00001917
Iteration 95/1000 | Loss: 0.00001917
Iteration 96/1000 | Loss: 0.00001917
Iteration 97/1000 | Loss: 0.00001917
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00018624
Iteration 115/1000 | Loss: 0.00001935
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001909
Iteration 118/1000 | Loss: 0.00001909
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001907
Iteration 125/1000 | Loss: 0.00001907
Iteration 126/1000 | Loss: 0.00001907
Iteration 127/1000 | Loss: 0.00001907
Iteration 128/1000 | Loss: 0.00001907
Iteration 129/1000 | Loss: 0.00001907
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001906
Iteration 133/1000 | Loss: 0.00001906
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001905
Iteration 138/1000 | Loss: 0.00001905
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001905
Iteration 141/1000 | Loss: 0.00001905
Iteration 142/1000 | Loss: 0.00001905
Iteration 143/1000 | Loss: 0.00001905
Iteration 144/1000 | Loss: 0.00001905
Iteration 145/1000 | Loss: 0.00001905
Iteration 146/1000 | Loss: 0.00001904
Iteration 147/1000 | Loss: 0.00001904
Iteration 148/1000 | Loss: 0.00001904
Iteration 149/1000 | Loss: 0.00001904
Iteration 150/1000 | Loss: 0.00001904
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001903
Iteration 153/1000 | Loss: 0.00001903
Iteration 154/1000 | Loss: 0.00001903
Iteration 155/1000 | Loss: 0.00001903
Iteration 156/1000 | Loss: 0.00001903
Iteration 157/1000 | Loss: 0.00001903
Iteration 158/1000 | Loss: 0.00001903
Iteration 159/1000 | Loss: 0.00001903
Iteration 160/1000 | Loss: 0.00001903
Iteration 161/1000 | Loss: 0.00001903
Iteration 162/1000 | Loss: 0.00001903
Iteration 163/1000 | Loss: 0.00001903
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001902
Iteration 167/1000 | Loss: 0.00001902
Iteration 168/1000 | Loss: 0.00001902
Iteration 169/1000 | Loss: 0.00001902
Iteration 170/1000 | Loss: 0.00001902
Iteration 171/1000 | Loss: 0.00001902
Iteration 172/1000 | Loss: 0.00001902
Iteration 173/1000 | Loss: 0.00001902
Iteration 174/1000 | Loss: 0.00001902
Iteration 175/1000 | Loss: 0.00001902
Iteration 176/1000 | Loss: 0.00001901
Iteration 177/1000 | Loss: 0.00001901
Iteration 178/1000 | Loss: 0.00001901
Iteration 179/1000 | Loss: 0.00001901
Iteration 180/1000 | Loss: 0.00001901
Iteration 181/1000 | Loss: 0.00001901
Iteration 182/1000 | Loss: 0.00001901
Iteration 183/1000 | Loss: 0.00001901
Iteration 184/1000 | Loss: 0.00001901
Iteration 185/1000 | Loss: 0.00001901
Iteration 186/1000 | Loss: 0.00001901
Iteration 187/1000 | Loss: 0.00001901
Iteration 188/1000 | Loss: 0.00001901
Iteration 189/1000 | Loss: 0.00001901
Iteration 190/1000 | Loss: 0.00001901
Iteration 191/1000 | Loss: 0.00001901
Iteration 192/1000 | Loss: 0.00001901
Iteration 193/1000 | Loss: 0.00001901
Iteration 194/1000 | Loss: 0.00001901
Iteration 195/1000 | Loss: 0.00001901
Iteration 196/1000 | Loss: 0.00001900
Iteration 197/1000 | Loss: 0.00001900
Iteration 198/1000 | Loss: 0.00001900
Iteration 199/1000 | Loss: 0.00001900
Iteration 200/1000 | Loss: 0.00001900
Iteration 201/1000 | Loss: 0.00001900
Iteration 202/1000 | Loss: 0.00001900
Iteration 203/1000 | Loss: 0.00001900
Iteration 204/1000 | Loss: 0.00001900
Iteration 205/1000 | Loss: 0.00001900
Iteration 206/1000 | Loss: 0.00001900
Iteration 207/1000 | Loss: 0.00001900
Iteration 208/1000 | Loss: 0.00001900
Iteration 209/1000 | Loss: 0.00001900
Iteration 210/1000 | Loss: 0.00001899
Iteration 211/1000 | Loss: 0.00001899
Iteration 212/1000 | Loss: 0.00001899
Iteration 213/1000 | Loss: 0.00001899
Iteration 214/1000 | Loss: 0.00001899
Iteration 215/1000 | Loss: 0.00001899
Iteration 216/1000 | Loss: 0.00001899
Iteration 217/1000 | Loss: 0.00001899
Iteration 218/1000 | Loss: 0.00001899
Iteration 219/1000 | Loss: 0.00001899
Iteration 220/1000 | Loss: 0.00001899
Iteration 221/1000 | Loss: 0.00001899
Iteration 222/1000 | Loss: 0.00001899
Iteration 223/1000 | Loss: 0.00001899
Iteration 224/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.8994258425664157e-05, 1.8994258425664157e-05, 1.8994258425664157e-05, 1.8994258425664157e-05, 1.8994258425664157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8994258425664157e-05

Optimization complete. Final v2v error: 3.342578649520874 mm

Highest mean error: 22.0003604888916 mm for frame 134

Lowest mean error: 2.6079490184783936 mm for frame 76

Saving results

Total time: 113.83804130554199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396629
Iteration 2/25 | Loss: 0.00117687
Iteration 3/25 | Loss: 0.00098958
Iteration 4/25 | Loss: 0.00097221
Iteration 5/25 | Loss: 0.00096912
Iteration 6/25 | Loss: 0.00096855
Iteration 7/25 | Loss: 0.00096855
Iteration 8/25 | Loss: 0.00096855
Iteration 9/25 | Loss: 0.00096855
Iteration 10/25 | Loss: 0.00096855
Iteration 11/25 | Loss: 0.00096855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009685520781204104, 0.0009685520781204104, 0.0009685520781204104, 0.0009685520781204104, 0.0009685520781204104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009685520781204104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26991892
Iteration 2/25 | Loss: 0.00098468
Iteration 3/25 | Loss: 0.00098468
Iteration 4/25 | Loss: 0.00098468
Iteration 5/25 | Loss: 0.00098468
Iteration 6/25 | Loss: 0.00098468
Iteration 7/25 | Loss: 0.00098468
Iteration 8/25 | Loss: 0.00098468
Iteration 9/25 | Loss: 0.00098468
Iteration 10/25 | Loss: 0.00098468
Iteration 11/25 | Loss: 0.00098468
Iteration 12/25 | Loss: 0.00098468
Iteration 13/25 | Loss: 0.00098468
Iteration 14/25 | Loss: 0.00098468
Iteration 15/25 | Loss: 0.00098468
Iteration 16/25 | Loss: 0.00098468
Iteration 17/25 | Loss: 0.00098468
Iteration 18/25 | Loss: 0.00098468
Iteration 19/25 | Loss: 0.00098468
Iteration 20/25 | Loss: 0.00098468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009846765315160155, 0.0009846765315160155, 0.0009846765315160155, 0.0009846765315160155, 0.0009846765315160155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009846765315160155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098468
Iteration 2/1000 | Loss: 0.00002236
Iteration 3/1000 | Loss: 0.00001675
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001083
Iteration 65/1000 | Loss: 0.00001083
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001071
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001066
Iteration 145/1000 | Loss: 0.00001066
Iteration 146/1000 | Loss: 0.00001065
Iteration 147/1000 | Loss: 0.00001065
Iteration 148/1000 | Loss: 0.00001065
Iteration 149/1000 | Loss: 0.00001065
Iteration 150/1000 | Loss: 0.00001064
Iteration 151/1000 | Loss: 0.00001064
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001063
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001061
Iteration 181/1000 | Loss: 0.00001061
Iteration 182/1000 | Loss: 0.00001061
Iteration 183/1000 | Loss: 0.00001061
Iteration 184/1000 | Loss: 0.00001061
Iteration 185/1000 | Loss: 0.00001061
Iteration 186/1000 | Loss: 0.00001061
Iteration 187/1000 | Loss: 0.00001061
Iteration 188/1000 | Loss: 0.00001061
Iteration 189/1000 | Loss: 0.00001061
Iteration 190/1000 | Loss: 0.00001061
Iteration 191/1000 | Loss: 0.00001061
Iteration 192/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.061244256561622e-05, 1.061244256561622e-05, 1.061244256561622e-05, 1.061244256561622e-05, 1.061244256561622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.061244256561622e-05

Optimization complete. Final v2v error: 2.839169979095459 mm

Highest mean error: 3.3333520889282227 mm for frame 24

Lowest mean error: 2.5936827659606934 mm for frame 133

Saving results

Total time: 37.31236147880554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467941
Iteration 2/25 | Loss: 0.00117572
Iteration 3/25 | Loss: 0.00106712
Iteration 4/25 | Loss: 0.00104782
Iteration 5/25 | Loss: 0.00104207
Iteration 6/25 | Loss: 0.00104130
Iteration 7/25 | Loss: 0.00104130
Iteration 8/25 | Loss: 0.00104130
Iteration 9/25 | Loss: 0.00104130
Iteration 10/25 | Loss: 0.00104130
Iteration 11/25 | Loss: 0.00104130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010413042036816478, 0.0010413042036816478, 0.0010413042036816478, 0.0010413042036816478, 0.0010413042036816478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010413042036816478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27631831
Iteration 2/25 | Loss: 0.00109252
Iteration 3/25 | Loss: 0.00109252
Iteration 4/25 | Loss: 0.00109252
Iteration 5/25 | Loss: 0.00109252
Iteration 6/25 | Loss: 0.00109252
Iteration 7/25 | Loss: 0.00109252
Iteration 8/25 | Loss: 0.00109252
Iteration 9/25 | Loss: 0.00109252
Iteration 10/25 | Loss: 0.00109252
Iteration 11/25 | Loss: 0.00109252
Iteration 12/25 | Loss: 0.00109252
Iteration 13/25 | Loss: 0.00109252
Iteration 14/25 | Loss: 0.00109252
Iteration 15/25 | Loss: 0.00109252
Iteration 16/25 | Loss: 0.00109252
Iteration 17/25 | Loss: 0.00109252
Iteration 18/25 | Loss: 0.00109252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010925160022452474, 0.0010925160022452474, 0.0010925160022452474, 0.0010925160022452474, 0.0010925160022452474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010925160022452474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109252
Iteration 2/1000 | Loss: 0.00002293
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001472
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001414
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001327
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001325
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001324
Iteration 67/1000 | Loss: 0.00001324
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001324
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001324
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001322
Iteration 89/1000 | Loss: 0.00001322
Iteration 90/1000 | Loss: 0.00001322
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.3217877494753338e-05, 1.3217877494753338e-05, 1.3217877494753338e-05, 1.3217877494753338e-05, 1.3217877494753338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3217877494753338e-05

Optimization complete. Final v2v error: 3.119266986846924 mm

Highest mean error: 3.4993176460266113 mm for frame 148

Lowest mean error: 2.737908363342285 mm for frame 38

Saving results

Total time: 37.464993476867676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575459
Iteration 2/25 | Loss: 0.00123821
Iteration 3/25 | Loss: 0.00105733
Iteration 4/25 | Loss: 0.00102563
Iteration 5/25 | Loss: 0.00102011
Iteration 6/25 | Loss: 0.00101179
Iteration 7/25 | Loss: 0.00101047
Iteration 8/25 | Loss: 0.00100982
Iteration 9/25 | Loss: 0.00100941
Iteration 10/25 | Loss: 0.00100884
Iteration 11/25 | Loss: 0.00100826
Iteration 12/25 | Loss: 0.00100796
Iteration 13/25 | Loss: 0.00100785
Iteration 14/25 | Loss: 0.00100784
Iteration 15/25 | Loss: 0.00100783
Iteration 16/25 | Loss: 0.00100783
Iteration 17/25 | Loss: 0.00100783
Iteration 18/25 | Loss: 0.00100783
Iteration 19/25 | Loss: 0.00100783
Iteration 20/25 | Loss: 0.00100782
Iteration 21/25 | Loss: 0.00100782
Iteration 22/25 | Loss: 0.00100782
Iteration 23/25 | Loss: 0.00100782
Iteration 24/25 | Loss: 0.00100782
Iteration 25/25 | Loss: 0.00100782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10380483
Iteration 2/25 | Loss: 0.00103342
Iteration 3/25 | Loss: 0.00103342
Iteration 4/25 | Loss: 0.00103342
Iteration 5/25 | Loss: 0.00103342
Iteration 6/25 | Loss: 0.00103341
Iteration 7/25 | Loss: 0.00103341
Iteration 8/25 | Loss: 0.00103341
Iteration 9/25 | Loss: 0.00103341
Iteration 10/25 | Loss: 0.00103341
Iteration 11/25 | Loss: 0.00103341
Iteration 12/25 | Loss: 0.00103341
Iteration 13/25 | Loss: 0.00103341
Iteration 14/25 | Loss: 0.00103341
Iteration 15/25 | Loss: 0.00103341
Iteration 16/25 | Loss: 0.00103341
Iteration 17/25 | Loss: 0.00103341
Iteration 18/25 | Loss: 0.00103341
Iteration 19/25 | Loss: 0.00103341
Iteration 20/25 | Loss: 0.00103341
Iteration 21/25 | Loss: 0.00103341
Iteration 22/25 | Loss: 0.00103341
Iteration 23/25 | Loss: 0.00103341
Iteration 24/25 | Loss: 0.00103341
Iteration 25/25 | Loss: 0.00103341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103341
Iteration 2/1000 | Loss: 0.00004447
Iteration 3/1000 | Loss: 0.00003163
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00008506
Iteration 10/1000 | Loss: 0.00002103
Iteration 11/1000 | Loss: 0.00002033
Iteration 12/1000 | Loss: 0.00001977
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001850
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001791
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001779
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001777
Iteration 90/1000 | Loss: 0.00001777
Iteration 91/1000 | Loss: 0.00001777
Iteration 92/1000 | Loss: 0.00001777
Iteration 93/1000 | Loss: 0.00001777
Iteration 94/1000 | Loss: 0.00001777
Iteration 95/1000 | Loss: 0.00001777
Iteration 96/1000 | Loss: 0.00001777
Iteration 97/1000 | Loss: 0.00001777
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001777
Iteration 100/1000 | Loss: 0.00001777
Iteration 101/1000 | Loss: 0.00001777
Iteration 102/1000 | Loss: 0.00001777
Iteration 103/1000 | Loss: 0.00001777
Iteration 104/1000 | Loss: 0.00001777
Iteration 105/1000 | Loss: 0.00001777
Iteration 106/1000 | Loss: 0.00001777
Iteration 107/1000 | Loss: 0.00001777
Iteration 108/1000 | Loss: 0.00001777
Iteration 109/1000 | Loss: 0.00001777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.7774120351532474e-05, 1.7774120351532474e-05, 1.7774120351532474e-05, 1.7774120351532474e-05, 1.7774120351532474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7774120351532474e-05

Optimization complete. Final v2v error: 3.432003974914551 mm

Highest mean error: 5.482924461364746 mm for frame 109

Lowest mean error: 2.6817264556884766 mm for frame 2

Saving results

Total time: 55.96775484085083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824987
Iteration 2/25 | Loss: 0.00132646
Iteration 3/25 | Loss: 0.00102794
Iteration 4/25 | Loss: 0.00099312
Iteration 5/25 | Loss: 0.00098699
Iteration 6/25 | Loss: 0.00098588
Iteration 7/25 | Loss: 0.00098562
Iteration 8/25 | Loss: 0.00098554
Iteration 9/25 | Loss: 0.00098554
Iteration 10/25 | Loss: 0.00098554
Iteration 11/25 | Loss: 0.00098554
Iteration 12/25 | Loss: 0.00098554
Iteration 13/25 | Loss: 0.00098554
Iteration 14/25 | Loss: 0.00098554
Iteration 15/25 | Loss: 0.00098554
Iteration 16/25 | Loss: 0.00098554
Iteration 17/25 | Loss: 0.00098554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009855433600023389, 0.0009855433600023389, 0.0009855433600023389, 0.0009855433600023389, 0.0009855433600023389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009855433600023389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28330767
Iteration 2/25 | Loss: 0.00115223
Iteration 3/25 | Loss: 0.00115223
Iteration 4/25 | Loss: 0.00115223
Iteration 5/25 | Loss: 0.00115223
Iteration 6/25 | Loss: 0.00115223
Iteration 7/25 | Loss: 0.00115223
Iteration 8/25 | Loss: 0.00115223
Iteration 9/25 | Loss: 0.00115223
Iteration 10/25 | Loss: 0.00115223
Iteration 11/25 | Loss: 0.00115223
Iteration 12/25 | Loss: 0.00115223
Iteration 13/25 | Loss: 0.00115223
Iteration 14/25 | Loss: 0.00115223
Iteration 15/25 | Loss: 0.00115223
Iteration 16/25 | Loss: 0.00115223
Iteration 17/25 | Loss: 0.00115223
Iteration 18/25 | Loss: 0.00115223
Iteration 19/25 | Loss: 0.00115223
Iteration 20/25 | Loss: 0.00115223
Iteration 21/25 | Loss: 0.00115223
Iteration 22/25 | Loss: 0.00115223
Iteration 23/25 | Loss: 0.00115223
Iteration 24/25 | Loss: 0.00115223
Iteration 25/25 | Loss: 0.00115223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115223
Iteration 2/1000 | Loss: 0.00002233
Iteration 3/1000 | Loss: 0.00001534
Iteration 4/1000 | Loss: 0.00001306
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001096
Iteration 8/1000 | Loss: 0.00001064
Iteration 9/1000 | Loss: 0.00001039
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000983
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000982
Iteration 18/1000 | Loss: 0.00000982
Iteration 19/1000 | Loss: 0.00000981
Iteration 20/1000 | Loss: 0.00000981
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000979
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000977
Iteration 26/1000 | Loss: 0.00000977
Iteration 27/1000 | Loss: 0.00000977
Iteration 28/1000 | Loss: 0.00000977
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000975
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000974
Iteration 33/1000 | Loss: 0.00000974
Iteration 34/1000 | Loss: 0.00000974
Iteration 35/1000 | Loss: 0.00000973
Iteration 36/1000 | Loss: 0.00000973
Iteration 37/1000 | Loss: 0.00000973
Iteration 38/1000 | Loss: 0.00000973
Iteration 39/1000 | Loss: 0.00000972
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000972
Iteration 42/1000 | Loss: 0.00000971
Iteration 43/1000 | Loss: 0.00000971
Iteration 44/1000 | Loss: 0.00000971
Iteration 45/1000 | Loss: 0.00000970
Iteration 46/1000 | Loss: 0.00000970
Iteration 47/1000 | Loss: 0.00000970
Iteration 48/1000 | Loss: 0.00000969
Iteration 49/1000 | Loss: 0.00000969
Iteration 50/1000 | Loss: 0.00000968
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000968
Iteration 53/1000 | Loss: 0.00000967
Iteration 54/1000 | Loss: 0.00000967
Iteration 55/1000 | Loss: 0.00000967
Iteration 56/1000 | Loss: 0.00000967
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000967
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000964
Iteration 65/1000 | Loss: 0.00000963
Iteration 66/1000 | Loss: 0.00000963
Iteration 67/1000 | Loss: 0.00000962
Iteration 68/1000 | Loss: 0.00000962
Iteration 69/1000 | Loss: 0.00000962
Iteration 70/1000 | Loss: 0.00000961
Iteration 71/1000 | Loss: 0.00000961
Iteration 72/1000 | Loss: 0.00000961
Iteration 73/1000 | Loss: 0.00000960
Iteration 74/1000 | Loss: 0.00000960
Iteration 75/1000 | Loss: 0.00000959
Iteration 76/1000 | Loss: 0.00000959
Iteration 77/1000 | Loss: 0.00000958
Iteration 78/1000 | Loss: 0.00000958
Iteration 79/1000 | Loss: 0.00000958
Iteration 80/1000 | Loss: 0.00000957
Iteration 81/1000 | Loss: 0.00000957
Iteration 82/1000 | Loss: 0.00000955
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000948
Iteration 102/1000 | Loss: 0.00000948
Iteration 103/1000 | Loss: 0.00000948
Iteration 104/1000 | Loss: 0.00000948
Iteration 105/1000 | Loss: 0.00000948
Iteration 106/1000 | Loss: 0.00000948
Iteration 107/1000 | Loss: 0.00000947
Iteration 108/1000 | Loss: 0.00000947
Iteration 109/1000 | Loss: 0.00000947
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000946
Iteration 112/1000 | Loss: 0.00000946
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000945
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000945
Iteration 118/1000 | Loss: 0.00000945
Iteration 119/1000 | Loss: 0.00000945
Iteration 120/1000 | Loss: 0.00000945
Iteration 121/1000 | Loss: 0.00000945
Iteration 122/1000 | Loss: 0.00000945
Iteration 123/1000 | Loss: 0.00000945
Iteration 124/1000 | Loss: 0.00000945
Iteration 125/1000 | Loss: 0.00000945
Iteration 126/1000 | Loss: 0.00000944
Iteration 127/1000 | Loss: 0.00000944
Iteration 128/1000 | Loss: 0.00000944
Iteration 129/1000 | Loss: 0.00000944
Iteration 130/1000 | Loss: 0.00000944
Iteration 131/1000 | Loss: 0.00000944
Iteration 132/1000 | Loss: 0.00000944
Iteration 133/1000 | Loss: 0.00000944
Iteration 134/1000 | Loss: 0.00000944
Iteration 135/1000 | Loss: 0.00000944
Iteration 136/1000 | Loss: 0.00000944
Iteration 137/1000 | Loss: 0.00000944
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000944
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00000943
Iteration 147/1000 | Loss: 0.00000943
Iteration 148/1000 | Loss: 0.00000943
Iteration 149/1000 | Loss: 0.00000943
Iteration 150/1000 | Loss: 0.00000943
Iteration 151/1000 | Loss: 0.00000943
Iteration 152/1000 | Loss: 0.00000943
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000942
Iteration 155/1000 | Loss: 0.00000942
Iteration 156/1000 | Loss: 0.00000942
Iteration 157/1000 | Loss: 0.00000942
Iteration 158/1000 | Loss: 0.00000942
Iteration 159/1000 | Loss: 0.00000942
Iteration 160/1000 | Loss: 0.00000942
Iteration 161/1000 | Loss: 0.00000942
Iteration 162/1000 | Loss: 0.00000942
Iteration 163/1000 | Loss: 0.00000942
Iteration 164/1000 | Loss: 0.00000942
Iteration 165/1000 | Loss: 0.00000942
Iteration 166/1000 | Loss: 0.00000942
Iteration 167/1000 | Loss: 0.00000942
Iteration 168/1000 | Loss: 0.00000942
Iteration 169/1000 | Loss: 0.00000942
Iteration 170/1000 | Loss: 0.00000941
Iteration 171/1000 | Loss: 0.00000941
Iteration 172/1000 | Loss: 0.00000941
Iteration 173/1000 | Loss: 0.00000941
Iteration 174/1000 | Loss: 0.00000941
Iteration 175/1000 | Loss: 0.00000941
Iteration 176/1000 | Loss: 0.00000941
Iteration 177/1000 | Loss: 0.00000941
Iteration 178/1000 | Loss: 0.00000941
Iteration 179/1000 | Loss: 0.00000941
Iteration 180/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [9.412345207238104e-06, 9.412345207238104e-06, 9.412345207238104e-06, 9.412345207238104e-06, 9.412345207238104e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.412345207238104e-06

Optimization complete. Final v2v error: 2.6940810680389404 mm

Highest mean error: 3.111008644104004 mm for frame 102

Lowest mean error: 2.4590938091278076 mm for frame 4

Saving results

Total time: 43.43026518821716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810764
Iteration 2/25 | Loss: 0.00135824
Iteration 3/25 | Loss: 0.00108759
Iteration 4/25 | Loss: 0.00107018
Iteration 5/25 | Loss: 0.00106718
Iteration 6/25 | Loss: 0.00106718
Iteration 7/25 | Loss: 0.00106718
Iteration 8/25 | Loss: 0.00106718
Iteration 9/25 | Loss: 0.00106718
Iteration 10/25 | Loss: 0.00106718
Iteration 11/25 | Loss: 0.00106718
Iteration 12/25 | Loss: 0.00106718
Iteration 13/25 | Loss: 0.00106718
Iteration 14/25 | Loss: 0.00106718
Iteration 15/25 | Loss: 0.00106718
Iteration 16/25 | Loss: 0.00106718
Iteration 17/25 | Loss: 0.00106718
Iteration 18/25 | Loss: 0.00106718
Iteration 19/25 | Loss: 0.00106718
Iteration 20/25 | Loss: 0.00106718
Iteration 21/25 | Loss: 0.00106718
Iteration 22/25 | Loss: 0.00106718
Iteration 23/25 | Loss: 0.00106718
Iteration 24/25 | Loss: 0.00106718
Iteration 25/25 | Loss: 0.00106718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15568578
Iteration 2/25 | Loss: 0.00080378
Iteration 3/25 | Loss: 0.00080378
Iteration 4/25 | Loss: 0.00080377
Iteration 5/25 | Loss: 0.00080377
Iteration 6/25 | Loss: 0.00080377
Iteration 7/25 | Loss: 0.00080377
Iteration 8/25 | Loss: 0.00080377
Iteration 9/25 | Loss: 0.00080377
Iteration 10/25 | Loss: 0.00080377
Iteration 11/25 | Loss: 0.00080377
Iteration 12/25 | Loss: 0.00080377
Iteration 13/25 | Loss: 0.00080377
Iteration 14/25 | Loss: 0.00080377
Iteration 15/25 | Loss: 0.00080377
Iteration 16/25 | Loss: 0.00080377
Iteration 17/25 | Loss: 0.00080377
Iteration 18/25 | Loss: 0.00080377
Iteration 19/25 | Loss: 0.00080377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008037728257477283, 0.0008037728257477283, 0.0008037728257477283, 0.0008037728257477283, 0.0008037728257477283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008037728257477283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080377
Iteration 2/1000 | Loss: 0.00002829
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001743
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001697
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001673
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001667
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001666
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001660
Iteration 49/1000 | Loss: 0.00001660
Iteration 50/1000 | Loss: 0.00001660
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001660
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001660
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001657
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001657
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.6567742932238616e-05, 1.6567742932238616e-05, 1.6567742932238616e-05, 1.6567742932238616e-05, 1.6567742932238616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6567742932238616e-05

Optimization complete. Final v2v error: 3.4114344120025635 mm

Highest mean error: 3.8143341541290283 mm for frame 11

Lowest mean error: 3.101762294769287 mm for frame 201

Saving results

Total time: 32.96710228919983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027997
Iteration 2/25 | Loss: 0.00278575
Iteration 3/25 | Loss: 0.00200859
Iteration 4/25 | Loss: 0.00167817
Iteration 5/25 | Loss: 0.00161264
Iteration 6/25 | Loss: 0.00155635
Iteration 7/25 | Loss: 0.00142918
Iteration 8/25 | Loss: 0.00133501
Iteration 9/25 | Loss: 0.00129234
Iteration 10/25 | Loss: 0.00127242
Iteration 11/25 | Loss: 0.00125185
Iteration 12/25 | Loss: 0.00123666
Iteration 13/25 | Loss: 0.00122491
Iteration 14/25 | Loss: 0.00122092
Iteration 15/25 | Loss: 0.00120945
Iteration 16/25 | Loss: 0.00119760
Iteration 17/25 | Loss: 0.00119646
Iteration 18/25 | Loss: 0.00119580
Iteration 19/25 | Loss: 0.00118659
Iteration 20/25 | Loss: 0.00117316
Iteration 21/25 | Loss: 0.00117451
Iteration 22/25 | Loss: 0.00116253
Iteration 23/25 | Loss: 0.00115180
Iteration 24/25 | Loss: 0.00114756
Iteration 25/25 | Loss: 0.00114450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28997099
Iteration 2/25 | Loss: 0.00398557
Iteration 3/25 | Loss: 0.00257769
Iteration 4/25 | Loss: 0.00227044
Iteration 5/25 | Loss: 0.00227044
Iteration 6/25 | Loss: 0.00227044
Iteration 7/25 | Loss: 0.00227044
Iteration 8/25 | Loss: 0.00227044
Iteration 9/25 | Loss: 0.00227044
Iteration 10/25 | Loss: 0.00227044
Iteration 11/25 | Loss: 0.00227044
Iteration 12/25 | Loss: 0.00227044
Iteration 13/25 | Loss: 0.00227044
Iteration 14/25 | Loss: 0.00227044
Iteration 15/25 | Loss: 0.00227044
Iteration 16/25 | Loss: 0.00227044
Iteration 17/25 | Loss: 0.00227044
Iteration 18/25 | Loss: 0.00227044
Iteration 19/25 | Loss: 0.00227044
Iteration 20/25 | Loss: 0.00227044
Iteration 21/25 | Loss: 0.00227044
Iteration 22/25 | Loss: 0.00227044
Iteration 23/25 | Loss: 0.00227044
Iteration 24/25 | Loss: 0.00227044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002270439639687538, 0.002270439639687538, 0.002270439639687538, 0.002270439639687538, 0.002270439639687538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002270439639687538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227044
Iteration 2/1000 | Loss: 0.00198508
Iteration 3/1000 | Loss: 0.00140693
Iteration 4/1000 | Loss: 0.00055772
Iteration 5/1000 | Loss: 0.00045387
Iteration 6/1000 | Loss: 0.00066138
Iteration 7/1000 | Loss: 0.00048537
Iteration 8/1000 | Loss: 0.00022555
Iteration 9/1000 | Loss: 0.00012085
Iteration 10/1000 | Loss: 0.00042180
Iteration 11/1000 | Loss: 0.00109597
Iteration 12/1000 | Loss: 0.00107186
Iteration 13/1000 | Loss: 0.00072095
Iteration 14/1000 | Loss: 0.00047801
Iteration 15/1000 | Loss: 0.00012453
Iteration 16/1000 | Loss: 0.00010441
Iteration 17/1000 | Loss: 0.00009929
Iteration 18/1000 | Loss: 0.00009525
Iteration 19/1000 | Loss: 0.00009492
Iteration 20/1000 | Loss: 0.00007844
Iteration 21/1000 | Loss: 0.00098967
Iteration 22/1000 | Loss: 0.00187493
Iteration 23/1000 | Loss: 0.00010630
Iteration 24/1000 | Loss: 0.00010207
Iteration 25/1000 | Loss: 0.00008686
Iteration 26/1000 | Loss: 0.00007586
Iteration 27/1000 | Loss: 0.00007483
Iteration 28/1000 | Loss: 0.00007652
Iteration 29/1000 | Loss: 0.00066090
Iteration 30/1000 | Loss: 0.00132901
Iteration 31/1000 | Loss: 0.00043839
Iteration 32/1000 | Loss: 0.00039801
Iteration 33/1000 | Loss: 0.00024511
Iteration 34/1000 | Loss: 0.00007173
Iteration 35/1000 | Loss: 0.00006863
Iteration 36/1000 | Loss: 0.00008172
Iteration 37/1000 | Loss: 0.00006408
Iteration 38/1000 | Loss: 0.00006480
Iteration 39/1000 | Loss: 0.00007459
Iteration 40/1000 | Loss: 0.00067056
Iteration 41/1000 | Loss: 0.00129016
Iteration 42/1000 | Loss: 0.00048433
Iteration 43/1000 | Loss: 0.00052649
Iteration 44/1000 | Loss: 0.00035020
Iteration 45/1000 | Loss: 0.00009334
Iteration 46/1000 | Loss: 0.00021382
Iteration 47/1000 | Loss: 0.00009314
Iteration 48/1000 | Loss: 0.00030452
Iteration 49/1000 | Loss: 0.00016416
Iteration 50/1000 | Loss: 0.00020348
Iteration 51/1000 | Loss: 0.00006254
Iteration 52/1000 | Loss: 0.00008644
Iteration 53/1000 | Loss: 0.00006400
Iteration 54/1000 | Loss: 0.00005614
Iteration 55/1000 | Loss: 0.00005930
Iteration 56/1000 | Loss: 0.00005681
Iteration 57/1000 | Loss: 0.00006774
Iteration 58/1000 | Loss: 0.00005755
Iteration 59/1000 | Loss: 0.00004323
Iteration 60/1000 | Loss: 0.00005745
Iteration 61/1000 | Loss: 0.00004672
Iteration 62/1000 | Loss: 0.00006119
Iteration 63/1000 | Loss: 0.00005551
Iteration 64/1000 | Loss: 0.00006563
Iteration 65/1000 | Loss: 0.00006497
Iteration 66/1000 | Loss: 0.00004051
Iteration 67/1000 | Loss: 0.00006289
Iteration 68/1000 | Loss: 0.00005842
Iteration 69/1000 | Loss: 0.00006897
Iteration 70/1000 | Loss: 0.00006319
Iteration 71/1000 | Loss: 0.00005642
Iteration 72/1000 | Loss: 0.00005445
Iteration 73/1000 | Loss: 0.00005847
Iteration 74/1000 | Loss: 0.00006523
Iteration 75/1000 | Loss: 0.00003811
Iteration 76/1000 | Loss: 0.00003833
Iteration 77/1000 | Loss: 0.00005129
Iteration 78/1000 | Loss: 0.00005398
Iteration 79/1000 | Loss: 0.00005202
Iteration 80/1000 | Loss: 0.00005162
Iteration 81/1000 | Loss: 0.00005506
Iteration 82/1000 | Loss: 0.00006679
Iteration 83/1000 | Loss: 0.00005594
Iteration 84/1000 | Loss: 0.00006068
Iteration 85/1000 | Loss: 0.00005251
Iteration 86/1000 | Loss: 0.00005636
Iteration 87/1000 | Loss: 0.00004951
Iteration 88/1000 | Loss: 0.00005571
Iteration 89/1000 | Loss: 0.00005089
Iteration 90/1000 | Loss: 0.00006351
Iteration 91/1000 | Loss: 0.00005132
Iteration 92/1000 | Loss: 0.00004538
Iteration 93/1000 | Loss: 0.00006482
Iteration 94/1000 | Loss: 0.00005822
Iteration 95/1000 | Loss: 0.00006170
Iteration 96/1000 | Loss: 0.00005810
Iteration 97/1000 | Loss: 0.00006188
Iteration 98/1000 | Loss: 0.00006038
Iteration 99/1000 | Loss: 0.00005720
Iteration 100/1000 | Loss: 0.00005870
Iteration 101/1000 | Loss: 0.00005821
Iteration 102/1000 | Loss: 0.00005991
Iteration 103/1000 | Loss: 0.00005887
Iteration 104/1000 | Loss: 0.00004299
Iteration 105/1000 | Loss: 0.00005948
Iteration 106/1000 | Loss: 0.00005826
Iteration 107/1000 | Loss: 0.00006298
Iteration 108/1000 | Loss: 0.00006105
Iteration 109/1000 | Loss: 0.00004589
Iteration 110/1000 | Loss: 0.00003982
Iteration 111/1000 | Loss: 0.00005689
Iteration 112/1000 | Loss: 0.00006179
Iteration 113/1000 | Loss: 0.00006273
Iteration 114/1000 | Loss: 0.00006109
Iteration 115/1000 | Loss: 0.00006236
Iteration 116/1000 | Loss: 0.00006150
Iteration 117/1000 | Loss: 0.00006309
Iteration 118/1000 | Loss: 0.00006853
Iteration 119/1000 | Loss: 0.00006220
Iteration 120/1000 | Loss: 0.00006219
Iteration 121/1000 | Loss: 0.00006291
Iteration 122/1000 | Loss: 0.00006157
Iteration 123/1000 | Loss: 0.00006279
Iteration 124/1000 | Loss: 0.00006152
Iteration 125/1000 | Loss: 0.00005801
Iteration 126/1000 | Loss: 0.00005965
Iteration 127/1000 | Loss: 0.00005852
Iteration 128/1000 | Loss: 0.00006716
Iteration 129/1000 | Loss: 0.00005989
Iteration 130/1000 | Loss: 0.00006445
Iteration 131/1000 | Loss: 0.00006064
Iteration 132/1000 | Loss: 0.00006814
Iteration 133/1000 | Loss: 0.00006480
Iteration 134/1000 | Loss: 0.00005825
Iteration 135/1000 | Loss: 0.00005647
Iteration 136/1000 | Loss: 0.00006822
Iteration 137/1000 | Loss: 0.00005741
Iteration 138/1000 | Loss: 0.00006767
Iteration 139/1000 | Loss: 0.00005477
Iteration 140/1000 | Loss: 0.00006256
Iteration 141/1000 | Loss: 0.00007696
Iteration 142/1000 | Loss: 0.00004508
Iteration 143/1000 | Loss: 0.00003936
Iteration 144/1000 | Loss: 0.00004337
Iteration 145/1000 | Loss: 0.00003624
Iteration 146/1000 | Loss: 0.00004411
Iteration 147/1000 | Loss: 0.00004842
Iteration 148/1000 | Loss: 0.00004603
Iteration 149/1000 | Loss: 0.00004627
Iteration 150/1000 | Loss: 0.00005053
Iteration 151/1000 | Loss: 0.00003581
Iteration 152/1000 | Loss: 0.00004443
Iteration 153/1000 | Loss: 0.00004969
Iteration 154/1000 | Loss: 0.00004434
Iteration 155/1000 | Loss: 0.00003628
Iteration 156/1000 | Loss: 0.00003815
Iteration 157/1000 | Loss: 0.00003881
Iteration 158/1000 | Loss: 0.00004475
Iteration 159/1000 | Loss: 0.00003340
Iteration 160/1000 | Loss: 0.00003308
Iteration 161/1000 | Loss: 0.00004130
Iteration 162/1000 | Loss: 0.00004494
Iteration 163/1000 | Loss: 0.00004818
Iteration 164/1000 | Loss: 0.00005181
Iteration 165/1000 | Loss: 0.00004329
Iteration 166/1000 | Loss: 0.00004124
Iteration 167/1000 | Loss: 0.00004863
Iteration 168/1000 | Loss: 0.00004420
Iteration 169/1000 | Loss: 0.00004101
Iteration 170/1000 | Loss: 0.00005384
Iteration 171/1000 | Loss: 0.00004160
Iteration 172/1000 | Loss: 0.00003510
Iteration 173/1000 | Loss: 0.00004209
Iteration 174/1000 | Loss: 0.00003278
Iteration 175/1000 | Loss: 0.00003267
Iteration 176/1000 | Loss: 0.00003266
Iteration 177/1000 | Loss: 0.00003266
Iteration 178/1000 | Loss: 0.00003257
Iteration 179/1000 | Loss: 0.00003255
Iteration 180/1000 | Loss: 0.00003255
Iteration 181/1000 | Loss: 0.00003254
Iteration 182/1000 | Loss: 0.00003254
Iteration 183/1000 | Loss: 0.00003254
Iteration 184/1000 | Loss: 0.00003254
Iteration 185/1000 | Loss: 0.00003253
Iteration 186/1000 | Loss: 0.00003253
Iteration 187/1000 | Loss: 0.00003252
Iteration 188/1000 | Loss: 0.00003252
Iteration 189/1000 | Loss: 0.00003245
Iteration 190/1000 | Loss: 0.00003986
Iteration 191/1000 | Loss: 0.00004418
Iteration 192/1000 | Loss: 0.00004319
Iteration 193/1000 | Loss: 0.00004679
Iteration 194/1000 | Loss: 0.00004125
Iteration 195/1000 | Loss: 0.00004894
Iteration 196/1000 | Loss: 0.00004624
Iteration 197/1000 | Loss: 0.00003918
Iteration 198/1000 | Loss: 0.00004038
Iteration 199/1000 | Loss: 0.00004830
Iteration 200/1000 | Loss: 0.00004483
Iteration 201/1000 | Loss: 0.00004854
Iteration 202/1000 | Loss: 0.00004849
Iteration 203/1000 | Loss: 0.00003960
Iteration 204/1000 | Loss: 0.00004434
Iteration 205/1000 | Loss: 0.00004374
Iteration 206/1000 | Loss: 0.00004911
Iteration 207/1000 | Loss: 0.00004327
Iteration 208/1000 | Loss: 0.00004671
Iteration 209/1000 | Loss: 0.00004914
Iteration 210/1000 | Loss: 0.00005239
Iteration 211/1000 | Loss: 0.00004627
Iteration 212/1000 | Loss: 0.00005084
Iteration 213/1000 | Loss: 0.00003375
Iteration 214/1000 | Loss: 0.00003432
Iteration 215/1000 | Loss: 0.00003946
Iteration 216/1000 | Loss: 0.00004355
Iteration 217/1000 | Loss: 0.00004856
Iteration 218/1000 | Loss: 0.00004398
Iteration 219/1000 | Loss: 0.00005011
Iteration 220/1000 | Loss: 0.00004620
Iteration 221/1000 | Loss: 0.00004824
Iteration 222/1000 | Loss: 0.00004939
Iteration 223/1000 | Loss: 0.00003356
Iteration 224/1000 | Loss: 0.00003271
Iteration 225/1000 | Loss: 0.00003378
Iteration 226/1000 | Loss: 0.00003459
Iteration 227/1000 | Loss: 0.00003235
Iteration 228/1000 | Loss: 0.00003889
Iteration 229/1000 | Loss: 0.00003951
Iteration 230/1000 | Loss: 0.00004815
Iteration 231/1000 | Loss: 0.00004579
Iteration 232/1000 | Loss: 0.00003345
Iteration 233/1000 | Loss: 0.00003305
Iteration 234/1000 | Loss: 0.00003233
Iteration 235/1000 | Loss: 0.00004592
Iteration 236/1000 | Loss: 0.00004765
Iteration 237/1000 | Loss: 0.00003292
Iteration 238/1000 | Loss: 0.00003264
Iteration 239/1000 | Loss: 0.00003251
Iteration 240/1000 | Loss: 0.00003360
Iteration 241/1000 | Loss: 0.00003356
Iteration 242/1000 | Loss: 0.00004771
Iteration 243/1000 | Loss: 0.00004640
Iteration 244/1000 | Loss: 0.00004800
Iteration 245/1000 | Loss: 0.00004556
Iteration 246/1000 | Loss: 0.00004821
Iteration 247/1000 | Loss: 0.00004505
Iteration 248/1000 | Loss: 0.00003361
Iteration 249/1000 | Loss: 0.00003318
Iteration 250/1000 | Loss: 0.00003374
Iteration 251/1000 | Loss: 0.00003360
Iteration 252/1000 | Loss: 0.00003876
Iteration 253/1000 | Loss: 0.00003938
Iteration 254/1000 | Loss: 0.00003499
Iteration 255/1000 | Loss: 0.00003359
Iteration 256/1000 | Loss: 0.00003364
Iteration 257/1000 | Loss: 0.00004426
Iteration 258/1000 | Loss: 0.00004693
Iteration 259/1000 | Loss: 0.00003533
Iteration 260/1000 | Loss: 0.00003645
Iteration 261/1000 | Loss: 0.00003237
Iteration 262/1000 | Loss: 0.00003235
Iteration 263/1000 | Loss: 0.00003234
Iteration 264/1000 | Loss: 0.00003232
Iteration 265/1000 | Loss: 0.00003232
Iteration 266/1000 | Loss: 0.00003386
Iteration 267/1000 | Loss: 0.00003374
Iteration 268/1000 | Loss: 0.00003871
Iteration 269/1000 | Loss: 0.00003781
Iteration 270/1000 | Loss: 0.00004754
Iteration 271/1000 | Loss: 0.00004669
Iteration 272/1000 | Loss: 0.00040442
Iteration 273/1000 | Loss: 0.00049216
Iteration 274/1000 | Loss: 0.00034992
Iteration 275/1000 | Loss: 0.00004634
Iteration 276/1000 | Loss: 0.00003678
Iteration 277/1000 | Loss: 0.00003411
Iteration 278/1000 | Loss: 0.00003323
Iteration 279/1000 | Loss: 0.00003272
Iteration 280/1000 | Loss: 0.00003243
Iteration 281/1000 | Loss: 0.00038016
Iteration 282/1000 | Loss: 0.00006178
Iteration 283/1000 | Loss: 0.00004282
Iteration 284/1000 | Loss: 0.00003428
Iteration 285/1000 | Loss: 0.00003195
Iteration 286/1000 | Loss: 0.00003135
Iteration 287/1000 | Loss: 0.00003099
Iteration 288/1000 | Loss: 0.00003063
Iteration 289/1000 | Loss: 0.00003040
Iteration 290/1000 | Loss: 0.00003019
Iteration 291/1000 | Loss: 0.00003017
Iteration 292/1000 | Loss: 0.00003016
Iteration 293/1000 | Loss: 0.00003013
Iteration 294/1000 | Loss: 0.00003012
Iteration 295/1000 | Loss: 0.00003011
Iteration 296/1000 | Loss: 0.00003011
Iteration 297/1000 | Loss: 0.00003010
Iteration 298/1000 | Loss: 0.00041020
Iteration 299/1000 | Loss: 0.00039936
Iteration 300/1000 | Loss: 0.00007368
Iteration 301/1000 | Loss: 0.00022013
Iteration 302/1000 | Loss: 0.00030690
Iteration 303/1000 | Loss: 0.00041780
Iteration 304/1000 | Loss: 0.00003225
Iteration 305/1000 | Loss: 0.00002925
Iteration 306/1000 | Loss: 0.00002769
Iteration 307/1000 | Loss: 0.00002635
Iteration 308/1000 | Loss: 0.00002546
Iteration 309/1000 | Loss: 0.00041715
Iteration 310/1000 | Loss: 0.00002730
Iteration 311/1000 | Loss: 0.00002500
Iteration 312/1000 | Loss: 0.00002420
Iteration 313/1000 | Loss: 0.00041852
Iteration 314/1000 | Loss: 0.00002512
Iteration 315/1000 | Loss: 0.00002328
Iteration 316/1000 | Loss: 0.00002227
Iteration 317/1000 | Loss: 0.00002154
Iteration 318/1000 | Loss: 0.00002101
Iteration 319/1000 | Loss: 0.00002064
Iteration 320/1000 | Loss: 0.00002056
Iteration 321/1000 | Loss: 0.00002045
Iteration 322/1000 | Loss: 0.00002035
Iteration 323/1000 | Loss: 0.00002031
Iteration 324/1000 | Loss: 0.00002029
Iteration 325/1000 | Loss: 0.00002029
Iteration 326/1000 | Loss: 0.00002029
Iteration 327/1000 | Loss: 0.00002028
Iteration 328/1000 | Loss: 0.00002028
Iteration 329/1000 | Loss: 0.00002028
Iteration 330/1000 | Loss: 0.00002028
Iteration 331/1000 | Loss: 0.00002027
Iteration 332/1000 | Loss: 0.00002027
Iteration 333/1000 | Loss: 0.00002027
Iteration 334/1000 | Loss: 0.00002026
Iteration 335/1000 | Loss: 0.00002024
Iteration 336/1000 | Loss: 0.00002024
Iteration 337/1000 | Loss: 0.00002024
Iteration 338/1000 | Loss: 0.00002023
Iteration 339/1000 | Loss: 0.00002023
Iteration 340/1000 | Loss: 0.00002023
Iteration 341/1000 | Loss: 0.00002022
Iteration 342/1000 | Loss: 0.00002022
Iteration 343/1000 | Loss: 0.00002021
Iteration 344/1000 | Loss: 0.00002021
Iteration 345/1000 | Loss: 0.00002020
Iteration 346/1000 | Loss: 0.00002020
Iteration 347/1000 | Loss: 0.00002020
Iteration 348/1000 | Loss: 0.00002020
Iteration 349/1000 | Loss: 0.00002019
Iteration 350/1000 | Loss: 0.00002019
Iteration 351/1000 | Loss: 0.00002019
Iteration 352/1000 | Loss: 0.00002019
Iteration 353/1000 | Loss: 0.00002018
Iteration 354/1000 | Loss: 0.00002018
Iteration 355/1000 | Loss: 0.00002018
Iteration 356/1000 | Loss: 0.00002017
Iteration 357/1000 | Loss: 0.00002017
Iteration 358/1000 | Loss: 0.00002017
Iteration 359/1000 | Loss: 0.00002017
Iteration 360/1000 | Loss: 0.00002017
Iteration 361/1000 | Loss: 0.00002017
Iteration 362/1000 | Loss: 0.00002017
Iteration 363/1000 | Loss: 0.00002017
Iteration 364/1000 | Loss: 0.00002017
Iteration 365/1000 | Loss: 0.00002017
Iteration 366/1000 | Loss: 0.00002017
Iteration 367/1000 | Loss: 0.00002017
Iteration 368/1000 | Loss: 0.00002017
Iteration 369/1000 | Loss: 0.00002017
Iteration 370/1000 | Loss: 0.00002017
Iteration 371/1000 | Loss: 0.00002017
Iteration 372/1000 | Loss: 0.00002017
Iteration 373/1000 | Loss: 0.00002017
Iteration 374/1000 | Loss: 0.00002017
Iteration 375/1000 | Loss: 0.00002017
Iteration 376/1000 | Loss: 0.00002017
Iteration 377/1000 | Loss: 0.00002017
Iteration 378/1000 | Loss: 0.00002017
Iteration 379/1000 | Loss: 0.00002017
Iteration 380/1000 | Loss: 0.00002017
Iteration 381/1000 | Loss: 0.00002017
Iteration 382/1000 | Loss: 0.00002017
Iteration 383/1000 | Loss: 0.00002017
Iteration 384/1000 | Loss: 0.00002017
Iteration 385/1000 | Loss: 0.00002017
Iteration 386/1000 | Loss: 0.00002017
Iteration 387/1000 | Loss: 0.00002017
Iteration 388/1000 | Loss: 0.00002017
Iteration 389/1000 | Loss: 0.00002017
Iteration 390/1000 | Loss: 0.00002017
Iteration 391/1000 | Loss: 0.00002017
Iteration 392/1000 | Loss: 0.00002017
Iteration 393/1000 | Loss: 0.00002017
Iteration 394/1000 | Loss: 0.00002017
Iteration 395/1000 | Loss: 0.00002017
Iteration 396/1000 | Loss: 0.00002017
Iteration 397/1000 | Loss: 0.00002017
Iteration 398/1000 | Loss: 0.00002017
Iteration 399/1000 | Loss: 0.00002017
Iteration 400/1000 | Loss: 0.00002017
Iteration 401/1000 | Loss: 0.00002017
Iteration 402/1000 | Loss: 0.00002017
Iteration 403/1000 | Loss: 0.00002017
Iteration 404/1000 | Loss: 0.00002017
Iteration 405/1000 | Loss: 0.00002017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [2.0166635295026936e-05, 2.0166635295026936e-05, 2.0166635295026936e-05, 2.0166635295026936e-05, 2.0166635295026936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0166635295026936e-05

Optimization complete. Final v2v error: 3.3396198749542236 mm

Highest mean error: 11.323735237121582 mm for frame 209

Lowest mean error: 2.9117414951324463 mm for frame 110

Saving results

Total time: 532.2756636142731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015956
Iteration 2/25 | Loss: 0.01015956
Iteration 3/25 | Loss: 0.00249926
Iteration 4/25 | Loss: 0.00176320
Iteration 5/25 | Loss: 0.00160749
Iteration 6/25 | Loss: 0.00157440
Iteration 7/25 | Loss: 0.00146495
Iteration 8/25 | Loss: 0.00148670
Iteration 9/25 | Loss: 0.00138071
Iteration 10/25 | Loss: 0.00125747
Iteration 11/25 | Loss: 0.00123091
Iteration 12/25 | Loss: 0.00118547
Iteration 13/25 | Loss: 0.00119427
Iteration 14/25 | Loss: 0.00118513
Iteration 15/25 | Loss: 0.00116998
Iteration 16/25 | Loss: 0.00116567
Iteration 17/25 | Loss: 0.00116888
Iteration 18/25 | Loss: 0.00116689
Iteration 19/25 | Loss: 0.00116377
Iteration 20/25 | Loss: 0.00115958
Iteration 21/25 | Loss: 0.00115472
Iteration 22/25 | Loss: 0.00115378
Iteration 23/25 | Loss: 0.00115492
Iteration 24/25 | Loss: 0.00115209
Iteration 25/25 | Loss: 0.00115265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27495670
Iteration 2/25 | Loss: 0.00233284
Iteration 3/25 | Loss: 0.00207990
Iteration 4/25 | Loss: 0.00207990
Iteration 5/25 | Loss: 0.00207990
Iteration 6/25 | Loss: 0.00207990
Iteration 7/25 | Loss: 0.00207990
Iteration 8/25 | Loss: 0.00207990
Iteration 9/25 | Loss: 0.00207990
Iteration 10/25 | Loss: 0.00207990
Iteration 11/25 | Loss: 0.00207990
Iteration 12/25 | Loss: 0.00207990
Iteration 13/25 | Loss: 0.00207990
Iteration 14/25 | Loss: 0.00207990
Iteration 15/25 | Loss: 0.00207990
Iteration 16/25 | Loss: 0.00207990
Iteration 17/25 | Loss: 0.00207990
Iteration 18/25 | Loss: 0.00207990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002079899189993739, 0.002079899189993739, 0.002079899189993739, 0.002079899189993739, 0.002079899189993739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002079899189993739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207990
Iteration 2/1000 | Loss: 0.00086292
Iteration 3/1000 | Loss: 0.00036250
Iteration 4/1000 | Loss: 0.00030888
Iteration 5/1000 | Loss: 0.00034959
Iteration 6/1000 | Loss: 0.00016932
Iteration 7/1000 | Loss: 0.00029842
Iteration 8/1000 | Loss: 0.00034822
Iteration 9/1000 | Loss: 0.00018940
Iteration 10/1000 | Loss: 0.00030214
Iteration 11/1000 | Loss: 0.00065134
Iteration 12/1000 | Loss: 0.00039957
Iteration 13/1000 | Loss: 0.00034472
Iteration 14/1000 | Loss: 0.00032741
Iteration 15/1000 | Loss: 0.00006833
Iteration 16/1000 | Loss: 0.00014339
Iteration 17/1000 | Loss: 0.00009671
Iteration 18/1000 | Loss: 0.00006018
Iteration 19/1000 | Loss: 0.00006289
Iteration 20/1000 | Loss: 0.00008944
Iteration 21/1000 | Loss: 0.00009601
Iteration 22/1000 | Loss: 0.00004736
Iteration 23/1000 | Loss: 0.00005853
Iteration 24/1000 | Loss: 0.00004547
Iteration 25/1000 | Loss: 0.00004197
Iteration 26/1000 | Loss: 0.00019877
Iteration 27/1000 | Loss: 0.00004165
Iteration 28/1000 | Loss: 0.00004422
Iteration 29/1000 | Loss: 0.00022667
Iteration 30/1000 | Loss: 0.00023370
Iteration 31/1000 | Loss: 0.00046309
Iteration 32/1000 | Loss: 0.00036268
Iteration 33/1000 | Loss: 0.00057129
Iteration 34/1000 | Loss: 0.00019978
Iteration 35/1000 | Loss: 0.00043699
Iteration 36/1000 | Loss: 0.00016846
Iteration 37/1000 | Loss: 0.00011009
Iteration 38/1000 | Loss: 0.00013570
Iteration 39/1000 | Loss: 0.00014477
Iteration 40/1000 | Loss: 0.00014995
Iteration 41/1000 | Loss: 0.00004834
Iteration 42/1000 | Loss: 0.00009832
Iteration 43/1000 | Loss: 0.00011609
Iteration 44/1000 | Loss: 0.00055233
Iteration 45/1000 | Loss: 0.00021731
Iteration 46/1000 | Loss: 0.00030408
Iteration 47/1000 | Loss: 0.00030784
Iteration 48/1000 | Loss: 0.00023149
Iteration 49/1000 | Loss: 0.00065220
Iteration 50/1000 | Loss: 0.00007410
Iteration 51/1000 | Loss: 0.00005344
Iteration 52/1000 | Loss: 0.00003513
Iteration 53/1000 | Loss: 0.00018094
Iteration 54/1000 | Loss: 0.00004830
Iteration 55/1000 | Loss: 0.00004403
Iteration 56/1000 | Loss: 0.00004103
Iteration 57/1000 | Loss: 0.00003900
Iteration 58/1000 | Loss: 0.00003381
Iteration 59/1000 | Loss: 0.00003100
Iteration 60/1000 | Loss: 0.00003028
Iteration 61/1000 | Loss: 0.00002971
Iteration 62/1000 | Loss: 0.00002925
Iteration 63/1000 | Loss: 0.00002890
Iteration 64/1000 | Loss: 0.00018982
Iteration 65/1000 | Loss: 0.00017355
Iteration 66/1000 | Loss: 0.00003734
Iteration 67/1000 | Loss: 0.00003466
Iteration 68/1000 | Loss: 0.00003299
Iteration 69/1000 | Loss: 0.00003152
Iteration 70/1000 | Loss: 0.00003060
Iteration 71/1000 | Loss: 0.00002984
Iteration 72/1000 | Loss: 0.00002885
Iteration 73/1000 | Loss: 0.00002855
Iteration 74/1000 | Loss: 0.00002809
Iteration 75/1000 | Loss: 0.00002778
Iteration 76/1000 | Loss: 0.00004447
Iteration 77/1000 | Loss: 0.00004124
Iteration 78/1000 | Loss: 0.00004481
Iteration 79/1000 | Loss: 0.00002912
Iteration 80/1000 | Loss: 0.00002803
Iteration 81/1000 | Loss: 0.00002746
Iteration 82/1000 | Loss: 0.00002719
Iteration 83/1000 | Loss: 0.00002711
Iteration 84/1000 | Loss: 0.00002687
Iteration 85/1000 | Loss: 0.00002670
Iteration 86/1000 | Loss: 0.00016836
Iteration 87/1000 | Loss: 0.00013207
Iteration 88/1000 | Loss: 0.00003455
Iteration 89/1000 | Loss: 0.00003247
Iteration 90/1000 | Loss: 0.00003063
Iteration 91/1000 | Loss: 0.00002948
Iteration 92/1000 | Loss: 0.00002868
Iteration 93/1000 | Loss: 0.00002824
Iteration 94/1000 | Loss: 0.00011951
Iteration 95/1000 | Loss: 0.00005137
Iteration 96/1000 | Loss: 0.00003051
Iteration 97/1000 | Loss: 0.00002784
Iteration 98/1000 | Loss: 0.00002695
Iteration 99/1000 | Loss: 0.00002657
Iteration 100/1000 | Loss: 0.00002643
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002638
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002618
Iteration 106/1000 | Loss: 0.00002617
Iteration 107/1000 | Loss: 0.00002609
Iteration 108/1000 | Loss: 0.00002602
Iteration 109/1000 | Loss: 0.00002601
Iteration 110/1000 | Loss: 0.00002600
Iteration 111/1000 | Loss: 0.00002600
Iteration 112/1000 | Loss: 0.00002600
Iteration 113/1000 | Loss: 0.00002600
Iteration 114/1000 | Loss: 0.00002600
Iteration 115/1000 | Loss: 0.00002599
Iteration 116/1000 | Loss: 0.00002599
Iteration 117/1000 | Loss: 0.00002599
Iteration 118/1000 | Loss: 0.00002598
Iteration 119/1000 | Loss: 0.00002590
Iteration 120/1000 | Loss: 0.00002587
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002586
Iteration 123/1000 | Loss: 0.00002585
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Iteration 128/1000 | Loss: 0.00002584
Iteration 129/1000 | Loss: 0.00002584
Iteration 130/1000 | Loss: 0.00002583
Iteration 131/1000 | Loss: 0.00002583
Iteration 132/1000 | Loss: 0.00002583
Iteration 133/1000 | Loss: 0.00002583
Iteration 134/1000 | Loss: 0.00002582
Iteration 135/1000 | Loss: 0.00002582
Iteration 136/1000 | Loss: 0.00002582
Iteration 137/1000 | Loss: 0.00002582
Iteration 138/1000 | Loss: 0.00002582
Iteration 139/1000 | Loss: 0.00002582
Iteration 140/1000 | Loss: 0.00002582
Iteration 141/1000 | Loss: 0.00002581
Iteration 142/1000 | Loss: 0.00002581
Iteration 143/1000 | Loss: 0.00002581
Iteration 144/1000 | Loss: 0.00002581
Iteration 145/1000 | Loss: 0.00002580
Iteration 146/1000 | Loss: 0.00002580
Iteration 147/1000 | Loss: 0.00002580
Iteration 148/1000 | Loss: 0.00002580
Iteration 149/1000 | Loss: 0.00002580
Iteration 150/1000 | Loss: 0.00002580
Iteration 151/1000 | Loss: 0.00002580
Iteration 152/1000 | Loss: 0.00002580
Iteration 153/1000 | Loss: 0.00002580
Iteration 154/1000 | Loss: 0.00002579
Iteration 155/1000 | Loss: 0.00002579
Iteration 156/1000 | Loss: 0.00002579
Iteration 157/1000 | Loss: 0.00002578
Iteration 158/1000 | Loss: 0.00002578
Iteration 159/1000 | Loss: 0.00002578
Iteration 160/1000 | Loss: 0.00002577
Iteration 161/1000 | Loss: 0.00002577
Iteration 162/1000 | Loss: 0.00002577
Iteration 163/1000 | Loss: 0.00002576
Iteration 164/1000 | Loss: 0.00002576
Iteration 165/1000 | Loss: 0.00002576
Iteration 166/1000 | Loss: 0.00002576
Iteration 167/1000 | Loss: 0.00002575
Iteration 168/1000 | Loss: 0.00002575
Iteration 169/1000 | Loss: 0.00002575
Iteration 170/1000 | Loss: 0.00002575
Iteration 171/1000 | Loss: 0.00002575
Iteration 172/1000 | Loss: 0.00002574
Iteration 173/1000 | Loss: 0.00002574
Iteration 174/1000 | Loss: 0.00002574
Iteration 175/1000 | Loss: 0.00002574
Iteration 176/1000 | Loss: 0.00002574
Iteration 177/1000 | Loss: 0.00002574
Iteration 178/1000 | Loss: 0.00002574
Iteration 179/1000 | Loss: 0.00002573
Iteration 180/1000 | Loss: 0.00002573
Iteration 181/1000 | Loss: 0.00002573
Iteration 182/1000 | Loss: 0.00002573
Iteration 183/1000 | Loss: 0.00002572
Iteration 184/1000 | Loss: 0.00002572
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00002571
Iteration 187/1000 | Loss: 0.00002571
Iteration 188/1000 | Loss: 0.00002571
Iteration 189/1000 | Loss: 0.00002571
Iteration 190/1000 | Loss: 0.00002571
Iteration 191/1000 | Loss: 0.00002571
Iteration 192/1000 | Loss: 0.00002571
Iteration 193/1000 | Loss: 0.00002571
Iteration 194/1000 | Loss: 0.00002571
Iteration 195/1000 | Loss: 0.00002571
Iteration 196/1000 | Loss: 0.00002570
Iteration 197/1000 | Loss: 0.00002570
Iteration 198/1000 | Loss: 0.00002570
Iteration 199/1000 | Loss: 0.00002570
Iteration 200/1000 | Loss: 0.00002570
Iteration 201/1000 | Loss: 0.00002570
Iteration 202/1000 | Loss: 0.00002570
Iteration 203/1000 | Loss: 0.00002569
Iteration 204/1000 | Loss: 0.00002569
Iteration 205/1000 | Loss: 0.00002568
Iteration 206/1000 | Loss: 0.00002568
Iteration 207/1000 | Loss: 0.00002568
Iteration 208/1000 | Loss: 0.00002568
Iteration 209/1000 | Loss: 0.00002567
Iteration 210/1000 | Loss: 0.00002567
Iteration 211/1000 | Loss: 0.00002567
Iteration 212/1000 | Loss: 0.00002567
Iteration 213/1000 | Loss: 0.00002566
Iteration 214/1000 | Loss: 0.00002566
Iteration 215/1000 | Loss: 0.00002566
Iteration 216/1000 | Loss: 0.00002566
Iteration 217/1000 | Loss: 0.00002566
Iteration 218/1000 | Loss: 0.00002566
Iteration 219/1000 | Loss: 0.00002566
Iteration 220/1000 | Loss: 0.00002566
Iteration 221/1000 | Loss: 0.00002566
Iteration 222/1000 | Loss: 0.00002566
Iteration 223/1000 | Loss: 0.00002566
Iteration 224/1000 | Loss: 0.00002566
Iteration 225/1000 | Loss: 0.00002565
Iteration 226/1000 | Loss: 0.00002565
Iteration 227/1000 | Loss: 0.00002565
Iteration 228/1000 | Loss: 0.00002565
Iteration 229/1000 | Loss: 0.00002565
Iteration 230/1000 | Loss: 0.00002565
Iteration 231/1000 | Loss: 0.00002565
Iteration 232/1000 | Loss: 0.00002565
Iteration 233/1000 | Loss: 0.00002565
Iteration 234/1000 | Loss: 0.00002565
Iteration 235/1000 | Loss: 0.00002565
Iteration 236/1000 | Loss: 0.00002565
Iteration 237/1000 | Loss: 0.00002565
Iteration 238/1000 | Loss: 0.00002565
Iteration 239/1000 | Loss: 0.00002565
Iteration 240/1000 | Loss: 0.00002565
Iteration 241/1000 | Loss: 0.00002565
Iteration 242/1000 | Loss: 0.00002565
Iteration 243/1000 | Loss: 0.00002565
Iteration 244/1000 | Loss: 0.00002565
Iteration 245/1000 | Loss: 0.00002565
Iteration 246/1000 | Loss: 0.00002565
Iteration 247/1000 | Loss: 0.00002565
Iteration 248/1000 | Loss: 0.00002565
Iteration 249/1000 | Loss: 0.00002565
Iteration 250/1000 | Loss: 0.00002565
Iteration 251/1000 | Loss: 0.00002565
Iteration 252/1000 | Loss: 0.00002565
Iteration 253/1000 | Loss: 0.00002565
Iteration 254/1000 | Loss: 0.00002565
Iteration 255/1000 | Loss: 0.00002565
Iteration 256/1000 | Loss: 0.00002565
Iteration 257/1000 | Loss: 0.00002565
Iteration 258/1000 | Loss: 0.00002565
Iteration 259/1000 | Loss: 0.00002565
Iteration 260/1000 | Loss: 0.00002565
Iteration 261/1000 | Loss: 0.00002565
Iteration 262/1000 | Loss: 0.00002565
Iteration 263/1000 | Loss: 0.00002565
Iteration 264/1000 | Loss: 0.00002565
Iteration 265/1000 | Loss: 0.00002565
Iteration 266/1000 | Loss: 0.00002565
Iteration 267/1000 | Loss: 0.00002565
Iteration 268/1000 | Loss: 0.00002565
Iteration 269/1000 | Loss: 0.00002565
Iteration 270/1000 | Loss: 0.00002565
Iteration 271/1000 | Loss: 0.00002565
Iteration 272/1000 | Loss: 0.00002565
Iteration 273/1000 | Loss: 0.00002565
Iteration 274/1000 | Loss: 0.00002565
Iteration 275/1000 | Loss: 0.00002565
Iteration 276/1000 | Loss: 0.00002565
Iteration 277/1000 | Loss: 0.00002565
Iteration 278/1000 | Loss: 0.00002565
Iteration 279/1000 | Loss: 0.00002565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [2.564790156611707e-05, 2.564790156611707e-05, 2.564790156611707e-05, 2.564790156611707e-05, 2.564790156611707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.564790156611707e-05

Optimization complete. Final v2v error: 3.9236555099487305 mm

Highest mean error: 19.977174758911133 mm for frame 84

Lowest mean error: 3.208054542541504 mm for frame 44

Saving results

Total time: 227.50577807426453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585329
Iteration 2/25 | Loss: 0.00109880
Iteration 3/25 | Loss: 0.00101125
Iteration 4/25 | Loss: 0.00098951
Iteration 5/25 | Loss: 0.00098171
Iteration 6/25 | Loss: 0.00097974
Iteration 7/25 | Loss: 0.00097974
Iteration 8/25 | Loss: 0.00097974
Iteration 9/25 | Loss: 0.00097974
Iteration 10/25 | Loss: 0.00097974
Iteration 11/25 | Loss: 0.00097974
Iteration 12/25 | Loss: 0.00097974
Iteration 13/25 | Loss: 0.00097974
Iteration 14/25 | Loss: 0.00097974
Iteration 15/25 | Loss: 0.00097974
Iteration 16/25 | Loss: 0.00097974
Iteration 17/25 | Loss: 0.00097974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000979740172624588, 0.000979740172624588, 0.000979740172624588, 0.000979740172624588, 0.000979740172624588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000979740172624588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07621288
Iteration 2/25 | Loss: 0.00117642
Iteration 3/25 | Loss: 0.00117642
Iteration 4/25 | Loss: 0.00117642
Iteration 5/25 | Loss: 0.00117641
Iteration 6/25 | Loss: 0.00117641
Iteration 7/25 | Loss: 0.00117641
Iteration 8/25 | Loss: 0.00117641
Iteration 9/25 | Loss: 0.00117641
Iteration 10/25 | Loss: 0.00117641
Iteration 11/25 | Loss: 0.00117641
Iteration 12/25 | Loss: 0.00117641
Iteration 13/25 | Loss: 0.00117641
Iteration 14/25 | Loss: 0.00117641
Iteration 15/25 | Loss: 0.00117641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011764136143028736, 0.0011764136143028736, 0.0011764136143028736, 0.0011764136143028736, 0.0011764136143028736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011764136143028736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117641
Iteration 2/1000 | Loss: 0.00002926
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001658
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001451
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001427
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001421
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.4207635103957728e-05, 1.4207635103957728e-05, 1.4207635103957728e-05, 1.4207635103957728e-05, 1.4207635103957728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4207635103957728e-05

Optimization complete. Final v2v error: 3.22995924949646 mm

Highest mean error: 3.4268746376037598 mm for frame 51

Lowest mean error: 2.9429547786712646 mm for frame 0

Saving results

Total time: 31.026652574539185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846847
Iteration 2/25 | Loss: 0.00103688
Iteration 3/25 | Loss: 0.00094743
Iteration 4/25 | Loss: 0.00093671
Iteration 5/25 | Loss: 0.00093325
Iteration 6/25 | Loss: 0.00093239
Iteration 7/25 | Loss: 0.00093239
Iteration 8/25 | Loss: 0.00093239
Iteration 9/25 | Loss: 0.00093239
Iteration 10/25 | Loss: 0.00093239
Iteration 11/25 | Loss: 0.00093239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009323861449956894, 0.0009323861449956894, 0.0009323861449956894, 0.0009323861449956894, 0.0009323861449956894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009323861449956894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36547303
Iteration 2/25 | Loss: 0.00114247
Iteration 3/25 | Loss: 0.00114247
Iteration 4/25 | Loss: 0.00114247
Iteration 5/25 | Loss: 0.00114247
Iteration 6/25 | Loss: 0.00114247
Iteration 7/25 | Loss: 0.00114247
Iteration 8/25 | Loss: 0.00114247
Iteration 9/25 | Loss: 0.00114247
Iteration 10/25 | Loss: 0.00114247
Iteration 11/25 | Loss: 0.00114247
Iteration 12/25 | Loss: 0.00114247
Iteration 13/25 | Loss: 0.00114247
Iteration 14/25 | Loss: 0.00114247
Iteration 15/25 | Loss: 0.00114247
Iteration 16/25 | Loss: 0.00114247
Iteration 17/25 | Loss: 0.00114247
Iteration 18/25 | Loss: 0.00114247
Iteration 19/25 | Loss: 0.00114247
Iteration 20/25 | Loss: 0.00114247
Iteration 21/25 | Loss: 0.00114247
Iteration 22/25 | Loss: 0.00114247
Iteration 23/25 | Loss: 0.00114247
Iteration 24/25 | Loss: 0.00114247
Iteration 25/25 | Loss: 0.00114247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114247
Iteration 2/1000 | Loss: 0.00001756
Iteration 3/1000 | Loss: 0.00001143
Iteration 4/1000 | Loss: 0.00000985
Iteration 5/1000 | Loss: 0.00000904
Iteration 6/1000 | Loss: 0.00000845
Iteration 7/1000 | Loss: 0.00000821
Iteration 8/1000 | Loss: 0.00000816
Iteration 9/1000 | Loss: 0.00000811
Iteration 10/1000 | Loss: 0.00000810
Iteration 11/1000 | Loss: 0.00000807
Iteration 12/1000 | Loss: 0.00000806
Iteration 13/1000 | Loss: 0.00000804
Iteration 14/1000 | Loss: 0.00000786
Iteration 15/1000 | Loss: 0.00000784
Iteration 16/1000 | Loss: 0.00000775
Iteration 17/1000 | Loss: 0.00000774
Iteration 18/1000 | Loss: 0.00000770
Iteration 19/1000 | Loss: 0.00000769
Iteration 20/1000 | Loss: 0.00000768
Iteration 21/1000 | Loss: 0.00000767
Iteration 22/1000 | Loss: 0.00000765
Iteration 23/1000 | Loss: 0.00000765
Iteration 24/1000 | Loss: 0.00000765
Iteration 25/1000 | Loss: 0.00000765
Iteration 26/1000 | Loss: 0.00000764
Iteration 27/1000 | Loss: 0.00000762
Iteration 28/1000 | Loss: 0.00000762
Iteration 29/1000 | Loss: 0.00000762
Iteration 30/1000 | Loss: 0.00000762
Iteration 31/1000 | Loss: 0.00000761
Iteration 32/1000 | Loss: 0.00000761
Iteration 33/1000 | Loss: 0.00000761
Iteration 34/1000 | Loss: 0.00000760
Iteration 35/1000 | Loss: 0.00000760
Iteration 36/1000 | Loss: 0.00000760
Iteration 37/1000 | Loss: 0.00000760
Iteration 38/1000 | Loss: 0.00000760
Iteration 39/1000 | Loss: 0.00000759
Iteration 40/1000 | Loss: 0.00000758
Iteration 41/1000 | Loss: 0.00000758
Iteration 42/1000 | Loss: 0.00000757
Iteration 43/1000 | Loss: 0.00000757
Iteration 44/1000 | Loss: 0.00000757
Iteration 45/1000 | Loss: 0.00000757
Iteration 46/1000 | Loss: 0.00000757
Iteration 47/1000 | Loss: 0.00000757
Iteration 48/1000 | Loss: 0.00000757
Iteration 49/1000 | Loss: 0.00000757
Iteration 50/1000 | Loss: 0.00000756
Iteration 51/1000 | Loss: 0.00000756
Iteration 52/1000 | Loss: 0.00000756
Iteration 53/1000 | Loss: 0.00000755
Iteration 54/1000 | Loss: 0.00000754
Iteration 55/1000 | Loss: 0.00000754
Iteration 56/1000 | Loss: 0.00000754
Iteration 57/1000 | Loss: 0.00000754
Iteration 58/1000 | Loss: 0.00000753
Iteration 59/1000 | Loss: 0.00000753
Iteration 60/1000 | Loss: 0.00000753
Iteration 61/1000 | Loss: 0.00000752
Iteration 62/1000 | Loss: 0.00000752
Iteration 63/1000 | Loss: 0.00000750
Iteration 64/1000 | Loss: 0.00000750
Iteration 65/1000 | Loss: 0.00000750
Iteration 66/1000 | Loss: 0.00000750
Iteration 67/1000 | Loss: 0.00000750
Iteration 68/1000 | Loss: 0.00000750
Iteration 69/1000 | Loss: 0.00000749
Iteration 70/1000 | Loss: 0.00000749
Iteration 71/1000 | Loss: 0.00000749
Iteration 72/1000 | Loss: 0.00000748
Iteration 73/1000 | Loss: 0.00000748
Iteration 74/1000 | Loss: 0.00000745
Iteration 75/1000 | Loss: 0.00000745
Iteration 76/1000 | Loss: 0.00000745
Iteration 77/1000 | Loss: 0.00000745
Iteration 78/1000 | Loss: 0.00000745
Iteration 79/1000 | Loss: 0.00000745
Iteration 80/1000 | Loss: 0.00000744
Iteration 81/1000 | Loss: 0.00000744
Iteration 82/1000 | Loss: 0.00000743
Iteration 83/1000 | Loss: 0.00000743
Iteration 84/1000 | Loss: 0.00000743
Iteration 85/1000 | Loss: 0.00000742
Iteration 86/1000 | Loss: 0.00000742
Iteration 87/1000 | Loss: 0.00000742
Iteration 88/1000 | Loss: 0.00000742
Iteration 89/1000 | Loss: 0.00000742
Iteration 90/1000 | Loss: 0.00000742
Iteration 91/1000 | Loss: 0.00000742
Iteration 92/1000 | Loss: 0.00000742
Iteration 93/1000 | Loss: 0.00000742
Iteration 94/1000 | Loss: 0.00000742
Iteration 95/1000 | Loss: 0.00000742
Iteration 96/1000 | Loss: 0.00000742
Iteration 97/1000 | Loss: 0.00000742
Iteration 98/1000 | Loss: 0.00000741
Iteration 99/1000 | Loss: 0.00000741
Iteration 100/1000 | Loss: 0.00000741
Iteration 101/1000 | Loss: 0.00000741
Iteration 102/1000 | Loss: 0.00000741
Iteration 103/1000 | Loss: 0.00000741
Iteration 104/1000 | Loss: 0.00000741
Iteration 105/1000 | Loss: 0.00000740
Iteration 106/1000 | Loss: 0.00000740
Iteration 107/1000 | Loss: 0.00000740
Iteration 108/1000 | Loss: 0.00000740
Iteration 109/1000 | Loss: 0.00000740
Iteration 110/1000 | Loss: 0.00000740
Iteration 111/1000 | Loss: 0.00000739
Iteration 112/1000 | Loss: 0.00000739
Iteration 113/1000 | Loss: 0.00000739
Iteration 114/1000 | Loss: 0.00000739
Iteration 115/1000 | Loss: 0.00000739
Iteration 116/1000 | Loss: 0.00000739
Iteration 117/1000 | Loss: 0.00000739
Iteration 118/1000 | Loss: 0.00000739
Iteration 119/1000 | Loss: 0.00000738
Iteration 120/1000 | Loss: 0.00000738
Iteration 121/1000 | Loss: 0.00000738
Iteration 122/1000 | Loss: 0.00000738
Iteration 123/1000 | Loss: 0.00000738
Iteration 124/1000 | Loss: 0.00000738
Iteration 125/1000 | Loss: 0.00000738
Iteration 126/1000 | Loss: 0.00000738
Iteration 127/1000 | Loss: 0.00000738
Iteration 128/1000 | Loss: 0.00000738
Iteration 129/1000 | Loss: 0.00000738
Iteration 130/1000 | Loss: 0.00000737
Iteration 131/1000 | Loss: 0.00000737
Iteration 132/1000 | Loss: 0.00000737
Iteration 133/1000 | Loss: 0.00000737
Iteration 134/1000 | Loss: 0.00000737
Iteration 135/1000 | Loss: 0.00000737
Iteration 136/1000 | Loss: 0.00000737
Iteration 137/1000 | Loss: 0.00000736
Iteration 138/1000 | Loss: 0.00000736
Iteration 139/1000 | Loss: 0.00000736
Iteration 140/1000 | Loss: 0.00000736
Iteration 141/1000 | Loss: 0.00000736
Iteration 142/1000 | Loss: 0.00000736
Iteration 143/1000 | Loss: 0.00000735
Iteration 144/1000 | Loss: 0.00000735
Iteration 145/1000 | Loss: 0.00000735
Iteration 146/1000 | Loss: 0.00000735
Iteration 147/1000 | Loss: 0.00000735
Iteration 148/1000 | Loss: 0.00000735
Iteration 149/1000 | Loss: 0.00000735
Iteration 150/1000 | Loss: 0.00000735
Iteration 151/1000 | Loss: 0.00000735
Iteration 152/1000 | Loss: 0.00000735
Iteration 153/1000 | Loss: 0.00000735
Iteration 154/1000 | Loss: 0.00000735
Iteration 155/1000 | Loss: 0.00000735
Iteration 156/1000 | Loss: 0.00000734
Iteration 157/1000 | Loss: 0.00000734
Iteration 158/1000 | Loss: 0.00000734
Iteration 159/1000 | Loss: 0.00000734
Iteration 160/1000 | Loss: 0.00000734
Iteration 161/1000 | Loss: 0.00000734
Iteration 162/1000 | Loss: 0.00000734
Iteration 163/1000 | Loss: 0.00000734
Iteration 164/1000 | Loss: 0.00000734
Iteration 165/1000 | Loss: 0.00000734
Iteration 166/1000 | Loss: 0.00000734
Iteration 167/1000 | Loss: 0.00000734
Iteration 168/1000 | Loss: 0.00000734
Iteration 169/1000 | Loss: 0.00000734
Iteration 170/1000 | Loss: 0.00000734
Iteration 171/1000 | Loss: 0.00000734
Iteration 172/1000 | Loss: 0.00000734
Iteration 173/1000 | Loss: 0.00000734
Iteration 174/1000 | Loss: 0.00000734
Iteration 175/1000 | Loss: 0.00000734
Iteration 176/1000 | Loss: 0.00000734
Iteration 177/1000 | Loss: 0.00000734
Iteration 178/1000 | Loss: 0.00000734
Iteration 179/1000 | Loss: 0.00000734
Iteration 180/1000 | Loss: 0.00000734
Iteration 181/1000 | Loss: 0.00000733
Iteration 182/1000 | Loss: 0.00000733
Iteration 183/1000 | Loss: 0.00000733
Iteration 184/1000 | Loss: 0.00000733
Iteration 185/1000 | Loss: 0.00000733
Iteration 186/1000 | Loss: 0.00000733
Iteration 187/1000 | Loss: 0.00000733
Iteration 188/1000 | Loss: 0.00000733
Iteration 189/1000 | Loss: 0.00000733
Iteration 190/1000 | Loss: 0.00000733
Iteration 191/1000 | Loss: 0.00000733
Iteration 192/1000 | Loss: 0.00000733
Iteration 193/1000 | Loss: 0.00000733
Iteration 194/1000 | Loss: 0.00000733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [7.33363503968576e-06, 7.33363503968576e-06, 7.33363503968576e-06, 7.33363503968576e-06, 7.33363503968576e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.33363503968576e-06

Optimization complete. Final v2v error: 2.347215414047241 mm

Highest mean error: 2.7296760082244873 mm for frame 91

Lowest mean error: 2.12852144241333 mm for frame 0

Saving results

Total time: 35.138944149017334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502018
Iteration 2/25 | Loss: 0.00122363
Iteration 3/25 | Loss: 0.00105915
Iteration 4/25 | Loss: 0.00104647
Iteration 5/25 | Loss: 0.00104386
Iteration 6/25 | Loss: 0.00104381
Iteration 7/25 | Loss: 0.00104381
Iteration 8/25 | Loss: 0.00104381
Iteration 9/25 | Loss: 0.00104381
Iteration 10/25 | Loss: 0.00104381
Iteration 11/25 | Loss: 0.00104381
Iteration 12/25 | Loss: 0.00104381
Iteration 13/25 | Loss: 0.00104381
Iteration 14/25 | Loss: 0.00104381
Iteration 15/25 | Loss: 0.00104381
Iteration 16/25 | Loss: 0.00104381
Iteration 17/25 | Loss: 0.00104381
Iteration 18/25 | Loss: 0.00104381
Iteration 19/25 | Loss: 0.00104381
Iteration 20/25 | Loss: 0.00104381
Iteration 21/25 | Loss: 0.00104381
Iteration 22/25 | Loss: 0.00104381
Iteration 23/25 | Loss: 0.00104381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010438074823468924, 0.0010438074823468924, 0.0010438074823468924, 0.0010438074823468924, 0.0010438074823468924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010438074823468924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29518867
Iteration 2/25 | Loss: 0.00119507
Iteration 3/25 | Loss: 0.00119506
Iteration 4/25 | Loss: 0.00119506
Iteration 5/25 | Loss: 0.00119506
Iteration 6/25 | Loss: 0.00119506
Iteration 7/25 | Loss: 0.00119506
Iteration 8/25 | Loss: 0.00119506
Iteration 9/25 | Loss: 0.00119506
Iteration 10/25 | Loss: 0.00119506
Iteration 11/25 | Loss: 0.00119506
Iteration 12/25 | Loss: 0.00119506
Iteration 13/25 | Loss: 0.00119506
Iteration 14/25 | Loss: 0.00119506
Iteration 15/25 | Loss: 0.00119506
Iteration 16/25 | Loss: 0.00119506
Iteration 17/25 | Loss: 0.00119506
Iteration 18/25 | Loss: 0.00119506
Iteration 19/25 | Loss: 0.00119506
Iteration 20/25 | Loss: 0.00119506
Iteration 21/25 | Loss: 0.00119506
Iteration 22/25 | Loss: 0.00119506
Iteration 23/25 | Loss: 0.00119506
Iteration 24/25 | Loss: 0.00119506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011950606713071465, 0.0011950606713071465, 0.0011950606713071465, 0.0011950606713071465, 0.0011950606713071465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011950606713071465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119506
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00002409
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001736
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001691
Iteration 11/1000 | Loss: 0.00001675
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001614
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001607
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001605
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001604
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001601
Iteration 39/1000 | Loss: 0.00001598
Iteration 40/1000 | Loss: 0.00001598
Iteration 41/1000 | Loss: 0.00001598
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001596
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001595
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001594
Iteration 60/1000 | Loss: 0.00001594
Iteration 61/1000 | Loss: 0.00001594
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001592
Iteration 67/1000 | Loss: 0.00001592
Iteration 68/1000 | Loss: 0.00001592
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001591
Iteration 71/1000 | Loss: 0.00001591
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Iteration 121/1000 | Loss: 0.00001583
Iteration 122/1000 | Loss: 0.00001583
Iteration 123/1000 | Loss: 0.00001583
Iteration 124/1000 | Loss: 0.00001583
Iteration 125/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5829771655262448e-05, 1.5829771655262448e-05, 1.5829771655262448e-05, 1.5829771655262448e-05, 1.5829771655262448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5829771655262448e-05

Optimization complete. Final v2v error: 3.2888448238372803 mm

Highest mean error: 4.1215691566467285 mm for frame 73

Lowest mean error: 2.8215250968933105 mm for frame 101

Saving results

Total time: 42.26532435417175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825307
Iteration 2/25 | Loss: 0.00178516
Iteration 3/25 | Loss: 0.00127099
Iteration 4/25 | Loss: 0.00120084
Iteration 5/25 | Loss: 0.00120760
Iteration 6/25 | Loss: 0.00116076
Iteration 7/25 | Loss: 0.00114319
Iteration 8/25 | Loss: 0.00110638
Iteration 9/25 | Loss: 0.00109155
Iteration 10/25 | Loss: 0.00110199
Iteration 11/25 | Loss: 0.00108769
Iteration 12/25 | Loss: 0.00107880
Iteration 13/25 | Loss: 0.00107324
Iteration 14/25 | Loss: 0.00107278
Iteration 15/25 | Loss: 0.00106998
Iteration 16/25 | Loss: 0.00106949
Iteration 17/25 | Loss: 0.00106892
Iteration 18/25 | Loss: 0.00106865
Iteration 19/25 | Loss: 0.00106837
Iteration 20/25 | Loss: 0.00106809
Iteration 21/25 | Loss: 0.00106806
Iteration 22/25 | Loss: 0.00106806
Iteration 23/25 | Loss: 0.00106806
Iteration 24/25 | Loss: 0.00106806
Iteration 25/25 | Loss: 0.00106806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24187040
Iteration 2/25 | Loss: 0.00124084
Iteration 3/25 | Loss: 0.00124084
Iteration 4/25 | Loss: 0.00124084
Iteration 5/25 | Loss: 0.00124084
Iteration 6/25 | Loss: 0.00124084
Iteration 7/25 | Loss: 0.00124084
Iteration 8/25 | Loss: 0.00124084
Iteration 9/25 | Loss: 0.00124084
Iteration 10/25 | Loss: 0.00124084
Iteration 11/25 | Loss: 0.00124084
Iteration 12/25 | Loss: 0.00124084
Iteration 13/25 | Loss: 0.00124084
Iteration 14/25 | Loss: 0.00124084
Iteration 15/25 | Loss: 0.00124084
Iteration 16/25 | Loss: 0.00124084
Iteration 17/25 | Loss: 0.00124084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012408355250954628, 0.0012408355250954628, 0.0012408355250954628, 0.0012408355250954628, 0.0012408355250954628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012408355250954628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124084
Iteration 2/1000 | Loss: 0.00211141
Iteration 3/1000 | Loss: 0.00097499
Iteration 4/1000 | Loss: 0.00169273
Iteration 5/1000 | Loss: 0.00049615
Iteration 6/1000 | Loss: 0.00010804
Iteration 7/1000 | Loss: 0.00013637
Iteration 8/1000 | Loss: 0.00012246
Iteration 9/1000 | Loss: 0.00009697
Iteration 10/1000 | Loss: 0.00010124
Iteration 11/1000 | Loss: 0.00055733
Iteration 12/1000 | Loss: 0.00050234
Iteration 13/1000 | Loss: 0.00032597
Iteration 14/1000 | Loss: 0.00016418
Iteration 15/1000 | Loss: 0.00016537
Iteration 16/1000 | Loss: 0.00013094
Iteration 17/1000 | Loss: 0.00014182
Iteration 18/1000 | Loss: 0.00029334
Iteration 19/1000 | Loss: 0.00014260
Iteration 20/1000 | Loss: 0.00011169
Iteration 21/1000 | Loss: 0.00007432
Iteration 22/1000 | Loss: 0.00008178
Iteration 23/1000 | Loss: 0.00008376
Iteration 24/1000 | Loss: 0.00032247
Iteration 25/1000 | Loss: 0.00015367
Iteration 26/1000 | Loss: 0.00012230
Iteration 27/1000 | Loss: 0.00007736
Iteration 28/1000 | Loss: 0.00009633
Iteration 29/1000 | Loss: 0.00046322
Iteration 30/1000 | Loss: 0.00022160
Iteration 31/1000 | Loss: 0.00011629
Iteration 32/1000 | Loss: 0.00014251
Iteration 33/1000 | Loss: 0.00016554
Iteration 34/1000 | Loss: 0.00011454
Iteration 35/1000 | Loss: 0.00006865
Iteration 36/1000 | Loss: 0.00010621
Iteration 37/1000 | Loss: 0.00021644
Iteration 38/1000 | Loss: 0.00014351
Iteration 39/1000 | Loss: 0.00015853
Iteration 40/1000 | Loss: 0.00021935
Iteration 41/1000 | Loss: 0.00019362
Iteration 42/1000 | Loss: 0.00010484
Iteration 43/1000 | Loss: 0.00009622
Iteration 44/1000 | Loss: 0.00017450
Iteration 45/1000 | Loss: 0.00008701
Iteration 46/1000 | Loss: 0.00007676
Iteration 47/1000 | Loss: 0.00008002
Iteration 48/1000 | Loss: 0.00031859
Iteration 49/1000 | Loss: 0.00007752
Iteration 50/1000 | Loss: 0.00007164
Iteration 51/1000 | Loss: 0.00006930
Iteration 52/1000 | Loss: 0.00023842
Iteration 53/1000 | Loss: 0.00012498
Iteration 54/1000 | Loss: 0.00007264
Iteration 55/1000 | Loss: 0.00007044
Iteration 56/1000 | Loss: 0.00022540
Iteration 57/1000 | Loss: 0.00012544
Iteration 58/1000 | Loss: 0.00022476
Iteration 59/1000 | Loss: 0.00010669
Iteration 60/1000 | Loss: 0.00022259
Iteration 61/1000 | Loss: 0.00013414
Iteration 62/1000 | Loss: 0.00006773
Iteration 63/1000 | Loss: 0.00004736
Iteration 64/1000 | Loss: 0.00007400
Iteration 65/1000 | Loss: 0.00036232
Iteration 66/1000 | Loss: 0.00013011
Iteration 67/1000 | Loss: 0.00007148
Iteration 68/1000 | Loss: 0.00020924
Iteration 69/1000 | Loss: 0.00014030
Iteration 70/1000 | Loss: 0.00011983
Iteration 71/1000 | Loss: 0.00008435
Iteration 72/1000 | Loss: 0.00006811
Iteration 73/1000 | Loss: 0.00007524
Iteration 74/1000 | Loss: 0.00008530
Iteration 75/1000 | Loss: 0.00006154
Iteration 76/1000 | Loss: 0.00017519
Iteration 77/1000 | Loss: 0.00018745
Iteration 78/1000 | Loss: 0.00007786
Iteration 79/1000 | Loss: 0.00007014
Iteration 80/1000 | Loss: 0.00006624
Iteration 81/1000 | Loss: 0.00005453
Iteration 82/1000 | Loss: 0.00006316
Iteration 83/1000 | Loss: 0.00034988
Iteration 84/1000 | Loss: 0.00054949
Iteration 85/1000 | Loss: 0.00019958
Iteration 86/1000 | Loss: 0.00011058
Iteration 87/1000 | Loss: 0.00007527
Iteration 88/1000 | Loss: 0.00007182
Iteration 89/1000 | Loss: 0.00006080
Iteration 90/1000 | Loss: 0.00008033
Iteration 91/1000 | Loss: 0.00006728
Iteration 92/1000 | Loss: 0.00005595
Iteration 93/1000 | Loss: 0.00006911
Iteration 94/1000 | Loss: 0.00012212
Iteration 95/1000 | Loss: 0.00007305
Iteration 96/1000 | Loss: 0.00071264
Iteration 97/1000 | Loss: 0.00022034
Iteration 98/1000 | Loss: 0.00018527
Iteration 99/1000 | Loss: 0.00009152
Iteration 100/1000 | Loss: 0.00007388
Iteration 101/1000 | Loss: 0.00007079
Iteration 102/1000 | Loss: 0.00006941
Iteration 103/1000 | Loss: 0.00005818
Iteration 104/1000 | Loss: 0.00007082
Iteration 105/1000 | Loss: 0.00006587
Iteration 106/1000 | Loss: 0.00014955
Iteration 107/1000 | Loss: 0.00016017
Iteration 108/1000 | Loss: 0.00016379
Iteration 109/1000 | Loss: 0.00006562
Iteration 110/1000 | Loss: 0.00008045
Iteration 111/1000 | Loss: 0.00005725
Iteration 112/1000 | Loss: 0.00007156
Iteration 113/1000 | Loss: 0.00004420
Iteration 114/1000 | Loss: 0.00005414
Iteration 115/1000 | Loss: 0.00004540
Iteration 116/1000 | Loss: 0.00006728
Iteration 117/1000 | Loss: 0.00008926
Iteration 118/1000 | Loss: 0.00006817
Iteration 119/1000 | Loss: 0.00006697
Iteration 120/1000 | Loss: 0.00008892
Iteration 121/1000 | Loss: 0.00008842
Iteration 122/1000 | Loss: 0.00008539
Iteration 123/1000 | Loss: 0.00008767
Iteration 124/1000 | Loss: 0.00008633
Iteration 125/1000 | Loss: 0.00009100
Iteration 126/1000 | Loss: 0.00008604
Iteration 127/1000 | Loss: 0.00009130
Iteration 128/1000 | Loss: 0.00008148
Iteration 129/1000 | Loss: 0.00008305
Iteration 130/1000 | Loss: 0.00008579
Iteration 131/1000 | Loss: 0.00008708
Iteration 132/1000 | Loss: 0.00008572
Iteration 133/1000 | Loss: 0.00008309
Iteration 134/1000 | Loss: 0.00008694
Iteration 135/1000 | Loss: 0.00004441
Iteration 136/1000 | Loss: 0.00003686
Iteration 137/1000 | Loss: 0.00006639
Iteration 138/1000 | Loss: 0.00008589
Iteration 139/1000 | Loss: 0.00008392
Iteration 140/1000 | Loss: 0.00008224
Iteration 141/1000 | Loss: 0.00008553
Iteration 142/1000 | Loss: 0.00008653
Iteration 143/1000 | Loss: 0.00009213
Iteration 144/1000 | Loss: 0.00007425
Iteration 145/1000 | Loss: 0.00009401
Iteration 146/1000 | Loss: 0.00007677
Iteration 147/1000 | Loss: 0.00009035
Iteration 148/1000 | Loss: 0.00007603
Iteration 149/1000 | Loss: 0.00007288
Iteration 150/1000 | Loss: 0.00006775
Iteration 151/1000 | Loss: 0.00006502
Iteration 152/1000 | Loss: 0.00005782
Iteration 153/1000 | Loss: 0.00008707
Iteration 154/1000 | Loss: 0.00007270
Iteration 155/1000 | Loss: 0.00006983
Iteration 156/1000 | Loss: 0.00006180
Iteration 157/1000 | Loss: 0.00005412
Iteration 158/1000 | Loss: 0.00007666
Iteration 159/1000 | Loss: 0.00004963
Iteration 160/1000 | Loss: 0.00003027
Iteration 161/1000 | Loss: 0.00003367
Iteration 162/1000 | Loss: 0.00002929
Iteration 163/1000 | Loss: 0.00007995
Iteration 164/1000 | Loss: 0.00006682
Iteration 165/1000 | Loss: 0.00004167
Iteration 166/1000 | Loss: 0.00004402
Iteration 167/1000 | Loss: 0.00003513
Iteration 168/1000 | Loss: 0.00003492
Iteration 169/1000 | Loss: 0.00005761
Iteration 170/1000 | Loss: 0.00003943
Iteration 171/1000 | Loss: 0.00004699
Iteration 172/1000 | Loss: 0.00004019
Iteration 173/1000 | Loss: 0.00003325
Iteration 174/1000 | Loss: 0.00004070
Iteration 175/1000 | Loss: 0.00004488
Iteration 176/1000 | Loss: 0.00005895
Iteration 177/1000 | Loss: 0.00004445
Iteration 178/1000 | Loss: 0.00003960
Iteration 179/1000 | Loss: 0.00006096
Iteration 180/1000 | Loss: 0.00004439
Iteration 181/1000 | Loss: 0.00004678
Iteration 182/1000 | Loss: 0.00004461
Iteration 183/1000 | Loss: 0.00003904
Iteration 184/1000 | Loss: 0.00004534
Iteration 185/1000 | Loss: 0.00003804
Iteration 186/1000 | Loss: 0.00004377
Iteration 187/1000 | Loss: 0.00004400
Iteration 188/1000 | Loss: 0.00004558
Iteration 189/1000 | Loss: 0.00003787
Iteration 190/1000 | Loss: 0.00004252
Iteration 191/1000 | Loss: 0.00005481
Iteration 192/1000 | Loss: 0.00004399
Iteration 193/1000 | Loss: 0.00004589
Iteration 194/1000 | Loss: 0.00004238
Iteration 195/1000 | Loss: 0.00004597
Iteration 196/1000 | Loss: 0.00005692
Iteration 197/1000 | Loss: 0.00004955
Iteration 198/1000 | Loss: 0.00003337
Iteration 199/1000 | Loss: 0.00003216
Iteration 200/1000 | Loss: 0.00003119
Iteration 201/1000 | Loss: 0.00003048
Iteration 202/1000 | Loss: 0.00002763
Iteration 203/1000 | Loss: 0.00003459
Iteration 204/1000 | Loss: 0.00003800
Iteration 205/1000 | Loss: 0.00003047
Iteration 206/1000 | Loss: 0.00003981
Iteration 207/1000 | Loss: 0.00003140
Iteration 208/1000 | Loss: 0.00002271
Iteration 209/1000 | Loss: 0.00002138
Iteration 210/1000 | Loss: 0.00003159
Iteration 211/1000 | Loss: 0.00003050
Iteration 212/1000 | Loss: 0.00002361
Iteration 213/1000 | Loss: 0.00004959
Iteration 214/1000 | Loss: 0.00004982
Iteration 215/1000 | Loss: 0.00003801
Iteration 216/1000 | Loss: 0.00002359
Iteration 217/1000 | Loss: 0.00003389
Iteration 218/1000 | Loss: 0.00003109
Iteration 219/1000 | Loss: 0.00003418
Iteration 220/1000 | Loss: 0.00003552
Iteration 221/1000 | Loss: 0.00004546
Iteration 222/1000 | Loss: 0.00004476
Iteration 223/1000 | Loss: 0.00004795
Iteration 224/1000 | Loss: 0.00004249
Iteration 225/1000 | Loss: 0.00004072
Iteration 226/1000 | Loss: 0.00003156
Iteration 227/1000 | Loss: 0.00002870
Iteration 228/1000 | Loss: 0.00002956
Iteration 229/1000 | Loss: 0.00002966
Iteration 230/1000 | Loss: 0.00002658
Iteration 231/1000 | Loss: 0.00002019
Iteration 232/1000 | Loss: 0.00003092
Iteration 233/1000 | Loss: 0.00003043
Iteration 234/1000 | Loss: 0.00003033
Iteration 235/1000 | Loss: 0.00002690
Iteration 236/1000 | Loss: 0.00002994
Iteration 237/1000 | Loss: 0.00002061
Iteration 238/1000 | Loss: 0.00002683
Iteration 239/1000 | Loss: 0.00002920
Iteration 240/1000 | Loss: 0.00003019
Iteration 241/1000 | Loss: 0.00002374
Iteration 242/1000 | Loss: 0.00003491
Iteration 243/1000 | Loss: 0.00003128
Iteration 244/1000 | Loss: 0.00003124
Iteration 245/1000 | Loss: 0.00004096
Iteration 246/1000 | Loss: 0.00004290
Iteration 247/1000 | Loss: 0.00004067
Iteration 248/1000 | Loss: 0.00003052
Iteration 249/1000 | Loss: 0.00002395
Iteration 250/1000 | Loss: 0.00002763
Iteration 251/1000 | Loss: 0.00002780
Iteration 252/1000 | Loss: 0.00003215
Iteration 253/1000 | Loss: 0.00002637
Iteration 254/1000 | Loss: 0.00002042
Iteration 255/1000 | Loss: 0.00003250
Iteration 256/1000 | Loss: 0.00003122
Iteration 257/1000 | Loss: 0.00002886
Iteration 258/1000 | Loss: 0.00003224
Iteration 259/1000 | Loss: 0.00002899
Iteration 260/1000 | Loss: 0.00002726
Iteration 261/1000 | Loss: 0.00004071
Iteration 262/1000 | Loss: 0.00003529
Iteration 263/1000 | Loss: 0.00003665
Iteration 264/1000 | Loss: 0.00004220
Iteration 265/1000 | Loss: 0.00003247
Iteration 266/1000 | Loss: 0.00002189
Iteration 267/1000 | Loss: 0.00003337
Iteration 268/1000 | Loss: 0.00002772
Iteration 269/1000 | Loss: 0.00003816
Iteration 270/1000 | Loss: 0.00003816
Iteration 271/1000 | Loss: 0.00004262
Iteration 272/1000 | Loss: 0.00003356
Iteration 273/1000 | Loss: 0.00003833
Iteration 274/1000 | Loss: 0.00003470
Iteration 275/1000 | Loss: 0.00003906
Iteration 276/1000 | Loss: 0.00003964
Iteration 277/1000 | Loss: 0.00004388
Iteration 278/1000 | Loss: 0.00003782
Iteration 279/1000 | Loss: 0.00004284
Iteration 280/1000 | Loss: 0.00003691
Iteration 281/1000 | Loss: 0.00004247
Iteration 282/1000 | Loss: 0.00004029
Iteration 283/1000 | Loss: 0.00004469
Iteration 284/1000 | Loss: 0.00004297
Iteration 285/1000 | Loss: 0.00004345
Iteration 286/1000 | Loss: 0.00003988
Iteration 287/1000 | Loss: 0.00004498
Iteration 288/1000 | Loss: 0.00004556
Iteration 289/1000 | Loss: 0.00004601
Iteration 290/1000 | Loss: 0.00004085
Iteration 291/1000 | Loss: 0.00003889
Iteration 292/1000 | Loss: 0.00003415
Iteration 293/1000 | Loss: 0.00002724
Iteration 294/1000 | Loss: 0.00002631
Iteration 295/1000 | Loss: 0.00004223
Iteration 296/1000 | Loss: 0.00003943
Iteration 297/1000 | Loss: 0.00004012
Iteration 298/1000 | Loss: 0.00003526
Iteration 299/1000 | Loss: 0.00002580
Iteration 300/1000 | Loss: 0.00002581
Iteration 301/1000 | Loss: 0.00003749
Iteration 302/1000 | Loss: 0.00004396
Iteration 303/1000 | Loss: 0.00004373
Iteration 304/1000 | Loss: 0.00002762
Iteration 305/1000 | Loss: 0.00003605
Iteration 306/1000 | Loss: 0.00003489
Iteration 307/1000 | Loss: 0.00002951
Iteration 308/1000 | Loss: 0.00004231
Iteration 309/1000 | Loss: 0.00004424
Iteration 310/1000 | Loss: 0.00004258
Iteration 311/1000 | Loss: 0.00003970
Iteration 312/1000 | Loss: 0.00004026
Iteration 313/1000 | Loss: 0.00004414
Iteration 314/1000 | Loss: 0.00004289
Iteration 315/1000 | Loss: 0.00004376
Iteration 316/1000 | Loss: 0.00004319
Iteration 317/1000 | Loss: 0.00002778
Iteration 318/1000 | Loss: 0.00002489
Iteration 319/1000 | Loss: 0.00003029
Iteration 320/1000 | Loss: 0.00002612
Iteration 321/1000 | Loss: 0.00003574
Iteration 322/1000 | Loss: 0.00004386
Iteration 323/1000 | Loss: 0.00004349
Iteration 324/1000 | Loss: 0.00004496
Iteration 325/1000 | Loss: 0.00004320
Iteration 326/1000 | Loss: 0.00004482
Iteration 327/1000 | Loss: 0.00004632
Iteration 328/1000 | Loss: 0.00004444
Iteration 329/1000 | Loss: 0.00004502
Iteration 330/1000 | Loss: 0.00004381
Iteration 331/1000 | Loss: 0.00004280
Iteration 332/1000 | Loss: 0.00003632
Iteration 333/1000 | Loss: 0.00004004
Iteration 334/1000 | Loss: 0.00003118
Iteration 335/1000 | Loss: 0.00004206
Iteration 336/1000 | Loss: 0.00003506
Iteration 337/1000 | Loss: 0.00003385
Iteration 338/1000 | Loss: 0.00004639
Iteration 339/1000 | Loss: 0.00004238
Iteration 340/1000 | Loss: 0.00003467
Iteration 341/1000 | Loss: 0.00003775
Iteration 342/1000 | Loss: 0.00004659
Iteration 343/1000 | Loss: 0.00004518
Iteration 344/1000 | Loss: 0.00004252
Iteration 345/1000 | Loss: 0.00003456
Iteration 346/1000 | Loss: 0.00003479
Iteration 347/1000 | Loss: 0.00003686
Iteration 348/1000 | Loss: 0.00003875
Iteration 349/1000 | Loss: 0.00003453
Iteration 350/1000 | Loss: 0.00002715
Iteration 351/1000 | Loss: 0.00003386
Iteration 352/1000 | Loss: 0.00004262
Iteration 353/1000 | Loss: 0.00004302
Iteration 354/1000 | Loss: 0.00005170
Iteration 355/1000 | Loss: 0.00002623
Iteration 356/1000 | Loss: 0.00006971
Iteration 357/1000 | Loss: 0.00006055
Iteration 358/1000 | Loss: 0.00002197
Iteration 359/1000 | Loss: 0.00002120
Iteration 360/1000 | Loss: 0.00002073
Iteration 361/1000 | Loss: 0.00002010
Iteration 362/1000 | Loss: 0.00001971
Iteration 363/1000 | Loss: 0.00001949
Iteration 364/1000 | Loss: 0.00001938
Iteration 365/1000 | Loss: 0.00001936
Iteration 366/1000 | Loss: 0.00001928
Iteration 367/1000 | Loss: 0.00001926
Iteration 368/1000 | Loss: 0.00001925
Iteration 369/1000 | Loss: 0.00001925
Iteration 370/1000 | Loss: 0.00001924
Iteration 371/1000 | Loss: 0.00001924
Iteration 372/1000 | Loss: 0.00001923
Iteration 373/1000 | Loss: 0.00001923
Iteration 374/1000 | Loss: 0.00001922
Iteration 375/1000 | Loss: 0.00001922
Iteration 376/1000 | Loss: 0.00001922
Iteration 377/1000 | Loss: 0.00001922
Iteration 378/1000 | Loss: 0.00001921
Iteration 379/1000 | Loss: 0.00001921
Iteration 380/1000 | Loss: 0.00001921
Iteration 381/1000 | Loss: 0.00001921
Iteration 382/1000 | Loss: 0.00001921
Iteration 383/1000 | Loss: 0.00001921
Iteration 384/1000 | Loss: 0.00001921
Iteration 385/1000 | Loss: 0.00001921
Iteration 386/1000 | Loss: 0.00001921
Iteration 387/1000 | Loss: 0.00001921
Iteration 388/1000 | Loss: 0.00001921
Iteration 389/1000 | Loss: 0.00001921
Iteration 390/1000 | Loss: 0.00001920
Iteration 391/1000 | Loss: 0.00001920
Iteration 392/1000 | Loss: 0.00001920
Iteration 393/1000 | Loss: 0.00001920
Iteration 394/1000 | Loss: 0.00001919
Iteration 395/1000 | Loss: 0.00001919
Iteration 396/1000 | Loss: 0.00001919
Iteration 397/1000 | Loss: 0.00001918
Iteration 398/1000 | Loss: 0.00001918
Iteration 399/1000 | Loss: 0.00001918
Iteration 400/1000 | Loss: 0.00001917
Iteration 401/1000 | Loss: 0.00001917
Iteration 402/1000 | Loss: 0.00001917
Iteration 403/1000 | Loss: 0.00001916
Iteration 404/1000 | Loss: 0.00001916
Iteration 405/1000 | Loss: 0.00001916
Iteration 406/1000 | Loss: 0.00001916
Iteration 407/1000 | Loss: 0.00001916
Iteration 408/1000 | Loss: 0.00001916
Iteration 409/1000 | Loss: 0.00001916
Iteration 410/1000 | Loss: 0.00001916
Iteration 411/1000 | Loss: 0.00001916
Iteration 412/1000 | Loss: 0.00001915
Iteration 413/1000 | Loss: 0.00001915
Iteration 414/1000 | Loss: 0.00001914
Iteration 415/1000 | Loss: 0.00001914
Iteration 416/1000 | Loss: 0.00001914
Iteration 417/1000 | Loss: 0.00001914
Iteration 418/1000 | Loss: 0.00001914
Iteration 419/1000 | Loss: 0.00001914
Iteration 420/1000 | Loss: 0.00001914
Iteration 421/1000 | Loss: 0.00001913
Iteration 422/1000 | Loss: 0.00001913
Iteration 423/1000 | Loss: 0.00001913
Iteration 424/1000 | Loss: 0.00001913
Iteration 425/1000 | Loss: 0.00001913
Iteration 426/1000 | Loss: 0.00001912
Iteration 427/1000 | Loss: 0.00001912
Iteration 428/1000 | Loss: 0.00001912
Iteration 429/1000 | Loss: 0.00001911
Iteration 430/1000 | Loss: 0.00001910
Iteration 431/1000 | Loss: 0.00001910
Iteration 432/1000 | Loss: 0.00001909
Iteration 433/1000 | Loss: 0.00001909
Iteration 434/1000 | Loss: 0.00001909
Iteration 435/1000 | Loss: 0.00001909
Iteration 436/1000 | Loss: 0.00001908
Iteration 437/1000 | Loss: 0.00001908
Iteration 438/1000 | Loss: 0.00001908
Iteration 439/1000 | Loss: 0.00001908
Iteration 440/1000 | Loss: 0.00001908
Iteration 441/1000 | Loss: 0.00001908
Iteration 442/1000 | Loss: 0.00001908
Iteration 443/1000 | Loss: 0.00001907
Iteration 444/1000 | Loss: 0.00001907
Iteration 445/1000 | Loss: 0.00001907
Iteration 446/1000 | Loss: 0.00001906
Iteration 447/1000 | Loss: 0.00001906
Iteration 448/1000 | Loss: 0.00001906
Iteration 449/1000 | Loss: 0.00001905
Iteration 450/1000 | Loss: 0.00001905
Iteration 451/1000 | Loss: 0.00001904
Iteration 452/1000 | Loss: 0.00001903
Iteration 453/1000 | Loss: 0.00001903
Iteration 454/1000 | Loss: 0.00001903
Iteration 455/1000 | Loss: 0.00001903
Iteration 456/1000 | Loss: 0.00001902
Iteration 457/1000 | Loss: 0.00001902
Iteration 458/1000 | Loss: 0.00001902
Iteration 459/1000 | Loss: 0.00001902
Iteration 460/1000 | Loss: 0.00001902
Iteration 461/1000 | Loss: 0.00001901
Iteration 462/1000 | Loss: 0.00001901
Iteration 463/1000 | Loss: 0.00001901
Iteration 464/1000 | Loss: 0.00001900
Iteration 465/1000 | Loss: 0.00001900
Iteration 466/1000 | Loss: 0.00001900
Iteration 467/1000 | Loss: 0.00001900
Iteration 468/1000 | Loss: 0.00001899
Iteration 469/1000 | Loss: 0.00001899
Iteration 470/1000 | Loss: 0.00001899
Iteration 471/1000 | Loss: 0.00001899
Iteration 472/1000 | Loss: 0.00001899
Iteration 473/1000 | Loss: 0.00001899
Iteration 474/1000 | Loss: 0.00001899
Iteration 475/1000 | Loss: 0.00001899
Iteration 476/1000 | Loss: 0.00001899
Iteration 477/1000 | Loss: 0.00001899
Iteration 478/1000 | Loss: 0.00001899
Iteration 479/1000 | Loss: 0.00001899
Iteration 480/1000 | Loss: 0.00001899
Iteration 481/1000 | Loss: 0.00001899
Iteration 482/1000 | Loss: 0.00001899
Iteration 483/1000 | Loss: 0.00001899
Iteration 484/1000 | Loss: 0.00001899
Iteration 485/1000 | Loss: 0.00001899
Iteration 486/1000 | Loss: 0.00001899
Iteration 487/1000 | Loss: 0.00001899
Iteration 488/1000 | Loss: 0.00001899
Iteration 489/1000 | Loss: 0.00001899
Iteration 490/1000 | Loss: 0.00001899
Iteration 491/1000 | Loss: 0.00001899
Iteration 492/1000 | Loss: 0.00001899
Iteration 493/1000 | Loss: 0.00001899
Iteration 494/1000 | Loss: 0.00001899
Iteration 495/1000 | Loss: 0.00001899
Iteration 496/1000 | Loss: 0.00001899
Iteration 497/1000 | Loss: 0.00001899
Iteration 498/1000 | Loss: 0.00001899
Iteration 499/1000 | Loss: 0.00001899
Iteration 500/1000 | Loss: 0.00001899
Iteration 501/1000 | Loss: 0.00001899
Iteration 502/1000 | Loss: 0.00001899
Iteration 503/1000 | Loss: 0.00001899
Iteration 504/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 504. Stopping optimization.
Last 5 losses: [1.8992288460140117e-05, 1.8992288460140117e-05, 1.8992288460140117e-05, 1.8992288460140117e-05, 1.8992288460140117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8992288460140117e-05

Optimization complete. Final v2v error: 3.6333391666412354 mm

Highest mean error: 5.514132022857666 mm for frame 66

Lowest mean error: 3.0815865993499756 mm for frame 151

Saving results

Total time: 628.0491168498993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951508
Iteration 2/25 | Loss: 0.00141784
Iteration 3/25 | Loss: 0.00116414
Iteration 4/25 | Loss: 0.00112808
Iteration 5/25 | Loss: 0.00111551
Iteration 6/25 | Loss: 0.00111064
Iteration 7/25 | Loss: 0.00110974
Iteration 8/25 | Loss: 0.00110944
Iteration 9/25 | Loss: 0.00110933
Iteration 10/25 | Loss: 0.00110933
Iteration 11/25 | Loss: 0.00110933
Iteration 12/25 | Loss: 0.00110933
Iteration 13/25 | Loss: 0.00110933
Iteration 14/25 | Loss: 0.00110933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011093337088823318, 0.0011093337088823318, 0.0011093337088823318, 0.0011093337088823318, 0.0011093337088823318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011093337088823318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03871346
Iteration 2/25 | Loss: 0.00117371
Iteration 3/25 | Loss: 0.00117364
Iteration 4/25 | Loss: 0.00117364
Iteration 5/25 | Loss: 0.00117364
Iteration 6/25 | Loss: 0.00117364
Iteration 7/25 | Loss: 0.00117364
Iteration 8/25 | Loss: 0.00117364
Iteration 9/25 | Loss: 0.00117364
Iteration 10/25 | Loss: 0.00117364
Iteration 11/25 | Loss: 0.00117364
Iteration 12/25 | Loss: 0.00117364
Iteration 13/25 | Loss: 0.00117364
Iteration 14/25 | Loss: 0.00117364
Iteration 15/25 | Loss: 0.00117364
Iteration 16/25 | Loss: 0.00117364
Iteration 17/25 | Loss: 0.00117364
Iteration 18/25 | Loss: 0.00117364
Iteration 19/25 | Loss: 0.00117364
Iteration 20/25 | Loss: 0.00117364
Iteration 21/25 | Loss: 0.00117364
Iteration 22/25 | Loss: 0.00117364
Iteration 23/25 | Loss: 0.00117364
Iteration 24/25 | Loss: 0.00117364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011736385058611631, 0.0011736385058611631, 0.0011736385058611631, 0.0011736385058611631, 0.0011736385058611631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011736385058611631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117364
Iteration 2/1000 | Loss: 0.00005416
Iteration 3/1000 | Loss: 0.00003613
Iteration 4/1000 | Loss: 0.00003215
Iteration 5/1000 | Loss: 0.00003022
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00002896
Iteration 8/1000 | Loss: 0.00002872
Iteration 9/1000 | Loss: 0.00002843
Iteration 10/1000 | Loss: 0.00002831
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00002800
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00002795
Iteration 16/1000 | Loss: 0.00002793
Iteration 17/1000 | Loss: 0.00002791
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002784
Iteration 20/1000 | Loss: 0.00002784
Iteration 21/1000 | Loss: 0.00002784
Iteration 22/1000 | Loss: 0.00002784
Iteration 23/1000 | Loss: 0.00002784
Iteration 24/1000 | Loss: 0.00002784
Iteration 25/1000 | Loss: 0.00002784
Iteration 26/1000 | Loss: 0.00002784
Iteration 27/1000 | Loss: 0.00002784
Iteration 28/1000 | Loss: 0.00002783
Iteration 29/1000 | Loss: 0.00002783
Iteration 30/1000 | Loss: 0.00002783
Iteration 31/1000 | Loss: 0.00002782
Iteration 32/1000 | Loss: 0.00002782
Iteration 33/1000 | Loss: 0.00002782
Iteration 34/1000 | Loss: 0.00002781
Iteration 35/1000 | Loss: 0.00002781
Iteration 36/1000 | Loss: 0.00002781
Iteration 37/1000 | Loss: 0.00002781
Iteration 38/1000 | Loss: 0.00002781
Iteration 39/1000 | Loss: 0.00002781
Iteration 40/1000 | Loss: 0.00002781
Iteration 41/1000 | Loss: 0.00002781
Iteration 42/1000 | Loss: 0.00002780
Iteration 43/1000 | Loss: 0.00002780
Iteration 44/1000 | Loss: 0.00002780
Iteration 45/1000 | Loss: 0.00002780
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00002779
Iteration 48/1000 | Loss: 0.00002779
Iteration 49/1000 | Loss: 0.00002779
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002779
Iteration 53/1000 | Loss: 0.00002779
Iteration 54/1000 | Loss: 0.00002779
Iteration 55/1000 | Loss: 0.00002779
Iteration 56/1000 | Loss: 0.00002778
Iteration 57/1000 | Loss: 0.00002778
Iteration 58/1000 | Loss: 0.00002778
Iteration 59/1000 | Loss: 0.00002778
Iteration 60/1000 | Loss: 0.00002778
Iteration 61/1000 | Loss: 0.00002778
Iteration 62/1000 | Loss: 0.00002778
Iteration 63/1000 | Loss: 0.00002778
Iteration 64/1000 | Loss: 0.00002777
Iteration 65/1000 | Loss: 0.00002777
Iteration 66/1000 | Loss: 0.00002776
Iteration 67/1000 | Loss: 0.00002775
Iteration 68/1000 | Loss: 0.00002775
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002774
Iteration 71/1000 | Loss: 0.00002774
Iteration 72/1000 | Loss: 0.00002774
Iteration 73/1000 | Loss: 0.00002774
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002772
Iteration 79/1000 | Loss: 0.00002772
Iteration 80/1000 | Loss: 0.00002772
Iteration 81/1000 | Loss: 0.00002772
Iteration 82/1000 | Loss: 0.00002772
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002771
Iteration 85/1000 | Loss: 0.00002771
Iteration 86/1000 | Loss: 0.00002771
Iteration 87/1000 | Loss: 0.00002771
Iteration 88/1000 | Loss: 0.00002771
Iteration 89/1000 | Loss: 0.00002771
Iteration 90/1000 | Loss: 0.00002771
Iteration 91/1000 | Loss: 0.00002770
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002770
Iteration 95/1000 | Loss: 0.00002770
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.7699679776560515e-05, 2.7699679776560515e-05, 2.7699679776560515e-05, 2.7699679776560515e-05, 2.7699679776560515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7699679776560515e-05

Optimization complete. Final v2v error: 4.147502422332764 mm

Highest mean error: 4.807295799255371 mm for frame 181

Lowest mean error: 3.5646281242370605 mm for frame 143

Saving results

Total time: 42.94610142707825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540975
Iteration 2/25 | Loss: 0.00130665
Iteration 3/25 | Loss: 0.00107285
Iteration 4/25 | Loss: 0.00100309
Iteration 5/25 | Loss: 0.00099958
Iteration 6/25 | Loss: 0.00099624
Iteration 7/25 | Loss: 0.00099577
Iteration 8/25 | Loss: 0.00099559
Iteration 9/25 | Loss: 0.00099559
Iteration 10/25 | Loss: 0.00099559
Iteration 11/25 | Loss: 0.00099559
Iteration 12/25 | Loss: 0.00099559
Iteration 13/25 | Loss: 0.00099559
Iteration 14/25 | Loss: 0.00099558
Iteration 15/25 | Loss: 0.00099558
Iteration 16/25 | Loss: 0.00099558
Iteration 17/25 | Loss: 0.00099558
Iteration 18/25 | Loss: 0.00099558
Iteration 19/25 | Loss: 0.00099558
Iteration 20/25 | Loss: 0.00099558
Iteration 21/25 | Loss: 0.00099558
Iteration 22/25 | Loss: 0.00099558
Iteration 23/25 | Loss: 0.00099558
Iteration 24/25 | Loss: 0.00099557
Iteration 25/25 | Loss: 0.00099557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.14015341
Iteration 2/25 | Loss: 0.00121744
Iteration 3/25 | Loss: 0.00121742
Iteration 4/25 | Loss: 0.00121742
Iteration 5/25 | Loss: 0.00121742
Iteration 6/25 | Loss: 0.00121742
Iteration 7/25 | Loss: 0.00121742
Iteration 8/25 | Loss: 0.00121742
Iteration 9/25 | Loss: 0.00121742
Iteration 10/25 | Loss: 0.00121742
Iteration 11/25 | Loss: 0.00121742
Iteration 12/25 | Loss: 0.00121742
Iteration 13/25 | Loss: 0.00121742
Iteration 14/25 | Loss: 0.00121742
Iteration 15/25 | Loss: 0.00121742
Iteration 16/25 | Loss: 0.00121742
Iteration 17/25 | Loss: 0.00121742
Iteration 18/25 | Loss: 0.00121742
Iteration 19/25 | Loss: 0.00121742
Iteration 20/25 | Loss: 0.00121742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012174193980172276, 0.0012174193980172276, 0.0012174193980172276, 0.0012174193980172276, 0.0012174193980172276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012174193980172276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121742
Iteration 2/1000 | Loss: 0.00002449
Iteration 3/1000 | Loss: 0.00001736
Iteration 4/1000 | Loss: 0.00001446
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001236
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001178
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001119
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001091
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001088
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001087
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001082
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001075
Iteration 97/1000 | Loss: 0.00001075
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001075
Iteration 100/1000 | Loss: 0.00001075
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001074
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001073
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001073
Iteration 137/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.0726251275627874e-05, 1.0726251275627874e-05, 1.0726251275627874e-05, 1.0726251275627874e-05, 1.0726251275627874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0726251275627874e-05

Optimization complete. Final v2v error: 2.815164566040039 mm

Highest mean error: 3.4793787002563477 mm for frame 167

Lowest mean error: 2.4104645252227783 mm for frame 102

Saving results

Total time: 45.952646255493164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847506
Iteration 2/25 | Loss: 0.00119480
Iteration 3/25 | Loss: 0.00108201
Iteration 4/25 | Loss: 0.00104585
Iteration 5/25 | Loss: 0.00103535
Iteration 6/25 | Loss: 0.00103323
Iteration 7/25 | Loss: 0.00103307
Iteration 8/25 | Loss: 0.00103307
Iteration 9/25 | Loss: 0.00103307
Iteration 10/25 | Loss: 0.00103307
Iteration 11/25 | Loss: 0.00103307
Iteration 12/25 | Loss: 0.00103307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010330722434446216, 0.0010330722434446216, 0.0010330722434446216, 0.0010330722434446216, 0.0010330722434446216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010330722434446216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84342229
Iteration 2/25 | Loss: 0.00147348
Iteration 3/25 | Loss: 0.00147346
Iteration 4/25 | Loss: 0.00147345
Iteration 5/25 | Loss: 0.00147345
Iteration 6/25 | Loss: 0.00147345
Iteration 7/25 | Loss: 0.00147345
Iteration 8/25 | Loss: 0.00147345
Iteration 9/25 | Loss: 0.00147345
Iteration 10/25 | Loss: 0.00147345
Iteration 11/25 | Loss: 0.00147345
Iteration 12/25 | Loss: 0.00147345
Iteration 13/25 | Loss: 0.00147345
Iteration 14/25 | Loss: 0.00147345
Iteration 15/25 | Loss: 0.00147345
Iteration 16/25 | Loss: 0.00147345
Iteration 17/25 | Loss: 0.00147345
Iteration 18/25 | Loss: 0.00147345
Iteration 19/25 | Loss: 0.00147345
Iteration 20/25 | Loss: 0.00147345
Iteration 21/25 | Loss: 0.00147345
Iteration 22/25 | Loss: 0.00147345
Iteration 23/25 | Loss: 0.00147345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014734520809724927, 0.0014734520809724927, 0.0014734520809724927, 0.0014734520809724927, 0.0014734520809724927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014734520809724927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147345
Iteration 2/1000 | Loss: 0.00004729
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002321
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002059
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001866
Iteration 12/1000 | Loss: 0.00001860
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001857
Iteration 15/1000 | Loss: 0.00001853
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001808
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001805
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001803
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001798
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001796
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001794
Iteration 49/1000 | Loss: 0.00001791
Iteration 50/1000 | Loss: 0.00001791
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001790
Iteration 53/1000 | Loss: 0.00001790
Iteration 54/1000 | Loss: 0.00001789
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001787
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001785
Iteration 67/1000 | Loss: 0.00001785
Iteration 68/1000 | Loss: 0.00001785
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001783
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001780
Iteration 99/1000 | Loss: 0.00001780
Iteration 100/1000 | Loss: 0.00001780
Iteration 101/1000 | Loss: 0.00001780
Iteration 102/1000 | Loss: 0.00001780
Iteration 103/1000 | Loss: 0.00001779
Iteration 104/1000 | Loss: 0.00001779
Iteration 105/1000 | Loss: 0.00001779
Iteration 106/1000 | Loss: 0.00001779
Iteration 107/1000 | Loss: 0.00001778
Iteration 108/1000 | Loss: 0.00001778
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001777
Iteration 113/1000 | Loss: 0.00001777
Iteration 114/1000 | Loss: 0.00001777
Iteration 115/1000 | Loss: 0.00001776
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001775
Iteration 120/1000 | Loss: 0.00001775
Iteration 121/1000 | Loss: 0.00001775
Iteration 122/1000 | Loss: 0.00001775
Iteration 123/1000 | Loss: 0.00001775
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001775
Iteration 126/1000 | Loss: 0.00001775
Iteration 127/1000 | Loss: 0.00001775
Iteration 128/1000 | Loss: 0.00001775
Iteration 129/1000 | Loss: 0.00001775
Iteration 130/1000 | Loss: 0.00001775
Iteration 131/1000 | Loss: 0.00001775
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001774
Iteration 135/1000 | Loss: 0.00001774
Iteration 136/1000 | Loss: 0.00001774
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001774
Iteration 141/1000 | Loss: 0.00001773
Iteration 142/1000 | Loss: 0.00001773
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001773
Iteration 148/1000 | Loss: 0.00001773
Iteration 149/1000 | Loss: 0.00001773
Iteration 150/1000 | Loss: 0.00001773
Iteration 151/1000 | Loss: 0.00001773
Iteration 152/1000 | Loss: 0.00001773
Iteration 153/1000 | Loss: 0.00001773
Iteration 154/1000 | Loss: 0.00001772
Iteration 155/1000 | Loss: 0.00001772
Iteration 156/1000 | Loss: 0.00001772
Iteration 157/1000 | Loss: 0.00001772
Iteration 158/1000 | Loss: 0.00001772
Iteration 159/1000 | Loss: 0.00001772
Iteration 160/1000 | Loss: 0.00001772
Iteration 161/1000 | Loss: 0.00001772
Iteration 162/1000 | Loss: 0.00001771
Iteration 163/1000 | Loss: 0.00001771
Iteration 164/1000 | Loss: 0.00001771
Iteration 165/1000 | Loss: 0.00001771
Iteration 166/1000 | Loss: 0.00001771
Iteration 167/1000 | Loss: 0.00001771
Iteration 168/1000 | Loss: 0.00001771
Iteration 169/1000 | Loss: 0.00001771
Iteration 170/1000 | Loss: 0.00001771
Iteration 171/1000 | Loss: 0.00001771
Iteration 172/1000 | Loss: 0.00001770
Iteration 173/1000 | Loss: 0.00001770
Iteration 174/1000 | Loss: 0.00001770
Iteration 175/1000 | Loss: 0.00001770
Iteration 176/1000 | Loss: 0.00001770
Iteration 177/1000 | Loss: 0.00001770
Iteration 178/1000 | Loss: 0.00001770
Iteration 179/1000 | Loss: 0.00001770
Iteration 180/1000 | Loss: 0.00001769
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001769
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001769
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001769
Iteration 191/1000 | Loss: 0.00001768
Iteration 192/1000 | Loss: 0.00001768
Iteration 193/1000 | Loss: 0.00001768
Iteration 194/1000 | Loss: 0.00001768
Iteration 195/1000 | Loss: 0.00001768
Iteration 196/1000 | Loss: 0.00001768
Iteration 197/1000 | Loss: 0.00001768
Iteration 198/1000 | Loss: 0.00001767
Iteration 199/1000 | Loss: 0.00001767
Iteration 200/1000 | Loss: 0.00001767
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001767
Iteration 204/1000 | Loss: 0.00001767
Iteration 205/1000 | Loss: 0.00001767
Iteration 206/1000 | Loss: 0.00001767
Iteration 207/1000 | Loss: 0.00001767
Iteration 208/1000 | Loss: 0.00001767
Iteration 209/1000 | Loss: 0.00001767
Iteration 210/1000 | Loss: 0.00001767
Iteration 211/1000 | Loss: 0.00001767
Iteration 212/1000 | Loss: 0.00001767
Iteration 213/1000 | Loss: 0.00001767
Iteration 214/1000 | Loss: 0.00001767
Iteration 215/1000 | Loss: 0.00001767
Iteration 216/1000 | Loss: 0.00001766
Iteration 217/1000 | Loss: 0.00001766
Iteration 218/1000 | Loss: 0.00001766
Iteration 219/1000 | Loss: 0.00001766
Iteration 220/1000 | Loss: 0.00001766
Iteration 221/1000 | Loss: 0.00001766
Iteration 222/1000 | Loss: 0.00001766
Iteration 223/1000 | Loss: 0.00001766
Iteration 224/1000 | Loss: 0.00001766
Iteration 225/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.7661866877460852e-05, 1.7661866877460852e-05, 1.7661866877460852e-05, 1.7661866877460852e-05, 1.7661866877460852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7661866877460852e-05

Optimization complete. Final v2v error: 3.580688714981079 mm

Highest mean error: 4.075860023498535 mm for frame 49

Lowest mean error: 3.2460122108459473 mm for frame 130

Saving results

Total time: 44.21441650390625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406081
Iteration 2/25 | Loss: 0.00128835
Iteration 3/25 | Loss: 0.00124048
Iteration 4/25 | Loss: 0.00123181
Iteration 5/25 | Loss: 0.00122903
Iteration 6/25 | Loss: 0.00122856
Iteration 7/25 | Loss: 0.00122856
Iteration 8/25 | Loss: 0.00122856
Iteration 9/25 | Loss: 0.00122856
Iteration 10/25 | Loss: 0.00122856
Iteration 11/25 | Loss: 0.00122856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012285555712878704, 0.0012285555712878704, 0.0012285555712878704, 0.0012285555712878704, 0.0012285555712878704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012285555712878704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95183635
Iteration 2/25 | Loss: 0.00081941
Iteration 3/25 | Loss: 0.00081940
Iteration 4/25 | Loss: 0.00081940
Iteration 5/25 | Loss: 0.00081940
Iteration 6/25 | Loss: 0.00081940
Iteration 7/25 | Loss: 0.00081940
Iteration 8/25 | Loss: 0.00081940
Iteration 9/25 | Loss: 0.00081940
Iteration 10/25 | Loss: 0.00081940
Iteration 11/25 | Loss: 0.00081940
Iteration 12/25 | Loss: 0.00081940
Iteration 13/25 | Loss: 0.00081940
Iteration 14/25 | Loss: 0.00081940
Iteration 15/25 | Loss: 0.00081940
Iteration 16/25 | Loss: 0.00081940
Iteration 17/25 | Loss: 0.00081940
Iteration 18/25 | Loss: 0.00081940
Iteration 19/25 | Loss: 0.00081940
Iteration 20/25 | Loss: 0.00081940
Iteration 21/25 | Loss: 0.00081940
Iteration 22/25 | Loss: 0.00081940
Iteration 23/25 | Loss: 0.00081940
Iteration 24/25 | Loss: 0.00081940
Iteration 25/25 | Loss: 0.00081940

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081940
Iteration 2/1000 | Loss: 0.00003109
Iteration 3/1000 | Loss: 0.00001932
Iteration 4/1000 | Loss: 0.00001663
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001385
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001249
Iteration 60/1000 | Loss: 0.00001249
Iteration 61/1000 | Loss: 0.00001249
Iteration 62/1000 | Loss: 0.00001249
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001248
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001237
Iteration 98/1000 | Loss: 0.00001237
Iteration 99/1000 | Loss: 0.00001237
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001235
Iteration 111/1000 | Loss: 0.00001235
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001233
Iteration 123/1000 | Loss: 0.00001233
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001224
Iteration 154/1000 | Loss: 0.00001224
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001223
Iteration 159/1000 | Loss: 0.00001223
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001220
Iteration 179/1000 | Loss: 0.00001220
Iteration 180/1000 | Loss: 0.00001220
Iteration 181/1000 | Loss: 0.00001220
Iteration 182/1000 | Loss: 0.00001220
Iteration 183/1000 | Loss: 0.00001220
Iteration 184/1000 | Loss: 0.00001220
Iteration 185/1000 | Loss: 0.00001220
Iteration 186/1000 | Loss: 0.00001220
Iteration 187/1000 | Loss: 0.00001220
Iteration 188/1000 | Loss: 0.00001220
Iteration 189/1000 | Loss: 0.00001220
Iteration 190/1000 | Loss: 0.00001220
Iteration 191/1000 | Loss: 0.00001220
Iteration 192/1000 | Loss: 0.00001220
Iteration 193/1000 | Loss: 0.00001220
Iteration 194/1000 | Loss: 0.00001220
Iteration 195/1000 | Loss: 0.00001220
Iteration 196/1000 | Loss: 0.00001220
Iteration 197/1000 | Loss: 0.00001220
Iteration 198/1000 | Loss: 0.00001220
Iteration 199/1000 | Loss: 0.00001220
Iteration 200/1000 | Loss: 0.00001220
Iteration 201/1000 | Loss: 0.00001220
Iteration 202/1000 | Loss: 0.00001220
Iteration 203/1000 | Loss: 0.00001220
Iteration 204/1000 | Loss: 0.00001220
Iteration 205/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.2195703675388359e-05, 1.2195703675388359e-05, 1.2195703675388359e-05, 1.2195703675388359e-05, 1.2195703675388359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2195703675388359e-05

Optimization complete. Final v2v error: 2.9888718128204346 mm

Highest mean error: 4.229124546051025 mm for frame 78

Lowest mean error: 2.8095157146453857 mm for frame 101

Saving results

Total time: 43.86915040016174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825750
Iteration 2/25 | Loss: 0.00187365
Iteration 3/25 | Loss: 0.00150310
Iteration 4/25 | Loss: 0.00146612
Iteration 5/25 | Loss: 0.00144514
Iteration 6/25 | Loss: 0.00147006
Iteration 7/25 | Loss: 0.00142112
Iteration 8/25 | Loss: 0.00140276
Iteration 9/25 | Loss: 0.00139493
Iteration 10/25 | Loss: 0.00139277
Iteration 11/25 | Loss: 0.00138057
Iteration 12/25 | Loss: 0.00138245
Iteration 13/25 | Loss: 0.00138324
Iteration 14/25 | Loss: 0.00138230
Iteration 15/25 | Loss: 0.00138831
Iteration 16/25 | Loss: 0.00138638
Iteration 17/25 | Loss: 0.00137269
Iteration 18/25 | Loss: 0.00137601
Iteration 19/25 | Loss: 0.00136997
Iteration 20/25 | Loss: 0.00136883
Iteration 21/25 | Loss: 0.00136880
Iteration 22/25 | Loss: 0.00136880
Iteration 23/25 | Loss: 0.00136880
Iteration 24/25 | Loss: 0.00136880
Iteration 25/25 | Loss: 0.00136880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70634580
Iteration 2/25 | Loss: 0.00089684
Iteration 3/25 | Loss: 0.00084904
Iteration 4/25 | Loss: 0.00084904
Iteration 5/25 | Loss: 0.00084904
Iteration 6/25 | Loss: 0.00084904
Iteration 7/25 | Loss: 0.00084904
Iteration 8/25 | Loss: 0.00084904
Iteration 9/25 | Loss: 0.00084904
Iteration 10/25 | Loss: 0.00084904
Iteration 11/25 | Loss: 0.00084904
Iteration 12/25 | Loss: 0.00084904
Iteration 13/25 | Loss: 0.00084904
Iteration 14/25 | Loss: 0.00084904
Iteration 15/25 | Loss: 0.00084904
Iteration 16/25 | Loss: 0.00084904
Iteration 17/25 | Loss: 0.00084904
Iteration 18/25 | Loss: 0.00084904
Iteration 19/25 | Loss: 0.00084904
Iteration 20/25 | Loss: 0.00084904
Iteration 21/25 | Loss: 0.00084904
Iteration 22/25 | Loss: 0.00084904
Iteration 23/25 | Loss: 0.00084904
Iteration 24/25 | Loss: 0.00084904
Iteration 25/25 | Loss: 0.00084904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084904
Iteration 2/1000 | Loss: 0.00007955
Iteration 3/1000 | Loss: 0.00002786
Iteration 4/1000 | Loss: 0.00004439
Iteration 5/1000 | Loss: 0.00006519
Iteration 6/1000 | Loss: 0.00002359
Iteration 7/1000 | Loss: 0.00004284
Iteration 8/1000 | Loss: 0.00002246
Iteration 9/1000 | Loss: 0.00002187
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00002121
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002087
Iteration 14/1000 | Loss: 0.00002086
Iteration 15/1000 | Loss: 0.00002084
Iteration 16/1000 | Loss: 0.00002084
Iteration 17/1000 | Loss: 0.00002083
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002077
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002076
Iteration 23/1000 | Loss: 0.00002076
Iteration 24/1000 | Loss: 0.00002075
Iteration 25/1000 | Loss: 0.00002075
Iteration 26/1000 | Loss: 0.00002075
Iteration 27/1000 | Loss: 0.00002074
Iteration 28/1000 | Loss: 0.00002074
Iteration 29/1000 | Loss: 0.00002074
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002071
Iteration 33/1000 | Loss: 0.00002070
Iteration 34/1000 | Loss: 0.00002067
Iteration 35/1000 | Loss: 0.00002066
Iteration 36/1000 | Loss: 0.00002065
Iteration 37/1000 | Loss: 0.00002065
Iteration 38/1000 | Loss: 0.00002065
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00002065
Iteration 41/1000 | Loss: 0.00002065
Iteration 42/1000 | Loss: 0.00002065
Iteration 43/1000 | Loss: 0.00002065
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002065
Iteration 46/1000 | Loss: 0.00002065
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002064
Iteration 51/1000 | Loss: 0.00002064
Iteration 52/1000 | Loss: 0.00003701
Iteration 53/1000 | Loss: 0.00002086
Iteration 54/1000 | Loss: 0.00002059
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002056
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002055
Iteration 69/1000 | Loss: 0.00002055
Iteration 70/1000 | Loss: 0.00002055
Iteration 71/1000 | Loss: 0.00002055
Iteration 72/1000 | Loss: 0.00002055
Iteration 73/1000 | Loss: 0.00002055
Iteration 74/1000 | Loss: 0.00002055
Iteration 75/1000 | Loss: 0.00002055
Iteration 76/1000 | Loss: 0.00002055
Iteration 77/1000 | Loss: 0.00002055
Iteration 78/1000 | Loss: 0.00002054
Iteration 79/1000 | Loss: 0.00002054
Iteration 80/1000 | Loss: 0.00002054
Iteration 81/1000 | Loss: 0.00002054
Iteration 82/1000 | Loss: 0.00002054
Iteration 83/1000 | Loss: 0.00002054
Iteration 84/1000 | Loss: 0.00002054
Iteration 85/1000 | Loss: 0.00002054
Iteration 86/1000 | Loss: 0.00002054
Iteration 87/1000 | Loss: 0.00002054
Iteration 88/1000 | Loss: 0.00002054
Iteration 89/1000 | Loss: 0.00003264
Iteration 90/1000 | Loss: 0.00002056
Iteration 91/1000 | Loss: 0.00002054
Iteration 92/1000 | Loss: 0.00002054
Iteration 93/1000 | Loss: 0.00002053
Iteration 94/1000 | Loss: 0.00002053
Iteration 95/1000 | Loss: 0.00002052
Iteration 96/1000 | Loss: 0.00002052
Iteration 97/1000 | Loss: 0.00002052
Iteration 98/1000 | Loss: 0.00002052
Iteration 99/1000 | Loss: 0.00002052
Iteration 100/1000 | Loss: 0.00002052
Iteration 101/1000 | Loss: 0.00002051
Iteration 102/1000 | Loss: 0.00002051
Iteration 103/1000 | Loss: 0.00002051
Iteration 104/1000 | Loss: 0.00002051
Iteration 105/1000 | Loss: 0.00002051
Iteration 106/1000 | Loss: 0.00002051
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002051
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002050
Iteration 112/1000 | Loss: 0.00002050
Iteration 113/1000 | Loss: 0.00002050
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002049
Iteration 118/1000 | Loss: 0.00002049
Iteration 119/1000 | Loss: 0.00002049
Iteration 120/1000 | Loss: 0.00002048
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002047
Iteration 125/1000 | Loss: 0.00002047
Iteration 126/1000 | Loss: 0.00002047
Iteration 127/1000 | Loss: 0.00002047
Iteration 128/1000 | Loss: 0.00002047
Iteration 129/1000 | Loss: 0.00002047
Iteration 130/1000 | Loss: 0.00002046
Iteration 131/1000 | Loss: 0.00002046
Iteration 132/1000 | Loss: 0.00002046
Iteration 133/1000 | Loss: 0.00002046
Iteration 134/1000 | Loss: 0.00002046
Iteration 135/1000 | Loss: 0.00002046
Iteration 136/1000 | Loss: 0.00002046
Iteration 137/1000 | Loss: 0.00002046
Iteration 138/1000 | Loss: 0.00002046
Iteration 139/1000 | Loss: 0.00002046
Iteration 140/1000 | Loss: 0.00002046
Iteration 141/1000 | Loss: 0.00002046
Iteration 142/1000 | Loss: 0.00002046
Iteration 143/1000 | Loss: 0.00002046
Iteration 144/1000 | Loss: 0.00002046
Iteration 145/1000 | Loss: 0.00002045
Iteration 146/1000 | Loss: 0.00002045
Iteration 147/1000 | Loss: 0.00002045
Iteration 148/1000 | Loss: 0.00002045
Iteration 149/1000 | Loss: 0.00002045
Iteration 150/1000 | Loss: 0.00002045
Iteration 151/1000 | Loss: 0.00002045
Iteration 152/1000 | Loss: 0.00002045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.045461405941751e-05, 2.045461405941751e-05, 2.045461405941751e-05, 2.045461405941751e-05, 2.045461405941751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.045461405941751e-05

Optimization complete. Final v2v error: 3.773540496826172 mm

Highest mean error: 4.235443115234375 mm for frame 193

Lowest mean error: 3.3580663204193115 mm for frame 238

Saving results

Total time: 78.93346047401428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431091
Iteration 2/25 | Loss: 0.00134262
Iteration 3/25 | Loss: 0.00126858
Iteration 4/25 | Loss: 0.00125310
Iteration 5/25 | Loss: 0.00125025
Iteration 6/25 | Loss: 0.00124987
Iteration 7/25 | Loss: 0.00124987
Iteration 8/25 | Loss: 0.00124987
Iteration 9/25 | Loss: 0.00124987
Iteration 10/25 | Loss: 0.00124987
Iteration 11/25 | Loss: 0.00124987
Iteration 12/25 | Loss: 0.00124987
Iteration 13/25 | Loss: 0.00124987
Iteration 14/25 | Loss: 0.00124987
Iteration 15/25 | Loss: 0.00124987
Iteration 16/25 | Loss: 0.00124987
Iteration 17/25 | Loss: 0.00124987
Iteration 18/25 | Loss: 0.00124987
Iteration 19/25 | Loss: 0.00124987
Iteration 20/25 | Loss: 0.00124987
Iteration 21/25 | Loss: 0.00124987
Iteration 22/25 | Loss: 0.00124987
Iteration 23/25 | Loss: 0.00124987
Iteration 24/25 | Loss: 0.00124987
Iteration 25/25 | Loss: 0.00124987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42257929
Iteration 2/25 | Loss: 0.00078403
Iteration 3/25 | Loss: 0.00078403
Iteration 4/25 | Loss: 0.00078403
Iteration 5/25 | Loss: 0.00078403
Iteration 6/25 | Loss: 0.00078403
Iteration 7/25 | Loss: 0.00078403
Iteration 8/25 | Loss: 0.00078403
Iteration 9/25 | Loss: 0.00078403
Iteration 10/25 | Loss: 0.00078403
Iteration 11/25 | Loss: 0.00078403
Iteration 12/25 | Loss: 0.00078403
Iteration 13/25 | Loss: 0.00078403
Iteration 14/25 | Loss: 0.00078403
Iteration 15/25 | Loss: 0.00078403
Iteration 16/25 | Loss: 0.00078403
Iteration 17/25 | Loss: 0.00078403
Iteration 18/25 | Loss: 0.00078403
Iteration 19/25 | Loss: 0.00078403
Iteration 20/25 | Loss: 0.00078403
Iteration 21/25 | Loss: 0.00078403
Iteration 22/25 | Loss: 0.00078403
Iteration 23/25 | Loss: 0.00078403
Iteration 24/25 | Loss: 0.00078403
Iteration 25/25 | Loss: 0.00078403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078403
Iteration 2/1000 | Loss: 0.00003067
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001891
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00001702
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001589
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001505
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001483
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001471
Iteration 125/1000 | Loss: 0.00001471
Iteration 126/1000 | Loss: 0.00001471
Iteration 127/1000 | Loss: 0.00001471
Iteration 128/1000 | Loss: 0.00001471
Iteration 129/1000 | Loss: 0.00001471
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001470
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001470
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.4702914086228702e-05, 1.4702914086228702e-05, 1.4702914086228702e-05, 1.4702914086228702e-05, 1.4702914086228702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4702914086228702e-05

Optimization complete. Final v2v error: 3.289156913757324 mm

Highest mean error: 3.5311710834503174 mm for frame 104

Lowest mean error: 3.135676622390747 mm for frame 158

Saving results

Total time: 38.8359854221344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871265
Iteration 2/25 | Loss: 0.00134577
Iteration 3/25 | Loss: 0.00127712
Iteration 4/25 | Loss: 0.00126772
Iteration 5/25 | Loss: 0.00126505
Iteration 6/25 | Loss: 0.00126436
Iteration 7/25 | Loss: 0.00126436
Iteration 8/25 | Loss: 0.00126436
Iteration 9/25 | Loss: 0.00126436
Iteration 10/25 | Loss: 0.00126436
Iteration 11/25 | Loss: 0.00126436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012643630616366863, 0.0012643630616366863, 0.0012643630616366863, 0.0012643630616366863, 0.0012643630616366863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012643630616366863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99932027
Iteration 2/25 | Loss: 0.00088465
Iteration 3/25 | Loss: 0.00088464
Iteration 4/25 | Loss: 0.00088464
Iteration 5/25 | Loss: 0.00088464
Iteration 6/25 | Loss: 0.00088464
Iteration 7/25 | Loss: 0.00088464
Iteration 8/25 | Loss: 0.00088464
Iteration 9/25 | Loss: 0.00088464
Iteration 10/25 | Loss: 0.00088464
Iteration 11/25 | Loss: 0.00088464
Iteration 12/25 | Loss: 0.00088464
Iteration 13/25 | Loss: 0.00088464
Iteration 14/25 | Loss: 0.00088464
Iteration 15/25 | Loss: 0.00088464
Iteration 16/25 | Loss: 0.00088464
Iteration 17/25 | Loss: 0.00088464
Iteration 18/25 | Loss: 0.00088464
Iteration 19/25 | Loss: 0.00088464
Iteration 20/25 | Loss: 0.00088464
Iteration 21/25 | Loss: 0.00088464
Iteration 22/25 | Loss: 0.00088464
Iteration 23/25 | Loss: 0.00088464
Iteration 24/25 | Loss: 0.00088464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008846379932947457, 0.0008846379932947457, 0.0008846379932947457, 0.0008846379932947457, 0.0008846379932947457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008846379932947457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088464
Iteration 2/1000 | Loss: 0.00003378
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001228
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00001212
Iteration 114/1000 | Loss: 0.00001212
Iteration 115/1000 | Loss: 0.00001212
Iteration 116/1000 | Loss: 0.00001212
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001212
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001203
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001202
Iteration 149/1000 | Loss: 0.00001202
Iteration 150/1000 | Loss: 0.00001202
Iteration 151/1000 | Loss: 0.00001202
Iteration 152/1000 | Loss: 0.00001202
Iteration 153/1000 | Loss: 0.00001202
Iteration 154/1000 | Loss: 0.00001202
Iteration 155/1000 | Loss: 0.00001202
Iteration 156/1000 | Loss: 0.00001202
Iteration 157/1000 | Loss: 0.00001202
Iteration 158/1000 | Loss: 0.00001202
Iteration 159/1000 | Loss: 0.00001202
Iteration 160/1000 | Loss: 0.00001201
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001201
Iteration 167/1000 | Loss: 0.00001201
Iteration 168/1000 | Loss: 0.00001200
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001200
Iteration 171/1000 | Loss: 0.00001200
Iteration 172/1000 | Loss: 0.00001200
Iteration 173/1000 | Loss: 0.00001200
Iteration 174/1000 | Loss: 0.00001200
Iteration 175/1000 | Loss: 0.00001200
Iteration 176/1000 | Loss: 0.00001200
Iteration 177/1000 | Loss: 0.00001200
Iteration 178/1000 | Loss: 0.00001200
Iteration 179/1000 | Loss: 0.00001200
Iteration 180/1000 | Loss: 0.00001199
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001198
Iteration 186/1000 | Loss: 0.00001198
Iteration 187/1000 | Loss: 0.00001198
Iteration 188/1000 | Loss: 0.00001198
Iteration 189/1000 | Loss: 0.00001198
Iteration 190/1000 | Loss: 0.00001198
Iteration 191/1000 | Loss: 0.00001198
Iteration 192/1000 | Loss: 0.00001198
Iteration 193/1000 | Loss: 0.00001198
Iteration 194/1000 | Loss: 0.00001198
Iteration 195/1000 | Loss: 0.00001198
Iteration 196/1000 | Loss: 0.00001198
Iteration 197/1000 | Loss: 0.00001198
Iteration 198/1000 | Loss: 0.00001198
Iteration 199/1000 | Loss: 0.00001197
Iteration 200/1000 | Loss: 0.00001197
Iteration 201/1000 | Loss: 0.00001197
Iteration 202/1000 | Loss: 0.00001197
Iteration 203/1000 | Loss: 0.00001197
Iteration 204/1000 | Loss: 0.00001197
Iteration 205/1000 | Loss: 0.00001197
Iteration 206/1000 | Loss: 0.00001197
Iteration 207/1000 | Loss: 0.00001197
Iteration 208/1000 | Loss: 0.00001197
Iteration 209/1000 | Loss: 0.00001197
Iteration 210/1000 | Loss: 0.00001197
Iteration 211/1000 | Loss: 0.00001197
Iteration 212/1000 | Loss: 0.00001197
Iteration 213/1000 | Loss: 0.00001197
Iteration 214/1000 | Loss: 0.00001196
Iteration 215/1000 | Loss: 0.00001196
Iteration 216/1000 | Loss: 0.00001196
Iteration 217/1000 | Loss: 0.00001196
Iteration 218/1000 | Loss: 0.00001196
Iteration 219/1000 | Loss: 0.00001196
Iteration 220/1000 | Loss: 0.00001196
Iteration 221/1000 | Loss: 0.00001196
Iteration 222/1000 | Loss: 0.00001196
Iteration 223/1000 | Loss: 0.00001196
Iteration 224/1000 | Loss: 0.00001196
Iteration 225/1000 | Loss: 0.00001196
Iteration 226/1000 | Loss: 0.00001196
Iteration 227/1000 | Loss: 0.00001196
Iteration 228/1000 | Loss: 0.00001196
Iteration 229/1000 | Loss: 0.00001196
Iteration 230/1000 | Loss: 0.00001196
Iteration 231/1000 | Loss: 0.00001196
Iteration 232/1000 | Loss: 0.00001196
Iteration 233/1000 | Loss: 0.00001196
Iteration 234/1000 | Loss: 0.00001196
Iteration 235/1000 | Loss: 0.00001196
Iteration 236/1000 | Loss: 0.00001196
Iteration 237/1000 | Loss: 0.00001196
Iteration 238/1000 | Loss: 0.00001196
Iteration 239/1000 | Loss: 0.00001196
Iteration 240/1000 | Loss: 0.00001196
Iteration 241/1000 | Loss: 0.00001196
Iteration 242/1000 | Loss: 0.00001196
Iteration 243/1000 | Loss: 0.00001196
Iteration 244/1000 | Loss: 0.00001196
Iteration 245/1000 | Loss: 0.00001196
Iteration 246/1000 | Loss: 0.00001196
Iteration 247/1000 | Loss: 0.00001196
Iteration 248/1000 | Loss: 0.00001196
Iteration 249/1000 | Loss: 0.00001196
Iteration 250/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.1956622074649204e-05, 1.1956622074649204e-05, 1.1956622074649204e-05, 1.1956622074649204e-05, 1.1956622074649204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1956622074649204e-05

Optimization complete. Final v2v error: 2.9690961837768555 mm

Highest mean error: 3.4348456859588623 mm for frame 118

Lowest mean error: 2.7517459392547607 mm for frame 130

Saving results

Total time: 42.24396204948425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792678
Iteration 2/25 | Loss: 0.00131376
Iteration 3/25 | Loss: 0.00124489
Iteration 4/25 | Loss: 0.00123887
Iteration 5/25 | Loss: 0.00123739
Iteration 6/25 | Loss: 0.00123739
Iteration 7/25 | Loss: 0.00123739
Iteration 8/25 | Loss: 0.00123739
Iteration 9/25 | Loss: 0.00123701
Iteration 10/25 | Loss: 0.00123701
Iteration 11/25 | Loss: 0.00123701
Iteration 12/25 | Loss: 0.00123701
Iteration 13/25 | Loss: 0.00123701
Iteration 14/25 | Loss: 0.00123701
Iteration 15/25 | Loss: 0.00123701
Iteration 16/25 | Loss: 0.00123701
Iteration 17/25 | Loss: 0.00123701
Iteration 18/25 | Loss: 0.00123701
Iteration 19/25 | Loss: 0.00123701
Iteration 20/25 | Loss: 0.00123701
Iteration 21/25 | Loss: 0.00123701
Iteration 22/25 | Loss: 0.00123701
Iteration 23/25 | Loss: 0.00123701
Iteration 24/25 | Loss: 0.00123701
Iteration 25/25 | Loss: 0.00123701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42936909
Iteration 2/25 | Loss: 0.00084622
Iteration 3/25 | Loss: 0.00084621
Iteration 4/25 | Loss: 0.00084621
Iteration 5/25 | Loss: 0.00084621
Iteration 6/25 | Loss: 0.00084621
Iteration 7/25 | Loss: 0.00084620
Iteration 8/25 | Loss: 0.00084620
Iteration 9/25 | Loss: 0.00084620
Iteration 10/25 | Loss: 0.00084620
Iteration 11/25 | Loss: 0.00084620
Iteration 12/25 | Loss: 0.00084620
Iteration 13/25 | Loss: 0.00084620
Iteration 14/25 | Loss: 0.00084620
Iteration 15/25 | Loss: 0.00084620
Iteration 16/25 | Loss: 0.00084620
Iteration 17/25 | Loss: 0.00084620
Iteration 18/25 | Loss: 0.00084620
Iteration 19/25 | Loss: 0.00084620
Iteration 20/25 | Loss: 0.00084620
Iteration 21/25 | Loss: 0.00084620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008462037076242268, 0.0008462037076242268, 0.0008462037076242268, 0.0008462037076242268, 0.0008462037076242268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008462037076242268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084620
Iteration 2/1000 | Loss: 0.00002302
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001248
Iteration 7/1000 | Loss: 0.00001201
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001119
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001113
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001112
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001104
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001103
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001101
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001100
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.0961812222376466e-05, 1.0961812222376466e-05, 1.0961812222376466e-05, 1.0961812222376466e-05, 1.0961812222376466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0961812222376466e-05

Optimization complete. Final v2v error: 2.851198434829712 mm

Highest mean error: 2.9729905128479004 mm for frame 109

Lowest mean error: 2.7236573696136475 mm for frame 156

Saving results

Total time: 31.819854497909546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032838
Iteration 2/25 | Loss: 0.00190628
Iteration 3/25 | Loss: 0.00147046
Iteration 4/25 | Loss: 0.00141176
Iteration 5/25 | Loss: 0.00139060
Iteration 6/25 | Loss: 0.00138566
Iteration 7/25 | Loss: 0.00138454
Iteration 8/25 | Loss: 0.00138423
Iteration 9/25 | Loss: 0.00138423
Iteration 10/25 | Loss: 0.00138423
Iteration 11/25 | Loss: 0.00138423
Iteration 12/25 | Loss: 0.00138423
Iteration 13/25 | Loss: 0.00138423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013842289336025715, 0.0013842289336025715, 0.0013842289336025715, 0.0013842289336025715, 0.0013842289336025715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013842289336025715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91191155
Iteration 2/25 | Loss: 0.00075421
Iteration 3/25 | Loss: 0.00075421
Iteration 4/25 | Loss: 0.00075421
Iteration 5/25 | Loss: 0.00075421
Iteration 6/25 | Loss: 0.00075421
Iteration 7/25 | Loss: 0.00075421
Iteration 8/25 | Loss: 0.00075421
Iteration 9/25 | Loss: 0.00075421
Iteration 10/25 | Loss: 0.00075421
Iteration 11/25 | Loss: 0.00075421
Iteration 12/25 | Loss: 0.00075421
Iteration 13/25 | Loss: 0.00075421
Iteration 14/25 | Loss: 0.00075421
Iteration 15/25 | Loss: 0.00075421
Iteration 16/25 | Loss: 0.00075421
Iteration 17/25 | Loss: 0.00075421
Iteration 18/25 | Loss: 0.00075421
Iteration 19/25 | Loss: 0.00075421
Iteration 20/25 | Loss: 0.00075421
Iteration 21/25 | Loss: 0.00075421
Iteration 22/25 | Loss: 0.00075421
Iteration 23/25 | Loss: 0.00075421
Iteration 24/25 | Loss: 0.00075421
Iteration 25/25 | Loss: 0.00075421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075421
Iteration 2/1000 | Loss: 0.00006901
Iteration 3/1000 | Loss: 0.00004731
Iteration 4/1000 | Loss: 0.00003740
Iteration 5/1000 | Loss: 0.00003499
Iteration 6/1000 | Loss: 0.00003366
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003131
Iteration 9/1000 | Loss: 0.00003059
Iteration 10/1000 | Loss: 0.00003013
Iteration 11/1000 | Loss: 0.00002971
Iteration 12/1000 | Loss: 0.00002944
Iteration 13/1000 | Loss: 0.00002911
Iteration 14/1000 | Loss: 0.00002889
Iteration 15/1000 | Loss: 0.00002863
Iteration 16/1000 | Loss: 0.00002842
Iteration 17/1000 | Loss: 0.00002825
Iteration 18/1000 | Loss: 0.00002814
Iteration 19/1000 | Loss: 0.00002807
Iteration 20/1000 | Loss: 0.00002803
Iteration 21/1000 | Loss: 0.00002802
Iteration 22/1000 | Loss: 0.00002801
Iteration 23/1000 | Loss: 0.00002798
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002793
Iteration 27/1000 | Loss: 0.00002791
Iteration 28/1000 | Loss: 0.00002788
Iteration 29/1000 | Loss: 0.00002788
Iteration 30/1000 | Loss: 0.00002788
Iteration 31/1000 | Loss: 0.00002788
Iteration 32/1000 | Loss: 0.00002787
Iteration 33/1000 | Loss: 0.00002787
Iteration 34/1000 | Loss: 0.00002787
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00002786
Iteration 37/1000 | Loss: 0.00002786
Iteration 38/1000 | Loss: 0.00002781
Iteration 39/1000 | Loss: 0.00002779
Iteration 40/1000 | Loss: 0.00002776
Iteration 41/1000 | Loss: 0.00002776
Iteration 42/1000 | Loss: 0.00002773
Iteration 43/1000 | Loss: 0.00002772
Iteration 44/1000 | Loss: 0.00002772
Iteration 45/1000 | Loss: 0.00002771
Iteration 46/1000 | Loss: 0.00002771
Iteration 47/1000 | Loss: 0.00002770
Iteration 48/1000 | Loss: 0.00002769
Iteration 49/1000 | Loss: 0.00002769
Iteration 50/1000 | Loss: 0.00002769
Iteration 51/1000 | Loss: 0.00002768
Iteration 52/1000 | Loss: 0.00002768
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002766
Iteration 55/1000 | Loss: 0.00002766
Iteration 56/1000 | Loss: 0.00002766
Iteration 57/1000 | Loss: 0.00002766
Iteration 58/1000 | Loss: 0.00002766
Iteration 59/1000 | Loss: 0.00002766
Iteration 60/1000 | Loss: 0.00002766
Iteration 61/1000 | Loss: 0.00002766
Iteration 62/1000 | Loss: 0.00002765
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00002765
Iteration 65/1000 | Loss: 0.00002765
Iteration 66/1000 | Loss: 0.00002764
Iteration 67/1000 | Loss: 0.00002764
Iteration 68/1000 | Loss: 0.00002764
Iteration 69/1000 | Loss: 0.00002763
Iteration 70/1000 | Loss: 0.00002763
Iteration 71/1000 | Loss: 0.00002763
Iteration 72/1000 | Loss: 0.00002763
Iteration 73/1000 | Loss: 0.00002763
Iteration 74/1000 | Loss: 0.00002762
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00002762
Iteration 77/1000 | Loss: 0.00002762
Iteration 78/1000 | Loss: 0.00002761
Iteration 79/1000 | Loss: 0.00002761
Iteration 80/1000 | Loss: 0.00002761
Iteration 81/1000 | Loss: 0.00002761
Iteration 82/1000 | Loss: 0.00002760
Iteration 83/1000 | Loss: 0.00002760
Iteration 84/1000 | Loss: 0.00002760
Iteration 85/1000 | Loss: 0.00002760
Iteration 86/1000 | Loss: 0.00002759
Iteration 87/1000 | Loss: 0.00002759
Iteration 88/1000 | Loss: 0.00002759
Iteration 89/1000 | Loss: 0.00002759
Iteration 90/1000 | Loss: 0.00002758
Iteration 91/1000 | Loss: 0.00002758
Iteration 92/1000 | Loss: 0.00002758
Iteration 93/1000 | Loss: 0.00002757
Iteration 94/1000 | Loss: 0.00002757
Iteration 95/1000 | Loss: 0.00002757
Iteration 96/1000 | Loss: 0.00002756
Iteration 97/1000 | Loss: 0.00002756
Iteration 98/1000 | Loss: 0.00002756
Iteration 99/1000 | Loss: 0.00002756
Iteration 100/1000 | Loss: 0.00002756
Iteration 101/1000 | Loss: 0.00002756
Iteration 102/1000 | Loss: 0.00002755
Iteration 103/1000 | Loss: 0.00002755
Iteration 104/1000 | Loss: 0.00002755
Iteration 105/1000 | Loss: 0.00002755
Iteration 106/1000 | Loss: 0.00002755
Iteration 107/1000 | Loss: 0.00002755
Iteration 108/1000 | Loss: 0.00002754
Iteration 109/1000 | Loss: 0.00002754
Iteration 110/1000 | Loss: 0.00002754
Iteration 111/1000 | Loss: 0.00002754
Iteration 112/1000 | Loss: 0.00002754
Iteration 113/1000 | Loss: 0.00002754
Iteration 114/1000 | Loss: 0.00002754
Iteration 115/1000 | Loss: 0.00002754
Iteration 116/1000 | Loss: 0.00002753
Iteration 117/1000 | Loss: 0.00002753
Iteration 118/1000 | Loss: 0.00002753
Iteration 119/1000 | Loss: 0.00002753
Iteration 120/1000 | Loss: 0.00002753
Iteration 121/1000 | Loss: 0.00002753
Iteration 122/1000 | Loss: 0.00002753
Iteration 123/1000 | Loss: 0.00002753
Iteration 124/1000 | Loss: 0.00002752
Iteration 125/1000 | Loss: 0.00002752
Iteration 126/1000 | Loss: 0.00002752
Iteration 127/1000 | Loss: 0.00002752
Iteration 128/1000 | Loss: 0.00002752
Iteration 129/1000 | Loss: 0.00002752
Iteration 130/1000 | Loss: 0.00002752
Iteration 131/1000 | Loss: 0.00002752
Iteration 132/1000 | Loss: 0.00002751
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002751
Iteration 136/1000 | Loss: 0.00002751
Iteration 137/1000 | Loss: 0.00002751
Iteration 138/1000 | Loss: 0.00002751
Iteration 139/1000 | Loss: 0.00002751
Iteration 140/1000 | Loss: 0.00002751
Iteration 141/1000 | Loss: 0.00002751
Iteration 142/1000 | Loss: 0.00002750
Iteration 143/1000 | Loss: 0.00002750
Iteration 144/1000 | Loss: 0.00002750
Iteration 145/1000 | Loss: 0.00002750
Iteration 146/1000 | Loss: 0.00002750
Iteration 147/1000 | Loss: 0.00002750
Iteration 148/1000 | Loss: 0.00002750
Iteration 149/1000 | Loss: 0.00002750
Iteration 150/1000 | Loss: 0.00002750
Iteration 151/1000 | Loss: 0.00002750
Iteration 152/1000 | Loss: 0.00002750
Iteration 153/1000 | Loss: 0.00002749
Iteration 154/1000 | Loss: 0.00002749
Iteration 155/1000 | Loss: 0.00002749
Iteration 156/1000 | Loss: 0.00002749
Iteration 157/1000 | Loss: 0.00002749
Iteration 158/1000 | Loss: 0.00002749
Iteration 159/1000 | Loss: 0.00002749
Iteration 160/1000 | Loss: 0.00002749
Iteration 161/1000 | Loss: 0.00002749
Iteration 162/1000 | Loss: 0.00002748
Iteration 163/1000 | Loss: 0.00002748
Iteration 164/1000 | Loss: 0.00002748
Iteration 165/1000 | Loss: 0.00002748
Iteration 166/1000 | Loss: 0.00002748
Iteration 167/1000 | Loss: 0.00002748
Iteration 168/1000 | Loss: 0.00002748
Iteration 169/1000 | Loss: 0.00002748
Iteration 170/1000 | Loss: 0.00002748
Iteration 171/1000 | Loss: 0.00002748
Iteration 172/1000 | Loss: 0.00002748
Iteration 173/1000 | Loss: 0.00002748
Iteration 174/1000 | Loss: 0.00002748
Iteration 175/1000 | Loss: 0.00002748
Iteration 176/1000 | Loss: 0.00002748
Iteration 177/1000 | Loss: 0.00002748
Iteration 178/1000 | Loss: 0.00002747
Iteration 179/1000 | Loss: 0.00002747
Iteration 180/1000 | Loss: 0.00002747
Iteration 181/1000 | Loss: 0.00002747
Iteration 182/1000 | Loss: 0.00002747
Iteration 183/1000 | Loss: 0.00002747
Iteration 184/1000 | Loss: 0.00002747
Iteration 185/1000 | Loss: 0.00002747
Iteration 186/1000 | Loss: 0.00002747
Iteration 187/1000 | Loss: 0.00002747
Iteration 188/1000 | Loss: 0.00002747
Iteration 189/1000 | Loss: 0.00002747
Iteration 190/1000 | Loss: 0.00002747
Iteration 191/1000 | Loss: 0.00002747
Iteration 192/1000 | Loss: 0.00002747
Iteration 193/1000 | Loss: 0.00002747
Iteration 194/1000 | Loss: 0.00002747
Iteration 195/1000 | Loss: 0.00002747
Iteration 196/1000 | Loss: 0.00002747
Iteration 197/1000 | Loss: 0.00002747
Iteration 198/1000 | Loss: 0.00002747
Iteration 199/1000 | Loss: 0.00002747
Iteration 200/1000 | Loss: 0.00002747
Iteration 201/1000 | Loss: 0.00002747
Iteration 202/1000 | Loss: 0.00002747
Iteration 203/1000 | Loss: 0.00002747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.7472717192722484e-05, 2.7472717192722484e-05, 2.7472717192722484e-05, 2.7472717192722484e-05, 2.7472717192722484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7472717192722484e-05

Optimization complete. Final v2v error: 4.333680629730225 mm

Highest mean error: 5.033255577087402 mm for frame 163

Lowest mean error: 3.8507347106933594 mm for frame 136

Saving results

Total time: 54.096458435058594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470868
Iteration 2/25 | Loss: 0.00137812
Iteration 3/25 | Loss: 0.00128057
Iteration 4/25 | Loss: 0.00127092
Iteration 5/25 | Loss: 0.00126875
Iteration 6/25 | Loss: 0.00126875
Iteration 7/25 | Loss: 0.00126875
Iteration 8/25 | Loss: 0.00126875
Iteration 9/25 | Loss: 0.00126875
Iteration 10/25 | Loss: 0.00126875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012687520356848836, 0.0012687520356848836, 0.0012687520356848836, 0.0012687520356848836, 0.0012687520356848836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012687520356848836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72147846
Iteration 2/25 | Loss: 0.00085076
Iteration 3/25 | Loss: 0.00085075
Iteration 4/25 | Loss: 0.00085075
Iteration 5/25 | Loss: 0.00085075
Iteration 6/25 | Loss: 0.00085075
Iteration 7/25 | Loss: 0.00085075
Iteration 8/25 | Loss: 0.00085075
Iteration 9/25 | Loss: 0.00085075
Iteration 10/25 | Loss: 0.00085075
Iteration 11/25 | Loss: 0.00085075
Iteration 12/25 | Loss: 0.00085075
Iteration 13/25 | Loss: 0.00085075
Iteration 14/25 | Loss: 0.00085075
Iteration 15/25 | Loss: 0.00085075
Iteration 16/25 | Loss: 0.00085075
Iteration 17/25 | Loss: 0.00085075
Iteration 18/25 | Loss: 0.00085075
Iteration 19/25 | Loss: 0.00085075
Iteration 20/25 | Loss: 0.00085075
Iteration 21/25 | Loss: 0.00085075
Iteration 22/25 | Loss: 0.00085075
Iteration 23/25 | Loss: 0.00085075
Iteration 24/25 | Loss: 0.00085075
Iteration 25/25 | Loss: 0.00085075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085075
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001609
Iteration 7/1000 | Loss: 0.00001570
Iteration 8/1000 | Loss: 0.00001540
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001451
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001448
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001443
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001433
Iteration 30/1000 | Loss: 0.00001432
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001426
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001421
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001417
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001405
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001403
Iteration 81/1000 | Loss: 0.00001403
Iteration 82/1000 | Loss: 0.00001403
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001399
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001395
Iteration 124/1000 | Loss: 0.00001395
Iteration 125/1000 | Loss: 0.00001395
Iteration 126/1000 | Loss: 0.00001395
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001392
Iteration 140/1000 | Loss: 0.00001392
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001392
Iteration 143/1000 | Loss: 0.00001392
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001390
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001390
Iteration 160/1000 | Loss: 0.00001390
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001390
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Iteration 179/1000 | Loss: 0.00001389
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3884753570891917e-05, 1.3884753570891917e-05, 1.3884753570891917e-05, 1.3884753570891917e-05, 1.3884753570891917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3884753570891917e-05

Optimization complete. Final v2v error: 3.1797029972076416 mm

Highest mean error: 3.457197427749634 mm for frame 90

Lowest mean error: 2.926541805267334 mm for frame 161

Saving results

Total time: 45.38753652572632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892468
Iteration 2/25 | Loss: 0.00148988
Iteration 3/25 | Loss: 0.00135030
Iteration 4/25 | Loss: 0.00133348
Iteration 5/25 | Loss: 0.00132814
Iteration 6/25 | Loss: 0.00132701
Iteration 7/25 | Loss: 0.00132701
Iteration 8/25 | Loss: 0.00132701
Iteration 9/25 | Loss: 0.00132701
Iteration 10/25 | Loss: 0.00132701
Iteration 11/25 | Loss: 0.00132701
Iteration 12/25 | Loss: 0.00132701
Iteration 13/25 | Loss: 0.00132701
Iteration 14/25 | Loss: 0.00132701
Iteration 15/25 | Loss: 0.00132701
Iteration 16/25 | Loss: 0.00132701
Iteration 17/25 | Loss: 0.00132701
Iteration 18/25 | Loss: 0.00132701
Iteration 19/25 | Loss: 0.00132701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013270096387714148, 0.0013270096387714148, 0.0013270096387714148, 0.0013270096387714148, 0.0013270096387714148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013270096387714148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39135885
Iteration 2/25 | Loss: 0.00111076
Iteration 3/25 | Loss: 0.00111060
Iteration 4/25 | Loss: 0.00111060
Iteration 5/25 | Loss: 0.00111060
Iteration 6/25 | Loss: 0.00111060
Iteration 7/25 | Loss: 0.00111060
Iteration 8/25 | Loss: 0.00111060
Iteration 9/25 | Loss: 0.00111060
Iteration 10/25 | Loss: 0.00111060
Iteration 11/25 | Loss: 0.00111060
Iteration 12/25 | Loss: 0.00111060
Iteration 13/25 | Loss: 0.00111060
Iteration 14/25 | Loss: 0.00111060
Iteration 15/25 | Loss: 0.00111060
Iteration 16/25 | Loss: 0.00111060
Iteration 17/25 | Loss: 0.00111060
Iteration 18/25 | Loss: 0.00111060
Iteration 19/25 | Loss: 0.00111060
Iteration 20/25 | Loss: 0.00111060
Iteration 21/25 | Loss: 0.00111060
Iteration 22/25 | Loss: 0.00111060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011105999583378434, 0.0011105999583378434, 0.0011105999583378434, 0.0011105999583378434, 0.0011105999583378434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011105999583378434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111060
Iteration 2/1000 | Loss: 0.00006368
Iteration 3/1000 | Loss: 0.00003877
Iteration 4/1000 | Loss: 0.00003184
Iteration 5/1000 | Loss: 0.00003017
Iteration 6/1000 | Loss: 0.00002897
Iteration 7/1000 | Loss: 0.00002819
Iteration 8/1000 | Loss: 0.00002765
Iteration 9/1000 | Loss: 0.00002720
Iteration 10/1000 | Loss: 0.00002687
Iteration 11/1000 | Loss: 0.00002659
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002634
Iteration 14/1000 | Loss: 0.00002625
Iteration 15/1000 | Loss: 0.00002616
Iteration 16/1000 | Loss: 0.00002614
Iteration 17/1000 | Loss: 0.00002614
Iteration 18/1000 | Loss: 0.00002614
Iteration 19/1000 | Loss: 0.00002609
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002605
Iteration 22/1000 | Loss: 0.00002603
Iteration 23/1000 | Loss: 0.00002603
Iteration 24/1000 | Loss: 0.00002602
Iteration 25/1000 | Loss: 0.00002602
Iteration 26/1000 | Loss: 0.00002601
Iteration 27/1000 | Loss: 0.00002599
Iteration 28/1000 | Loss: 0.00002596
Iteration 29/1000 | Loss: 0.00002596
Iteration 30/1000 | Loss: 0.00002595
Iteration 31/1000 | Loss: 0.00002595
Iteration 32/1000 | Loss: 0.00002595
Iteration 33/1000 | Loss: 0.00002594
Iteration 34/1000 | Loss: 0.00002594
Iteration 35/1000 | Loss: 0.00002593
Iteration 36/1000 | Loss: 0.00002592
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002592
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002590
Iteration 42/1000 | Loss: 0.00002589
Iteration 43/1000 | Loss: 0.00002589
Iteration 44/1000 | Loss: 0.00002588
Iteration 45/1000 | Loss: 0.00002588
Iteration 46/1000 | Loss: 0.00002586
Iteration 47/1000 | Loss: 0.00002586
Iteration 48/1000 | Loss: 0.00002586
Iteration 49/1000 | Loss: 0.00002586
Iteration 50/1000 | Loss: 0.00002586
Iteration 51/1000 | Loss: 0.00002585
Iteration 52/1000 | Loss: 0.00002585
Iteration 53/1000 | Loss: 0.00002585
Iteration 54/1000 | Loss: 0.00002585
Iteration 55/1000 | Loss: 0.00002584
Iteration 56/1000 | Loss: 0.00002584
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002584
Iteration 59/1000 | Loss: 0.00002584
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002583
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002583
Iteration 66/1000 | Loss: 0.00002583
Iteration 67/1000 | Loss: 0.00002583
Iteration 68/1000 | Loss: 0.00002582
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002582
Iteration 72/1000 | Loss: 0.00002582
Iteration 73/1000 | Loss: 0.00002582
Iteration 74/1000 | Loss: 0.00002582
Iteration 75/1000 | Loss: 0.00002582
Iteration 76/1000 | Loss: 0.00002582
Iteration 77/1000 | Loss: 0.00002581
Iteration 78/1000 | Loss: 0.00002581
Iteration 79/1000 | Loss: 0.00002581
Iteration 80/1000 | Loss: 0.00002581
Iteration 81/1000 | Loss: 0.00002581
Iteration 82/1000 | Loss: 0.00002581
Iteration 83/1000 | Loss: 0.00002581
Iteration 84/1000 | Loss: 0.00002581
Iteration 85/1000 | Loss: 0.00002581
Iteration 86/1000 | Loss: 0.00002580
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002580
Iteration 92/1000 | Loss: 0.00002580
Iteration 93/1000 | Loss: 0.00002580
Iteration 94/1000 | Loss: 0.00002580
Iteration 95/1000 | Loss: 0.00002579
Iteration 96/1000 | Loss: 0.00002579
Iteration 97/1000 | Loss: 0.00002579
Iteration 98/1000 | Loss: 0.00002579
Iteration 99/1000 | Loss: 0.00002578
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00002578
Iteration 102/1000 | Loss: 0.00002578
Iteration 103/1000 | Loss: 0.00002578
Iteration 104/1000 | Loss: 0.00002578
Iteration 105/1000 | Loss: 0.00002578
Iteration 106/1000 | Loss: 0.00002578
Iteration 107/1000 | Loss: 0.00002577
Iteration 108/1000 | Loss: 0.00002577
Iteration 109/1000 | Loss: 0.00002577
Iteration 110/1000 | Loss: 0.00002577
Iteration 111/1000 | Loss: 0.00002577
Iteration 112/1000 | Loss: 0.00002577
Iteration 113/1000 | Loss: 0.00002577
Iteration 114/1000 | Loss: 0.00002576
Iteration 115/1000 | Loss: 0.00002576
Iteration 116/1000 | Loss: 0.00002576
Iteration 117/1000 | Loss: 0.00002576
Iteration 118/1000 | Loss: 0.00002576
Iteration 119/1000 | Loss: 0.00002576
Iteration 120/1000 | Loss: 0.00002576
Iteration 121/1000 | Loss: 0.00002576
Iteration 122/1000 | Loss: 0.00002575
Iteration 123/1000 | Loss: 0.00002575
Iteration 124/1000 | Loss: 0.00002575
Iteration 125/1000 | Loss: 0.00002575
Iteration 126/1000 | Loss: 0.00002575
Iteration 127/1000 | Loss: 0.00002574
Iteration 128/1000 | Loss: 0.00002574
Iteration 129/1000 | Loss: 0.00002574
Iteration 130/1000 | Loss: 0.00002574
Iteration 131/1000 | Loss: 0.00002574
Iteration 132/1000 | Loss: 0.00002573
Iteration 133/1000 | Loss: 0.00002573
Iteration 134/1000 | Loss: 0.00002573
Iteration 135/1000 | Loss: 0.00002573
Iteration 136/1000 | Loss: 0.00002573
Iteration 137/1000 | Loss: 0.00002573
Iteration 138/1000 | Loss: 0.00002573
Iteration 139/1000 | Loss: 0.00002573
Iteration 140/1000 | Loss: 0.00002573
Iteration 141/1000 | Loss: 0.00002573
Iteration 142/1000 | Loss: 0.00002573
Iteration 143/1000 | Loss: 0.00002573
Iteration 144/1000 | Loss: 0.00002572
Iteration 145/1000 | Loss: 0.00002572
Iteration 146/1000 | Loss: 0.00002572
Iteration 147/1000 | Loss: 0.00002572
Iteration 148/1000 | Loss: 0.00002572
Iteration 149/1000 | Loss: 0.00002572
Iteration 150/1000 | Loss: 0.00002572
Iteration 151/1000 | Loss: 0.00002572
Iteration 152/1000 | Loss: 0.00002572
Iteration 153/1000 | Loss: 0.00002572
Iteration 154/1000 | Loss: 0.00002572
Iteration 155/1000 | Loss: 0.00002572
Iteration 156/1000 | Loss: 0.00002572
Iteration 157/1000 | Loss: 0.00002572
Iteration 158/1000 | Loss: 0.00002572
Iteration 159/1000 | Loss: 0.00002571
Iteration 160/1000 | Loss: 0.00002571
Iteration 161/1000 | Loss: 0.00002571
Iteration 162/1000 | Loss: 0.00002571
Iteration 163/1000 | Loss: 0.00002571
Iteration 164/1000 | Loss: 0.00002571
Iteration 165/1000 | Loss: 0.00002571
Iteration 166/1000 | Loss: 0.00002571
Iteration 167/1000 | Loss: 0.00002571
Iteration 168/1000 | Loss: 0.00002571
Iteration 169/1000 | Loss: 0.00002571
Iteration 170/1000 | Loss: 0.00002571
Iteration 171/1000 | Loss: 0.00002571
Iteration 172/1000 | Loss: 0.00002571
Iteration 173/1000 | Loss: 0.00002571
Iteration 174/1000 | Loss: 0.00002571
Iteration 175/1000 | Loss: 0.00002571
Iteration 176/1000 | Loss: 0.00002571
Iteration 177/1000 | Loss: 0.00002571
Iteration 178/1000 | Loss: 0.00002571
Iteration 179/1000 | Loss: 0.00002571
Iteration 180/1000 | Loss: 0.00002571
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002571
Iteration 183/1000 | Loss: 0.00002571
Iteration 184/1000 | Loss: 0.00002571
Iteration 185/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.5708473913255148e-05, 2.5708473913255148e-05, 2.5708473913255148e-05, 2.5708473913255148e-05, 2.5708473913255148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5708473913255148e-05

Optimization complete. Final v2v error: 3.8386709690093994 mm

Highest mean error: 5.832550525665283 mm for frame 88

Lowest mean error: 2.770328998565674 mm for frame 47

Saving results

Total time: 41.69875240325928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770401
Iteration 2/25 | Loss: 0.00142941
Iteration 3/25 | Loss: 0.00127907
Iteration 4/25 | Loss: 0.00126430
Iteration 5/25 | Loss: 0.00126127
Iteration 6/25 | Loss: 0.00126069
Iteration 7/25 | Loss: 0.00126069
Iteration 8/25 | Loss: 0.00126069
Iteration 9/25 | Loss: 0.00126069
Iteration 10/25 | Loss: 0.00126069
Iteration 11/25 | Loss: 0.00126069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012606942327693105, 0.0012606942327693105, 0.0012606942327693105, 0.0012606942327693105, 0.0012606942327693105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012606942327693105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42170763
Iteration 2/25 | Loss: 0.00083021
Iteration 3/25 | Loss: 0.00083021
Iteration 4/25 | Loss: 0.00083020
Iteration 5/25 | Loss: 0.00083020
Iteration 6/25 | Loss: 0.00083020
Iteration 7/25 | Loss: 0.00083020
Iteration 8/25 | Loss: 0.00083020
Iteration 9/25 | Loss: 0.00083020
Iteration 10/25 | Loss: 0.00083020
Iteration 11/25 | Loss: 0.00083020
Iteration 12/25 | Loss: 0.00083020
Iteration 13/25 | Loss: 0.00083020
Iteration 14/25 | Loss: 0.00083020
Iteration 15/25 | Loss: 0.00083020
Iteration 16/25 | Loss: 0.00083020
Iteration 17/25 | Loss: 0.00083020
Iteration 18/25 | Loss: 0.00083020
Iteration 19/25 | Loss: 0.00083020
Iteration 20/25 | Loss: 0.00083020
Iteration 21/25 | Loss: 0.00083020
Iteration 22/25 | Loss: 0.00083020
Iteration 23/25 | Loss: 0.00083020
Iteration 24/25 | Loss: 0.00083020
Iteration 25/25 | Loss: 0.00083020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083020
Iteration 2/1000 | Loss: 0.00003276
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001548
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001279
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001275
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001261
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001257
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001257
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001254
Iteration 44/1000 | Loss: 0.00001254
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001253
Iteration 48/1000 | Loss: 0.00001253
Iteration 49/1000 | Loss: 0.00001253
Iteration 50/1000 | Loss: 0.00001252
Iteration 51/1000 | Loss: 0.00001252
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001236
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001225
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001223
Iteration 180/1000 | Loss: 0.00001223
Iteration 181/1000 | Loss: 0.00001223
Iteration 182/1000 | Loss: 0.00001223
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001222
Iteration 187/1000 | Loss: 0.00001222
Iteration 188/1000 | Loss: 0.00001222
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001221
Iteration 198/1000 | Loss: 0.00001221
Iteration 199/1000 | Loss: 0.00001221
Iteration 200/1000 | Loss: 0.00001221
Iteration 201/1000 | Loss: 0.00001221
Iteration 202/1000 | Loss: 0.00001220
Iteration 203/1000 | Loss: 0.00001220
Iteration 204/1000 | Loss: 0.00001220
Iteration 205/1000 | Loss: 0.00001220
Iteration 206/1000 | Loss: 0.00001220
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001219
Iteration 209/1000 | Loss: 0.00001219
Iteration 210/1000 | Loss: 0.00001219
Iteration 211/1000 | Loss: 0.00001219
Iteration 212/1000 | Loss: 0.00001219
Iteration 213/1000 | Loss: 0.00001219
Iteration 214/1000 | Loss: 0.00001219
Iteration 215/1000 | Loss: 0.00001219
Iteration 216/1000 | Loss: 0.00001219
Iteration 217/1000 | Loss: 0.00001219
Iteration 218/1000 | Loss: 0.00001219
Iteration 219/1000 | Loss: 0.00001218
Iteration 220/1000 | Loss: 0.00001218
Iteration 221/1000 | Loss: 0.00001218
Iteration 222/1000 | Loss: 0.00001218
Iteration 223/1000 | Loss: 0.00001218
Iteration 224/1000 | Loss: 0.00001218
Iteration 225/1000 | Loss: 0.00001218
Iteration 226/1000 | Loss: 0.00001218
Iteration 227/1000 | Loss: 0.00001218
Iteration 228/1000 | Loss: 0.00001218
Iteration 229/1000 | Loss: 0.00001218
Iteration 230/1000 | Loss: 0.00001218
Iteration 231/1000 | Loss: 0.00001218
Iteration 232/1000 | Loss: 0.00001218
Iteration 233/1000 | Loss: 0.00001218
Iteration 234/1000 | Loss: 0.00001218
Iteration 235/1000 | Loss: 0.00001218
Iteration 236/1000 | Loss: 0.00001218
Iteration 237/1000 | Loss: 0.00001218
Iteration 238/1000 | Loss: 0.00001218
Iteration 239/1000 | Loss: 0.00001218
Iteration 240/1000 | Loss: 0.00001218
Iteration 241/1000 | Loss: 0.00001218
Iteration 242/1000 | Loss: 0.00001218
Iteration 243/1000 | Loss: 0.00001218
Iteration 244/1000 | Loss: 0.00001218
Iteration 245/1000 | Loss: 0.00001218
Iteration 246/1000 | Loss: 0.00001218
Iteration 247/1000 | Loss: 0.00001218
Iteration 248/1000 | Loss: 0.00001218
Iteration 249/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.2177546523162164e-05, 1.2177546523162164e-05, 1.2177546523162164e-05, 1.2177546523162164e-05, 1.2177546523162164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2177546523162164e-05

Optimization complete. Final v2v error: 2.9927215576171875 mm

Highest mean error: 3.2645492553710938 mm for frame 108

Lowest mean error: 2.839664936065674 mm for frame 4

Saving results

Total time: 47.88522005081177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607574
Iteration 2/25 | Loss: 0.00145269
Iteration 3/25 | Loss: 0.00132361
Iteration 4/25 | Loss: 0.00131110
Iteration 5/25 | Loss: 0.00130827
Iteration 6/25 | Loss: 0.00130801
Iteration 7/25 | Loss: 0.00130801
Iteration 8/25 | Loss: 0.00130801
Iteration 9/25 | Loss: 0.00130801
Iteration 10/25 | Loss: 0.00130801
Iteration 11/25 | Loss: 0.00130801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013080068165436387, 0.0013080068165436387, 0.0013080068165436387, 0.0013080068165436387, 0.0013080068165436387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013080068165436387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.15128851
Iteration 2/25 | Loss: 0.00095435
Iteration 3/25 | Loss: 0.00095428
Iteration 4/25 | Loss: 0.00095428
Iteration 5/25 | Loss: 0.00095428
Iteration 6/25 | Loss: 0.00095428
Iteration 7/25 | Loss: 0.00095428
Iteration 8/25 | Loss: 0.00095428
Iteration 9/25 | Loss: 0.00095428
Iteration 10/25 | Loss: 0.00095428
Iteration 11/25 | Loss: 0.00095428
Iteration 12/25 | Loss: 0.00095428
Iteration 13/25 | Loss: 0.00095428
Iteration 14/25 | Loss: 0.00095428
Iteration 15/25 | Loss: 0.00095428
Iteration 16/25 | Loss: 0.00095428
Iteration 17/25 | Loss: 0.00095428
Iteration 18/25 | Loss: 0.00095428
Iteration 19/25 | Loss: 0.00095428
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009542806656099856, 0.0009542806656099856, 0.0009542806656099856, 0.0009542806656099856, 0.0009542806656099856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009542806656099856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095428
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001562
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001554
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001553
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001551
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001544
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001541
Iteration 94/1000 | Loss: 0.00001541
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001538
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001537
Iteration 124/1000 | Loss: 0.00001537
Iteration 125/1000 | Loss: 0.00001537
Iteration 126/1000 | Loss: 0.00001537
Iteration 127/1000 | Loss: 0.00001537
Iteration 128/1000 | Loss: 0.00001537
Iteration 129/1000 | Loss: 0.00001537
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001536
Iteration 135/1000 | Loss: 0.00001536
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001534
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001534
Iteration 142/1000 | Loss: 0.00001534
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001534
Iteration 145/1000 | Loss: 0.00001533
Iteration 146/1000 | Loss: 0.00001533
Iteration 147/1000 | Loss: 0.00001533
Iteration 148/1000 | Loss: 0.00001533
Iteration 149/1000 | Loss: 0.00001533
Iteration 150/1000 | Loss: 0.00001533
Iteration 151/1000 | Loss: 0.00001533
Iteration 152/1000 | Loss: 0.00001533
Iteration 153/1000 | Loss: 0.00001532
Iteration 154/1000 | Loss: 0.00001532
Iteration 155/1000 | Loss: 0.00001532
Iteration 156/1000 | Loss: 0.00001531
Iteration 157/1000 | Loss: 0.00001531
Iteration 158/1000 | Loss: 0.00001531
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Iteration 161/1000 | Loss: 0.00001530
Iteration 162/1000 | Loss: 0.00001529
Iteration 163/1000 | Loss: 0.00001529
Iteration 164/1000 | Loss: 0.00001529
Iteration 165/1000 | Loss: 0.00001529
Iteration 166/1000 | Loss: 0.00001529
Iteration 167/1000 | Loss: 0.00001529
Iteration 168/1000 | Loss: 0.00001529
Iteration 169/1000 | Loss: 0.00001529
Iteration 170/1000 | Loss: 0.00001529
Iteration 171/1000 | Loss: 0.00001529
Iteration 172/1000 | Loss: 0.00001529
Iteration 173/1000 | Loss: 0.00001529
Iteration 174/1000 | Loss: 0.00001529
Iteration 175/1000 | Loss: 0.00001529
Iteration 176/1000 | Loss: 0.00001529
Iteration 177/1000 | Loss: 0.00001529
Iteration 178/1000 | Loss: 0.00001529
Iteration 179/1000 | Loss: 0.00001529
Iteration 180/1000 | Loss: 0.00001529
Iteration 181/1000 | Loss: 0.00001529
Iteration 182/1000 | Loss: 0.00001529
Iteration 183/1000 | Loss: 0.00001529
Iteration 184/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.5285288100130856e-05, 1.5285288100130856e-05, 1.5285288100130856e-05, 1.5285288100130856e-05, 1.5285288100130856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5285288100130856e-05

Optimization complete. Final v2v error: 3.2915682792663574 mm

Highest mean error: 4.3311872482299805 mm for frame 33

Lowest mean error: 2.881013870239258 mm for frame 158

Saving results

Total time: 39.67354917526245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879056
Iteration 2/25 | Loss: 0.00137201
Iteration 3/25 | Loss: 0.00128391
Iteration 4/25 | Loss: 0.00126968
Iteration 5/25 | Loss: 0.00126581
Iteration 6/25 | Loss: 0.00126489
Iteration 7/25 | Loss: 0.00126489
Iteration 8/25 | Loss: 0.00126489
Iteration 9/25 | Loss: 0.00126489
Iteration 10/25 | Loss: 0.00126489
Iteration 11/25 | Loss: 0.00126489
Iteration 12/25 | Loss: 0.00126489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012648896081373096, 0.0012648896081373096, 0.0012648896081373096, 0.0012648896081373096, 0.0012648896081373096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012648896081373096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36608756
Iteration 2/25 | Loss: 0.00088900
Iteration 3/25 | Loss: 0.00088900
Iteration 4/25 | Loss: 0.00088900
Iteration 5/25 | Loss: 0.00088900
Iteration 6/25 | Loss: 0.00088900
Iteration 7/25 | Loss: 0.00088900
Iteration 8/25 | Loss: 0.00088900
Iteration 9/25 | Loss: 0.00088900
Iteration 10/25 | Loss: 0.00088900
Iteration 11/25 | Loss: 0.00088900
Iteration 12/25 | Loss: 0.00088900
Iteration 13/25 | Loss: 0.00088900
Iteration 14/25 | Loss: 0.00088900
Iteration 15/25 | Loss: 0.00088900
Iteration 16/25 | Loss: 0.00088900
Iteration 17/25 | Loss: 0.00088900
Iteration 18/25 | Loss: 0.00088900
Iteration 19/25 | Loss: 0.00088900
Iteration 20/25 | Loss: 0.00088900
Iteration 21/25 | Loss: 0.00088900
Iteration 22/25 | Loss: 0.00088900
Iteration 23/25 | Loss: 0.00088900
Iteration 24/25 | Loss: 0.00088900
Iteration 25/25 | Loss: 0.00088900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008889976888895035, 0.0008889976888895035, 0.0008889976888895035, 0.0008889976888895035, 0.0008889976888895035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008889976888895035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088900
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00002495
Iteration 4/1000 | Loss: 0.00002302
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001966
Iteration 8/1000 | Loss: 0.00001919
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001859
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001854
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001845
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001835
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001829
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001828
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001827
Iteration 67/1000 | Loss: 0.00001827
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001824
Iteration 76/1000 | Loss: 0.00001824
Iteration 77/1000 | Loss: 0.00001824
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001823
Iteration 80/1000 | Loss: 0.00001823
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001821
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001820
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.8196245946455747e-05, 1.8196245946455747e-05, 1.8196245946455747e-05, 1.8196245946455747e-05, 1.8196245946455747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8196245946455747e-05

Optimization complete. Final v2v error: 3.6454129219055176 mm

Highest mean error: 4.115596771240234 mm for frame 167

Lowest mean error: 3.0895731449127197 mm for frame 237

Saving results

Total time: 38.86549663543701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043518
Iteration 2/25 | Loss: 0.01043518
Iteration 3/25 | Loss: 0.00271957
Iteration 4/25 | Loss: 0.00189400
Iteration 5/25 | Loss: 0.00186743
Iteration 6/25 | Loss: 0.00187055
Iteration 7/25 | Loss: 0.00165535
Iteration 8/25 | Loss: 0.00160032
Iteration 9/25 | Loss: 0.00156079
Iteration 10/25 | Loss: 0.00150283
Iteration 11/25 | Loss: 0.00143308
Iteration 12/25 | Loss: 0.00138581
Iteration 13/25 | Loss: 0.00134911
Iteration 14/25 | Loss: 0.00132309
Iteration 15/25 | Loss: 0.00130817
Iteration 16/25 | Loss: 0.00132768
Iteration 17/25 | Loss: 0.00131985
Iteration 18/25 | Loss: 0.00127187
Iteration 19/25 | Loss: 0.00128479
Iteration 20/25 | Loss: 0.00126601
Iteration 21/25 | Loss: 0.00126119
Iteration 22/25 | Loss: 0.00125146
Iteration 23/25 | Loss: 0.00125779
Iteration 24/25 | Loss: 0.00126150
Iteration 25/25 | Loss: 0.00125328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48355770
Iteration 2/25 | Loss: 0.00137673
Iteration 3/25 | Loss: 0.00117449
Iteration 4/25 | Loss: 0.00094910
Iteration 5/25 | Loss: 0.00094910
Iteration 6/25 | Loss: 0.00094910
Iteration 7/25 | Loss: 0.00094910
Iteration 8/25 | Loss: 0.00094910
Iteration 9/25 | Loss: 0.00094910
Iteration 10/25 | Loss: 0.00094910
Iteration 11/25 | Loss: 0.00094910
Iteration 12/25 | Loss: 0.00094910
Iteration 13/25 | Loss: 0.00094910
Iteration 14/25 | Loss: 0.00094910
Iteration 15/25 | Loss: 0.00094910
Iteration 16/25 | Loss: 0.00094910
Iteration 17/25 | Loss: 0.00094910
Iteration 18/25 | Loss: 0.00094910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009490956435911357, 0.0009490956435911357, 0.0009490956435911357, 0.0009490956435911357, 0.0009490956435911357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009490956435911357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094910
Iteration 2/1000 | Loss: 0.00020947
Iteration 3/1000 | Loss: 0.00117567
Iteration 4/1000 | Loss: 0.00339798
Iteration 5/1000 | Loss: 0.00117952
Iteration 6/1000 | Loss: 0.00133893
Iteration 7/1000 | Loss: 0.00098601
Iteration 8/1000 | Loss: 0.00162624
Iteration 9/1000 | Loss: 0.00063421
Iteration 10/1000 | Loss: 0.00101169
Iteration 11/1000 | Loss: 0.00071219
Iteration 12/1000 | Loss: 0.00027359
Iteration 13/1000 | Loss: 0.00083671
Iteration 14/1000 | Loss: 0.00063087
Iteration 15/1000 | Loss: 0.00018906
Iteration 16/1000 | Loss: 0.00015659
Iteration 17/1000 | Loss: 0.00064038
Iteration 18/1000 | Loss: 0.00109798
Iteration 19/1000 | Loss: 0.00017156
Iteration 20/1000 | Loss: 0.00011772
Iteration 21/1000 | Loss: 0.00012485
Iteration 22/1000 | Loss: 0.00043667
Iteration 23/1000 | Loss: 0.00039494
Iteration 24/1000 | Loss: 0.00024576
Iteration 25/1000 | Loss: 0.00032462
Iteration 26/1000 | Loss: 0.00045818
Iteration 27/1000 | Loss: 0.00015945
Iteration 28/1000 | Loss: 0.00036507
Iteration 29/1000 | Loss: 0.00005421
Iteration 30/1000 | Loss: 0.00009746
Iteration 31/1000 | Loss: 0.00025214
Iteration 32/1000 | Loss: 0.00007320
Iteration 33/1000 | Loss: 0.00015575
Iteration 34/1000 | Loss: 0.00006082
Iteration 35/1000 | Loss: 0.00003713
Iteration 36/1000 | Loss: 0.00012002
Iteration 37/1000 | Loss: 0.00009322
Iteration 38/1000 | Loss: 0.00006865
Iteration 39/1000 | Loss: 0.00008609
Iteration 40/1000 | Loss: 0.00022345
Iteration 41/1000 | Loss: 0.00011944
Iteration 42/1000 | Loss: 0.00004748
Iteration 43/1000 | Loss: 0.00007576
Iteration 44/1000 | Loss: 0.00011824
Iteration 45/1000 | Loss: 0.00004773
Iteration 46/1000 | Loss: 0.00009269
Iteration 47/1000 | Loss: 0.00011646
Iteration 48/1000 | Loss: 0.00013888
Iteration 49/1000 | Loss: 0.00015534
Iteration 50/1000 | Loss: 0.00004297
Iteration 51/1000 | Loss: 0.00009401
Iteration 52/1000 | Loss: 0.00012149
Iteration 53/1000 | Loss: 0.00043294
Iteration 54/1000 | Loss: 0.00026460
Iteration 55/1000 | Loss: 0.00039142
Iteration 56/1000 | Loss: 0.00015948
Iteration 57/1000 | Loss: 0.00012045
Iteration 58/1000 | Loss: 0.00014990
Iteration 59/1000 | Loss: 0.00033560
Iteration 60/1000 | Loss: 0.00007643
Iteration 61/1000 | Loss: 0.00006875
Iteration 62/1000 | Loss: 0.00012868
Iteration 63/1000 | Loss: 0.00009355
Iteration 64/1000 | Loss: 0.00012058
Iteration 65/1000 | Loss: 0.00009850
Iteration 66/1000 | Loss: 0.00011810
Iteration 67/1000 | Loss: 0.00025047
Iteration 68/1000 | Loss: 0.00010995
Iteration 69/1000 | Loss: 0.00008881
Iteration 70/1000 | Loss: 0.00013931
Iteration 71/1000 | Loss: 0.00024178
Iteration 72/1000 | Loss: 0.00045215
Iteration 73/1000 | Loss: 0.00022459
Iteration 74/1000 | Loss: 0.00007366
Iteration 75/1000 | Loss: 0.00019224
Iteration 76/1000 | Loss: 0.00013219
Iteration 77/1000 | Loss: 0.00008971
Iteration 78/1000 | Loss: 0.00008193
Iteration 79/1000 | Loss: 0.00007691
Iteration 80/1000 | Loss: 0.00003471
Iteration 81/1000 | Loss: 0.00018248
Iteration 82/1000 | Loss: 0.00014455
Iteration 83/1000 | Loss: 0.00008169
Iteration 84/1000 | Loss: 0.00008281
Iteration 85/1000 | Loss: 0.00020712
Iteration 86/1000 | Loss: 0.00007894
Iteration 87/1000 | Loss: 0.00009731
Iteration 88/1000 | Loss: 0.00004687
Iteration 89/1000 | Loss: 0.00004297
Iteration 90/1000 | Loss: 0.00013320
Iteration 91/1000 | Loss: 0.00013483
Iteration 92/1000 | Loss: 0.00011545
Iteration 93/1000 | Loss: 0.00008969
Iteration 94/1000 | Loss: 0.00008920
Iteration 95/1000 | Loss: 0.00012929
Iteration 96/1000 | Loss: 0.00012059
Iteration 97/1000 | Loss: 0.00029456
Iteration 98/1000 | Loss: 0.00011682
Iteration 99/1000 | Loss: 0.00010928
Iteration 100/1000 | Loss: 0.00005705
Iteration 101/1000 | Loss: 0.00005732
Iteration 102/1000 | Loss: 0.00008977
Iteration 103/1000 | Loss: 0.00010144
Iteration 104/1000 | Loss: 0.00009688
Iteration 105/1000 | Loss: 0.00012494
Iteration 106/1000 | Loss: 0.00008686
Iteration 107/1000 | Loss: 0.00009639
Iteration 108/1000 | Loss: 0.00007736
Iteration 109/1000 | Loss: 0.00033014
Iteration 110/1000 | Loss: 0.00011415
Iteration 111/1000 | Loss: 0.00021803
Iteration 112/1000 | Loss: 0.00011991
Iteration 113/1000 | Loss: 0.00015417
Iteration 114/1000 | Loss: 0.00009670
Iteration 115/1000 | Loss: 0.00009144
Iteration 116/1000 | Loss: 0.00008093
Iteration 117/1000 | Loss: 0.00022083
Iteration 118/1000 | Loss: 0.00021623
Iteration 119/1000 | Loss: 0.00003799
Iteration 120/1000 | Loss: 0.00008433
Iteration 121/1000 | Loss: 0.00009447
Iteration 122/1000 | Loss: 0.00035819
Iteration 123/1000 | Loss: 0.00003177
Iteration 124/1000 | Loss: 0.00004907
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002454
Iteration 127/1000 | Loss: 0.00002418
Iteration 128/1000 | Loss: 0.00002530
Iteration 129/1000 | Loss: 0.00023349
Iteration 130/1000 | Loss: 0.00135728
Iteration 131/1000 | Loss: 0.00138573
Iteration 132/1000 | Loss: 0.00040466
Iteration 133/1000 | Loss: 0.00084102
Iteration 134/1000 | Loss: 0.00015576
Iteration 135/1000 | Loss: 0.00004762
Iteration 136/1000 | Loss: 0.00002369
Iteration 137/1000 | Loss: 0.00005079
Iteration 138/1000 | Loss: 0.00003441
Iteration 139/1000 | Loss: 0.00002271
Iteration 140/1000 | Loss: 0.00002465
Iteration 141/1000 | Loss: 0.00002243
Iteration 142/1000 | Loss: 0.00002242
Iteration 143/1000 | Loss: 0.00002447
Iteration 144/1000 | Loss: 0.00002226
Iteration 145/1000 | Loss: 0.00002226
Iteration 146/1000 | Loss: 0.00002226
Iteration 147/1000 | Loss: 0.00002226
Iteration 148/1000 | Loss: 0.00002226
Iteration 149/1000 | Loss: 0.00002226
Iteration 150/1000 | Loss: 0.00002226
Iteration 151/1000 | Loss: 0.00002226
Iteration 152/1000 | Loss: 0.00002226
Iteration 153/1000 | Loss: 0.00002226
Iteration 154/1000 | Loss: 0.00002225
Iteration 155/1000 | Loss: 0.00002225
Iteration 156/1000 | Loss: 0.00002225
Iteration 157/1000 | Loss: 0.00002225
Iteration 158/1000 | Loss: 0.00005001
Iteration 159/1000 | Loss: 0.00002220
Iteration 160/1000 | Loss: 0.00002215
Iteration 161/1000 | Loss: 0.00002215
Iteration 162/1000 | Loss: 0.00002215
Iteration 163/1000 | Loss: 0.00002215
Iteration 164/1000 | Loss: 0.00002215
Iteration 165/1000 | Loss: 0.00002215
Iteration 166/1000 | Loss: 0.00002214
Iteration 167/1000 | Loss: 0.00002214
Iteration 168/1000 | Loss: 0.00002213
Iteration 169/1000 | Loss: 0.00002213
Iteration 170/1000 | Loss: 0.00002212
Iteration 171/1000 | Loss: 0.00002212
Iteration 172/1000 | Loss: 0.00002560
Iteration 173/1000 | Loss: 0.00003729
Iteration 174/1000 | Loss: 0.00005539
Iteration 175/1000 | Loss: 0.00003114
Iteration 176/1000 | Loss: 0.00016638
Iteration 177/1000 | Loss: 0.00003040
Iteration 178/1000 | Loss: 0.00002861
Iteration 179/1000 | Loss: 0.00002194
Iteration 180/1000 | Loss: 0.00002192
Iteration 181/1000 | Loss: 0.00002188
Iteration 182/1000 | Loss: 0.00002187
Iteration 183/1000 | Loss: 0.00002187
Iteration 184/1000 | Loss: 0.00002186
Iteration 185/1000 | Loss: 0.00002186
Iteration 186/1000 | Loss: 0.00002185
Iteration 187/1000 | Loss: 0.00002185
Iteration 188/1000 | Loss: 0.00002185
Iteration 189/1000 | Loss: 0.00002184
Iteration 190/1000 | Loss: 0.00002184
Iteration 191/1000 | Loss: 0.00005674
Iteration 192/1000 | Loss: 0.00002185
Iteration 193/1000 | Loss: 0.00002181
Iteration 194/1000 | Loss: 0.00002178
Iteration 195/1000 | Loss: 0.00002178
Iteration 196/1000 | Loss: 0.00002177
Iteration 197/1000 | Loss: 0.00002176
Iteration 198/1000 | Loss: 0.00002176
Iteration 199/1000 | Loss: 0.00002176
Iteration 200/1000 | Loss: 0.00002176
Iteration 201/1000 | Loss: 0.00002176
Iteration 202/1000 | Loss: 0.00002176
Iteration 203/1000 | Loss: 0.00002176
Iteration 204/1000 | Loss: 0.00002176
Iteration 205/1000 | Loss: 0.00002175
Iteration 206/1000 | Loss: 0.00002175
Iteration 207/1000 | Loss: 0.00002174
Iteration 208/1000 | Loss: 0.00002174
Iteration 209/1000 | Loss: 0.00002174
Iteration 210/1000 | Loss: 0.00002173
Iteration 211/1000 | Loss: 0.00002173
Iteration 212/1000 | Loss: 0.00002172
Iteration 213/1000 | Loss: 0.00002172
Iteration 214/1000 | Loss: 0.00002172
Iteration 215/1000 | Loss: 0.00002171
Iteration 216/1000 | Loss: 0.00002171
Iteration 217/1000 | Loss: 0.00002171
Iteration 218/1000 | Loss: 0.00002171
Iteration 219/1000 | Loss: 0.00002170
Iteration 220/1000 | Loss: 0.00002170
Iteration 221/1000 | Loss: 0.00002169
Iteration 222/1000 | Loss: 0.00002169
Iteration 223/1000 | Loss: 0.00002238
Iteration 224/1000 | Loss: 0.00002174
Iteration 225/1000 | Loss: 0.00002220
Iteration 226/1000 | Loss: 0.00002169
Iteration 227/1000 | Loss: 0.00002185
Iteration 228/1000 | Loss: 0.00002172
Iteration 229/1000 | Loss: 0.00002163
Iteration 230/1000 | Loss: 0.00002163
Iteration 231/1000 | Loss: 0.00002163
Iteration 232/1000 | Loss: 0.00002163
Iteration 233/1000 | Loss: 0.00002163
Iteration 234/1000 | Loss: 0.00002163
Iteration 235/1000 | Loss: 0.00002163
Iteration 236/1000 | Loss: 0.00002163
Iteration 237/1000 | Loss: 0.00002163
Iteration 238/1000 | Loss: 0.00002163
Iteration 239/1000 | Loss: 0.00002163
Iteration 240/1000 | Loss: 0.00002163
Iteration 241/1000 | Loss: 0.00002163
Iteration 242/1000 | Loss: 0.00002163
Iteration 243/1000 | Loss: 0.00002162
Iteration 244/1000 | Loss: 0.00002162
Iteration 245/1000 | Loss: 0.00002162
Iteration 246/1000 | Loss: 0.00002162
Iteration 247/1000 | Loss: 0.00002162
Iteration 248/1000 | Loss: 0.00002162
Iteration 249/1000 | Loss: 0.00002162
Iteration 250/1000 | Loss: 0.00002162
Iteration 251/1000 | Loss: 0.00002162
Iteration 252/1000 | Loss: 0.00002162
Iteration 253/1000 | Loss: 0.00002162
Iteration 254/1000 | Loss: 0.00002162
Iteration 255/1000 | Loss: 0.00002162
Iteration 256/1000 | Loss: 0.00002162
Iteration 257/1000 | Loss: 0.00002162
Iteration 258/1000 | Loss: 0.00002162
Iteration 259/1000 | Loss: 0.00002162
Iteration 260/1000 | Loss: 0.00002162
Iteration 261/1000 | Loss: 0.00002162
Iteration 262/1000 | Loss: 0.00002162
Iteration 263/1000 | Loss: 0.00002162
Iteration 264/1000 | Loss: 0.00002161
Iteration 265/1000 | Loss: 0.00002161
Iteration 266/1000 | Loss: 0.00002161
Iteration 267/1000 | Loss: 0.00002161
Iteration 268/1000 | Loss: 0.00002161
Iteration 269/1000 | Loss: 0.00002161
Iteration 270/1000 | Loss: 0.00002161
Iteration 271/1000 | Loss: 0.00002161
Iteration 272/1000 | Loss: 0.00002161
Iteration 273/1000 | Loss: 0.00002161
Iteration 274/1000 | Loss: 0.00002161
Iteration 275/1000 | Loss: 0.00002161
Iteration 276/1000 | Loss: 0.00002161
Iteration 277/1000 | Loss: 0.00002161
Iteration 278/1000 | Loss: 0.00002161
Iteration 279/1000 | Loss: 0.00002161
Iteration 280/1000 | Loss: 0.00002161
Iteration 281/1000 | Loss: 0.00002161
Iteration 282/1000 | Loss: 0.00002161
Iteration 283/1000 | Loss: 0.00002161
Iteration 284/1000 | Loss: 0.00002161
Iteration 285/1000 | Loss: 0.00002161
Iteration 286/1000 | Loss: 0.00002161
Iteration 287/1000 | Loss: 0.00002161
Iteration 288/1000 | Loss: 0.00002161
Iteration 289/1000 | Loss: 0.00002161
Iteration 290/1000 | Loss: 0.00002161
Iteration 291/1000 | Loss: 0.00002161
Iteration 292/1000 | Loss: 0.00002161
Iteration 293/1000 | Loss: 0.00002161
Iteration 294/1000 | Loss: 0.00002161
Iteration 295/1000 | Loss: 0.00002161
Iteration 296/1000 | Loss: 0.00002161
Iteration 297/1000 | Loss: 0.00002161
Iteration 298/1000 | Loss: 0.00002161
Iteration 299/1000 | Loss: 0.00002161
Iteration 300/1000 | Loss: 0.00002161
Iteration 301/1000 | Loss: 0.00002161
Iteration 302/1000 | Loss: 0.00002161
Iteration 303/1000 | Loss: 0.00002161
Iteration 304/1000 | Loss: 0.00002161
Iteration 305/1000 | Loss: 0.00002161
Iteration 306/1000 | Loss: 0.00002161
Iteration 307/1000 | Loss: 0.00002161
Iteration 308/1000 | Loss: 0.00002161
Iteration 309/1000 | Loss: 0.00002161
Iteration 310/1000 | Loss: 0.00002161
Iteration 311/1000 | Loss: 0.00002161
Iteration 312/1000 | Loss: 0.00002161
Iteration 313/1000 | Loss: 0.00002161
Iteration 314/1000 | Loss: 0.00002161
Iteration 315/1000 | Loss: 0.00002161
Iteration 316/1000 | Loss: 0.00002161
Iteration 317/1000 | Loss: 0.00002161
Iteration 318/1000 | Loss: 0.00002161
Iteration 319/1000 | Loss: 0.00002161
Iteration 320/1000 | Loss: 0.00002161
Iteration 321/1000 | Loss: 0.00002161
Iteration 322/1000 | Loss: 0.00002161
Iteration 323/1000 | Loss: 0.00002161
Iteration 324/1000 | Loss: 0.00002161
Iteration 325/1000 | Loss: 0.00002161
Iteration 326/1000 | Loss: 0.00002161
Iteration 327/1000 | Loss: 0.00002161
Iteration 328/1000 | Loss: 0.00002161
Iteration 329/1000 | Loss: 0.00002161
Iteration 330/1000 | Loss: 0.00002161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [2.1612499040202238e-05, 2.1612499040202238e-05, 2.1612499040202238e-05, 2.1612499040202238e-05, 2.1612499040202238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1612499040202238e-05

Optimization complete. Final v2v error: 3.9342901706695557 mm

Highest mean error: 5.833614349365234 mm for frame 85

Lowest mean error: 3.1648292541503906 mm for frame 215

Saving results

Total time: 314.8764662742615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796354
Iteration 2/25 | Loss: 0.00134072
Iteration 3/25 | Loss: 0.00124099
Iteration 4/25 | Loss: 0.00123183
Iteration 5/25 | Loss: 0.00122935
Iteration 6/25 | Loss: 0.00122933
Iteration 7/25 | Loss: 0.00122933
Iteration 8/25 | Loss: 0.00122933
Iteration 9/25 | Loss: 0.00122933
Iteration 10/25 | Loss: 0.00122933
Iteration 11/25 | Loss: 0.00122933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012293258914723992, 0.0012293258914723992, 0.0012293258914723992, 0.0012293258914723992, 0.0012293258914723992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012293258914723992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43093479
Iteration 2/25 | Loss: 0.00078290
Iteration 3/25 | Loss: 0.00078290
Iteration 4/25 | Loss: 0.00078290
Iteration 5/25 | Loss: 0.00078290
Iteration 6/25 | Loss: 0.00078290
Iteration 7/25 | Loss: 0.00078290
Iteration 8/25 | Loss: 0.00078290
Iteration 9/25 | Loss: 0.00078289
Iteration 10/25 | Loss: 0.00078289
Iteration 11/25 | Loss: 0.00078289
Iteration 12/25 | Loss: 0.00078289
Iteration 13/25 | Loss: 0.00078289
Iteration 14/25 | Loss: 0.00078289
Iteration 15/25 | Loss: 0.00078289
Iteration 16/25 | Loss: 0.00078289
Iteration 17/25 | Loss: 0.00078289
Iteration 18/25 | Loss: 0.00078289
Iteration 19/25 | Loss: 0.00078289
Iteration 20/25 | Loss: 0.00078289
Iteration 21/25 | Loss: 0.00078289
Iteration 22/25 | Loss: 0.00078289
Iteration 23/25 | Loss: 0.00078289
Iteration 24/25 | Loss: 0.00078289
Iteration 25/25 | Loss: 0.00078289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078289
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001269
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001227
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001191
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001191
Iteration 54/1000 | Loss: 0.00001191
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001179
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001177
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001176
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001176
Iteration 108/1000 | Loss: 0.00001176
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001175
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001164
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001162
Iteration 159/1000 | Loss: 0.00001162
Iteration 160/1000 | Loss: 0.00001162
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001162
Iteration 167/1000 | Loss: 0.00001162
Iteration 168/1000 | Loss: 0.00001162
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001161
Iteration 173/1000 | Loss: 0.00001161
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001160
Iteration 180/1000 | Loss: 0.00001160
Iteration 181/1000 | Loss: 0.00001160
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Iteration 206/1000 | Loss: 0.00001158
Iteration 207/1000 | Loss: 0.00001158
Iteration 208/1000 | Loss: 0.00001158
Iteration 209/1000 | Loss: 0.00001158
Iteration 210/1000 | Loss: 0.00001158
Iteration 211/1000 | Loss: 0.00001158
Iteration 212/1000 | Loss: 0.00001158
Iteration 213/1000 | Loss: 0.00001158
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001158
Iteration 221/1000 | Loss: 0.00001158
Iteration 222/1000 | Loss: 0.00001158
Iteration 223/1000 | Loss: 0.00001158
Iteration 224/1000 | Loss: 0.00001158
Iteration 225/1000 | Loss: 0.00001158
Iteration 226/1000 | Loss: 0.00001158
Iteration 227/1000 | Loss: 0.00001158
Iteration 228/1000 | Loss: 0.00001158
Iteration 229/1000 | Loss: 0.00001158
Iteration 230/1000 | Loss: 0.00001158
Iteration 231/1000 | Loss: 0.00001158
Iteration 232/1000 | Loss: 0.00001158
Iteration 233/1000 | Loss: 0.00001158
Iteration 234/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.1579503734537866e-05, 1.1579503734537866e-05, 1.1579503734537866e-05, 1.1579503734537866e-05, 1.1579503734537866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1579503734537866e-05

Optimization complete. Final v2v error: 2.9027631282806396 mm

Highest mean error: 3.062068462371826 mm for frame 26

Lowest mean error: 2.759122371673584 mm for frame 8

Saving results

Total time: 39.479384899139404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819754
Iteration 2/25 | Loss: 0.00138945
Iteration 3/25 | Loss: 0.00128715
Iteration 4/25 | Loss: 0.00126546
Iteration 5/25 | Loss: 0.00125834
Iteration 6/25 | Loss: 0.00125667
Iteration 7/25 | Loss: 0.00125667
Iteration 8/25 | Loss: 0.00125667
Iteration 9/25 | Loss: 0.00125667
Iteration 10/25 | Loss: 0.00125667
Iteration 11/25 | Loss: 0.00125667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00125667464453727, 0.00125667464453727, 0.00125667464453727, 0.00125667464453727, 0.00125667464453727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00125667464453727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58515084
Iteration 2/25 | Loss: 0.00083080
Iteration 3/25 | Loss: 0.00083080
Iteration 4/25 | Loss: 0.00083080
Iteration 5/25 | Loss: 0.00083080
Iteration 6/25 | Loss: 0.00083080
Iteration 7/25 | Loss: 0.00083080
Iteration 8/25 | Loss: 0.00083080
Iteration 9/25 | Loss: 0.00083080
Iteration 10/25 | Loss: 0.00083080
Iteration 11/25 | Loss: 0.00083080
Iteration 12/25 | Loss: 0.00083080
Iteration 13/25 | Loss: 0.00083080
Iteration 14/25 | Loss: 0.00083080
Iteration 15/25 | Loss: 0.00083080
Iteration 16/25 | Loss: 0.00083080
Iteration 17/25 | Loss: 0.00083080
Iteration 18/25 | Loss: 0.00083080
Iteration 19/25 | Loss: 0.00083080
Iteration 20/25 | Loss: 0.00083080
Iteration 21/25 | Loss: 0.00083080
Iteration 22/25 | Loss: 0.00083080
Iteration 23/25 | Loss: 0.00083080
Iteration 24/25 | Loss: 0.00083080
Iteration 25/25 | Loss: 0.00083080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083080
Iteration 2/1000 | Loss: 0.00004936
Iteration 3/1000 | Loss: 0.00003719
Iteration 4/1000 | Loss: 0.00003259
Iteration 5/1000 | Loss: 0.00003025
Iteration 6/1000 | Loss: 0.00002890
Iteration 7/1000 | Loss: 0.00002790
Iteration 8/1000 | Loss: 0.00002718
Iteration 9/1000 | Loss: 0.00002660
Iteration 10/1000 | Loss: 0.00002619
Iteration 11/1000 | Loss: 0.00002588
Iteration 12/1000 | Loss: 0.00002560
Iteration 13/1000 | Loss: 0.00002534
Iteration 14/1000 | Loss: 0.00002521
Iteration 15/1000 | Loss: 0.00002503
Iteration 16/1000 | Loss: 0.00002495
Iteration 17/1000 | Loss: 0.00002482
Iteration 18/1000 | Loss: 0.00002481
Iteration 19/1000 | Loss: 0.00002479
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002473
Iteration 22/1000 | Loss: 0.00002473
Iteration 23/1000 | Loss: 0.00002472
Iteration 24/1000 | Loss: 0.00002471
Iteration 25/1000 | Loss: 0.00002471
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002470
Iteration 28/1000 | Loss: 0.00002470
Iteration 29/1000 | Loss: 0.00002470
Iteration 30/1000 | Loss: 0.00002469
Iteration 31/1000 | Loss: 0.00002469
Iteration 32/1000 | Loss: 0.00002469
Iteration 33/1000 | Loss: 0.00002468
Iteration 34/1000 | Loss: 0.00002468
Iteration 35/1000 | Loss: 0.00002467
Iteration 36/1000 | Loss: 0.00002467
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002466
Iteration 39/1000 | Loss: 0.00002465
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002465
Iteration 42/1000 | Loss: 0.00002465
Iteration 43/1000 | Loss: 0.00002465
Iteration 44/1000 | Loss: 0.00002464
Iteration 45/1000 | Loss: 0.00002464
Iteration 46/1000 | Loss: 0.00002464
Iteration 47/1000 | Loss: 0.00002464
Iteration 48/1000 | Loss: 0.00002464
Iteration 49/1000 | Loss: 0.00002463
Iteration 50/1000 | Loss: 0.00002463
Iteration 51/1000 | Loss: 0.00002463
Iteration 52/1000 | Loss: 0.00002463
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002462
Iteration 56/1000 | Loss: 0.00002462
Iteration 57/1000 | Loss: 0.00002462
Iteration 58/1000 | Loss: 0.00002462
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002460
Iteration 67/1000 | Loss: 0.00002460
Iteration 68/1000 | Loss: 0.00002460
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002459
Iteration 71/1000 | Loss: 0.00002458
Iteration 72/1000 | Loss: 0.00002458
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002457
Iteration 75/1000 | Loss: 0.00002457
Iteration 76/1000 | Loss: 0.00002457
Iteration 77/1000 | Loss: 0.00002457
Iteration 78/1000 | Loss: 0.00002456
Iteration 79/1000 | Loss: 0.00002456
Iteration 80/1000 | Loss: 0.00002456
Iteration 81/1000 | Loss: 0.00002455
Iteration 82/1000 | Loss: 0.00002455
Iteration 83/1000 | Loss: 0.00002455
Iteration 84/1000 | Loss: 0.00002454
Iteration 85/1000 | Loss: 0.00002454
Iteration 86/1000 | Loss: 0.00002454
Iteration 87/1000 | Loss: 0.00002454
Iteration 88/1000 | Loss: 0.00002454
Iteration 89/1000 | Loss: 0.00002454
Iteration 90/1000 | Loss: 0.00002454
Iteration 91/1000 | Loss: 0.00002454
Iteration 92/1000 | Loss: 0.00002454
Iteration 93/1000 | Loss: 0.00002454
Iteration 94/1000 | Loss: 0.00002453
Iteration 95/1000 | Loss: 0.00002453
Iteration 96/1000 | Loss: 0.00002453
Iteration 97/1000 | Loss: 0.00002452
Iteration 98/1000 | Loss: 0.00002452
Iteration 99/1000 | Loss: 0.00002452
Iteration 100/1000 | Loss: 0.00002452
Iteration 101/1000 | Loss: 0.00002452
Iteration 102/1000 | Loss: 0.00002452
Iteration 103/1000 | Loss: 0.00002452
Iteration 104/1000 | Loss: 0.00002452
Iteration 105/1000 | Loss: 0.00002452
Iteration 106/1000 | Loss: 0.00002452
Iteration 107/1000 | Loss: 0.00002452
Iteration 108/1000 | Loss: 0.00002451
Iteration 109/1000 | Loss: 0.00002451
Iteration 110/1000 | Loss: 0.00002451
Iteration 111/1000 | Loss: 0.00002451
Iteration 112/1000 | Loss: 0.00002451
Iteration 113/1000 | Loss: 0.00002451
Iteration 114/1000 | Loss: 0.00002451
Iteration 115/1000 | Loss: 0.00002451
Iteration 116/1000 | Loss: 0.00002451
Iteration 117/1000 | Loss: 0.00002451
Iteration 118/1000 | Loss: 0.00002451
Iteration 119/1000 | Loss: 0.00002451
Iteration 120/1000 | Loss: 0.00002451
Iteration 121/1000 | Loss: 0.00002450
Iteration 122/1000 | Loss: 0.00002450
Iteration 123/1000 | Loss: 0.00002450
Iteration 124/1000 | Loss: 0.00002450
Iteration 125/1000 | Loss: 0.00002450
Iteration 126/1000 | Loss: 0.00002450
Iteration 127/1000 | Loss: 0.00002450
Iteration 128/1000 | Loss: 0.00002450
Iteration 129/1000 | Loss: 0.00002450
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002448
Iteration 135/1000 | Loss: 0.00002448
Iteration 136/1000 | Loss: 0.00002448
Iteration 137/1000 | Loss: 0.00002448
Iteration 138/1000 | Loss: 0.00002448
Iteration 139/1000 | Loss: 0.00002448
Iteration 140/1000 | Loss: 0.00002448
Iteration 141/1000 | Loss: 0.00002447
Iteration 142/1000 | Loss: 0.00002447
Iteration 143/1000 | Loss: 0.00002447
Iteration 144/1000 | Loss: 0.00002447
Iteration 145/1000 | Loss: 0.00002446
Iteration 146/1000 | Loss: 0.00002446
Iteration 147/1000 | Loss: 0.00002446
Iteration 148/1000 | Loss: 0.00002446
Iteration 149/1000 | Loss: 0.00002446
Iteration 150/1000 | Loss: 0.00002446
Iteration 151/1000 | Loss: 0.00002446
Iteration 152/1000 | Loss: 0.00002445
Iteration 153/1000 | Loss: 0.00002445
Iteration 154/1000 | Loss: 0.00002445
Iteration 155/1000 | Loss: 0.00002445
Iteration 156/1000 | Loss: 0.00002445
Iteration 157/1000 | Loss: 0.00002445
Iteration 158/1000 | Loss: 0.00002445
Iteration 159/1000 | Loss: 0.00002445
Iteration 160/1000 | Loss: 0.00002445
Iteration 161/1000 | Loss: 0.00002445
Iteration 162/1000 | Loss: 0.00002444
Iteration 163/1000 | Loss: 0.00002444
Iteration 164/1000 | Loss: 0.00002444
Iteration 165/1000 | Loss: 0.00002444
Iteration 166/1000 | Loss: 0.00002444
Iteration 167/1000 | Loss: 0.00002444
Iteration 168/1000 | Loss: 0.00002444
Iteration 169/1000 | Loss: 0.00002444
Iteration 170/1000 | Loss: 0.00002444
Iteration 171/1000 | Loss: 0.00002444
Iteration 172/1000 | Loss: 0.00002443
Iteration 173/1000 | Loss: 0.00002443
Iteration 174/1000 | Loss: 0.00002443
Iteration 175/1000 | Loss: 0.00002443
Iteration 176/1000 | Loss: 0.00002443
Iteration 177/1000 | Loss: 0.00002443
Iteration 178/1000 | Loss: 0.00002443
Iteration 179/1000 | Loss: 0.00002443
Iteration 180/1000 | Loss: 0.00002443
Iteration 181/1000 | Loss: 0.00002442
Iteration 182/1000 | Loss: 0.00002442
Iteration 183/1000 | Loss: 0.00002442
Iteration 184/1000 | Loss: 0.00002442
Iteration 185/1000 | Loss: 0.00002442
Iteration 186/1000 | Loss: 0.00002442
Iteration 187/1000 | Loss: 0.00002442
Iteration 188/1000 | Loss: 0.00002442
Iteration 189/1000 | Loss: 0.00002442
Iteration 190/1000 | Loss: 0.00002441
Iteration 191/1000 | Loss: 0.00002441
Iteration 192/1000 | Loss: 0.00002441
Iteration 193/1000 | Loss: 0.00002441
Iteration 194/1000 | Loss: 0.00002441
Iteration 195/1000 | Loss: 0.00002441
Iteration 196/1000 | Loss: 0.00002441
Iteration 197/1000 | Loss: 0.00002441
Iteration 198/1000 | Loss: 0.00002440
Iteration 199/1000 | Loss: 0.00002440
Iteration 200/1000 | Loss: 0.00002440
Iteration 201/1000 | Loss: 0.00002440
Iteration 202/1000 | Loss: 0.00002440
Iteration 203/1000 | Loss: 0.00002440
Iteration 204/1000 | Loss: 0.00002440
Iteration 205/1000 | Loss: 0.00002440
Iteration 206/1000 | Loss: 0.00002440
Iteration 207/1000 | Loss: 0.00002440
Iteration 208/1000 | Loss: 0.00002440
Iteration 209/1000 | Loss: 0.00002440
Iteration 210/1000 | Loss: 0.00002440
Iteration 211/1000 | Loss: 0.00002439
Iteration 212/1000 | Loss: 0.00002439
Iteration 213/1000 | Loss: 0.00002439
Iteration 214/1000 | Loss: 0.00002439
Iteration 215/1000 | Loss: 0.00002439
Iteration 216/1000 | Loss: 0.00002439
Iteration 217/1000 | Loss: 0.00002439
Iteration 218/1000 | Loss: 0.00002439
Iteration 219/1000 | Loss: 0.00002439
Iteration 220/1000 | Loss: 0.00002439
Iteration 221/1000 | Loss: 0.00002439
Iteration 222/1000 | Loss: 0.00002439
Iteration 223/1000 | Loss: 0.00002438
Iteration 224/1000 | Loss: 0.00002438
Iteration 225/1000 | Loss: 0.00002438
Iteration 226/1000 | Loss: 0.00002438
Iteration 227/1000 | Loss: 0.00002438
Iteration 228/1000 | Loss: 0.00002438
Iteration 229/1000 | Loss: 0.00002438
Iteration 230/1000 | Loss: 0.00002438
Iteration 231/1000 | Loss: 0.00002438
Iteration 232/1000 | Loss: 0.00002438
Iteration 233/1000 | Loss: 0.00002438
Iteration 234/1000 | Loss: 0.00002438
Iteration 235/1000 | Loss: 0.00002438
Iteration 236/1000 | Loss: 0.00002438
Iteration 237/1000 | Loss: 0.00002438
Iteration 238/1000 | Loss: 0.00002438
Iteration 239/1000 | Loss: 0.00002438
Iteration 240/1000 | Loss: 0.00002438
Iteration 241/1000 | Loss: 0.00002438
Iteration 242/1000 | Loss: 0.00002438
Iteration 243/1000 | Loss: 0.00002438
Iteration 244/1000 | Loss: 0.00002438
Iteration 245/1000 | Loss: 0.00002438
Iteration 246/1000 | Loss: 0.00002438
Iteration 247/1000 | Loss: 0.00002438
Iteration 248/1000 | Loss: 0.00002438
Iteration 249/1000 | Loss: 0.00002438
Iteration 250/1000 | Loss: 0.00002438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.4384306016145274e-05, 2.4384306016145274e-05, 2.4384306016145274e-05, 2.4384306016145274e-05, 2.4384306016145274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4384306016145274e-05

Optimization complete. Final v2v error: 4.038074016571045 mm

Highest mean error: 6.154684543609619 mm for frame 59

Lowest mean error: 2.954718828201294 mm for frame 46

Saving results

Total time: 48.514915466308594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778791
Iteration 2/25 | Loss: 0.00154790
Iteration 3/25 | Loss: 0.00138226
Iteration 4/25 | Loss: 0.00134713
Iteration 5/25 | Loss: 0.00134661
Iteration 6/25 | Loss: 0.00132817
Iteration 7/25 | Loss: 0.00131007
Iteration 8/25 | Loss: 0.00130479
Iteration 9/25 | Loss: 0.00130408
Iteration 10/25 | Loss: 0.00130392
Iteration 11/25 | Loss: 0.00130379
Iteration 12/25 | Loss: 0.00130367
Iteration 13/25 | Loss: 0.00130361
Iteration 14/25 | Loss: 0.00130360
Iteration 15/25 | Loss: 0.00130360
Iteration 16/25 | Loss: 0.00130360
Iteration 17/25 | Loss: 0.00130360
Iteration 18/25 | Loss: 0.00130360
Iteration 19/25 | Loss: 0.00130360
Iteration 20/25 | Loss: 0.00130360
Iteration 21/25 | Loss: 0.00130360
Iteration 22/25 | Loss: 0.00130359
Iteration 23/25 | Loss: 0.00130359
Iteration 24/25 | Loss: 0.00130359
Iteration 25/25 | Loss: 0.00130359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45659435
Iteration 2/25 | Loss: 0.00085691
Iteration 3/25 | Loss: 0.00085689
Iteration 4/25 | Loss: 0.00085689
Iteration 5/25 | Loss: 0.00085689
Iteration 6/25 | Loss: 0.00085689
Iteration 7/25 | Loss: 0.00085689
Iteration 8/25 | Loss: 0.00085689
Iteration 9/25 | Loss: 0.00085689
Iteration 10/25 | Loss: 0.00085689
Iteration 11/25 | Loss: 0.00085689
Iteration 12/25 | Loss: 0.00085689
Iteration 13/25 | Loss: 0.00085689
Iteration 14/25 | Loss: 0.00085689
Iteration 15/25 | Loss: 0.00085689
Iteration 16/25 | Loss: 0.00085689
Iteration 17/25 | Loss: 0.00085689
Iteration 18/25 | Loss: 0.00085689
Iteration 19/25 | Loss: 0.00085689
Iteration 20/25 | Loss: 0.00085689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008568887133151293, 0.0008568887133151293, 0.0008568887133151293, 0.0008568887133151293, 0.0008568887133151293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008568887133151293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085689
Iteration 2/1000 | Loss: 0.00004818
Iteration 3/1000 | Loss: 0.00060679
Iteration 4/1000 | Loss: 0.00089592
Iteration 5/1000 | Loss: 0.00076150
Iteration 6/1000 | Loss: 0.00057884
Iteration 7/1000 | Loss: 0.00049895
Iteration 8/1000 | Loss: 0.00038564
Iteration 9/1000 | Loss: 0.00037062
Iteration 10/1000 | Loss: 0.00016646
Iteration 11/1000 | Loss: 0.00028202
Iteration 12/1000 | Loss: 0.00044504
Iteration 13/1000 | Loss: 0.00038031
Iteration 14/1000 | Loss: 0.00053706
Iteration 15/1000 | Loss: 0.00040469
Iteration 16/1000 | Loss: 0.00034327
Iteration 17/1000 | Loss: 0.00029364
Iteration 18/1000 | Loss: 0.00026979
Iteration 19/1000 | Loss: 0.00020065
Iteration 20/1000 | Loss: 0.00012844
Iteration 21/1000 | Loss: 0.00010331
Iteration 22/1000 | Loss: 0.00009824
Iteration 23/1000 | Loss: 0.00016328
Iteration 24/1000 | Loss: 0.00021519
Iteration 25/1000 | Loss: 0.00019398
Iteration 26/1000 | Loss: 0.00005000
Iteration 27/1000 | Loss: 0.00024542
Iteration 28/1000 | Loss: 0.00024308
Iteration 29/1000 | Loss: 0.00034599
Iteration 30/1000 | Loss: 0.00079308
Iteration 31/1000 | Loss: 0.00037231
Iteration 32/1000 | Loss: 0.00038810
Iteration 33/1000 | Loss: 0.00030405
Iteration 34/1000 | Loss: 0.00019965
Iteration 35/1000 | Loss: 0.00028795
Iteration 36/1000 | Loss: 0.00025347
Iteration 37/1000 | Loss: 0.00025404
Iteration 38/1000 | Loss: 0.00020146
Iteration 39/1000 | Loss: 0.00030045
Iteration 40/1000 | Loss: 0.00027203
Iteration 41/1000 | Loss: 0.00028407
Iteration 42/1000 | Loss: 0.00022453
Iteration 43/1000 | Loss: 0.00021676
Iteration 44/1000 | Loss: 0.00019541
Iteration 45/1000 | Loss: 0.00019289
Iteration 46/1000 | Loss: 0.00022396
Iteration 47/1000 | Loss: 0.00024328
Iteration 48/1000 | Loss: 0.00009233
Iteration 49/1000 | Loss: 0.00013063
Iteration 50/1000 | Loss: 0.00009433
Iteration 51/1000 | Loss: 0.00005351
Iteration 52/1000 | Loss: 0.00020014
Iteration 53/1000 | Loss: 0.00012677
Iteration 54/1000 | Loss: 0.00024243
Iteration 55/1000 | Loss: 0.00027368
Iteration 56/1000 | Loss: 0.00033201
Iteration 57/1000 | Loss: 0.00022174
Iteration 58/1000 | Loss: 0.00041650
Iteration 59/1000 | Loss: 0.00018412
Iteration 60/1000 | Loss: 0.00014997
Iteration 61/1000 | Loss: 0.00021245
Iteration 62/1000 | Loss: 0.00013798
Iteration 63/1000 | Loss: 0.00008574
Iteration 64/1000 | Loss: 0.00006208
Iteration 65/1000 | Loss: 0.00003960
Iteration 66/1000 | Loss: 0.00014752
Iteration 67/1000 | Loss: 0.00004436
Iteration 68/1000 | Loss: 0.00034996
Iteration 69/1000 | Loss: 0.00021615
Iteration 70/1000 | Loss: 0.00051495
Iteration 71/1000 | Loss: 0.00023511
Iteration 72/1000 | Loss: 0.00013728
Iteration 73/1000 | Loss: 0.00005319
Iteration 74/1000 | Loss: 0.00008928
Iteration 75/1000 | Loss: 0.00003789
Iteration 76/1000 | Loss: 0.00029565
Iteration 77/1000 | Loss: 0.00024678
Iteration 78/1000 | Loss: 0.00014312
Iteration 79/1000 | Loss: 0.00016587
Iteration 80/1000 | Loss: 0.00010858
Iteration 81/1000 | Loss: 0.00027773
Iteration 82/1000 | Loss: 0.00012388
Iteration 83/1000 | Loss: 0.00005094
Iteration 84/1000 | Loss: 0.00007509
Iteration 85/1000 | Loss: 0.00004469
Iteration 86/1000 | Loss: 0.00003510
Iteration 87/1000 | Loss: 0.00003221
Iteration 88/1000 | Loss: 0.00003110
Iteration 89/1000 | Loss: 0.00006889
Iteration 90/1000 | Loss: 0.00017748
Iteration 91/1000 | Loss: 0.00017705
Iteration 92/1000 | Loss: 0.00018679
Iteration 93/1000 | Loss: 0.00021406
Iteration 94/1000 | Loss: 0.00023966
Iteration 95/1000 | Loss: 0.00018181
Iteration 96/1000 | Loss: 0.00020550
Iteration 97/1000 | Loss: 0.00010803
Iteration 98/1000 | Loss: 0.00004163
Iteration 99/1000 | Loss: 0.00003393
Iteration 100/1000 | Loss: 0.00003053
Iteration 101/1000 | Loss: 0.00002899
Iteration 102/1000 | Loss: 0.00019760
Iteration 103/1000 | Loss: 0.00022260
Iteration 104/1000 | Loss: 0.00005233
Iteration 105/1000 | Loss: 0.00007974
Iteration 106/1000 | Loss: 0.00015450
Iteration 107/1000 | Loss: 0.00013924
Iteration 108/1000 | Loss: 0.00017630
Iteration 109/1000 | Loss: 0.00023174
Iteration 110/1000 | Loss: 0.00008248
Iteration 111/1000 | Loss: 0.00048787
Iteration 112/1000 | Loss: 0.00034048
Iteration 113/1000 | Loss: 0.00006395
Iteration 114/1000 | Loss: 0.00003945
Iteration 115/1000 | Loss: 0.00010397
Iteration 116/1000 | Loss: 0.00003153
Iteration 117/1000 | Loss: 0.00018886
Iteration 118/1000 | Loss: 0.00011119
Iteration 119/1000 | Loss: 0.00006755
Iteration 120/1000 | Loss: 0.00005700
Iteration 121/1000 | Loss: 0.00018916
Iteration 122/1000 | Loss: 0.00006520
Iteration 123/1000 | Loss: 0.00027534
Iteration 124/1000 | Loss: 0.00003645
Iteration 125/1000 | Loss: 0.00002852
Iteration 126/1000 | Loss: 0.00002666
Iteration 127/1000 | Loss: 0.00002527
Iteration 128/1000 | Loss: 0.00002382
Iteration 129/1000 | Loss: 0.00002302
Iteration 130/1000 | Loss: 0.00002255
Iteration 131/1000 | Loss: 0.00002221
Iteration 132/1000 | Loss: 0.00002193
Iteration 133/1000 | Loss: 0.00002173
Iteration 134/1000 | Loss: 0.00002468
Iteration 135/1000 | Loss: 0.00002158
Iteration 136/1000 | Loss: 0.00002124
Iteration 137/1000 | Loss: 0.00002121
Iteration 138/1000 | Loss: 0.00002103
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002093
Iteration 141/1000 | Loss: 0.00002092
Iteration 142/1000 | Loss: 0.00002092
Iteration 143/1000 | Loss: 0.00002091
Iteration 144/1000 | Loss: 0.00002091
Iteration 145/1000 | Loss: 0.00002091
Iteration 146/1000 | Loss: 0.00002091
Iteration 147/1000 | Loss: 0.00002090
Iteration 148/1000 | Loss: 0.00002090
Iteration 149/1000 | Loss: 0.00002090
Iteration 150/1000 | Loss: 0.00002089
Iteration 151/1000 | Loss: 0.00002089
Iteration 152/1000 | Loss: 0.00002086
Iteration 153/1000 | Loss: 0.00002084
Iteration 154/1000 | Loss: 0.00002084
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002082
Iteration 158/1000 | Loss: 0.00002079
Iteration 159/1000 | Loss: 0.00002074
Iteration 160/1000 | Loss: 0.00002072
Iteration 161/1000 | Loss: 0.00002072
Iteration 162/1000 | Loss: 0.00002072
Iteration 163/1000 | Loss: 0.00002072
Iteration 164/1000 | Loss: 0.00002072
Iteration 165/1000 | Loss: 0.00002072
Iteration 166/1000 | Loss: 0.00002072
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002070
Iteration 169/1000 | Loss: 0.00002070
Iteration 170/1000 | Loss: 0.00002069
Iteration 171/1000 | Loss: 0.00002069
Iteration 172/1000 | Loss: 0.00002069
Iteration 173/1000 | Loss: 0.00002068
Iteration 174/1000 | Loss: 0.00002068
Iteration 175/1000 | Loss: 0.00002068
Iteration 176/1000 | Loss: 0.00002067
Iteration 177/1000 | Loss: 0.00002066
Iteration 178/1000 | Loss: 0.00002065
Iteration 179/1000 | Loss: 0.00002065
Iteration 180/1000 | Loss: 0.00002065
Iteration 181/1000 | Loss: 0.00002065
Iteration 182/1000 | Loss: 0.00002065
Iteration 183/1000 | Loss: 0.00002065
Iteration 184/1000 | Loss: 0.00002065
Iteration 185/1000 | Loss: 0.00002065
Iteration 186/1000 | Loss: 0.00002064
Iteration 187/1000 | Loss: 0.00002063
Iteration 188/1000 | Loss: 0.00002063
Iteration 189/1000 | Loss: 0.00002063
Iteration 190/1000 | Loss: 0.00002063
Iteration 191/1000 | Loss: 0.00002062
Iteration 192/1000 | Loss: 0.00002062
Iteration 193/1000 | Loss: 0.00002061
Iteration 194/1000 | Loss: 0.00002061
Iteration 195/1000 | Loss: 0.00002061
Iteration 196/1000 | Loss: 0.00002061
Iteration 197/1000 | Loss: 0.00002061
Iteration 198/1000 | Loss: 0.00002060
Iteration 199/1000 | Loss: 0.00002060
Iteration 200/1000 | Loss: 0.00002060
Iteration 201/1000 | Loss: 0.00002060
Iteration 202/1000 | Loss: 0.00002060
Iteration 203/1000 | Loss: 0.00002059
Iteration 204/1000 | Loss: 0.00002059
Iteration 205/1000 | Loss: 0.00002059
Iteration 206/1000 | Loss: 0.00002059
Iteration 207/1000 | Loss: 0.00002059
Iteration 208/1000 | Loss: 0.00002059
Iteration 209/1000 | Loss: 0.00002059
Iteration 210/1000 | Loss: 0.00002058
Iteration 211/1000 | Loss: 0.00002058
Iteration 212/1000 | Loss: 0.00002058
Iteration 213/1000 | Loss: 0.00002058
Iteration 214/1000 | Loss: 0.00002058
Iteration 215/1000 | Loss: 0.00002058
Iteration 216/1000 | Loss: 0.00002058
Iteration 217/1000 | Loss: 0.00002058
Iteration 218/1000 | Loss: 0.00002058
Iteration 219/1000 | Loss: 0.00002058
Iteration 220/1000 | Loss: 0.00002057
Iteration 221/1000 | Loss: 0.00002057
Iteration 222/1000 | Loss: 0.00002057
Iteration 223/1000 | Loss: 0.00002057
Iteration 224/1000 | Loss: 0.00002057
Iteration 225/1000 | Loss: 0.00002057
Iteration 226/1000 | Loss: 0.00002057
Iteration 227/1000 | Loss: 0.00002057
Iteration 228/1000 | Loss: 0.00002057
Iteration 229/1000 | Loss: 0.00002057
Iteration 230/1000 | Loss: 0.00002057
Iteration 231/1000 | Loss: 0.00002057
Iteration 232/1000 | Loss: 0.00002057
Iteration 233/1000 | Loss: 0.00002057
Iteration 234/1000 | Loss: 0.00002057
Iteration 235/1000 | Loss: 0.00002057
Iteration 236/1000 | Loss: 0.00002057
Iteration 237/1000 | Loss: 0.00002057
Iteration 238/1000 | Loss: 0.00002057
Iteration 239/1000 | Loss: 0.00002057
Iteration 240/1000 | Loss: 0.00002057
Iteration 241/1000 | Loss: 0.00002057
Iteration 242/1000 | Loss: 0.00002057
Iteration 243/1000 | Loss: 0.00002057
Iteration 244/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.0566345483530313e-05, 2.0566345483530313e-05, 2.0566345483530313e-05, 2.0566345483530313e-05, 2.0566345483530313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0566345483530313e-05

Optimization complete. Final v2v error: 3.60241961479187 mm

Highest mean error: 5.970292091369629 mm for frame 215

Lowest mean error: 2.9565913677215576 mm for frame 110

Saving results

Total time: 256.59699630737305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018700
Iteration 2/25 | Loss: 0.00283776
Iteration 3/25 | Loss: 0.00196240
Iteration 4/25 | Loss: 0.00196878
Iteration 5/25 | Loss: 0.00189086
Iteration 6/25 | Loss: 0.00175714
Iteration 7/25 | Loss: 0.00161226
Iteration 8/25 | Loss: 0.00152258
Iteration 9/25 | Loss: 0.00148840
Iteration 10/25 | Loss: 0.00144946
Iteration 11/25 | Loss: 0.00138416
Iteration 12/25 | Loss: 0.00136592
Iteration 13/25 | Loss: 0.00137378
Iteration 14/25 | Loss: 0.00135443
Iteration 15/25 | Loss: 0.00134253
Iteration 16/25 | Loss: 0.00134215
Iteration 17/25 | Loss: 0.00134065
Iteration 18/25 | Loss: 0.00134401
Iteration 19/25 | Loss: 0.00133840
Iteration 20/25 | Loss: 0.00133418
Iteration 21/25 | Loss: 0.00133320
Iteration 22/25 | Loss: 0.00133698
Iteration 23/25 | Loss: 0.00133674
Iteration 24/25 | Loss: 0.00133580
Iteration 25/25 | Loss: 0.00133380

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46509016
Iteration 2/25 | Loss: 0.00161932
Iteration 3/25 | Loss: 0.00161931
Iteration 4/25 | Loss: 0.00161931
Iteration 5/25 | Loss: 0.00161931
Iteration 6/25 | Loss: 0.00161931
Iteration 7/25 | Loss: 0.00161931
Iteration 8/25 | Loss: 0.00161931
Iteration 9/25 | Loss: 0.00161931
Iteration 10/25 | Loss: 0.00161931
Iteration 11/25 | Loss: 0.00161931
Iteration 12/25 | Loss: 0.00161527
Iteration 13/25 | Loss: 0.00161527
Iteration 14/25 | Loss: 0.00161527
Iteration 15/25 | Loss: 0.00161527
Iteration 16/25 | Loss: 0.00161527
Iteration 17/25 | Loss: 0.00161527
Iteration 18/25 | Loss: 0.00161527
Iteration 19/25 | Loss: 0.00161527
Iteration 20/25 | Loss: 0.00161527
Iteration 21/25 | Loss: 0.00161527
Iteration 22/25 | Loss: 0.00161527
Iteration 23/25 | Loss: 0.00161527
Iteration 24/25 | Loss: 0.00161527
Iteration 25/25 | Loss: 0.00161527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161527
Iteration 2/1000 | Loss: 0.00019714
Iteration 3/1000 | Loss: 0.00029187
Iteration 4/1000 | Loss: 0.00031918
Iteration 5/1000 | Loss: 0.00057813
Iteration 6/1000 | Loss: 0.00030429
Iteration 7/1000 | Loss: 0.00014950
Iteration 8/1000 | Loss: 0.00019472
Iteration 9/1000 | Loss: 0.00027975
Iteration 10/1000 | Loss: 0.00042820
Iteration 11/1000 | Loss: 0.00037688
Iteration 12/1000 | Loss: 0.00007571
Iteration 13/1000 | Loss: 0.00006305
Iteration 14/1000 | Loss: 0.00068109
Iteration 15/1000 | Loss: 0.00080009
Iteration 16/1000 | Loss: 0.00010082
Iteration 17/1000 | Loss: 0.00008689
Iteration 18/1000 | Loss: 0.00005967
Iteration 19/1000 | Loss: 0.00004980
Iteration 20/1000 | Loss: 0.00004834
Iteration 21/1000 | Loss: 0.00004293
Iteration 22/1000 | Loss: 0.00043229
Iteration 23/1000 | Loss: 0.00004855
Iteration 24/1000 | Loss: 0.00004235
Iteration 25/1000 | Loss: 0.00003789
Iteration 26/1000 | Loss: 0.00003604
Iteration 27/1000 | Loss: 0.00003436
Iteration 28/1000 | Loss: 0.00075870
Iteration 29/1000 | Loss: 0.00043917
Iteration 30/1000 | Loss: 0.00005468
Iteration 31/1000 | Loss: 0.00003863
Iteration 32/1000 | Loss: 0.00003179
Iteration 33/1000 | Loss: 0.00002531
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001746
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001683
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001664
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001658
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00001653
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001653
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001652
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001651
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00001651
Iteration 95/1000 | Loss: 0.00001651
Iteration 96/1000 | Loss: 0.00001651
Iteration 97/1000 | Loss: 0.00001651
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001650
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Iteration 132/1000 | Loss: 0.00001650
Iteration 133/1000 | Loss: 0.00001650
Iteration 134/1000 | Loss: 0.00001650
Iteration 135/1000 | Loss: 0.00001650
Iteration 136/1000 | Loss: 0.00001650
Iteration 137/1000 | Loss: 0.00001650
Iteration 138/1000 | Loss: 0.00001650
Iteration 139/1000 | Loss: 0.00001650
Iteration 140/1000 | Loss: 0.00001650
Iteration 141/1000 | Loss: 0.00001650
Iteration 142/1000 | Loss: 0.00001650
Iteration 143/1000 | Loss: 0.00001650
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001650
Iteration 146/1000 | Loss: 0.00001650
Iteration 147/1000 | Loss: 0.00001650
Iteration 148/1000 | Loss: 0.00001650
Iteration 149/1000 | Loss: 0.00001650
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001650
Iteration 154/1000 | Loss: 0.00001650
Iteration 155/1000 | Loss: 0.00001650
Iteration 156/1000 | Loss: 0.00001650
Iteration 157/1000 | Loss: 0.00001650
Iteration 158/1000 | Loss: 0.00001650
Iteration 159/1000 | Loss: 0.00001650
Iteration 160/1000 | Loss: 0.00001650
Iteration 161/1000 | Loss: 0.00001650
Iteration 162/1000 | Loss: 0.00001650
Iteration 163/1000 | Loss: 0.00001650
Iteration 164/1000 | Loss: 0.00001650
Iteration 165/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.649782461754512e-05, 1.649782461754512e-05, 1.649782461754512e-05, 1.649782461754512e-05, 1.649782461754512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.649782461754512e-05

Optimization complete. Final v2v error: 3.426220655441284 mm

Highest mean error: 5.051377296447754 mm for frame 78

Lowest mean error: 3.002242088317871 mm for frame 112

Saving results

Total time: 118.0752305984497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691105
Iteration 2/25 | Loss: 0.00153851
Iteration 3/25 | Loss: 0.00138665
Iteration 4/25 | Loss: 0.00136290
Iteration 5/25 | Loss: 0.00135345
Iteration 6/25 | Loss: 0.00135585
Iteration 7/25 | Loss: 0.00135034
Iteration 8/25 | Loss: 0.00135226
Iteration 9/25 | Loss: 0.00134904
Iteration 10/25 | Loss: 0.00134600
Iteration 11/25 | Loss: 0.00134085
Iteration 12/25 | Loss: 0.00133854
Iteration 13/25 | Loss: 0.00133762
Iteration 14/25 | Loss: 0.00133738
Iteration 15/25 | Loss: 0.00133817
Iteration 16/25 | Loss: 0.00133703
Iteration 17/25 | Loss: 0.00133674
Iteration 18/25 | Loss: 0.00133775
Iteration 19/25 | Loss: 0.00133443
Iteration 20/25 | Loss: 0.00133312
Iteration 21/25 | Loss: 0.00133108
Iteration 22/25 | Loss: 0.00133084
Iteration 23/25 | Loss: 0.00133080
Iteration 24/25 | Loss: 0.00133080
Iteration 25/25 | Loss: 0.00133080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59895957
Iteration 2/25 | Loss: 0.00080731
Iteration 3/25 | Loss: 0.00080731
Iteration 4/25 | Loss: 0.00080731
Iteration 5/25 | Loss: 0.00080731
Iteration 6/25 | Loss: 0.00080731
Iteration 7/25 | Loss: 0.00080731
Iteration 8/25 | Loss: 0.00080731
Iteration 9/25 | Loss: 0.00080731
Iteration 10/25 | Loss: 0.00080730
Iteration 11/25 | Loss: 0.00080730
Iteration 12/25 | Loss: 0.00080730
Iteration 13/25 | Loss: 0.00080730
Iteration 14/25 | Loss: 0.00080730
Iteration 15/25 | Loss: 0.00080730
Iteration 16/25 | Loss: 0.00080730
Iteration 17/25 | Loss: 0.00080730
Iteration 18/25 | Loss: 0.00080730
Iteration 19/25 | Loss: 0.00080730
Iteration 20/25 | Loss: 0.00080730
Iteration 21/25 | Loss: 0.00080730
Iteration 22/25 | Loss: 0.00080730
Iteration 23/25 | Loss: 0.00080730
Iteration 24/25 | Loss: 0.00080730
Iteration 25/25 | Loss: 0.00080730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008073039934970438, 0.0008073039934970438, 0.0008073039934970438, 0.0008073039934970438, 0.0008073039934970438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008073039934970438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080730
Iteration 2/1000 | Loss: 0.00007282
Iteration 3/1000 | Loss: 0.00004553
Iteration 4/1000 | Loss: 0.00003362
Iteration 5/1000 | Loss: 0.00003075
Iteration 6/1000 | Loss: 0.00002925
Iteration 7/1000 | Loss: 0.00002818
Iteration 8/1000 | Loss: 0.00002757
Iteration 9/1000 | Loss: 0.00002699
Iteration 10/1000 | Loss: 0.00002659
Iteration 11/1000 | Loss: 0.00002625
Iteration 12/1000 | Loss: 0.00002598
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002566
Iteration 15/1000 | Loss: 0.00002556
Iteration 16/1000 | Loss: 0.00002545
Iteration 17/1000 | Loss: 0.00002542
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002530
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002523
Iteration 24/1000 | Loss: 0.00002523
Iteration 25/1000 | Loss: 0.00002515
Iteration 26/1000 | Loss: 0.00002511
Iteration 27/1000 | Loss: 0.00002509
Iteration 28/1000 | Loss: 0.00002508
Iteration 29/1000 | Loss: 0.00002508
Iteration 30/1000 | Loss: 0.00002508
Iteration 31/1000 | Loss: 0.00002507
Iteration 32/1000 | Loss: 0.00002507
Iteration 33/1000 | Loss: 0.00002506
Iteration 34/1000 | Loss: 0.00002505
Iteration 35/1000 | Loss: 0.00002505
Iteration 36/1000 | Loss: 0.00002505
Iteration 37/1000 | Loss: 0.00002505
Iteration 38/1000 | Loss: 0.00002505
Iteration 39/1000 | Loss: 0.00002505
Iteration 40/1000 | Loss: 0.00002504
Iteration 41/1000 | Loss: 0.00002504
Iteration 42/1000 | Loss: 0.00002504
Iteration 43/1000 | Loss: 0.00002503
Iteration 44/1000 | Loss: 0.00002503
Iteration 45/1000 | Loss: 0.00002503
Iteration 46/1000 | Loss: 0.00002502
Iteration 47/1000 | Loss: 0.00002502
Iteration 48/1000 | Loss: 0.00002502
Iteration 49/1000 | Loss: 0.00002502
Iteration 50/1000 | Loss: 0.00002501
Iteration 51/1000 | Loss: 0.00002501
Iteration 52/1000 | Loss: 0.00002500
Iteration 53/1000 | Loss: 0.00002500
Iteration 54/1000 | Loss: 0.00002500
Iteration 55/1000 | Loss: 0.00002500
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002499
Iteration 58/1000 | Loss: 0.00002499
Iteration 59/1000 | Loss: 0.00002499
Iteration 60/1000 | Loss: 0.00002499
Iteration 61/1000 | Loss: 0.00002499
Iteration 62/1000 | Loss: 0.00002499
Iteration 63/1000 | Loss: 0.00002498
Iteration 64/1000 | Loss: 0.00002498
Iteration 65/1000 | Loss: 0.00002498
Iteration 66/1000 | Loss: 0.00002498
Iteration 67/1000 | Loss: 0.00002497
Iteration 68/1000 | Loss: 0.00002497
Iteration 69/1000 | Loss: 0.00002497
Iteration 70/1000 | Loss: 0.00002497
Iteration 71/1000 | Loss: 0.00002497
Iteration 72/1000 | Loss: 0.00002497
Iteration 73/1000 | Loss: 0.00002497
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002496
Iteration 76/1000 | Loss: 0.00002496
Iteration 77/1000 | Loss: 0.00002496
Iteration 78/1000 | Loss: 0.00002496
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002495
Iteration 85/1000 | Loss: 0.00002495
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002495
Iteration 88/1000 | Loss: 0.00002495
Iteration 89/1000 | Loss: 0.00002495
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002494
Iteration 96/1000 | Loss: 0.00002494
Iteration 97/1000 | Loss: 0.00002494
Iteration 98/1000 | Loss: 0.00002494
Iteration 99/1000 | Loss: 0.00002494
Iteration 100/1000 | Loss: 0.00002494
Iteration 101/1000 | Loss: 0.00002494
Iteration 102/1000 | Loss: 0.00002493
Iteration 103/1000 | Loss: 0.00002493
Iteration 104/1000 | Loss: 0.00002493
Iteration 105/1000 | Loss: 0.00002493
Iteration 106/1000 | Loss: 0.00002493
Iteration 107/1000 | Loss: 0.00002493
Iteration 108/1000 | Loss: 0.00002493
Iteration 109/1000 | Loss: 0.00002493
Iteration 110/1000 | Loss: 0.00002493
Iteration 111/1000 | Loss: 0.00002493
Iteration 112/1000 | Loss: 0.00002493
Iteration 113/1000 | Loss: 0.00002493
Iteration 114/1000 | Loss: 0.00002493
Iteration 115/1000 | Loss: 0.00002492
Iteration 116/1000 | Loss: 0.00002492
Iteration 117/1000 | Loss: 0.00002492
Iteration 118/1000 | Loss: 0.00002492
Iteration 119/1000 | Loss: 0.00002492
Iteration 120/1000 | Loss: 0.00002492
Iteration 121/1000 | Loss: 0.00002492
Iteration 122/1000 | Loss: 0.00002492
Iteration 123/1000 | Loss: 0.00002491
Iteration 124/1000 | Loss: 0.00002491
Iteration 125/1000 | Loss: 0.00002491
Iteration 126/1000 | Loss: 0.00002491
Iteration 127/1000 | Loss: 0.00002491
Iteration 128/1000 | Loss: 0.00002491
Iteration 129/1000 | Loss: 0.00002491
Iteration 130/1000 | Loss: 0.00002491
Iteration 131/1000 | Loss: 0.00002491
Iteration 132/1000 | Loss: 0.00002490
Iteration 133/1000 | Loss: 0.00002490
Iteration 134/1000 | Loss: 0.00002490
Iteration 135/1000 | Loss: 0.00002490
Iteration 136/1000 | Loss: 0.00002490
Iteration 137/1000 | Loss: 0.00002490
Iteration 138/1000 | Loss: 0.00002490
Iteration 139/1000 | Loss: 0.00002490
Iteration 140/1000 | Loss: 0.00002490
Iteration 141/1000 | Loss: 0.00002490
Iteration 142/1000 | Loss: 0.00002489
Iteration 143/1000 | Loss: 0.00002489
Iteration 144/1000 | Loss: 0.00002489
Iteration 145/1000 | Loss: 0.00002489
Iteration 146/1000 | Loss: 0.00002488
Iteration 147/1000 | Loss: 0.00002488
Iteration 148/1000 | Loss: 0.00002488
Iteration 149/1000 | Loss: 0.00002488
Iteration 150/1000 | Loss: 0.00002488
Iteration 151/1000 | Loss: 0.00002487
Iteration 152/1000 | Loss: 0.00002487
Iteration 153/1000 | Loss: 0.00002487
Iteration 154/1000 | Loss: 0.00002487
Iteration 155/1000 | Loss: 0.00002487
Iteration 156/1000 | Loss: 0.00002487
Iteration 157/1000 | Loss: 0.00002486
Iteration 158/1000 | Loss: 0.00002486
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002486
Iteration 161/1000 | Loss: 0.00002485
Iteration 162/1000 | Loss: 0.00002485
Iteration 163/1000 | Loss: 0.00002485
Iteration 164/1000 | Loss: 0.00002485
Iteration 165/1000 | Loss: 0.00002485
Iteration 166/1000 | Loss: 0.00002485
Iteration 167/1000 | Loss: 0.00002485
Iteration 168/1000 | Loss: 0.00002485
Iteration 169/1000 | Loss: 0.00002485
Iteration 170/1000 | Loss: 0.00002484
Iteration 171/1000 | Loss: 0.00002484
Iteration 172/1000 | Loss: 0.00002484
Iteration 173/1000 | Loss: 0.00002484
Iteration 174/1000 | Loss: 0.00002484
Iteration 175/1000 | Loss: 0.00002484
Iteration 176/1000 | Loss: 0.00002484
Iteration 177/1000 | Loss: 0.00002484
Iteration 178/1000 | Loss: 0.00002484
Iteration 179/1000 | Loss: 0.00002484
Iteration 180/1000 | Loss: 0.00002484
Iteration 181/1000 | Loss: 0.00002484
Iteration 182/1000 | Loss: 0.00002484
Iteration 183/1000 | Loss: 0.00002484
Iteration 184/1000 | Loss: 0.00002484
Iteration 185/1000 | Loss: 0.00002484
Iteration 186/1000 | Loss: 0.00002483
Iteration 187/1000 | Loss: 0.00002483
Iteration 188/1000 | Loss: 0.00002483
Iteration 189/1000 | Loss: 0.00002483
Iteration 190/1000 | Loss: 0.00002483
Iteration 191/1000 | Loss: 0.00002483
Iteration 192/1000 | Loss: 0.00002483
Iteration 193/1000 | Loss: 0.00002483
Iteration 194/1000 | Loss: 0.00002482
Iteration 195/1000 | Loss: 0.00002482
Iteration 196/1000 | Loss: 0.00002482
Iteration 197/1000 | Loss: 0.00002482
Iteration 198/1000 | Loss: 0.00002482
Iteration 199/1000 | Loss: 0.00002482
Iteration 200/1000 | Loss: 0.00002482
Iteration 201/1000 | Loss: 0.00002481
Iteration 202/1000 | Loss: 0.00002481
Iteration 203/1000 | Loss: 0.00002481
Iteration 204/1000 | Loss: 0.00002481
Iteration 205/1000 | Loss: 0.00002481
Iteration 206/1000 | Loss: 0.00002481
Iteration 207/1000 | Loss: 0.00002481
Iteration 208/1000 | Loss: 0.00002481
Iteration 209/1000 | Loss: 0.00002481
Iteration 210/1000 | Loss: 0.00002481
Iteration 211/1000 | Loss: 0.00002480
Iteration 212/1000 | Loss: 0.00002480
Iteration 213/1000 | Loss: 0.00002480
Iteration 214/1000 | Loss: 0.00002480
Iteration 215/1000 | Loss: 0.00002480
Iteration 216/1000 | Loss: 0.00002480
Iteration 217/1000 | Loss: 0.00002480
Iteration 218/1000 | Loss: 0.00002480
Iteration 219/1000 | Loss: 0.00002480
Iteration 220/1000 | Loss: 0.00002480
Iteration 221/1000 | Loss: 0.00002480
Iteration 222/1000 | Loss: 0.00002480
Iteration 223/1000 | Loss: 0.00002479
Iteration 224/1000 | Loss: 0.00002479
Iteration 225/1000 | Loss: 0.00002479
Iteration 226/1000 | Loss: 0.00002479
Iteration 227/1000 | Loss: 0.00002479
Iteration 228/1000 | Loss: 0.00002479
Iteration 229/1000 | Loss: 0.00002479
Iteration 230/1000 | Loss: 0.00002479
Iteration 231/1000 | Loss: 0.00002478
Iteration 232/1000 | Loss: 0.00002478
Iteration 233/1000 | Loss: 0.00002478
Iteration 234/1000 | Loss: 0.00002478
Iteration 235/1000 | Loss: 0.00002478
Iteration 236/1000 | Loss: 0.00002478
Iteration 237/1000 | Loss: 0.00002478
Iteration 238/1000 | Loss: 0.00002478
Iteration 239/1000 | Loss: 0.00002478
Iteration 240/1000 | Loss: 0.00002478
Iteration 241/1000 | Loss: 0.00002478
Iteration 242/1000 | Loss: 0.00002478
Iteration 243/1000 | Loss: 0.00002478
Iteration 244/1000 | Loss: 0.00002478
Iteration 245/1000 | Loss: 0.00002478
Iteration 246/1000 | Loss: 0.00002478
Iteration 247/1000 | Loss: 0.00002478
Iteration 248/1000 | Loss: 0.00002478
Iteration 249/1000 | Loss: 0.00002478
Iteration 250/1000 | Loss: 0.00002478
Iteration 251/1000 | Loss: 0.00002478
Iteration 252/1000 | Loss: 0.00002478
Iteration 253/1000 | Loss: 0.00002478
Iteration 254/1000 | Loss: 0.00002478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.477953785273712e-05, 2.477953785273712e-05, 2.477953785273712e-05, 2.477953785273712e-05, 2.477953785273712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.477953785273712e-05

Optimization complete. Final v2v error: 4.156672477722168 mm

Highest mean error: 5.899026393890381 mm for frame 112

Lowest mean error: 3.3270535469055176 mm for frame 20

Saving results

Total time: 76.65438675880432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755417
Iteration 2/25 | Loss: 0.00165393
Iteration 3/25 | Loss: 0.00137795
Iteration 4/25 | Loss: 0.00134716
Iteration 5/25 | Loss: 0.00134136
Iteration 6/25 | Loss: 0.00134022
Iteration 7/25 | Loss: 0.00134022
Iteration 8/25 | Loss: 0.00134022
Iteration 9/25 | Loss: 0.00134022
Iteration 10/25 | Loss: 0.00134022
Iteration 11/25 | Loss: 0.00134022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013402196345850825, 0.0013402196345850825, 0.0013402196345850825, 0.0013402196345850825, 0.0013402196345850825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013402196345850825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.13844633
Iteration 2/25 | Loss: 0.00086902
Iteration 3/25 | Loss: 0.00086902
Iteration 4/25 | Loss: 0.00086902
Iteration 5/25 | Loss: 0.00086901
Iteration 6/25 | Loss: 0.00086901
Iteration 7/25 | Loss: 0.00086901
Iteration 8/25 | Loss: 0.00086901
Iteration 9/25 | Loss: 0.00086901
Iteration 10/25 | Loss: 0.00086901
Iteration 11/25 | Loss: 0.00086901
Iteration 12/25 | Loss: 0.00086901
Iteration 13/25 | Loss: 0.00086901
Iteration 14/25 | Loss: 0.00086901
Iteration 15/25 | Loss: 0.00086901
Iteration 16/25 | Loss: 0.00086901
Iteration 17/25 | Loss: 0.00086901
Iteration 18/25 | Loss: 0.00086901
Iteration 19/25 | Loss: 0.00086901
Iteration 20/25 | Loss: 0.00086901
Iteration 21/25 | Loss: 0.00086901
Iteration 22/25 | Loss: 0.00086901
Iteration 23/25 | Loss: 0.00086901
Iteration 24/25 | Loss: 0.00086901
Iteration 25/25 | Loss: 0.00086901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086901
Iteration 2/1000 | Loss: 0.00004517
Iteration 3/1000 | Loss: 0.00003034
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002301
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002171
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002109
Iteration 11/1000 | Loss: 0.00002081
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002048
Iteration 15/1000 | Loss: 0.00002036
Iteration 16/1000 | Loss: 0.00002035
Iteration 17/1000 | Loss: 0.00002028
Iteration 18/1000 | Loss: 0.00002023
Iteration 19/1000 | Loss: 0.00002022
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00002004
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002002
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001997
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001995
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001994
Iteration 50/1000 | Loss: 0.00001994
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001992
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001991
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001990
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001990
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001989
Iteration 67/1000 | Loss: 0.00001989
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001987
Iteration 77/1000 | Loss: 0.00001986
Iteration 78/1000 | Loss: 0.00001986
Iteration 79/1000 | Loss: 0.00001986
Iteration 80/1000 | Loss: 0.00001986
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001984
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001983
Iteration 100/1000 | Loss: 0.00001983
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001982
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001981
Iteration 120/1000 | Loss: 0.00001981
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001979
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001978
Iteration 137/1000 | Loss: 0.00001978
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001977
Iteration 140/1000 | Loss: 0.00001977
Iteration 141/1000 | Loss: 0.00001977
Iteration 142/1000 | Loss: 0.00001976
Iteration 143/1000 | Loss: 0.00001976
Iteration 144/1000 | Loss: 0.00001976
Iteration 145/1000 | Loss: 0.00001976
Iteration 146/1000 | Loss: 0.00001975
Iteration 147/1000 | Loss: 0.00001975
Iteration 148/1000 | Loss: 0.00001975
Iteration 149/1000 | Loss: 0.00001975
Iteration 150/1000 | Loss: 0.00001974
Iteration 151/1000 | Loss: 0.00001974
Iteration 152/1000 | Loss: 0.00001974
Iteration 153/1000 | Loss: 0.00001974
Iteration 154/1000 | Loss: 0.00001973
Iteration 155/1000 | Loss: 0.00001973
Iteration 156/1000 | Loss: 0.00001973
Iteration 157/1000 | Loss: 0.00001973
Iteration 158/1000 | Loss: 0.00001973
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001971
Iteration 164/1000 | Loss: 0.00001971
Iteration 165/1000 | Loss: 0.00001971
Iteration 166/1000 | Loss: 0.00001971
Iteration 167/1000 | Loss: 0.00001970
Iteration 168/1000 | Loss: 0.00001970
Iteration 169/1000 | Loss: 0.00001970
Iteration 170/1000 | Loss: 0.00001970
Iteration 171/1000 | Loss: 0.00001969
Iteration 172/1000 | Loss: 0.00001969
Iteration 173/1000 | Loss: 0.00001969
Iteration 174/1000 | Loss: 0.00001969
Iteration 175/1000 | Loss: 0.00001969
Iteration 176/1000 | Loss: 0.00001969
Iteration 177/1000 | Loss: 0.00001969
Iteration 178/1000 | Loss: 0.00001969
Iteration 179/1000 | Loss: 0.00001968
Iteration 180/1000 | Loss: 0.00001968
Iteration 181/1000 | Loss: 0.00001968
Iteration 182/1000 | Loss: 0.00001968
Iteration 183/1000 | Loss: 0.00001968
Iteration 184/1000 | Loss: 0.00001968
Iteration 185/1000 | Loss: 0.00001968
Iteration 186/1000 | Loss: 0.00001968
Iteration 187/1000 | Loss: 0.00001968
Iteration 188/1000 | Loss: 0.00001968
Iteration 189/1000 | Loss: 0.00001968
Iteration 190/1000 | Loss: 0.00001968
Iteration 191/1000 | Loss: 0.00001967
Iteration 192/1000 | Loss: 0.00001967
Iteration 193/1000 | Loss: 0.00001967
Iteration 194/1000 | Loss: 0.00001967
Iteration 195/1000 | Loss: 0.00001967
Iteration 196/1000 | Loss: 0.00001967
Iteration 197/1000 | Loss: 0.00001967
Iteration 198/1000 | Loss: 0.00001967
Iteration 199/1000 | Loss: 0.00001967
Iteration 200/1000 | Loss: 0.00001966
Iteration 201/1000 | Loss: 0.00001966
Iteration 202/1000 | Loss: 0.00001966
Iteration 203/1000 | Loss: 0.00001966
Iteration 204/1000 | Loss: 0.00001966
Iteration 205/1000 | Loss: 0.00001966
Iteration 206/1000 | Loss: 0.00001966
Iteration 207/1000 | Loss: 0.00001966
Iteration 208/1000 | Loss: 0.00001966
Iteration 209/1000 | Loss: 0.00001966
Iteration 210/1000 | Loss: 0.00001966
Iteration 211/1000 | Loss: 0.00001966
Iteration 212/1000 | Loss: 0.00001966
Iteration 213/1000 | Loss: 0.00001965
Iteration 214/1000 | Loss: 0.00001965
Iteration 215/1000 | Loss: 0.00001965
Iteration 216/1000 | Loss: 0.00001965
Iteration 217/1000 | Loss: 0.00001965
Iteration 218/1000 | Loss: 0.00001965
Iteration 219/1000 | Loss: 0.00001965
Iteration 220/1000 | Loss: 0.00001965
Iteration 221/1000 | Loss: 0.00001965
Iteration 222/1000 | Loss: 0.00001965
Iteration 223/1000 | Loss: 0.00001965
Iteration 224/1000 | Loss: 0.00001964
Iteration 225/1000 | Loss: 0.00001964
Iteration 226/1000 | Loss: 0.00001964
Iteration 227/1000 | Loss: 0.00001964
Iteration 228/1000 | Loss: 0.00001964
Iteration 229/1000 | Loss: 0.00001964
Iteration 230/1000 | Loss: 0.00001964
Iteration 231/1000 | Loss: 0.00001964
Iteration 232/1000 | Loss: 0.00001964
Iteration 233/1000 | Loss: 0.00001964
Iteration 234/1000 | Loss: 0.00001963
Iteration 235/1000 | Loss: 0.00001963
Iteration 236/1000 | Loss: 0.00001963
Iteration 237/1000 | Loss: 0.00001963
Iteration 238/1000 | Loss: 0.00001963
Iteration 239/1000 | Loss: 0.00001963
Iteration 240/1000 | Loss: 0.00001963
Iteration 241/1000 | Loss: 0.00001963
Iteration 242/1000 | Loss: 0.00001963
Iteration 243/1000 | Loss: 0.00001963
Iteration 244/1000 | Loss: 0.00001963
Iteration 245/1000 | Loss: 0.00001963
Iteration 246/1000 | Loss: 0.00001963
Iteration 247/1000 | Loss: 0.00001963
Iteration 248/1000 | Loss: 0.00001963
Iteration 249/1000 | Loss: 0.00001963
Iteration 250/1000 | Loss: 0.00001963
Iteration 251/1000 | Loss: 0.00001963
Iteration 252/1000 | Loss: 0.00001963
Iteration 253/1000 | Loss: 0.00001963
Iteration 254/1000 | Loss: 0.00001963
Iteration 255/1000 | Loss: 0.00001963
Iteration 256/1000 | Loss: 0.00001963
Iteration 257/1000 | Loss: 0.00001963
Iteration 258/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.9629102098406292e-05, 1.9629102098406292e-05, 1.9629102098406292e-05, 1.9629102098406292e-05, 1.9629102098406292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9629102098406292e-05

Optimization complete. Final v2v error: 3.6488935947418213 mm

Highest mean error: 5.1010918617248535 mm for frame 161

Lowest mean error: 3.022765636444092 mm for frame 189

Saving results

Total time: 50.49962902069092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641899
Iteration 2/25 | Loss: 0.00143493
Iteration 3/25 | Loss: 0.00133259
Iteration 4/25 | Loss: 0.00132171
Iteration 5/25 | Loss: 0.00131853
Iteration 6/25 | Loss: 0.00131824
Iteration 7/25 | Loss: 0.00131824
Iteration 8/25 | Loss: 0.00131824
Iteration 9/25 | Loss: 0.00131824
Iteration 10/25 | Loss: 0.00131824
Iteration 11/25 | Loss: 0.00131824
Iteration 12/25 | Loss: 0.00131824
Iteration 13/25 | Loss: 0.00131824
Iteration 14/25 | Loss: 0.00131824
Iteration 15/25 | Loss: 0.00131824
Iteration 16/25 | Loss: 0.00131824
Iteration 17/25 | Loss: 0.00131824
Iteration 18/25 | Loss: 0.00131824
Iteration 19/25 | Loss: 0.00131824
Iteration 20/25 | Loss: 0.00131824
Iteration 21/25 | Loss: 0.00131824
Iteration 22/25 | Loss: 0.00131824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013182440306991339, 0.0013182440306991339, 0.0013182440306991339, 0.0013182440306991339, 0.0013182440306991339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013182440306991339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.13658142
Iteration 2/25 | Loss: 0.00089310
Iteration 3/25 | Loss: 0.00089304
Iteration 4/25 | Loss: 0.00089304
Iteration 5/25 | Loss: 0.00089304
Iteration 6/25 | Loss: 0.00089304
Iteration 7/25 | Loss: 0.00089304
Iteration 8/25 | Loss: 0.00089304
Iteration 9/25 | Loss: 0.00089304
Iteration 10/25 | Loss: 0.00089304
Iteration 11/25 | Loss: 0.00089304
Iteration 12/25 | Loss: 0.00089304
Iteration 13/25 | Loss: 0.00089304
Iteration 14/25 | Loss: 0.00089304
Iteration 15/25 | Loss: 0.00089304
Iteration 16/25 | Loss: 0.00089304
Iteration 17/25 | Loss: 0.00089304
Iteration 18/25 | Loss: 0.00089304
Iteration 19/25 | Loss: 0.00089304
Iteration 20/25 | Loss: 0.00089304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008930354961194098, 0.0008930354961194098, 0.0008930354961194098, 0.0008930354961194098, 0.0008930354961194098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008930354961194098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089304
Iteration 2/1000 | Loss: 0.00003135
Iteration 3/1000 | Loss: 0.00002369
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001971
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001817
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001733
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001690
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001655
Iteration 38/1000 | Loss: 0.00001655
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001653
Iteration 41/1000 | Loss: 0.00001653
Iteration 42/1000 | Loss: 0.00001653
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001652
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001642
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001641
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001640
Iteration 111/1000 | Loss: 0.00001640
Iteration 112/1000 | Loss: 0.00001640
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001640
Iteration 116/1000 | Loss: 0.00001640
Iteration 117/1000 | Loss: 0.00001640
Iteration 118/1000 | Loss: 0.00001640
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Iteration 127/1000 | Loss: 0.00001639
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001639
Iteration 131/1000 | Loss: 0.00001639
Iteration 132/1000 | Loss: 0.00001639
Iteration 133/1000 | Loss: 0.00001639
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001639
Iteration 138/1000 | Loss: 0.00001639
Iteration 139/1000 | Loss: 0.00001638
Iteration 140/1000 | Loss: 0.00001638
Iteration 141/1000 | Loss: 0.00001638
Iteration 142/1000 | Loss: 0.00001638
Iteration 143/1000 | Loss: 0.00001638
Iteration 144/1000 | Loss: 0.00001638
Iteration 145/1000 | Loss: 0.00001638
Iteration 146/1000 | Loss: 0.00001638
Iteration 147/1000 | Loss: 0.00001638
Iteration 148/1000 | Loss: 0.00001638
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001638
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001638
Iteration 162/1000 | Loss: 0.00001638
Iteration 163/1000 | Loss: 0.00001638
Iteration 164/1000 | Loss: 0.00001638
Iteration 165/1000 | Loss: 0.00001638
Iteration 166/1000 | Loss: 0.00001638
Iteration 167/1000 | Loss: 0.00001638
Iteration 168/1000 | Loss: 0.00001638
Iteration 169/1000 | Loss: 0.00001638
Iteration 170/1000 | Loss: 0.00001638
Iteration 171/1000 | Loss: 0.00001638
Iteration 172/1000 | Loss: 0.00001638
Iteration 173/1000 | Loss: 0.00001638
Iteration 174/1000 | Loss: 0.00001638
Iteration 175/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.6379304724978283e-05, 1.6379304724978283e-05, 1.6379304724978283e-05, 1.6379304724978283e-05, 1.6379304724978283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6379304724978283e-05

Optimization complete. Final v2v error: 3.407057762145996 mm

Highest mean error: 4.019508361816406 mm for frame 140

Lowest mean error: 3.168431520462036 mm for frame 16

Saving results

Total time: 41.22155165672302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493514
Iteration 2/25 | Loss: 0.00141830
Iteration 3/25 | Loss: 0.00133130
Iteration 4/25 | Loss: 0.00131715
Iteration 5/25 | Loss: 0.00131225
Iteration 6/25 | Loss: 0.00131189
Iteration 7/25 | Loss: 0.00131189
Iteration 8/25 | Loss: 0.00131189
Iteration 9/25 | Loss: 0.00131189
Iteration 10/25 | Loss: 0.00131189
Iteration 11/25 | Loss: 0.00131189
Iteration 12/25 | Loss: 0.00131189
Iteration 13/25 | Loss: 0.00131189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013118876377120614, 0.0013118876377120614, 0.0013118876377120614, 0.0013118876377120614, 0.0013118876377120614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013118876377120614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40571237
Iteration 2/25 | Loss: 0.00090145
Iteration 3/25 | Loss: 0.00090145
Iteration 4/25 | Loss: 0.00090145
Iteration 5/25 | Loss: 0.00090144
Iteration 6/25 | Loss: 0.00090144
Iteration 7/25 | Loss: 0.00090144
Iteration 8/25 | Loss: 0.00090144
Iteration 9/25 | Loss: 0.00090144
Iteration 10/25 | Loss: 0.00090144
Iteration 11/25 | Loss: 0.00090144
Iteration 12/25 | Loss: 0.00090144
Iteration 13/25 | Loss: 0.00090144
Iteration 14/25 | Loss: 0.00090144
Iteration 15/25 | Loss: 0.00090144
Iteration 16/25 | Loss: 0.00090144
Iteration 17/25 | Loss: 0.00090144
Iteration 18/25 | Loss: 0.00090144
Iteration 19/25 | Loss: 0.00090144
Iteration 20/25 | Loss: 0.00090144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009014417300932109, 0.0009014417300932109, 0.0009014417300932109, 0.0009014417300932109, 0.0009014417300932109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009014417300932109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090144
Iteration 2/1000 | Loss: 0.00004879
Iteration 3/1000 | Loss: 0.00003315
Iteration 4/1000 | Loss: 0.00002887
Iteration 5/1000 | Loss: 0.00002697
Iteration 6/1000 | Loss: 0.00002599
Iteration 7/1000 | Loss: 0.00002531
Iteration 8/1000 | Loss: 0.00002473
Iteration 9/1000 | Loss: 0.00002435
Iteration 10/1000 | Loss: 0.00002405
Iteration 11/1000 | Loss: 0.00002379
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002346
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002345
Iteration 17/1000 | Loss: 0.00002345
Iteration 18/1000 | Loss: 0.00002344
Iteration 19/1000 | Loss: 0.00002344
Iteration 20/1000 | Loss: 0.00002343
Iteration 21/1000 | Loss: 0.00002342
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002340
Iteration 24/1000 | Loss: 0.00002339
Iteration 25/1000 | Loss: 0.00002338
Iteration 26/1000 | Loss: 0.00002334
Iteration 27/1000 | Loss: 0.00002332
Iteration 28/1000 | Loss: 0.00002331
Iteration 29/1000 | Loss: 0.00002325
Iteration 30/1000 | Loss: 0.00002325
Iteration 31/1000 | Loss: 0.00002325
Iteration 32/1000 | Loss: 0.00002325
Iteration 33/1000 | Loss: 0.00002324
Iteration 34/1000 | Loss: 0.00002324
Iteration 35/1000 | Loss: 0.00002324
Iteration 36/1000 | Loss: 0.00002324
Iteration 37/1000 | Loss: 0.00002324
Iteration 38/1000 | Loss: 0.00002323
Iteration 39/1000 | Loss: 0.00002323
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002321
Iteration 42/1000 | Loss: 0.00002320
Iteration 43/1000 | Loss: 0.00002319
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002302
Iteration 47/1000 | Loss: 0.00002302
Iteration 48/1000 | Loss: 0.00002302
Iteration 49/1000 | Loss: 0.00002302
Iteration 50/1000 | Loss: 0.00002302
Iteration 51/1000 | Loss: 0.00002302
Iteration 52/1000 | Loss: 0.00002302
Iteration 53/1000 | Loss: 0.00002302
Iteration 54/1000 | Loss: 0.00002302
Iteration 55/1000 | Loss: 0.00002302
Iteration 56/1000 | Loss: 0.00002302
Iteration 57/1000 | Loss: 0.00002302
Iteration 58/1000 | Loss: 0.00002301
Iteration 59/1000 | Loss: 0.00002301
Iteration 60/1000 | Loss: 0.00002301
Iteration 61/1000 | Loss: 0.00002301
Iteration 62/1000 | Loss: 0.00002301
Iteration 63/1000 | Loss: 0.00002301
Iteration 64/1000 | Loss: 0.00002301
Iteration 65/1000 | Loss: 0.00002301
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002301
Iteration 70/1000 | Loss: 0.00002301
Iteration 71/1000 | Loss: 0.00002301
Iteration 72/1000 | Loss: 0.00002301
Iteration 73/1000 | Loss: 0.00002301
Iteration 74/1000 | Loss: 0.00002301
Iteration 75/1000 | Loss: 0.00002301
Iteration 76/1000 | Loss: 0.00002301
Iteration 77/1000 | Loss: 0.00002301
Iteration 78/1000 | Loss: 0.00002301
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002301
Iteration 82/1000 | Loss: 0.00002301
Iteration 83/1000 | Loss: 0.00002301
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002301
Iteration 86/1000 | Loss: 0.00002301
Iteration 87/1000 | Loss: 0.00002301
Iteration 88/1000 | Loss: 0.00002301
Iteration 89/1000 | Loss: 0.00002301
Iteration 90/1000 | Loss: 0.00002301
Iteration 91/1000 | Loss: 0.00002301
Iteration 92/1000 | Loss: 0.00002301
Iteration 93/1000 | Loss: 0.00002301
Iteration 94/1000 | Loss: 0.00002301
Iteration 95/1000 | Loss: 0.00002301
Iteration 96/1000 | Loss: 0.00002301
Iteration 97/1000 | Loss: 0.00002301
Iteration 98/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.3011678422335535e-05, 2.3011678422335535e-05, 2.3011678422335535e-05, 2.3011678422335535e-05, 2.3011678422335535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3011678422335535e-05

Optimization complete. Final v2v error: 3.790867328643799 mm

Highest mean error: 4.789936542510986 mm for frame 78

Lowest mean error: 3.362265110015869 mm for frame 113

Saving results

Total time: 35.1107964515686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397193
Iteration 2/25 | Loss: 0.00131929
Iteration 3/25 | Loss: 0.00125231
Iteration 4/25 | Loss: 0.00124208
Iteration 5/25 | Loss: 0.00124082
Iteration 6/25 | Loss: 0.00124082
Iteration 7/25 | Loss: 0.00124082
Iteration 8/25 | Loss: 0.00124082
Iteration 9/25 | Loss: 0.00124082
Iteration 10/25 | Loss: 0.00124082
Iteration 11/25 | Loss: 0.00124082
Iteration 12/25 | Loss: 0.00124082
Iteration 13/25 | Loss: 0.00124082
Iteration 14/25 | Loss: 0.00124082
Iteration 15/25 | Loss: 0.00124082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012408194597810507, 0.0012408194597810507, 0.0012408194597810507, 0.0012408194597810507, 0.0012408194597810507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012408194597810507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42799890
Iteration 2/25 | Loss: 0.00078308
Iteration 3/25 | Loss: 0.00078308
Iteration 4/25 | Loss: 0.00078308
Iteration 5/25 | Loss: 0.00078308
Iteration 6/25 | Loss: 0.00078308
Iteration 7/25 | Loss: 0.00078308
Iteration 8/25 | Loss: 0.00078308
Iteration 9/25 | Loss: 0.00078308
Iteration 10/25 | Loss: 0.00078308
Iteration 11/25 | Loss: 0.00078308
Iteration 12/25 | Loss: 0.00078308
Iteration 13/25 | Loss: 0.00078308
Iteration 14/25 | Loss: 0.00078308
Iteration 15/25 | Loss: 0.00078308
Iteration 16/25 | Loss: 0.00078308
Iteration 17/25 | Loss: 0.00078308
Iteration 18/25 | Loss: 0.00078308
Iteration 19/25 | Loss: 0.00078308
Iteration 20/25 | Loss: 0.00078308
Iteration 21/25 | Loss: 0.00078308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007830774993635714, 0.0007830774993635714, 0.0007830774993635714, 0.0007830774993635714, 0.0007830774993635714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007830774993635714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078308
Iteration 2/1000 | Loss: 0.00002211
Iteration 3/1000 | Loss: 0.00001867
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001426
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001408
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001405
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001379
Iteration 127/1000 | Loss: 0.00001379
Iteration 128/1000 | Loss: 0.00001379
Iteration 129/1000 | Loss: 0.00001378
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001378
Iteration 132/1000 | Loss: 0.00001378
Iteration 133/1000 | Loss: 0.00001378
Iteration 134/1000 | Loss: 0.00001378
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001378
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.3775535990134813e-05, 1.3775535990134813e-05, 1.3775535990134813e-05, 1.3775535990134813e-05, 1.3775535990134813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3775535990134813e-05

Optimization complete. Final v2v error: 3.1512627601623535 mm

Highest mean error: 3.239165782928467 mm for frame 208

Lowest mean error: 3.0689501762390137 mm for frame 231

Saving results

Total time: 42.68092203140259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786859
Iteration 2/25 | Loss: 0.00148237
Iteration 3/25 | Loss: 0.00127952
Iteration 4/25 | Loss: 0.00126497
Iteration 5/25 | Loss: 0.00125957
Iteration 6/25 | Loss: 0.00125812
Iteration 7/25 | Loss: 0.00125812
Iteration 8/25 | Loss: 0.00125812
Iteration 9/25 | Loss: 0.00125812
Iteration 10/25 | Loss: 0.00125812
Iteration 11/25 | Loss: 0.00125812
Iteration 12/25 | Loss: 0.00125812
Iteration 13/25 | Loss: 0.00125812
Iteration 14/25 | Loss: 0.00125812
Iteration 15/25 | Loss: 0.00125812
Iteration 16/25 | Loss: 0.00125812
Iteration 17/25 | Loss: 0.00125812
Iteration 18/25 | Loss: 0.00125812
Iteration 19/25 | Loss: 0.00125812
Iteration 20/25 | Loss: 0.00125812
Iteration 21/25 | Loss: 0.00125812
Iteration 22/25 | Loss: 0.00125812
Iteration 23/25 | Loss: 0.00125812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012581232003867626, 0.0012581232003867626, 0.0012581232003867626, 0.0012581232003867626, 0.0012581232003867626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012581232003867626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21914709
Iteration 2/25 | Loss: 0.00088809
Iteration 3/25 | Loss: 0.00088809
Iteration 4/25 | Loss: 0.00088809
Iteration 5/25 | Loss: 0.00088809
Iteration 6/25 | Loss: 0.00088809
Iteration 7/25 | Loss: 0.00088809
Iteration 8/25 | Loss: 0.00088809
Iteration 9/25 | Loss: 0.00088808
Iteration 10/25 | Loss: 0.00088808
Iteration 11/25 | Loss: 0.00088808
Iteration 12/25 | Loss: 0.00088808
Iteration 13/25 | Loss: 0.00088808
Iteration 14/25 | Loss: 0.00088808
Iteration 15/25 | Loss: 0.00088808
Iteration 16/25 | Loss: 0.00088808
Iteration 17/25 | Loss: 0.00088808
Iteration 18/25 | Loss: 0.00088808
Iteration 19/25 | Loss: 0.00088808
Iteration 20/25 | Loss: 0.00088808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008880842942744493, 0.0008880842942744493, 0.0008880842942744493, 0.0008880842942744493, 0.0008880842942744493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008880842942744493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088808
Iteration 2/1000 | Loss: 0.00004297
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00002135
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001721
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001595
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001593
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001592
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001576
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001560
Iteration 83/1000 | Loss: 0.00001560
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001559
Iteration 87/1000 | Loss: 0.00001559
Iteration 88/1000 | Loss: 0.00001559
Iteration 89/1000 | Loss: 0.00001559
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001552
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001551
Iteration 139/1000 | Loss: 0.00001551
Iteration 140/1000 | Loss: 0.00001551
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001551
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001551
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001551
Iteration 158/1000 | Loss: 0.00001551
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.5505724149988964e-05, 1.5505724149988964e-05, 1.5505724149988964e-05, 1.5505724149988964e-05, 1.5505724149988964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5505724149988964e-05

Optimization complete. Final v2v error: 3.2784006595611572 mm

Highest mean error: 4.517580986022949 mm for frame 69

Lowest mean error: 2.797255754470825 mm for frame 102

Saving results

Total time: 44.001781702041626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767440
Iteration 2/25 | Loss: 0.00144306
Iteration 3/25 | Loss: 0.00131196
Iteration 4/25 | Loss: 0.00130196
Iteration 5/25 | Loss: 0.00130028
Iteration 6/25 | Loss: 0.00130028
Iteration 7/25 | Loss: 0.00130028
Iteration 8/25 | Loss: 0.00130028
Iteration 9/25 | Loss: 0.00130028
Iteration 10/25 | Loss: 0.00130028
Iteration 11/25 | Loss: 0.00130028
Iteration 12/25 | Loss: 0.00130028
Iteration 13/25 | Loss: 0.00130028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013002767227590084, 0.0013002767227590084, 0.0013002767227590084, 0.0013002767227590084, 0.0013002767227590084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013002767227590084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39694238
Iteration 2/25 | Loss: 0.00067064
Iteration 3/25 | Loss: 0.00067064
Iteration 4/25 | Loss: 0.00067064
Iteration 5/25 | Loss: 0.00067064
Iteration 6/25 | Loss: 0.00067064
Iteration 7/25 | Loss: 0.00067064
Iteration 8/25 | Loss: 0.00067064
Iteration 9/25 | Loss: 0.00067064
Iteration 10/25 | Loss: 0.00067064
Iteration 11/25 | Loss: 0.00067064
Iteration 12/25 | Loss: 0.00067064
Iteration 13/25 | Loss: 0.00067064
Iteration 14/25 | Loss: 0.00067064
Iteration 15/25 | Loss: 0.00067064
Iteration 16/25 | Loss: 0.00067064
Iteration 17/25 | Loss: 0.00067064
Iteration 18/25 | Loss: 0.00067064
Iteration 19/25 | Loss: 0.00067064
Iteration 20/25 | Loss: 0.00067064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006706406711600721, 0.0006706406711600721, 0.0006706406711600721, 0.0006706406711600721, 0.0006706406711600721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006706406711600721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067064
Iteration 2/1000 | Loss: 0.00003916
Iteration 3/1000 | Loss: 0.00002763
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002094
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001846
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00001763
Iteration 35/1000 | Loss: 0.00001763
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001761
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00001743
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001740
Iteration 68/1000 | Loss: 0.00001740
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001740
Iteration 71/1000 | Loss: 0.00001740
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001739
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001737
Iteration 78/1000 | Loss: 0.00001737
Iteration 79/1000 | Loss: 0.00001737
Iteration 80/1000 | Loss: 0.00001736
Iteration 81/1000 | Loss: 0.00001736
Iteration 82/1000 | Loss: 0.00001735
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001734
Iteration 86/1000 | Loss: 0.00001734
Iteration 87/1000 | Loss: 0.00001734
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001733
Iteration 90/1000 | Loss: 0.00001733
Iteration 91/1000 | Loss: 0.00001733
Iteration 92/1000 | Loss: 0.00001733
Iteration 93/1000 | Loss: 0.00001733
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001728
Iteration 124/1000 | Loss: 0.00001728
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001728
Iteration 128/1000 | Loss: 0.00001728
Iteration 129/1000 | Loss: 0.00001728
Iteration 130/1000 | Loss: 0.00001728
Iteration 131/1000 | Loss: 0.00001728
Iteration 132/1000 | Loss: 0.00001728
Iteration 133/1000 | Loss: 0.00001728
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001728
Iteration 136/1000 | Loss: 0.00001728
Iteration 137/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.728041206661146e-05, 1.728041206661146e-05, 1.728041206661146e-05, 1.728041206661146e-05, 1.728041206661146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728041206661146e-05

Optimization complete. Final v2v error: 3.513078212738037 mm

Highest mean error: 4.045843601226807 mm for frame 232

Lowest mean error: 2.9823877811431885 mm for frame 78

Saving results

Total time: 43.95623326301575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016210
Iteration 2/25 | Loss: 0.01016210
Iteration 3/25 | Loss: 0.01016210
Iteration 4/25 | Loss: 0.01016209
Iteration 5/25 | Loss: 0.01016208
Iteration 6/25 | Loss: 0.00195132
Iteration 7/25 | Loss: 0.00162468
Iteration 8/25 | Loss: 0.00150624
Iteration 9/25 | Loss: 0.00144747
Iteration 10/25 | Loss: 0.00141579
Iteration 11/25 | Loss: 0.00137636
Iteration 12/25 | Loss: 0.00138177
Iteration 13/25 | Loss: 0.00136650
Iteration 14/25 | Loss: 0.00135847
Iteration 15/25 | Loss: 0.00135619
Iteration 16/25 | Loss: 0.00135468
Iteration 17/25 | Loss: 0.00136897
Iteration 18/25 | Loss: 0.00134014
Iteration 19/25 | Loss: 0.00134421
Iteration 20/25 | Loss: 0.00134075
Iteration 21/25 | Loss: 0.00133725
Iteration 22/25 | Loss: 0.00132776
Iteration 23/25 | Loss: 0.00133248
Iteration 24/25 | Loss: 0.00133041
Iteration 25/25 | Loss: 0.00132408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43495131
Iteration 2/25 | Loss: 0.00196247
Iteration 3/25 | Loss: 0.00137980
Iteration 4/25 | Loss: 0.00137950
Iteration 5/25 | Loss: 0.00137949
Iteration 6/25 | Loss: 0.00137949
Iteration 7/25 | Loss: 0.00137949
Iteration 8/25 | Loss: 0.00137949
Iteration 9/25 | Loss: 0.00137949
Iteration 10/25 | Loss: 0.00137949
Iteration 11/25 | Loss: 0.00138382
Iteration 12/25 | Loss: 0.00138316
Iteration 13/25 | Loss: 0.00138315
Iteration 14/25 | Loss: 0.00138315
Iteration 15/25 | Loss: 0.00138315
Iteration 16/25 | Loss: 0.00138315
Iteration 17/25 | Loss: 0.00138315
Iteration 18/25 | Loss: 0.00138315
Iteration 19/25 | Loss: 0.00138315
Iteration 20/25 | Loss: 0.00138315
Iteration 21/25 | Loss: 0.00138315
Iteration 22/25 | Loss: 0.00138315
Iteration 23/25 | Loss: 0.00138315
Iteration 24/25 | Loss: 0.00138315
Iteration 25/25 | Loss: 0.00138315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138315
Iteration 2/1000 | Loss: 0.00060964
Iteration 3/1000 | Loss: 0.00323083
Iteration 4/1000 | Loss: 0.00051247
Iteration 5/1000 | Loss: 0.00018153
Iteration 6/1000 | Loss: 0.00011528
Iteration 7/1000 | Loss: 0.00024344
Iteration 8/1000 | Loss: 0.00019693
Iteration 9/1000 | Loss: 0.00021122
Iteration 10/1000 | Loss: 0.00056735
Iteration 11/1000 | Loss: 0.00013103
Iteration 12/1000 | Loss: 0.00031542
Iteration 13/1000 | Loss: 0.00028377
Iteration 14/1000 | Loss: 0.00017030
Iteration 15/1000 | Loss: 0.00019458
Iteration 16/1000 | Loss: 0.00017318
Iteration 17/1000 | Loss: 0.00045022
Iteration 18/1000 | Loss: 0.00013183
Iteration 19/1000 | Loss: 0.00018119
Iteration 20/1000 | Loss: 0.00012745
Iteration 21/1000 | Loss: 0.00017783
Iteration 22/1000 | Loss: 0.00018352
Iteration 23/1000 | Loss: 0.00005077
Iteration 24/1000 | Loss: 0.00035860
Iteration 25/1000 | Loss: 0.00010169
Iteration 26/1000 | Loss: 0.00007484
Iteration 27/1000 | Loss: 0.00036680
Iteration 28/1000 | Loss: 0.00025248
Iteration 29/1000 | Loss: 0.00023382
Iteration 30/1000 | Loss: 0.00012007
Iteration 31/1000 | Loss: 0.00006335
Iteration 32/1000 | Loss: 0.00006838
Iteration 33/1000 | Loss: 0.00033269
Iteration 34/1000 | Loss: 0.00016476
Iteration 35/1000 | Loss: 0.00023154
Iteration 36/1000 | Loss: 0.00007257
Iteration 37/1000 | Loss: 0.00009360
Iteration 38/1000 | Loss: 0.00006857
Iteration 39/1000 | Loss: 0.00016667
Iteration 40/1000 | Loss: 0.00016556
Iteration 41/1000 | Loss: 0.00006610
Iteration 42/1000 | Loss: 0.00017582
Iteration 43/1000 | Loss: 0.00017539
Iteration 44/1000 | Loss: 0.00053408
Iteration 45/1000 | Loss: 0.00020692
Iteration 46/1000 | Loss: 0.00010357
Iteration 47/1000 | Loss: 0.00013311
Iteration 48/1000 | Loss: 0.00025994
Iteration 49/1000 | Loss: 0.00024183
Iteration 50/1000 | Loss: 0.00014330
Iteration 51/1000 | Loss: 0.00015840
Iteration 52/1000 | Loss: 0.00010997
Iteration 53/1000 | Loss: 0.00031231
Iteration 54/1000 | Loss: 0.00027137
Iteration 55/1000 | Loss: 0.00015603
Iteration 56/1000 | Loss: 0.00070206
Iteration 57/1000 | Loss: 0.00036441
Iteration 58/1000 | Loss: 0.00010889
Iteration 59/1000 | Loss: 0.00027604
Iteration 60/1000 | Loss: 0.00014497
Iteration 61/1000 | Loss: 0.00027403
Iteration 62/1000 | Loss: 0.00013031
Iteration 63/1000 | Loss: 0.00004843
Iteration 64/1000 | Loss: 0.00020406
Iteration 65/1000 | Loss: 0.00013871
Iteration 66/1000 | Loss: 0.00017262
Iteration 67/1000 | Loss: 0.00058404
Iteration 68/1000 | Loss: 0.00019093
Iteration 69/1000 | Loss: 0.00014012
Iteration 70/1000 | Loss: 0.00014009
Iteration 71/1000 | Loss: 0.00012641
Iteration 72/1000 | Loss: 0.00012255
Iteration 73/1000 | Loss: 0.00007003
Iteration 74/1000 | Loss: 0.00010687
Iteration 75/1000 | Loss: 0.00013043
Iteration 76/1000 | Loss: 0.00008855
Iteration 77/1000 | Loss: 0.00017298
Iteration 78/1000 | Loss: 0.00009362
Iteration 79/1000 | Loss: 0.00010381
Iteration 80/1000 | Loss: 0.00004217
Iteration 81/1000 | Loss: 0.00010503
Iteration 82/1000 | Loss: 0.00014089
Iteration 83/1000 | Loss: 0.00004798
Iteration 84/1000 | Loss: 0.00017505
Iteration 85/1000 | Loss: 0.00012752
Iteration 86/1000 | Loss: 0.00007663
Iteration 87/1000 | Loss: 0.00014294
Iteration 88/1000 | Loss: 0.00012501
Iteration 89/1000 | Loss: 0.00016833
Iteration 90/1000 | Loss: 0.00005451
Iteration 91/1000 | Loss: 0.00003956
Iteration 92/1000 | Loss: 0.00010746
Iteration 93/1000 | Loss: 0.00005053
Iteration 94/1000 | Loss: 0.00004480
Iteration 95/1000 | Loss: 0.00007703
Iteration 96/1000 | Loss: 0.00003845
Iteration 97/1000 | Loss: 0.00004842
Iteration 98/1000 | Loss: 0.00003803
Iteration 99/1000 | Loss: 0.00021528
Iteration 100/1000 | Loss: 0.00021311
Iteration 101/1000 | Loss: 0.00015674
Iteration 102/1000 | Loss: 0.00014319
Iteration 103/1000 | Loss: 0.00008527
Iteration 104/1000 | Loss: 0.00004969
Iteration 105/1000 | Loss: 0.00004700
Iteration 106/1000 | Loss: 0.00025542
Iteration 107/1000 | Loss: 0.00019291
Iteration 108/1000 | Loss: 0.00006289
Iteration 109/1000 | Loss: 0.00006420
Iteration 110/1000 | Loss: 0.00004657
Iteration 111/1000 | Loss: 0.00006102
Iteration 112/1000 | Loss: 0.00009105
Iteration 113/1000 | Loss: 0.00006918
Iteration 114/1000 | Loss: 0.00004065
Iteration 115/1000 | Loss: 0.00004486
Iteration 116/1000 | Loss: 0.00003902
Iteration 117/1000 | Loss: 0.00003707
Iteration 118/1000 | Loss: 0.00003698
Iteration 119/1000 | Loss: 0.00003695
Iteration 120/1000 | Loss: 0.00005043
Iteration 121/1000 | Loss: 0.00003656
Iteration 122/1000 | Loss: 0.00003652
Iteration 123/1000 | Loss: 0.00003634
Iteration 124/1000 | Loss: 0.00003633
Iteration 125/1000 | Loss: 0.00003633
Iteration 126/1000 | Loss: 0.00003632
Iteration 127/1000 | Loss: 0.00003631
Iteration 128/1000 | Loss: 0.00003630
Iteration 129/1000 | Loss: 0.00003630
Iteration 130/1000 | Loss: 0.00003628
Iteration 131/1000 | Loss: 0.00003627
Iteration 132/1000 | Loss: 0.00003626
Iteration 133/1000 | Loss: 0.00003626
Iteration 134/1000 | Loss: 0.00003625
Iteration 135/1000 | Loss: 0.00003625
Iteration 136/1000 | Loss: 0.00003625
Iteration 137/1000 | Loss: 0.00003624
Iteration 138/1000 | Loss: 0.00003624
Iteration 139/1000 | Loss: 0.00003624
Iteration 140/1000 | Loss: 0.00003624
Iteration 141/1000 | Loss: 0.00003624
Iteration 142/1000 | Loss: 0.00003624
Iteration 143/1000 | Loss: 0.00003623
Iteration 144/1000 | Loss: 0.00003623
Iteration 145/1000 | Loss: 0.00003623
Iteration 146/1000 | Loss: 0.00003623
Iteration 147/1000 | Loss: 0.00003623
Iteration 148/1000 | Loss: 0.00003623
Iteration 149/1000 | Loss: 0.00003623
Iteration 150/1000 | Loss: 0.00003623
Iteration 151/1000 | Loss: 0.00003622
Iteration 152/1000 | Loss: 0.00003622
Iteration 153/1000 | Loss: 0.00003621
Iteration 154/1000 | Loss: 0.00003620
Iteration 155/1000 | Loss: 0.00003619
Iteration 156/1000 | Loss: 0.00003619
Iteration 157/1000 | Loss: 0.00003619
Iteration 158/1000 | Loss: 0.00003619
Iteration 159/1000 | Loss: 0.00003619
Iteration 160/1000 | Loss: 0.00003619
Iteration 161/1000 | Loss: 0.00003618
Iteration 162/1000 | Loss: 0.00003618
Iteration 163/1000 | Loss: 0.00008030
Iteration 164/1000 | Loss: 0.00003614
Iteration 165/1000 | Loss: 0.00003613
Iteration 166/1000 | Loss: 0.00003613
Iteration 167/1000 | Loss: 0.00003612
Iteration 168/1000 | Loss: 0.00003612
Iteration 169/1000 | Loss: 0.00003611
Iteration 170/1000 | Loss: 0.00003610
Iteration 171/1000 | Loss: 0.00003610
Iteration 172/1000 | Loss: 0.00003610
Iteration 173/1000 | Loss: 0.00003609
Iteration 174/1000 | Loss: 0.00003608
Iteration 175/1000 | Loss: 0.00003608
Iteration 176/1000 | Loss: 0.00003608
Iteration 177/1000 | Loss: 0.00003608
Iteration 178/1000 | Loss: 0.00003607
Iteration 179/1000 | Loss: 0.00003607
Iteration 180/1000 | Loss: 0.00003606
Iteration 181/1000 | Loss: 0.00003606
Iteration 182/1000 | Loss: 0.00003606
Iteration 183/1000 | Loss: 0.00003605
Iteration 184/1000 | Loss: 0.00003605
Iteration 185/1000 | Loss: 0.00003604
Iteration 186/1000 | Loss: 0.00003604
Iteration 187/1000 | Loss: 0.00003604
Iteration 188/1000 | Loss: 0.00003604
Iteration 189/1000 | Loss: 0.00003604
Iteration 190/1000 | Loss: 0.00003604
Iteration 191/1000 | Loss: 0.00003604
Iteration 192/1000 | Loss: 0.00003604
Iteration 193/1000 | Loss: 0.00003604
Iteration 194/1000 | Loss: 0.00003603
Iteration 195/1000 | Loss: 0.00003603
Iteration 196/1000 | Loss: 0.00003603
Iteration 197/1000 | Loss: 0.00003602
Iteration 198/1000 | Loss: 0.00003602
Iteration 199/1000 | Loss: 0.00003602
Iteration 200/1000 | Loss: 0.00003602
Iteration 201/1000 | Loss: 0.00003602
Iteration 202/1000 | Loss: 0.00003601
Iteration 203/1000 | Loss: 0.00028363
Iteration 204/1000 | Loss: 0.00019387
Iteration 205/1000 | Loss: 0.00019069
Iteration 206/1000 | Loss: 0.00024120
Iteration 207/1000 | Loss: 0.00016915
Iteration 208/1000 | Loss: 0.00027930
Iteration 209/1000 | Loss: 0.00011879
Iteration 210/1000 | Loss: 0.00010105
Iteration 211/1000 | Loss: 0.00004785
Iteration 212/1000 | Loss: 0.00014359
Iteration 213/1000 | Loss: 0.00003679
Iteration 214/1000 | Loss: 0.00004534
Iteration 215/1000 | Loss: 0.00003561
Iteration 216/1000 | Loss: 0.00028213
Iteration 217/1000 | Loss: 0.00004538
Iteration 218/1000 | Loss: 0.00018724
Iteration 219/1000 | Loss: 0.00003742
Iteration 220/1000 | Loss: 0.00004214
Iteration 221/1000 | Loss: 0.00004929
Iteration 222/1000 | Loss: 0.00005844
Iteration 223/1000 | Loss: 0.00003600
Iteration 224/1000 | Loss: 0.00003482
Iteration 225/1000 | Loss: 0.00006264
Iteration 226/1000 | Loss: 0.00005008
Iteration 227/1000 | Loss: 0.00004298
Iteration 228/1000 | Loss: 0.00003401
Iteration 229/1000 | Loss: 0.00005295
Iteration 230/1000 | Loss: 0.00004970
Iteration 231/1000 | Loss: 0.00004148
Iteration 232/1000 | Loss: 0.00003666
Iteration 233/1000 | Loss: 0.00003362
Iteration 234/1000 | Loss: 0.00003713
Iteration 235/1000 | Loss: 0.00003785
Iteration 236/1000 | Loss: 0.00004449
Iteration 237/1000 | Loss: 0.00003349
Iteration 238/1000 | Loss: 0.00003348
Iteration 239/1000 | Loss: 0.00003348
Iteration 240/1000 | Loss: 0.00003620
Iteration 241/1000 | Loss: 0.00003371
Iteration 242/1000 | Loss: 0.00003339
Iteration 243/1000 | Loss: 0.00003339
Iteration 244/1000 | Loss: 0.00003339
Iteration 245/1000 | Loss: 0.00003339
Iteration 246/1000 | Loss: 0.00003338
Iteration 247/1000 | Loss: 0.00003338
Iteration 248/1000 | Loss: 0.00003338
Iteration 249/1000 | Loss: 0.00003338
Iteration 250/1000 | Loss: 0.00003338
Iteration 251/1000 | Loss: 0.00003338
Iteration 252/1000 | Loss: 0.00003338
Iteration 253/1000 | Loss: 0.00003338
Iteration 254/1000 | Loss: 0.00003338
Iteration 255/1000 | Loss: 0.00003345
Iteration 256/1000 | Loss: 0.00003337
Iteration 257/1000 | Loss: 0.00003337
Iteration 258/1000 | Loss: 0.00003337
Iteration 259/1000 | Loss: 0.00003337
Iteration 260/1000 | Loss: 0.00003337
Iteration 261/1000 | Loss: 0.00003337
Iteration 262/1000 | Loss: 0.00003337
Iteration 263/1000 | Loss: 0.00003337
Iteration 264/1000 | Loss: 0.00003337
Iteration 265/1000 | Loss: 0.00003337
Iteration 266/1000 | Loss: 0.00003337
Iteration 267/1000 | Loss: 0.00003337
Iteration 268/1000 | Loss: 0.00003337
Iteration 269/1000 | Loss: 0.00003337
Iteration 270/1000 | Loss: 0.00003337
Iteration 271/1000 | Loss: 0.00003337
Iteration 272/1000 | Loss: 0.00003337
Iteration 273/1000 | Loss: 0.00003337
Iteration 274/1000 | Loss: 0.00003337
Iteration 275/1000 | Loss: 0.00003337
Iteration 276/1000 | Loss: 0.00003337
Iteration 277/1000 | Loss: 0.00003337
Iteration 278/1000 | Loss: 0.00003337
Iteration 279/1000 | Loss: 0.00003337
Iteration 280/1000 | Loss: 0.00003337
Iteration 281/1000 | Loss: 0.00003337
Iteration 282/1000 | Loss: 0.00003337
Iteration 283/1000 | Loss: 0.00003337
Iteration 284/1000 | Loss: 0.00003337
Iteration 285/1000 | Loss: 0.00003337
Iteration 286/1000 | Loss: 0.00003337
Iteration 287/1000 | Loss: 0.00003337
Iteration 288/1000 | Loss: 0.00003337
Iteration 289/1000 | Loss: 0.00003337
Iteration 290/1000 | Loss: 0.00003337
Iteration 291/1000 | Loss: 0.00003337
Iteration 292/1000 | Loss: 0.00003337
Iteration 293/1000 | Loss: 0.00003337
Iteration 294/1000 | Loss: 0.00003337
Iteration 295/1000 | Loss: 0.00003337
Iteration 296/1000 | Loss: 0.00003337
Iteration 297/1000 | Loss: 0.00003337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [3.337071757414378e-05, 3.337071757414378e-05, 3.337071757414378e-05, 3.337071757414378e-05, 3.337071757414378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.337071757414378e-05

Optimization complete. Final v2v error: 3.631248950958252 mm

Highest mean error: 10.39883804321289 mm for frame 180

Lowest mean error: 2.984877586364746 mm for frame 33

Saving results

Total time: 307.0128493309021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411688
Iteration 2/25 | Loss: 0.00148468
Iteration 3/25 | Loss: 0.00127945
Iteration 4/25 | Loss: 0.00126384
Iteration 5/25 | Loss: 0.00126166
Iteration 6/25 | Loss: 0.00126109
Iteration 7/25 | Loss: 0.00126109
Iteration 8/25 | Loss: 0.00126109
Iteration 9/25 | Loss: 0.00126109
Iteration 10/25 | Loss: 0.00126109
Iteration 11/25 | Loss: 0.00126109
Iteration 12/25 | Loss: 0.00126109
Iteration 13/25 | Loss: 0.00126109
Iteration 14/25 | Loss: 0.00126109
Iteration 15/25 | Loss: 0.00126109
Iteration 16/25 | Loss: 0.00126109
Iteration 17/25 | Loss: 0.00126109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012610908597707748, 0.0012610908597707748, 0.0012610908597707748, 0.0012610908597707748, 0.0012610908597707748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012610908597707748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42067528
Iteration 2/25 | Loss: 0.00066872
Iteration 3/25 | Loss: 0.00066872
Iteration 4/25 | Loss: 0.00066872
Iteration 5/25 | Loss: 0.00066872
Iteration 6/25 | Loss: 0.00066872
Iteration 7/25 | Loss: 0.00066872
Iteration 8/25 | Loss: 0.00066872
Iteration 9/25 | Loss: 0.00066872
Iteration 10/25 | Loss: 0.00066872
Iteration 11/25 | Loss: 0.00066872
Iteration 12/25 | Loss: 0.00066872
Iteration 13/25 | Loss: 0.00066872
Iteration 14/25 | Loss: 0.00066872
Iteration 15/25 | Loss: 0.00066872
Iteration 16/25 | Loss: 0.00066872
Iteration 17/25 | Loss: 0.00066872
Iteration 18/25 | Loss: 0.00066872
Iteration 19/25 | Loss: 0.00066872
Iteration 20/25 | Loss: 0.00066872
Iteration 21/25 | Loss: 0.00066872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006687160348519683, 0.0006687160348519683, 0.0006687160348519683, 0.0006687160348519683, 0.0006687160348519683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006687160348519683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066872
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001741
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001602
Iteration 10/1000 | Loss: 0.00001590
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001548
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001536
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001532
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001507
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001496
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001492
Iteration 88/1000 | Loss: 0.00001492
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001492
Iteration 92/1000 | Loss: 0.00001492
Iteration 93/1000 | Loss: 0.00001492
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001491
Iteration 100/1000 | Loss: 0.00001491
Iteration 101/1000 | Loss: 0.00001491
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001490
Iteration 104/1000 | Loss: 0.00001490
Iteration 105/1000 | Loss: 0.00001490
Iteration 106/1000 | Loss: 0.00001490
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001488
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001487
Iteration 137/1000 | Loss: 0.00001487
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001487
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.487449299020227e-05, 1.487449299020227e-05, 1.487449299020227e-05, 1.487449299020227e-05, 1.487449299020227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.487449299020227e-05

Optimization complete. Final v2v error: 3.2978146076202393 mm

Highest mean error: 3.425555944442749 mm for frame 28

Lowest mean error: 3.1452229022979736 mm for frame 3

Saving results

Total time: 38.699552059173584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742003
Iteration 2/25 | Loss: 0.00218208
Iteration 3/25 | Loss: 0.00177771
Iteration 4/25 | Loss: 0.00169926
Iteration 5/25 | Loss: 0.00162788
Iteration 6/25 | Loss: 0.00159708
Iteration 7/25 | Loss: 0.00157803
Iteration 8/25 | Loss: 0.00156958
Iteration 9/25 | Loss: 0.00182735
Iteration 10/25 | Loss: 0.00160742
Iteration 11/25 | Loss: 0.00204214
Iteration 12/25 | Loss: 0.00140702
Iteration 13/25 | Loss: 0.00133481
Iteration 14/25 | Loss: 0.00131295
Iteration 15/25 | Loss: 0.00130948
Iteration 16/25 | Loss: 0.00130845
Iteration 17/25 | Loss: 0.00130699
Iteration 18/25 | Loss: 0.00130690
Iteration 19/25 | Loss: 0.00130689
Iteration 20/25 | Loss: 0.00130689
Iteration 21/25 | Loss: 0.00130689
Iteration 22/25 | Loss: 0.00130689
Iteration 23/25 | Loss: 0.00130689
Iteration 24/25 | Loss: 0.00130689
Iteration 25/25 | Loss: 0.00130689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46741772
Iteration 2/25 | Loss: 0.00122931
Iteration 3/25 | Loss: 0.00096269
Iteration 4/25 | Loss: 0.00096220
Iteration 5/25 | Loss: 0.00096220
Iteration 6/25 | Loss: 0.00096220
Iteration 7/25 | Loss: 0.00096220
Iteration 8/25 | Loss: 0.00096220
Iteration 9/25 | Loss: 0.00096220
Iteration 10/25 | Loss: 0.00096219
Iteration 11/25 | Loss: 0.00096219
Iteration 12/25 | Loss: 0.00096219
Iteration 13/25 | Loss: 0.00096219
Iteration 14/25 | Loss: 0.00096219
Iteration 15/25 | Loss: 0.00096219
Iteration 16/25 | Loss: 0.00096219
Iteration 17/25 | Loss: 0.00096219
Iteration 18/25 | Loss: 0.00096219
Iteration 19/25 | Loss: 0.00096219
Iteration 20/25 | Loss: 0.00096219
Iteration 21/25 | Loss: 0.00096219
Iteration 22/25 | Loss: 0.00096219
Iteration 23/25 | Loss: 0.00096219
Iteration 24/25 | Loss: 0.00096219
Iteration 25/25 | Loss: 0.00096219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096219
Iteration 2/1000 | Loss: 0.00006909
Iteration 3/1000 | Loss: 0.00004473
Iteration 4/1000 | Loss: 0.00003626
Iteration 5/1000 | Loss: 0.00003294
Iteration 6/1000 | Loss: 0.00003100
Iteration 7/1000 | Loss: 0.00002961
Iteration 8/1000 | Loss: 0.00002883
Iteration 9/1000 | Loss: 0.00002797
Iteration 10/1000 | Loss: 0.00002743
Iteration 11/1000 | Loss: 0.00002700
Iteration 12/1000 | Loss: 0.00002667
Iteration 13/1000 | Loss: 0.00002641
Iteration 14/1000 | Loss: 0.00002632
Iteration 15/1000 | Loss: 0.00002618
Iteration 16/1000 | Loss: 0.00002615
Iteration 17/1000 | Loss: 0.00002610
Iteration 18/1000 | Loss: 0.00002606
Iteration 19/1000 | Loss: 0.00002604
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002580
Iteration 22/1000 | Loss: 0.00002578
Iteration 23/1000 | Loss: 0.00002578
Iteration 24/1000 | Loss: 0.00002574
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002570
Iteration 27/1000 | Loss: 0.00002569
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002567
Iteration 31/1000 | Loss: 0.00002566
Iteration 32/1000 | Loss: 0.00002566
Iteration 33/1000 | Loss: 0.00002565
Iteration 34/1000 | Loss: 0.00002565
Iteration 35/1000 | Loss: 0.00002564
Iteration 36/1000 | Loss: 0.00002563
Iteration 37/1000 | Loss: 0.00002562
Iteration 38/1000 | Loss: 0.00002559
Iteration 39/1000 | Loss: 0.00002554
Iteration 40/1000 | Loss: 0.00002550
Iteration 41/1000 | Loss: 0.00002550
Iteration 42/1000 | Loss: 0.00002550
Iteration 43/1000 | Loss: 0.00002548
Iteration 44/1000 | Loss: 0.00002545
Iteration 45/1000 | Loss: 0.00002545
Iteration 46/1000 | Loss: 0.00002544
Iteration 47/1000 | Loss: 0.00002543
Iteration 48/1000 | Loss: 0.00002543
Iteration 49/1000 | Loss: 0.00002541
Iteration 50/1000 | Loss: 0.00002541
Iteration 51/1000 | Loss: 0.00002541
Iteration 52/1000 | Loss: 0.00002541
Iteration 53/1000 | Loss: 0.00002540
Iteration 54/1000 | Loss: 0.00002540
Iteration 55/1000 | Loss: 0.00002539
Iteration 56/1000 | Loss: 0.00002538
Iteration 57/1000 | Loss: 0.00002538
Iteration 58/1000 | Loss: 0.00002538
Iteration 59/1000 | Loss: 0.00002537
Iteration 60/1000 | Loss: 0.00002537
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002534
Iteration 65/1000 | Loss: 0.00002533
Iteration 66/1000 | Loss: 0.00002533
Iteration 67/1000 | Loss: 0.00002533
Iteration 68/1000 | Loss: 0.00002532
Iteration 69/1000 | Loss: 0.00002532
Iteration 70/1000 | Loss: 0.00002532
Iteration 71/1000 | Loss: 0.00002531
Iteration 72/1000 | Loss: 0.00002531
Iteration 73/1000 | Loss: 0.00002531
Iteration 74/1000 | Loss: 0.00002530
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002529
Iteration 78/1000 | Loss: 0.00002529
Iteration 79/1000 | Loss: 0.00002528
Iteration 80/1000 | Loss: 0.00002528
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002526
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002524
Iteration 89/1000 | Loss: 0.00002524
Iteration 90/1000 | Loss: 0.00002523
Iteration 91/1000 | Loss: 0.00002523
Iteration 92/1000 | Loss: 0.00002523
Iteration 93/1000 | Loss: 0.00002522
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002521
Iteration 96/1000 | Loss: 0.00002520
Iteration 97/1000 | Loss: 0.00002520
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002519
Iteration 100/1000 | Loss: 0.00002519
Iteration 101/1000 | Loss: 0.00002518
Iteration 102/1000 | Loss: 0.00002518
Iteration 103/1000 | Loss: 0.00002518
Iteration 104/1000 | Loss: 0.00002518
Iteration 105/1000 | Loss: 0.00002518
Iteration 106/1000 | Loss: 0.00002518
Iteration 107/1000 | Loss: 0.00002518
Iteration 108/1000 | Loss: 0.00002518
Iteration 109/1000 | Loss: 0.00002518
Iteration 110/1000 | Loss: 0.00002518
Iteration 111/1000 | Loss: 0.00002518
Iteration 112/1000 | Loss: 0.00002518
Iteration 113/1000 | Loss: 0.00002518
Iteration 114/1000 | Loss: 0.00002518
Iteration 115/1000 | Loss: 0.00002518
Iteration 116/1000 | Loss: 0.00002518
Iteration 117/1000 | Loss: 0.00002518
Iteration 118/1000 | Loss: 0.00002518
Iteration 119/1000 | Loss: 0.00002518
Iteration 120/1000 | Loss: 0.00002518
Iteration 121/1000 | Loss: 0.00002518
Iteration 122/1000 | Loss: 0.00002518
Iteration 123/1000 | Loss: 0.00002518
Iteration 124/1000 | Loss: 0.00002518
Iteration 125/1000 | Loss: 0.00002518
Iteration 126/1000 | Loss: 0.00002518
Iteration 127/1000 | Loss: 0.00002518
Iteration 128/1000 | Loss: 0.00002518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.5178178475471213e-05, 2.5178178475471213e-05, 2.5178178475471213e-05, 2.5178178475471213e-05, 2.5178178475471213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5178178475471213e-05

Optimization complete. Final v2v error: 4.0613112449646 mm

Highest mean error: 6.317018985748291 mm for frame 158

Lowest mean error: 3.1010026931762695 mm for frame 19

Saving results

Total time: 78.92845129966736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838528
Iteration 2/25 | Loss: 0.00147462
Iteration 3/25 | Loss: 0.00134486
Iteration 4/25 | Loss: 0.00126858
Iteration 5/25 | Loss: 0.00126658
Iteration 6/25 | Loss: 0.00126282
Iteration 7/25 | Loss: 0.00126199
Iteration 8/25 | Loss: 0.00126183
Iteration 9/25 | Loss: 0.00126178
Iteration 10/25 | Loss: 0.00126177
Iteration 11/25 | Loss: 0.00126177
Iteration 12/25 | Loss: 0.00126177
Iteration 13/25 | Loss: 0.00126177
Iteration 14/25 | Loss: 0.00126177
Iteration 15/25 | Loss: 0.00126177
Iteration 16/25 | Loss: 0.00126177
Iteration 17/25 | Loss: 0.00126177
Iteration 18/25 | Loss: 0.00126177
Iteration 19/25 | Loss: 0.00126177
Iteration 20/25 | Loss: 0.00126177
Iteration 21/25 | Loss: 0.00126177
Iteration 22/25 | Loss: 0.00126177
Iteration 23/25 | Loss: 0.00126177
Iteration 24/25 | Loss: 0.00126176
Iteration 25/25 | Loss: 0.00126176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54609060
Iteration 2/25 | Loss: 0.00092890
Iteration 3/25 | Loss: 0.00092888
Iteration 4/25 | Loss: 0.00092888
Iteration 5/25 | Loss: 0.00092888
Iteration 6/25 | Loss: 0.00092888
Iteration 7/25 | Loss: 0.00092888
Iteration 8/25 | Loss: 0.00092888
Iteration 9/25 | Loss: 0.00092888
Iteration 10/25 | Loss: 0.00092888
Iteration 11/25 | Loss: 0.00092888
Iteration 12/25 | Loss: 0.00092888
Iteration 13/25 | Loss: 0.00092888
Iteration 14/25 | Loss: 0.00092888
Iteration 15/25 | Loss: 0.00092888
Iteration 16/25 | Loss: 0.00092888
Iteration 17/25 | Loss: 0.00092888
Iteration 18/25 | Loss: 0.00092888
Iteration 19/25 | Loss: 0.00092888
Iteration 20/25 | Loss: 0.00092888
Iteration 21/25 | Loss: 0.00092888
Iteration 22/25 | Loss: 0.00092888
Iteration 23/25 | Loss: 0.00092888
Iteration 24/25 | Loss: 0.00092888
Iteration 25/25 | Loss: 0.00092888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092888
Iteration 2/1000 | Loss: 0.00002201
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001275
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001271
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001250
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001249
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001245
Iteration 100/1000 | Loss: 0.00001245
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001243
Iteration 106/1000 | Loss: 0.00001243
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001242
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001240
Iteration 122/1000 | Loss: 0.00001240
Iteration 123/1000 | Loss: 0.00001240
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.2390496522129979e-05, 1.2390496522129979e-05, 1.2390496522129979e-05, 1.2390496522129979e-05, 1.2390496522129979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2390496522129979e-05

Optimization complete. Final v2v error: 2.9962117671966553 mm

Highest mean error: 3.35360050201416 mm for frame 237

Lowest mean error: 2.778064250946045 mm for frame 81

Saving results

Total time: 48.13462519645691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109908
Iteration 2/25 | Loss: 0.00194961
Iteration 3/25 | Loss: 0.00147708
Iteration 4/25 | Loss: 0.00144215
Iteration 5/25 | Loss: 0.00143382
Iteration 6/25 | Loss: 0.00143269
Iteration 7/25 | Loss: 0.00143269
Iteration 8/25 | Loss: 0.00143269
Iteration 9/25 | Loss: 0.00143269
Iteration 10/25 | Loss: 0.00143269
Iteration 11/25 | Loss: 0.00143269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014326918171718717, 0.0014326918171718717, 0.0014326918171718717, 0.0014326918171718717, 0.0014326918171718717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014326918171718717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99084866
Iteration 2/25 | Loss: 0.00097289
Iteration 3/25 | Loss: 0.00097285
Iteration 4/25 | Loss: 0.00097285
Iteration 5/25 | Loss: 0.00097285
Iteration 6/25 | Loss: 0.00097285
Iteration 7/25 | Loss: 0.00097285
Iteration 8/25 | Loss: 0.00097285
Iteration 9/25 | Loss: 0.00097285
Iteration 10/25 | Loss: 0.00097285
Iteration 11/25 | Loss: 0.00097285
Iteration 12/25 | Loss: 0.00097285
Iteration 13/25 | Loss: 0.00097285
Iteration 14/25 | Loss: 0.00097285
Iteration 15/25 | Loss: 0.00097285
Iteration 16/25 | Loss: 0.00097285
Iteration 17/25 | Loss: 0.00097285
Iteration 18/25 | Loss: 0.00097285
Iteration 19/25 | Loss: 0.00097285
Iteration 20/25 | Loss: 0.00097285
Iteration 21/25 | Loss: 0.00097285
Iteration 22/25 | Loss: 0.00097285
Iteration 23/25 | Loss: 0.00097285
Iteration 24/25 | Loss: 0.00097285
Iteration 25/25 | Loss: 0.00097285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097285
Iteration 2/1000 | Loss: 0.00008404
Iteration 3/1000 | Loss: 0.00004623
Iteration 4/1000 | Loss: 0.00003755
Iteration 5/1000 | Loss: 0.00003454
Iteration 6/1000 | Loss: 0.00003286
Iteration 7/1000 | Loss: 0.00003180
Iteration 8/1000 | Loss: 0.00003105
Iteration 9/1000 | Loss: 0.00003058
Iteration 10/1000 | Loss: 0.00002997
Iteration 11/1000 | Loss: 0.00002963
Iteration 12/1000 | Loss: 0.00002932
Iteration 13/1000 | Loss: 0.00002905
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002866
Iteration 16/1000 | Loss: 0.00002852
Iteration 17/1000 | Loss: 0.00002852
Iteration 18/1000 | Loss: 0.00002846
Iteration 19/1000 | Loss: 0.00002839
Iteration 20/1000 | Loss: 0.00002834
Iteration 21/1000 | Loss: 0.00002829
Iteration 22/1000 | Loss: 0.00002829
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002825
Iteration 25/1000 | Loss: 0.00002823
Iteration 26/1000 | Loss: 0.00002822
Iteration 27/1000 | Loss: 0.00002822
Iteration 28/1000 | Loss: 0.00002819
Iteration 29/1000 | Loss: 0.00002819
Iteration 30/1000 | Loss: 0.00002818
Iteration 31/1000 | Loss: 0.00002815
Iteration 32/1000 | Loss: 0.00002814
Iteration 33/1000 | Loss: 0.00002812
Iteration 34/1000 | Loss: 0.00002811
Iteration 35/1000 | Loss: 0.00002809
Iteration 36/1000 | Loss: 0.00002807
Iteration 37/1000 | Loss: 0.00002805
Iteration 38/1000 | Loss: 0.00002805
Iteration 39/1000 | Loss: 0.00002804
Iteration 40/1000 | Loss: 0.00002802
Iteration 41/1000 | Loss: 0.00002802
Iteration 42/1000 | Loss: 0.00002802
Iteration 43/1000 | Loss: 0.00002801
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00002801
Iteration 46/1000 | Loss: 0.00002800
Iteration 47/1000 | Loss: 0.00002800
Iteration 48/1000 | Loss: 0.00002799
Iteration 49/1000 | Loss: 0.00002799
Iteration 50/1000 | Loss: 0.00002799
Iteration 51/1000 | Loss: 0.00002798
Iteration 52/1000 | Loss: 0.00002798
Iteration 53/1000 | Loss: 0.00002798
Iteration 54/1000 | Loss: 0.00002795
Iteration 55/1000 | Loss: 0.00002795
Iteration 56/1000 | Loss: 0.00002795
Iteration 57/1000 | Loss: 0.00002795
Iteration 58/1000 | Loss: 0.00002795
Iteration 59/1000 | Loss: 0.00002793
Iteration 60/1000 | Loss: 0.00002793
Iteration 61/1000 | Loss: 0.00002793
Iteration 62/1000 | Loss: 0.00002793
Iteration 63/1000 | Loss: 0.00002792
Iteration 64/1000 | Loss: 0.00002792
Iteration 65/1000 | Loss: 0.00002792
Iteration 66/1000 | Loss: 0.00002792
Iteration 67/1000 | Loss: 0.00002792
Iteration 68/1000 | Loss: 0.00002792
Iteration 69/1000 | Loss: 0.00002792
Iteration 70/1000 | Loss: 0.00002792
Iteration 71/1000 | Loss: 0.00002792
Iteration 72/1000 | Loss: 0.00002792
Iteration 73/1000 | Loss: 0.00002792
Iteration 74/1000 | Loss: 0.00002792
Iteration 75/1000 | Loss: 0.00002791
Iteration 76/1000 | Loss: 0.00002791
Iteration 77/1000 | Loss: 0.00002791
Iteration 78/1000 | Loss: 0.00002791
Iteration 79/1000 | Loss: 0.00002791
Iteration 80/1000 | Loss: 0.00002791
Iteration 81/1000 | Loss: 0.00002791
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002791
Iteration 84/1000 | Loss: 0.00002791
Iteration 85/1000 | Loss: 0.00002791
Iteration 86/1000 | Loss: 0.00002791
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002790
Iteration 89/1000 | Loss: 0.00002790
Iteration 90/1000 | Loss: 0.00002790
Iteration 91/1000 | Loss: 0.00002790
Iteration 92/1000 | Loss: 0.00002790
Iteration 93/1000 | Loss: 0.00002789
Iteration 94/1000 | Loss: 0.00002789
Iteration 95/1000 | Loss: 0.00002789
Iteration 96/1000 | Loss: 0.00002789
Iteration 97/1000 | Loss: 0.00002788
Iteration 98/1000 | Loss: 0.00002788
Iteration 99/1000 | Loss: 0.00002788
Iteration 100/1000 | Loss: 0.00002788
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00002788
Iteration 103/1000 | Loss: 0.00002788
Iteration 104/1000 | Loss: 0.00002788
Iteration 105/1000 | Loss: 0.00002787
Iteration 106/1000 | Loss: 0.00002787
Iteration 107/1000 | Loss: 0.00002787
Iteration 108/1000 | Loss: 0.00002787
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002787
Iteration 111/1000 | Loss: 0.00002787
Iteration 112/1000 | Loss: 0.00002787
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00002786
Iteration 115/1000 | Loss: 0.00002786
Iteration 116/1000 | Loss: 0.00002786
Iteration 117/1000 | Loss: 0.00002786
Iteration 118/1000 | Loss: 0.00002786
Iteration 119/1000 | Loss: 0.00002786
Iteration 120/1000 | Loss: 0.00002786
Iteration 121/1000 | Loss: 0.00002786
Iteration 122/1000 | Loss: 0.00002786
Iteration 123/1000 | Loss: 0.00002786
Iteration 124/1000 | Loss: 0.00002786
Iteration 125/1000 | Loss: 0.00002786
Iteration 126/1000 | Loss: 0.00002786
Iteration 127/1000 | Loss: 0.00002785
Iteration 128/1000 | Loss: 0.00002785
Iteration 129/1000 | Loss: 0.00002785
Iteration 130/1000 | Loss: 0.00002785
Iteration 131/1000 | Loss: 0.00002785
Iteration 132/1000 | Loss: 0.00002785
Iteration 133/1000 | Loss: 0.00002784
Iteration 134/1000 | Loss: 0.00002784
Iteration 135/1000 | Loss: 0.00002784
Iteration 136/1000 | Loss: 0.00002784
Iteration 137/1000 | Loss: 0.00002784
Iteration 138/1000 | Loss: 0.00002784
Iteration 139/1000 | Loss: 0.00002783
Iteration 140/1000 | Loss: 0.00002783
Iteration 141/1000 | Loss: 0.00002783
Iteration 142/1000 | Loss: 0.00002783
Iteration 143/1000 | Loss: 0.00002783
Iteration 144/1000 | Loss: 0.00002783
Iteration 145/1000 | Loss: 0.00002783
Iteration 146/1000 | Loss: 0.00002783
Iteration 147/1000 | Loss: 0.00002783
Iteration 148/1000 | Loss: 0.00002783
Iteration 149/1000 | Loss: 0.00002783
Iteration 150/1000 | Loss: 0.00002783
Iteration 151/1000 | Loss: 0.00002783
Iteration 152/1000 | Loss: 0.00002783
Iteration 153/1000 | Loss: 0.00002783
Iteration 154/1000 | Loss: 0.00002783
Iteration 155/1000 | Loss: 0.00002783
Iteration 156/1000 | Loss: 0.00002783
Iteration 157/1000 | Loss: 0.00002783
Iteration 158/1000 | Loss: 0.00002783
Iteration 159/1000 | Loss: 0.00002783
Iteration 160/1000 | Loss: 0.00002783
Iteration 161/1000 | Loss: 0.00002783
Iteration 162/1000 | Loss: 0.00002783
Iteration 163/1000 | Loss: 0.00002783
Iteration 164/1000 | Loss: 0.00002783
Iteration 165/1000 | Loss: 0.00002783
Iteration 166/1000 | Loss: 0.00002783
Iteration 167/1000 | Loss: 0.00002783
Iteration 168/1000 | Loss: 0.00002783
Iteration 169/1000 | Loss: 0.00002783
Iteration 170/1000 | Loss: 0.00002783
Iteration 171/1000 | Loss: 0.00002783
Iteration 172/1000 | Loss: 0.00002783
Iteration 173/1000 | Loss: 0.00002783
Iteration 174/1000 | Loss: 0.00002783
Iteration 175/1000 | Loss: 0.00002783
Iteration 176/1000 | Loss: 0.00002783
Iteration 177/1000 | Loss: 0.00002783
Iteration 178/1000 | Loss: 0.00002783
Iteration 179/1000 | Loss: 0.00002783
Iteration 180/1000 | Loss: 0.00002783
Iteration 181/1000 | Loss: 0.00002783
Iteration 182/1000 | Loss: 0.00002783
Iteration 183/1000 | Loss: 0.00002783
Iteration 184/1000 | Loss: 0.00002783
Iteration 185/1000 | Loss: 0.00002783
Iteration 186/1000 | Loss: 0.00002783
Iteration 187/1000 | Loss: 0.00002783
Iteration 188/1000 | Loss: 0.00002783
Iteration 189/1000 | Loss: 0.00002783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.782560659397859e-05, 2.782560659397859e-05, 2.782560659397859e-05, 2.782560659397859e-05, 2.782560659397859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.782560659397859e-05

Optimization complete. Final v2v error: 4.406383514404297 mm

Highest mean error: 5.703853130340576 mm for frame 68

Lowest mean error: 3.5839221477508545 mm for frame 27

Saving results

Total time: 53.571651220321655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427121
Iteration 2/25 | Loss: 0.00139637
Iteration 3/25 | Loss: 0.00130745
Iteration 4/25 | Loss: 0.00129270
Iteration 5/25 | Loss: 0.00128771
Iteration 6/25 | Loss: 0.00128718
Iteration 7/25 | Loss: 0.00128718
Iteration 8/25 | Loss: 0.00128718
Iteration 9/25 | Loss: 0.00128718
Iteration 10/25 | Loss: 0.00128718
Iteration 11/25 | Loss: 0.00128718
Iteration 12/25 | Loss: 0.00128718
Iteration 13/25 | Loss: 0.00128718
Iteration 14/25 | Loss: 0.00128718
Iteration 15/25 | Loss: 0.00128718
Iteration 16/25 | Loss: 0.00128718
Iteration 17/25 | Loss: 0.00128718
Iteration 18/25 | Loss: 0.00128718
Iteration 19/25 | Loss: 0.00128718
Iteration 20/25 | Loss: 0.00128718
Iteration 21/25 | Loss: 0.00128718
Iteration 22/25 | Loss: 0.00128718
Iteration 23/25 | Loss: 0.00128718
Iteration 24/25 | Loss: 0.00128718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001287180813960731, 0.001287180813960731, 0.001287180813960731, 0.001287180813960731, 0.001287180813960731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001287180813960731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47096217
Iteration 2/25 | Loss: 0.00088655
Iteration 3/25 | Loss: 0.00088655
Iteration 4/25 | Loss: 0.00088655
Iteration 5/25 | Loss: 0.00088655
Iteration 6/25 | Loss: 0.00088655
Iteration 7/25 | Loss: 0.00088655
Iteration 8/25 | Loss: 0.00088655
Iteration 9/25 | Loss: 0.00088655
Iteration 10/25 | Loss: 0.00088655
Iteration 11/25 | Loss: 0.00088655
Iteration 12/25 | Loss: 0.00088655
Iteration 13/25 | Loss: 0.00088655
Iteration 14/25 | Loss: 0.00088655
Iteration 15/25 | Loss: 0.00088655
Iteration 16/25 | Loss: 0.00088655
Iteration 17/25 | Loss: 0.00088655
Iteration 18/25 | Loss: 0.00088655
Iteration 19/25 | Loss: 0.00088655
Iteration 20/25 | Loss: 0.00088655
Iteration 21/25 | Loss: 0.00088655
Iteration 22/25 | Loss: 0.00088655
Iteration 23/25 | Loss: 0.00088655
Iteration 24/25 | Loss: 0.00088655
Iteration 25/25 | Loss: 0.00088655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088655
Iteration 2/1000 | Loss: 0.00003667
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002349
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002147
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00002044
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001877
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001860
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001853
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001850
Iteration 87/1000 | Loss: 0.00001850
Iteration 88/1000 | Loss: 0.00001850
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001849
Iteration 91/1000 | Loss: 0.00001849
Iteration 92/1000 | Loss: 0.00001849
Iteration 93/1000 | Loss: 0.00001849
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001849
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001848
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001845
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001845
Iteration 143/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.844817234086804e-05, 1.844817234086804e-05, 1.844817234086804e-05, 1.844817234086804e-05, 1.844817234086804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.844817234086804e-05

Optimization complete. Final v2v error: 3.653557538986206 mm

Highest mean error: 4.389849662780762 mm for frame 19

Lowest mean error: 3.4491982460021973 mm for frame 8

Saving results

Total time: 39.45289754867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869016
Iteration 2/25 | Loss: 0.00157170
Iteration 3/25 | Loss: 0.00134076
Iteration 4/25 | Loss: 0.00132145
Iteration 5/25 | Loss: 0.00131776
Iteration 6/25 | Loss: 0.00131749
Iteration 7/25 | Loss: 0.00131749
Iteration 8/25 | Loss: 0.00131749
Iteration 9/25 | Loss: 0.00131749
Iteration 10/25 | Loss: 0.00131749
Iteration 11/25 | Loss: 0.00131749
Iteration 12/25 | Loss: 0.00131749
Iteration 13/25 | Loss: 0.00131749
Iteration 14/25 | Loss: 0.00131749
Iteration 15/25 | Loss: 0.00131749
Iteration 16/25 | Loss: 0.00131749
Iteration 17/25 | Loss: 0.00131749
Iteration 18/25 | Loss: 0.00131749
Iteration 19/25 | Loss: 0.00131749
Iteration 20/25 | Loss: 0.00131749
Iteration 21/25 | Loss: 0.00131749
Iteration 22/25 | Loss: 0.00131749
Iteration 23/25 | Loss: 0.00131749
Iteration 24/25 | Loss: 0.00131749
Iteration 25/25 | Loss: 0.00131749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96640056
Iteration 2/25 | Loss: 0.00057173
Iteration 3/25 | Loss: 0.00057173
Iteration 4/25 | Loss: 0.00057173
Iteration 5/25 | Loss: 0.00057173
Iteration 6/25 | Loss: 0.00057173
Iteration 7/25 | Loss: 0.00057173
Iteration 8/25 | Loss: 0.00057173
Iteration 9/25 | Loss: 0.00057173
Iteration 10/25 | Loss: 0.00057173
Iteration 11/25 | Loss: 0.00057173
Iteration 12/25 | Loss: 0.00057173
Iteration 13/25 | Loss: 0.00057173
Iteration 14/25 | Loss: 0.00057173
Iteration 15/25 | Loss: 0.00057173
Iteration 16/25 | Loss: 0.00057173
Iteration 17/25 | Loss: 0.00057173
Iteration 18/25 | Loss: 0.00057173
Iteration 19/25 | Loss: 0.00057173
Iteration 20/25 | Loss: 0.00057173
Iteration 21/25 | Loss: 0.00057173
Iteration 22/25 | Loss: 0.00057173
Iteration 23/25 | Loss: 0.00057173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005717252497561276, 0.0005717252497561276, 0.0005717252497561276, 0.0005717252497561276, 0.0005717252497561276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005717252497561276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057173
Iteration 2/1000 | Loss: 0.00004497
Iteration 3/1000 | Loss: 0.00003620
Iteration 4/1000 | Loss: 0.00003293
Iteration 5/1000 | Loss: 0.00003160
Iteration 6/1000 | Loss: 0.00003075
Iteration 7/1000 | Loss: 0.00002993
Iteration 8/1000 | Loss: 0.00002954
Iteration 9/1000 | Loss: 0.00002918
Iteration 10/1000 | Loss: 0.00002881
Iteration 11/1000 | Loss: 0.00002851
Iteration 12/1000 | Loss: 0.00002847
Iteration 13/1000 | Loss: 0.00002823
Iteration 14/1000 | Loss: 0.00002804
Iteration 15/1000 | Loss: 0.00002800
Iteration 16/1000 | Loss: 0.00002800
Iteration 17/1000 | Loss: 0.00002799
Iteration 18/1000 | Loss: 0.00002799
Iteration 19/1000 | Loss: 0.00002799
Iteration 20/1000 | Loss: 0.00002799
Iteration 21/1000 | Loss: 0.00002799
Iteration 22/1000 | Loss: 0.00002799
Iteration 23/1000 | Loss: 0.00002799
Iteration 24/1000 | Loss: 0.00002799
Iteration 25/1000 | Loss: 0.00002798
Iteration 26/1000 | Loss: 0.00002798
Iteration 27/1000 | Loss: 0.00002795
Iteration 28/1000 | Loss: 0.00002794
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002793
Iteration 31/1000 | Loss: 0.00002793
Iteration 32/1000 | Loss: 0.00002793
Iteration 33/1000 | Loss: 0.00002793
Iteration 34/1000 | Loss: 0.00002793
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002793
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002792
Iteration 40/1000 | Loss: 0.00002792
Iteration 41/1000 | Loss: 0.00002792
Iteration 42/1000 | Loss: 0.00002792
Iteration 43/1000 | Loss: 0.00002792
Iteration 44/1000 | Loss: 0.00002792
Iteration 45/1000 | Loss: 0.00002792
Iteration 46/1000 | Loss: 0.00002791
Iteration 47/1000 | Loss: 0.00002791
Iteration 48/1000 | Loss: 0.00002791
Iteration 49/1000 | Loss: 0.00002791
Iteration 50/1000 | Loss: 0.00002791
Iteration 51/1000 | Loss: 0.00002791
Iteration 52/1000 | Loss: 0.00002791
Iteration 53/1000 | Loss: 0.00002791
Iteration 54/1000 | Loss: 0.00002790
Iteration 55/1000 | Loss: 0.00002787
Iteration 56/1000 | Loss: 0.00002787
Iteration 57/1000 | Loss: 0.00002787
Iteration 58/1000 | Loss: 0.00002786
Iteration 59/1000 | Loss: 0.00002785
Iteration 60/1000 | Loss: 0.00002785
Iteration 61/1000 | Loss: 0.00002784
Iteration 62/1000 | Loss: 0.00002784
Iteration 63/1000 | Loss: 0.00002784
Iteration 64/1000 | Loss: 0.00002784
Iteration 65/1000 | Loss: 0.00002784
Iteration 66/1000 | Loss: 0.00002784
Iteration 67/1000 | Loss: 0.00002784
Iteration 68/1000 | Loss: 0.00002784
Iteration 69/1000 | Loss: 0.00002784
Iteration 70/1000 | Loss: 0.00002784
Iteration 71/1000 | Loss: 0.00002783
Iteration 72/1000 | Loss: 0.00002783
Iteration 73/1000 | Loss: 0.00002783
Iteration 74/1000 | Loss: 0.00002783
Iteration 75/1000 | Loss: 0.00002782
Iteration 76/1000 | Loss: 0.00002782
Iteration 77/1000 | Loss: 0.00002782
Iteration 78/1000 | Loss: 0.00002782
Iteration 79/1000 | Loss: 0.00002782
Iteration 80/1000 | Loss: 0.00002782
Iteration 81/1000 | Loss: 0.00002782
Iteration 82/1000 | Loss: 0.00002782
Iteration 83/1000 | Loss: 0.00002782
Iteration 84/1000 | Loss: 0.00002782
Iteration 85/1000 | Loss: 0.00002782
Iteration 86/1000 | Loss: 0.00002782
Iteration 87/1000 | Loss: 0.00002782
Iteration 88/1000 | Loss: 0.00002782
Iteration 89/1000 | Loss: 0.00002782
Iteration 90/1000 | Loss: 0.00002782
Iteration 91/1000 | Loss: 0.00002782
Iteration 92/1000 | Loss: 0.00002782
Iteration 93/1000 | Loss: 0.00002782
Iteration 94/1000 | Loss: 0.00002782
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00002782
Iteration 97/1000 | Loss: 0.00002782
Iteration 98/1000 | Loss: 0.00002782
Iteration 99/1000 | Loss: 0.00002782
Iteration 100/1000 | Loss: 0.00002782
Iteration 101/1000 | Loss: 0.00002782
Iteration 102/1000 | Loss: 0.00002782
Iteration 103/1000 | Loss: 0.00002782
Iteration 104/1000 | Loss: 0.00002782
Iteration 105/1000 | Loss: 0.00002782
Iteration 106/1000 | Loss: 0.00002782
Iteration 107/1000 | Loss: 0.00002782
Iteration 108/1000 | Loss: 0.00002782
Iteration 109/1000 | Loss: 0.00002782
Iteration 110/1000 | Loss: 0.00002782
Iteration 111/1000 | Loss: 0.00002782
Iteration 112/1000 | Loss: 0.00002782
Iteration 113/1000 | Loss: 0.00002782
Iteration 114/1000 | Loss: 0.00002782
Iteration 115/1000 | Loss: 0.00002782
Iteration 116/1000 | Loss: 0.00002782
Iteration 117/1000 | Loss: 0.00002782
Iteration 118/1000 | Loss: 0.00002782
Iteration 119/1000 | Loss: 0.00002782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.781512557703536e-05, 2.781512557703536e-05, 2.781512557703536e-05, 2.781512557703536e-05, 2.781512557703536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.781512557703536e-05

Optimization complete. Final v2v error: 4.461240768432617 mm

Highest mean error: 4.769917964935303 mm for frame 2

Lowest mean error: 4.133964538574219 mm for frame 81

Saving results

Total time: 32.899561643600464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106637
Iteration 2/25 | Loss: 0.01106637
Iteration 3/25 | Loss: 0.01106636
Iteration 4/25 | Loss: 0.01106636
Iteration 5/25 | Loss: 0.01106636
Iteration 6/25 | Loss: 0.01106636
Iteration 7/25 | Loss: 0.01106636
Iteration 8/25 | Loss: 0.01106636
Iteration 9/25 | Loss: 0.01106635
Iteration 10/25 | Loss: 0.01106635
Iteration 11/25 | Loss: 0.01106635
Iteration 12/25 | Loss: 0.01106635
Iteration 13/25 | Loss: 0.01106634
Iteration 14/25 | Loss: 0.01106634
Iteration 15/25 | Loss: 0.01106634
Iteration 16/25 | Loss: 0.01106633
Iteration 17/25 | Loss: 0.01106633
Iteration 18/25 | Loss: 0.01106633
Iteration 19/25 | Loss: 0.01106633
Iteration 20/25 | Loss: 0.01106633
Iteration 21/25 | Loss: 0.01106633
Iteration 22/25 | Loss: 0.01106632
Iteration 23/25 | Loss: 0.01106632
Iteration 24/25 | Loss: 0.01106632
Iteration 25/25 | Loss: 0.01106632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31328523
Iteration 2/25 | Loss: 0.17662226
Iteration 3/25 | Loss: 0.17547873
Iteration 4/25 | Loss: 0.17547868
Iteration 5/25 | Loss: 0.17547867
Iteration 6/25 | Loss: 0.17547865
Iteration 7/25 | Loss: 0.17547865
Iteration 8/25 | Loss: 0.17547864
Iteration 9/25 | Loss: 0.17547864
Iteration 10/25 | Loss: 0.17547864
Iteration 11/25 | Loss: 0.17547864
Iteration 12/25 | Loss: 0.17547864
Iteration 13/25 | Loss: 0.17547864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17547863721847534, 0.17547863721847534, 0.17547863721847534, 0.17547863721847534, 0.17547863721847534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17547863721847534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17547864
Iteration 2/1000 | Loss: 0.00503456
Iteration 3/1000 | Loss: 0.00287310
Iteration 4/1000 | Loss: 0.00339959
Iteration 5/1000 | Loss: 0.00164823
Iteration 6/1000 | Loss: 0.00155997
Iteration 7/1000 | Loss: 0.00208010
Iteration 8/1000 | Loss: 0.00115070
Iteration 9/1000 | Loss: 0.00175427
Iteration 10/1000 | Loss: 0.00123064
Iteration 11/1000 | Loss: 0.00213797
Iteration 12/1000 | Loss: 0.00126557
Iteration 13/1000 | Loss: 0.00182560
Iteration 14/1000 | Loss: 0.00125670
Iteration 15/1000 | Loss: 0.00121298
Iteration 16/1000 | Loss: 0.00221497
Iteration 17/1000 | Loss: 0.00135920
Iteration 18/1000 | Loss: 0.00126141
Iteration 19/1000 | Loss: 0.00077292
Iteration 20/1000 | Loss: 0.00051212
Iteration 21/1000 | Loss: 0.00007331
Iteration 22/1000 | Loss: 0.00005392
Iteration 23/1000 | Loss: 0.00004918
Iteration 24/1000 | Loss: 0.00004154
Iteration 25/1000 | Loss: 0.00003646
Iteration 26/1000 | Loss: 0.00003325
Iteration 27/1000 | Loss: 0.00003487
Iteration 28/1000 | Loss: 0.00002950
Iteration 29/1000 | Loss: 0.00002693
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002554
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002452
Iteration 35/1000 | Loss: 0.00002423
Iteration 36/1000 | Loss: 0.00002411
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002400
Iteration 39/1000 | Loss: 0.00002393
Iteration 40/1000 | Loss: 0.00002389
Iteration 41/1000 | Loss: 0.00002389
Iteration 42/1000 | Loss: 0.00002388
Iteration 43/1000 | Loss: 0.00002388
Iteration 44/1000 | Loss: 0.00002387
Iteration 45/1000 | Loss: 0.00002387
Iteration 46/1000 | Loss: 0.00002387
Iteration 47/1000 | Loss: 0.00002386
Iteration 48/1000 | Loss: 0.00002385
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002383
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002382
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002379
Iteration 56/1000 | Loss: 0.00002377
Iteration 57/1000 | Loss: 0.00002377
Iteration 58/1000 | Loss: 0.00002377
Iteration 59/1000 | Loss: 0.00002377
Iteration 60/1000 | Loss: 0.00002376
Iteration 61/1000 | Loss: 0.00002376
Iteration 62/1000 | Loss: 0.00002372
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002367
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002366
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002365
Iteration 71/1000 | Loss: 0.00002365
Iteration 72/1000 | Loss: 0.00002364
Iteration 73/1000 | Loss: 0.00002364
Iteration 74/1000 | Loss: 0.00002364
Iteration 75/1000 | Loss: 0.00002364
Iteration 76/1000 | Loss: 0.00002364
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002363
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002362
Iteration 96/1000 | Loss: 0.00002362
Iteration 97/1000 | Loss: 0.00002362
Iteration 98/1000 | Loss: 0.00002362
Iteration 99/1000 | Loss: 0.00002362
Iteration 100/1000 | Loss: 0.00002362
Iteration 101/1000 | Loss: 0.00002361
Iteration 102/1000 | Loss: 0.00002361
Iteration 103/1000 | Loss: 0.00002361
Iteration 104/1000 | Loss: 0.00002361
Iteration 105/1000 | Loss: 0.00002361
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002361
Iteration 108/1000 | Loss: 0.00002360
Iteration 109/1000 | Loss: 0.00002360
Iteration 110/1000 | Loss: 0.00002360
Iteration 111/1000 | Loss: 0.00002360
Iteration 112/1000 | Loss: 0.00002360
Iteration 113/1000 | Loss: 0.00002360
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002360
Iteration 124/1000 | Loss: 0.00002360
Iteration 125/1000 | Loss: 0.00002360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.3597380277351476e-05, 2.3597380277351476e-05, 2.3597380277351476e-05, 2.3597380277351476e-05, 2.3597380277351476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3597380277351476e-05

Optimization complete. Final v2v error: 3.9273178577423096 mm

Highest mean error: 5.053213596343994 mm for frame 76

Lowest mean error: 3.399371385574341 mm for frame 24

Saving results

Total time: 76.67932987213135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425370
Iteration 2/25 | Loss: 0.00131715
Iteration 3/25 | Loss: 0.00125471
Iteration 4/25 | Loss: 0.00124621
Iteration 5/25 | Loss: 0.00124388
Iteration 6/25 | Loss: 0.00124339
Iteration 7/25 | Loss: 0.00124339
Iteration 8/25 | Loss: 0.00124339
Iteration 9/25 | Loss: 0.00124339
Iteration 10/25 | Loss: 0.00124339
Iteration 11/25 | Loss: 0.00124339
Iteration 12/25 | Loss: 0.00124339
Iteration 13/25 | Loss: 0.00124339
Iteration 14/25 | Loss: 0.00124339
Iteration 15/25 | Loss: 0.00124339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001243389560841024, 0.001243389560841024, 0.001243389560841024, 0.001243389560841024, 0.001243389560841024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243389560841024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76948786
Iteration 2/25 | Loss: 0.00083433
Iteration 3/25 | Loss: 0.00083433
Iteration 4/25 | Loss: 0.00083433
Iteration 5/25 | Loss: 0.00083433
Iteration 6/25 | Loss: 0.00083432
Iteration 7/25 | Loss: 0.00083432
Iteration 8/25 | Loss: 0.00083432
Iteration 9/25 | Loss: 0.00083432
Iteration 10/25 | Loss: 0.00083432
Iteration 11/25 | Loss: 0.00083432
Iteration 12/25 | Loss: 0.00083432
Iteration 13/25 | Loss: 0.00083432
Iteration 14/25 | Loss: 0.00083432
Iteration 15/25 | Loss: 0.00083432
Iteration 16/25 | Loss: 0.00083432
Iteration 17/25 | Loss: 0.00083432
Iteration 18/25 | Loss: 0.00083432
Iteration 19/25 | Loss: 0.00083432
Iteration 20/25 | Loss: 0.00083432
Iteration 21/25 | Loss: 0.00083432
Iteration 22/25 | Loss: 0.00083432
Iteration 23/25 | Loss: 0.00083432
Iteration 24/25 | Loss: 0.00083432
Iteration 25/25 | Loss: 0.00083432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083432
Iteration 2/1000 | Loss: 0.00003678
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001838
Iteration 5/1000 | Loss: 0.00001722
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001533
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001444
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001407
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001395
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001386
Iteration 108/1000 | Loss: 0.00001386
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001385
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001379
Iteration 131/1000 | Loss: 0.00001379
Iteration 132/1000 | Loss: 0.00001379
Iteration 133/1000 | Loss: 0.00001379
Iteration 134/1000 | Loss: 0.00001378
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001376
Iteration 138/1000 | Loss: 0.00001376
Iteration 139/1000 | Loss: 0.00001376
Iteration 140/1000 | Loss: 0.00001376
Iteration 141/1000 | Loss: 0.00001376
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001376
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001376
Iteration 154/1000 | Loss: 0.00001375
Iteration 155/1000 | Loss: 0.00001375
Iteration 156/1000 | Loss: 0.00001375
Iteration 157/1000 | Loss: 0.00001375
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001373
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001372
Iteration 164/1000 | Loss: 0.00001372
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001371
Iteration 168/1000 | Loss: 0.00001371
Iteration 169/1000 | Loss: 0.00001371
Iteration 170/1000 | Loss: 0.00001371
Iteration 171/1000 | Loss: 0.00001371
Iteration 172/1000 | Loss: 0.00001370
Iteration 173/1000 | Loss: 0.00001370
Iteration 174/1000 | Loss: 0.00001370
Iteration 175/1000 | Loss: 0.00001370
Iteration 176/1000 | Loss: 0.00001370
Iteration 177/1000 | Loss: 0.00001370
Iteration 178/1000 | Loss: 0.00001370
Iteration 179/1000 | Loss: 0.00001369
Iteration 180/1000 | Loss: 0.00001369
Iteration 181/1000 | Loss: 0.00001369
Iteration 182/1000 | Loss: 0.00001369
Iteration 183/1000 | Loss: 0.00001369
Iteration 184/1000 | Loss: 0.00001369
Iteration 185/1000 | Loss: 0.00001369
Iteration 186/1000 | Loss: 0.00001369
Iteration 187/1000 | Loss: 0.00001369
Iteration 188/1000 | Loss: 0.00001368
Iteration 189/1000 | Loss: 0.00001368
Iteration 190/1000 | Loss: 0.00001368
Iteration 191/1000 | Loss: 0.00001368
Iteration 192/1000 | Loss: 0.00001368
Iteration 193/1000 | Loss: 0.00001368
Iteration 194/1000 | Loss: 0.00001368
Iteration 195/1000 | Loss: 0.00001368
Iteration 196/1000 | Loss: 0.00001368
Iteration 197/1000 | Loss: 0.00001368
Iteration 198/1000 | Loss: 0.00001367
Iteration 199/1000 | Loss: 0.00001367
Iteration 200/1000 | Loss: 0.00001367
Iteration 201/1000 | Loss: 0.00001367
Iteration 202/1000 | Loss: 0.00001367
Iteration 203/1000 | Loss: 0.00001367
Iteration 204/1000 | Loss: 0.00001367
Iteration 205/1000 | Loss: 0.00001367
Iteration 206/1000 | Loss: 0.00001366
Iteration 207/1000 | Loss: 0.00001366
Iteration 208/1000 | Loss: 0.00001366
Iteration 209/1000 | Loss: 0.00001366
Iteration 210/1000 | Loss: 0.00001366
Iteration 211/1000 | Loss: 0.00001366
Iteration 212/1000 | Loss: 0.00001366
Iteration 213/1000 | Loss: 0.00001366
Iteration 214/1000 | Loss: 0.00001366
Iteration 215/1000 | Loss: 0.00001366
Iteration 216/1000 | Loss: 0.00001366
Iteration 217/1000 | Loss: 0.00001366
Iteration 218/1000 | Loss: 0.00001366
Iteration 219/1000 | Loss: 0.00001366
Iteration 220/1000 | Loss: 0.00001366
Iteration 221/1000 | Loss: 0.00001366
Iteration 222/1000 | Loss: 0.00001366
Iteration 223/1000 | Loss: 0.00001366
Iteration 224/1000 | Loss: 0.00001366
Iteration 225/1000 | Loss: 0.00001366
Iteration 226/1000 | Loss: 0.00001366
Iteration 227/1000 | Loss: 0.00001366
Iteration 228/1000 | Loss: 0.00001365
Iteration 229/1000 | Loss: 0.00001365
Iteration 230/1000 | Loss: 0.00001365
Iteration 231/1000 | Loss: 0.00001365
Iteration 232/1000 | Loss: 0.00001365
Iteration 233/1000 | Loss: 0.00001365
Iteration 234/1000 | Loss: 0.00001365
Iteration 235/1000 | Loss: 0.00001365
Iteration 236/1000 | Loss: 0.00001365
Iteration 237/1000 | Loss: 0.00001365
Iteration 238/1000 | Loss: 0.00001365
Iteration 239/1000 | Loss: 0.00001365
Iteration 240/1000 | Loss: 0.00001365
Iteration 241/1000 | Loss: 0.00001365
Iteration 242/1000 | Loss: 0.00001365
Iteration 243/1000 | Loss: 0.00001365
Iteration 244/1000 | Loss: 0.00001365
Iteration 245/1000 | Loss: 0.00001365
Iteration 246/1000 | Loss: 0.00001365
Iteration 247/1000 | Loss: 0.00001365
Iteration 248/1000 | Loss: 0.00001365
Iteration 249/1000 | Loss: 0.00001365
Iteration 250/1000 | Loss: 0.00001365
Iteration 251/1000 | Loss: 0.00001365
Iteration 252/1000 | Loss: 0.00001365
Iteration 253/1000 | Loss: 0.00001365
Iteration 254/1000 | Loss: 0.00001365
Iteration 255/1000 | Loss: 0.00001365
Iteration 256/1000 | Loss: 0.00001365
Iteration 257/1000 | Loss: 0.00001365
Iteration 258/1000 | Loss: 0.00001365
Iteration 259/1000 | Loss: 0.00001365
Iteration 260/1000 | Loss: 0.00001365
Iteration 261/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.3654832400789019e-05, 1.3654832400789019e-05, 1.3654832400789019e-05, 1.3654832400789019e-05, 1.3654832400789019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3654832400789019e-05

Optimization complete. Final v2v error: 3.1465091705322266 mm

Highest mean error: 4.063692569732666 mm for frame 75

Lowest mean error: 2.874835729598999 mm for frame 95

Saving results

Total time: 44.78039836883545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410751
Iteration 2/25 | Loss: 0.00131978
Iteration 3/25 | Loss: 0.00123791
Iteration 4/25 | Loss: 0.00122366
Iteration 5/25 | Loss: 0.00121913
Iteration 6/25 | Loss: 0.00121884
Iteration 7/25 | Loss: 0.00121884
Iteration 8/25 | Loss: 0.00121884
Iteration 9/25 | Loss: 0.00121884
Iteration 10/25 | Loss: 0.00121884
Iteration 11/25 | Loss: 0.00121884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012188423424959183, 0.0012188423424959183, 0.0012188423424959183, 0.0012188423424959183, 0.0012188423424959183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012188423424959183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83523810
Iteration 2/25 | Loss: 0.00074767
Iteration 3/25 | Loss: 0.00074767
Iteration 4/25 | Loss: 0.00074767
Iteration 5/25 | Loss: 0.00074767
Iteration 6/25 | Loss: 0.00074767
Iteration 7/25 | Loss: 0.00074767
Iteration 8/25 | Loss: 0.00074767
Iteration 9/25 | Loss: 0.00074767
Iteration 10/25 | Loss: 0.00074767
Iteration 11/25 | Loss: 0.00074767
Iteration 12/25 | Loss: 0.00074767
Iteration 13/25 | Loss: 0.00074767
Iteration 14/25 | Loss: 0.00074767
Iteration 15/25 | Loss: 0.00074767
Iteration 16/25 | Loss: 0.00074767
Iteration 17/25 | Loss: 0.00074767
Iteration 18/25 | Loss: 0.00074767
Iteration 19/25 | Loss: 0.00074767
Iteration 20/25 | Loss: 0.00074767
Iteration 21/25 | Loss: 0.00074767
Iteration 22/25 | Loss: 0.00074767
Iteration 23/25 | Loss: 0.00074767
Iteration 24/25 | Loss: 0.00074767
Iteration 25/25 | Loss: 0.00074767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007476697210222483, 0.0007476697210222483, 0.0007476697210222483, 0.0007476697210222483, 0.0007476697210222483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007476697210222483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074767
Iteration 2/1000 | Loss: 0.00002364
Iteration 3/1000 | Loss: 0.00001756
Iteration 4/1000 | Loss: 0.00001649
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001398
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001390
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001371
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001370
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001367
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001363
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001362
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00001362
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001361
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001356
Iteration 139/1000 | Loss: 0.00001356
Iteration 140/1000 | Loss: 0.00001356
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001356
Iteration 143/1000 | Loss: 0.00001356
Iteration 144/1000 | Loss: 0.00001356
Iteration 145/1000 | Loss: 0.00001356
Iteration 146/1000 | Loss: 0.00001356
Iteration 147/1000 | Loss: 0.00001356
Iteration 148/1000 | Loss: 0.00001356
Iteration 149/1000 | Loss: 0.00001356
Iteration 150/1000 | Loss: 0.00001356
Iteration 151/1000 | Loss: 0.00001356
Iteration 152/1000 | Loss: 0.00001356
Iteration 153/1000 | Loss: 0.00001356
Iteration 154/1000 | Loss: 0.00001356
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001356
Iteration 162/1000 | Loss: 0.00001356
Iteration 163/1000 | Loss: 0.00001356
Iteration 164/1000 | Loss: 0.00001356
Iteration 165/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.3559080798586365e-05, 1.3559080798586365e-05, 1.3559080798586365e-05, 1.3559080798586365e-05, 1.3559080798586365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3559080798586365e-05

Optimization complete. Final v2v error: 3.1452667713165283 mm

Highest mean error: 3.4718902111053467 mm for frame 129

Lowest mean error: 3.001098155975342 mm for frame 114

Saving results

Total time: 37.68263506889343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389969
Iteration 2/25 | Loss: 0.00141539
Iteration 3/25 | Loss: 0.00128226
Iteration 4/25 | Loss: 0.00125801
Iteration 5/25 | Loss: 0.00125084
Iteration 6/25 | Loss: 0.00124938
Iteration 7/25 | Loss: 0.00124919
Iteration 8/25 | Loss: 0.00124919
Iteration 9/25 | Loss: 0.00124919
Iteration 10/25 | Loss: 0.00124919
Iteration 11/25 | Loss: 0.00124919
Iteration 12/25 | Loss: 0.00124919
Iteration 13/25 | Loss: 0.00124919
Iteration 14/25 | Loss: 0.00124919
Iteration 15/25 | Loss: 0.00124919
Iteration 16/25 | Loss: 0.00124919
Iteration 17/25 | Loss: 0.00124919
Iteration 18/25 | Loss: 0.00124919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012491889065131545, 0.0012491889065131545, 0.0012491889065131545, 0.0012491889065131545, 0.0012491889065131545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012491889065131545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39399552
Iteration 2/25 | Loss: 0.00072343
Iteration 3/25 | Loss: 0.00072343
Iteration 4/25 | Loss: 0.00072343
Iteration 5/25 | Loss: 0.00072343
Iteration 6/25 | Loss: 0.00072343
Iteration 7/25 | Loss: 0.00072343
Iteration 8/25 | Loss: 0.00072343
Iteration 9/25 | Loss: 0.00072343
Iteration 10/25 | Loss: 0.00072343
Iteration 11/25 | Loss: 0.00072343
Iteration 12/25 | Loss: 0.00072343
Iteration 13/25 | Loss: 0.00072343
Iteration 14/25 | Loss: 0.00072343
Iteration 15/25 | Loss: 0.00072343
Iteration 16/25 | Loss: 0.00072343
Iteration 17/25 | Loss: 0.00072343
Iteration 18/25 | Loss: 0.00072343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007234304794110358, 0.0007234304794110358, 0.0007234304794110358, 0.0007234304794110358, 0.0007234304794110358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007234304794110358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072343
Iteration 2/1000 | Loss: 0.00005440
Iteration 3/1000 | Loss: 0.00003529
Iteration 4/1000 | Loss: 0.00002858
Iteration 5/1000 | Loss: 0.00002609
Iteration 6/1000 | Loss: 0.00002460
Iteration 7/1000 | Loss: 0.00002322
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002137
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002029
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001974
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001963
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001917
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001913
Iteration 97/1000 | Loss: 0.00001913
Iteration 98/1000 | Loss: 0.00001913
Iteration 99/1000 | Loss: 0.00001913
Iteration 100/1000 | Loss: 0.00001913
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001912
Iteration 108/1000 | Loss: 0.00001912
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001911
Iteration 118/1000 | Loss: 0.00001911
Iteration 119/1000 | Loss: 0.00001911
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001909
Iteration 128/1000 | Loss: 0.00001909
Iteration 129/1000 | Loss: 0.00001909
Iteration 130/1000 | Loss: 0.00001909
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001909
Iteration 133/1000 | Loss: 0.00001909
Iteration 134/1000 | Loss: 0.00001909
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001909
Iteration 137/1000 | Loss: 0.00001909
Iteration 138/1000 | Loss: 0.00001909
Iteration 139/1000 | Loss: 0.00001908
Iteration 140/1000 | Loss: 0.00001908
Iteration 141/1000 | Loss: 0.00001908
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001908
Iteration 144/1000 | Loss: 0.00001908
Iteration 145/1000 | Loss: 0.00001907
Iteration 146/1000 | Loss: 0.00001907
Iteration 147/1000 | Loss: 0.00001907
Iteration 148/1000 | Loss: 0.00001907
Iteration 149/1000 | Loss: 0.00001907
Iteration 150/1000 | Loss: 0.00001907
Iteration 151/1000 | Loss: 0.00001907
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001907
Iteration 154/1000 | Loss: 0.00001907
Iteration 155/1000 | Loss: 0.00001907
Iteration 156/1000 | Loss: 0.00001907
Iteration 157/1000 | Loss: 0.00001906
Iteration 158/1000 | Loss: 0.00001906
Iteration 159/1000 | Loss: 0.00001906
Iteration 160/1000 | Loss: 0.00001906
Iteration 161/1000 | Loss: 0.00001906
Iteration 162/1000 | Loss: 0.00001906
Iteration 163/1000 | Loss: 0.00001906
Iteration 164/1000 | Loss: 0.00001906
Iteration 165/1000 | Loss: 0.00001906
Iteration 166/1000 | Loss: 0.00001905
Iteration 167/1000 | Loss: 0.00001905
Iteration 168/1000 | Loss: 0.00001905
Iteration 169/1000 | Loss: 0.00001905
Iteration 170/1000 | Loss: 0.00001905
Iteration 171/1000 | Loss: 0.00001905
Iteration 172/1000 | Loss: 0.00001905
Iteration 173/1000 | Loss: 0.00001905
Iteration 174/1000 | Loss: 0.00001905
Iteration 175/1000 | Loss: 0.00001905
Iteration 176/1000 | Loss: 0.00001904
Iteration 177/1000 | Loss: 0.00001904
Iteration 178/1000 | Loss: 0.00001904
Iteration 179/1000 | Loss: 0.00001904
Iteration 180/1000 | Loss: 0.00001904
Iteration 181/1000 | Loss: 0.00001904
Iteration 182/1000 | Loss: 0.00001904
Iteration 183/1000 | Loss: 0.00001904
Iteration 184/1000 | Loss: 0.00001904
Iteration 185/1000 | Loss: 0.00001903
Iteration 186/1000 | Loss: 0.00001903
Iteration 187/1000 | Loss: 0.00001903
Iteration 188/1000 | Loss: 0.00001903
Iteration 189/1000 | Loss: 0.00001903
Iteration 190/1000 | Loss: 0.00001903
Iteration 191/1000 | Loss: 0.00001903
Iteration 192/1000 | Loss: 0.00001903
Iteration 193/1000 | Loss: 0.00001903
Iteration 194/1000 | Loss: 0.00001903
Iteration 195/1000 | Loss: 0.00001903
Iteration 196/1000 | Loss: 0.00001903
Iteration 197/1000 | Loss: 0.00001903
Iteration 198/1000 | Loss: 0.00001903
Iteration 199/1000 | Loss: 0.00001903
Iteration 200/1000 | Loss: 0.00001903
Iteration 201/1000 | Loss: 0.00001903
Iteration 202/1000 | Loss: 0.00001903
Iteration 203/1000 | Loss: 0.00001903
Iteration 204/1000 | Loss: 0.00001903
Iteration 205/1000 | Loss: 0.00001903
Iteration 206/1000 | Loss: 0.00001903
Iteration 207/1000 | Loss: 0.00001903
Iteration 208/1000 | Loss: 0.00001903
Iteration 209/1000 | Loss: 0.00001903
Iteration 210/1000 | Loss: 0.00001903
Iteration 211/1000 | Loss: 0.00001903
Iteration 212/1000 | Loss: 0.00001903
Iteration 213/1000 | Loss: 0.00001903
Iteration 214/1000 | Loss: 0.00001903
Iteration 215/1000 | Loss: 0.00001903
Iteration 216/1000 | Loss: 0.00001903
Iteration 217/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.9031735064345412e-05, 1.9031735064345412e-05, 1.9031735064345412e-05, 1.9031735064345412e-05, 1.9031735064345412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9031735064345412e-05

Optimization complete. Final v2v error: 3.6824028491973877 mm

Highest mean error: 4.37652063369751 mm for frame 69

Lowest mean error: 3.217503070831299 mm for frame 83

Saving results

Total time: 46.15533947944641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501946
Iteration 2/25 | Loss: 0.00159122
Iteration 3/25 | Loss: 0.00138594
Iteration 4/25 | Loss: 0.00136532
Iteration 5/25 | Loss: 0.00135947
Iteration 6/25 | Loss: 0.00135897
Iteration 7/25 | Loss: 0.00135897
Iteration 8/25 | Loss: 0.00135897
Iteration 9/25 | Loss: 0.00135897
Iteration 10/25 | Loss: 0.00135897
Iteration 11/25 | Loss: 0.00135897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001358967274427414, 0.001358967274427414, 0.001358967274427414, 0.001358967274427414, 0.001358967274427414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001358967274427414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43105996
Iteration 2/25 | Loss: 0.00083828
Iteration 3/25 | Loss: 0.00083827
Iteration 4/25 | Loss: 0.00083827
Iteration 5/25 | Loss: 0.00083827
Iteration 6/25 | Loss: 0.00083827
Iteration 7/25 | Loss: 0.00083827
Iteration 8/25 | Loss: 0.00083827
Iteration 9/25 | Loss: 0.00083827
Iteration 10/25 | Loss: 0.00083827
Iteration 11/25 | Loss: 0.00083827
Iteration 12/25 | Loss: 0.00083827
Iteration 13/25 | Loss: 0.00083827
Iteration 14/25 | Loss: 0.00083827
Iteration 15/25 | Loss: 0.00083827
Iteration 16/25 | Loss: 0.00083827
Iteration 17/25 | Loss: 0.00083827
Iteration 18/25 | Loss: 0.00083827
Iteration 19/25 | Loss: 0.00083827
Iteration 20/25 | Loss: 0.00083827
Iteration 21/25 | Loss: 0.00083827
Iteration 22/25 | Loss: 0.00083827
Iteration 23/25 | Loss: 0.00083827
Iteration 24/25 | Loss: 0.00083827
Iteration 25/25 | Loss: 0.00083827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083827
Iteration 2/1000 | Loss: 0.00005798
Iteration 3/1000 | Loss: 0.00003774
Iteration 4/1000 | Loss: 0.00003398
Iteration 5/1000 | Loss: 0.00003280
Iteration 6/1000 | Loss: 0.00003147
Iteration 7/1000 | Loss: 0.00003014
Iteration 8/1000 | Loss: 0.00002943
Iteration 9/1000 | Loss: 0.00002899
Iteration 10/1000 | Loss: 0.00002866
Iteration 11/1000 | Loss: 0.00002842
Iteration 12/1000 | Loss: 0.00002825
Iteration 13/1000 | Loss: 0.00002806
Iteration 14/1000 | Loss: 0.00002785
Iteration 15/1000 | Loss: 0.00002766
Iteration 16/1000 | Loss: 0.00002760
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002749
Iteration 19/1000 | Loss: 0.00002740
Iteration 20/1000 | Loss: 0.00002732
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002728
Iteration 23/1000 | Loss: 0.00002728
Iteration 24/1000 | Loss: 0.00002728
Iteration 25/1000 | Loss: 0.00002728
Iteration 26/1000 | Loss: 0.00002727
Iteration 27/1000 | Loss: 0.00002726
Iteration 28/1000 | Loss: 0.00002726
Iteration 29/1000 | Loss: 0.00002726
Iteration 30/1000 | Loss: 0.00002725
Iteration 31/1000 | Loss: 0.00002725
Iteration 32/1000 | Loss: 0.00002725
Iteration 33/1000 | Loss: 0.00002725
Iteration 34/1000 | Loss: 0.00002725
Iteration 35/1000 | Loss: 0.00002722
Iteration 36/1000 | Loss: 0.00002722
Iteration 37/1000 | Loss: 0.00002722
Iteration 38/1000 | Loss: 0.00002722
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002722
Iteration 42/1000 | Loss: 0.00002722
Iteration 43/1000 | Loss: 0.00002722
Iteration 44/1000 | Loss: 0.00002722
Iteration 45/1000 | Loss: 0.00002722
Iteration 46/1000 | Loss: 0.00002721
Iteration 47/1000 | Loss: 0.00002721
Iteration 48/1000 | Loss: 0.00002720
Iteration 49/1000 | Loss: 0.00002720
Iteration 50/1000 | Loss: 0.00002720
Iteration 51/1000 | Loss: 0.00002720
Iteration 52/1000 | Loss: 0.00002719
Iteration 53/1000 | Loss: 0.00002719
Iteration 54/1000 | Loss: 0.00002719
Iteration 55/1000 | Loss: 0.00002719
Iteration 56/1000 | Loss: 0.00002718
Iteration 57/1000 | Loss: 0.00002718
Iteration 58/1000 | Loss: 0.00002718
Iteration 59/1000 | Loss: 0.00002718
Iteration 60/1000 | Loss: 0.00002718
Iteration 61/1000 | Loss: 0.00002717
Iteration 62/1000 | Loss: 0.00002717
Iteration 63/1000 | Loss: 0.00002717
Iteration 64/1000 | Loss: 0.00002717
Iteration 65/1000 | Loss: 0.00002717
Iteration 66/1000 | Loss: 0.00002716
Iteration 67/1000 | Loss: 0.00002716
Iteration 68/1000 | Loss: 0.00002716
Iteration 69/1000 | Loss: 0.00002715
Iteration 70/1000 | Loss: 0.00002715
Iteration 71/1000 | Loss: 0.00002715
Iteration 72/1000 | Loss: 0.00002715
Iteration 73/1000 | Loss: 0.00002714
Iteration 74/1000 | Loss: 0.00002714
Iteration 75/1000 | Loss: 0.00002714
Iteration 76/1000 | Loss: 0.00002714
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002713
Iteration 79/1000 | Loss: 0.00002713
Iteration 80/1000 | Loss: 0.00002713
Iteration 81/1000 | Loss: 0.00002712
Iteration 82/1000 | Loss: 0.00002712
Iteration 83/1000 | Loss: 0.00002712
Iteration 84/1000 | Loss: 0.00002712
Iteration 85/1000 | Loss: 0.00002712
Iteration 86/1000 | Loss: 0.00002712
Iteration 87/1000 | Loss: 0.00002711
Iteration 88/1000 | Loss: 0.00002711
Iteration 89/1000 | Loss: 0.00002711
Iteration 90/1000 | Loss: 0.00002711
Iteration 91/1000 | Loss: 0.00002711
Iteration 92/1000 | Loss: 0.00002711
Iteration 93/1000 | Loss: 0.00002711
Iteration 94/1000 | Loss: 0.00002711
Iteration 95/1000 | Loss: 0.00002711
Iteration 96/1000 | Loss: 0.00002711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.711300112423487e-05, 2.711300112423487e-05, 2.711300112423487e-05, 2.711300112423487e-05, 2.711300112423487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.711300112423487e-05

Optimization complete. Final v2v error: 4.416675090789795 mm

Highest mean error: 5.05941104888916 mm for frame 179

Lowest mean error: 3.970421314239502 mm for frame 153

Saving results

Total time: 45.405781984329224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768308
Iteration 2/25 | Loss: 0.00144566
Iteration 3/25 | Loss: 0.00128745
Iteration 4/25 | Loss: 0.00126026
Iteration 5/25 | Loss: 0.00126462
Iteration 6/25 | Loss: 0.00126282
Iteration 7/25 | Loss: 0.00125716
Iteration 8/25 | Loss: 0.00125327
Iteration 9/25 | Loss: 0.00125759
Iteration 10/25 | Loss: 0.00125382
Iteration 11/25 | Loss: 0.00125186
Iteration 12/25 | Loss: 0.00125178
Iteration 13/25 | Loss: 0.00125178
Iteration 14/25 | Loss: 0.00125178
Iteration 15/25 | Loss: 0.00125177
Iteration 16/25 | Loss: 0.00125177
Iteration 17/25 | Loss: 0.00125177
Iteration 18/25 | Loss: 0.00125177
Iteration 19/25 | Loss: 0.00125177
Iteration 20/25 | Loss: 0.00125177
Iteration 21/25 | Loss: 0.00125177
Iteration 22/25 | Loss: 0.00125177
Iteration 23/25 | Loss: 0.00125177
Iteration 24/25 | Loss: 0.00125177
Iteration 25/25 | Loss: 0.00125177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48082066
Iteration 2/25 | Loss: 0.00095188
Iteration 3/25 | Loss: 0.00090420
Iteration 4/25 | Loss: 0.00089797
Iteration 5/25 | Loss: 0.00089796
Iteration 6/25 | Loss: 0.00089796
Iteration 7/25 | Loss: 0.00089796
Iteration 8/25 | Loss: 0.00089796
Iteration 9/25 | Loss: 0.00089796
Iteration 10/25 | Loss: 0.00089796
Iteration 11/25 | Loss: 0.00089796
Iteration 12/25 | Loss: 0.00089796
Iteration 13/25 | Loss: 0.00089796
Iteration 14/25 | Loss: 0.00089796
Iteration 15/25 | Loss: 0.00089796
Iteration 16/25 | Loss: 0.00089796
Iteration 17/25 | Loss: 0.00089796
Iteration 18/25 | Loss: 0.00089796
Iteration 19/25 | Loss: 0.00089796
Iteration 20/25 | Loss: 0.00089796
Iteration 21/25 | Loss: 0.00089796
Iteration 22/25 | Loss: 0.00089796
Iteration 23/25 | Loss: 0.00089796
Iteration 24/25 | Loss: 0.00089796
Iteration 25/25 | Loss: 0.00089796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000897963356692344, 0.000897963356692344, 0.000897963356692344, 0.000897963356692344, 0.000897963356692344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000897963356692344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089796
Iteration 2/1000 | Loss: 0.00013732
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00005087
Iteration 6/1000 | Loss: 0.00004765
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00002446
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001478
Iteration 12/1000 | Loss: 0.00003055
Iteration 13/1000 | Loss: 0.00003971
Iteration 14/1000 | Loss: 0.00001443
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00004079
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001370
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00004398
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001359
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001359
Iteration 36/1000 | Loss: 0.00001359
Iteration 37/1000 | Loss: 0.00001359
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001356
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00001356
Iteration 43/1000 | Loss: 0.00001355
Iteration 44/1000 | Loss: 0.00001355
Iteration 45/1000 | Loss: 0.00002360
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002497
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00002783
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00002337
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00003207
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001648
Iteration 114/1000 | Loss: 0.00001570
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001328
Iteration 139/1000 | Loss: 0.00001328
Iteration 140/1000 | Loss: 0.00001328
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.3278923688631039e-05, 1.3278923688631039e-05, 1.3278923688631039e-05, 1.3278923688631039e-05, 1.3278923688631039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3278923688631039e-05

Optimization complete. Final v2v error: 3.1197447776794434 mm

Highest mean error: 3.808117151260376 mm for frame 44

Lowest mean error: 2.816744327545166 mm for frame 204

Saving results

Total time: 79.66475677490234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455315
Iteration 2/25 | Loss: 0.00131453
Iteration 3/25 | Loss: 0.00123112
Iteration 4/25 | Loss: 0.00122342
Iteration 5/25 | Loss: 0.00122132
Iteration 6/25 | Loss: 0.00122089
Iteration 7/25 | Loss: 0.00122089
Iteration 8/25 | Loss: 0.00122089
Iteration 9/25 | Loss: 0.00122089
Iteration 10/25 | Loss: 0.00122089
Iteration 11/25 | Loss: 0.00122089
Iteration 12/25 | Loss: 0.00122089
Iteration 13/25 | Loss: 0.00122089
Iteration 14/25 | Loss: 0.00122089
Iteration 15/25 | Loss: 0.00122089
Iteration 16/25 | Loss: 0.00122089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012208906700834632, 0.0012208906700834632, 0.0012208906700834632, 0.0012208906700834632, 0.0012208906700834632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012208906700834632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37201977
Iteration 2/25 | Loss: 0.00073915
Iteration 3/25 | Loss: 0.00073913
Iteration 4/25 | Loss: 0.00073913
Iteration 5/25 | Loss: 0.00073913
Iteration 6/25 | Loss: 0.00073913
Iteration 7/25 | Loss: 0.00073913
Iteration 8/25 | Loss: 0.00073913
Iteration 9/25 | Loss: 0.00073913
Iteration 10/25 | Loss: 0.00073913
Iteration 11/25 | Loss: 0.00073913
Iteration 12/25 | Loss: 0.00073913
Iteration 13/25 | Loss: 0.00073913
Iteration 14/25 | Loss: 0.00073913
Iteration 15/25 | Loss: 0.00073913
Iteration 16/25 | Loss: 0.00073913
Iteration 17/25 | Loss: 0.00073913
Iteration 18/25 | Loss: 0.00073913
Iteration 19/25 | Loss: 0.00073913
Iteration 20/25 | Loss: 0.00073913
Iteration 21/25 | Loss: 0.00073913
Iteration 22/25 | Loss: 0.00073913
Iteration 23/25 | Loss: 0.00073913
Iteration 24/25 | Loss: 0.00073913
Iteration 25/25 | Loss: 0.00073913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073913
Iteration 2/1000 | Loss: 0.00003119
Iteration 3/1000 | Loss: 0.00002168
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001524
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001415
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001311
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001292
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001285
Iteration 31/1000 | Loss: 0.00001285
Iteration 32/1000 | Loss: 0.00001284
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001282
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001278
Iteration 41/1000 | Loss: 0.00001278
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001277
Iteration 45/1000 | Loss: 0.00001277
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001261
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001249
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001243
Iteration 116/1000 | Loss: 0.00001243
Iteration 117/1000 | Loss: 0.00001243
Iteration 118/1000 | Loss: 0.00001242
Iteration 119/1000 | Loss: 0.00001242
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001238
Iteration 132/1000 | Loss: 0.00001238
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001233
Iteration 167/1000 | Loss: 0.00001233
Iteration 168/1000 | Loss: 0.00001233
Iteration 169/1000 | Loss: 0.00001233
Iteration 170/1000 | Loss: 0.00001233
Iteration 171/1000 | Loss: 0.00001233
Iteration 172/1000 | Loss: 0.00001233
Iteration 173/1000 | Loss: 0.00001233
Iteration 174/1000 | Loss: 0.00001233
Iteration 175/1000 | Loss: 0.00001233
Iteration 176/1000 | Loss: 0.00001233
Iteration 177/1000 | Loss: 0.00001233
Iteration 178/1000 | Loss: 0.00001233
Iteration 179/1000 | Loss: 0.00001232
Iteration 180/1000 | Loss: 0.00001232
Iteration 181/1000 | Loss: 0.00001232
Iteration 182/1000 | Loss: 0.00001232
Iteration 183/1000 | Loss: 0.00001232
Iteration 184/1000 | Loss: 0.00001232
Iteration 185/1000 | Loss: 0.00001232
Iteration 186/1000 | Loss: 0.00001232
Iteration 187/1000 | Loss: 0.00001232
Iteration 188/1000 | Loss: 0.00001232
Iteration 189/1000 | Loss: 0.00001232
Iteration 190/1000 | Loss: 0.00001232
Iteration 191/1000 | Loss: 0.00001232
Iteration 192/1000 | Loss: 0.00001232
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001231
Iteration 195/1000 | Loss: 0.00001231
Iteration 196/1000 | Loss: 0.00001231
Iteration 197/1000 | Loss: 0.00001231
Iteration 198/1000 | Loss: 0.00001231
Iteration 199/1000 | Loss: 0.00001231
Iteration 200/1000 | Loss: 0.00001231
Iteration 201/1000 | Loss: 0.00001231
Iteration 202/1000 | Loss: 0.00001231
Iteration 203/1000 | Loss: 0.00001231
Iteration 204/1000 | Loss: 0.00001231
Iteration 205/1000 | Loss: 0.00001231
Iteration 206/1000 | Loss: 0.00001231
Iteration 207/1000 | Loss: 0.00001231
Iteration 208/1000 | Loss: 0.00001231
Iteration 209/1000 | Loss: 0.00001231
Iteration 210/1000 | Loss: 0.00001231
Iteration 211/1000 | Loss: 0.00001231
Iteration 212/1000 | Loss: 0.00001231
Iteration 213/1000 | Loss: 0.00001231
Iteration 214/1000 | Loss: 0.00001231
Iteration 215/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.2311183127167169e-05, 1.2311183127167169e-05, 1.2311183127167169e-05, 1.2311183127167169e-05, 1.2311183127167169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2311183127167169e-05

Optimization complete. Final v2v error: 2.997089385986328 mm

Highest mean error: 5.176539421081543 mm for frame 220

Lowest mean error: 2.7597029209136963 mm for frame 88

Saving results

Total time: 51.47505307197571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808913
Iteration 2/25 | Loss: 0.00238820
Iteration 3/25 | Loss: 0.00192814
Iteration 4/25 | Loss: 0.00182104
Iteration 5/25 | Loss: 0.00152842
Iteration 6/25 | Loss: 0.00144594
Iteration 7/25 | Loss: 0.00144296
Iteration 8/25 | Loss: 0.00144288
Iteration 9/25 | Loss: 0.00144288
Iteration 10/25 | Loss: 0.00144288
Iteration 11/25 | Loss: 0.00144288
Iteration 12/25 | Loss: 0.00144288
Iteration 13/25 | Loss: 0.00144288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014428755966946483, 0.0014428755966946483, 0.0014428755966946483, 0.0014428755966946483, 0.0014428755966946483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014428755966946483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42157316
Iteration 2/25 | Loss: 0.00076658
Iteration 3/25 | Loss: 0.00076657
Iteration 4/25 | Loss: 0.00076657
Iteration 5/25 | Loss: 0.00076657
Iteration 6/25 | Loss: 0.00076657
Iteration 7/25 | Loss: 0.00076657
Iteration 8/25 | Loss: 0.00076657
Iteration 9/25 | Loss: 0.00076657
Iteration 10/25 | Loss: 0.00076657
Iteration 11/25 | Loss: 0.00076657
Iteration 12/25 | Loss: 0.00076657
Iteration 13/25 | Loss: 0.00076657
Iteration 14/25 | Loss: 0.00076657
Iteration 15/25 | Loss: 0.00076657
Iteration 16/25 | Loss: 0.00076657
Iteration 17/25 | Loss: 0.00076657
Iteration 18/25 | Loss: 0.00076657
Iteration 19/25 | Loss: 0.00076657
Iteration 20/25 | Loss: 0.00076657
Iteration 21/25 | Loss: 0.00076657
Iteration 22/25 | Loss: 0.00076657
Iteration 23/25 | Loss: 0.00076657
Iteration 24/25 | Loss: 0.00076657
Iteration 25/25 | Loss: 0.00076657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076657
Iteration 2/1000 | Loss: 0.00004658
Iteration 3/1000 | Loss: 0.00002966
Iteration 4/1000 | Loss: 0.00002632
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002323
Iteration 9/1000 | Loss: 0.00002315
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002284
Iteration 12/1000 | Loss: 0.00002273
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002273
Iteration 16/1000 | Loss: 0.00002273
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002273
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002273
Iteration 22/1000 | Loss: 0.00002272
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002270
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002264
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002261
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002261
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002260
Iteration 40/1000 | Loss: 0.00002260
Iteration 41/1000 | Loss: 0.00002259
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002259
Iteration 44/1000 | Loss: 0.00002259
Iteration 45/1000 | Loss: 0.00002259
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002257
Iteration 49/1000 | Loss: 0.00002257
Iteration 50/1000 | Loss: 0.00002257
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002257
Iteration 54/1000 | Loss: 0.00002257
Iteration 55/1000 | Loss: 0.00002257
Iteration 56/1000 | Loss: 0.00002257
Iteration 57/1000 | Loss: 0.00002256
Iteration 58/1000 | Loss: 0.00002256
Iteration 59/1000 | Loss: 0.00002256
Iteration 60/1000 | Loss: 0.00002255
Iteration 61/1000 | Loss: 0.00002255
Iteration 62/1000 | Loss: 0.00002255
Iteration 63/1000 | Loss: 0.00002255
Iteration 64/1000 | Loss: 0.00002255
Iteration 65/1000 | Loss: 0.00002255
Iteration 66/1000 | Loss: 0.00002255
Iteration 67/1000 | Loss: 0.00002255
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002255
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002255
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002255
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.2553291273652576e-05, 2.2553291273652576e-05, 2.2553291273652576e-05, 2.2553291273652576e-05, 2.2553291273652576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2553291273652576e-05

Optimization complete. Final v2v error: 4.034567356109619 mm

Highest mean error: 4.214487075805664 mm for frame 202

Lowest mean error: 3.865145683288574 mm for frame 10

Saving results

Total time: 34.327632188797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414284
Iteration 2/25 | Loss: 0.00136459
Iteration 3/25 | Loss: 0.00126657
Iteration 4/25 | Loss: 0.00125713
Iteration 5/25 | Loss: 0.00125361
Iteration 6/25 | Loss: 0.00125330
Iteration 7/25 | Loss: 0.00125326
Iteration 8/25 | Loss: 0.00125326
Iteration 9/25 | Loss: 0.00125326
Iteration 10/25 | Loss: 0.00125326
Iteration 11/25 | Loss: 0.00125326
Iteration 12/25 | Loss: 0.00125326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012532639084383845, 0.0012532639084383845, 0.0012532639084383845, 0.0012532639084383845, 0.0012532639084383845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012532639084383845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12329268
Iteration 2/25 | Loss: 0.00085985
Iteration 3/25 | Loss: 0.00085984
Iteration 4/25 | Loss: 0.00085984
Iteration 5/25 | Loss: 0.00085984
Iteration 6/25 | Loss: 0.00085984
Iteration 7/25 | Loss: 0.00085984
Iteration 8/25 | Loss: 0.00085984
Iteration 9/25 | Loss: 0.00085984
Iteration 10/25 | Loss: 0.00085984
Iteration 11/25 | Loss: 0.00085983
Iteration 12/25 | Loss: 0.00085983
Iteration 13/25 | Loss: 0.00085983
Iteration 14/25 | Loss: 0.00085983
Iteration 15/25 | Loss: 0.00085983
Iteration 16/25 | Loss: 0.00085983
Iteration 17/25 | Loss: 0.00085983
Iteration 18/25 | Loss: 0.00085983
Iteration 19/25 | Loss: 0.00085983
Iteration 20/25 | Loss: 0.00085983
Iteration 21/25 | Loss: 0.00085983
Iteration 22/25 | Loss: 0.00085983
Iteration 23/25 | Loss: 0.00085983
Iteration 24/25 | Loss: 0.00085983
Iteration 25/25 | Loss: 0.00085983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085983
Iteration 2/1000 | Loss: 0.00003249
Iteration 3/1000 | Loss: 0.00002194
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001536
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001483
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001423
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001418
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001413
Iteration 125/1000 | Loss: 0.00001413
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001408
Iteration 146/1000 | Loss: 0.00001408
Iteration 147/1000 | Loss: 0.00001408
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001408
Iteration 150/1000 | Loss: 0.00001408
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001407
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001406
Iteration 169/1000 | Loss: 0.00001406
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001405
Iteration 179/1000 | Loss: 0.00001405
Iteration 180/1000 | Loss: 0.00001405
Iteration 181/1000 | Loss: 0.00001405
Iteration 182/1000 | Loss: 0.00001405
Iteration 183/1000 | Loss: 0.00001405
Iteration 184/1000 | Loss: 0.00001405
Iteration 185/1000 | Loss: 0.00001405
Iteration 186/1000 | Loss: 0.00001405
Iteration 187/1000 | Loss: 0.00001405
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001404
Iteration 190/1000 | Loss: 0.00001404
Iteration 191/1000 | Loss: 0.00001404
Iteration 192/1000 | Loss: 0.00001404
Iteration 193/1000 | Loss: 0.00001404
Iteration 194/1000 | Loss: 0.00001404
Iteration 195/1000 | Loss: 0.00001404
Iteration 196/1000 | Loss: 0.00001404
Iteration 197/1000 | Loss: 0.00001404
Iteration 198/1000 | Loss: 0.00001404
Iteration 199/1000 | Loss: 0.00001404
Iteration 200/1000 | Loss: 0.00001404
Iteration 201/1000 | Loss: 0.00001404
Iteration 202/1000 | Loss: 0.00001404
Iteration 203/1000 | Loss: 0.00001404
Iteration 204/1000 | Loss: 0.00001404
Iteration 205/1000 | Loss: 0.00001404
Iteration 206/1000 | Loss: 0.00001404
Iteration 207/1000 | Loss: 0.00001403
Iteration 208/1000 | Loss: 0.00001403
Iteration 209/1000 | Loss: 0.00001403
Iteration 210/1000 | Loss: 0.00001403
Iteration 211/1000 | Loss: 0.00001403
Iteration 212/1000 | Loss: 0.00001403
Iteration 213/1000 | Loss: 0.00001403
Iteration 214/1000 | Loss: 0.00001403
Iteration 215/1000 | Loss: 0.00001403
Iteration 216/1000 | Loss: 0.00001403
Iteration 217/1000 | Loss: 0.00001402
Iteration 218/1000 | Loss: 0.00001402
Iteration 219/1000 | Loss: 0.00001402
Iteration 220/1000 | Loss: 0.00001402
Iteration 221/1000 | Loss: 0.00001402
Iteration 222/1000 | Loss: 0.00001402
Iteration 223/1000 | Loss: 0.00001402
Iteration 224/1000 | Loss: 0.00001402
Iteration 225/1000 | Loss: 0.00001402
Iteration 226/1000 | Loss: 0.00001402
Iteration 227/1000 | Loss: 0.00001402
Iteration 228/1000 | Loss: 0.00001402
Iteration 229/1000 | Loss: 0.00001402
Iteration 230/1000 | Loss: 0.00001402
Iteration 231/1000 | Loss: 0.00001402
Iteration 232/1000 | Loss: 0.00001402
Iteration 233/1000 | Loss: 0.00001402
Iteration 234/1000 | Loss: 0.00001402
Iteration 235/1000 | Loss: 0.00001402
Iteration 236/1000 | Loss: 0.00001402
Iteration 237/1000 | Loss: 0.00001402
Iteration 238/1000 | Loss: 0.00001401
Iteration 239/1000 | Loss: 0.00001401
Iteration 240/1000 | Loss: 0.00001401
Iteration 241/1000 | Loss: 0.00001401
Iteration 242/1000 | Loss: 0.00001401
Iteration 243/1000 | Loss: 0.00001401
Iteration 244/1000 | Loss: 0.00001401
Iteration 245/1000 | Loss: 0.00001401
Iteration 246/1000 | Loss: 0.00001401
Iteration 247/1000 | Loss: 0.00001401
Iteration 248/1000 | Loss: 0.00001401
Iteration 249/1000 | Loss: 0.00001401
Iteration 250/1000 | Loss: 0.00001401
Iteration 251/1000 | Loss: 0.00001401
Iteration 252/1000 | Loss: 0.00001401
Iteration 253/1000 | Loss: 0.00001401
Iteration 254/1000 | Loss: 0.00001401
Iteration 255/1000 | Loss: 0.00001401
Iteration 256/1000 | Loss: 0.00001400
Iteration 257/1000 | Loss: 0.00001400
Iteration 258/1000 | Loss: 0.00001400
Iteration 259/1000 | Loss: 0.00001400
Iteration 260/1000 | Loss: 0.00001400
Iteration 261/1000 | Loss: 0.00001400
Iteration 262/1000 | Loss: 0.00001400
Iteration 263/1000 | Loss: 0.00001400
Iteration 264/1000 | Loss: 0.00001400
Iteration 265/1000 | Loss: 0.00001400
Iteration 266/1000 | Loss: 0.00001400
Iteration 267/1000 | Loss: 0.00001400
Iteration 268/1000 | Loss: 0.00001400
Iteration 269/1000 | Loss: 0.00001400
Iteration 270/1000 | Loss: 0.00001400
Iteration 271/1000 | Loss: 0.00001400
Iteration 272/1000 | Loss: 0.00001400
Iteration 273/1000 | Loss: 0.00001400
Iteration 274/1000 | Loss: 0.00001400
Iteration 275/1000 | Loss: 0.00001400
Iteration 276/1000 | Loss: 0.00001400
Iteration 277/1000 | Loss: 0.00001400
Iteration 278/1000 | Loss: 0.00001399
Iteration 279/1000 | Loss: 0.00001399
Iteration 280/1000 | Loss: 0.00001399
Iteration 281/1000 | Loss: 0.00001399
Iteration 282/1000 | Loss: 0.00001399
Iteration 283/1000 | Loss: 0.00001399
Iteration 284/1000 | Loss: 0.00001399
Iteration 285/1000 | Loss: 0.00001399
Iteration 286/1000 | Loss: 0.00001399
Iteration 287/1000 | Loss: 0.00001399
Iteration 288/1000 | Loss: 0.00001399
Iteration 289/1000 | Loss: 0.00001399
Iteration 290/1000 | Loss: 0.00001399
Iteration 291/1000 | Loss: 0.00001399
Iteration 292/1000 | Loss: 0.00001399
Iteration 293/1000 | Loss: 0.00001399
Iteration 294/1000 | Loss: 0.00001398
Iteration 295/1000 | Loss: 0.00001398
Iteration 296/1000 | Loss: 0.00001398
Iteration 297/1000 | Loss: 0.00001398
Iteration 298/1000 | Loss: 0.00001398
Iteration 299/1000 | Loss: 0.00001398
Iteration 300/1000 | Loss: 0.00001398
Iteration 301/1000 | Loss: 0.00001398
Iteration 302/1000 | Loss: 0.00001398
Iteration 303/1000 | Loss: 0.00001398
Iteration 304/1000 | Loss: 0.00001398
Iteration 305/1000 | Loss: 0.00001398
Iteration 306/1000 | Loss: 0.00001398
Iteration 307/1000 | Loss: 0.00001398
Iteration 308/1000 | Loss: 0.00001398
Iteration 309/1000 | Loss: 0.00001398
Iteration 310/1000 | Loss: 0.00001398
Iteration 311/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.3983046301291324e-05, 1.3983046301291324e-05, 1.3983046301291324e-05, 1.3983046301291324e-05, 1.3983046301291324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3983046301291324e-05

Optimization complete. Final v2v error: 3.202939987182617 mm

Highest mean error: 3.6688311100006104 mm for frame 38

Lowest mean error: 2.936046838760376 mm for frame 26

Saving results

Total time: 49.636213302612305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919305
Iteration 2/25 | Loss: 0.00220565
Iteration 3/25 | Loss: 0.00163923
Iteration 4/25 | Loss: 0.00156096
Iteration 5/25 | Loss: 0.00155376
Iteration 6/25 | Loss: 0.00154475
Iteration 7/25 | Loss: 0.00153181
Iteration 8/25 | Loss: 0.00150790
Iteration 9/25 | Loss: 0.00149810
Iteration 10/25 | Loss: 0.00149445
Iteration 11/25 | Loss: 0.00149126
Iteration 12/25 | Loss: 0.00149124
Iteration 13/25 | Loss: 0.00148732
Iteration 14/25 | Loss: 0.00148515
Iteration 15/25 | Loss: 0.00148314
Iteration 16/25 | Loss: 0.00148182
Iteration 17/25 | Loss: 0.00148104
Iteration 18/25 | Loss: 0.00147962
Iteration 19/25 | Loss: 0.00147888
Iteration 20/25 | Loss: 0.00147822
Iteration 21/25 | Loss: 0.00147926
Iteration 22/25 | Loss: 0.00148586
Iteration 23/25 | Loss: 0.00148405
Iteration 24/25 | Loss: 0.00148155
Iteration 25/25 | Loss: 0.00148070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40708780
Iteration 2/25 | Loss: 0.00273490
Iteration 3/25 | Loss: 0.00273490
Iteration 4/25 | Loss: 0.00273489
Iteration 5/25 | Loss: 0.00273489
Iteration 6/25 | Loss: 0.00273489
Iteration 7/25 | Loss: 0.00273489
Iteration 8/25 | Loss: 0.00273489
Iteration 9/25 | Loss: 0.00273489
Iteration 10/25 | Loss: 0.00273489
Iteration 11/25 | Loss: 0.00273489
Iteration 12/25 | Loss: 0.00273489
Iteration 13/25 | Loss: 0.00273489
Iteration 14/25 | Loss: 0.00273489
Iteration 15/25 | Loss: 0.00273489
Iteration 16/25 | Loss: 0.00273489
Iteration 17/25 | Loss: 0.00273489
Iteration 18/25 | Loss: 0.00273489
Iteration 19/25 | Loss: 0.00273489
Iteration 20/25 | Loss: 0.00273489
Iteration 21/25 | Loss: 0.00273489
Iteration 22/25 | Loss: 0.00273489
Iteration 23/25 | Loss: 0.00273489
Iteration 24/25 | Loss: 0.00273489
Iteration 25/25 | Loss: 0.00273489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273489
Iteration 2/1000 | Loss: 0.00027937
Iteration 3/1000 | Loss: 0.00174981
Iteration 4/1000 | Loss: 0.00236553
Iteration 5/1000 | Loss: 0.00232861
Iteration 6/1000 | Loss: 0.00204423
Iteration 7/1000 | Loss: 0.00116464
Iteration 8/1000 | Loss: 0.00078684
Iteration 9/1000 | Loss: 0.00182252
Iteration 10/1000 | Loss: 0.00035996
Iteration 11/1000 | Loss: 0.00012163
Iteration 12/1000 | Loss: 0.00069287
Iteration 13/1000 | Loss: 0.00010677
Iteration 14/1000 | Loss: 0.00116201
Iteration 15/1000 | Loss: 0.00084869
Iteration 16/1000 | Loss: 0.00066388
Iteration 17/1000 | Loss: 0.00127179
Iteration 18/1000 | Loss: 0.00006569
Iteration 19/1000 | Loss: 0.00062838
Iteration 20/1000 | Loss: 0.00006342
Iteration 21/1000 | Loss: 0.00005573
Iteration 22/1000 | Loss: 0.00004704
Iteration 23/1000 | Loss: 0.00003978
Iteration 24/1000 | Loss: 0.00104491
Iteration 25/1000 | Loss: 0.00006113
Iteration 26/1000 | Loss: 0.00004029
Iteration 27/1000 | Loss: 0.00005387
Iteration 28/1000 | Loss: 0.00003801
Iteration 29/1000 | Loss: 0.00005210
Iteration 30/1000 | Loss: 0.00005235
Iteration 31/1000 | Loss: 0.00005005
Iteration 32/1000 | Loss: 0.00003895
Iteration 33/1000 | Loss: 0.00003476
Iteration 34/1000 | Loss: 0.00004531
Iteration 35/1000 | Loss: 0.00002951
Iteration 36/1000 | Loss: 0.00082525
Iteration 37/1000 | Loss: 0.00003518
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00003359
Iteration 40/1000 | Loss: 0.00002856
Iteration 41/1000 | Loss: 0.00003910
Iteration 42/1000 | Loss: 0.00004467
Iteration 43/1000 | Loss: 0.00004184
Iteration 44/1000 | Loss: 0.00004380
Iteration 45/1000 | Loss: 0.00004632
Iteration 46/1000 | Loss: 0.00003430
Iteration 47/1000 | Loss: 0.00002961
Iteration 48/1000 | Loss: 0.00003066
Iteration 49/1000 | Loss: 0.00004727
Iteration 50/1000 | Loss: 0.00003615
Iteration 51/1000 | Loss: 0.00002618
Iteration 52/1000 | Loss: 0.00004388
Iteration 53/1000 | Loss: 0.00003834
Iteration 54/1000 | Loss: 0.00002613
Iteration 55/1000 | Loss: 0.00004162
Iteration 56/1000 | Loss: 0.00004783
Iteration 57/1000 | Loss: 0.00004300
Iteration 58/1000 | Loss: 0.00003583
Iteration 59/1000 | Loss: 0.00004000
Iteration 60/1000 | Loss: 0.00004415
Iteration 61/1000 | Loss: 0.00003849
Iteration 62/1000 | Loss: 0.00004684
Iteration 63/1000 | Loss: 0.00003309
Iteration 64/1000 | Loss: 0.00004044
Iteration 65/1000 | Loss: 0.00005374
Iteration 66/1000 | Loss: 0.00005342
Iteration 67/1000 | Loss: 0.00003963
Iteration 68/1000 | Loss: 0.00003012
Iteration 69/1000 | Loss: 0.00004216
Iteration 70/1000 | Loss: 0.00004445
Iteration 71/1000 | Loss: 0.00002597
Iteration 72/1000 | Loss: 0.00002404
Iteration 73/1000 | Loss: 0.00002318
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002235
Iteration 76/1000 | Loss: 0.00002211
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002133
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002101
Iteration 82/1000 | Loss: 0.00002100
Iteration 83/1000 | Loss: 0.00002099
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002096
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002090
Iteration 89/1000 | Loss: 0.00002090
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002087
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002082
Iteration 100/1000 | Loss: 0.00002082
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002080
Iteration 103/1000 | Loss: 0.00002080
Iteration 104/1000 | Loss: 0.00002079
Iteration 105/1000 | Loss: 0.00002078
Iteration 106/1000 | Loss: 0.00002078
Iteration 107/1000 | Loss: 0.00002078
Iteration 108/1000 | Loss: 0.00002077
Iteration 109/1000 | Loss: 0.00002077
Iteration 110/1000 | Loss: 0.00002077
Iteration 111/1000 | Loss: 0.00002076
Iteration 112/1000 | Loss: 0.00002076
Iteration 113/1000 | Loss: 0.00002075
Iteration 114/1000 | Loss: 0.00002075
Iteration 115/1000 | Loss: 0.00002075
Iteration 116/1000 | Loss: 0.00002075
Iteration 117/1000 | Loss: 0.00002074
Iteration 118/1000 | Loss: 0.00002074
Iteration 119/1000 | Loss: 0.00002073
Iteration 120/1000 | Loss: 0.00002071
Iteration 121/1000 | Loss: 0.00002070
Iteration 122/1000 | Loss: 0.00002069
Iteration 123/1000 | Loss: 0.00002069
Iteration 124/1000 | Loss: 0.00002068
Iteration 125/1000 | Loss: 0.00002068
Iteration 126/1000 | Loss: 0.00002067
Iteration 127/1000 | Loss: 0.00002067
Iteration 128/1000 | Loss: 0.00002067
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002065
Iteration 131/1000 | Loss: 0.00002064
Iteration 132/1000 | Loss: 0.00002064
Iteration 133/1000 | Loss: 0.00002064
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002062
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002061
Iteration 139/1000 | Loss: 0.00002060
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002059
Iteration 142/1000 | Loss: 0.00002059
Iteration 143/1000 | Loss: 0.00002058
Iteration 144/1000 | Loss: 0.00002058
Iteration 145/1000 | Loss: 0.00002058
Iteration 146/1000 | Loss: 0.00002058
Iteration 147/1000 | Loss: 0.00002058
Iteration 148/1000 | Loss: 0.00002057
Iteration 149/1000 | Loss: 0.00002057
Iteration 150/1000 | Loss: 0.00002057
Iteration 151/1000 | Loss: 0.00002057
Iteration 152/1000 | Loss: 0.00002057
Iteration 153/1000 | Loss: 0.00002057
Iteration 154/1000 | Loss: 0.00002057
Iteration 155/1000 | Loss: 0.00002056
Iteration 156/1000 | Loss: 0.00002056
Iteration 157/1000 | Loss: 0.00002056
Iteration 158/1000 | Loss: 0.00002056
Iteration 159/1000 | Loss: 0.00002056
Iteration 160/1000 | Loss: 0.00002056
Iteration 161/1000 | Loss: 0.00002055
Iteration 162/1000 | Loss: 0.00002055
Iteration 163/1000 | Loss: 0.00002055
Iteration 164/1000 | Loss: 0.00002055
Iteration 165/1000 | Loss: 0.00002054
Iteration 166/1000 | Loss: 0.00002054
Iteration 167/1000 | Loss: 0.00002054
Iteration 168/1000 | Loss: 0.00002054
Iteration 169/1000 | Loss: 0.00002054
Iteration 170/1000 | Loss: 0.00002054
Iteration 171/1000 | Loss: 0.00002054
Iteration 172/1000 | Loss: 0.00002054
Iteration 173/1000 | Loss: 0.00002054
Iteration 174/1000 | Loss: 0.00002054
Iteration 175/1000 | Loss: 0.00002054
Iteration 176/1000 | Loss: 0.00002053
Iteration 177/1000 | Loss: 0.00002053
Iteration 178/1000 | Loss: 0.00002053
Iteration 179/1000 | Loss: 0.00002053
Iteration 180/1000 | Loss: 0.00002053
Iteration 181/1000 | Loss: 0.00002053
Iteration 182/1000 | Loss: 0.00002053
Iteration 183/1000 | Loss: 0.00002052
Iteration 184/1000 | Loss: 0.00002052
Iteration 185/1000 | Loss: 0.00002052
Iteration 186/1000 | Loss: 0.00002052
Iteration 187/1000 | Loss: 0.00002052
Iteration 188/1000 | Loss: 0.00002052
Iteration 189/1000 | Loss: 0.00002052
Iteration 190/1000 | Loss: 0.00002052
Iteration 191/1000 | Loss: 0.00002052
Iteration 192/1000 | Loss: 0.00002052
Iteration 193/1000 | Loss: 0.00002052
Iteration 194/1000 | Loss: 0.00002052
Iteration 195/1000 | Loss: 0.00002052
Iteration 196/1000 | Loss: 0.00002052
Iteration 197/1000 | Loss: 0.00002052
Iteration 198/1000 | Loss: 0.00002052
Iteration 199/1000 | Loss: 0.00002052
Iteration 200/1000 | Loss: 0.00002052
Iteration 201/1000 | Loss: 0.00002052
Iteration 202/1000 | Loss: 0.00002052
Iteration 203/1000 | Loss: 0.00002052
Iteration 204/1000 | Loss: 0.00002051
Iteration 205/1000 | Loss: 0.00002051
Iteration 206/1000 | Loss: 0.00002051
Iteration 207/1000 | Loss: 0.00002051
Iteration 208/1000 | Loss: 0.00002051
Iteration 209/1000 | Loss: 0.00002051
Iteration 210/1000 | Loss: 0.00002051
Iteration 211/1000 | Loss: 0.00002051
Iteration 212/1000 | Loss: 0.00002051
Iteration 213/1000 | Loss: 0.00002051
Iteration 214/1000 | Loss: 0.00002051
Iteration 215/1000 | Loss: 0.00002051
Iteration 216/1000 | Loss: 0.00002051
Iteration 217/1000 | Loss: 0.00002051
Iteration 218/1000 | Loss: 0.00002051
Iteration 219/1000 | Loss: 0.00002051
Iteration 220/1000 | Loss: 0.00002051
Iteration 221/1000 | Loss: 0.00002051
Iteration 222/1000 | Loss: 0.00002051
Iteration 223/1000 | Loss: 0.00002051
Iteration 224/1000 | Loss: 0.00002051
Iteration 225/1000 | Loss: 0.00002051
Iteration 226/1000 | Loss: 0.00002051
Iteration 227/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.051486990239937e-05, 2.051486990239937e-05, 2.051486990239937e-05, 2.051486990239937e-05, 2.051486990239937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.051486990239937e-05

Optimization complete. Final v2v error: 3.4923336505889893 mm

Highest mean error: 12.74294662475586 mm for frame 53

Lowest mean error: 2.8581998348236084 mm for frame 107

Saving results

Total time: 171.5241823196411
