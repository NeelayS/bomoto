Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=255, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14280-14335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386529
Iteration 2/25 | Loss: 0.00134153
Iteration 3/25 | Loss: 0.00128714
Iteration 4/25 | Loss: 0.00127975
Iteration 5/25 | Loss: 0.00127759
Iteration 6/25 | Loss: 0.00127751
Iteration 7/25 | Loss: 0.00127751
Iteration 8/25 | Loss: 0.00127751
Iteration 9/25 | Loss: 0.00127751
Iteration 10/25 | Loss: 0.00127751
Iteration 11/25 | Loss: 0.00127751
Iteration 12/25 | Loss: 0.00127751
Iteration 13/25 | Loss: 0.00127751
Iteration 14/25 | Loss: 0.00127751
Iteration 15/25 | Loss: 0.00127751
Iteration 16/25 | Loss: 0.00127751
Iteration 17/25 | Loss: 0.00127751
Iteration 18/25 | Loss: 0.00127751
Iteration 19/25 | Loss: 0.00127751
Iteration 20/25 | Loss: 0.00127751
Iteration 21/25 | Loss: 0.00127751
Iteration 22/25 | Loss: 0.00127751
Iteration 23/25 | Loss: 0.00127751
Iteration 24/25 | Loss: 0.00127751
Iteration 25/25 | Loss: 0.00127751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36733449
Iteration 2/25 | Loss: 0.00140816
Iteration 3/25 | Loss: 0.00140816
Iteration 4/25 | Loss: 0.00140816
Iteration 5/25 | Loss: 0.00140816
Iteration 6/25 | Loss: 0.00140816
Iteration 7/25 | Loss: 0.00140816
Iteration 8/25 | Loss: 0.00140816
Iteration 9/25 | Loss: 0.00140816
Iteration 10/25 | Loss: 0.00140816
Iteration 11/25 | Loss: 0.00140816
Iteration 12/25 | Loss: 0.00140816
Iteration 13/25 | Loss: 0.00140816
Iteration 14/25 | Loss: 0.00140816
Iteration 15/25 | Loss: 0.00140816
Iteration 16/25 | Loss: 0.00140816
Iteration 17/25 | Loss: 0.00140816
Iteration 18/25 | Loss: 0.00140816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014081571716815233, 0.0014081571716815233, 0.0014081571716815233, 0.0014081571716815233, 0.0014081571716815233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014081571716815233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140816
Iteration 2/1000 | Loss: 0.00002816
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001198
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001118
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001112
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001095
Iteration 19/1000 | Loss: 0.00001092
Iteration 20/1000 | Loss: 0.00001090
Iteration 21/1000 | Loss: 0.00001081
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001060
Iteration 38/1000 | Loss: 0.00001057
Iteration 39/1000 | Loss: 0.00001057
Iteration 40/1000 | Loss: 0.00001056
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001055
Iteration 43/1000 | Loss: 0.00001053
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001052
Iteration 50/1000 | Loss: 0.00001051
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001048
Iteration 57/1000 | Loss: 0.00001048
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001046
Iteration 63/1000 | Loss: 0.00001045
Iteration 64/1000 | Loss: 0.00001045
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00001045
Iteration 67/1000 | Loss: 0.00001045
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001044
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001043
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001042
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001040
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001035
Iteration 106/1000 | Loss: 0.00001035
Iteration 107/1000 | Loss: 0.00001034
Iteration 108/1000 | Loss: 0.00001034
Iteration 109/1000 | Loss: 0.00001034
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001031
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001030
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001029
Iteration 140/1000 | Loss: 0.00001029
Iteration 141/1000 | Loss: 0.00001029
Iteration 142/1000 | Loss: 0.00001028
Iteration 143/1000 | Loss: 0.00001028
Iteration 144/1000 | Loss: 0.00001027
Iteration 145/1000 | Loss: 0.00001027
Iteration 146/1000 | Loss: 0.00001027
Iteration 147/1000 | Loss: 0.00001027
Iteration 148/1000 | Loss: 0.00001027
Iteration 149/1000 | Loss: 0.00001026
Iteration 150/1000 | Loss: 0.00001026
Iteration 151/1000 | Loss: 0.00001026
Iteration 152/1000 | Loss: 0.00001026
Iteration 153/1000 | Loss: 0.00001026
Iteration 154/1000 | Loss: 0.00001026
Iteration 155/1000 | Loss: 0.00001026
Iteration 156/1000 | Loss: 0.00001026
Iteration 157/1000 | Loss: 0.00001026
Iteration 158/1000 | Loss: 0.00001026
Iteration 159/1000 | Loss: 0.00001026
Iteration 160/1000 | Loss: 0.00001026
Iteration 161/1000 | Loss: 0.00001026
Iteration 162/1000 | Loss: 0.00001026
Iteration 163/1000 | Loss: 0.00001025
Iteration 164/1000 | Loss: 0.00001025
Iteration 165/1000 | Loss: 0.00001025
Iteration 166/1000 | Loss: 0.00001025
Iteration 167/1000 | Loss: 0.00001025
Iteration 168/1000 | Loss: 0.00001025
Iteration 169/1000 | Loss: 0.00001025
Iteration 170/1000 | Loss: 0.00001025
Iteration 171/1000 | Loss: 0.00001024
Iteration 172/1000 | Loss: 0.00001024
Iteration 173/1000 | Loss: 0.00001024
Iteration 174/1000 | Loss: 0.00001024
Iteration 175/1000 | Loss: 0.00001024
Iteration 176/1000 | Loss: 0.00001024
Iteration 177/1000 | Loss: 0.00001024
Iteration 178/1000 | Loss: 0.00001024
Iteration 179/1000 | Loss: 0.00001024
Iteration 180/1000 | Loss: 0.00001024
Iteration 181/1000 | Loss: 0.00001024
Iteration 182/1000 | Loss: 0.00001024
Iteration 183/1000 | Loss: 0.00001024
Iteration 184/1000 | Loss: 0.00001023
Iteration 185/1000 | Loss: 0.00001023
Iteration 186/1000 | Loss: 0.00001023
Iteration 187/1000 | Loss: 0.00001023
Iteration 188/1000 | Loss: 0.00001023
Iteration 189/1000 | Loss: 0.00001023
Iteration 190/1000 | Loss: 0.00001023
Iteration 191/1000 | Loss: 0.00001023
Iteration 192/1000 | Loss: 0.00001023
Iteration 193/1000 | Loss: 0.00001023
Iteration 194/1000 | Loss: 0.00001023
Iteration 195/1000 | Loss: 0.00001023
Iteration 196/1000 | Loss: 0.00001023
Iteration 197/1000 | Loss: 0.00001022
Iteration 198/1000 | Loss: 0.00001022
Iteration 199/1000 | Loss: 0.00001022
Iteration 200/1000 | Loss: 0.00001022
Iteration 201/1000 | Loss: 0.00001022
Iteration 202/1000 | Loss: 0.00001021
Iteration 203/1000 | Loss: 0.00001021
Iteration 204/1000 | Loss: 0.00001021
Iteration 205/1000 | Loss: 0.00001021
Iteration 206/1000 | Loss: 0.00001021
Iteration 207/1000 | Loss: 0.00001021
Iteration 208/1000 | Loss: 0.00001020
Iteration 209/1000 | Loss: 0.00001020
Iteration 210/1000 | Loss: 0.00001020
Iteration 211/1000 | Loss: 0.00001020
Iteration 212/1000 | Loss: 0.00001020
Iteration 213/1000 | Loss: 0.00001020
Iteration 214/1000 | Loss: 0.00001019
Iteration 215/1000 | Loss: 0.00001019
Iteration 216/1000 | Loss: 0.00001019
Iteration 217/1000 | Loss: 0.00001019
Iteration 218/1000 | Loss: 0.00001019
Iteration 219/1000 | Loss: 0.00001019
Iteration 220/1000 | Loss: 0.00001018
Iteration 221/1000 | Loss: 0.00001018
Iteration 222/1000 | Loss: 0.00001018
Iteration 223/1000 | Loss: 0.00001018
Iteration 224/1000 | Loss: 0.00001018
Iteration 225/1000 | Loss: 0.00001018
Iteration 226/1000 | Loss: 0.00001018
Iteration 227/1000 | Loss: 0.00001018
Iteration 228/1000 | Loss: 0.00001018
Iteration 229/1000 | Loss: 0.00001018
Iteration 230/1000 | Loss: 0.00001017
Iteration 231/1000 | Loss: 0.00001017
Iteration 232/1000 | Loss: 0.00001017
Iteration 233/1000 | Loss: 0.00001017
Iteration 234/1000 | Loss: 0.00001017
Iteration 235/1000 | Loss: 0.00001017
Iteration 236/1000 | Loss: 0.00001017
Iteration 237/1000 | Loss: 0.00001016
Iteration 238/1000 | Loss: 0.00001016
Iteration 239/1000 | Loss: 0.00001016
Iteration 240/1000 | Loss: 0.00001016
Iteration 241/1000 | Loss: 0.00001016
Iteration 242/1000 | Loss: 0.00001016
Iteration 243/1000 | Loss: 0.00001016
Iteration 244/1000 | Loss: 0.00001016
Iteration 245/1000 | Loss: 0.00001016
Iteration 246/1000 | Loss: 0.00001016
Iteration 247/1000 | Loss: 0.00001016
Iteration 248/1000 | Loss: 0.00001016
Iteration 249/1000 | Loss: 0.00001016
Iteration 250/1000 | Loss: 0.00001016
Iteration 251/1000 | Loss: 0.00001016
Iteration 252/1000 | Loss: 0.00001016
Iteration 253/1000 | Loss: 0.00001015
Iteration 254/1000 | Loss: 0.00001015
Iteration 255/1000 | Loss: 0.00001015
Iteration 256/1000 | Loss: 0.00001015
Iteration 257/1000 | Loss: 0.00001015
Iteration 258/1000 | Loss: 0.00001015
Iteration 259/1000 | Loss: 0.00001015
Iteration 260/1000 | Loss: 0.00001015
Iteration 261/1000 | Loss: 0.00001015
Iteration 262/1000 | Loss: 0.00001015
Iteration 263/1000 | Loss: 0.00001015
Iteration 264/1000 | Loss: 0.00001015
Iteration 265/1000 | Loss: 0.00001015
Iteration 266/1000 | Loss: 0.00001015
Iteration 267/1000 | Loss: 0.00001015
Iteration 268/1000 | Loss: 0.00001015
Iteration 269/1000 | Loss: 0.00001015
Iteration 270/1000 | Loss: 0.00001015
Iteration 271/1000 | Loss: 0.00001015
Iteration 272/1000 | Loss: 0.00001015
Iteration 273/1000 | Loss: 0.00001015
Iteration 274/1000 | Loss: 0.00001015
Iteration 275/1000 | Loss: 0.00001015
Iteration 276/1000 | Loss: 0.00001015
Iteration 277/1000 | Loss: 0.00001015
Iteration 278/1000 | Loss: 0.00001015
Iteration 279/1000 | Loss: 0.00001015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.0153769835596904e-05, 1.0153769835596904e-05, 1.0153769835596904e-05, 1.0153769835596904e-05, 1.0153769835596904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0153769835596904e-05

Optimization complete. Final v2v error: 2.7482998371124268 mm

Highest mean error: 3.193584680557251 mm for frame 79

Lowest mean error: 2.6218209266662598 mm for frame 13

Saving results

Total time: 52.65169930458069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544446
Iteration 2/25 | Loss: 0.00153500
Iteration 3/25 | Loss: 0.00137584
Iteration 4/25 | Loss: 0.00136354
Iteration 5/25 | Loss: 0.00136161
Iteration 6/25 | Loss: 0.00136161
Iteration 7/25 | Loss: 0.00136161
Iteration 8/25 | Loss: 0.00136161
Iteration 9/25 | Loss: 0.00136161
Iteration 10/25 | Loss: 0.00136161
Iteration 11/25 | Loss: 0.00136161
Iteration 12/25 | Loss: 0.00136161
Iteration 13/25 | Loss: 0.00136161
Iteration 14/25 | Loss: 0.00136161
Iteration 15/25 | Loss: 0.00136161
Iteration 16/25 | Loss: 0.00136161
Iteration 17/25 | Loss: 0.00136161
Iteration 18/25 | Loss: 0.00136161
Iteration 19/25 | Loss: 0.00136161
Iteration 20/25 | Loss: 0.00136161
Iteration 21/25 | Loss: 0.00136161
Iteration 22/25 | Loss: 0.00136161
Iteration 23/25 | Loss: 0.00136161
Iteration 24/25 | Loss: 0.00136161
Iteration 25/25 | Loss: 0.00136161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07612252
Iteration 2/25 | Loss: 0.00098797
Iteration 3/25 | Loss: 0.00098794
Iteration 4/25 | Loss: 0.00098794
Iteration 5/25 | Loss: 0.00098794
Iteration 6/25 | Loss: 0.00098794
Iteration 7/25 | Loss: 0.00098794
Iteration 8/25 | Loss: 0.00098794
Iteration 9/25 | Loss: 0.00098794
Iteration 10/25 | Loss: 0.00098794
Iteration 11/25 | Loss: 0.00098794
Iteration 12/25 | Loss: 0.00098794
Iteration 13/25 | Loss: 0.00098794
Iteration 14/25 | Loss: 0.00098794
Iteration 15/25 | Loss: 0.00098794
Iteration 16/25 | Loss: 0.00098794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000987938023172319, 0.000987938023172319, 0.000987938023172319, 0.000987938023172319, 0.000987938023172319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000987938023172319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098794
Iteration 2/1000 | Loss: 0.00003379
Iteration 3/1000 | Loss: 0.00002443
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002152
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002017
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001910
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001800
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001798
Iteration 22/1000 | Loss: 0.00001798
Iteration 23/1000 | Loss: 0.00001797
Iteration 24/1000 | Loss: 0.00001797
Iteration 25/1000 | Loss: 0.00001797
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001777
Iteration 58/1000 | Loss: 0.00001777
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00001777
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001768
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001766
Iteration 107/1000 | Loss: 0.00001766
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001765
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001764
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001764
Iteration 134/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.7641898011788726e-05, 1.7641898011788726e-05, 1.7641898011788726e-05, 1.7641898011788726e-05, 1.7641898011788726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7641898011788726e-05

Optimization complete. Final v2v error: 3.5790276527404785 mm

Highest mean error: 4.147873878479004 mm for frame 50

Lowest mean error: 3.1652605533599854 mm for frame 74

Saving results

Total time: 43.287952184677124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816428
Iteration 2/25 | Loss: 0.00152396
Iteration 3/25 | Loss: 0.00131596
Iteration 4/25 | Loss: 0.00129660
Iteration 5/25 | Loss: 0.00129257
Iteration 6/25 | Loss: 0.00129196
Iteration 7/25 | Loss: 0.00129196
Iteration 8/25 | Loss: 0.00129196
Iteration 9/25 | Loss: 0.00129196
Iteration 10/25 | Loss: 0.00129196
Iteration 11/25 | Loss: 0.00129196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012919624568894506, 0.0012919624568894506, 0.0012919624568894506, 0.0012919624568894506, 0.0012919624568894506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919624568894506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92338592
Iteration 2/25 | Loss: 0.00094319
Iteration 3/25 | Loss: 0.00094318
Iteration 4/25 | Loss: 0.00094318
Iteration 5/25 | Loss: 0.00094318
Iteration 6/25 | Loss: 0.00094318
Iteration 7/25 | Loss: 0.00094318
Iteration 8/25 | Loss: 0.00094317
Iteration 9/25 | Loss: 0.00094317
Iteration 10/25 | Loss: 0.00094317
Iteration 11/25 | Loss: 0.00094317
Iteration 12/25 | Loss: 0.00094317
Iteration 13/25 | Loss: 0.00094317
Iteration 14/25 | Loss: 0.00094317
Iteration 15/25 | Loss: 0.00094317
Iteration 16/25 | Loss: 0.00094317
Iteration 17/25 | Loss: 0.00094317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000943174643907696, 0.000943174643907696, 0.000943174643907696, 0.000943174643907696, 0.000943174643907696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000943174643907696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094317
Iteration 2/1000 | Loss: 0.00004823
Iteration 3/1000 | Loss: 0.00003627
Iteration 4/1000 | Loss: 0.00002980
Iteration 5/1000 | Loss: 0.00002752
Iteration 6/1000 | Loss: 0.00002615
Iteration 7/1000 | Loss: 0.00002492
Iteration 8/1000 | Loss: 0.00002409
Iteration 9/1000 | Loss: 0.00002351
Iteration 10/1000 | Loss: 0.00002314
Iteration 11/1000 | Loss: 0.00002275
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002199
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002171
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002164
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002161
Iteration 23/1000 | Loss: 0.00002156
Iteration 24/1000 | Loss: 0.00002153
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002151
Iteration 27/1000 | Loss: 0.00002150
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002146
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002145
Iteration 33/1000 | Loss: 0.00002145
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00002141
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00002140
Iteration 46/1000 | Loss: 0.00002140
Iteration 47/1000 | Loss: 0.00002140
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002134
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002132
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002131
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002121
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002119
Iteration 103/1000 | Loss: 0.00002119
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002116
Iteration 109/1000 | Loss: 0.00002116
Iteration 110/1000 | Loss: 0.00002116
Iteration 111/1000 | Loss: 0.00002116
Iteration 112/1000 | Loss: 0.00002115
Iteration 113/1000 | Loss: 0.00002115
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002114
Iteration 119/1000 | Loss: 0.00002114
Iteration 120/1000 | Loss: 0.00002114
Iteration 121/1000 | Loss: 0.00002114
Iteration 122/1000 | Loss: 0.00002114
Iteration 123/1000 | Loss: 0.00002114
Iteration 124/1000 | Loss: 0.00002114
Iteration 125/1000 | Loss: 0.00002114
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Iteration 133/1000 | Loss: 0.00002113
Iteration 134/1000 | Loss: 0.00002113
Iteration 135/1000 | Loss: 0.00002113
Iteration 136/1000 | Loss: 0.00002113
Iteration 137/1000 | Loss: 0.00002113
Iteration 138/1000 | Loss: 0.00002112
Iteration 139/1000 | Loss: 0.00002112
Iteration 140/1000 | Loss: 0.00002112
Iteration 141/1000 | Loss: 0.00002112
Iteration 142/1000 | Loss: 0.00002112
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002112
Iteration 145/1000 | Loss: 0.00002112
Iteration 146/1000 | Loss: 0.00002112
Iteration 147/1000 | Loss: 0.00002112
Iteration 148/1000 | Loss: 0.00002111
Iteration 149/1000 | Loss: 0.00002111
Iteration 150/1000 | Loss: 0.00002111
Iteration 151/1000 | Loss: 0.00002111
Iteration 152/1000 | Loss: 0.00002111
Iteration 153/1000 | Loss: 0.00002111
Iteration 154/1000 | Loss: 0.00002111
Iteration 155/1000 | Loss: 0.00002111
Iteration 156/1000 | Loss: 0.00002111
Iteration 157/1000 | Loss: 0.00002111
Iteration 158/1000 | Loss: 0.00002111
Iteration 159/1000 | Loss: 0.00002110
Iteration 160/1000 | Loss: 0.00002110
Iteration 161/1000 | Loss: 0.00002110
Iteration 162/1000 | Loss: 0.00002110
Iteration 163/1000 | Loss: 0.00002110
Iteration 164/1000 | Loss: 0.00002110
Iteration 165/1000 | Loss: 0.00002110
Iteration 166/1000 | Loss: 0.00002109
Iteration 167/1000 | Loss: 0.00002109
Iteration 168/1000 | Loss: 0.00002109
Iteration 169/1000 | Loss: 0.00002109
Iteration 170/1000 | Loss: 0.00002109
Iteration 171/1000 | Loss: 0.00002109
Iteration 172/1000 | Loss: 0.00002109
Iteration 173/1000 | Loss: 0.00002109
Iteration 174/1000 | Loss: 0.00002109
Iteration 175/1000 | Loss: 0.00002108
Iteration 176/1000 | Loss: 0.00002108
Iteration 177/1000 | Loss: 0.00002108
Iteration 178/1000 | Loss: 0.00002108
Iteration 179/1000 | Loss: 0.00002108
Iteration 180/1000 | Loss: 0.00002108
Iteration 181/1000 | Loss: 0.00002108
Iteration 182/1000 | Loss: 0.00002108
Iteration 183/1000 | Loss: 0.00002108
Iteration 184/1000 | Loss: 0.00002108
Iteration 185/1000 | Loss: 0.00002108
Iteration 186/1000 | Loss: 0.00002108
Iteration 187/1000 | Loss: 0.00002108
Iteration 188/1000 | Loss: 0.00002108
Iteration 189/1000 | Loss: 0.00002108
Iteration 190/1000 | Loss: 0.00002108
Iteration 191/1000 | Loss: 0.00002108
Iteration 192/1000 | Loss: 0.00002108
Iteration 193/1000 | Loss: 0.00002108
Iteration 194/1000 | Loss: 0.00002108
Iteration 195/1000 | Loss: 0.00002108
Iteration 196/1000 | Loss: 0.00002108
Iteration 197/1000 | Loss: 0.00002108
Iteration 198/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.1080650185467675e-05, 2.1080650185467675e-05, 2.1080650185467675e-05, 2.1080650185467675e-05, 2.1080650185467675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1080650185467675e-05

Optimization complete. Final v2v error: 4.011620998382568 mm

Highest mean error: 4.224821090698242 mm for frame 130

Lowest mean error: 3.781944751739502 mm for frame 108

Saving results

Total time: 46.676722288131714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551901
Iteration 2/25 | Loss: 0.00147214
Iteration 3/25 | Loss: 0.00135747
Iteration 4/25 | Loss: 0.00134848
Iteration 5/25 | Loss: 0.00134843
Iteration 6/25 | Loss: 0.00134843
Iteration 7/25 | Loss: 0.00134843
Iteration 8/25 | Loss: 0.00134843
Iteration 9/25 | Loss: 0.00134843
Iteration 10/25 | Loss: 0.00134843
Iteration 11/25 | Loss: 0.00134843
Iteration 12/25 | Loss: 0.00134843
Iteration 13/25 | Loss: 0.00134843
Iteration 14/25 | Loss: 0.00134843
Iteration 15/25 | Loss: 0.00134843
Iteration 16/25 | Loss: 0.00134843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013484334340319037, 0.0013484334340319037, 0.0013484334340319037, 0.0013484334340319037, 0.0013484334340319037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013484334340319037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26191318
Iteration 2/25 | Loss: 0.00121988
Iteration 3/25 | Loss: 0.00121983
Iteration 4/25 | Loss: 0.00121983
Iteration 5/25 | Loss: 0.00121982
Iteration 6/25 | Loss: 0.00121982
Iteration 7/25 | Loss: 0.00121982
Iteration 8/25 | Loss: 0.00121982
Iteration 9/25 | Loss: 0.00121982
Iteration 10/25 | Loss: 0.00121982
Iteration 11/25 | Loss: 0.00121982
Iteration 12/25 | Loss: 0.00121982
Iteration 13/25 | Loss: 0.00121982
Iteration 14/25 | Loss: 0.00121982
Iteration 15/25 | Loss: 0.00121982
Iteration 16/25 | Loss: 0.00121982
Iteration 17/25 | Loss: 0.00121982
Iteration 18/25 | Loss: 0.00121982
Iteration 19/25 | Loss: 0.00121982
Iteration 20/25 | Loss: 0.00121982
Iteration 21/25 | Loss: 0.00121982
Iteration 22/25 | Loss: 0.00121982
Iteration 23/25 | Loss: 0.00121982
Iteration 24/25 | Loss: 0.00121982
Iteration 25/25 | Loss: 0.00121982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121982
Iteration 2/1000 | Loss: 0.00004297
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001838
Iteration 8/1000 | Loss: 0.00001804
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001680
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001586
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001562
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001548
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001542
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001536
Iteration 63/1000 | Loss: 0.00001536
Iteration 64/1000 | Loss: 0.00001536
Iteration 65/1000 | Loss: 0.00001536
Iteration 66/1000 | Loss: 0.00001536
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001534
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5247103874571621e-05, 1.5247103874571621e-05, 1.5247103874571621e-05, 1.5247103874571621e-05, 1.5247103874571621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5247103874571621e-05

Optimization complete. Final v2v error: 3.383363962173462 mm

Highest mean error: 3.5581650733947754 mm for frame 157

Lowest mean error: 3.1996874809265137 mm for frame 183

Saving results

Total time: 43.35486578941345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060217
Iteration 2/25 | Loss: 0.01060217
Iteration 3/25 | Loss: 0.01060217
Iteration 4/25 | Loss: 0.00474248
Iteration 5/25 | Loss: 0.00320190
Iteration 6/25 | Loss: 0.00280553
Iteration 7/25 | Loss: 0.00260529
Iteration 8/25 | Loss: 0.00250025
Iteration 9/25 | Loss: 0.00237777
Iteration 10/25 | Loss: 0.00232327
Iteration 11/25 | Loss: 0.00229556
Iteration 12/25 | Loss: 0.00229304
Iteration 13/25 | Loss: 0.00227051
Iteration 14/25 | Loss: 0.00226967
Iteration 15/25 | Loss: 0.00225291
Iteration 16/25 | Loss: 0.00222568
Iteration 17/25 | Loss: 0.00221317
Iteration 18/25 | Loss: 0.00222233
Iteration 19/25 | Loss: 0.00218965
Iteration 20/25 | Loss: 0.00217022
Iteration 21/25 | Loss: 0.00215650
Iteration 22/25 | Loss: 0.00214509
Iteration 23/25 | Loss: 0.00215088
Iteration 24/25 | Loss: 0.00214983
Iteration 25/25 | Loss: 0.00213672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69050455
Iteration 2/25 | Loss: 0.00956671
Iteration 3/25 | Loss: 0.00919213
Iteration 4/25 | Loss: 0.00919213
Iteration 5/25 | Loss: 0.00919213
Iteration 6/25 | Loss: 0.00919213
Iteration 7/25 | Loss: 0.00919213
Iteration 8/25 | Loss: 0.00919213
Iteration 9/25 | Loss: 0.00919213
Iteration 10/25 | Loss: 0.00919213
Iteration 11/25 | Loss: 0.00919213
Iteration 12/25 | Loss: 0.00919213
Iteration 13/25 | Loss: 0.00919213
Iteration 14/25 | Loss: 0.00919213
Iteration 15/25 | Loss: 0.00919213
Iteration 16/25 | Loss: 0.00919213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.009192130528390408, 0.009192130528390408, 0.009192130528390408, 0.009192130528390408, 0.009192130528390408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009192130528390408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00919213
Iteration 2/1000 | Loss: 0.00602640
Iteration 3/1000 | Loss: 0.00167717
Iteration 4/1000 | Loss: 0.00085474
Iteration 5/1000 | Loss: 0.00062103
Iteration 6/1000 | Loss: 0.00110305
Iteration 7/1000 | Loss: 0.00056410
Iteration 8/1000 | Loss: 0.00050540
Iteration 9/1000 | Loss: 0.00030455
Iteration 10/1000 | Loss: 0.00056006
Iteration 11/1000 | Loss: 0.00066770
Iteration 12/1000 | Loss: 0.00055817
Iteration 13/1000 | Loss: 0.00102115
Iteration 14/1000 | Loss: 0.00061146
Iteration 15/1000 | Loss: 0.00076966
Iteration 16/1000 | Loss: 0.00034274
Iteration 17/1000 | Loss: 0.00044127
Iteration 18/1000 | Loss: 0.00030480
Iteration 19/1000 | Loss: 0.00325748
Iteration 20/1000 | Loss: 0.00650576
Iteration 21/1000 | Loss: 0.00598799
Iteration 22/1000 | Loss: 0.00313210
Iteration 23/1000 | Loss: 0.00125081
Iteration 24/1000 | Loss: 0.00277884
Iteration 25/1000 | Loss: 0.00130685
Iteration 26/1000 | Loss: 0.00133301
Iteration 27/1000 | Loss: 0.00068132
Iteration 28/1000 | Loss: 0.00096306
Iteration 29/1000 | Loss: 0.00077695
Iteration 30/1000 | Loss: 0.00024720
Iteration 31/1000 | Loss: 0.00442680
Iteration 32/1000 | Loss: 0.00141664
Iteration 33/1000 | Loss: 0.00261102
Iteration 34/1000 | Loss: 0.00147151
Iteration 35/1000 | Loss: 0.00223955
Iteration 36/1000 | Loss: 0.00079236
Iteration 37/1000 | Loss: 0.00034586
Iteration 38/1000 | Loss: 0.00033311
Iteration 39/1000 | Loss: 0.00053261
Iteration 40/1000 | Loss: 0.00141008
Iteration 41/1000 | Loss: 0.00063813
Iteration 42/1000 | Loss: 0.00100728
Iteration 43/1000 | Loss: 0.00097664
Iteration 44/1000 | Loss: 0.00079368
Iteration 45/1000 | Loss: 0.00108732
Iteration 46/1000 | Loss: 0.00119406
Iteration 47/1000 | Loss: 0.00113830
Iteration 48/1000 | Loss: 0.00181399
Iteration 49/1000 | Loss: 0.00151487
Iteration 50/1000 | Loss: 0.00116564
Iteration 51/1000 | Loss: 0.00045008
Iteration 52/1000 | Loss: 0.00086409
Iteration 53/1000 | Loss: 0.00029920
Iteration 54/1000 | Loss: 0.00145785
Iteration 55/1000 | Loss: 0.00076487
Iteration 56/1000 | Loss: 0.00057609
Iteration 57/1000 | Loss: 0.00037491
Iteration 58/1000 | Loss: 0.00103146
Iteration 59/1000 | Loss: 0.00078214
Iteration 60/1000 | Loss: 0.00057941
Iteration 61/1000 | Loss: 0.00023367
Iteration 62/1000 | Loss: 0.00039101
Iteration 63/1000 | Loss: 0.00112781
Iteration 64/1000 | Loss: 0.00037315
Iteration 65/1000 | Loss: 0.00096489
Iteration 66/1000 | Loss: 0.00035756
Iteration 67/1000 | Loss: 0.00027856
Iteration 68/1000 | Loss: 0.00115249
Iteration 69/1000 | Loss: 0.00053205
Iteration 70/1000 | Loss: 0.00033045
Iteration 71/1000 | Loss: 0.00061003
Iteration 72/1000 | Loss: 0.00058961
Iteration 73/1000 | Loss: 0.00045199
Iteration 74/1000 | Loss: 0.00063763
Iteration 75/1000 | Loss: 0.00031846
Iteration 76/1000 | Loss: 0.00044484
Iteration 77/1000 | Loss: 0.00071029
Iteration 78/1000 | Loss: 0.00040142
Iteration 79/1000 | Loss: 0.00094947
Iteration 80/1000 | Loss: 0.00050244
Iteration 81/1000 | Loss: 0.00045703
Iteration 82/1000 | Loss: 0.00057867
Iteration 83/1000 | Loss: 0.00027797
Iteration 84/1000 | Loss: 0.00016079
Iteration 85/1000 | Loss: 0.00035372
Iteration 86/1000 | Loss: 0.00045591
Iteration 87/1000 | Loss: 0.00022186
Iteration 88/1000 | Loss: 0.00056582
Iteration 89/1000 | Loss: 0.00067536
Iteration 90/1000 | Loss: 0.00058292
Iteration 91/1000 | Loss: 0.00021899
Iteration 92/1000 | Loss: 0.00012688
Iteration 93/1000 | Loss: 0.00035342
Iteration 94/1000 | Loss: 0.00085935
Iteration 95/1000 | Loss: 0.00024040
Iteration 96/1000 | Loss: 0.00023192
Iteration 97/1000 | Loss: 0.00026683
Iteration 98/1000 | Loss: 0.00019573
Iteration 99/1000 | Loss: 0.00098270
Iteration 100/1000 | Loss: 0.00019553
Iteration 101/1000 | Loss: 0.00014193
Iteration 102/1000 | Loss: 0.00011388
Iteration 103/1000 | Loss: 0.00025829
Iteration 104/1000 | Loss: 0.00019324
Iteration 105/1000 | Loss: 0.00013116
Iteration 106/1000 | Loss: 0.00011733
Iteration 107/1000 | Loss: 0.00011663
Iteration 108/1000 | Loss: 0.00013932
Iteration 109/1000 | Loss: 0.00011007
Iteration 110/1000 | Loss: 0.00034190
Iteration 111/1000 | Loss: 0.00022422
Iteration 112/1000 | Loss: 0.00010290
Iteration 113/1000 | Loss: 0.00109915
Iteration 114/1000 | Loss: 0.00102942
Iteration 115/1000 | Loss: 0.00088135
Iteration 116/1000 | Loss: 0.00063029
Iteration 117/1000 | Loss: 0.00009198
Iteration 118/1000 | Loss: 0.00008107
Iteration 119/1000 | Loss: 0.00040623
Iteration 120/1000 | Loss: 0.00008373
Iteration 121/1000 | Loss: 0.00008473
Iteration 122/1000 | Loss: 0.00007916
Iteration 123/1000 | Loss: 0.00006954
Iteration 124/1000 | Loss: 0.00027576
Iteration 125/1000 | Loss: 0.00050943
Iteration 126/1000 | Loss: 0.00040945
Iteration 127/1000 | Loss: 0.00010419
Iteration 128/1000 | Loss: 0.00007786
Iteration 129/1000 | Loss: 0.00007215
Iteration 130/1000 | Loss: 0.00006671
Iteration 131/1000 | Loss: 0.00069606
Iteration 132/1000 | Loss: 0.00016274
Iteration 133/1000 | Loss: 0.00008857
Iteration 134/1000 | Loss: 0.00013765
Iteration 135/1000 | Loss: 0.00006764
Iteration 136/1000 | Loss: 0.00006345
Iteration 137/1000 | Loss: 0.00006125
Iteration 138/1000 | Loss: 0.00007074
Iteration 139/1000 | Loss: 0.00029518
Iteration 140/1000 | Loss: 0.00025615
Iteration 141/1000 | Loss: 0.00007177
Iteration 142/1000 | Loss: 0.00006059
Iteration 143/1000 | Loss: 0.00006012
Iteration 144/1000 | Loss: 0.00007307
Iteration 145/1000 | Loss: 0.00007360
Iteration 146/1000 | Loss: 0.00006310
Iteration 147/1000 | Loss: 0.00006283
Iteration 148/1000 | Loss: 0.00007371
Iteration 149/1000 | Loss: 0.00007408
Iteration 150/1000 | Loss: 0.00006273
Iteration 151/1000 | Loss: 0.00007296
Iteration 152/1000 | Loss: 0.00006373
Iteration 153/1000 | Loss: 0.00006647
Iteration 154/1000 | Loss: 0.00007340
Iteration 155/1000 | Loss: 0.00006760
Iteration 156/1000 | Loss: 0.00006936
Iteration 157/1000 | Loss: 0.00022715
Iteration 158/1000 | Loss: 0.00006749
Iteration 159/1000 | Loss: 0.00007077
Iteration 160/1000 | Loss: 0.00007950
Iteration 161/1000 | Loss: 0.00007562
Iteration 162/1000 | Loss: 0.00007137
Iteration 163/1000 | Loss: 0.00006417
Iteration 164/1000 | Loss: 0.00006453
Iteration 165/1000 | Loss: 0.00006311
Iteration 166/1000 | Loss: 0.00030534
Iteration 167/1000 | Loss: 0.00029074
Iteration 168/1000 | Loss: 0.00006653
Iteration 169/1000 | Loss: 0.00006304
Iteration 170/1000 | Loss: 0.00006342
Iteration 171/1000 | Loss: 0.00029133
Iteration 172/1000 | Loss: 0.00047320
Iteration 173/1000 | Loss: 0.00011754
Iteration 174/1000 | Loss: 0.00018407
Iteration 175/1000 | Loss: 0.00020660
Iteration 176/1000 | Loss: 0.00009068
Iteration 177/1000 | Loss: 0.00007011
Iteration 178/1000 | Loss: 0.00006106
Iteration 179/1000 | Loss: 0.00007270
Iteration 180/1000 | Loss: 0.00007062
Iteration 181/1000 | Loss: 0.00008904
Iteration 182/1000 | Loss: 0.00033788
Iteration 183/1000 | Loss: 0.00027947
Iteration 184/1000 | Loss: 0.00029097
Iteration 185/1000 | Loss: 0.00039424
Iteration 186/1000 | Loss: 0.00033667
Iteration 187/1000 | Loss: 0.00023090
Iteration 188/1000 | Loss: 0.00008732
Iteration 189/1000 | Loss: 0.00016431
Iteration 190/1000 | Loss: 0.00016275
Iteration 191/1000 | Loss: 0.00033720
Iteration 192/1000 | Loss: 0.00014817
Iteration 193/1000 | Loss: 0.00007751
Iteration 194/1000 | Loss: 0.00017739
Iteration 195/1000 | Loss: 0.00007011
Iteration 196/1000 | Loss: 0.00009682
Iteration 197/1000 | Loss: 0.00007968
Iteration 198/1000 | Loss: 0.00006422
Iteration 199/1000 | Loss: 0.00023581
Iteration 200/1000 | Loss: 0.00024177
Iteration 201/1000 | Loss: 0.00022293
Iteration 202/1000 | Loss: 0.00024026
Iteration 203/1000 | Loss: 0.00016082
Iteration 204/1000 | Loss: 0.00033420
Iteration 205/1000 | Loss: 0.00031876
Iteration 206/1000 | Loss: 0.00012885
Iteration 207/1000 | Loss: 0.00015533
Iteration 208/1000 | Loss: 0.00013111
Iteration 209/1000 | Loss: 0.00025925
Iteration 210/1000 | Loss: 0.00007235
Iteration 211/1000 | Loss: 0.00015658
Iteration 212/1000 | Loss: 0.00006970
Iteration 213/1000 | Loss: 0.00006081
Iteration 214/1000 | Loss: 0.00006409
Iteration 215/1000 | Loss: 0.00007088
Iteration 216/1000 | Loss: 0.00007104
Iteration 217/1000 | Loss: 0.00029749
Iteration 218/1000 | Loss: 0.00027820
Iteration 219/1000 | Loss: 0.00029085
Iteration 220/1000 | Loss: 0.00023820
Iteration 221/1000 | Loss: 0.00007113
Iteration 222/1000 | Loss: 0.00007210
Iteration 223/1000 | Loss: 0.00007214
Iteration 224/1000 | Loss: 0.00007293
Iteration 225/1000 | Loss: 0.00024200
Iteration 226/1000 | Loss: 0.00006744
Iteration 227/1000 | Loss: 0.00006172
Iteration 228/1000 | Loss: 0.00028317
Iteration 229/1000 | Loss: 0.00007217
Iteration 230/1000 | Loss: 0.00006528
Iteration 231/1000 | Loss: 0.00006167
Iteration 232/1000 | Loss: 0.00008677
Iteration 233/1000 | Loss: 0.00028223
Iteration 234/1000 | Loss: 0.00017183
Iteration 235/1000 | Loss: 0.00006630
Iteration 236/1000 | Loss: 0.00005879
Iteration 237/1000 | Loss: 0.00005761
Iteration 238/1000 | Loss: 0.00005681
Iteration 239/1000 | Loss: 0.00005628
Iteration 240/1000 | Loss: 0.00005591
Iteration 241/1000 | Loss: 0.00041941
Iteration 242/1000 | Loss: 0.00008196
Iteration 243/1000 | Loss: 0.00006693
Iteration 244/1000 | Loss: 0.00005746
Iteration 245/1000 | Loss: 0.00005518
Iteration 246/1000 | Loss: 0.00005406
Iteration 247/1000 | Loss: 0.00005357
Iteration 248/1000 | Loss: 0.00005942
Iteration 249/1000 | Loss: 0.00005616
Iteration 250/1000 | Loss: 0.00005307
Iteration 251/1000 | Loss: 0.00005284
Iteration 252/1000 | Loss: 0.00007069
Iteration 253/1000 | Loss: 0.00006373
Iteration 254/1000 | Loss: 0.00007308
Iteration 255/1000 | Loss: 0.00005394
Iteration 256/1000 | Loss: 0.00006954
Iteration 257/1000 | Loss: 0.00007194
Iteration 258/1000 | Loss: 0.00013628
Iteration 259/1000 | Loss: 0.00005716
Iteration 260/1000 | Loss: 0.00005410
Iteration 261/1000 | Loss: 0.00005985
Iteration 262/1000 | Loss: 0.00005208
Iteration 263/1000 | Loss: 0.00005153
Iteration 264/1000 | Loss: 0.00005685
Iteration 265/1000 | Loss: 0.00006011
Iteration 266/1000 | Loss: 0.00005818
Iteration 267/1000 | Loss: 0.00005571
Iteration 268/1000 | Loss: 0.00006057
Iteration 269/1000 | Loss: 0.00006267
Iteration 270/1000 | Loss: 0.00005408
Iteration 271/1000 | Loss: 0.00005252
Iteration 272/1000 | Loss: 0.00006056
Iteration 273/1000 | Loss: 0.00005556
Iteration 274/1000 | Loss: 0.00005469
Iteration 275/1000 | Loss: 0.00006022
Iteration 276/1000 | Loss: 0.00005359
Iteration 277/1000 | Loss: 0.00005267
Iteration 278/1000 | Loss: 0.00006117
Iteration 279/1000 | Loss: 0.00006320
Iteration 280/1000 | Loss: 0.00006080
Iteration 281/1000 | Loss: 0.00006175
Iteration 282/1000 | Loss: 0.00005604
Iteration 283/1000 | Loss: 0.00005309
Iteration 284/1000 | Loss: 0.00006017
Iteration 285/1000 | Loss: 0.00006387
Iteration 286/1000 | Loss: 0.00006003
Iteration 287/1000 | Loss: 0.00006159
Iteration 288/1000 | Loss: 0.00005995
Iteration 289/1000 | Loss: 0.00023744
Iteration 290/1000 | Loss: 0.00005695
Iteration 291/1000 | Loss: 0.00005996
Iteration 292/1000 | Loss: 0.00005984
Iteration 293/1000 | Loss: 0.00006833
Iteration 294/1000 | Loss: 0.00031096
Iteration 295/1000 | Loss: 0.00005600
Iteration 296/1000 | Loss: 0.00005856
Iteration 297/1000 | Loss: 0.00005775
Iteration 298/1000 | Loss: 0.00006004
Iteration 299/1000 | Loss: 0.00005677
Iteration 300/1000 | Loss: 0.00009517
Iteration 301/1000 | Loss: 0.00005675
Iteration 302/1000 | Loss: 0.00005298
Iteration 303/1000 | Loss: 0.00006296
Iteration 304/1000 | Loss: 0.00005698
Iteration 305/1000 | Loss: 0.00005860
Iteration 306/1000 | Loss: 0.00006108
Iteration 307/1000 | Loss: 0.00005501
Iteration 308/1000 | Loss: 0.00005361
Iteration 309/1000 | Loss: 0.00005780
Iteration 310/1000 | Loss: 0.00005873
Iteration 311/1000 | Loss: 0.00006478
Iteration 312/1000 | Loss: 0.00006679
Iteration 313/1000 | Loss: 0.00005939
Iteration 314/1000 | Loss: 0.00005938
Iteration 315/1000 | Loss: 0.00006323
Iteration 316/1000 | Loss: 0.00005864
Iteration 317/1000 | Loss: 0.00025889
Iteration 318/1000 | Loss: 0.00005458
Iteration 319/1000 | Loss: 0.00005845
Iteration 320/1000 | Loss: 0.00005847
Iteration 321/1000 | Loss: 0.00005766
Iteration 322/1000 | Loss: 0.00005914
Iteration 323/1000 | Loss: 0.00005651
Iteration 324/1000 | Loss: 0.00019733
Iteration 325/1000 | Loss: 0.00008697
Iteration 326/1000 | Loss: 0.00013281
Iteration 327/1000 | Loss: 0.00008780
Iteration 328/1000 | Loss: 0.00007058
Iteration 329/1000 | Loss: 0.00008450
Iteration 330/1000 | Loss: 0.00006418
Iteration 331/1000 | Loss: 0.00006094
Iteration 332/1000 | Loss: 0.00005964
Iteration 333/1000 | Loss: 0.00005696
Iteration 334/1000 | Loss: 0.00009276
Iteration 335/1000 | Loss: 0.00006657
Iteration 336/1000 | Loss: 0.00006793
Iteration 337/1000 | Loss: 0.00005504
Iteration 338/1000 | Loss: 0.00005310
Iteration 339/1000 | Loss: 0.00011527
Iteration 340/1000 | Loss: 0.00005679
Iteration 341/1000 | Loss: 0.00006548
Iteration 342/1000 | Loss: 0.00006548
Iteration 343/1000 | Loss: 0.00007907
Iteration 344/1000 | Loss: 0.00008015
Iteration 345/1000 | Loss: 0.00025237
Iteration 346/1000 | Loss: 0.00007047
Iteration 347/1000 | Loss: 0.00008608
Iteration 348/1000 | Loss: 0.00005673
Iteration 349/1000 | Loss: 0.00009722
Iteration 350/1000 | Loss: 0.00005594
Iteration 351/1000 | Loss: 0.00005974
Iteration 352/1000 | Loss: 0.00005601
Iteration 353/1000 | Loss: 0.00005352
Iteration 354/1000 | Loss: 0.00005641
Iteration 355/1000 | Loss: 0.00005579
Iteration 356/1000 | Loss: 0.00006039
Iteration 357/1000 | Loss: 0.00005596
Iteration 358/1000 | Loss: 0.00006260
Iteration 359/1000 | Loss: 0.00005381
Iteration 360/1000 | Loss: 0.00006202
Iteration 361/1000 | Loss: 0.00006737
Iteration 362/1000 | Loss: 0.00006175
Iteration 363/1000 | Loss: 0.00005862
Iteration 364/1000 | Loss: 0.00006070
Iteration 365/1000 | Loss: 0.00005721
Iteration 366/1000 | Loss: 0.00005970
Iteration 367/1000 | Loss: 0.00007920
Iteration 368/1000 | Loss: 0.00006690
Iteration 369/1000 | Loss: 0.00006545
Iteration 370/1000 | Loss: 0.00006569
Iteration 371/1000 | Loss: 0.00006542
Iteration 372/1000 | Loss: 0.00006597
Iteration 373/1000 | Loss: 0.00006631
Iteration 374/1000 | Loss: 0.00006497
Iteration 375/1000 | Loss: 0.00006609
Iteration 376/1000 | Loss: 0.00006660
Iteration 377/1000 | Loss: 0.00006571
Iteration 378/1000 | Loss: 0.00006623
Iteration 379/1000 | Loss: 0.00006526
Iteration 380/1000 | Loss: 0.00006653
Iteration 381/1000 | Loss: 0.00006492
Iteration 382/1000 | Loss: 0.00006656
Iteration 383/1000 | Loss: 0.00006502
Iteration 384/1000 | Loss: 0.00006585
Iteration 385/1000 | Loss: 0.00006547
Iteration 386/1000 | Loss: 0.00006577
Iteration 387/1000 | Loss: 0.00006525
Iteration 388/1000 | Loss: 0.00006668
Iteration 389/1000 | Loss: 0.00006341
Iteration 390/1000 | Loss: 0.00006393
Iteration 391/1000 | Loss: 0.00006329
Iteration 392/1000 | Loss: 0.00006270
Iteration 393/1000 | Loss: 0.00005734
Iteration 394/1000 | Loss: 0.00006177
Iteration 395/1000 | Loss: 0.00006102
Iteration 396/1000 | Loss: 0.00005341
Iteration 397/1000 | Loss: 0.00005206
Iteration 398/1000 | Loss: 0.00005695
Iteration 399/1000 | Loss: 0.00006577
Iteration 400/1000 | Loss: 0.00006144
Iteration 401/1000 | Loss: 0.00006499
Iteration 402/1000 | Loss: 0.00006059
Iteration 403/1000 | Loss: 0.00006416
Iteration 404/1000 | Loss: 0.00005992
Iteration 405/1000 | Loss: 0.00005774
Iteration 406/1000 | Loss: 0.00005861
Iteration 407/1000 | Loss: 0.00006443
Iteration 408/1000 | Loss: 0.00006594
Iteration 409/1000 | Loss: 0.00006495
Iteration 410/1000 | Loss: 0.00006473
Iteration 411/1000 | Loss: 0.00006417
Iteration 412/1000 | Loss: 0.00006574
Iteration 413/1000 | Loss: 0.00020200
Iteration 414/1000 | Loss: 0.00012446
Iteration 415/1000 | Loss: 0.00005791
Iteration 416/1000 | Loss: 0.00005944
Iteration 417/1000 | Loss: 0.00005603
Iteration 418/1000 | Loss: 0.00005848
Iteration 419/1000 | Loss: 0.00005687
Iteration 420/1000 | Loss: 0.00005873
Iteration 421/1000 | Loss: 0.00005623
Iteration 422/1000 | Loss: 0.00006977
Iteration 423/1000 | Loss: 0.00006669
Iteration 424/1000 | Loss: 0.00006857
Iteration 425/1000 | Loss: 0.00005854
Iteration 426/1000 | Loss: 0.00008912
Iteration 427/1000 | Loss: 0.00006385
Iteration 428/1000 | Loss: 0.00006440
Iteration 429/1000 | Loss: 0.00006267
Iteration 430/1000 | Loss: 0.00008219
Iteration 431/1000 | Loss: 0.00006983
Iteration 432/1000 | Loss: 0.00006678
Iteration 433/1000 | Loss: 0.00005687
Iteration 434/1000 | Loss: 0.00005985
Iteration 435/1000 | Loss: 0.00006887
Iteration 436/1000 | Loss: 0.00008007
Iteration 437/1000 | Loss: 0.00006463
Iteration 438/1000 | Loss: 0.00006610
Iteration 439/1000 | Loss: 0.00007911
Iteration 440/1000 | Loss: 0.00006672
Iteration 441/1000 | Loss: 0.00006504
Iteration 442/1000 | Loss: 0.00006620
Iteration 443/1000 | Loss: 0.00006408
Iteration 444/1000 | Loss: 0.00006585
Iteration 445/1000 | Loss: 0.00006468
Iteration 446/1000 | Loss: 0.00006621
Iteration 447/1000 | Loss: 0.00006525
Iteration 448/1000 | Loss: 0.00018986
Iteration 449/1000 | Loss: 0.00006457
Iteration 450/1000 | Loss: 0.00006751
Iteration 451/1000 | Loss: 0.00006457
Iteration 452/1000 | Loss: 0.00006656
Iteration 453/1000 | Loss: 0.00006453
Iteration 454/1000 | Loss: 0.00016895
Iteration 455/1000 | Loss: 0.00006504
Iteration 456/1000 | Loss: 0.00006627
Iteration 457/1000 | Loss: 0.00006558
Iteration 458/1000 | Loss: 0.00014504
Iteration 459/1000 | Loss: 0.00006521
Iteration 460/1000 | Loss: 0.00007830
Iteration 461/1000 | Loss: 0.00006355
Iteration 462/1000 | Loss: 0.00009302
Iteration 463/1000 | Loss: 0.00011132
Iteration 464/1000 | Loss: 0.00008701
Iteration 465/1000 | Loss: 0.00006620
Iteration 466/1000 | Loss: 0.00008162
Iteration 467/1000 | Loss: 0.00005947
Iteration 468/1000 | Loss: 0.00006043
Iteration 469/1000 | Loss: 0.00006906
Iteration 470/1000 | Loss: 0.00005866
Iteration 471/1000 | Loss: 0.00006864
Iteration 472/1000 | Loss: 0.00006005
Iteration 473/1000 | Loss: 0.00006379
Iteration 474/1000 | Loss: 0.00006562
Iteration 475/1000 | Loss: 0.00006428
Iteration 476/1000 | Loss: 0.00006610
Iteration 477/1000 | Loss: 0.00005849
Iteration 478/1000 | Loss: 0.00006668
Iteration 479/1000 | Loss: 0.00006428
Iteration 480/1000 | Loss: 0.00005019
Iteration 481/1000 | Loss: 0.00006260
Iteration 482/1000 | Loss: 0.00006237
Iteration 483/1000 | Loss: 0.00005722
Iteration 484/1000 | Loss: 0.00005333
Iteration 485/1000 | Loss: 0.00005333
Iteration 486/1000 | Loss: 0.00005333
Iteration 487/1000 | Loss: 0.00006049
Iteration 488/1000 | Loss: 0.00006423
Iteration 489/1000 | Loss: 0.00006635
Iteration 490/1000 | Loss: 0.00006502
Iteration 491/1000 | Loss: 0.00006625
Iteration 492/1000 | Loss: 0.00006667
Iteration 493/1000 | Loss: 0.00006631
Iteration 494/1000 | Loss: 0.00006453
Iteration 495/1000 | Loss: 0.00006349
Iteration 496/1000 | Loss: 0.00005870
Iteration 497/1000 | Loss: 0.00006633
Iteration 498/1000 | Loss: 0.00006688
Iteration 499/1000 | Loss: 0.00006558
Iteration 500/1000 | Loss: 0.00006486
Iteration 501/1000 | Loss: 0.00005838
Iteration 502/1000 | Loss: 0.00006854
Iteration 503/1000 | Loss: 0.00006485
Iteration 504/1000 | Loss: 0.00006222
Iteration 505/1000 | Loss: 0.00005839
Iteration 506/1000 | Loss: 0.00006643
Iteration 507/1000 | Loss: 0.00005439
Iteration 508/1000 | Loss: 0.00005775
Iteration 509/1000 | Loss: 0.00006571
Iteration 510/1000 | Loss: 0.00006141
Iteration 511/1000 | Loss: 0.00005952
Iteration 512/1000 | Loss: 0.00006955
Iteration 513/1000 | Loss: 0.00006517
Iteration 514/1000 | Loss: 0.00006313
Iteration 515/1000 | Loss: 0.00005756
Iteration 516/1000 | Loss: 0.00006169
Iteration 517/1000 | Loss: 0.00006632
Iteration 518/1000 | Loss: 0.00006273
Iteration 519/1000 | Loss: 0.00006251
Iteration 520/1000 | Loss: 0.00006721
Iteration 521/1000 | Loss: 0.00005852
Iteration 522/1000 | Loss: 0.00005948
Iteration 523/1000 | Loss: 0.00005850
Iteration 524/1000 | Loss: 0.00005929
Iteration 525/1000 | Loss: 0.00005929
Iteration 526/1000 | Loss: 0.00005974
Iteration 527/1000 | Loss: 0.00005832
Iteration 528/1000 | Loss: 0.00014558
Iteration 529/1000 | Loss: 0.00005798
Iteration 530/1000 | Loss: 0.00006599
Iteration 531/1000 | Loss: 0.00005799
Iteration 532/1000 | Loss: 0.00005971
Iteration 533/1000 | Loss: 0.00005935
Iteration 534/1000 | Loss: 0.00026001
Iteration 535/1000 | Loss: 0.00005759
Iteration 536/1000 | Loss: 0.00008152
Iteration 537/1000 | Loss: 0.00006059
Iteration 538/1000 | Loss: 0.00007377
Iteration 539/1000 | Loss: 0.00005850
Iteration 540/1000 | Loss: 0.00006276
Iteration 541/1000 | Loss: 0.00005888
Iteration 542/1000 | Loss: 0.00006120
Iteration 543/1000 | Loss: 0.00005929
Iteration 544/1000 | Loss: 0.00006537
Iteration 545/1000 | Loss: 0.00006450
Iteration 546/1000 | Loss: 0.00005816
Iteration 547/1000 | Loss: 0.00009443
Iteration 548/1000 | Loss: 0.00007639
Iteration 549/1000 | Loss: 0.00006297
Iteration 550/1000 | Loss: 0.00005663
Iteration 551/1000 | Loss: 0.00005729
Iteration 552/1000 | Loss: 0.00005961
Iteration 553/1000 | Loss: 0.00006292
Iteration 554/1000 | Loss: 0.00005915
Iteration 555/1000 | Loss: 0.00006272
Iteration 556/1000 | Loss: 0.00005945
Iteration 557/1000 | Loss: 0.00005659
Iteration 558/1000 | Loss: 0.00005754
Iteration 559/1000 | Loss: 0.00005129
Iteration 560/1000 | Loss: 0.00007475
Iteration 561/1000 | Loss: 0.00005856
Iteration 562/1000 | Loss: 0.00006294
Iteration 563/1000 | Loss: 0.00006339
Iteration 564/1000 | Loss: 0.00006766
Iteration 565/1000 | Loss: 0.00006361
Iteration 566/1000 | Loss: 0.00006097
Iteration 567/1000 | Loss: 0.00006335
Iteration 568/1000 | Loss: 0.00006061
Iteration 569/1000 | Loss: 0.00006289
Iteration 570/1000 | Loss: 0.00006035
Iteration 571/1000 | Loss: 0.00006438
Iteration 572/1000 | Loss: 0.00006020
Iteration 573/1000 | Loss: 0.00005932
Iteration 574/1000 | Loss: 0.00006726
Iteration 575/1000 | Loss: 0.00005849
Iteration 576/1000 | Loss: 0.00005186
Iteration 577/1000 | Loss: 0.00006417
Iteration 578/1000 | Loss: 0.00006140
Iteration 579/1000 | Loss: 0.00006051
Iteration 580/1000 | Loss: 0.00006198
Iteration 581/1000 | Loss: 0.00038508
Iteration 582/1000 | Loss: 0.00028204
Iteration 583/1000 | Loss: 0.00007082
Iteration 584/1000 | Loss: 0.00005857
Iteration 585/1000 | Loss: 0.00005349
Iteration 586/1000 | Loss: 0.00005824
Iteration 587/1000 | Loss: 0.00005610
Iteration 588/1000 | Loss: 0.00005390
Iteration 589/1000 | Loss: 0.00005421
Iteration 590/1000 | Loss: 0.00005201
Iteration 591/1000 | Loss: 0.00005374
Iteration 592/1000 | Loss: 0.00004910
Iteration 593/1000 | Loss: 0.00005074
Iteration 594/1000 | Loss: 0.00005743
Iteration 595/1000 | Loss: 0.00005515
Iteration 596/1000 | Loss: 0.00010861
Iteration 597/1000 | Loss: 0.00005958
Iteration 598/1000 | Loss: 0.00004948
Iteration 599/1000 | Loss: 0.00005003
Iteration 600/1000 | Loss: 0.00005447
Iteration 601/1000 | Loss: 0.00005446
Iteration 602/1000 | Loss: 0.00006119
Iteration 603/1000 | Loss: 0.00005446
Iteration 604/1000 | Loss: 0.00005443
Iteration 605/1000 | Loss: 0.00006042
Iteration 606/1000 | Loss: 0.00006322
Iteration 607/1000 | Loss: 0.00016303
Iteration 608/1000 | Loss: 0.00005129
Iteration 609/1000 | Loss: 0.00005180
Iteration 610/1000 | Loss: 0.00006385
Iteration 611/1000 | Loss: 0.00006260
Iteration 612/1000 | Loss: 0.00006536
Iteration 613/1000 | Loss: 0.00006241
Iteration 614/1000 | Loss: 0.00005636
Iteration 615/1000 | Loss: 0.00006227
Iteration 616/1000 | Loss: 0.00006286
Iteration 617/1000 | Loss: 0.00005038
Iteration 618/1000 | Loss: 0.00005148
Iteration 619/1000 | Loss: 0.00005912
Iteration 620/1000 | Loss: 0.00005867
Iteration 621/1000 | Loss: 0.00006279
Iteration 622/1000 | Loss: 0.00038431
Iteration 623/1000 | Loss: 0.00006457
Iteration 624/1000 | Loss: 0.00005016
Iteration 625/1000 | Loss: 0.00005822
Iteration 626/1000 | Loss: 0.00005134
Iteration 627/1000 | Loss: 0.00005020
Iteration 628/1000 | Loss: 0.00004979
Iteration 629/1000 | Loss: 0.00004938
Iteration 630/1000 | Loss: 0.00004873
Iteration 631/1000 | Loss: 0.00021966
Iteration 632/1000 | Loss: 0.00005683
Iteration 633/1000 | Loss: 0.00008550
Iteration 634/1000 | Loss: 0.00005114
Iteration 635/1000 | Loss: 0.00007233
Iteration 636/1000 | Loss: 0.00004881
Iteration 637/1000 | Loss: 0.00004834
Iteration 638/1000 | Loss: 0.00004809
Iteration 639/1000 | Loss: 0.00004799
Iteration 640/1000 | Loss: 0.00004799
Iteration 641/1000 | Loss: 0.00004793
Iteration 642/1000 | Loss: 0.00005381
Iteration 643/1000 | Loss: 0.00005263
Iteration 644/1000 | Loss: 0.00004948
Iteration 645/1000 | Loss: 0.00004873
Iteration 646/1000 | Loss: 0.00005328
Iteration 647/1000 | Loss: 0.00004892
Iteration 648/1000 | Loss: 0.00004834
Iteration 649/1000 | Loss: 0.00005123
Iteration 650/1000 | Loss: 0.00005311
Iteration 651/1000 | Loss: 0.00004832
Iteration 652/1000 | Loss: 0.00004798
Iteration 653/1000 | Loss: 0.00004780
Iteration 654/1000 | Loss: 0.00004779
Iteration 655/1000 | Loss: 0.00004779
Iteration 656/1000 | Loss: 0.00004779
Iteration 657/1000 | Loss: 0.00004779
Iteration 658/1000 | Loss: 0.00004778
Iteration 659/1000 | Loss: 0.00004778
Iteration 660/1000 | Loss: 0.00004778
Iteration 661/1000 | Loss: 0.00004777
Iteration 662/1000 | Loss: 0.00004777
Iteration 663/1000 | Loss: 0.00004777
Iteration 664/1000 | Loss: 0.00004777
Iteration 665/1000 | Loss: 0.00004777
Iteration 666/1000 | Loss: 0.00004776
Iteration 667/1000 | Loss: 0.00004776
Iteration 668/1000 | Loss: 0.00004776
Iteration 669/1000 | Loss: 0.00004775
Iteration 670/1000 | Loss: 0.00004774
Iteration 671/1000 | Loss: 0.00004772
Iteration 672/1000 | Loss: 0.00004769
Iteration 673/1000 | Loss: 0.00004769
Iteration 674/1000 | Loss: 0.00004768
Iteration 675/1000 | Loss: 0.00004768
Iteration 676/1000 | Loss: 0.00004768
Iteration 677/1000 | Loss: 0.00004766
Iteration 678/1000 | Loss: 0.00004766
Iteration 679/1000 | Loss: 0.00004766
Iteration 680/1000 | Loss: 0.00004766
Iteration 681/1000 | Loss: 0.00004766
Iteration 682/1000 | Loss: 0.00004765
Iteration 683/1000 | Loss: 0.00004765
Iteration 684/1000 | Loss: 0.00004765
Iteration 685/1000 | Loss: 0.00004765
Iteration 686/1000 | Loss: 0.00004764
Iteration 687/1000 | Loss: 0.00004764
Iteration 688/1000 | Loss: 0.00004763
Iteration 689/1000 | Loss: 0.00004763
Iteration 690/1000 | Loss: 0.00004763
Iteration 691/1000 | Loss: 0.00004763
Iteration 692/1000 | Loss: 0.00004763
Iteration 693/1000 | Loss: 0.00004763
Iteration 694/1000 | Loss: 0.00004763
Iteration 695/1000 | Loss: 0.00004763
Iteration 696/1000 | Loss: 0.00004763
Iteration 697/1000 | Loss: 0.00004762
Iteration 698/1000 | Loss: 0.00004762
Iteration 699/1000 | Loss: 0.00004762
Iteration 700/1000 | Loss: 0.00004762
Iteration 701/1000 | Loss: 0.00004762
Iteration 702/1000 | Loss: 0.00004762
Iteration 703/1000 | Loss: 0.00004762
Iteration 704/1000 | Loss: 0.00004762
Iteration 705/1000 | Loss: 0.00004761
Iteration 706/1000 | Loss: 0.00004761
Iteration 707/1000 | Loss: 0.00004761
Iteration 708/1000 | Loss: 0.00004760
Iteration 709/1000 | Loss: 0.00004760
Iteration 710/1000 | Loss: 0.00004759
Iteration 711/1000 | Loss: 0.00004757
Iteration 712/1000 | Loss: 0.00004756
Iteration 713/1000 | Loss: 0.00004755
Iteration 714/1000 | Loss: 0.00004754
Iteration 715/1000 | Loss: 0.00004754
Iteration 716/1000 | Loss: 0.00004753
Iteration 717/1000 | Loss: 0.00004753
Iteration 718/1000 | Loss: 0.00004753
Iteration 719/1000 | Loss: 0.00004753
Iteration 720/1000 | Loss: 0.00004752
Iteration 721/1000 | Loss: 0.00004752
Iteration 722/1000 | Loss: 0.00004752
Iteration 723/1000 | Loss: 0.00004752
Iteration 724/1000 | Loss: 0.00004752
Iteration 725/1000 | Loss: 0.00004751
Iteration 726/1000 | Loss: 0.00004751
Iteration 727/1000 | Loss: 0.00004751
Iteration 728/1000 | Loss: 0.00004751
Iteration 729/1000 | Loss: 0.00004751
Iteration 730/1000 | Loss: 0.00004751
Iteration 731/1000 | Loss: 0.00004751
Iteration 732/1000 | Loss: 0.00004751
Iteration 733/1000 | Loss: 0.00004751
Iteration 734/1000 | Loss: 0.00004751
Iteration 735/1000 | Loss: 0.00004750
Iteration 736/1000 | Loss: 0.00004750
Iteration 737/1000 | Loss: 0.00004750
Iteration 738/1000 | Loss: 0.00004750
Iteration 739/1000 | Loss: 0.00004750
Iteration 740/1000 | Loss: 0.00004750
Iteration 741/1000 | Loss: 0.00004750
Iteration 742/1000 | Loss: 0.00004750
Iteration 743/1000 | Loss: 0.00004750
Iteration 744/1000 | Loss: 0.00004750
Iteration 745/1000 | Loss: 0.00004750
Iteration 746/1000 | Loss: 0.00004749
Iteration 747/1000 | Loss: 0.00004749
Iteration 748/1000 | Loss: 0.00004749
Iteration 749/1000 | Loss: 0.00004749
Iteration 750/1000 | Loss: 0.00004749
Iteration 751/1000 | Loss: 0.00004749
Iteration 752/1000 | Loss: 0.00004749
Iteration 753/1000 | Loss: 0.00004749
Iteration 754/1000 | Loss: 0.00004749
Iteration 755/1000 | Loss: 0.00004749
Iteration 756/1000 | Loss: 0.00004749
Iteration 757/1000 | Loss: 0.00004749
Iteration 758/1000 | Loss: 0.00004749
Iteration 759/1000 | Loss: 0.00004749
Iteration 760/1000 | Loss: 0.00004748
Iteration 761/1000 | Loss: 0.00004748
Iteration 762/1000 | Loss: 0.00004748
Iteration 763/1000 | Loss: 0.00004748
Iteration 764/1000 | Loss: 0.00004748
Iteration 765/1000 | Loss: 0.00004748
Iteration 766/1000 | Loss: 0.00004748
Iteration 767/1000 | Loss: 0.00004748
Iteration 768/1000 | Loss: 0.00004748
Iteration 769/1000 | Loss: 0.00004748
Iteration 770/1000 | Loss: 0.00004748
Iteration 771/1000 | Loss: 0.00004748
Iteration 772/1000 | Loss: 0.00004748
Iteration 773/1000 | Loss: 0.00004747
Iteration 774/1000 | Loss: 0.00004747
Iteration 775/1000 | Loss: 0.00004747
Iteration 776/1000 | Loss: 0.00004747
Iteration 777/1000 | Loss: 0.00004747
Iteration 778/1000 | Loss: 0.00004747
Iteration 779/1000 | Loss: 0.00004747
Iteration 780/1000 | Loss: 0.00004747
Iteration 781/1000 | Loss: 0.00004746
Iteration 782/1000 | Loss: 0.00004746
Iteration 783/1000 | Loss: 0.00004746
Iteration 784/1000 | Loss: 0.00004746
Iteration 785/1000 | Loss: 0.00004746
Iteration 786/1000 | Loss: 0.00004746
Iteration 787/1000 | Loss: 0.00004746
Iteration 788/1000 | Loss: 0.00004746
Iteration 789/1000 | Loss: 0.00004746
Iteration 790/1000 | Loss: 0.00004745
Iteration 791/1000 | Loss: 0.00004745
Iteration 792/1000 | Loss: 0.00004745
Iteration 793/1000 | Loss: 0.00004745
Iteration 794/1000 | Loss: 0.00004745
Iteration 795/1000 | Loss: 0.00004745
Iteration 796/1000 | Loss: 0.00004745
Iteration 797/1000 | Loss: 0.00004745
Iteration 798/1000 | Loss: 0.00004745
Iteration 799/1000 | Loss: 0.00004745
Iteration 800/1000 | Loss: 0.00004745
Iteration 801/1000 | Loss: 0.00004745
Iteration 802/1000 | Loss: 0.00004745
Iteration 803/1000 | Loss: 0.00004744
Iteration 804/1000 | Loss: 0.00004744
Iteration 805/1000 | Loss: 0.00004744
Iteration 806/1000 | Loss: 0.00004744
Iteration 807/1000 | Loss: 0.00004744
Iteration 808/1000 | Loss: 0.00004744
Iteration 809/1000 | Loss: 0.00004743
Iteration 810/1000 | Loss: 0.00004743
Iteration 811/1000 | Loss: 0.00004743
Iteration 812/1000 | Loss: 0.00004743
Iteration 813/1000 | Loss: 0.00004742
Iteration 814/1000 | Loss: 0.00004742
Iteration 815/1000 | Loss: 0.00004742
Iteration 816/1000 | Loss: 0.00004742
Iteration 817/1000 | Loss: 0.00004742
Iteration 818/1000 | Loss: 0.00004741
Iteration 819/1000 | Loss: 0.00004741
Iteration 820/1000 | Loss: 0.00004741
Iteration 821/1000 | Loss: 0.00004741
Iteration 822/1000 | Loss: 0.00004741
Iteration 823/1000 | Loss: 0.00004741
Iteration 824/1000 | Loss: 0.00004741
Iteration 825/1000 | Loss: 0.00004741
Iteration 826/1000 | Loss: 0.00004741
Iteration 827/1000 | Loss: 0.00004741
Iteration 828/1000 | Loss: 0.00004740
Iteration 829/1000 | Loss: 0.00004740
Iteration 830/1000 | Loss: 0.00004740
Iteration 831/1000 | Loss: 0.00004740
Iteration 832/1000 | Loss: 0.00004740
Iteration 833/1000 | Loss: 0.00004740
Iteration 834/1000 | Loss: 0.00004740
Iteration 835/1000 | Loss: 0.00004740
Iteration 836/1000 | Loss: 0.00004740
Iteration 837/1000 | Loss: 0.00004740
Iteration 838/1000 | Loss: 0.00004740
Iteration 839/1000 | Loss: 0.00004740
Iteration 840/1000 | Loss: 0.00004739
Iteration 841/1000 | Loss: 0.00004739
Iteration 842/1000 | Loss: 0.00004739
Iteration 843/1000 | Loss: 0.00004739
Iteration 844/1000 | Loss: 0.00004739
Iteration 845/1000 | Loss: 0.00004739
Iteration 846/1000 | Loss: 0.00004739
Iteration 847/1000 | Loss: 0.00004739
Iteration 848/1000 | Loss: 0.00004739
Iteration 849/1000 | Loss: 0.00004739
Iteration 850/1000 | Loss: 0.00004739
Iteration 851/1000 | Loss: 0.00004739
Iteration 852/1000 | Loss: 0.00004739
Iteration 853/1000 | Loss: 0.00004738
Iteration 854/1000 | Loss: 0.00004738
Iteration 855/1000 | Loss: 0.00004738
Iteration 856/1000 | Loss: 0.00004738
Iteration 857/1000 | Loss: 0.00004738
Iteration 858/1000 | Loss: 0.00004738
Iteration 859/1000 | Loss: 0.00004738
Iteration 860/1000 | Loss: 0.00004738
Iteration 861/1000 | Loss: 0.00004738
Iteration 862/1000 | Loss: 0.00004738
Iteration 863/1000 | Loss: 0.00004738
Iteration 864/1000 | Loss: 0.00004738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 864. Stopping optimization.
Last 5 losses: [4.738419738714583e-05, 4.738419738714583e-05, 4.738419738714583e-05, 4.738419738714583e-05, 4.738419738714583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.738419738714583e-05

Optimization complete. Final v2v error: 4.8300909996032715 mm

Highest mean error: 12.2626314163208 mm for frame 0

Lowest mean error: 3.7233028411865234 mm for frame 189

Saving results

Total time: 1090.4800384044647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531743
Iteration 2/25 | Loss: 0.00158610
Iteration 3/25 | Loss: 0.00141390
Iteration 4/25 | Loss: 0.00139341
Iteration 5/25 | Loss: 0.00138686
Iteration 6/25 | Loss: 0.00138489
Iteration 7/25 | Loss: 0.00138478
Iteration 8/25 | Loss: 0.00138478
Iteration 9/25 | Loss: 0.00138478
Iteration 10/25 | Loss: 0.00138478
Iteration 11/25 | Loss: 0.00138478
Iteration 12/25 | Loss: 0.00138478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013847770169377327, 0.0013847770169377327, 0.0013847770169377327, 0.0013847770169377327, 0.0013847770169377327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013847770169377327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85943156
Iteration 2/25 | Loss: 0.00153128
Iteration 3/25 | Loss: 0.00153128
Iteration 4/25 | Loss: 0.00153128
Iteration 5/25 | Loss: 0.00153128
Iteration 6/25 | Loss: 0.00153128
Iteration 7/25 | Loss: 0.00153128
Iteration 8/25 | Loss: 0.00153128
Iteration 9/25 | Loss: 0.00153128
Iteration 10/25 | Loss: 0.00153128
Iteration 11/25 | Loss: 0.00153128
Iteration 12/25 | Loss: 0.00153128
Iteration 13/25 | Loss: 0.00153128
Iteration 14/25 | Loss: 0.00153128
Iteration 15/25 | Loss: 0.00153128
Iteration 16/25 | Loss: 0.00153128
Iteration 17/25 | Loss: 0.00153128
Iteration 18/25 | Loss: 0.00153128
Iteration 19/25 | Loss: 0.00153128
Iteration 20/25 | Loss: 0.00153128
Iteration 21/25 | Loss: 0.00153128
Iteration 22/25 | Loss: 0.00153128
Iteration 23/25 | Loss: 0.00153128
Iteration 24/25 | Loss: 0.00153128
Iteration 25/25 | Loss: 0.00153128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153128
Iteration 2/1000 | Loss: 0.00006293
Iteration 3/1000 | Loss: 0.00004224
Iteration 4/1000 | Loss: 0.00003572
Iteration 5/1000 | Loss: 0.00003393
Iteration 6/1000 | Loss: 0.00003228
Iteration 7/1000 | Loss: 0.00003157
Iteration 8/1000 | Loss: 0.00003062
Iteration 9/1000 | Loss: 0.00002997
Iteration 10/1000 | Loss: 0.00002952
Iteration 11/1000 | Loss: 0.00002906
Iteration 12/1000 | Loss: 0.00002870
Iteration 13/1000 | Loss: 0.00002843
Iteration 14/1000 | Loss: 0.00002819
Iteration 15/1000 | Loss: 0.00002796
Iteration 16/1000 | Loss: 0.00002779
Iteration 17/1000 | Loss: 0.00002778
Iteration 18/1000 | Loss: 0.00002764
Iteration 19/1000 | Loss: 0.00002759
Iteration 20/1000 | Loss: 0.00002742
Iteration 21/1000 | Loss: 0.00002738
Iteration 22/1000 | Loss: 0.00002736
Iteration 23/1000 | Loss: 0.00002735
Iteration 24/1000 | Loss: 0.00002734
Iteration 25/1000 | Loss: 0.00002733
Iteration 26/1000 | Loss: 0.00002733
Iteration 27/1000 | Loss: 0.00002732
Iteration 28/1000 | Loss: 0.00002731
Iteration 29/1000 | Loss: 0.00002730
Iteration 30/1000 | Loss: 0.00002730
Iteration 31/1000 | Loss: 0.00002729
Iteration 32/1000 | Loss: 0.00002729
Iteration 33/1000 | Loss: 0.00002728
Iteration 34/1000 | Loss: 0.00002728
Iteration 35/1000 | Loss: 0.00002727
Iteration 36/1000 | Loss: 0.00002726
Iteration 37/1000 | Loss: 0.00002725
Iteration 38/1000 | Loss: 0.00002723
Iteration 39/1000 | Loss: 0.00002723
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002722
Iteration 42/1000 | Loss: 0.00002716
Iteration 43/1000 | Loss: 0.00002716
Iteration 44/1000 | Loss: 0.00002716
Iteration 45/1000 | Loss: 0.00002716
Iteration 46/1000 | Loss: 0.00002716
Iteration 47/1000 | Loss: 0.00002715
Iteration 48/1000 | Loss: 0.00002715
Iteration 49/1000 | Loss: 0.00002715
Iteration 50/1000 | Loss: 0.00002715
Iteration 51/1000 | Loss: 0.00002714
Iteration 52/1000 | Loss: 0.00002713
Iteration 53/1000 | Loss: 0.00002713
Iteration 54/1000 | Loss: 0.00002712
Iteration 55/1000 | Loss: 0.00002712
Iteration 56/1000 | Loss: 0.00002712
Iteration 57/1000 | Loss: 0.00002712
Iteration 58/1000 | Loss: 0.00002711
Iteration 59/1000 | Loss: 0.00002711
Iteration 60/1000 | Loss: 0.00002711
Iteration 61/1000 | Loss: 0.00002710
Iteration 62/1000 | Loss: 0.00002710
Iteration 63/1000 | Loss: 0.00002710
Iteration 64/1000 | Loss: 0.00002710
Iteration 65/1000 | Loss: 0.00002710
Iteration 66/1000 | Loss: 0.00002710
Iteration 67/1000 | Loss: 0.00002710
Iteration 68/1000 | Loss: 0.00002710
Iteration 69/1000 | Loss: 0.00002710
Iteration 70/1000 | Loss: 0.00002710
Iteration 71/1000 | Loss: 0.00002710
Iteration 72/1000 | Loss: 0.00002709
Iteration 73/1000 | Loss: 0.00002709
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00002709
Iteration 76/1000 | Loss: 0.00002709
Iteration 77/1000 | Loss: 0.00002709
Iteration 78/1000 | Loss: 0.00002709
Iteration 79/1000 | Loss: 0.00002708
Iteration 80/1000 | Loss: 0.00002708
Iteration 81/1000 | Loss: 0.00002708
Iteration 82/1000 | Loss: 0.00002708
Iteration 83/1000 | Loss: 0.00002708
Iteration 84/1000 | Loss: 0.00002708
Iteration 85/1000 | Loss: 0.00002708
Iteration 86/1000 | Loss: 0.00002708
Iteration 87/1000 | Loss: 0.00002708
Iteration 88/1000 | Loss: 0.00002707
Iteration 89/1000 | Loss: 0.00002707
Iteration 90/1000 | Loss: 0.00002707
Iteration 91/1000 | Loss: 0.00002707
Iteration 92/1000 | Loss: 0.00002706
Iteration 93/1000 | Loss: 0.00002706
Iteration 94/1000 | Loss: 0.00002706
Iteration 95/1000 | Loss: 0.00002706
Iteration 96/1000 | Loss: 0.00002706
Iteration 97/1000 | Loss: 0.00002706
Iteration 98/1000 | Loss: 0.00002706
Iteration 99/1000 | Loss: 0.00002706
Iteration 100/1000 | Loss: 0.00002706
Iteration 101/1000 | Loss: 0.00002706
Iteration 102/1000 | Loss: 0.00002706
Iteration 103/1000 | Loss: 0.00002706
Iteration 104/1000 | Loss: 0.00002706
Iteration 105/1000 | Loss: 0.00002706
Iteration 106/1000 | Loss: 0.00002705
Iteration 107/1000 | Loss: 0.00002705
Iteration 108/1000 | Loss: 0.00002705
Iteration 109/1000 | Loss: 0.00002705
Iteration 110/1000 | Loss: 0.00002705
Iteration 111/1000 | Loss: 0.00002705
Iteration 112/1000 | Loss: 0.00002705
Iteration 113/1000 | Loss: 0.00002704
Iteration 114/1000 | Loss: 0.00002704
Iteration 115/1000 | Loss: 0.00002704
Iteration 116/1000 | Loss: 0.00002704
Iteration 117/1000 | Loss: 0.00002704
Iteration 118/1000 | Loss: 0.00002704
Iteration 119/1000 | Loss: 0.00002704
Iteration 120/1000 | Loss: 0.00002704
Iteration 121/1000 | Loss: 0.00002704
Iteration 122/1000 | Loss: 0.00002704
Iteration 123/1000 | Loss: 0.00002704
Iteration 124/1000 | Loss: 0.00002704
Iteration 125/1000 | Loss: 0.00002703
Iteration 126/1000 | Loss: 0.00002703
Iteration 127/1000 | Loss: 0.00002703
Iteration 128/1000 | Loss: 0.00002703
Iteration 129/1000 | Loss: 0.00002703
Iteration 130/1000 | Loss: 0.00002703
Iteration 131/1000 | Loss: 0.00002703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.7034673621528782e-05, 2.7034673621528782e-05, 2.7034673621528782e-05, 2.7034673621528782e-05, 2.7034673621528782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7034673621528782e-05

Optimization complete. Final v2v error: 4.337677955627441 mm

Highest mean error: 5.0738019943237305 mm for frame 63

Lowest mean error: 3.7667338848114014 mm for frame 173

Saving results

Total time: 46.961421251297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022800
Iteration 2/25 | Loss: 0.01022800
Iteration 3/25 | Loss: 0.01022800
Iteration 4/25 | Loss: 0.01022800
Iteration 5/25 | Loss: 0.01022800
Iteration 6/25 | Loss: 0.01022800
Iteration 7/25 | Loss: 0.01022800
Iteration 8/25 | Loss: 0.01022799
Iteration 9/25 | Loss: 0.01022799
Iteration 10/25 | Loss: 0.01022799
Iteration 11/25 | Loss: 0.01022799
Iteration 12/25 | Loss: 0.01022799
Iteration 13/25 | Loss: 0.01022799
Iteration 14/25 | Loss: 0.01022799
Iteration 15/25 | Loss: 0.01022799
Iteration 16/25 | Loss: 0.01022799
Iteration 17/25 | Loss: 0.01022799
Iteration 18/25 | Loss: 0.01022798
Iteration 19/25 | Loss: 0.01022798
Iteration 20/25 | Loss: 0.01022798
Iteration 21/25 | Loss: 0.01022798
Iteration 22/25 | Loss: 0.01022798
Iteration 23/25 | Loss: 0.01022798
Iteration 24/25 | Loss: 0.01022798
Iteration 25/25 | Loss: 0.01022798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77131093
Iteration 2/25 | Loss: 0.06433082
Iteration 3/25 | Loss: 0.06431536
Iteration 4/25 | Loss: 0.06431535
Iteration 5/25 | Loss: 0.06431533
Iteration 6/25 | Loss: 0.06431533
Iteration 7/25 | Loss: 0.06431533
Iteration 8/25 | Loss: 0.06431533
Iteration 9/25 | Loss: 0.06431533
Iteration 10/25 | Loss: 0.06431533
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.06431533396244049, 0.06431533396244049, 0.06431533396244049, 0.06431533396244049, 0.06431533396244049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06431533396244049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06431533
Iteration 2/1000 | Loss: 0.00348357
Iteration 3/1000 | Loss: 0.00053361
Iteration 4/1000 | Loss: 0.00026100
Iteration 5/1000 | Loss: 0.00014430
Iteration 6/1000 | Loss: 0.00009992
Iteration 7/1000 | Loss: 0.00007613
Iteration 8/1000 | Loss: 0.00006461
Iteration 9/1000 | Loss: 0.00005499
Iteration 10/1000 | Loss: 0.00004821
Iteration 11/1000 | Loss: 0.00004226
Iteration 12/1000 | Loss: 0.00003853
Iteration 13/1000 | Loss: 0.00003545
Iteration 14/1000 | Loss: 0.00003286
Iteration 15/1000 | Loss: 0.00003076
Iteration 16/1000 | Loss: 0.00002896
Iteration 17/1000 | Loss: 0.00002715
Iteration 18/1000 | Loss: 0.00002614
Iteration 19/1000 | Loss: 0.00002520
Iteration 20/1000 | Loss: 0.00002439
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002147
Iteration 26/1000 | Loss: 0.00002118
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002056
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00002017
Iteration 31/1000 | Loss: 0.00002000
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001984
Iteration 36/1000 | Loss: 0.00001983
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001958
Iteration 42/1000 | Loss: 0.00001956
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001951
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001945
Iteration 61/1000 | Loss: 0.00001945
Iteration 62/1000 | Loss: 0.00001944
Iteration 63/1000 | Loss: 0.00001944
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001941
Iteration 68/1000 | Loss: 0.00001941
Iteration 69/1000 | Loss: 0.00001941
Iteration 70/1000 | Loss: 0.00001941
Iteration 71/1000 | Loss: 0.00001941
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001940
Iteration 76/1000 | Loss: 0.00001940
Iteration 77/1000 | Loss: 0.00001940
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001939
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001938
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001938
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001936
Iteration 101/1000 | Loss: 0.00001936
Iteration 102/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.9359076759428717e-05, 1.9359076759428717e-05, 1.9359076759428717e-05, 1.9359076759428717e-05, 1.9359076759428717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9359076759428717e-05

Optimization complete. Final v2v error: 3.5937392711639404 mm

Highest mean error: 5.372507572174072 mm for frame 48

Lowest mean error: 2.947493076324463 mm for frame 189

Saving results

Total time: 66.50758790969849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991308
Iteration 2/25 | Loss: 0.00249547
Iteration 3/25 | Loss: 0.00190958
Iteration 4/25 | Loss: 0.00168552
Iteration 5/25 | Loss: 0.00160358
Iteration 6/25 | Loss: 0.00155479
Iteration 7/25 | Loss: 0.00153681
Iteration 8/25 | Loss: 0.00150038
Iteration 9/25 | Loss: 0.00150185
Iteration 10/25 | Loss: 0.00147067
Iteration 11/25 | Loss: 0.00146960
Iteration 12/25 | Loss: 0.00146052
Iteration 13/25 | Loss: 0.00145968
Iteration 14/25 | Loss: 0.00145249
Iteration 15/25 | Loss: 0.00144837
Iteration 16/25 | Loss: 0.00144575
Iteration 17/25 | Loss: 0.00144305
Iteration 18/25 | Loss: 0.00145023
Iteration 19/25 | Loss: 0.00144384
Iteration 20/25 | Loss: 0.00145494
Iteration 21/25 | Loss: 0.00145235
Iteration 22/25 | Loss: 0.00145211
Iteration 23/25 | Loss: 0.00144293
Iteration 24/25 | Loss: 0.00143992
Iteration 25/25 | Loss: 0.00143384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30066097
Iteration 2/25 | Loss: 0.00210465
Iteration 3/25 | Loss: 0.00210464
Iteration 4/25 | Loss: 0.00199723
Iteration 5/25 | Loss: 0.00199691
Iteration 6/25 | Loss: 0.00199691
Iteration 7/25 | Loss: 0.00199691
Iteration 8/25 | Loss: 0.00199691
Iteration 9/25 | Loss: 0.00199691
Iteration 10/25 | Loss: 0.00199691
Iteration 11/25 | Loss: 0.00199691
Iteration 12/25 | Loss: 0.00199691
Iteration 13/25 | Loss: 0.00199691
Iteration 14/25 | Loss: 0.00199691
Iteration 15/25 | Loss: 0.00199691
Iteration 16/25 | Loss: 0.00199691
Iteration 17/25 | Loss: 0.00199691
Iteration 18/25 | Loss: 0.00199691
Iteration 19/25 | Loss: 0.00199691
Iteration 20/25 | Loss: 0.00199691
Iteration 21/25 | Loss: 0.00199691
Iteration 22/25 | Loss: 0.00199691
Iteration 23/25 | Loss: 0.00199691
Iteration 24/25 | Loss: 0.00199691
Iteration 25/25 | Loss: 0.00199691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0019969067070633173, 0.0019969067070633173, 0.0019969067070633173, 0.0019969067070633173, 0.0019969067070633173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019969067070633173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199691
Iteration 2/1000 | Loss: 0.00086890
Iteration 3/1000 | Loss: 0.00048673
Iteration 4/1000 | Loss: 0.00012497
Iteration 5/1000 | Loss: 0.00034461
Iteration 6/1000 | Loss: 0.00032974
Iteration 7/1000 | Loss: 0.00033992
Iteration 8/1000 | Loss: 0.00036884
Iteration 9/1000 | Loss: 0.00033841
Iteration 10/1000 | Loss: 0.00030625
Iteration 11/1000 | Loss: 0.00024775
Iteration 12/1000 | Loss: 0.00008765
Iteration 13/1000 | Loss: 0.00008371
Iteration 14/1000 | Loss: 0.00029806
Iteration 15/1000 | Loss: 0.00068819
Iteration 16/1000 | Loss: 0.00039415
Iteration 17/1000 | Loss: 0.00036054
Iteration 18/1000 | Loss: 0.00037334
Iteration 19/1000 | Loss: 0.00043793
Iteration 20/1000 | Loss: 0.00022011
Iteration 21/1000 | Loss: 0.00043784
Iteration 22/1000 | Loss: 0.00048217
Iteration 23/1000 | Loss: 0.00026538
Iteration 24/1000 | Loss: 0.00008296
Iteration 25/1000 | Loss: 0.00055327
Iteration 26/1000 | Loss: 0.00024490
Iteration 27/1000 | Loss: 0.00022961
Iteration 28/1000 | Loss: 0.00049099
Iteration 29/1000 | Loss: 0.00040552
Iteration 30/1000 | Loss: 0.00034515
Iteration 31/1000 | Loss: 0.00031674
Iteration 32/1000 | Loss: 0.00009981
Iteration 33/1000 | Loss: 0.00008705
Iteration 34/1000 | Loss: 0.00008123
Iteration 35/1000 | Loss: 0.00007693
Iteration 36/1000 | Loss: 0.00007339
Iteration 37/1000 | Loss: 0.00023452
Iteration 38/1000 | Loss: 0.00036627
Iteration 39/1000 | Loss: 0.00007391
Iteration 40/1000 | Loss: 0.00039581
Iteration 41/1000 | Loss: 0.00299675
Iteration 42/1000 | Loss: 0.00399248
Iteration 43/1000 | Loss: 0.00118631
Iteration 44/1000 | Loss: 0.00046424
Iteration 45/1000 | Loss: 0.00020466
Iteration 46/1000 | Loss: 0.00008705
Iteration 47/1000 | Loss: 0.00016699
Iteration 48/1000 | Loss: 0.00005642
Iteration 49/1000 | Loss: 0.00004692
Iteration 50/1000 | Loss: 0.00004899
Iteration 51/1000 | Loss: 0.00003898
Iteration 52/1000 | Loss: 0.00003158
Iteration 53/1000 | Loss: 0.00037267
Iteration 54/1000 | Loss: 0.00059250
Iteration 55/1000 | Loss: 0.00003220
Iteration 56/1000 | Loss: 0.00002831
Iteration 57/1000 | Loss: 0.00045972
Iteration 58/1000 | Loss: 0.00186042
Iteration 59/1000 | Loss: 0.00005767
Iteration 60/1000 | Loss: 0.00024254
Iteration 61/1000 | Loss: 0.00002479
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00001930
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001315
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001307
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001306
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001302
Iteration 148/1000 | Loss: 0.00001302
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001301
Iteration 152/1000 | Loss: 0.00001301
Iteration 153/1000 | Loss: 0.00001301
Iteration 154/1000 | Loss: 0.00001301
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001300
Iteration 166/1000 | Loss: 0.00001300
Iteration 167/1000 | Loss: 0.00001300
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001298
Iteration 179/1000 | Loss: 0.00001298
Iteration 180/1000 | Loss: 0.00001298
Iteration 181/1000 | Loss: 0.00001298
Iteration 182/1000 | Loss: 0.00001298
Iteration 183/1000 | Loss: 0.00001298
Iteration 184/1000 | Loss: 0.00001298
Iteration 185/1000 | Loss: 0.00001298
Iteration 186/1000 | Loss: 0.00001298
Iteration 187/1000 | Loss: 0.00001298
Iteration 188/1000 | Loss: 0.00001298
Iteration 189/1000 | Loss: 0.00001298
Iteration 190/1000 | Loss: 0.00001298
Iteration 191/1000 | Loss: 0.00001298
Iteration 192/1000 | Loss: 0.00001298
Iteration 193/1000 | Loss: 0.00001298
Iteration 194/1000 | Loss: 0.00001298
Iteration 195/1000 | Loss: 0.00001298
Iteration 196/1000 | Loss: 0.00001298
Iteration 197/1000 | Loss: 0.00001298
Iteration 198/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.2979361599718686e-05, 1.2979361599718686e-05, 1.2979361599718686e-05, 1.2979361599718686e-05, 1.2979361599718686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2979361599718686e-05

Optimization complete. Final v2v error: 3.0105948448181152 mm

Highest mean error: 5.193708419799805 mm for frame 55

Lowest mean error: 2.722121238708496 mm for frame 39

Saving results

Total time: 157.5891695022583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400909
Iteration 2/25 | Loss: 0.00140070
Iteration 3/25 | Loss: 0.00129334
Iteration 4/25 | Loss: 0.00127972
Iteration 5/25 | Loss: 0.00127747
Iteration 6/25 | Loss: 0.00127738
Iteration 7/25 | Loss: 0.00127738
Iteration 8/25 | Loss: 0.00127738
Iteration 9/25 | Loss: 0.00127738
Iteration 10/25 | Loss: 0.00127738
Iteration 11/25 | Loss: 0.00127738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012773798080161214, 0.0012773798080161214, 0.0012773798080161214, 0.0012773798080161214, 0.0012773798080161214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012773798080161214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29551303
Iteration 2/25 | Loss: 0.00120600
Iteration 3/25 | Loss: 0.00120599
Iteration 4/25 | Loss: 0.00120599
Iteration 5/25 | Loss: 0.00120599
Iteration 6/25 | Loss: 0.00120599
Iteration 7/25 | Loss: 0.00120599
Iteration 8/25 | Loss: 0.00120599
Iteration 9/25 | Loss: 0.00120599
Iteration 10/25 | Loss: 0.00120599
Iteration 11/25 | Loss: 0.00120599
Iteration 12/25 | Loss: 0.00120599
Iteration 13/25 | Loss: 0.00120599
Iteration 14/25 | Loss: 0.00120599
Iteration 15/25 | Loss: 0.00120599
Iteration 16/25 | Loss: 0.00120599
Iteration 17/25 | Loss: 0.00120599
Iteration 18/25 | Loss: 0.00120599
Iteration 19/25 | Loss: 0.00120599
Iteration 20/25 | Loss: 0.00120599
Iteration 21/25 | Loss: 0.00120599
Iteration 22/25 | Loss: 0.00120599
Iteration 23/25 | Loss: 0.00120599
Iteration 24/25 | Loss: 0.00120599
Iteration 25/25 | Loss: 0.00120599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120599
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001931
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001475
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001268
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001251
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001242
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001239
Iteration 67/1000 | Loss: 0.00001239
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001227
Iteration 159/1000 | Loss: 0.00001227
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001227
Iteration 163/1000 | Loss: 0.00001227
Iteration 164/1000 | Loss: 0.00001227
Iteration 165/1000 | Loss: 0.00001227
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001227
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Iteration 170/1000 | Loss: 0.00001227
Iteration 171/1000 | Loss: 0.00001227
Iteration 172/1000 | Loss: 0.00001227
Iteration 173/1000 | Loss: 0.00001227
Iteration 174/1000 | Loss: 0.00001227
Iteration 175/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.227272969117621e-05, 1.227272969117621e-05, 1.227272969117621e-05, 1.227272969117621e-05, 1.227272969117621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.227272969117621e-05

Optimization complete. Final v2v error: 3.0212340354919434 mm

Highest mean error: 3.3677978515625 mm for frame 78

Lowest mean error: 2.8236117362976074 mm for frame 20

Saving results

Total time: 42.771822690963745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801223
Iteration 2/25 | Loss: 0.00144520
Iteration 3/25 | Loss: 0.00133657
Iteration 4/25 | Loss: 0.00132593
Iteration 5/25 | Loss: 0.00132329
Iteration 6/25 | Loss: 0.00132329
Iteration 7/25 | Loss: 0.00132329
Iteration 8/25 | Loss: 0.00132329
Iteration 9/25 | Loss: 0.00132329
Iteration 10/25 | Loss: 0.00132329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001323292963206768, 0.001323292963206768, 0.001323292963206768, 0.001323292963206768, 0.001323292963206768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001323292963206768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.40170002
Iteration 2/25 | Loss: 0.00126988
Iteration 3/25 | Loss: 0.00126978
Iteration 4/25 | Loss: 0.00126978
Iteration 5/25 | Loss: 0.00126978
Iteration 6/25 | Loss: 0.00126978
Iteration 7/25 | Loss: 0.00126978
Iteration 8/25 | Loss: 0.00126978
Iteration 9/25 | Loss: 0.00126978
Iteration 10/25 | Loss: 0.00126978
Iteration 11/25 | Loss: 0.00126978
Iteration 12/25 | Loss: 0.00126978
Iteration 13/25 | Loss: 0.00126978
Iteration 14/25 | Loss: 0.00126978
Iteration 15/25 | Loss: 0.00126978
Iteration 16/25 | Loss: 0.00126978
Iteration 17/25 | Loss: 0.00126978
Iteration 18/25 | Loss: 0.00126978
Iteration 19/25 | Loss: 0.00126978
Iteration 20/25 | Loss: 0.00126978
Iteration 21/25 | Loss: 0.00126978
Iteration 22/25 | Loss: 0.00126978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012697795173153281, 0.0012697795173153281, 0.0012697795173153281, 0.0012697795173153281, 0.0012697795173153281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012697795173153281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126978
Iteration 2/1000 | Loss: 0.00003283
Iteration 3/1000 | Loss: 0.00002538
Iteration 4/1000 | Loss: 0.00002308
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002088
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001974
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001781
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001778
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001742
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001740
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001739
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001735
Iteration 79/1000 | Loss: 0.00001735
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Iteration 82/1000 | Loss: 0.00001734
Iteration 83/1000 | Loss: 0.00001734
Iteration 84/1000 | Loss: 0.00001734
Iteration 85/1000 | Loss: 0.00001734
Iteration 86/1000 | Loss: 0.00001734
Iteration 87/1000 | Loss: 0.00001734
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.733447606966365e-05, 1.733447606966365e-05, 1.733447606966365e-05, 1.733447606966365e-05, 1.733447606966365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.733447606966365e-05

Optimization complete. Final v2v error: 3.550389289855957 mm

Highest mean error: 4.466976642608643 mm for frame 109

Lowest mean error: 3.1245040893554688 mm for frame 216

Saving results

Total time: 45.27877378463745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401820
Iteration 2/25 | Loss: 0.00135616
Iteration 3/25 | Loss: 0.00130004
Iteration 4/25 | Loss: 0.00129655
Iteration 5/25 | Loss: 0.00129655
Iteration 6/25 | Loss: 0.00129655
Iteration 7/25 | Loss: 0.00129655
Iteration 8/25 | Loss: 0.00129655
Iteration 9/25 | Loss: 0.00129655
Iteration 10/25 | Loss: 0.00129655
Iteration 11/25 | Loss: 0.00129655
Iteration 12/25 | Loss: 0.00129655
Iteration 13/25 | Loss: 0.00129655
Iteration 14/25 | Loss: 0.00129655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012965458445250988, 0.0012965458445250988, 0.0012965458445250988, 0.0012965458445250988, 0.0012965458445250988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012965458445250988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53844988
Iteration 2/25 | Loss: 0.00123898
Iteration 3/25 | Loss: 0.00123896
Iteration 4/25 | Loss: 0.00123896
Iteration 5/25 | Loss: 0.00123896
Iteration 6/25 | Loss: 0.00123896
Iteration 7/25 | Loss: 0.00123896
Iteration 8/25 | Loss: 0.00123896
Iteration 9/25 | Loss: 0.00123895
Iteration 10/25 | Loss: 0.00123895
Iteration 11/25 | Loss: 0.00123895
Iteration 12/25 | Loss: 0.00123895
Iteration 13/25 | Loss: 0.00123895
Iteration 14/25 | Loss: 0.00123895
Iteration 15/25 | Loss: 0.00123895
Iteration 16/25 | Loss: 0.00123895
Iteration 17/25 | Loss: 0.00123895
Iteration 18/25 | Loss: 0.00123895
Iteration 19/25 | Loss: 0.00123895
Iteration 20/25 | Loss: 0.00123895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012389542534947395, 0.0012389542534947395, 0.0012389542534947395, 0.0012389542534947395, 0.0012389542534947395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012389542534947395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123895
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001474
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001203
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001177
Iteration 23/1000 | Loss: 0.00001177
Iteration 24/1000 | Loss: 0.00001177
Iteration 25/1000 | Loss: 0.00001176
Iteration 26/1000 | Loss: 0.00001176
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001127
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001125
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001117
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001117
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001116
Iteration 95/1000 | Loss: 0.00001116
Iteration 96/1000 | Loss: 0.00001116
Iteration 97/1000 | Loss: 0.00001116
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1086236554547213e-05, 1.1086236554547213e-05, 1.1086236554547213e-05, 1.1086236554547213e-05, 1.1086236554547213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1086236554547213e-05

Optimization complete. Final v2v error: 2.8613271713256836 mm

Highest mean error: 3.0726478099823 mm for frame 105

Lowest mean error: 2.725470542907715 mm for frame 195

Saving results

Total time: 44.875383138656616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780150
Iteration 2/25 | Loss: 0.00134762
Iteration 3/25 | Loss: 0.00128556
Iteration 4/25 | Loss: 0.00127887
Iteration 5/25 | Loss: 0.00127663
Iteration 6/25 | Loss: 0.00127649
Iteration 7/25 | Loss: 0.00127649
Iteration 8/25 | Loss: 0.00127649
Iteration 9/25 | Loss: 0.00127649
Iteration 10/25 | Loss: 0.00127649
Iteration 11/25 | Loss: 0.00127649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012764878338202834, 0.0012764878338202834, 0.0012764878338202834, 0.0012764878338202834, 0.0012764878338202834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012764878338202834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31218588
Iteration 2/25 | Loss: 0.00135215
Iteration 3/25 | Loss: 0.00135215
Iteration 4/25 | Loss: 0.00135215
Iteration 5/25 | Loss: 0.00135215
Iteration 6/25 | Loss: 0.00135215
Iteration 7/25 | Loss: 0.00135215
Iteration 8/25 | Loss: 0.00135215
Iteration 9/25 | Loss: 0.00135215
Iteration 10/25 | Loss: 0.00135215
Iteration 11/25 | Loss: 0.00135215
Iteration 12/25 | Loss: 0.00135215
Iteration 13/25 | Loss: 0.00135215
Iteration 14/25 | Loss: 0.00135215
Iteration 15/25 | Loss: 0.00135215
Iteration 16/25 | Loss: 0.00135215
Iteration 17/25 | Loss: 0.00135215
Iteration 18/25 | Loss: 0.00135215
Iteration 19/25 | Loss: 0.00135215
Iteration 20/25 | Loss: 0.00135215
Iteration 21/25 | Loss: 0.00135215
Iteration 22/25 | Loss: 0.00135215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001352145103737712, 0.001352145103737712, 0.001352145103737712, 0.001352145103737712, 0.001352145103737712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001352145103737712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135214
Iteration 2/1000 | Loss: 0.00002498
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001112
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001111
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001105
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001103
Iteration 47/1000 | Loss: 0.00001101
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001100
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001087
Iteration 79/1000 | Loss: 0.00001087
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001083
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001077
Iteration 122/1000 | Loss: 0.00001077
Iteration 123/1000 | Loss: 0.00001077
Iteration 124/1000 | Loss: 0.00001077
Iteration 125/1000 | Loss: 0.00001077
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001076
Iteration 133/1000 | Loss: 0.00001076
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001070
Iteration 166/1000 | Loss: 0.00001070
Iteration 167/1000 | Loss: 0.00001070
Iteration 168/1000 | Loss: 0.00001070
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001070
Iteration 172/1000 | Loss: 0.00001070
Iteration 173/1000 | Loss: 0.00001069
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Iteration 176/1000 | Loss: 0.00001069
Iteration 177/1000 | Loss: 0.00001068
Iteration 178/1000 | Loss: 0.00001068
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001066
Iteration 193/1000 | Loss: 0.00001066
Iteration 194/1000 | Loss: 0.00001066
Iteration 195/1000 | Loss: 0.00001065
Iteration 196/1000 | Loss: 0.00001065
Iteration 197/1000 | Loss: 0.00001065
Iteration 198/1000 | Loss: 0.00001065
Iteration 199/1000 | Loss: 0.00001065
Iteration 200/1000 | Loss: 0.00001065
Iteration 201/1000 | Loss: 0.00001065
Iteration 202/1000 | Loss: 0.00001065
Iteration 203/1000 | Loss: 0.00001065
Iteration 204/1000 | Loss: 0.00001065
Iteration 205/1000 | Loss: 0.00001065
Iteration 206/1000 | Loss: 0.00001065
Iteration 207/1000 | Loss: 0.00001065
Iteration 208/1000 | Loss: 0.00001065
Iteration 209/1000 | Loss: 0.00001065
Iteration 210/1000 | Loss: 0.00001065
Iteration 211/1000 | Loss: 0.00001065
Iteration 212/1000 | Loss: 0.00001065
Iteration 213/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.0648672287061345e-05, 1.0648672287061345e-05, 1.0648672287061345e-05, 1.0648672287061345e-05, 1.0648672287061345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0648672287061345e-05

Optimization complete. Final v2v error: 2.7963855266571045 mm

Highest mean error: 2.96563458442688 mm for frame 48

Lowest mean error: 2.6603448390960693 mm for frame 124

Saving results

Total time: 41.757018089294434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406141
Iteration 2/25 | Loss: 0.00139382
Iteration 3/25 | Loss: 0.00128944
Iteration 4/25 | Loss: 0.00127640
Iteration 5/25 | Loss: 0.00127372
Iteration 6/25 | Loss: 0.00127319
Iteration 7/25 | Loss: 0.00127319
Iteration 8/25 | Loss: 0.00127319
Iteration 9/25 | Loss: 0.00127319
Iteration 10/25 | Loss: 0.00127319
Iteration 11/25 | Loss: 0.00127318
Iteration 12/25 | Loss: 0.00127318
Iteration 13/25 | Loss: 0.00127318
Iteration 14/25 | Loss: 0.00127318
Iteration 15/25 | Loss: 0.00127318
Iteration 16/25 | Loss: 0.00127318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001273175235837698, 0.001273175235837698, 0.001273175235837698, 0.001273175235837698, 0.001273175235837698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273175235837698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31954348
Iteration 2/25 | Loss: 0.00150932
Iteration 3/25 | Loss: 0.00150932
Iteration 4/25 | Loss: 0.00150932
Iteration 5/25 | Loss: 0.00150932
Iteration 6/25 | Loss: 0.00150932
Iteration 7/25 | Loss: 0.00150932
Iteration 8/25 | Loss: 0.00150931
Iteration 9/25 | Loss: 0.00150931
Iteration 10/25 | Loss: 0.00150931
Iteration 11/25 | Loss: 0.00150931
Iteration 12/25 | Loss: 0.00150931
Iteration 13/25 | Loss: 0.00150931
Iteration 14/25 | Loss: 0.00150931
Iteration 15/25 | Loss: 0.00150931
Iteration 16/25 | Loss: 0.00150931
Iteration 17/25 | Loss: 0.00150931
Iteration 18/25 | Loss: 0.00150931
Iteration 19/25 | Loss: 0.00150931
Iteration 20/25 | Loss: 0.00150931
Iteration 21/25 | Loss: 0.00150931
Iteration 22/25 | Loss: 0.00150931
Iteration 23/25 | Loss: 0.00150931
Iteration 24/25 | Loss: 0.00150931
Iteration 25/25 | Loss: 0.00150931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150931
Iteration 2/1000 | Loss: 0.00002846
Iteration 3/1000 | Loss: 0.00001867
Iteration 4/1000 | Loss: 0.00001529
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001197
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001093
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001067
Iteration 24/1000 | Loss: 0.00001067
Iteration 25/1000 | Loss: 0.00001066
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001058
Iteration 30/1000 | Loss: 0.00001055
Iteration 31/1000 | Loss: 0.00001054
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001053
Iteration 34/1000 | Loss: 0.00001053
Iteration 35/1000 | Loss: 0.00001049
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001036
Iteration 52/1000 | Loss: 0.00001036
Iteration 53/1000 | Loss: 0.00001035
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001028
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001026
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001025
Iteration 72/1000 | Loss: 0.00001024
Iteration 73/1000 | Loss: 0.00001024
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001023
Iteration 79/1000 | Loss: 0.00001023
Iteration 80/1000 | Loss: 0.00001023
Iteration 81/1000 | Loss: 0.00001023
Iteration 82/1000 | Loss: 0.00001023
Iteration 83/1000 | Loss: 0.00001023
Iteration 84/1000 | Loss: 0.00001023
Iteration 85/1000 | Loss: 0.00001022
Iteration 86/1000 | Loss: 0.00001022
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001021
Iteration 91/1000 | Loss: 0.00001021
Iteration 92/1000 | Loss: 0.00001021
Iteration 93/1000 | Loss: 0.00001021
Iteration 94/1000 | Loss: 0.00001021
Iteration 95/1000 | Loss: 0.00001021
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001019
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001018
Iteration 108/1000 | Loss: 0.00001018
Iteration 109/1000 | Loss: 0.00001018
Iteration 110/1000 | Loss: 0.00001017
Iteration 111/1000 | Loss: 0.00001017
Iteration 112/1000 | Loss: 0.00001017
Iteration 113/1000 | Loss: 0.00001017
Iteration 114/1000 | Loss: 0.00001016
Iteration 115/1000 | Loss: 0.00001016
Iteration 116/1000 | Loss: 0.00001016
Iteration 117/1000 | Loss: 0.00001016
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001015
Iteration 121/1000 | Loss: 0.00001015
Iteration 122/1000 | Loss: 0.00001015
Iteration 123/1000 | Loss: 0.00001015
Iteration 124/1000 | Loss: 0.00001014
Iteration 125/1000 | Loss: 0.00001014
Iteration 126/1000 | Loss: 0.00001014
Iteration 127/1000 | Loss: 0.00001014
Iteration 128/1000 | Loss: 0.00001014
Iteration 129/1000 | Loss: 0.00001014
Iteration 130/1000 | Loss: 0.00001014
Iteration 131/1000 | Loss: 0.00001014
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001012
Iteration 137/1000 | Loss: 0.00001012
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001011
Iteration 140/1000 | Loss: 0.00001011
Iteration 141/1000 | Loss: 0.00001011
Iteration 142/1000 | Loss: 0.00001011
Iteration 143/1000 | Loss: 0.00001011
Iteration 144/1000 | Loss: 0.00001010
Iteration 145/1000 | Loss: 0.00001010
Iteration 146/1000 | Loss: 0.00001010
Iteration 147/1000 | Loss: 0.00001010
Iteration 148/1000 | Loss: 0.00001010
Iteration 149/1000 | Loss: 0.00001010
Iteration 150/1000 | Loss: 0.00001008
Iteration 151/1000 | Loss: 0.00001008
Iteration 152/1000 | Loss: 0.00001008
Iteration 153/1000 | Loss: 0.00001008
Iteration 154/1000 | Loss: 0.00001007
Iteration 155/1000 | Loss: 0.00001007
Iteration 156/1000 | Loss: 0.00001007
Iteration 157/1000 | Loss: 0.00001007
Iteration 158/1000 | Loss: 0.00001006
Iteration 159/1000 | Loss: 0.00001006
Iteration 160/1000 | Loss: 0.00001006
Iteration 161/1000 | Loss: 0.00001006
Iteration 162/1000 | Loss: 0.00001005
Iteration 163/1000 | Loss: 0.00001005
Iteration 164/1000 | Loss: 0.00001005
Iteration 165/1000 | Loss: 0.00001005
Iteration 166/1000 | Loss: 0.00001005
Iteration 167/1000 | Loss: 0.00001005
Iteration 168/1000 | Loss: 0.00001005
Iteration 169/1000 | Loss: 0.00001004
Iteration 170/1000 | Loss: 0.00001004
Iteration 171/1000 | Loss: 0.00001004
Iteration 172/1000 | Loss: 0.00001004
Iteration 173/1000 | Loss: 0.00001004
Iteration 174/1000 | Loss: 0.00001004
Iteration 175/1000 | Loss: 0.00001004
Iteration 176/1000 | Loss: 0.00001004
Iteration 177/1000 | Loss: 0.00001004
Iteration 178/1000 | Loss: 0.00001004
Iteration 179/1000 | Loss: 0.00001004
Iteration 180/1000 | Loss: 0.00001003
Iteration 181/1000 | Loss: 0.00001003
Iteration 182/1000 | Loss: 0.00001003
Iteration 183/1000 | Loss: 0.00001003
Iteration 184/1000 | Loss: 0.00001003
Iteration 185/1000 | Loss: 0.00001003
Iteration 186/1000 | Loss: 0.00001003
Iteration 187/1000 | Loss: 0.00001003
Iteration 188/1000 | Loss: 0.00001003
Iteration 189/1000 | Loss: 0.00001003
Iteration 190/1000 | Loss: 0.00001002
Iteration 191/1000 | Loss: 0.00001002
Iteration 192/1000 | Loss: 0.00001002
Iteration 193/1000 | Loss: 0.00001002
Iteration 194/1000 | Loss: 0.00001002
Iteration 195/1000 | Loss: 0.00001002
Iteration 196/1000 | Loss: 0.00001002
Iteration 197/1000 | Loss: 0.00001002
Iteration 198/1000 | Loss: 0.00001002
Iteration 199/1000 | Loss: 0.00001002
Iteration 200/1000 | Loss: 0.00001002
Iteration 201/1000 | Loss: 0.00001002
Iteration 202/1000 | Loss: 0.00001002
Iteration 203/1000 | Loss: 0.00001002
Iteration 204/1000 | Loss: 0.00001002
Iteration 205/1000 | Loss: 0.00001002
Iteration 206/1000 | Loss: 0.00001002
Iteration 207/1000 | Loss: 0.00001002
Iteration 208/1000 | Loss: 0.00001002
Iteration 209/1000 | Loss: 0.00001002
Iteration 210/1000 | Loss: 0.00001002
Iteration 211/1000 | Loss: 0.00001002
Iteration 212/1000 | Loss: 0.00001002
Iteration 213/1000 | Loss: 0.00001002
Iteration 214/1000 | Loss: 0.00001002
Iteration 215/1000 | Loss: 0.00001002
Iteration 216/1000 | Loss: 0.00001002
Iteration 217/1000 | Loss: 0.00001002
Iteration 218/1000 | Loss: 0.00001002
Iteration 219/1000 | Loss: 0.00001002
Iteration 220/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.0019478395406622e-05, 1.0019478395406622e-05, 1.0019478395406622e-05, 1.0019478395406622e-05, 1.0019478395406622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0019478395406622e-05

Optimization complete. Final v2v error: 2.713322401046753 mm

Highest mean error: 3.5505197048187256 mm for frame 66

Lowest mean error: 2.530663251876831 mm for frame 107

Saving results

Total time: 45.658050537109375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952988
Iteration 2/25 | Loss: 0.00193344
Iteration 3/25 | Loss: 0.00152428
Iteration 4/25 | Loss: 0.00151045
Iteration 5/25 | Loss: 0.00150504
Iteration 6/25 | Loss: 0.00150489
Iteration 7/25 | Loss: 0.00150489
Iteration 8/25 | Loss: 0.00150489
Iteration 9/25 | Loss: 0.00150489
Iteration 10/25 | Loss: 0.00150489
Iteration 11/25 | Loss: 0.00150489
Iteration 12/25 | Loss: 0.00150489
Iteration 13/25 | Loss: 0.00150489
Iteration 14/25 | Loss: 0.00150489
Iteration 15/25 | Loss: 0.00150489
Iteration 16/25 | Loss: 0.00150489
Iteration 17/25 | Loss: 0.00150489
Iteration 18/25 | Loss: 0.00150489
Iteration 19/25 | Loss: 0.00150489
Iteration 20/25 | Loss: 0.00150489
Iteration 21/25 | Loss: 0.00150489
Iteration 22/25 | Loss: 0.00150489
Iteration 23/25 | Loss: 0.00150489
Iteration 24/25 | Loss: 0.00150489
Iteration 25/25 | Loss: 0.00150489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76535505
Iteration 2/25 | Loss: 0.00165054
Iteration 3/25 | Loss: 0.00165054
Iteration 4/25 | Loss: 0.00165054
Iteration 5/25 | Loss: 0.00165054
Iteration 6/25 | Loss: 0.00165054
Iteration 7/25 | Loss: 0.00165054
Iteration 8/25 | Loss: 0.00165054
Iteration 9/25 | Loss: 0.00165054
Iteration 10/25 | Loss: 0.00165054
Iteration 11/25 | Loss: 0.00165054
Iteration 12/25 | Loss: 0.00165054
Iteration 13/25 | Loss: 0.00165054
Iteration 14/25 | Loss: 0.00165054
Iteration 15/25 | Loss: 0.00165054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016505399253219366, 0.0016505399253219366, 0.0016505399253219366, 0.0016505399253219366, 0.0016505399253219366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016505399253219366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165054
Iteration 2/1000 | Loss: 0.00007450
Iteration 3/1000 | Loss: 0.00004735
Iteration 4/1000 | Loss: 0.00004004
Iteration 5/1000 | Loss: 0.00003756
Iteration 6/1000 | Loss: 0.00003598
Iteration 7/1000 | Loss: 0.00003523
Iteration 8/1000 | Loss: 0.00003461
Iteration 9/1000 | Loss: 0.00003392
Iteration 10/1000 | Loss: 0.00003342
Iteration 11/1000 | Loss: 0.00003299
Iteration 12/1000 | Loss: 0.00003262
Iteration 13/1000 | Loss: 0.00003236
Iteration 14/1000 | Loss: 0.00003196
Iteration 15/1000 | Loss: 0.00003169
Iteration 16/1000 | Loss: 0.00003142
Iteration 17/1000 | Loss: 0.00003116
Iteration 18/1000 | Loss: 0.00003095
Iteration 19/1000 | Loss: 0.00003083
Iteration 20/1000 | Loss: 0.00003070
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00003058
Iteration 23/1000 | Loss: 0.00003051
Iteration 24/1000 | Loss: 0.00003050
Iteration 25/1000 | Loss: 0.00003046
Iteration 26/1000 | Loss: 0.00003046
Iteration 27/1000 | Loss: 0.00003044
Iteration 28/1000 | Loss: 0.00003044
Iteration 29/1000 | Loss: 0.00003043
Iteration 30/1000 | Loss: 0.00003043
Iteration 31/1000 | Loss: 0.00003042
Iteration 32/1000 | Loss: 0.00003042
Iteration 33/1000 | Loss: 0.00003042
Iteration 34/1000 | Loss: 0.00003041
Iteration 35/1000 | Loss: 0.00003041
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00003041
Iteration 38/1000 | Loss: 0.00003040
Iteration 39/1000 | Loss: 0.00003040
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00003039
Iteration 42/1000 | Loss: 0.00003039
Iteration 43/1000 | Loss: 0.00003038
Iteration 44/1000 | Loss: 0.00003038
Iteration 45/1000 | Loss: 0.00003038
Iteration 46/1000 | Loss: 0.00003038
Iteration 47/1000 | Loss: 0.00003038
Iteration 48/1000 | Loss: 0.00003038
Iteration 49/1000 | Loss: 0.00003038
Iteration 50/1000 | Loss: 0.00003037
Iteration 51/1000 | Loss: 0.00003037
Iteration 52/1000 | Loss: 0.00003037
Iteration 53/1000 | Loss: 0.00003036
Iteration 54/1000 | Loss: 0.00003036
Iteration 55/1000 | Loss: 0.00003036
Iteration 56/1000 | Loss: 0.00003036
Iteration 57/1000 | Loss: 0.00003036
Iteration 58/1000 | Loss: 0.00003036
Iteration 59/1000 | Loss: 0.00003036
Iteration 60/1000 | Loss: 0.00003036
Iteration 61/1000 | Loss: 0.00003035
Iteration 62/1000 | Loss: 0.00003035
Iteration 63/1000 | Loss: 0.00003035
Iteration 64/1000 | Loss: 0.00003034
Iteration 65/1000 | Loss: 0.00003034
Iteration 66/1000 | Loss: 0.00003034
Iteration 67/1000 | Loss: 0.00003033
Iteration 68/1000 | Loss: 0.00003033
Iteration 69/1000 | Loss: 0.00003033
Iteration 70/1000 | Loss: 0.00003032
Iteration 71/1000 | Loss: 0.00003032
Iteration 72/1000 | Loss: 0.00003031
Iteration 73/1000 | Loss: 0.00003031
Iteration 74/1000 | Loss: 0.00003030
Iteration 75/1000 | Loss: 0.00003030
Iteration 76/1000 | Loss: 0.00003030
Iteration 77/1000 | Loss: 0.00003030
Iteration 78/1000 | Loss: 0.00003030
Iteration 79/1000 | Loss: 0.00003029
Iteration 80/1000 | Loss: 0.00003029
Iteration 81/1000 | Loss: 0.00003029
Iteration 82/1000 | Loss: 0.00003029
Iteration 83/1000 | Loss: 0.00003028
Iteration 84/1000 | Loss: 0.00003028
Iteration 85/1000 | Loss: 0.00003028
Iteration 86/1000 | Loss: 0.00003028
Iteration 87/1000 | Loss: 0.00003028
Iteration 88/1000 | Loss: 0.00003028
Iteration 89/1000 | Loss: 0.00003028
Iteration 90/1000 | Loss: 0.00003027
Iteration 91/1000 | Loss: 0.00003027
Iteration 92/1000 | Loss: 0.00003027
Iteration 93/1000 | Loss: 0.00003027
Iteration 94/1000 | Loss: 0.00003026
Iteration 95/1000 | Loss: 0.00003026
Iteration 96/1000 | Loss: 0.00003026
Iteration 97/1000 | Loss: 0.00003026
Iteration 98/1000 | Loss: 0.00003026
Iteration 99/1000 | Loss: 0.00003025
Iteration 100/1000 | Loss: 0.00003025
Iteration 101/1000 | Loss: 0.00003025
Iteration 102/1000 | Loss: 0.00003025
Iteration 103/1000 | Loss: 0.00003024
Iteration 104/1000 | Loss: 0.00003024
Iteration 105/1000 | Loss: 0.00003024
Iteration 106/1000 | Loss: 0.00003024
Iteration 107/1000 | Loss: 0.00003024
Iteration 108/1000 | Loss: 0.00003024
Iteration 109/1000 | Loss: 0.00003023
Iteration 110/1000 | Loss: 0.00003023
Iteration 111/1000 | Loss: 0.00003023
Iteration 112/1000 | Loss: 0.00003023
Iteration 113/1000 | Loss: 0.00003023
Iteration 114/1000 | Loss: 0.00003023
Iteration 115/1000 | Loss: 0.00003023
Iteration 116/1000 | Loss: 0.00003023
Iteration 117/1000 | Loss: 0.00003022
Iteration 118/1000 | Loss: 0.00003022
Iteration 119/1000 | Loss: 0.00003022
Iteration 120/1000 | Loss: 0.00003022
Iteration 121/1000 | Loss: 0.00003022
Iteration 122/1000 | Loss: 0.00003022
Iteration 123/1000 | Loss: 0.00003022
Iteration 124/1000 | Loss: 0.00003022
Iteration 125/1000 | Loss: 0.00003022
Iteration 126/1000 | Loss: 0.00003022
Iteration 127/1000 | Loss: 0.00003021
Iteration 128/1000 | Loss: 0.00003021
Iteration 129/1000 | Loss: 0.00003021
Iteration 130/1000 | Loss: 0.00003021
Iteration 131/1000 | Loss: 0.00003021
Iteration 132/1000 | Loss: 0.00003021
Iteration 133/1000 | Loss: 0.00003021
Iteration 134/1000 | Loss: 0.00003021
Iteration 135/1000 | Loss: 0.00003021
Iteration 136/1000 | Loss: 0.00003021
Iteration 137/1000 | Loss: 0.00003020
Iteration 138/1000 | Loss: 0.00003020
Iteration 139/1000 | Loss: 0.00003020
Iteration 140/1000 | Loss: 0.00003020
Iteration 141/1000 | Loss: 0.00003020
Iteration 142/1000 | Loss: 0.00003020
Iteration 143/1000 | Loss: 0.00003020
Iteration 144/1000 | Loss: 0.00003020
Iteration 145/1000 | Loss: 0.00003020
Iteration 146/1000 | Loss: 0.00003020
Iteration 147/1000 | Loss: 0.00003020
Iteration 148/1000 | Loss: 0.00003020
Iteration 149/1000 | Loss: 0.00003020
Iteration 150/1000 | Loss: 0.00003020
Iteration 151/1000 | Loss: 0.00003020
Iteration 152/1000 | Loss: 0.00003019
Iteration 153/1000 | Loss: 0.00003019
Iteration 154/1000 | Loss: 0.00003019
Iteration 155/1000 | Loss: 0.00003019
Iteration 156/1000 | Loss: 0.00003019
Iteration 157/1000 | Loss: 0.00003019
Iteration 158/1000 | Loss: 0.00003019
Iteration 159/1000 | Loss: 0.00003019
Iteration 160/1000 | Loss: 0.00003019
Iteration 161/1000 | Loss: 0.00003019
Iteration 162/1000 | Loss: 0.00003019
Iteration 163/1000 | Loss: 0.00003019
Iteration 164/1000 | Loss: 0.00003019
Iteration 165/1000 | Loss: 0.00003018
Iteration 166/1000 | Loss: 0.00003018
Iteration 167/1000 | Loss: 0.00003018
Iteration 168/1000 | Loss: 0.00003018
Iteration 169/1000 | Loss: 0.00003018
Iteration 170/1000 | Loss: 0.00003018
Iteration 171/1000 | Loss: 0.00003018
Iteration 172/1000 | Loss: 0.00003018
Iteration 173/1000 | Loss: 0.00003018
Iteration 174/1000 | Loss: 0.00003018
Iteration 175/1000 | Loss: 0.00003018
Iteration 176/1000 | Loss: 0.00003018
Iteration 177/1000 | Loss: 0.00003018
Iteration 178/1000 | Loss: 0.00003018
Iteration 179/1000 | Loss: 0.00003018
Iteration 180/1000 | Loss: 0.00003018
Iteration 181/1000 | Loss: 0.00003018
Iteration 182/1000 | Loss: 0.00003018
Iteration 183/1000 | Loss: 0.00003018
Iteration 184/1000 | Loss: 0.00003018
Iteration 185/1000 | Loss: 0.00003018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.0184306524461135e-05, 3.0184306524461135e-05, 3.0184306524461135e-05, 3.0184306524461135e-05, 3.0184306524461135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0184306524461135e-05

Optimization complete. Final v2v error: 4.477823734283447 mm

Highest mean error: 5.620312690734863 mm for frame 105

Lowest mean error: 3.495806932449341 mm for frame 32

Saving results

Total time: 51.92356491088867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792907
Iteration 2/25 | Loss: 0.00151081
Iteration 3/25 | Loss: 0.00132259
Iteration 4/25 | Loss: 0.00130221
Iteration 5/25 | Loss: 0.00129611
Iteration 6/25 | Loss: 0.00129494
Iteration 7/25 | Loss: 0.00129494
Iteration 8/25 | Loss: 0.00129494
Iteration 9/25 | Loss: 0.00129494
Iteration 10/25 | Loss: 0.00129494
Iteration 11/25 | Loss: 0.00129494
Iteration 12/25 | Loss: 0.00129494
Iteration 13/25 | Loss: 0.00129494
Iteration 14/25 | Loss: 0.00129494
Iteration 15/25 | Loss: 0.00129494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012949401279911399, 0.0012949401279911399, 0.0012949401279911399, 0.0012949401279911399, 0.0012949401279911399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012949401279911399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32891357
Iteration 2/25 | Loss: 0.00170495
Iteration 3/25 | Loss: 0.00170495
Iteration 4/25 | Loss: 0.00170495
Iteration 5/25 | Loss: 0.00170495
Iteration 6/25 | Loss: 0.00170495
Iteration 7/25 | Loss: 0.00170495
Iteration 8/25 | Loss: 0.00170495
Iteration 9/25 | Loss: 0.00170495
Iteration 10/25 | Loss: 0.00170495
Iteration 11/25 | Loss: 0.00170495
Iteration 12/25 | Loss: 0.00170495
Iteration 13/25 | Loss: 0.00170495
Iteration 14/25 | Loss: 0.00170495
Iteration 15/25 | Loss: 0.00170495
Iteration 16/25 | Loss: 0.00170495
Iteration 17/25 | Loss: 0.00170495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017049480229616165, 0.0017049480229616165, 0.0017049480229616165, 0.0017049480229616165, 0.0017049480229616165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017049480229616165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170495
Iteration 2/1000 | Loss: 0.00004546
Iteration 3/1000 | Loss: 0.00003063
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001881
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001469
Iteration 20/1000 | Loss: 0.00001468
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001455
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001452
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001446
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00001441
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001431
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001428
Iteration 134/1000 | Loss: 0.00001428
Iteration 135/1000 | Loss: 0.00001428
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001427
Iteration 141/1000 | Loss: 0.00001427
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001425
Iteration 145/1000 | Loss: 0.00001425
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001424
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001420
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001416
Iteration 181/1000 | Loss: 0.00001416
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001415
Iteration 191/1000 | Loss: 0.00001415
Iteration 192/1000 | Loss: 0.00001414
Iteration 193/1000 | Loss: 0.00001414
Iteration 194/1000 | Loss: 0.00001414
Iteration 195/1000 | Loss: 0.00001413
Iteration 196/1000 | Loss: 0.00001413
Iteration 197/1000 | Loss: 0.00001413
Iteration 198/1000 | Loss: 0.00001412
Iteration 199/1000 | Loss: 0.00001412
Iteration 200/1000 | Loss: 0.00001412
Iteration 201/1000 | Loss: 0.00001411
Iteration 202/1000 | Loss: 0.00001411
Iteration 203/1000 | Loss: 0.00001411
Iteration 204/1000 | Loss: 0.00001411
Iteration 205/1000 | Loss: 0.00001411
Iteration 206/1000 | Loss: 0.00001411
Iteration 207/1000 | Loss: 0.00001411
Iteration 208/1000 | Loss: 0.00001411
Iteration 209/1000 | Loss: 0.00001411
Iteration 210/1000 | Loss: 0.00001410
Iteration 211/1000 | Loss: 0.00001410
Iteration 212/1000 | Loss: 0.00001409
Iteration 213/1000 | Loss: 0.00001409
Iteration 214/1000 | Loss: 0.00001409
Iteration 215/1000 | Loss: 0.00001409
Iteration 216/1000 | Loss: 0.00001408
Iteration 217/1000 | Loss: 0.00001408
Iteration 218/1000 | Loss: 0.00001408
Iteration 219/1000 | Loss: 0.00001408
Iteration 220/1000 | Loss: 0.00001408
Iteration 221/1000 | Loss: 0.00001408
Iteration 222/1000 | Loss: 0.00001408
Iteration 223/1000 | Loss: 0.00001408
Iteration 224/1000 | Loss: 0.00001408
Iteration 225/1000 | Loss: 0.00001408
Iteration 226/1000 | Loss: 0.00001408
Iteration 227/1000 | Loss: 0.00001407
Iteration 228/1000 | Loss: 0.00001407
Iteration 229/1000 | Loss: 0.00001407
Iteration 230/1000 | Loss: 0.00001407
Iteration 231/1000 | Loss: 0.00001407
Iteration 232/1000 | Loss: 0.00001406
Iteration 233/1000 | Loss: 0.00001406
Iteration 234/1000 | Loss: 0.00001406
Iteration 235/1000 | Loss: 0.00001406
Iteration 236/1000 | Loss: 0.00001406
Iteration 237/1000 | Loss: 0.00001406
Iteration 238/1000 | Loss: 0.00001405
Iteration 239/1000 | Loss: 0.00001405
Iteration 240/1000 | Loss: 0.00001405
Iteration 241/1000 | Loss: 0.00001405
Iteration 242/1000 | Loss: 0.00001405
Iteration 243/1000 | Loss: 0.00001405
Iteration 244/1000 | Loss: 0.00001405
Iteration 245/1000 | Loss: 0.00001404
Iteration 246/1000 | Loss: 0.00001404
Iteration 247/1000 | Loss: 0.00001404
Iteration 248/1000 | Loss: 0.00001404
Iteration 249/1000 | Loss: 0.00001404
Iteration 250/1000 | Loss: 0.00001404
Iteration 251/1000 | Loss: 0.00001404
Iteration 252/1000 | Loss: 0.00001404
Iteration 253/1000 | Loss: 0.00001404
Iteration 254/1000 | Loss: 0.00001403
Iteration 255/1000 | Loss: 0.00001403
Iteration 256/1000 | Loss: 0.00001403
Iteration 257/1000 | Loss: 0.00001403
Iteration 258/1000 | Loss: 0.00001403
Iteration 259/1000 | Loss: 0.00001403
Iteration 260/1000 | Loss: 0.00001403
Iteration 261/1000 | Loss: 0.00001403
Iteration 262/1000 | Loss: 0.00001403
Iteration 263/1000 | Loss: 0.00001403
Iteration 264/1000 | Loss: 0.00001403
Iteration 265/1000 | Loss: 0.00001403
Iteration 266/1000 | Loss: 0.00001403
Iteration 267/1000 | Loss: 0.00001402
Iteration 268/1000 | Loss: 0.00001402
Iteration 269/1000 | Loss: 0.00001402
Iteration 270/1000 | Loss: 0.00001402
Iteration 271/1000 | Loss: 0.00001401
Iteration 272/1000 | Loss: 0.00001401
Iteration 273/1000 | Loss: 0.00001401
Iteration 274/1000 | Loss: 0.00001401
Iteration 275/1000 | Loss: 0.00001400
Iteration 276/1000 | Loss: 0.00001400
Iteration 277/1000 | Loss: 0.00001400
Iteration 278/1000 | Loss: 0.00001400
Iteration 279/1000 | Loss: 0.00001400
Iteration 280/1000 | Loss: 0.00001400
Iteration 281/1000 | Loss: 0.00001400
Iteration 282/1000 | Loss: 0.00001400
Iteration 283/1000 | Loss: 0.00001400
Iteration 284/1000 | Loss: 0.00001399
Iteration 285/1000 | Loss: 0.00001399
Iteration 286/1000 | Loss: 0.00001399
Iteration 287/1000 | Loss: 0.00001399
Iteration 288/1000 | Loss: 0.00001398
Iteration 289/1000 | Loss: 0.00001398
Iteration 290/1000 | Loss: 0.00001398
Iteration 291/1000 | Loss: 0.00001398
Iteration 292/1000 | Loss: 0.00001398
Iteration 293/1000 | Loss: 0.00001398
Iteration 294/1000 | Loss: 0.00001398
Iteration 295/1000 | Loss: 0.00001398
Iteration 296/1000 | Loss: 0.00001398
Iteration 297/1000 | Loss: 0.00001397
Iteration 298/1000 | Loss: 0.00001397
Iteration 299/1000 | Loss: 0.00001397
Iteration 300/1000 | Loss: 0.00001397
Iteration 301/1000 | Loss: 0.00001397
Iteration 302/1000 | Loss: 0.00001397
Iteration 303/1000 | Loss: 0.00001397
Iteration 304/1000 | Loss: 0.00001397
Iteration 305/1000 | Loss: 0.00001397
Iteration 306/1000 | Loss: 0.00001396
Iteration 307/1000 | Loss: 0.00001396
Iteration 308/1000 | Loss: 0.00001396
Iteration 309/1000 | Loss: 0.00001396
Iteration 310/1000 | Loss: 0.00001396
Iteration 311/1000 | Loss: 0.00001396
Iteration 312/1000 | Loss: 0.00001396
Iteration 313/1000 | Loss: 0.00001396
Iteration 314/1000 | Loss: 0.00001395
Iteration 315/1000 | Loss: 0.00001395
Iteration 316/1000 | Loss: 0.00001395
Iteration 317/1000 | Loss: 0.00001395
Iteration 318/1000 | Loss: 0.00001395
Iteration 319/1000 | Loss: 0.00001395
Iteration 320/1000 | Loss: 0.00001395
Iteration 321/1000 | Loss: 0.00001395
Iteration 322/1000 | Loss: 0.00001394
Iteration 323/1000 | Loss: 0.00001394
Iteration 324/1000 | Loss: 0.00001394
Iteration 325/1000 | Loss: 0.00001394
Iteration 326/1000 | Loss: 0.00001394
Iteration 327/1000 | Loss: 0.00001394
Iteration 328/1000 | Loss: 0.00001394
Iteration 329/1000 | Loss: 0.00001394
Iteration 330/1000 | Loss: 0.00001394
Iteration 331/1000 | Loss: 0.00001394
Iteration 332/1000 | Loss: 0.00001394
Iteration 333/1000 | Loss: 0.00001394
Iteration 334/1000 | Loss: 0.00001394
Iteration 335/1000 | Loss: 0.00001394
Iteration 336/1000 | Loss: 0.00001393
Iteration 337/1000 | Loss: 0.00001393
Iteration 338/1000 | Loss: 0.00001393
Iteration 339/1000 | Loss: 0.00001393
Iteration 340/1000 | Loss: 0.00001393
Iteration 341/1000 | Loss: 0.00001393
Iteration 342/1000 | Loss: 0.00001393
Iteration 343/1000 | Loss: 0.00001393
Iteration 344/1000 | Loss: 0.00001393
Iteration 345/1000 | Loss: 0.00001393
Iteration 346/1000 | Loss: 0.00001393
Iteration 347/1000 | Loss: 0.00001393
Iteration 348/1000 | Loss: 0.00001393
Iteration 349/1000 | Loss: 0.00001393
Iteration 350/1000 | Loss: 0.00001393
Iteration 351/1000 | Loss: 0.00001393
Iteration 352/1000 | Loss: 0.00001393
Iteration 353/1000 | Loss: 0.00001393
Iteration 354/1000 | Loss: 0.00001393
Iteration 355/1000 | Loss: 0.00001393
Iteration 356/1000 | Loss: 0.00001393
Iteration 357/1000 | Loss: 0.00001393
Iteration 358/1000 | Loss: 0.00001393
Iteration 359/1000 | Loss: 0.00001393
Iteration 360/1000 | Loss: 0.00001393
Iteration 361/1000 | Loss: 0.00001393
Iteration 362/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.392941067024367e-05, 1.392941067024367e-05, 1.392941067024367e-05, 1.392941067024367e-05, 1.392941067024367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.392941067024367e-05

Optimization complete. Final v2v error: 3.190147638320923 mm

Highest mean error: 3.6384758949279785 mm for frame 80

Lowest mean error: 2.827406167984009 mm for frame 154

Saving results

Total time: 57.04640984535217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484307
Iteration 2/25 | Loss: 0.00147894
Iteration 3/25 | Loss: 0.00139013
Iteration 4/25 | Loss: 0.00137599
Iteration 5/25 | Loss: 0.00137103
Iteration 6/25 | Loss: 0.00137091
Iteration 7/25 | Loss: 0.00137091
Iteration 8/25 | Loss: 0.00137091
Iteration 9/25 | Loss: 0.00137091
Iteration 10/25 | Loss: 0.00137091
Iteration 11/25 | Loss: 0.00137091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001370911137200892, 0.001370911137200892, 0.001370911137200892, 0.001370911137200892, 0.001370911137200892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001370911137200892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32640600
Iteration 2/25 | Loss: 0.00150077
Iteration 3/25 | Loss: 0.00150075
Iteration 4/25 | Loss: 0.00150075
Iteration 5/25 | Loss: 0.00150075
Iteration 6/25 | Loss: 0.00150075
Iteration 7/25 | Loss: 0.00150075
Iteration 8/25 | Loss: 0.00150075
Iteration 9/25 | Loss: 0.00150075
Iteration 10/25 | Loss: 0.00150075
Iteration 11/25 | Loss: 0.00150075
Iteration 12/25 | Loss: 0.00150075
Iteration 13/25 | Loss: 0.00150075
Iteration 14/25 | Loss: 0.00150075
Iteration 15/25 | Loss: 0.00150075
Iteration 16/25 | Loss: 0.00150075
Iteration 17/25 | Loss: 0.00150075
Iteration 18/25 | Loss: 0.00150075
Iteration 19/25 | Loss: 0.00150075
Iteration 20/25 | Loss: 0.00150075
Iteration 21/25 | Loss: 0.00150075
Iteration 22/25 | Loss: 0.00150075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015007468173280358, 0.0015007468173280358, 0.0015007468173280358, 0.0015007468173280358, 0.0015007468173280358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015007468173280358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150075
Iteration 2/1000 | Loss: 0.00004453
Iteration 3/1000 | Loss: 0.00002972
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002345
Iteration 6/1000 | Loss: 0.00002210
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002020
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001943
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001911
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00001904
Iteration 17/1000 | Loss: 0.00001903
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001892
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001852
Iteration 37/1000 | Loss: 0.00001852
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00001851
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00001850
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001848
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001841
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001839
Iteration 64/1000 | Loss: 0.00001839
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001838
Iteration 67/1000 | Loss: 0.00001838
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001837
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001826
Iteration 93/1000 | Loss: 0.00001826
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001826
Iteration 96/1000 | Loss: 0.00001826
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001824
Iteration 102/1000 | Loss: 0.00001823
Iteration 103/1000 | Loss: 0.00001823
Iteration 104/1000 | Loss: 0.00001822
Iteration 105/1000 | Loss: 0.00001822
Iteration 106/1000 | Loss: 0.00001822
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001817
Iteration 127/1000 | Loss: 0.00001817
Iteration 128/1000 | Loss: 0.00001817
Iteration 129/1000 | Loss: 0.00001817
Iteration 130/1000 | Loss: 0.00001817
Iteration 131/1000 | Loss: 0.00001817
Iteration 132/1000 | Loss: 0.00001817
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001817
Iteration 136/1000 | Loss: 0.00001817
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001817
Iteration 139/1000 | Loss: 0.00001817
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001817
Iteration 142/1000 | Loss: 0.00001817
Iteration 143/1000 | Loss: 0.00001817
Iteration 144/1000 | Loss: 0.00001817
Iteration 145/1000 | Loss: 0.00001817
Iteration 146/1000 | Loss: 0.00001817
Iteration 147/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.816691474232357e-05, 1.816691474232357e-05, 1.816691474232357e-05, 1.816691474232357e-05, 1.816691474232357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.816691474232357e-05

Optimization complete. Final v2v error: 3.520176649093628 mm

Highest mean error: 5.117435455322266 mm for frame 215

Lowest mean error: 2.9203548431396484 mm for frame 42

Saving results

Total time: 48.05886101722717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986402
Iteration 2/25 | Loss: 0.00217544
Iteration 3/25 | Loss: 0.00176271
Iteration 4/25 | Loss: 0.00164764
Iteration 5/25 | Loss: 0.00178502
Iteration 6/25 | Loss: 0.00168695
Iteration 7/25 | Loss: 0.00152719
Iteration 8/25 | Loss: 0.00146463
Iteration 9/25 | Loss: 0.00141775
Iteration 10/25 | Loss: 0.00138652
Iteration 11/25 | Loss: 0.00136527
Iteration 12/25 | Loss: 0.00135534
Iteration 13/25 | Loss: 0.00134700
Iteration 14/25 | Loss: 0.00134392
Iteration 15/25 | Loss: 0.00134149
Iteration 16/25 | Loss: 0.00133960
Iteration 17/25 | Loss: 0.00133470
Iteration 18/25 | Loss: 0.00133315
Iteration 19/25 | Loss: 0.00133276
Iteration 20/25 | Loss: 0.00133154
Iteration 21/25 | Loss: 0.00133004
Iteration 22/25 | Loss: 0.00132949
Iteration 23/25 | Loss: 0.00132930
Iteration 24/25 | Loss: 0.00132918
Iteration 25/25 | Loss: 0.00132814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36469340
Iteration 2/25 | Loss: 0.00110737
Iteration 3/25 | Loss: 0.00110736
Iteration 4/25 | Loss: 0.00110736
Iteration 5/25 | Loss: 0.00110736
Iteration 6/25 | Loss: 0.00110736
Iteration 7/25 | Loss: 0.00110736
Iteration 8/25 | Loss: 0.00110736
Iteration 9/25 | Loss: 0.00110736
Iteration 10/25 | Loss: 0.00110736
Iteration 11/25 | Loss: 0.00110736
Iteration 12/25 | Loss: 0.00110736
Iteration 13/25 | Loss: 0.00110736
Iteration 14/25 | Loss: 0.00110736
Iteration 15/25 | Loss: 0.00110736
Iteration 16/25 | Loss: 0.00110736
Iteration 17/25 | Loss: 0.00110736
Iteration 18/25 | Loss: 0.00110736
Iteration 19/25 | Loss: 0.00110736
Iteration 20/25 | Loss: 0.00110736
Iteration 21/25 | Loss: 0.00110736
Iteration 22/25 | Loss: 0.00110736
Iteration 23/25 | Loss: 0.00110736
Iteration 24/25 | Loss: 0.00110736
Iteration 25/25 | Loss: 0.00110736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110736
Iteration 2/1000 | Loss: 0.00002969
Iteration 3/1000 | Loss: 0.00002326
Iteration 4/1000 | Loss: 0.00002151
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001935
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001848
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00016224
Iteration 13/1000 | Loss: 0.00013000
Iteration 14/1000 | Loss: 0.00011653
Iteration 15/1000 | Loss: 0.00005837
Iteration 16/1000 | Loss: 0.00002020
Iteration 17/1000 | Loss: 0.00001846
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00014553
Iteration 20/1000 | Loss: 0.00002424
Iteration 21/1000 | Loss: 0.00018425
Iteration 22/1000 | Loss: 0.00030309
Iteration 23/1000 | Loss: 0.00033955
Iteration 24/1000 | Loss: 0.00024571
Iteration 25/1000 | Loss: 0.00007286
Iteration 26/1000 | Loss: 0.00015841
Iteration 27/1000 | Loss: 0.00022049
Iteration 28/1000 | Loss: 0.00012263
Iteration 29/1000 | Loss: 0.00016219
Iteration 30/1000 | Loss: 0.00010895
Iteration 31/1000 | Loss: 0.00010461
Iteration 32/1000 | Loss: 0.00069511
Iteration 33/1000 | Loss: 0.00026552
Iteration 34/1000 | Loss: 0.00022978
Iteration 35/1000 | Loss: 0.00011167
Iteration 36/1000 | Loss: 0.00006535
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00014235
Iteration 39/1000 | Loss: 0.00007268
Iteration 40/1000 | Loss: 0.00008232
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001890
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001701
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001697
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001693
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001692
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001692
Iteration 134/1000 | Loss: 0.00001692
Iteration 135/1000 | Loss: 0.00001692
Iteration 136/1000 | Loss: 0.00001692
Iteration 137/1000 | Loss: 0.00001692
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001691
Iteration 142/1000 | Loss: 0.00001691
Iteration 143/1000 | Loss: 0.00001691
Iteration 144/1000 | Loss: 0.00001691
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001690
Iteration 148/1000 | Loss: 0.00001690
Iteration 149/1000 | Loss: 0.00001690
Iteration 150/1000 | Loss: 0.00001690
Iteration 151/1000 | Loss: 0.00001690
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001690
Iteration 154/1000 | Loss: 0.00001690
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001689
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Iteration 174/1000 | Loss: 0.00001689
Iteration 175/1000 | Loss: 0.00001689
Iteration 176/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.689020973572042e-05, 1.689020973572042e-05, 1.689020973572042e-05, 1.689020973572042e-05, 1.689020973572042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.689020973572042e-05

Optimization complete. Final v2v error: 3.4339778423309326 mm

Highest mean error: 5.577195644378662 mm for frame 118

Lowest mean error: 3.147998094558716 mm for frame 42

Saving results

Total time: 121.3662965297699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678663
Iteration 2/25 | Loss: 0.00177671
Iteration 3/25 | Loss: 0.00154610
Iteration 4/25 | Loss: 0.00148825
Iteration 5/25 | Loss: 0.00145452
Iteration 6/25 | Loss: 0.00143022
Iteration 7/25 | Loss: 0.00142176
Iteration 8/25 | Loss: 0.00143445
Iteration 9/25 | Loss: 0.00142863
Iteration 10/25 | Loss: 0.00142117
Iteration 11/25 | Loss: 0.00141546
Iteration 12/25 | Loss: 0.00141386
Iteration 13/25 | Loss: 0.00141185
Iteration 14/25 | Loss: 0.00141064
Iteration 15/25 | Loss: 0.00141034
Iteration 16/25 | Loss: 0.00141022
Iteration 17/25 | Loss: 0.00141015
Iteration 18/25 | Loss: 0.00141015
Iteration 19/25 | Loss: 0.00141015
Iteration 20/25 | Loss: 0.00141015
Iteration 21/25 | Loss: 0.00141015
Iteration 22/25 | Loss: 0.00141015
Iteration 23/25 | Loss: 0.00141015
Iteration 24/25 | Loss: 0.00141015
Iteration 25/25 | Loss: 0.00141014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20176148
Iteration 2/25 | Loss: 0.00134937
Iteration 3/25 | Loss: 0.00134933
Iteration 4/25 | Loss: 0.00134933
Iteration 5/25 | Loss: 0.00134933
Iteration 6/25 | Loss: 0.00134932
Iteration 7/25 | Loss: 0.00134932
Iteration 8/25 | Loss: 0.00134932
Iteration 9/25 | Loss: 0.00134932
Iteration 10/25 | Loss: 0.00134932
Iteration 11/25 | Loss: 0.00134932
Iteration 12/25 | Loss: 0.00134932
Iteration 13/25 | Loss: 0.00134932
Iteration 14/25 | Loss: 0.00134932
Iteration 15/25 | Loss: 0.00134932
Iteration 16/25 | Loss: 0.00134932
Iteration 17/25 | Loss: 0.00134932
Iteration 18/25 | Loss: 0.00134932
Iteration 19/25 | Loss: 0.00134932
Iteration 20/25 | Loss: 0.00134932
Iteration 21/25 | Loss: 0.00134932
Iteration 22/25 | Loss: 0.00134932
Iteration 23/25 | Loss: 0.00134932
Iteration 24/25 | Loss: 0.00134932
Iteration 25/25 | Loss: 0.00134932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134932
Iteration 2/1000 | Loss: 0.00003861
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002443
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002125
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00009409
Iteration 13/1000 | Loss: 0.00002527
Iteration 14/1000 | Loss: 0.00002148
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001807
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001767
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001753
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001752
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001751
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001743
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001742
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001739
Iteration 134/1000 | Loss: 0.00001739
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001739
Iteration 143/1000 | Loss: 0.00001739
Iteration 144/1000 | Loss: 0.00001739
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001738
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001738
Iteration 152/1000 | Loss: 0.00001738
Iteration 153/1000 | Loss: 0.00001738
Iteration 154/1000 | Loss: 0.00001738
Iteration 155/1000 | Loss: 0.00001738
Iteration 156/1000 | Loss: 0.00001738
Iteration 157/1000 | Loss: 0.00001738
Iteration 158/1000 | Loss: 0.00001738
Iteration 159/1000 | Loss: 0.00001738
Iteration 160/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.737770980980713e-05, 1.737770980980713e-05, 1.737770980980713e-05, 1.737770980980713e-05, 1.737770980980713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.737770980980713e-05

Optimization complete. Final v2v error: 3.487426280975342 mm

Highest mean error: 4.007939338684082 mm for frame 18

Lowest mean error: 3.0578420162200928 mm for frame 134

Saving results

Total time: 71.37412214279175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821784
Iteration 2/25 | Loss: 0.00151721
Iteration 3/25 | Loss: 0.00137749
Iteration 4/25 | Loss: 0.00135070
Iteration 5/25 | Loss: 0.00134305
Iteration 6/25 | Loss: 0.00134147
Iteration 7/25 | Loss: 0.00134145
Iteration 8/25 | Loss: 0.00134145
Iteration 9/25 | Loss: 0.00134145
Iteration 10/25 | Loss: 0.00134145
Iteration 11/25 | Loss: 0.00134145
Iteration 12/25 | Loss: 0.00134145
Iteration 13/25 | Loss: 0.00134145
Iteration 14/25 | Loss: 0.00134145
Iteration 15/25 | Loss: 0.00134145
Iteration 16/25 | Loss: 0.00134145
Iteration 17/25 | Loss: 0.00134145
Iteration 18/25 | Loss: 0.00134145
Iteration 19/25 | Loss: 0.00134145
Iteration 20/25 | Loss: 0.00134145
Iteration 21/25 | Loss: 0.00134145
Iteration 22/25 | Loss: 0.00134145
Iteration 23/25 | Loss: 0.00134145
Iteration 24/25 | Loss: 0.00134145
Iteration 25/25 | Loss: 0.00134145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013414542190730572, 0.0013414542190730572, 0.0013414542190730572, 0.0013414542190730572, 0.0013414542190730572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013414542190730572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85473204
Iteration 2/25 | Loss: 0.00179299
Iteration 3/25 | Loss: 0.00179299
Iteration 4/25 | Loss: 0.00179299
Iteration 5/25 | Loss: 0.00179299
Iteration 6/25 | Loss: 0.00179298
Iteration 7/25 | Loss: 0.00179298
Iteration 8/25 | Loss: 0.00179298
Iteration 9/25 | Loss: 0.00179298
Iteration 10/25 | Loss: 0.00179298
Iteration 11/25 | Loss: 0.00179298
Iteration 12/25 | Loss: 0.00179298
Iteration 13/25 | Loss: 0.00179298
Iteration 14/25 | Loss: 0.00179298
Iteration 15/25 | Loss: 0.00179298
Iteration 16/25 | Loss: 0.00179298
Iteration 17/25 | Loss: 0.00179298
Iteration 18/25 | Loss: 0.00179298
Iteration 19/25 | Loss: 0.00179298
Iteration 20/25 | Loss: 0.00179298
Iteration 21/25 | Loss: 0.00179298
Iteration 22/25 | Loss: 0.00179298
Iteration 23/25 | Loss: 0.00179298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017929839668795466, 0.0017929839668795466, 0.0017929839668795466, 0.0017929839668795466, 0.0017929839668795466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017929839668795466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179298
Iteration 2/1000 | Loss: 0.00005412
Iteration 3/1000 | Loss: 0.00003396
Iteration 4/1000 | Loss: 0.00002759
Iteration 5/1000 | Loss: 0.00002598
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002436
Iteration 8/1000 | Loss: 0.00002391
Iteration 9/1000 | Loss: 0.00002340
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002259
Iteration 13/1000 | Loss: 0.00002257
Iteration 14/1000 | Loss: 0.00002244
Iteration 15/1000 | Loss: 0.00002235
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002207
Iteration 18/1000 | Loss: 0.00002200
Iteration 19/1000 | Loss: 0.00002200
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002199
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002195
Iteration 24/1000 | Loss: 0.00002194
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002190
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002188
Iteration 33/1000 | Loss: 0.00002188
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002180
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002177
Iteration 60/1000 | Loss: 0.00002177
Iteration 61/1000 | Loss: 0.00002177
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002177
Iteration 64/1000 | Loss: 0.00002177
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002176
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002176
Iteration 71/1000 | Loss: 0.00002176
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002175
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002174
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002173
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002170
Iteration 102/1000 | Loss: 0.00002170
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002170
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002169
Iteration 107/1000 | Loss: 0.00002169
Iteration 108/1000 | Loss: 0.00002169
Iteration 109/1000 | Loss: 0.00002169
Iteration 110/1000 | Loss: 0.00002168
Iteration 111/1000 | Loss: 0.00002168
Iteration 112/1000 | Loss: 0.00002168
Iteration 113/1000 | Loss: 0.00002168
Iteration 114/1000 | Loss: 0.00002168
Iteration 115/1000 | Loss: 0.00002167
Iteration 116/1000 | Loss: 0.00002167
Iteration 117/1000 | Loss: 0.00002167
Iteration 118/1000 | Loss: 0.00002167
Iteration 119/1000 | Loss: 0.00002167
Iteration 120/1000 | Loss: 0.00002167
Iteration 121/1000 | Loss: 0.00002167
Iteration 122/1000 | Loss: 0.00002167
Iteration 123/1000 | Loss: 0.00002167
Iteration 124/1000 | Loss: 0.00002167
Iteration 125/1000 | Loss: 0.00002167
Iteration 126/1000 | Loss: 0.00002167
Iteration 127/1000 | Loss: 0.00002167
Iteration 128/1000 | Loss: 0.00002167
Iteration 129/1000 | Loss: 0.00002167
Iteration 130/1000 | Loss: 0.00002167
Iteration 131/1000 | Loss: 0.00002167
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002167
Iteration 143/1000 | Loss: 0.00002167
Iteration 144/1000 | Loss: 0.00002167
Iteration 145/1000 | Loss: 0.00002167
Iteration 146/1000 | Loss: 0.00002167
Iteration 147/1000 | Loss: 0.00002167
Iteration 148/1000 | Loss: 0.00002167
Iteration 149/1000 | Loss: 0.00002167
Iteration 150/1000 | Loss: 0.00002167
Iteration 151/1000 | Loss: 0.00002167
Iteration 152/1000 | Loss: 0.00002167
Iteration 153/1000 | Loss: 0.00002167
Iteration 154/1000 | Loss: 0.00002167
Iteration 155/1000 | Loss: 0.00002167
Iteration 156/1000 | Loss: 0.00002167
Iteration 157/1000 | Loss: 0.00002167
Iteration 158/1000 | Loss: 0.00002167
Iteration 159/1000 | Loss: 0.00002167
Iteration 160/1000 | Loss: 0.00002167
Iteration 161/1000 | Loss: 0.00002167
Iteration 162/1000 | Loss: 0.00002167
Iteration 163/1000 | Loss: 0.00002167
Iteration 164/1000 | Loss: 0.00002167
Iteration 165/1000 | Loss: 0.00002167
Iteration 166/1000 | Loss: 0.00002167
Iteration 167/1000 | Loss: 0.00002167
Iteration 168/1000 | Loss: 0.00002167
Iteration 169/1000 | Loss: 0.00002167
Iteration 170/1000 | Loss: 0.00002167
Iteration 171/1000 | Loss: 0.00002167
Iteration 172/1000 | Loss: 0.00002167
Iteration 173/1000 | Loss: 0.00002167
Iteration 174/1000 | Loss: 0.00002167
Iteration 175/1000 | Loss: 0.00002167
Iteration 176/1000 | Loss: 0.00002167
Iteration 177/1000 | Loss: 0.00002167
Iteration 178/1000 | Loss: 0.00002167
Iteration 179/1000 | Loss: 0.00002167
Iteration 180/1000 | Loss: 0.00002167
Iteration 181/1000 | Loss: 0.00002167
Iteration 182/1000 | Loss: 0.00002167
Iteration 183/1000 | Loss: 0.00002167
Iteration 184/1000 | Loss: 0.00002167
Iteration 185/1000 | Loss: 0.00002167
Iteration 186/1000 | Loss: 0.00002167
Iteration 187/1000 | Loss: 0.00002167
Iteration 188/1000 | Loss: 0.00002167
Iteration 189/1000 | Loss: 0.00002167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.166809281334281e-05, 2.166809281334281e-05, 2.166809281334281e-05, 2.166809281334281e-05, 2.166809281334281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.166809281334281e-05

Optimization complete. Final v2v error: 3.9300668239593506 mm

Highest mean error: 4.391523361206055 mm for frame 48

Lowest mean error: 3.626451015472412 mm for frame 130

Saving results

Total time: 42.00427985191345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827301
Iteration 2/25 | Loss: 0.00141538
Iteration 3/25 | Loss: 0.00135399
Iteration 4/25 | Loss: 0.00133805
Iteration 5/25 | Loss: 0.00133238
Iteration 6/25 | Loss: 0.00133238
Iteration 7/25 | Loss: 0.00133238
Iteration 8/25 | Loss: 0.00133238
Iteration 9/25 | Loss: 0.00133238
Iteration 10/25 | Loss: 0.00133238
Iteration 11/25 | Loss: 0.00133238
Iteration 12/25 | Loss: 0.00133238
Iteration 13/25 | Loss: 0.00133238
Iteration 14/25 | Loss: 0.00133238
Iteration 15/25 | Loss: 0.00133238
Iteration 16/25 | Loss: 0.00133238
Iteration 17/25 | Loss: 0.00133238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013323797611519694, 0.0013323797611519694, 0.0013323797611519694, 0.0013323797611519694, 0.0013323797611519694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013323797611519694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28956211
Iteration 2/25 | Loss: 0.00161548
Iteration 3/25 | Loss: 0.00161548
Iteration 4/25 | Loss: 0.00161548
Iteration 5/25 | Loss: 0.00161548
Iteration 6/25 | Loss: 0.00161548
Iteration 7/25 | Loss: 0.00161548
Iteration 8/25 | Loss: 0.00161548
Iteration 9/25 | Loss: 0.00161548
Iteration 10/25 | Loss: 0.00161548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0016154766781255603, 0.0016154766781255603, 0.0016154766781255603, 0.0016154766781255603, 0.0016154766781255603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016154766781255603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161548
Iteration 2/1000 | Loss: 0.00003258
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002133
Iteration 6/1000 | Loss: 0.00002082
Iteration 7/1000 | Loss: 0.00002062
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001972
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001926
Iteration 24/1000 | Loss: 0.00001925
Iteration 25/1000 | Loss: 0.00001921
Iteration 26/1000 | Loss: 0.00001921
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001918
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001913
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001903
Iteration 66/1000 | Loss: 0.00001903
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001899
Iteration 80/1000 | Loss: 0.00001899
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001896
Iteration 99/1000 | Loss: 0.00001896
Iteration 100/1000 | Loss: 0.00001896
Iteration 101/1000 | Loss: 0.00001896
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001896
Iteration 109/1000 | Loss: 0.00001896
Iteration 110/1000 | Loss: 0.00001896
Iteration 111/1000 | Loss: 0.00001896
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001893
Iteration 117/1000 | Loss: 0.00001893
Iteration 118/1000 | Loss: 0.00001893
Iteration 119/1000 | Loss: 0.00001893
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001891
Iteration 125/1000 | Loss: 0.00001891
Iteration 126/1000 | Loss: 0.00001891
Iteration 127/1000 | Loss: 0.00001891
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001890
Iteration 133/1000 | Loss: 0.00001890
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001888
Iteration 141/1000 | Loss: 0.00001888
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001886
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001886
Iteration 150/1000 | Loss: 0.00001886
Iteration 151/1000 | Loss: 0.00001886
Iteration 152/1000 | Loss: 0.00001886
Iteration 153/1000 | Loss: 0.00001886
Iteration 154/1000 | Loss: 0.00001886
Iteration 155/1000 | Loss: 0.00001886
Iteration 156/1000 | Loss: 0.00001886
Iteration 157/1000 | Loss: 0.00001886
Iteration 158/1000 | Loss: 0.00001886
Iteration 159/1000 | Loss: 0.00001886
Iteration 160/1000 | Loss: 0.00001886
Iteration 161/1000 | Loss: 0.00001885
Iteration 162/1000 | Loss: 0.00001885
Iteration 163/1000 | Loss: 0.00001885
Iteration 164/1000 | Loss: 0.00001885
Iteration 165/1000 | Loss: 0.00001885
Iteration 166/1000 | Loss: 0.00001885
Iteration 167/1000 | Loss: 0.00001885
Iteration 168/1000 | Loss: 0.00001885
Iteration 169/1000 | Loss: 0.00001885
Iteration 170/1000 | Loss: 0.00001885
Iteration 171/1000 | Loss: 0.00001885
Iteration 172/1000 | Loss: 0.00001885
Iteration 173/1000 | Loss: 0.00001885
Iteration 174/1000 | Loss: 0.00001885
Iteration 175/1000 | Loss: 0.00001885
Iteration 176/1000 | Loss: 0.00001885
Iteration 177/1000 | Loss: 0.00001885
Iteration 178/1000 | Loss: 0.00001885
Iteration 179/1000 | Loss: 0.00001885
Iteration 180/1000 | Loss: 0.00001885
Iteration 181/1000 | Loss: 0.00001885
Iteration 182/1000 | Loss: 0.00001885
Iteration 183/1000 | Loss: 0.00001885
Iteration 184/1000 | Loss: 0.00001885
Iteration 185/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.884570338006597e-05, 1.884570338006597e-05, 1.884570338006597e-05, 1.884570338006597e-05, 1.884570338006597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.884570338006597e-05

Optimization complete. Final v2v error: 3.6832571029663086 mm

Highest mean error: 3.7629170417785645 mm for frame 153

Lowest mean error: 3.496776580810547 mm for frame 2

Saving results

Total time: 43.24710941314697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780022
Iteration 2/25 | Loss: 0.00149946
Iteration 3/25 | Loss: 0.00135045
Iteration 4/25 | Loss: 0.00132945
Iteration 5/25 | Loss: 0.00132161
Iteration 6/25 | Loss: 0.00132059
Iteration 7/25 | Loss: 0.00132059
Iteration 8/25 | Loss: 0.00132059
Iteration 9/25 | Loss: 0.00132059
Iteration 10/25 | Loss: 0.00132059
Iteration 11/25 | Loss: 0.00132059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013205900322645903, 0.0013205900322645903, 0.0013205900322645903, 0.0013205900322645903, 0.0013205900322645903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013205900322645903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67208588
Iteration 2/25 | Loss: 0.00121940
Iteration 3/25 | Loss: 0.00121940
Iteration 4/25 | Loss: 0.00121940
Iteration 5/25 | Loss: 0.00121940
Iteration 6/25 | Loss: 0.00121940
Iteration 7/25 | Loss: 0.00121940
Iteration 8/25 | Loss: 0.00121940
Iteration 9/25 | Loss: 0.00121940
Iteration 10/25 | Loss: 0.00121940
Iteration 11/25 | Loss: 0.00121940
Iteration 12/25 | Loss: 0.00121940
Iteration 13/25 | Loss: 0.00121940
Iteration 14/25 | Loss: 0.00121940
Iteration 15/25 | Loss: 0.00121940
Iteration 16/25 | Loss: 0.00121940
Iteration 17/25 | Loss: 0.00121940
Iteration 18/25 | Loss: 0.00121940
Iteration 19/25 | Loss: 0.00121940
Iteration 20/25 | Loss: 0.00121940
Iteration 21/25 | Loss: 0.00121940
Iteration 22/25 | Loss: 0.00121940
Iteration 23/25 | Loss: 0.00121940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012194005539640784, 0.0012194005539640784, 0.0012194005539640784, 0.0012194005539640784, 0.0012194005539640784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012194005539640784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121940
Iteration 2/1000 | Loss: 0.00004088
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002466
Iteration 5/1000 | Loss: 0.00002319
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001956
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001916
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001910
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001908
Iteration 35/1000 | Loss: 0.00001907
Iteration 36/1000 | Loss: 0.00001907
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001906
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001906
Iteration 44/1000 | Loss: 0.00001906
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001905
Iteration 47/1000 | Loss: 0.00001905
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001905
Iteration 50/1000 | Loss: 0.00001905
Iteration 51/1000 | Loss: 0.00001905
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001905
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001904
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001903
Iteration 65/1000 | Loss: 0.00001903
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001900
Iteration 73/1000 | Loss: 0.00001900
Iteration 74/1000 | Loss: 0.00001900
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001897
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001896
Iteration 88/1000 | Loss: 0.00001896
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001894
Iteration 104/1000 | Loss: 0.00001894
Iteration 105/1000 | Loss: 0.00001894
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001891
Iteration 115/1000 | Loss: 0.00001891
Iteration 116/1000 | Loss: 0.00001891
Iteration 117/1000 | Loss: 0.00001891
Iteration 118/1000 | Loss: 0.00001890
Iteration 119/1000 | Loss: 0.00001890
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001888
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001888
Iteration 128/1000 | Loss: 0.00001888
Iteration 129/1000 | Loss: 0.00001888
Iteration 130/1000 | Loss: 0.00001887
Iteration 131/1000 | Loss: 0.00001887
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001887
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001886
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001886
Iteration 138/1000 | Loss: 0.00001886
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001886
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001885
Iteration 147/1000 | Loss: 0.00001885
Iteration 148/1000 | Loss: 0.00001885
Iteration 149/1000 | Loss: 0.00001885
Iteration 150/1000 | Loss: 0.00001885
Iteration 151/1000 | Loss: 0.00001884
Iteration 152/1000 | Loss: 0.00001884
Iteration 153/1000 | Loss: 0.00001884
Iteration 154/1000 | Loss: 0.00001884
Iteration 155/1000 | Loss: 0.00001884
Iteration 156/1000 | Loss: 0.00001884
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001884
Iteration 159/1000 | Loss: 0.00001884
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001884
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00001883
Iteration 164/1000 | Loss: 0.00001883
Iteration 165/1000 | Loss: 0.00001883
Iteration 166/1000 | Loss: 0.00001883
Iteration 167/1000 | Loss: 0.00001883
Iteration 168/1000 | Loss: 0.00001883
Iteration 169/1000 | Loss: 0.00001882
Iteration 170/1000 | Loss: 0.00001882
Iteration 171/1000 | Loss: 0.00001882
Iteration 172/1000 | Loss: 0.00001882
Iteration 173/1000 | Loss: 0.00001882
Iteration 174/1000 | Loss: 0.00001882
Iteration 175/1000 | Loss: 0.00001882
Iteration 176/1000 | Loss: 0.00001882
Iteration 177/1000 | Loss: 0.00001882
Iteration 178/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.8822653146344237e-05, 1.8822653146344237e-05, 1.8822653146344237e-05, 1.8822653146344237e-05, 1.8822653146344237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8822653146344237e-05

Optimization complete. Final v2v error: 3.713886260986328 mm

Highest mean error: 4.060075283050537 mm for frame 130

Lowest mean error: 3.423067569732666 mm for frame 73

Saving results

Total time: 45.2670419216156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810749
Iteration 2/25 | Loss: 0.00138448
Iteration 3/25 | Loss: 0.00129918
Iteration 4/25 | Loss: 0.00129007
Iteration 5/25 | Loss: 0.00128821
Iteration 6/25 | Loss: 0.00128812
Iteration 7/25 | Loss: 0.00128812
Iteration 8/25 | Loss: 0.00128812
Iteration 9/25 | Loss: 0.00128812
Iteration 10/25 | Loss: 0.00128812
Iteration 11/25 | Loss: 0.00128812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012881167931482196, 0.0012881167931482196, 0.0012881167931482196, 0.0012881167931482196, 0.0012881167931482196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012881167931482196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30087245
Iteration 2/25 | Loss: 0.00121141
Iteration 3/25 | Loss: 0.00121139
Iteration 4/25 | Loss: 0.00121139
Iteration 5/25 | Loss: 0.00121139
Iteration 6/25 | Loss: 0.00121139
Iteration 7/25 | Loss: 0.00121139
Iteration 8/25 | Loss: 0.00121139
Iteration 9/25 | Loss: 0.00121139
Iteration 10/25 | Loss: 0.00121139
Iteration 11/25 | Loss: 0.00121139
Iteration 12/25 | Loss: 0.00121139
Iteration 13/25 | Loss: 0.00121139
Iteration 14/25 | Loss: 0.00121139
Iteration 15/25 | Loss: 0.00121139
Iteration 16/25 | Loss: 0.00121139
Iteration 17/25 | Loss: 0.00121139
Iteration 18/25 | Loss: 0.00121139
Iteration 19/25 | Loss: 0.00121139
Iteration 20/25 | Loss: 0.00121139
Iteration 21/25 | Loss: 0.00121139
Iteration 22/25 | Loss: 0.00121139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001211390015669167, 0.001211390015669167, 0.001211390015669167, 0.001211390015669167, 0.001211390015669167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211390015669167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121139
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001542
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001268
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001128
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001116
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001063
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001039
Iteration 24/1000 | Loss: 0.00001037
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001026
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001022
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001019
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001018
Iteration 51/1000 | Loss: 0.00001018
Iteration 52/1000 | Loss: 0.00001018
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001017
Iteration 57/1000 | Loss: 0.00001017
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001013
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001013
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001011
Iteration 79/1000 | Loss: 0.00001011
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001009
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001007
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001005
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001003
Iteration 98/1000 | Loss: 0.00001003
Iteration 99/1000 | Loss: 0.00001003
Iteration 100/1000 | Loss: 0.00001003
Iteration 101/1000 | Loss: 0.00001003
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001002
Iteration 110/1000 | Loss: 0.00001002
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00001000
Iteration 114/1000 | Loss: 0.00001000
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00001000
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000999
Iteration 119/1000 | Loss: 0.00000999
Iteration 120/1000 | Loss: 0.00000999
Iteration 121/1000 | Loss: 0.00000999
Iteration 122/1000 | Loss: 0.00000999
Iteration 123/1000 | Loss: 0.00000999
Iteration 124/1000 | Loss: 0.00000998
Iteration 125/1000 | Loss: 0.00000998
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000996
Iteration 139/1000 | Loss: 0.00000996
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000994
Iteration 148/1000 | Loss: 0.00000994
Iteration 149/1000 | Loss: 0.00000994
Iteration 150/1000 | Loss: 0.00000994
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000993
Iteration 153/1000 | Loss: 0.00000993
Iteration 154/1000 | Loss: 0.00000993
Iteration 155/1000 | Loss: 0.00000993
Iteration 156/1000 | Loss: 0.00000993
Iteration 157/1000 | Loss: 0.00000993
Iteration 158/1000 | Loss: 0.00000993
Iteration 159/1000 | Loss: 0.00000993
Iteration 160/1000 | Loss: 0.00000993
Iteration 161/1000 | Loss: 0.00000993
Iteration 162/1000 | Loss: 0.00000992
Iteration 163/1000 | Loss: 0.00000992
Iteration 164/1000 | Loss: 0.00000992
Iteration 165/1000 | Loss: 0.00000992
Iteration 166/1000 | Loss: 0.00000992
Iteration 167/1000 | Loss: 0.00000992
Iteration 168/1000 | Loss: 0.00000992
Iteration 169/1000 | Loss: 0.00000992
Iteration 170/1000 | Loss: 0.00000992
Iteration 171/1000 | Loss: 0.00000992
Iteration 172/1000 | Loss: 0.00000992
Iteration 173/1000 | Loss: 0.00000992
Iteration 174/1000 | Loss: 0.00000992
Iteration 175/1000 | Loss: 0.00000991
Iteration 176/1000 | Loss: 0.00000991
Iteration 177/1000 | Loss: 0.00000991
Iteration 178/1000 | Loss: 0.00000991
Iteration 179/1000 | Loss: 0.00000991
Iteration 180/1000 | Loss: 0.00000991
Iteration 181/1000 | Loss: 0.00000991
Iteration 182/1000 | Loss: 0.00000991
Iteration 183/1000 | Loss: 0.00000991
Iteration 184/1000 | Loss: 0.00000991
Iteration 185/1000 | Loss: 0.00000991
Iteration 186/1000 | Loss: 0.00000991
Iteration 187/1000 | Loss: 0.00000991
Iteration 188/1000 | Loss: 0.00000991
Iteration 189/1000 | Loss: 0.00000991
Iteration 190/1000 | Loss: 0.00000991
Iteration 191/1000 | Loss: 0.00000991
Iteration 192/1000 | Loss: 0.00000991
Iteration 193/1000 | Loss: 0.00000991
Iteration 194/1000 | Loss: 0.00000991
Iteration 195/1000 | Loss: 0.00000991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [9.910791959555354e-06, 9.910791959555354e-06, 9.910791959555354e-06, 9.910791959555354e-06, 9.910791959555354e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.910791959555354e-06

Optimization complete. Final v2v error: 2.724583625793457 mm

Highest mean error: 3.049271821975708 mm for frame 0

Lowest mean error: 2.612231492996216 mm for frame 89

Saving results

Total time: 39.28767919540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379209
Iteration 2/25 | Loss: 0.00132271
Iteration 3/25 | Loss: 0.00127279
Iteration 4/25 | Loss: 0.00126426
Iteration 5/25 | Loss: 0.00126106
Iteration 6/25 | Loss: 0.00126040
Iteration 7/25 | Loss: 0.00126040
Iteration 8/25 | Loss: 0.00126040
Iteration 9/25 | Loss: 0.00126040
Iteration 10/25 | Loss: 0.00126040
Iteration 11/25 | Loss: 0.00126040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012603957438841462, 0.0012603957438841462, 0.0012603957438841462, 0.0012603957438841462, 0.0012603957438841462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012603957438841462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40289605
Iteration 2/25 | Loss: 0.00143663
Iteration 3/25 | Loss: 0.00143663
Iteration 4/25 | Loss: 0.00143663
Iteration 5/25 | Loss: 0.00143663
Iteration 6/25 | Loss: 0.00143663
Iteration 7/25 | Loss: 0.00143663
Iteration 8/25 | Loss: 0.00143663
Iteration 9/25 | Loss: 0.00143663
Iteration 10/25 | Loss: 0.00143663
Iteration 11/25 | Loss: 0.00143663
Iteration 12/25 | Loss: 0.00143663
Iteration 13/25 | Loss: 0.00143663
Iteration 14/25 | Loss: 0.00143663
Iteration 15/25 | Loss: 0.00143663
Iteration 16/25 | Loss: 0.00143663
Iteration 17/25 | Loss: 0.00143663
Iteration 18/25 | Loss: 0.00143663
Iteration 19/25 | Loss: 0.00143663
Iteration 20/25 | Loss: 0.00143663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014366303803399205, 0.0014366303803399205, 0.0014366303803399205, 0.0014366303803399205, 0.0014366303803399205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014366303803399205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143663
Iteration 2/1000 | Loss: 0.00002188
Iteration 3/1000 | Loss: 0.00001473
Iteration 4/1000 | Loss: 0.00001315
Iteration 5/1000 | Loss: 0.00001239
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001137
Iteration 8/1000 | Loss: 0.00001118
Iteration 9/1000 | Loss: 0.00001094
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001055
Iteration 17/1000 | Loss: 0.00001041
Iteration 18/1000 | Loss: 0.00001031
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001024
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001020
Iteration 31/1000 | Loss: 0.00001019
Iteration 32/1000 | Loss: 0.00001018
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001016
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001015
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001014
Iteration 41/1000 | Loss: 0.00001014
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001005
Iteration 46/1000 | Loss: 0.00001005
Iteration 47/1000 | Loss: 0.00001005
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001004
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000996
Iteration 78/1000 | Loss: 0.00000995
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000993
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000988
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000985
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000975
Iteration 116/1000 | Loss: 0.00000975
Iteration 117/1000 | Loss: 0.00000975
Iteration 118/1000 | Loss: 0.00000975
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000974
Iteration 121/1000 | Loss: 0.00000974
Iteration 122/1000 | Loss: 0.00000974
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000973
Iteration 132/1000 | Loss: 0.00000973
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000972
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000972
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000971
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000971
Iteration 147/1000 | Loss: 0.00000971
Iteration 148/1000 | Loss: 0.00000971
Iteration 149/1000 | Loss: 0.00000971
Iteration 150/1000 | Loss: 0.00000971
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000970
Iteration 157/1000 | Loss: 0.00000970
Iteration 158/1000 | Loss: 0.00000970
Iteration 159/1000 | Loss: 0.00000970
Iteration 160/1000 | Loss: 0.00000970
Iteration 161/1000 | Loss: 0.00000970
Iteration 162/1000 | Loss: 0.00000970
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000969
Iteration 166/1000 | Loss: 0.00000969
Iteration 167/1000 | Loss: 0.00000969
Iteration 168/1000 | Loss: 0.00000969
Iteration 169/1000 | Loss: 0.00000969
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000969
Iteration 181/1000 | Loss: 0.00000969
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000967
Iteration 187/1000 | Loss: 0.00000967
Iteration 188/1000 | Loss: 0.00000967
Iteration 189/1000 | Loss: 0.00000967
Iteration 190/1000 | Loss: 0.00000967
Iteration 191/1000 | Loss: 0.00000967
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00000967
Iteration 194/1000 | Loss: 0.00000967
Iteration 195/1000 | Loss: 0.00000967
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000966
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000966
Iteration 206/1000 | Loss: 0.00000966
Iteration 207/1000 | Loss: 0.00000966
Iteration 208/1000 | Loss: 0.00000966
Iteration 209/1000 | Loss: 0.00000965
Iteration 210/1000 | Loss: 0.00000965
Iteration 211/1000 | Loss: 0.00000965
Iteration 212/1000 | Loss: 0.00000965
Iteration 213/1000 | Loss: 0.00000964
Iteration 214/1000 | Loss: 0.00000964
Iteration 215/1000 | Loss: 0.00000964
Iteration 216/1000 | Loss: 0.00000964
Iteration 217/1000 | Loss: 0.00000964
Iteration 218/1000 | Loss: 0.00000964
Iteration 219/1000 | Loss: 0.00000964
Iteration 220/1000 | Loss: 0.00000964
Iteration 221/1000 | Loss: 0.00000964
Iteration 222/1000 | Loss: 0.00000964
Iteration 223/1000 | Loss: 0.00000963
Iteration 224/1000 | Loss: 0.00000963
Iteration 225/1000 | Loss: 0.00000963
Iteration 226/1000 | Loss: 0.00000963
Iteration 227/1000 | Loss: 0.00000963
Iteration 228/1000 | Loss: 0.00000963
Iteration 229/1000 | Loss: 0.00000963
Iteration 230/1000 | Loss: 0.00000962
Iteration 231/1000 | Loss: 0.00000962
Iteration 232/1000 | Loss: 0.00000962
Iteration 233/1000 | Loss: 0.00000962
Iteration 234/1000 | Loss: 0.00000962
Iteration 235/1000 | Loss: 0.00000962
Iteration 236/1000 | Loss: 0.00000962
Iteration 237/1000 | Loss: 0.00000962
Iteration 238/1000 | Loss: 0.00000962
Iteration 239/1000 | Loss: 0.00000962
Iteration 240/1000 | Loss: 0.00000962
Iteration 241/1000 | Loss: 0.00000962
Iteration 242/1000 | Loss: 0.00000962
Iteration 243/1000 | Loss: 0.00000962
Iteration 244/1000 | Loss: 0.00000962
Iteration 245/1000 | Loss: 0.00000962
Iteration 246/1000 | Loss: 0.00000962
Iteration 247/1000 | Loss: 0.00000962
Iteration 248/1000 | Loss: 0.00000962
Iteration 249/1000 | Loss: 0.00000962
Iteration 250/1000 | Loss: 0.00000962
Iteration 251/1000 | Loss: 0.00000962
Iteration 252/1000 | Loss: 0.00000961
Iteration 253/1000 | Loss: 0.00000961
Iteration 254/1000 | Loss: 0.00000961
Iteration 255/1000 | Loss: 0.00000961
Iteration 256/1000 | Loss: 0.00000961
Iteration 257/1000 | Loss: 0.00000961
Iteration 258/1000 | Loss: 0.00000961
Iteration 259/1000 | Loss: 0.00000961
Iteration 260/1000 | Loss: 0.00000961
Iteration 261/1000 | Loss: 0.00000961
Iteration 262/1000 | Loss: 0.00000961
Iteration 263/1000 | Loss: 0.00000961
Iteration 264/1000 | Loss: 0.00000961
Iteration 265/1000 | Loss: 0.00000961
Iteration 266/1000 | Loss: 0.00000961
Iteration 267/1000 | Loss: 0.00000961
Iteration 268/1000 | Loss: 0.00000961
Iteration 269/1000 | Loss: 0.00000961
Iteration 270/1000 | Loss: 0.00000961
Iteration 271/1000 | Loss: 0.00000961
Iteration 272/1000 | Loss: 0.00000961
Iteration 273/1000 | Loss: 0.00000961
Iteration 274/1000 | Loss: 0.00000961
Iteration 275/1000 | Loss: 0.00000961
Iteration 276/1000 | Loss: 0.00000961
Iteration 277/1000 | Loss: 0.00000961
Iteration 278/1000 | Loss: 0.00000961
Iteration 279/1000 | Loss: 0.00000961
Iteration 280/1000 | Loss: 0.00000961
Iteration 281/1000 | Loss: 0.00000961
Iteration 282/1000 | Loss: 0.00000961
Iteration 283/1000 | Loss: 0.00000961
Iteration 284/1000 | Loss: 0.00000961
Iteration 285/1000 | Loss: 0.00000961
Iteration 286/1000 | Loss: 0.00000961
Iteration 287/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [9.608324035070837e-06, 9.608324035070837e-06, 9.608324035070837e-06, 9.608324035070837e-06, 9.608324035070837e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.608324035070837e-06

Optimization complete. Final v2v error: 2.690751552581787 mm

Highest mean error: 2.8786628246307373 mm for frame 15

Lowest mean error: 2.5568172931671143 mm for frame 32

Saving results

Total time: 44.07675385475159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646029
Iteration 2/25 | Loss: 0.00169633
Iteration 3/25 | Loss: 0.00141345
Iteration 4/25 | Loss: 0.00137006
Iteration 5/25 | Loss: 0.00135934
Iteration 6/25 | Loss: 0.00135546
Iteration 7/25 | Loss: 0.00134737
Iteration 8/25 | Loss: 0.00134363
Iteration 9/25 | Loss: 0.00133676
Iteration 10/25 | Loss: 0.00133506
Iteration 11/25 | Loss: 0.00133486
Iteration 12/25 | Loss: 0.00133486
Iteration 13/25 | Loss: 0.00133485
Iteration 14/25 | Loss: 0.00133485
Iteration 15/25 | Loss: 0.00133485
Iteration 16/25 | Loss: 0.00133485
Iteration 17/25 | Loss: 0.00133485
Iteration 18/25 | Loss: 0.00133485
Iteration 19/25 | Loss: 0.00133485
Iteration 20/25 | Loss: 0.00133485
Iteration 21/25 | Loss: 0.00133485
Iteration 22/25 | Loss: 0.00133485
Iteration 23/25 | Loss: 0.00133484
Iteration 24/25 | Loss: 0.00133484
Iteration 25/25 | Loss: 0.00133484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96694350
Iteration 2/25 | Loss: 0.00164082
Iteration 3/25 | Loss: 0.00164051
Iteration 4/25 | Loss: 0.00164050
Iteration 5/25 | Loss: 0.00164050
Iteration 6/25 | Loss: 0.00164050
Iteration 7/25 | Loss: 0.00164050
Iteration 8/25 | Loss: 0.00164050
Iteration 9/25 | Loss: 0.00164050
Iteration 10/25 | Loss: 0.00164050
Iteration 11/25 | Loss: 0.00164050
Iteration 12/25 | Loss: 0.00164050
Iteration 13/25 | Loss: 0.00164050
Iteration 14/25 | Loss: 0.00164050
Iteration 15/25 | Loss: 0.00164050
Iteration 16/25 | Loss: 0.00164050
Iteration 17/25 | Loss: 0.00164050
Iteration 18/25 | Loss: 0.00164050
Iteration 19/25 | Loss: 0.00164050
Iteration 20/25 | Loss: 0.00164050
Iteration 21/25 | Loss: 0.00164050
Iteration 22/25 | Loss: 0.00164050
Iteration 23/25 | Loss: 0.00164050
Iteration 24/25 | Loss: 0.00164050
Iteration 25/25 | Loss: 0.00164050

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164050
Iteration 2/1000 | Loss: 0.00005760
Iteration 3/1000 | Loss: 0.00003835
Iteration 4/1000 | Loss: 0.00003170
Iteration 5/1000 | Loss: 0.00002924
Iteration 6/1000 | Loss: 0.00002749
Iteration 7/1000 | Loss: 0.00002619
Iteration 8/1000 | Loss: 0.00002529
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002414
Iteration 11/1000 | Loss: 0.00002369
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002310
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002271
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002243
Iteration 18/1000 | Loss: 0.00002230
Iteration 19/1000 | Loss: 0.00002228
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002214
Iteration 22/1000 | Loss: 0.00002210
Iteration 23/1000 | Loss: 0.00002210
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002205
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002198
Iteration 31/1000 | Loss: 0.00002198
Iteration 32/1000 | Loss: 0.00002196
Iteration 33/1000 | Loss: 0.00002196
Iteration 34/1000 | Loss: 0.00002195
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002194
Iteration 38/1000 | Loss: 0.00002193
Iteration 39/1000 | Loss: 0.00002192
Iteration 40/1000 | Loss: 0.00002192
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002189
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002185
Iteration 59/1000 | Loss: 0.00002185
Iteration 60/1000 | Loss: 0.00002185
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002183
Iteration 63/1000 | Loss: 0.00002183
Iteration 64/1000 | Loss: 0.00002183
Iteration 65/1000 | Loss: 0.00002183
Iteration 66/1000 | Loss: 0.00002182
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002177
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002175
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002174
Iteration 89/1000 | Loss: 0.00002174
Iteration 90/1000 | Loss: 0.00002174
Iteration 91/1000 | Loss: 0.00002174
Iteration 92/1000 | Loss: 0.00002174
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002174
Iteration 95/1000 | Loss: 0.00002174
Iteration 96/1000 | Loss: 0.00002174
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002170
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002170
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002169
Iteration 109/1000 | Loss: 0.00002169
Iteration 110/1000 | Loss: 0.00002169
Iteration 111/1000 | Loss: 0.00002169
Iteration 112/1000 | Loss: 0.00002169
Iteration 113/1000 | Loss: 0.00002169
Iteration 114/1000 | Loss: 0.00002168
Iteration 115/1000 | Loss: 0.00002168
Iteration 116/1000 | Loss: 0.00002168
Iteration 117/1000 | Loss: 0.00002168
Iteration 118/1000 | Loss: 0.00002168
Iteration 119/1000 | Loss: 0.00002168
Iteration 120/1000 | Loss: 0.00002167
Iteration 121/1000 | Loss: 0.00002167
Iteration 122/1000 | Loss: 0.00002166
Iteration 123/1000 | Loss: 0.00002166
Iteration 124/1000 | Loss: 0.00002166
Iteration 125/1000 | Loss: 0.00002165
Iteration 126/1000 | Loss: 0.00002165
Iteration 127/1000 | Loss: 0.00002165
Iteration 128/1000 | Loss: 0.00002164
Iteration 129/1000 | Loss: 0.00002164
Iteration 130/1000 | Loss: 0.00002164
Iteration 131/1000 | Loss: 0.00002163
Iteration 132/1000 | Loss: 0.00002163
Iteration 133/1000 | Loss: 0.00002163
Iteration 134/1000 | Loss: 0.00002162
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002162
Iteration 137/1000 | Loss: 0.00002162
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002159
Iteration 147/1000 | Loss: 0.00002159
Iteration 148/1000 | Loss: 0.00002159
Iteration 149/1000 | Loss: 0.00002159
Iteration 150/1000 | Loss: 0.00002158
Iteration 151/1000 | Loss: 0.00002158
Iteration 152/1000 | Loss: 0.00002158
Iteration 153/1000 | Loss: 0.00002158
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002158
Iteration 158/1000 | Loss: 0.00002158
Iteration 159/1000 | Loss: 0.00002158
Iteration 160/1000 | Loss: 0.00002158
Iteration 161/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.1579413441941142e-05, 2.1579413441941142e-05, 2.1579413441941142e-05, 2.1579413441941142e-05, 2.1579413441941142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1579413441941142e-05

Optimization complete. Final v2v error: 3.7844717502593994 mm

Highest mean error: 6.162346363067627 mm for frame 120

Lowest mean error: 2.9283411502838135 mm for frame 12

Saving results

Total time: 68.71634292602539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388061
Iteration 2/25 | Loss: 0.00134396
Iteration 3/25 | Loss: 0.00127601
Iteration 4/25 | Loss: 0.00126752
Iteration 5/25 | Loss: 0.00126451
Iteration 6/25 | Loss: 0.00126416
Iteration 7/25 | Loss: 0.00126416
Iteration 8/25 | Loss: 0.00126416
Iteration 9/25 | Loss: 0.00126416
Iteration 10/25 | Loss: 0.00126416
Iteration 11/25 | Loss: 0.00126416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012641622452065349, 0.0012641622452065349, 0.0012641622452065349, 0.0012641622452065349, 0.0012641622452065349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012641622452065349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74542356
Iteration 2/25 | Loss: 0.00133445
Iteration 3/25 | Loss: 0.00133444
Iteration 4/25 | Loss: 0.00133444
Iteration 5/25 | Loss: 0.00133444
Iteration 6/25 | Loss: 0.00133444
Iteration 7/25 | Loss: 0.00133444
Iteration 8/25 | Loss: 0.00133444
Iteration 9/25 | Loss: 0.00133444
Iteration 10/25 | Loss: 0.00133444
Iteration 11/25 | Loss: 0.00133444
Iteration 12/25 | Loss: 0.00133444
Iteration 13/25 | Loss: 0.00133444
Iteration 14/25 | Loss: 0.00133444
Iteration 15/25 | Loss: 0.00133444
Iteration 16/25 | Loss: 0.00133444
Iteration 17/25 | Loss: 0.00133444
Iteration 18/25 | Loss: 0.00133444
Iteration 19/25 | Loss: 0.00133444
Iteration 20/25 | Loss: 0.00133444
Iteration 21/25 | Loss: 0.00133444
Iteration 22/25 | Loss: 0.00133444
Iteration 23/25 | Loss: 0.00133444
Iteration 24/25 | Loss: 0.00133444
Iteration 25/25 | Loss: 0.00133444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133444
Iteration 2/1000 | Loss: 0.00002192
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001243
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001112
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001093
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001062
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001061
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001055
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001041
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001036
Iteration 44/1000 | Loss: 0.00001035
Iteration 45/1000 | Loss: 0.00001035
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001030
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001029
Iteration 72/1000 | Loss: 0.00001029
Iteration 73/1000 | Loss: 0.00001029
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001026
Iteration 89/1000 | Loss: 0.00001026
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001025
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001024
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001021
Iteration 105/1000 | Loss: 0.00001021
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001019
Iteration 138/1000 | Loss: 0.00001019
Iteration 139/1000 | Loss: 0.00001019
Iteration 140/1000 | Loss: 0.00001019
Iteration 141/1000 | Loss: 0.00001019
Iteration 142/1000 | Loss: 0.00001019
Iteration 143/1000 | Loss: 0.00001019
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001019
Iteration 149/1000 | Loss: 0.00001019
Iteration 150/1000 | Loss: 0.00001019
Iteration 151/1000 | Loss: 0.00001019
Iteration 152/1000 | Loss: 0.00001019
Iteration 153/1000 | Loss: 0.00001019
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001019
Iteration 170/1000 | Loss: 0.00001019
Iteration 171/1000 | Loss: 0.00001019
Iteration 172/1000 | Loss: 0.00001019
Iteration 173/1000 | Loss: 0.00001019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.0188560736423824e-05, 1.0188560736423824e-05, 1.0188560736423824e-05, 1.0188560736423824e-05, 1.0188560736423824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0188560736423824e-05

Optimization complete. Final v2v error: 2.786545753479004 mm

Highest mean error: 2.953359603881836 mm for frame 131

Lowest mean error: 2.7032406330108643 mm for frame 29

Saving results

Total time: 39.64671754837036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818484
Iteration 2/25 | Loss: 0.00141123
Iteration 3/25 | Loss: 0.00130595
Iteration 4/25 | Loss: 0.00129534
Iteration 5/25 | Loss: 0.00129331
Iteration 6/25 | Loss: 0.00129307
Iteration 7/25 | Loss: 0.00129307
Iteration 8/25 | Loss: 0.00129307
Iteration 9/25 | Loss: 0.00129307
Iteration 10/25 | Loss: 0.00129307
Iteration 11/25 | Loss: 0.00129307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012930688681080937, 0.0012930688681080937, 0.0012930688681080937, 0.0012930688681080937, 0.0012930688681080937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012930688681080937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29608560
Iteration 2/25 | Loss: 0.00121335
Iteration 3/25 | Loss: 0.00121333
Iteration 4/25 | Loss: 0.00121333
Iteration 5/25 | Loss: 0.00121333
Iteration 6/25 | Loss: 0.00121333
Iteration 7/25 | Loss: 0.00121333
Iteration 8/25 | Loss: 0.00121333
Iteration 9/25 | Loss: 0.00121333
Iteration 10/25 | Loss: 0.00121333
Iteration 11/25 | Loss: 0.00121332
Iteration 12/25 | Loss: 0.00121332
Iteration 13/25 | Loss: 0.00121332
Iteration 14/25 | Loss: 0.00121332
Iteration 15/25 | Loss: 0.00121332
Iteration 16/25 | Loss: 0.00121332
Iteration 17/25 | Loss: 0.00121332
Iteration 18/25 | Loss: 0.00121332
Iteration 19/25 | Loss: 0.00121332
Iteration 20/25 | Loss: 0.00121332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012133243726566434, 0.0012133243726566434, 0.0012133243726566434, 0.0012133243726566434, 0.0012133243726566434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012133243726566434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121332
Iteration 2/1000 | Loss: 0.00002322
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001268
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001111
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001097
Iteration 15/1000 | Loss: 0.00001095
Iteration 16/1000 | Loss: 0.00001091
Iteration 17/1000 | Loss: 0.00001089
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001083
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001068
Iteration 25/1000 | Loss: 0.00001067
Iteration 26/1000 | Loss: 0.00001067
Iteration 27/1000 | Loss: 0.00001066
Iteration 28/1000 | Loss: 0.00001066
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001060
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001059
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001057
Iteration 49/1000 | Loss: 0.00001057
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001057
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001055
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001054
Iteration 61/1000 | Loss: 0.00001054
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001051
Iteration 65/1000 | Loss: 0.00001051
Iteration 66/1000 | Loss: 0.00001050
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001049
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001041
Iteration 97/1000 | Loss: 0.00001041
Iteration 98/1000 | Loss: 0.00001040
Iteration 99/1000 | Loss: 0.00001040
Iteration 100/1000 | Loss: 0.00001040
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001039
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001038
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001038
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001037
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001035
Iteration 117/1000 | Loss: 0.00001035
Iteration 118/1000 | Loss: 0.00001035
Iteration 119/1000 | Loss: 0.00001035
Iteration 120/1000 | Loss: 0.00001035
Iteration 121/1000 | Loss: 0.00001034
Iteration 122/1000 | Loss: 0.00001034
Iteration 123/1000 | Loss: 0.00001034
Iteration 124/1000 | Loss: 0.00001034
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001033
Iteration 127/1000 | Loss: 0.00001033
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001030
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001030
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001029
Iteration 156/1000 | Loss: 0.00001029
Iteration 157/1000 | Loss: 0.00001029
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001028
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001027
Iteration 168/1000 | Loss: 0.00001027
Iteration 169/1000 | Loss: 0.00001027
Iteration 170/1000 | Loss: 0.00001027
Iteration 171/1000 | Loss: 0.00001027
Iteration 172/1000 | Loss: 0.00001027
Iteration 173/1000 | Loss: 0.00001027
Iteration 174/1000 | Loss: 0.00001027
Iteration 175/1000 | Loss: 0.00001027
Iteration 176/1000 | Loss: 0.00001027
Iteration 177/1000 | Loss: 0.00001027
Iteration 178/1000 | Loss: 0.00001027
Iteration 179/1000 | Loss: 0.00001027
Iteration 180/1000 | Loss: 0.00001027
Iteration 181/1000 | Loss: 0.00001027
Iteration 182/1000 | Loss: 0.00001027
Iteration 183/1000 | Loss: 0.00001027
Iteration 184/1000 | Loss: 0.00001027
Iteration 185/1000 | Loss: 0.00001026
Iteration 186/1000 | Loss: 0.00001026
Iteration 187/1000 | Loss: 0.00001026
Iteration 188/1000 | Loss: 0.00001026
Iteration 189/1000 | Loss: 0.00001026
Iteration 190/1000 | Loss: 0.00001026
Iteration 191/1000 | Loss: 0.00001026
Iteration 192/1000 | Loss: 0.00001026
Iteration 193/1000 | Loss: 0.00001026
Iteration 194/1000 | Loss: 0.00001026
Iteration 195/1000 | Loss: 0.00001026
Iteration 196/1000 | Loss: 0.00001026
Iteration 197/1000 | Loss: 0.00001026
Iteration 198/1000 | Loss: 0.00001025
Iteration 199/1000 | Loss: 0.00001025
Iteration 200/1000 | Loss: 0.00001025
Iteration 201/1000 | Loss: 0.00001025
Iteration 202/1000 | Loss: 0.00001025
Iteration 203/1000 | Loss: 0.00001025
Iteration 204/1000 | Loss: 0.00001025
Iteration 205/1000 | Loss: 0.00001025
Iteration 206/1000 | Loss: 0.00001025
Iteration 207/1000 | Loss: 0.00001025
Iteration 208/1000 | Loss: 0.00001025
Iteration 209/1000 | Loss: 0.00001025
Iteration 210/1000 | Loss: 0.00001025
Iteration 211/1000 | Loss: 0.00001025
Iteration 212/1000 | Loss: 0.00001025
Iteration 213/1000 | Loss: 0.00001025
Iteration 214/1000 | Loss: 0.00001025
Iteration 215/1000 | Loss: 0.00001025
Iteration 216/1000 | Loss: 0.00001025
Iteration 217/1000 | Loss: 0.00001025
Iteration 218/1000 | Loss: 0.00001025
Iteration 219/1000 | Loss: 0.00001025
Iteration 220/1000 | Loss: 0.00001025
Iteration 221/1000 | Loss: 0.00001025
Iteration 222/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.024830180540448e-05, 1.024830180540448e-05, 1.024830180540448e-05, 1.024830180540448e-05, 1.024830180540448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.024830180540448e-05

Optimization complete. Final v2v error: 2.769268274307251 mm

Highest mean error: 3.000952959060669 mm for frame 34

Lowest mean error: 2.6342713832855225 mm for frame 110

Saving results

Total time: 41.16672205924988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408557
Iteration 2/25 | Loss: 0.00137780
Iteration 3/25 | Loss: 0.00131166
Iteration 4/25 | Loss: 0.00130529
Iteration 5/25 | Loss: 0.00130442
Iteration 6/25 | Loss: 0.00130442
Iteration 7/25 | Loss: 0.00130442
Iteration 8/25 | Loss: 0.00130442
Iteration 9/25 | Loss: 0.00130442
Iteration 10/25 | Loss: 0.00130442
Iteration 11/25 | Loss: 0.00130442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013044169172644615, 0.0013044169172644615, 0.0013044169172644615, 0.0013044169172644615, 0.0013044169172644615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013044169172644615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27596927
Iteration 2/25 | Loss: 0.00172493
Iteration 3/25 | Loss: 0.00172490
Iteration 4/25 | Loss: 0.00172490
Iteration 5/25 | Loss: 0.00172490
Iteration 6/25 | Loss: 0.00172490
Iteration 7/25 | Loss: 0.00172490
Iteration 8/25 | Loss: 0.00172490
Iteration 9/25 | Loss: 0.00172490
Iteration 10/25 | Loss: 0.00172490
Iteration 11/25 | Loss: 0.00172490
Iteration 12/25 | Loss: 0.00172489
Iteration 13/25 | Loss: 0.00172489
Iteration 14/25 | Loss: 0.00172489
Iteration 15/25 | Loss: 0.00172489
Iteration 16/25 | Loss: 0.00172489
Iteration 17/25 | Loss: 0.00172489
Iteration 18/25 | Loss: 0.00172489
Iteration 19/25 | Loss: 0.00172489
Iteration 20/25 | Loss: 0.00172489
Iteration 21/25 | Loss: 0.00172489
Iteration 22/25 | Loss: 0.00172489
Iteration 23/25 | Loss: 0.00172489
Iteration 24/25 | Loss: 0.00172489
Iteration 25/25 | Loss: 0.00172489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172489
Iteration 2/1000 | Loss: 0.00003208
Iteration 3/1000 | Loss: 0.00002160
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001543
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001362
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001147
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001146
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001145
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001143
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001142
Iteration 113/1000 | Loss: 0.00001142
Iteration 114/1000 | Loss: 0.00001142
Iteration 115/1000 | Loss: 0.00001142
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001140
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001138
Iteration 153/1000 | Loss: 0.00001138
Iteration 154/1000 | Loss: 0.00001138
Iteration 155/1000 | Loss: 0.00001138
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001136
Iteration 164/1000 | Loss: 0.00001136
Iteration 165/1000 | Loss: 0.00001136
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001136
Iteration 168/1000 | Loss: 0.00001136
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001135
Iteration 176/1000 | Loss: 0.00001135
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001134
Iteration 180/1000 | Loss: 0.00001134
Iteration 181/1000 | Loss: 0.00001134
Iteration 182/1000 | Loss: 0.00001134
Iteration 183/1000 | Loss: 0.00001134
Iteration 184/1000 | Loss: 0.00001134
Iteration 185/1000 | Loss: 0.00001134
Iteration 186/1000 | Loss: 0.00001134
Iteration 187/1000 | Loss: 0.00001134
Iteration 188/1000 | Loss: 0.00001134
Iteration 189/1000 | Loss: 0.00001134
Iteration 190/1000 | Loss: 0.00001134
Iteration 191/1000 | Loss: 0.00001134
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001133
Iteration 195/1000 | Loss: 0.00001133
Iteration 196/1000 | Loss: 0.00001133
Iteration 197/1000 | Loss: 0.00001133
Iteration 198/1000 | Loss: 0.00001133
Iteration 199/1000 | Loss: 0.00001133
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001132
Iteration 213/1000 | Loss: 0.00001132
Iteration 214/1000 | Loss: 0.00001132
Iteration 215/1000 | Loss: 0.00001132
Iteration 216/1000 | Loss: 0.00001132
Iteration 217/1000 | Loss: 0.00001132
Iteration 218/1000 | Loss: 0.00001132
Iteration 219/1000 | Loss: 0.00001132
Iteration 220/1000 | Loss: 0.00001132
Iteration 221/1000 | Loss: 0.00001132
Iteration 222/1000 | Loss: 0.00001132
Iteration 223/1000 | Loss: 0.00001132
Iteration 224/1000 | Loss: 0.00001132
Iteration 225/1000 | Loss: 0.00001132
Iteration 226/1000 | Loss: 0.00001132
Iteration 227/1000 | Loss: 0.00001132
Iteration 228/1000 | Loss: 0.00001132
Iteration 229/1000 | Loss: 0.00001132
Iteration 230/1000 | Loss: 0.00001132
Iteration 231/1000 | Loss: 0.00001132
Iteration 232/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.1317105418129358e-05, 1.1317105418129358e-05, 1.1317105418129358e-05, 1.1317105418129358e-05, 1.1317105418129358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1317105418129358e-05

Optimization complete. Final v2v error: 2.8880691528320312 mm

Highest mean error: 3.122525930404663 mm for frame 84

Lowest mean error: 2.7269303798675537 mm for frame 18

Saving results

Total time: 46.023085594177246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971642
Iteration 2/25 | Loss: 0.00189071
Iteration 3/25 | Loss: 0.00155595
Iteration 4/25 | Loss: 0.00156810
Iteration 5/25 | Loss: 0.00165803
Iteration 6/25 | Loss: 0.00149136
Iteration 7/25 | Loss: 0.00150264
Iteration 8/25 | Loss: 0.00142938
Iteration 9/25 | Loss: 0.00140273
Iteration 10/25 | Loss: 0.00144787
Iteration 11/25 | Loss: 0.00139587
Iteration 12/25 | Loss: 0.00139340
Iteration 13/25 | Loss: 0.00139329
Iteration 14/25 | Loss: 0.00139328
Iteration 15/25 | Loss: 0.00139319
Iteration 16/25 | Loss: 0.00139317
Iteration 17/25 | Loss: 0.00139317
Iteration 18/25 | Loss: 0.00139317
Iteration 19/25 | Loss: 0.00139317
Iteration 20/25 | Loss: 0.00139316
Iteration 21/25 | Loss: 0.00139316
Iteration 22/25 | Loss: 0.00139316
Iteration 23/25 | Loss: 0.00139316
Iteration 24/25 | Loss: 0.00139316
Iteration 25/25 | Loss: 0.00139316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85497606
Iteration 2/25 | Loss: 0.00122532
Iteration 3/25 | Loss: 0.00122531
Iteration 4/25 | Loss: 0.00122531
Iteration 5/25 | Loss: 0.00122531
Iteration 6/25 | Loss: 0.00122531
Iteration 7/25 | Loss: 0.00122531
Iteration 8/25 | Loss: 0.00122531
Iteration 9/25 | Loss: 0.00122531
Iteration 10/25 | Loss: 0.00122531
Iteration 11/25 | Loss: 0.00122531
Iteration 12/25 | Loss: 0.00122531
Iteration 13/25 | Loss: 0.00122531
Iteration 14/25 | Loss: 0.00122531
Iteration 15/25 | Loss: 0.00122531
Iteration 16/25 | Loss: 0.00122531
Iteration 17/25 | Loss: 0.00122531
Iteration 18/25 | Loss: 0.00122531
Iteration 19/25 | Loss: 0.00122531
Iteration 20/25 | Loss: 0.00122531
Iteration 21/25 | Loss: 0.00122531
Iteration 22/25 | Loss: 0.00122531
Iteration 23/25 | Loss: 0.00122531
Iteration 24/25 | Loss: 0.00122531
Iteration 25/25 | Loss: 0.00122531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122531
Iteration 2/1000 | Loss: 0.00004517
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002888
Iteration 5/1000 | Loss: 0.00002707
Iteration 6/1000 | Loss: 0.00002621
Iteration 7/1000 | Loss: 0.00002545
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002428
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002353
Iteration 12/1000 | Loss: 0.00002326
Iteration 13/1000 | Loss: 0.00002308
Iteration 14/1000 | Loss: 0.00031952
Iteration 15/1000 | Loss: 0.00034837
Iteration 16/1000 | Loss: 0.00012818
Iteration 17/1000 | Loss: 0.00003825
Iteration 18/1000 | Loss: 0.00003634
Iteration 19/1000 | Loss: 0.00003882
Iteration 20/1000 | Loss: 0.00007291
Iteration 21/1000 | Loss: 0.00002129
Iteration 22/1000 | Loss: 0.00004108
Iteration 23/1000 | Loss: 0.00001981
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00001894
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00011301
Iteration 28/1000 | Loss: 0.00039266
Iteration 29/1000 | Loss: 0.00027267
Iteration 30/1000 | Loss: 0.00003507
Iteration 31/1000 | Loss: 0.00002517
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00001901
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001767
Iteration 43/1000 | Loss: 0.00001767
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001765
Iteration 47/1000 | Loss: 0.00001765
Iteration 48/1000 | Loss: 0.00001765
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001757
Iteration 67/1000 | Loss: 0.00001757
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001754
Iteration 85/1000 | Loss: 0.00001754
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001752
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001748
Iteration 129/1000 | Loss: 0.00001748
Iteration 130/1000 | Loss: 0.00001748
Iteration 131/1000 | Loss: 0.00001748
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001748
Iteration 136/1000 | Loss: 0.00001748
Iteration 137/1000 | Loss: 0.00001748
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.7477254004916176e-05, 1.7477254004916176e-05, 1.7477254004916176e-05, 1.7477254004916176e-05, 1.7477254004916176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7477254004916176e-05

Optimization complete. Final v2v error: 3.5393261909484863 mm

Highest mean error: 3.944253444671631 mm for frame 73

Lowest mean error: 3.270564556121826 mm for frame 120

Saving results

Total time: 82.54159450531006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845877
Iteration 2/25 | Loss: 0.00141532
Iteration 3/25 | Loss: 0.00133166
Iteration 4/25 | Loss: 0.00131846
Iteration 5/25 | Loss: 0.00131521
Iteration 6/25 | Loss: 0.00131521
Iteration 7/25 | Loss: 0.00131521
Iteration 8/25 | Loss: 0.00131521
Iteration 9/25 | Loss: 0.00131521
Iteration 10/25 | Loss: 0.00131521
Iteration 11/25 | Loss: 0.00131521
Iteration 12/25 | Loss: 0.00131521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013152083847671747, 0.0013152083847671747, 0.0013152083847671747, 0.0013152083847671747, 0.0013152083847671747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013152083847671747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97493124
Iteration 2/25 | Loss: 0.00124835
Iteration 3/25 | Loss: 0.00124835
Iteration 4/25 | Loss: 0.00124835
Iteration 5/25 | Loss: 0.00124835
Iteration 6/25 | Loss: 0.00124835
Iteration 7/25 | Loss: 0.00124835
Iteration 8/25 | Loss: 0.00124835
Iteration 9/25 | Loss: 0.00124835
Iteration 10/25 | Loss: 0.00124835
Iteration 11/25 | Loss: 0.00124835
Iteration 12/25 | Loss: 0.00124835
Iteration 13/25 | Loss: 0.00124835
Iteration 14/25 | Loss: 0.00124835
Iteration 15/25 | Loss: 0.00124835
Iteration 16/25 | Loss: 0.00124835
Iteration 17/25 | Loss: 0.00124835
Iteration 18/25 | Loss: 0.00124835
Iteration 19/25 | Loss: 0.00124835
Iteration 20/25 | Loss: 0.00124835
Iteration 21/25 | Loss: 0.00124835
Iteration 22/25 | Loss: 0.00124835
Iteration 23/25 | Loss: 0.00124835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012483480386435986, 0.0012483480386435986, 0.0012483480386435986, 0.0012483480386435986, 0.0012483480386435986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012483480386435986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124835
Iteration 2/1000 | Loss: 0.00002665
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001798
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001649
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001606
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001540
Iteration 18/1000 | Loss: 0.00001539
Iteration 19/1000 | Loss: 0.00001533
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001531
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001518
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001517
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001489
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001488
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001487
Iteration 51/1000 | Loss: 0.00001487
Iteration 52/1000 | Loss: 0.00001487
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00001486
Iteration 55/1000 | Loss: 0.00001486
Iteration 56/1000 | Loss: 0.00001485
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001485
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001485
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001484
Iteration 68/1000 | Loss: 0.00001484
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001482
Iteration 112/1000 | Loss: 0.00001482
Iteration 113/1000 | Loss: 0.00001482
Iteration 114/1000 | Loss: 0.00001482
Iteration 115/1000 | Loss: 0.00001482
Iteration 116/1000 | Loss: 0.00001482
Iteration 117/1000 | Loss: 0.00001482
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4815217582508922e-05, 1.4815217582508922e-05, 1.4815217582508922e-05, 1.4815217582508922e-05, 1.4815217582508922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4815217582508922e-05

Optimization complete. Final v2v error: 3.3001081943511963 mm

Highest mean error: 3.561856508255005 mm for frame 199

Lowest mean error: 3.052616596221924 mm for frame 4

Saving results

Total time: 39.062483072280884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00617510
Iteration 2/25 | Loss: 0.00162823
Iteration 3/25 | Loss: 0.00140153
Iteration 4/25 | Loss: 0.00138650
Iteration 5/25 | Loss: 0.00138375
Iteration 6/25 | Loss: 0.00138351
Iteration 7/25 | Loss: 0.00138351
Iteration 8/25 | Loss: 0.00138351
Iteration 9/25 | Loss: 0.00138351
Iteration 10/25 | Loss: 0.00138351
Iteration 11/25 | Loss: 0.00138351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013835052959620953, 0.0013835052959620953, 0.0013835052959620953, 0.0013835052959620953, 0.0013835052959620953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013835052959620953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12246048
Iteration 2/25 | Loss: 0.00106038
Iteration 3/25 | Loss: 0.00106035
Iteration 4/25 | Loss: 0.00106035
Iteration 5/25 | Loss: 0.00106035
Iteration 6/25 | Loss: 0.00106035
Iteration 7/25 | Loss: 0.00106035
Iteration 8/25 | Loss: 0.00106035
Iteration 9/25 | Loss: 0.00106035
Iteration 10/25 | Loss: 0.00106035
Iteration 11/25 | Loss: 0.00106035
Iteration 12/25 | Loss: 0.00106035
Iteration 13/25 | Loss: 0.00106035
Iteration 14/25 | Loss: 0.00106035
Iteration 15/25 | Loss: 0.00106035
Iteration 16/25 | Loss: 0.00106035
Iteration 17/25 | Loss: 0.00106035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010603523114696145, 0.0010603523114696145, 0.0010603523114696145, 0.0010603523114696145, 0.0010603523114696145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010603523114696145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106035
Iteration 2/1000 | Loss: 0.00004455
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00001983
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001822
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001768
Iteration 24/1000 | Loss: 0.00001767
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001743
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001735
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001731
Iteration 58/1000 | Loss: 0.00001731
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001728
Iteration 73/1000 | Loss: 0.00001728
Iteration 74/1000 | Loss: 0.00001728
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.728151437419001e-05, 1.728151437419001e-05, 1.728151437419001e-05, 1.728151437419001e-05, 1.728151437419001e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728151437419001e-05

Optimization complete. Final v2v error: 3.528914451599121 mm

Highest mean error: 4.126523494720459 mm for frame 52

Lowest mean error: 3.37107515335083 mm for frame 0

Saving results

Total time: 35.23324370384216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842463
Iteration 2/25 | Loss: 0.00152332
Iteration 3/25 | Loss: 0.00141695
Iteration 4/25 | Loss: 0.00140038
Iteration 5/25 | Loss: 0.00139651
Iteration 6/25 | Loss: 0.00139628
Iteration 7/25 | Loss: 0.00139628
Iteration 8/25 | Loss: 0.00139628
Iteration 9/25 | Loss: 0.00139628
Iteration 10/25 | Loss: 0.00139628
Iteration 11/25 | Loss: 0.00139628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013962836237624288, 0.0013962836237624288, 0.0013962836237624288, 0.0013962836237624288, 0.0013962836237624288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013962836237624288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25331414
Iteration 2/25 | Loss: 0.00154871
Iteration 3/25 | Loss: 0.00154871
Iteration 4/25 | Loss: 0.00154871
Iteration 5/25 | Loss: 0.00154871
Iteration 6/25 | Loss: 0.00154871
Iteration 7/25 | Loss: 0.00154870
Iteration 8/25 | Loss: 0.00154870
Iteration 9/25 | Loss: 0.00154870
Iteration 10/25 | Loss: 0.00154870
Iteration 11/25 | Loss: 0.00154870
Iteration 12/25 | Loss: 0.00154870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015487048076465726, 0.0015487048076465726, 0.0015487048076465726, 0.0015487048076465726, 0.0015487048076465726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015487048076465726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154870
Iteration 2/1000 | Loss: 0.00006159
Iteration 3/1000 | Loss: 0.00004936
Iteration 4/1000 | Loss: 0.00004190
Iteration 5/1000 | Loss: 0.00003965
Iteration 6/1000 | Loss: 0.00003823
Iteration 7/1000 | Loss: 0.00003715
Iteration 8/1000 | Loss: 0.00003635
Iteration 9/1000 | Loss: 0.00003582
Iteration 10/1000 | Loss: 0.00003541
Iteration 11/1000 | Loss: 0.00003510
Iteration 12/1000 | Loss: 0.00003478
Iteration 13/1000 | Loss: 0.00003451
Iteration 14/1000 | Loss: 0.00003429
Iteration 15/1000 | Loss: 0.00003414
Iteration 16/1000 | Loss: 0.00003412
Iteration 17/1000 | Loss: 0.00003408
Iteration 18/1000 | Loss: 0.00003400
Iteration 19/1000 | Loss: 0.00003386
Iteration 20/1000 | Loss: 0.00003385
Iteration 21/1000 | Loss: 0.00003385
Iteration 22/1000 | Loss: 0.00003384
Iteration 23/1000 | Loss: 0.00003384
Iteration 24/1000 | Loss: 0.00003384
Iteration 25/1000 | Loss: 0.00003384
Iteration 26/1000 | Loss: 0.00003384
Iteration 27/1000 | Loss: 0.00003384
Iteration 28/1000 | Loss: 0.00003384
Iteration 29/1000 | Loss: 0.00003384
Iteration 30/1000 | Loss: 0.00003384
Iteration 31/1000 | Loss: 0.00003384
Iteration 32/1000 | Loss: 0.00003384
Iteration 33/1000 | Loss: 0.00003383
Iteration 34/1000 | Loss: 0.00003382
Iteration 35/1000 | Loss: 0.00003380
Iteration 36/1000 | Loss: 0.00003380
Iteration 37/1000 | Loss: 0.00003380
Iteration 38/1000 | Loss: 0.00003379
Iteration 39/1000 | Loss: 0.00003379
Iteration 40/1000 | Loss: 0.00003378
Iteration 41/1000 | Loss: 0.00003376
Iteration 42/1000 | Loss: 0.00003375
Iteration 43/1000 | Loss: 0.00003375
Iteration 44/1000 | Loss: 0.00003374
Iteration 45/1000 | Loss: 0.00003374
Iteration 46/1000 | Loss: 0.00003373
Iteration 47/1000 | Loss: 0.00003372
Iteration 48/1000 | Loss: 0.00003372
Iteration 49/1000 | Loss: 0.00003372
Iteration 50/1000 | Loss: 0.00003371
Iteration 51/1000 | Loss: 0.00003371
Iteration 52/1000 | Loss: 0.00003371
Iteration 53/1000 | Loss: 0.00003371
Iteration 54/1000 | Loss: 0.00003371
Iteration 55/1000 | Loss: 0.00003371
Iteration 56/1000 | Loss: 0.00003371
Iteration 57/1000 | Loss: 0.00003370
Iteration 58/1000 | Loss: 0.00003370
Iteration 59/1000 | Loss: 0.00003370
Iteration 60/1000 | Loss: 0.00003369
Iteration 61/1000 | Loss: 0.00003369
Iteration 62/1000 | Loss: 0.00003369
Iteration 63/1000 | Loss: 0.00003368
Iteration 64/1000 | Loss: 0.00003368
Iteration 65/1000 | Loss: 0.00003368
Iteration 66/1000 | Loss: 0.00003368
Iteration 67/1000 | Loss: 0.00003367
Iteration 68/1000 | Loss: 0.00003367
Iteration 69/1000 | Loss: 0.00003367
Iteration 70/1000 | Loss: 0.00003366
Iteration 71/1000 | Loss: 0.00003366
Iteration 72/1000 | Loss: 0.00003365
Iteration 73/1000 | Loss: 0.00003365
Iteration 74/1000 | Loss: 0.00003365
Iteration 75/1000 | Loss: 0.00003364
Iteration 76/1000 | Loss: 0.00003364
Iteration 77/1000 | Loss: 0.00003364
Iteration 78/1000 | Loss: 0.00003364
Iteration 79/1000 | Loss: 0.00003364
Iteration 80/1000 | Loss: 0.00003364
Iteration 81/1000 | Loss: 0.00003364
Iteration 82/1000 | Loss: 0.00003363
Iteration 83/1000 | Loss: 0.00003363
Iteration 84/1000 | Loss: 0.00003363
Iteration 85/1000 | Loss: 0.00003362
Iteration 86/1000 | Loss: 0.00003362
Iteration 87/1000 | Loss: 0.00003361
Iteration 88/1000 | Loss: 0.00003361
Iteration 89/1000 | Loss: 0.00003361
Iteration 90/1000 | Loss: 0.00003361
Iteration 91/1000 | Loss: 0.00003361
Iteration 92/1000 | Loss: 0.00003361
Iteration 93/1000 | Loss: 0.00003360
Iteration 94/1000 | Loss: 0.00003360
Iteration 95/1000 | Loss: 0.00003360
Iteration 96/1000 | Loss: 0.00003359
Iteration 97/1000 | Loss: 0.00003359
Iteration 98/1000 | Loss: 0.00003358
Iteration 99/1000 | Loss: 0.00003358
Iteration 100/1000 | Loss: 0.00003358
Iteration 101/1000 | Loss: 0.00003358
Iteration 102/1000 | Loss: 0.00003358
Iteration 103/1000 | Loss: 0.00003358
Iteration 104/1000 | Loss: 0.00003358
Iteration 105/1000 | Loss: 0.00003358
Iteration 106/1000 | Loss: 0.00003357
Iteration 107/1000 | Loss: 0.00003357
Iteration 108/1000 | Loss: 0.00003357
Iteration 109/1000 | Loss: 0.00003357
Iteration 110/1000 | Loss: 0.00003357
Iteration 111/1000 | Loss: 0.00003357
Iteration 112/1000 | Loss: 0.00003357
Iteration 113/1000 | Loss: 0.00003357
Iteration 114/1000 | Loss: 0.00003357
Iteration 115/1000 | Loss: 0.00003357
Iteration 116/1000 | Loss: 0.00003357
Iteration 117/1000 | Loss: 0.00003357
Iteration 118/1000 | Loss: 0.00003357
Iteration 119/1000 | Loss: 0.00003357
Iteration 120/1000 | Loss: 0.00003357
Iteration 121/1000 | Loss: 0.00003357
Iteration 122/1000 | Loss: 0.00003357
Iteration 123/1000 | Loss: 0.00003357
Iteration 124/1000 | Loss: 0.00003357
Iteration 125/1000 | Loss: 0.00003357
Iteration 126/1000 | Loss: 0.00003357
Iteration 127/1000 | Loss: 0.00003357
Iteration 128/1000 | Loss: 0.00003357
Iteration 129/1000 | Loss: 0.00003357
Iteration 130/1000 | Loss: 0.00003357
Iteration 131/1000 | Loss: 0.00003357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.357011519256048e-05, 3.357011519256048e-05, 3.357011519256048e-05, 3.357011519256048e-05, 3.357011519256048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.357011519256048e-05

Optimization complete. Final v2v error: 4.825993537902832 mm

Highest mean error: 5.416715621948242 mm for frame 22

Lowest mean error: 4.309302806854248 mm for frame 87

Saving results

Total time: 38.435081243515015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847548
Iteration 2/25 | Loss: 0.00140533
Iteration 3/25 | Loss: 0.00133082
Iteration 4/25 | Loss: 0.00131755
Iteration 5/25 | Loss: 0.00131355
Iteration 6/25 | Loss: 0.00131276
Iteration 7/25 | Loss: 0.00131276
Iteration 8/25 | Loss: 0.00131276
Iteration 9/25 | Loss: 0.00131276
Iteration 10/25 | Loss: 0.00131276
Iteration 11/25 | Loss: 0.00131276
Iteration 12/25 | Loss: 0.00131276
Iteration 13/25 | Loss: 0.00131276
Iteration 14/25 | Loss: 0.00131276
Iteration 15/25 | Loss: 0.00131276
Iteration 16/25 | Loss: 0.00131276
Iteration 17/25 | Loss: 0.00131276
Iteration 18/25 | Loss: 0.00131276
Iteration 19/25 | Loss: 0.00131276
Iteration 20/25 | Loss: 0.00131276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013127592392265797, 0.0013127592392265797, 0.0013127592392265797, 0.0013127592392265797, 0.0013127592392265797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013127592392265797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27548480
Iteration 2/25 | Loss: 0.00181524
Iteration 3/25 | Loss: 0.00181523
Iteration 4/25 | Loss: 0.00181523
Iteration 5/25 | Loss: 0.00181523
Iteration 6/25 | Loss: 0.00181522
Iteration 7/25 | Loss: 0.00181522
Iteration 8/25 | Loss: 0.00181522
Iteration 9/25 | Loss: 0.00181522
Iteration 10/25 | Loss: 0.00181522
Iteration 11/25 | Loss: 0.00181522
Iteration 12/25 | Loss: 0.00181522
Iteration 13/25 | Loss: 0.00181522
Iteration 14/25 | Loss: 0.00181522
Iteration 15/25 | Loss: 0.00181522
Iteration 16/25 | Loss: 0.00181522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001815224066376686, 0.001815224066376686, 0.001815224066376686, 0.001815224066376686, 0.001815224066376686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001815224066376686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181522
Iteration 2/1000 | Loss: 0.00003212
Iteration 3/1000 | Loss: 0.00002136
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001506
Iteration 11/1000 | Loss: 0.00001486
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001410
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001408
Iteration 48/1000 | Loss: 0.00001408
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001405
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001405
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001398
Iteration 98/1000 | Loss: 0.00001398
Iteration 99/1000 | Loss: 0.00001398
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001390
Iteration 151/1000 | Loss: 0.00001390
Iteration 152/1000 | Loss: 0.00001390
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001388
Iteration 159/1000 | Loss: 0.00001388
Iteration 160/1000 | Loss: 0.00001388
Iteration 161/1000 | Loss: 0.00001388
Iteration 162/1000 | Loss: 0.00001388
Iteration 163/1000 | Loss: 0.00001388
Iteration 164/1000 | Loss: 0.00001388
Iteration 165/1000 | Loss: 0.00001388
Iteration 166/1000 | Loss: 0.00001388
Iteration 167/1000 | Loss: 0.00001388
Iteration 168/1000 | Loss: 0.00001388
Iteration 169/1000 | Loss: 0.00001388
Iteration 170/1000 | Loss: 0.00001388
Iteration 171/1000 | Loss: 0.00001388
Iteration 172/1000 | Loss: 0.00001387
Iteration 173/1000 | Loss: 0.00001387
Iteration 174/1000 | Loss: 0.00001387
Iteration 175/1000 | Loss: 0.00001387
Iteration 176/1000 | Loss: 0.00001387
Iteration 177/1000 | Loss: 0.00001387
Iteration 178/1000 | Loss: 0.00001387
Iteration 179/1000 | Loss: 0.00001387
Iteration 180/1000 | Loss: 0.00001387
Iteration 181/1000 | Loss: 0.00001387
Iteration 182/1000 | Loss: 0.00001387
Iteration 183/1000 | Loss: 0.00001387
Iteration 184/1000 | Loss: 0.00001387
Iteration 185/1000 | Loss: 0.00001387
Iteration 186/1000 | Loss: 0.00001387
Iteration 187/1000 | Loss: 0.00001387
Iteration 188/1000 | Loss: 0.00001387
Iteration 189/1000 | Loss: 0.00001387
Iteration 190/1000 | Loss: 0.00001387
Iteration 191/1000 | Loss: 0.00001387
Iteration 192/1000 | Loss: 0.00001387
Iteration 193/1000 | Loss: 0.00001387
Iteration 194/1000 | Loss: 0.00001387
Iteration 195/1000 | Loss: 0.00001387
Iteration 196/1000 | Loss: 0.00001387
Iteration 197/1000 | Loss: 0.00001387
Iteration 198/1000 | Loss: 0.00001387
Iteration 199/1000 | Loss: 0.00001387
Iteration 200/1000 | Loss: 0.00001387
Iteration 201/1000 | Loss: 0.00001387
Iteration 202/1000 | Loss: 0.00001387
Iteration 203/1000 | Loss: 0.00001387
Iteration 204/1000 | Loss: 0.00001387
Iteration 205/1000 | Loss: 0.00001387
Iteration 206/1000 | Loss: 0.00001387
Iteration 207/1000 | Loss: 0.00001387
Iteration 208/1000 | Loss: 0.00001387
Iteration 209/1000 | Loss: 0.00001387
Iteration 210/1000 | Loss: 0.00001387
Iteration 211/1000 | Loss: 0.00001387
Iteration 212/1000 | Loss: 0.00001387
Iteration 213/1000 | Loss: 0.00001387
Iteration 214/1000 | Loss: 0.00001387
Iteration 215/1000 | Loss: 0.00001387
Iteration 216/1000 | Loss: 0.00001387
Iteration 217/1000 | Loss: 0.00001387
Iteration 218/1000 | Loss: 0.00001387
Iteration 219/1000 | Loss: 0.00001387
Iteration 220/1000 | Loss: 0.00001387
Iteration 221/1000 | Loss: 0.00001387
Iteration 222/1000 | Loss: 0.00001387
Iteration 223/1000 | Loss: 0.00001387
Iteration 224/1000 | Loss: 0.00001387
Iteration 225/1000 | Loss: 0.00001387
Iteration 226/1000 | Loss: 0.00001387
Iteration 227/1000 | Loss: 0.00001387
Iteration 228/1000 | Loss: 0.00001387
Iteration 229/1000 | Loss: 0.00001387
Iteration 230/1000 | Loss: 0.00001387
Iteration 231/1000 | Loss: 0.00001387
Iteration 232/1000 | Loss: 0.00001387
Iteration 233/1000 | Loss: 0.00001387
Iteration 234/1000 | Loss: 0.00001387
Iteration 235/1000 | Loss: 0.00001387
Iteration 236/1000 | Loss: 0.00001387
Iteration 237/1000 | Loss: 0.00001387
Iteration 238/1000 | Loss: 0.00001387
Iteration 239/1000 | Loss: 0.00001387
Iteration 240/1000 | Loss: 0.00001387
Iteration 241/1000 | Loss: 0.00001387
Iteration 242/1000 | Loss: 0.00001387
Iteration 243/1000 | Loss: 0.00001387
Iteration 244/1000 | Loss: 0.00001387
Iteration 245/1000 | Loss: 0.00001387
Iteration 246/1000 | Loss: 0.00001387
Iteration 247/1000 | Loss: 0.00001387
Iteration 248/1000 | Loss: 0.00001387
Iteration 249/1000 | Loss: 0.00001387
Iteration 250/1000 | Loss: 0.00001387
Iteration 251/1000 | Loss: 0.00001387
Iteration 252/1000 | Loss: 0.00001387
Iteration 253/1000 | Loss: 0.00001387
Iteration 254/1000 | Loss: 0.00001387
Iteration 255/1000 | Loss: 0.00001387
Iteration 256/1000 | Loss: 0.00001387
Iteration 257/1000 | Loss: 0.00001387
Iteration 258/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.3866409062757157e-05, 1.3866409062757157e-05, 1.3866409062757157e-05, 1.3866409062757157e-05, 1.3866409062757157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3866409062757157e-05

Optimization complete. Final v2v error: 3.112278461456299 mm

Highest mean error: 4.069002151489258 mm for frame 39

Lowest mean error: 2.645936965942383 mm for frame 69

Saving results

Total time: 42.88802409172058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028453
Iteration 2/25 | Loss: 0.01028453
Iteration 3/25 | Loss: 0.01028452
Iteration 4/25 | Loss: 0.01028452
Iteration 5/25 | Loss: 0.01028452
Iteration 6/25 | Loss: 0.01028452
Iteration 7/25 | Loss: 0.01028452
Iteration 8/25 | Loss: 0.01028452
Iteration 9/25 | Loss: 0.01028452
Iteration 10/25 | Loss: 0.01028452
Iteration 11/25 | Loss: 0.01028452
Iteration 12/25 | Loss: 0.01028452
Iteration 13/25 | Loss: 0.01028451
Iteration 14/25 | Loss: 0.01028451
Iteration 15/25 | Loss: 0.01028451
Iteration 16/25 | Loss: 0.01028451
Iteration 17/25 | Loss: 0.01028451
Iteration 18/25 | Loss: 0.01028451
Iteration 19/25 | Loss: 0.01028451
Iteration 20/25 | Loss: 0.01028451
Iteration 21/25 | Loss: 0.01028451
Iteration 22/25 | Loss: 0.01028451
Iteration 23/25 | Loss: 0.01028450
Iteration 24/25 | Loss: 0.01028450
Iteration 25/25 | Loss: 0.01028450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45243096
Iteration 2/25 | Loss: 0.10139404
Iteration 3/25 | Loss: 0.10125443
Iteration 4/25 | Loss: 0.10095446
Iteration 5/25 | Loss: 0.10095446
Iteration 6/25 | Loss: 0.10095119
Iteration 7/25 | Loss: 0.10094398
Iteration 8/25 | Loss: 0.10094395
Iteration 9/25 | Loss: 0.10094392
Iteration 10/25 | Loss: 0.10094392
Iteration 11/25 | Loss: 0.10094392
Iteration 12/25 | Loss: 0.10094392
Iteration 13/25 | Loss: 0.10094392
Iteration 14/25 | Loss: 0.10094392
Iteration 15/25 | Loss: 0.10094392
Iteration 16/25 | Loss: 0.10094392
Iteration 17/25 | Loss: 0.10094392
Iteration 18/25 | Loss: 0.10094392
Iteration 19/25 | Loss: 0.10094392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.1009439155459404, 0.1009439155459404, 0.1009439155459404, 0.1009439155459404, 0.1009439155459404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1009439155459404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10094392
Iteration 2/1000 | Loss: 0.00049438
Iteration 3/1000 | Loss: 0.00011423
Iteration 4/1000 | Loss: 0.00009393
Iteration 5/1000 | Loss: 0.00004143
Iteration 6/1000 | Loss: 0.00019682
Iteration 7/1000 | Loss: 0.00004400
Iteration 8/1000 | Loss: 0.00005763
Iteration 9/1000 | Loss: 0.00005177
Iteration 10/1000 | Loss: 0.00002482
Iteration 11/1000 | Loss: 0.00008348
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00011239
Iteration 14/1000 | Loss: 0.00004779
Iteration 15/1000 | Loss: 0.00006737
Iteration 16/1000 | Loss: 0.00018894
Iteration 17/1000 | Loss: 0.00005996
Iteration 18/1000 | Loss: 0.00002937
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00024894
Iteration 22/1000 | Loss: 0.00048723
Iteration 23/1000 | Loss: 0.00003139
Iteration 24/1000 | Loss: 0.00010157
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00005504
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00002905
Iteration 31/1000 | Loss: 0.00003292
Iteration 32/1000 | Loss: 0.00017635
Iteration 33/1000 | Loss: 0.00004879
Iteration 34/1000 | Loss: 0.00002240
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00004723
Iteration 37/1000 | Loss: 0.00001281
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00010280
Iteration 43/1000 | Loss: 0.00018450
Iteration 44/1000 | Loss: 0.00001814
Iteration 45/1000 | Loss: 0.00002980
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00003084
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001170
Iteration 77/1000 | Loss: 0.00001170
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001157
Iteration 113/1000 | Loss: 0.00001157
Iteration 114/1000 | Loss: 0.00001157
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001156
Iteration 117/1000 | Loss: 0.00001156
Iteration 118/1000 | Loss: 0.00001156
Iteration 119/1000 | Loss: 0.00001155
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001154
Iteration 128/1000 | Loss: 0.00001154
Iteration 129/1000 | Loss: 0.00001154
Iteration 130/1000 | Loss: 0.00001154
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001152
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001152
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001150
Iteration 157/1000 | Loss: 0.00001150
Iteration 158/1000 | Loss: 0.00001150
Iteration 159/1000 | Loss: 0.00001150
Iteration 160/1000 | Loss: 0.00001150
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001150
Iteration 163/1000 | Loss: 0.00001150
Iteration 164/1000 | Loss: 0.00001150
Iteration 165/1000 | Loss: 0.00001150
Iteration 166/1000 | Loss: 0.00001150
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001149
Iteration 169/1000 | Loss: 0.00001149
Iteration 170/1000 | Loss: 0.00001149
Iteration 171/1000 | Loss: 0.00001149
Iteration 172/1000 | Loss: 0.00001149
Iteration 173/1000 | Loss: 0.00001148
Iteration 174/1000 | Loss: 0.00001148
Iteration 175/1000 | Loss: 0.00001148
Iteration 176/1000 | Loss: 0.00001148
Iteration 177/1000 | Loss: 0.00001148
Iteration 178/1000 | Loss: 0.00001148
Iteration 179/1000 | Loss: 0.00001148
Iteration 180/1000 | Loss: 0.00001147
Iteration 181/1000 | Loss: 0.00001147
Iteration 182/1000 | Loss: 0.00001147
Iteration 183/1000 | Loss: 0.00001147
Iteration 184/1000 | Loss: 0.00001147
Iteration 185/1000 | Loss: 0.00001147
Iteration 186/1000 | Loss: 0.00001147
Iteration 187/1000 | Loss: 0.00001147
Iteration 188/1000 | Loss: 0.00001147
Iteration 189/1000 | Loss: 0.00001147
Iteration 190/1000 | Loss: 0.00001147
Iteration 191/1000 | Loss: 0.00001147
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001146
Iteration 194/1000 | Loss: 0.00001146
Iteration 195/1000 | Loss: 0.00001146
Iteration 196/1000 | Loss: 0.00001146
Iteration 197/1000 | Loss: 0.00001146
Iteration 198/1000 | Loss: 0.00001146
Iteration 199/1000 | Loss: 0.00001146
Iteration 200/1000 | Loss: 0.00001146
Iteration 201/1000 | Loss: 0.00001146
Iteration 202/1000 | Loss: 0.00001146
Iteration 203/1000 | Loss: 0.00001146
Iteration 204/1000 | Loss: 0.00001146
Iteration 205/1000 | Loss: 0.00001146
Iteration 206/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.1462099791970104e-05, 1.1462099791970104e-05, 1.1462099791970104e-05, 1.1462099791970104e-05, 1.1462099791970104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1462099791970104e-05

Optimization complete. Final v2v error: 2.925402879714966 mm

Highest mean error: 3.131150484085083 mm for frame 205

Lowest mean error: 2.775059700012207 mm for frame 67

Saving results

Total time: 98.16578030586243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799878
Iteration 2/25 | Loss: 0.00138847
Iteration 3/25 | Loss: 0.00128130
Iteration 4/25 | Loss: 0.00127281
Iteration 5/25 | Loss: 0.00127087
Iteration 6/25 | Loss: 0.00127087
Iteration 7/25 | Loss: 0.00127087
Iteration 8/25 | Loss: 0.00127087
Iteration 9/25 | Loss: 0.00127087
Iteration 10/25 | Loss: 0.00127087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001270871376618743, 0.001270871376618743, 0.001270871376618743, 0.001270871376618743, 0.001270871376618743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001270871376618743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30745184
Iteration 2/25 | Loss: 0.00136225
Iteration 3/25 | Loss: 0.00136224
Iteration 4/25 | Loss: 0.00136224
Iteration 5/25 | Loss: 0.00136224
Iteration 6/25 | Loss: 0.00136224
Iteration 7/25 | Loss: 0.00136224
Iteration 8/25 | Loss: 0.00136224
Iteration 9/25 | Loss: 0.00136224
Iteration 10/25 | Loss: 0.00136224
Iteration 11/25 | Loss: 0.00136224
Iteration 12/25 | Loss: 0.00136224
Iteration 13/25 | Loss: 0.00136224
Iteration 14/25 | Loss: 0.00136224
Iteration 15/25 | Loss: 0.00136224
Iteration 16/25 | Loss: 0.00136224
Iteration 17/25 | Loss: 0.00136224
Iteration 18/25 | Loss: 0.00136224
Iteration 19/25 | Loss: 0.00136224
Iteration 20/25 | Loss: 0.00136224
Iteration 21/25 | Loss: 0.00136224
Iteration 22/25 | Loss: 0.00136224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013622385449707508, 0.0013622385449707508, 0.0013622385449707508, 0.0013622385449707508, 0.0013622385449707508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013622385449707508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136224
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001708
Iteration 4/1000 | Loss: 0.00001505
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001107
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00001106
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001104
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001103
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001096
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001090
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001088
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001087
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001086
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001081
Iteration 103/1000 | Loss: 0.00001081
Iteration 104/1000 | Loss: 0.00001081
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001080
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001079
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001078
Iteration 119/1000 | Loss: 0.00001078
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001077
Iteration 122/1000 | Loss: 0.00001077
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001069
Iteration 167/1000 | Loss: 0.00001069
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001067
Iteration 172/1000 | Loss: 0.00001067
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001066
Iteration 176/1000 | Loss: 0.00001066
Iteration 177/1000 | Loss: 0.00001066
Iteration 178/1000 | Loss: 0.00001066
Iteration 179/1000 | Loss: 0.00001066
Iteration 180/1000 | Loss: 0.00001066
Iteration 181/1000 | Loss: 0.00001066
Iteration 182/1000 | Loss: 0.00001066
Iteration 183/1000 | Loss: 0.00001066
Iteration 184/1000 | Loss: 0.00001066
Iteration 185/1000 | Loss: 0.00001065
Iteration 186/1000 | Loss: 0.00001065
Iteration 187/1000 | Loss: 0.00001065
Iteration 188/1000 | Loss: 0.00001065
Iteration 189/1000 | Loss: 0.00001065
Iteration 190/1000 | Loss: 0.00001065
Iteration 191/1000 | Loss: 0.00001065
Iteration 192/1000 | Loss: 0.00001065
Iteration 193/1000 | Loss: 0.00001065
Iteration 194/1000 | Loss: 0.00001065
Iteration 195/1000 | Loss: 0.00001065
Iteration 196/1000 | Loss: 0.00001065
Iteration 197/1000 | Loss: 0.00001065
Iteration 198/1000 | Loss: 0.00001065
Iteration 199/1000 | Loss: 0.00001065
Iteration 200/1000 | Loss: 0.00001065
Iteration 201/1000 | Loss: 0.00001065
Iteration 202/1000 | Loss: 0.00001065
Iteration 203/1000 | Loss: 0.00001065
Iteration 204/1000 | Loss: 0.00001065
Iteration 205/1000 | Loss: 0.00001065
Iteration 206/1000 | Loss: 0.00001065
Iteration 207/1000 | Loss: 0.00001065
Iteration 208/1000 | Loss: 0.00001065
Iteration 209/1000 | Loss: 0.00001065
Iteration 210/1000 | Loss: 0.00001065
Iteration 211/1000 | Loss: 0.00001065
Iteration 212/1000 | Loss: 0.00001065
Iteration 213/1000 | Loss: 0.00001065
Iteration 214/1000 | Loss: 0.00001065
Iteration 215/1000 | Loss: 0.00001065
Iteration 216/1000 | Loss: 0.00001065
Iteration 217/1000 | Loss: 0.00001065
Iteration 218/1000 | Loss: 0.00001065
Iteration 219/1000 | Loss: 0.00001065
Iteration 220/1000 | Loss: 0.00001065
Iteration 221/1000 | Loss: 0.00001065
Iteration 222/1000 | Loss: 0.00001065
Iteration 223/1000 | Loss: 0.00001065
Iteration 224/1000 | Loss: 0.00001065
Iteration 225/1000 | Loss: 0.00001065
Iteration 226/1000 | Loss: 0.00001065
Iteration 227/1000 | Loss: 0.00001065
Iteration 228/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.0652782293618657e-05, 1.0652782293618657e-05, 1.0652782293618657e-05, 1.0652782293618657e-05, 1.0652782293618657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0652782293618657e-05

Optimization complete. Final v2v error: 2.7838809490203857 mm

Highest mean error: 2.9951629638671875 mm for frame 45

Lowest mean error: 2.608093500137329 mm for frame 157

Saving results

Total time: 41.8278694152832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788705
Iteration 2/25 | Loss: 0.00161186
Iteration 3/25 | Loss: 0.00140682
Iteration 4/25 | Loss: 0.00139363
Iteration 5/25 | Loss: 0.00139273
Iteration 6/25 | Loss: 0.00139273
Iteration 7/25 | Loss: 0.00139273
Iteration 8/25 | Loss: 0.00139273
Iteration 9/25 | Loss: 0.00139273
Iteration 10/25 | Loss: 0.00139273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013927313266322017, 0.0013927313266322017, 0.0013927313266322017, 0.0013927313266322017, 0.0013927313266322017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013927313266322017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28125560
Iteration 2/25 | Loss: 0.00097748
Iteration 3/25 | Loss: 0.00097747
Iteration 4/25 | Loss: 0.00097747
Iteration 5/25 | Loss: 0.00097746
Iteration 6/25 | Loss: 0.00097746
Iteration 7/25 | Loss: 0.00097746
Iteration 8/25 | Loss: 0.00097746
Iteration 9/25 | Loss: 0.00097746
Iteration 10/25 | Loss: 0.00097746
Iteration 11/25 | Loss: 0.00097746
Iteration 12/25 | Loss: 0.00097746
Iteration 13/25 | Loss: 0.00097746
Iteration 14/25 | Loss: 0.00097746
Iteration 15/25 | Loss: 0.00097746
Iteration 16/25 | Loss: 0.00097746
Iteration 17/25 | Loss: 0.00097746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009774629725143313, 0.0009774629725143313, 0.0009774629725143313, 0.0009774629725143313, 0.0009774629725143313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009774629725143313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097746
Iteration 2/1000 | Loss: 0.00003171
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00002020
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001899
Iteration 9/1000 | Loss: 0.00001876
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001851
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00001850
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001848
Iteration 18/1000 | Loss: 0.00001847
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001845
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001840
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001832
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001831
Iteration 33/1000 | Loss: 0.00001831
Iteration 34/1000 | Loss: 0.00001830
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001829
Iteration 38/1000 | Loss: 0.00001829
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001824
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001817
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001817
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001815
Iteration 67/1000 | Loss: 0.00001815
Iteration 68/1000 | Loss: 0.00001815
Iteration 69/1000 | Loss: 0.00001815
Iteration 70/1000 | Loss: 0.00001815
Iteration 71/1000 | Loss: 0.00001815
Iteration 72/1000 | Loss: 0.00001815
Iteration 73/1000 | Loss: 0.00001815
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001814
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001812
Iteration 84/1000 | Loss: 0.00001812
Iteration 85/1000 | Loss: 0.00001812
Iteration 86/1000 | Loss: 0.00001812
Iteration 87/1000 | Loss: 0.00001812
Iteration 88/1000 | Loss: 0.00001812
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001811
Iteration 93/1000 | Loss: 0.00001811
Iteration 94/1000 | Loss: 0.00001811
Iteration 95/1000 | Loss: 0.00001811
Iteration 96/1000 | Loss: 0.00001810
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00001809
Iteration 99/1000 | Loss: 0.00001809
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001809
Iteration 102/1000 | Loss: 0.00001809
Iteration 103/1000 | Loss: 0.00001809
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001809
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001809
Iteration 108/1000 | Loss: 0.00001809
Iteration 109/1000 | Loss: 0.00001809
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001807
Iteration 115/1000 | Loss: 0.00001807
Iteration 116/1000 | Loss: 0.00001807
Iteration 117/1000 | Loss: 0.00001807
Iteration 118/1000 | Loss: 0.00001807
Iteration 119/1000 | Loss: 0.00001807
Iteration 120/1000 | Loss: 0.00001807
Iteration 121/1000 | Loss: 0.00001807
Iteration 122/1000 | Loss: 0.00001807
Iteration 123/1000 | Loss: 0.00001807
Iteration 124/1000 | Loss: 0.00001807
Iteration 125/1000 | Loss: 0.00001807
Iteration 126/1000 | Loss: 0.00001807
Iteration 127/1000 | Loss: 0.00001807
Iteration 128/1000 | Loss: 0.00001807
Iteration 129/1000 | Loss: 0.00001807
Iteration 130/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8068967619910836e-05, 1.8068967619910836e-05, 1.8068967619910836e-05, 1.8068967619910836e-05, 1.8068967619910836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8068967619910836e-05

Optimization complete. Final v2v error: 3.5503909587860107 mm

Highest mean error: 3.8434667587280273 mm for frame 55

Lowest mean error: 3.4196343421936035 mm for frame 166

Saving results

Total time: 36.57605266571045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992606
Iteration 2/25 | Loss: 0.00326811
Iteration 3/25 | Loss: 0.00215875
Iteration 4/25 | Loss: 0.00194760
Iteration 5/25 | Loss: 0.00211921
Iteration 6/25 | Loss: 0.00194865
Iteration 7/25 | Loss: 0.00171709
Iteration 8/25 | Loss: 0.00144931
Iteration 9/25 | Loss: 0.00136392
Iteration 10/25 | Loss: 0.00136746
Iteration 11/25 | Loss: 0.00134065
Iteration 12/25 | Loss: 0.00133480
Iteration 13/25 | Loss: 0.00133191
Iteration 14/25 | Loss: 0.00132894
Iteration 15/25 | Loss: 0.00132552
Iteration 16/25 | Loss: 0.00134201
Iteration 17/25 | Loss: 0.00133982
Iteration 18/25 | Loss: 0.00132948
Iteration 19/25 | Loss: 0.00133261
Iteration 20/25 | Loss: 0.00132064
Iteration 21/25 | Loss: 0.00132275
Iteration 22/25 | Loss: 0.00132564
Iteration 23/25 | Loss: 0.00132376
Iteration 24/25 | Loss: 0.00132119
Iteration 25/25 | Loss: 0.00132331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32499659
Iteration 2/25 | Loss: 0.00168312
Iteration 3/25 | Loss: 0.00161597
Iteration 4/25 | Loss: 0.00161596
Iteration 5/25 | Loss: 0.00161596
Iteration 6/25 | Loss: 0.00161596
Iteration 7/25 | Loss: 0.00161596
Iteration 8/25 | Loss: 0.00161596
Iteration 9/25 | Loss: 0.00161596
Iteration 10/25 | Loss: 0.00161596
Iteration 11/25 | Loss: 0.00161596
Iteration 12/25 | Loss: 0.00161596
Iteration 13/25 | Loss: 0.00161596
Iteration 14/25 | Loss: 0.00161596
Iteration 15/25 | Loss: 0.00161596
Iteration 16/25 | Loss: 0.00161596
Iteration 17/25 | Loss: 0.00161596
Iteration 18/25 | Loss: 0.00161596
Iteration 19/25 | Loss: 0.00161596
Iteration 20/25 | Loss: 0.00161596
Iteration 21/25 | Loss: 0.00161596
Iteration 22/25 | Loss: 0.00161596
Iteration 23/25 | Loss: 0.00161596
Iteration 24/25 | Loss: 0.00161596
Iteration 25/25 | Loss: 0.00161596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161596
Iteration 2/1000 | Loss: 0.00029459
Iteration 3/1000 | Loss: 0.00024185
Iteration 4/1000 | Loss: 0.00013229
Iteration 5/1000 | Loss: 0.00013440
Iteration 6/1000 | Loss: 0.00017407
Iteration 7/1000 | Loss: 0.00014170
Iteration 8/1000 | Loss: 0.00015984
Iteration 9/1000 | Loss: 0.00024780
Iteration 10/1000 | Loss: 0.00018424
Iteration 11/1000 | Loss: 0.00031914
Iteration 12/1000 | Loss: 0.00020787
Iteration 13/1000 | Loss: 0.00012392
Iteration 14/1000 | Loss: 0.00015508
Iteration 15/1000 | Loss: 0.00014647
Iteration 16/1000 | Loss: 0.00007757
Iteration 17/1000 | Loss: 0.00015978
Iteration 18/1000 | Loss: 0.00003215
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00013282
Iteration 21/1000 | Loss: 0.00007999
Iteration 22/1000 | Loss: 0.00015336
Iteration 23/1000 | Loss: 0.00013986
Iteration 24/1000 | Loss: 0.00015775
Iteration 25/1000 | Loss: 0.00031872
Iteration 26/1000 | Loss: 0.00022201
Iteration 27/1000 | Loss: 0.00015606
Iteration 28/1000 | Loss: 0.00011360
Iteration 29/1000 | Loss: 0.00014006
Iteration 30/1000 | Loss: 0.00009336
Iteration 31/1000 | Loss: 0.00011989
Iteration 32/1000 | Loss: 0.00009968
Iteration 33/1000 | Loss: 0.00018553
Iteration 34/1000 | Loss: 0.00015602
Iteration 35/1000 | Loss: 0.00010941
Iteration 36/1000 | Loss: 0.00004157
Iteration 37/1000 | Loss: 0.00018133
Iteration 38/1000 | Loss: 0.00017026
Iteration 39/1000 | Loss: 0.00017595
Iteration 40/1000 | Loss: 0.00014064
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00016623
Iteration 44/1000 | Loss: 0.00011709
Iteration 45/1000 | Loss: 0.00011287
Iteration 46/1000 | Loss: 0.00015034
Iteration 47/1000 | Loss: 0.00010926
Iteration 48/1000 | Loss: 0.00014252
Iteration 49/1000 | Loss: 0.00044254
Iteration 50/1000 | Loss: 0.00016148
Iteration 51/1000 | Loss: 0.00038496
Iteration 52/1000 | Loss: 0.00026996
Iteration 53/1000 | Loss: 0.00030211
Iteration 54/1000 | Loss: 0.00014840
Iteration 55/1000 | Loss: 0.00008591
Iteration 56/1000 | Loss: 0.00005252
Iteration 57/1000 | Loss: 0.00004791
Iteration 58/1000 | Loss: 0.00002486
Iteration 59/1000 | Loss: 0.00006785
Iteration 60/1000 | Loss: 0.00002345
Iteration 61/1000 | Loss: 0.00002241
Iteration 62/1000 | Loss: 0.00022774
Iteration 63/1000 | Loss: 0.00003751
Iteration 64/1000 | Loss: 0.00002155
Iteration 65/1000 | Loss: 0.00019617
Iteration 66/1000 | Loss: 0.00004871
Iteration 67/1000 | Loss: 0.00012801
Iteration 68/1000 | Loss: 0.00010123
Iteration 69/1000 | Loss: 0.00015146
Iteration 70/1000 | Loss: 0.00005400
Iteration 71/1000 | Loss: 0.00023475
Iteration 72/1000 | Loss: 0.00006847
Iteration 73/1000 | Loss: 0.00009988
Iteration 74/1000 | Loss: 0.00011114
Iteration 75/1000 | Loss: 0.00009568
Iteration 76/1000 | Loss: 0.00007662
Iteration 77/1000 | Loss: 0.00011654
Iteration 78/1000 | Loss: 0.00007910
Iteration 79/1000 | Loss: 0.00002107
Iteration 80/1000 | Loss: 0.00059629
Iteration 81/1000 | Loss: 0.00131758
Iteration 82/1000 | Loss: 0.00021723
Iteration 83/1000 | Loss: 0.00025816
Iteration 84/1000 | Loss: 0.00007902
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001502
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00011203
Iteration 97/1000 | Loss: 0.00030220
Iteration 98/1000 | Loss: 0.00005456
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00020042
Iteration 103/1000 | Loss: 0.00026915
Iteration 104/1000 | Loss: 0.00075232
Iteration 105/1000 | Loss: 0.00026482
Iteration 106/1000 | Loss: 0.00005383
Iteration 107/1000 | Loss: 0.00020349
Iteration 108/1000 | Loss: 0.00013538
Iteration 109/1000 | Loss: 0.00001980
Iteration 110/1000 | Loss: 0.00007407
Iteration 111/1000 | Loss: 0.00028330
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001492
Iteration 114/1000 | Loss: 0.00001454
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001414
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001404
Iteration 130/1000 | Loss: 0.00001404
Iteration 131/1000 | Loss: 0.00001404
Iteration 132/1000 | Loss: 0.00001404
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001404
Iteration 135/1000 | Loss: 0.00001403
Iteration 136/1000 | Loss: 0.00001403
Iteration 137/1000 | Loss: 0.00001403
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Iteration 149/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4021709830558393e-05, 1.4021709830558393e-05, 1.4021709830558393e-05, 1.4021709830558393e-05, 1.4021709830558393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4021709830558393e-05

Optimization complete. Final v2v error: 3.1219162940979004 mm

Highest mean error: 6.843303203582764 mm for frame 48

Lowest mean error: 2.8282742500305176 mm for frame 4

Saving results

Total time: 207.86436486244202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722394
Iteration 2/25 | Loss: 0.00169170
Iteration 3/25 | Loss: 0.00152580
Iteration 4/25 | Loss: 0.00133520
Iteration 5/25 | Loss: 0.00133071
Iteration 6/25 | Loss: 0.00132614
Iteration 7/25 | Loss: 0.00132201
Iteration 8/25 | Loss: 0.00132080
Iteration 9/25 | Loss: 0.00131939
Iteration 10/25 | Loss: 0.00131777
Iteration 11/25 | Loss: 0.00131710
Iteration 12/25 | Loss: 0.00131695
Iteration 13/25 | Loss: 0.00131694
Iteration 14/25 | Loss: 0.00131693
Iteration 15/25 | Loss: 0.00131692
Iteration 16/25 | Loss: 0.00131692
Iteration 17/25 | Loss: 0.00131691
Iteration 18/25 | Loss: 0.00131691
Iteration 19/25 | Loss: 0.00131691
Iteration 20/25 | Loss: 0.00131691
Iteration 21/25 | Loss: 0.00131691
Iteration 22/25 | Loss: 0.00131691
Iteration 23/25 | Loss: 0.00131691
Iteration 24/25 | Loss: 0.00131690
Iteration 25/25 | Loss: 0.00131690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78151107
Iteration 2/25 | Loss: 0.00151959
Iteration 3/25 | Loss: 0.00138834
Iteration 4/25 | Loss: 0.00138834
Iteration 5/25 | Loss: 0.00138834
Iteration 6/25 | Loss: 0.00138834
Iteration 7/25 | Loss: 0.00138834
Iteration 8/25 | Loss: 0.00138834
Iteration 9/25 | Loss: 0.00138834
Iteration 10/25 | Loss: 0.00138834
Iteration 11/25 | Loss: 0.00138834
Iteration 12/25 | Loss: 0.00138834
Iteration 13/25 | Loss: 0.00138834
Iteration 14/25 | Loss: 0.00138834
Iteration 15/25 | Loss: 0.00138834
Iteration 16/25 | Loss: 0.00138834
Iteration 17/25 | Loss: 0.00138833
Iteration 18/25 | Loss: 0.00138833
Iteration 19/25 | Loss: 0.00138833
Iteration 20/25 | Loss: 0.00138833
Iteration 21/25 | Loss: 0.00138833
Iteration 22/25 | Loss: 0.00138833
Iteration 23/25 | Loss: 0.00138833
Iteration 24/25 | Loss: 0.00138833
Iteration 25/25 | Loss: 0.00138833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138833
Iteration 2/1000 | Loss: 0.00017816
Iteration 3/1000 | Loss: 0.00016089
Iteration 4/1000 | Loss: 0.00015513
Iteration 5/1000 | Loss: 0.00092281
Iteration 6/1000 | Loss: 0.00009613
Iteration 7/1000 | Loss: 0.00004613
Iteration 8/1000 | Loss: 0.00003220
Iteration 9/1000 | Loss: 0.00059822
Iteration 10/1000 | Loss: 0.00005109
Iteration 11/1000 | Loss: 0.00002871
Iteration 12/1000 | Loss: 0.00002475
Iteration 13/1000 | Loss: 0.00015581
Iteration 14/1000 | Loss: 0.00002233
Iteration 15/1000 | Loss: 0.00002060
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00001985
Iteration 18/1000 | Loss: 0.00021717
Iteration 19/1000 | Loss: 0.00026373
Iteration 20/1000 | Loss: 0.00002399
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00015427
Iteration 23/1000 | Loss: 0.00036157
Iteration 24/1000 | Loss: 0.00002606
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00015530
Iteration 27/1000 | Loss: 0.00003404
Iteration 28/1000 | Loss: 0.00004022
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00008048
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002650
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001854
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00001851
Iteration 41/1000 | Loss: 0.00001851
Iteration 42/1000 | Loss: 0.00001850
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001823
Iteration 56/1000 | Loss: 0.00001823
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001821
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001816
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001812
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001807
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001806
Iteration 110/1000 | Loss: 0.00001806
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001804
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001804
Iteration 119/1000 | Loss: 0.00001804
Iteration 120/1000 | Loss: 0.00001804
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001804
Iteration 136/1000 | Loss: 0.00001804
Iteration 137/1000 | Loss: 0.00001804
Iteration 138/1000 | Loss: 0.00001804
Iteration 139/1000 | Loss: 0.00001804
Iteration 140/1000 | Loss: 0.00001804
Iteration 141/1000 | Loss: 0.00001804
Iteration 142/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.8043541786028072e-05, 1.8043541786028072e-05, 1.8043541786028072e-05, 1.8043541786028072e-05, 1.8043541786028072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8043541786028072e-05

Optimization complete. Final v2v error: 3.639528751373291 mm

Highest mean error: 4.214180946350098 mm for frame 107

Lowest mean error: 3.348078489303589 mm for frame 127

Saving results

Total time: 80.04605460166931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552092
Iteration 2/25 | Loss: 0.00154854
Iteration 3/25 | Loss: 0.00138216
Iteration 4/25 | Loss: 0.00136321
Iteration 5/25 | Loss: 0.00135828
Iteration 6/25 | Loss: 0.00135747
Iteration 7/25 | Loss: 0.00135747
Iteration 8/25 | Loss: 0.00135747
Iteration 9/25 | Loss: 0.00135747
Iteration 10/25 | Loss: 0.00135747
Iteration 11/25 | Loss: 0.00135747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001357472618110478, 0.001357472618110478, 0.001357472618110478, 0.001357472618110478, 0.001357472618110478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001357472618110478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65939057
Iteration 2/25 | Loss: 0.00176277
Iteration 3/25 | Loss: 0.00176274
Iteration 4/25 | Loss: 0.00176274
Iteration 5/25 | Loss: 0.00176274
Iteration 6/25 | Loss: 0.00176274
Iteration 7/25 | Loss: 0.00176274
Iteration 8/25 | Loss: 0.00176274
Iteration 9/25 | Loss: 0.00176274
Iteration 10/25 | Loss: 0.00176274
Iteration 11/25 | Loss: 0.00176274
Iteration 12/25 | Loss: 0.00176274
Iteration 13/25 | Loss: 0.00176274
Iteration 14/25 | Loss: 0.00176274
Iteration 15/25 | Loss: 0.00176274
Iteration 16/25 | Loss: 0.00176274
Iteration 17/25 | Loss: 0.00176274
Iteration 18/25 | Loss: 0.00176274
Iteration 19/25 | Loss: 0.00176274
Iteration 20/25 | Loss: 0.00176274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001762739266268909, 0.001762739266268909, 0.001762739266268909, 0.001762739266268909, 0.001762739266268909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001762739266268909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176274
Iteration 2/1000 | Loss: 0.00005340
Iteration 3/1000 | Loss: 0.00003276
Iteration 4/1000 | Loss: 0.00002853
Iteration 5/1000 | Loss: 0.00002659
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002324
Iteration 9/1000 | Loss: 0.00002275
Iteration 10/1000 | Loss: 0.00002226
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002120
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002117
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002099
Iteration 25/1000 | Loss: 0.00002098
Iteration 26/1000 | Loss: 0.00002097
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002096
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002093
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002085
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002082
Iteration 49/1000 | Loss: 0.00002081
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002080
Iteration 53/1000 | Loss: 0.00002080
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002077
Iteration 60/1000 | Loss: 0.00002077
Iteration 61/1000 | Loss: 0.00002077
Iteration 62/1000 | Loss: 0.00002076
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002073
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002065
Iteration 97/1000 | Loss: 0.00002065
Iteration 98/1000 | Loss: 0.00002065
Iteration 99/1000 | Loss: 0.00002065
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00002064
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002061
Iteration 115/1000 | Loss: 0.00002061
Iteration 116/1000 | Loss: 0.00002061
Iteration 117/1000 | Loss: 0.00002061
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002060
Iteration 120/1000 | Loss: 0.00002060
Iteration 121/1000 | Loss: 0.00002060
Iteration 122/1000 | Loss: 0.00002060
Iteration 123/1000 | Loss: 0.00002060
Iteration 124/1000 | Loss: 0.00002060
Iteration 125/1000 | Loss: 0.00002060
Iteration 126/1000 | Loss: 0.00002059
Iteration 127/1000 | Loss: 0.00002059
Iteration 128/1000 | Loss: 0.00002059
Iteration 129/1000 | Loss: 0.00002059
Iteration 130/1000 | Loss: 0.00002059
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002058
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002058
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002057
Iteration 140/1000 | Loss: 0.00002057
Iteration 141/1000 | Loss: 0.00002057
Iteration 142/1000 | Loss: 0.00002056
Iteration 143/1000 | Loss: 0.00002056
Iteration 144/1000 | Loss: 0.00002056
Iteration 145/1000 | Loss: 0.00002056
Iteration 146/1000 | Loss: 0.00002056
Iteration 147/1000 | Loss: 0.00002056
Iteration 148/1000 | Loss: 0.00002056
Iteration 149/1000 | Loss: 0.00002056
Iteration 150/1000 | Loss: 0.00002056
Iteration 151/1000 | Loss: 0.00002055
Iteration 152/1000 | Loss: 0.00002055
Iteration 153/1000 | Loss: 0.00002055
Iteration 154/1000 | Loss: 0.00002055
Iteration 155/1000 | Loss: 0.00002055
Iteration 156/1000 | Loss: 0.00002055
Iteration 157/1000 | Loss: 0.00002055
Iteration 158/1000 | Loss: 0.00002055
Iteration 159/1000 | Loss: 0.00002055
Iteration 160/1000 | Loss: 0.00002054
Iteration 161/1000 | Loss: 0.00002054
Iteration 162/1000 | Loss: 0.00002054
Iteration 163/1000 | Loss: 0.00002054
Iteration 164/1000 | Loss: 0.00002054
Iteration 165/1000 | Loss: 0.00002054
Iteration 166/1000 | Loss: 0.00002054
Iteration 167/1000 | Loss: 0.00002054
Iteration 168/1000 | Loss: 0.00002054
Iteration 169/1000 | Loss: 0.00002054
Iteration 170/1000 | Loss: 0.00002054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.0542247511912137e-05, 2.0542247511912137e-05, 2.0542247511912137e-05, 2.0542247511912137e-05, 2.0542247511912137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0542247511912137e-05

Optimization complete. Final v2v error: 3.764294147491455 mm

Highest mean error: 4.432324409484863 mm for frame 233

Lowest mean error: 3.3391356468200684 mm for frame 206

Saving results

Total time: 48.5973916053772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034486
Iteration 2/25 | Loss: 0.01034486
Iteration 3/25 | Loss: 0.01034486
Iteration 4/25 | Loss: 0.01034486
Iteration 5/25 | Loss: 0.01034486
Iteration 6/25 | Loss: 0.01034486
Iteration 7/25 | Loss: 0.01034485
Iteration 8/25 | Loss: 0.01034485
Iteration 9/25 | Loss: 0.01034485
Iteration 10/25 | Loss: 0.01034485
Iteration 11/25 | Loss: 0.01034485
Iteration 12/25 | Loss: 0.01034485
Iteration 13/25 | Loss: 0.01034485
Iteration 14/25 | Loss: 0.01034485
Iteration 15/25 | Loss: 0.01034485
Iteration 16/25 | Loss: 0.01034485
Iteration 17/25 | Loss: 0.01034485
Iteration 18/25 | Loss: 0.01034485
Iteration 19/25 | Loss: 0.01034485
Iteration 20/25 | Loss: 0.01034485
Iteration 21/25 | Loss: 0.01034484
Iteration 22/25 | Loss: 0.01034484
Iteration 23/25 | Loss: 0.01034484
Iteration 24/25 | Loss: 0.01034484
Iteration 25/25 | Loss: 0.01034484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79232538
Iteration 2/25 | Loss: 0.06871141
Iteration 3/25 | Loss: 0.06871141
Iteration 4/25 | Loss: 0.06871141
Iteration 5/25 | Loss: 0.06871140
Iteration 6/25 | Loss: 0.06871140
Iteration 7/25 | Loss: 0.06871140
Iteration 8/25 | Loss: 0.06871139
Iteration 9/25 | Loss: 0.06871140
Iteration 10/25 | Loss: 0.06871139
Iteration 11/25 | Loss: 0.06871139
Iteration 12/25 | Loss: 0.06871139
Iteration 13/25 | Loss: 0.06871139
Iteration 14/25 | Loss: 0.06871139
Iteration 15/25 | Loss: 0.06871139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.06871139258146286, 0.06871139258146286, 0.06871139258146286, 0.06871139258146286, 0.06871139258146286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06871139258146286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06871139
Iteration 2/1000 | Loss: 0.00641977
Iteration 3/1000 | Loss: 0.00134624
Iteration 4/1000 | Loss: 0.00044187
Iteration 5/1000 | Loss: 0.01115485
Iteration 6/1000 | Loss: 0.01056402
Iteration 7/1000 | Loss: 0.00099876
Iteration 8/1000 | Loss: 0.00706741
Iteration 9/1000 | Loss: 0.00044437
Iteration 10/1000 | Loss: 0.00168278
Iteration 11/1000 | Loss: 0.00018623
Iteration 12/1000 | Loss: 0.00011880
Iteration 13/1000 | Loss: 0.00083883
Iteration 14/1000 | Loss: 0.00047340
Iteration 15/1000 | Loss: 0.00037276
Iteration 16/1000 | Loss: 0.00007179
Iteration 17/1000 | Loss: 0.00480903
Iteration 18/1000 | Loss: 0.00298978
Iteration 19/1000 | Loss: 0.00501110
Iteration 20/1000 | Loss: 0.00017024
Iteration 21/1000 | Loss: 0.00031297
Iteration 22/1000 | Loss: 0.00091927
Iteration 23/1000 | Loss: 0.00032407
Iteration 24/1000 | Loss: 0.00004388
Iteration 25/1000 | Loss: 0.00051901
Iteration 26/1000 | Loss: 0.00865731
Iteration 27/1000 | Loss: 0.00033341
Iteration 28/1000 | Loss: 0.00010490
Iteration 29/1000 | Loss: 0.00088192
Iteration 30/1000 | Loss: 0.00016585
Iteration 31/1000 | Loss: 0.00167435
Iteration 32/1000 | Loss: 0.00023844
Iteration 33/1000 | Loss: 0.00073796
Iteration 34/1000 | Loss: 0.00004173
Iteration 35/1000 | Loss: 0.00004150
Iteration 36/1000 | Loss: 0.00003583
Iteration 37/1000 | Loss: 0.00008777
Iteration 38/1000 | Loss: 0.00020815
Iteration 39/1000 | Loss: 0.00003261
Iteration 40/1000 | Loss: 0.00022747
Iteration 41/1000 | Loss: 0.00006195
Iteration 42/1000 | Loss: 0.00090519
Iteration 43/1000 | Loss: 0.00044317
Iteration 44/1000 | Loss: 0.00028776
Iteration 45/1000 | Loss: 0.00192536
Iteration 46/1000 | Loss: 0.00021860
Iteration 47/1000 | Loss: 0.00011450
Iteration 48/1000 | Loss: 0.00006567
Iteration 49/1000 | Loss: 0.00002711
Iteration 50/1000 | Loss: 0.00002616
Iteration 51/1000 | Loss: 0.00029906
Iteration 52/1000 | Loss: 0.00009519
Iteration 53/1000 | Loss: 0.00066333
Iteration 54/1000 | Loss: 0.00003630
Iteration 55/1000 | Loss: 0.00010916
Iteration 56/1000 | Loss: 0.00005468
Iteration 57/1000 | Loss: 0.00006871
Iteration 58/1000 | Loss: 0.00002420
Iteration 59/1000 | Loss: 0.00002369
Iteration 60/1000 | Loss: 0.00010311
Iteration 61/1000 | Loss: 0.00008206
Iteration 62/1000 | Loss: 0.00067096
Iteration 63/1000 | Loss: 0.00005447
Iteration 64/1000 | Loss: 0.00009612
Iteration 65/1000 | Loss: 0.00002301
Iteration 66/1000 | Loss: 0.00009319
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002232
Iteration 69/1000 | Loss: 0.00002202
Iteration 70/1000 | Loss: 0.00008345
Iteration 71/1000 | Loss: 0.00003189
Iteration 72/1000 | Loss: 0.00009793
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002152
Iteration 75/1000 | Loss: 0.00002151
Iteration 76/1000 | Loss: 0.00002141
Iteration 77/1000 | Loss: 0.00002139
Iteration 78/1000 | Loss: 0.00026916
Iteration 79/1000 | Loss: 0.00018020
Iteration 80/1000 | Loss: 0.00002306
Iteration 81/1000 | Loss: 0.00002670
Iteration 82/1000 | Loss: 0.00004152
Iteration 83/1000 | Loss: 0.00016139
Iteration 84/1000 | Loss: 0.00003148
Iteration 85/1000 | Loss: 0.00005124
Iteration 86/1000 | Loss: 0.00002101
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002092
Iteration 91/1000 | Loss: 0.00002092
Iteration 92/1000 | Loss: 0.00002090
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002084
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002083
Iteration 100/1000 | Loss: 0.00002080
Iteration 101/1000 | Loss: 0.00002080
Iteration 102/1000 | Loss: 0.00002079
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002078
Iteration 105/1000 | Loss: 0.00002078
Iteration 106/1000 | Loss: 0.00002078
Iteration 107/1000 | Loss: 0.00002077
Iteration 108/1000 | Loss: 0.00002077
Iteration 109/1000 | Loss: 0.00002077
Iteration 110/1000 | Loss: 0.00002077
Iteration 111/1000 | Loss: 0.00002077
Iteration 112/1000 | Loss: 0.00002077
Iteration 113/1000 | Loss: 0.00002077
Iteration 114/1000 | Loss: 0.00002077
Iteration 115/1000 | Loss: 0.00002077
Iteration 116/1000 | Loss: 0.00002077
Iteration 117/1000 | Loss: 0.00002077
Iteration 118/1000 | Loss: 0.00002076
Iteration 119/1000 | Loss: 0.00002076
Iteration 120/1000 | Loss: 0.00002076
Iteration 121/1000 | Loss: 0.00002076
Iteration 122/1000 | Loss: 0.00002076
Iteration 123/1000 | Loss: 0.00002076
Iteration 124/1000 | Loss: 0.00002076
Iteration 125/1000 | Loss: 0.00002076
Iteration 126/1000 | Loss: 0.00002076
Iteration 127/1000 | Loss: 0.00002076
Iteration 128/1000 | Loss: 0.00002076
Iteration 129/1000 | Loss: 0.00002076
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002075
Iteration 138/1000 | Loss: 0.00002075
Iteration 139/1000 | Loss: 0.00002075
Iteration 140/1000 | Loss: 0.00002075
Iteration 141/1000 | Loss: 0.00002075
Iteration 142/1000 | Loss: 0.00002075
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002074
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002074
Iteration 147/1000 | Loss: 0.00002074
Iteration 148/1000 | Loss: 0.00002074
Iteration 149/1000 | Loss: 0.00002074
Iteration 150/1000 | Loss: 0.00002074
Iteration 151/1000 | Loss: 0.00002074
Iteration 152/1000 | Loss: 0.00002074
Iteration 153/1000 | Loss: 0.00002073
Iteration 154/1000 | Loss: 0.00002073
Iteration 155/1000 | Loss: 0.00002073
Iteration 156/1000 | Loss: 0.00002073
Iteration 157/1000 | Loss: 0.00002073
Iteration 158/1000 | Loss: 0.00002073
Iteration 159/1000 | Loss: 0.00002073
Iteration 160/1000 | Loss: 0.00002073
Iteration 161/1000 | Loss: 0.00002073
Iteration 162/1000 | Loss: 0.00002072
Iteration 163/1000 | Loss: 0.00002072
Iteration 164/1000 | Loss: 0.00002072
Iteration 165/1000 | Loss: 0.00002072
Iteration 166/1000 | Loss: 0.00002072
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00006205
Iteration 169/1000 | Loss: 0.00002077
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002071
Iteration 172/1000 | Loss: 0.00002071
Iteration 173/1000 | Loss: 0.00002071
Iteration 174/1000 | Loss: 0.00002071
Iteration 175/1000 | Loss: 0.00002071
Iteration 176/1000 | Loss: 0.00002071
Iteration 177/1000 | Loss: 0.00002071
Iteration 178/1000 | Loss: 0.00002071
Iteration 179/1000 | Loss: 0.00002071
Iteration 180/1000 | Loss: 0.00002070
Iteration 181/1000 | Loss: 0.00002070
Iteration 182/1000 | Loss: 0.00002070
Iteration 183/1000 | Loss: 0.00002070
Iteration 184/1000 | Loss: 0.00002069
Iteration 185/1000 | Loss: 0.00002069
Iteration 186/1000 | Loss: 0.00002069
Iteration 187/1000 | Loss: 0.00002069
Iteration 188/1000 | Loss: 0.00002069
Iteration 189/1000 | Loss: 0.00002068
Iteration 190/1000 | Loss: 0.00002068
Iteration 191/1000 | Loss: 0.00002067
Iteration 192/1000 | Loss: 0.00002067
Iteration 193/1000 | Loss: 0.00002067
Iteration 194/1000 | Loss: 0.00002067
Iteration 195/1000 | Loss: 0.00002067
Iteration 196/1000 | Loss: 0.00002067
Iteration 197/1000 | Loss: 0.00002067
Iteration 198/1000 | Loss: 0.00002067
Iteration 199/1000 | Loss: 0.00002067
Iteration 200/1000 | Loss: 0.00002067
Iteration 201/1000 | Loss: 0.00002067
Iteration 202/1000 | Loss: 0.00002067
Iteration 203/1000 | Loss: 0.00002067
Iteration 204/1000 | Loss: 0.00002066
Iteration 205/1000 | Loss: 0.00002066
Iteration 206/1000 | Loss: 0.00002066
Iteration 207/1000 | Loss: 0.00002066
Iteration 208/1000 | Loss: 0.00002066
Iteration 209/1000 | Loss: 0.00002065
Iteration 210/1000 | Loss: 0.00002065
Iteration 211/1000 | Loss: 0.00002064
Iteration 212/1000 | Loss: 0.00002063
Iteration 213/1000 | Loss: 0.00002063
Iteration 214/1000 | Loss: 0.00002060
Iteration 215/1000 | Loss: 0.00002060
Iteration 216/1000 | Loss: 0.00002060
Iteration 217/1000 | Loss: 0.00002060
Iteration 218/1000 | Loss: 0.00002060
Iteration 219/1000 | Loss: 0.00002060
Iteration 220/1000 | Loss: 0.00002060
Iteration 221/1000 | Loss: 0.00002059
Iteration 222/1000 | Loss: 0.00002059
Iteration 223/1000 | Loss: 0.00002059
Iteration 224/1000 | Loss: 0.00002059
Iteration 225/1000 | Loss: 0.00002058
Iteration 226/1000 | Loss: 0.00003247
Iteration 227/1000 | Loss: 0.00002909
Iteration 228/1000 | Loss: 0.00017131
Iteration 229/1000 | Loss: 0.00002106
Iteration 230/1000 | Loss: 0.00008135
Iteration 231/1000 | Loss: 0.00002429
Iteration 232/1000 | Loss: 0.00003264
Iteration 233/1000 | Loss: 0.00002172
Iteration 234/1000 | Loss: 0.00007039
Iteration 235/1000 | Loss: 0.00010192
Iteration 236/1000 | Loss: 0.00125067
Iteration 237/1000 | Loss: 0.00034681
Iteration 238/1000 | Loss: 0.00029527
Iteration 239/1000 | Loss: 0.00005788
Iteration 240/1000 | Loss: 0.00016857
Iteration 241/1000 | Loss: 0.00005325
Iteration 242/1000 | Loss: 0.00005489
Iteration 243/1000 | Loss: 0.00064848
Iteration 244/1000 | Loss: 0.00005533
Iteration 245/1000 | Loss: 0.00010584
Iteration 246/1000 | Loss: 0.00012274
Iteration 247/1000 | Loss: 0.00005041
Iteration 248/1000 | Loss: 0.00003246
Iteration 249/1000 | Loss: 0.00002062
Iteration 250/1000 | Loss: 0.00002059
Iteration 251/1000 | Loss: 0.00003654
Iteration 252/1000 | Loss: 0.00002070
Iteration 253/1000 | Loss: 0.00002055
Iteration 254/1000 | Loss: 0.00002054
Iteration 255/1000 | Loss: 0.00002052
Iteration 256/1000 | Loss: 0.00002052
Iteration 257/1000 | Loss: 0.00002052
Iteration 258/1000 | Loss: 0.00002052
Iteration 259/1000 | Loss: 0.00002052
Iteration 260/1000 | Loss: 0.00002052
Iteration 261/1000 | Loss: 0.00002052
Iteration 262/1000 | Loss: 0.00002052
Iteration 263/1000 | Loss: 0.00002051
Iteration 264/1000 | Loss: 0.00002051
Iteration 265/1000 | Loss: 0.00002051
Iteration 266/1000 | Loss: 0.00002051
Iteration 267/1000 | Loss: 0.00002050
Iteration 268/1000 | Loss: 0.00002050
Iteration 269/1000 | Loss: 0.00002050
Iteration 270/1000 | Loss: 0.00002050
Iteration 271/1000 | Loss: 0.00002050
Iteration 272/1000 | Loss: 0.00002050
Iteration 273/1000 | Loss: 0.00002050
Iteration 274/1000 | Loss: 0.00002050
Iteration 275/1000 | Loss: 0.00002049
Iteration 276/1000 | Loss: 0.00006741
Iteration 277/1000 | Loss: 0.00002053
Iteration 278/1000 | Loss: 0.00002053
Iteration 279/1000 | Loss: 0.00002053
Iteration 280/1000 | Loss: 0.00002052
Iteration 281/1000 | Loss: 0.00002052
Iteration 282/1000 | Loss: 0.00002052
Iteration 283/1000 | Loss: 0.00002052
Iteration 284/1000 | Loss: 0.00002051
Iteration 285/1000 | Loss: 0.00002051
Iteration 286/1000 | Loss: 0.00002051
Iteration 287/1000 | Loss: 0.00002051
Iteration 288/1000 | Loss: 0.00002051
Iteration 289/1000 | Loss: 0.00002051
Iteration 290/1000 | Loss: 0.00002050
Iteration 291/1000 | Loss: 0.00002050
Iteration 292/1000 | Loss: 0.00002050
Iteration 293/1000 | Loss: 0.00002049
Iteration 294/1000 | Loss: 0.00002049
Iteration 295/1000 | Loss: 0.00002049
Iteration 296/1000 | Loss: 0.00002049
Iteration 297/1000 | Loss: 0.00002049
Iteration 298/1000 | Loss: 0.00002049
Iteration 299/1000 | Loss: 0.00002049
Iteration 300/1000 | Loss: 0.00002049
Iteration 301/1000 | Loss: 0.00002049
Iteration 302/1000 | Loss: 0.00002049
Iteration 303/1000 | Loss: 0.00002049
Iteration 304/1000 | Loss: 0.00002048
Iteration 305/1000 | Loss: 0.00002048
Iteration 306/1000 | Loss: 0.00002048
Iteration 307/1000 | Loss: 0.00002048
Iteration 308/1000 | Loss: 0.00002048
Iteration 309/1000 | Loss: 0.00002048
Iteration 310/1000 | Loss: 0.00007026
Iteration 311/1000 | Loss: 0.00005860
Iteration 312/1000 | Loss: 0.00002051
Iteration 313/1000 | Loss: 0.00002049
Iteration 314/1000 | Loss: 0.00002049
Iteration 315/1000 | Loss: 0.00002048
Iteration 316/1000 | Loss: 0.00002048
Iteration 317/1000 | Loss: 0.00002048
Iteration 318/1000 | Loss: 0.00002048
Iteration 319/1000 | Loss: 0.00002048
Iteration 320/1000 | Loss: 0.00002048
Iteration 321/1000 | Loss: 0.00002048
Iteration 322/1000 | Loss: 0.00002048
Iteration 323/1000 | Loss: 0.00002047
Iteration 324/1000 | Loss: 0.00002047
Iteration 325/1000 | Loss: 0.00002047
Iteration 326/1000 | Loss: 0.00002047
Iteration 327/1000 | Loss: 0.00002047
Iteration 328/1000 | Loss: 0.00002047
Iteration 329/1000 | Loss: 0.00002047
Iteration 330/1000 | Loss: 0.00002047
Iteration 331/1000 | Loss: 0.00002047
Iteration 332/1000 | Loss: 0.00002047
Iteration 333/1000 | Loss: 0.00002047
Iteration 334/1000 | Loss: 0.00002047
Iteration 335/1000 | Loss: 0.00002047
Iteration 336/1000 | Loss: 0.00002047
Iteration 337/1000 | Loss: 0.00002047
Iteration 338/1000 | Loss: 0.00002047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.046878944383934e-05, 2.046878944383934e-05, 2.046878944383934e-05, 2.046878944383934e-05, 2.046878944383934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.046878944383934e-05

Optimization complete. Final v2v error: 3.7361812591552734 mm

Highest mean error: 4.82563591003418 mm for frame 218

Lowest mean error: 3.0278289318084717 mm for frame 118

Saving results

Total time: 205.1795494556427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772582
Iteration 2/25 | Loss: 0.00137352
Iteration 3/25 | Loss: 0.00128591
Iteration 4/25 | Loss: 0.00127946
Iteration 5/25 | Loss: 0.00127815
Iteration 6/25 | Loss: 0.00127815
Iteration 7/25 | Loss: 0.00127815
Iteration 8/25 | Loss: 0.00127815
Iteration 9/25 | Loss: 0.00127815
Iteration 10/25 | Loss: 0.00127815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001278148964047432, 0.001278148964047432, 0.001278148964047432, 0.001278148964047432, 0.001278148964047432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278148964047432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30566621
Iteration 2/25 | Loss: 0.00135724
Iteration 3/25 | Loss: 0.00135723
Iteration 4/25 | Loss: 0.00135723
Iteration 5/25 | Loss: 0.00135723
Iteration 6/25 | Loss: 0.00135723
Iteration 7/25 | Loss: 0.00135723
Iteration 8/25 | Loss: 0.00135723
Iteration 9/25 | Loss: 0.00135723
Iteration 10/25 | Loss: 0.00135723
Iteration 11/25 | Loss: 0.00135723
Iteration 12/25 | Loss: 0.00135723
Iteration 13/25 | Loss: 0.00135723
Iteration 14/25 | Loss: 0.00135723
Iteration 15/25 | Loss: 0.00135723
Iteration 16/25 | Loss: 0.00135723
Iteration 17/25 | Loss: 0.00135723
Iteration 18/25 | Loss: 0.00135723
Iteration 19/25 | Loss: 0.00135723
Iteration 20/25 | Loss: 0.00135723
Iteration 21/25 | Loss: 0.00135723
Iteration 22/25 | Loss: 0.00135723
Iteration 23/25 | Loss: 0.00135723
Iteration 24/25 | Loss: 0.00135723
Iteration 25/25 | Loss: 0.00135723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135723
Iteration 2/1000 | Loss: 0.00002660
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001291
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001115
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001108
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001085
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001082
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001069
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001067
Iteration 91/1000 | Loss: 0.00001067
Iteration 92/1000 | Loss: 0.00001067
Iteration 93/1000 | Loss: 0.00001067
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001062
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001061
Iteration 114/1000 | Loss: 0.00001061
Iteration 115/1000 | Loss: 0.00001061
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001057
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001053
Iteration 160/1000 | Loss: 0.00001053
Iteration 161/1000 | Loss: 0.00001053
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001051
Iteration 171/1000 | Loss: 0.00001051
Iteration 172/1000 | Loss: 0.00001051
Iteration 173/1000 | Loss: 0.00001051
Iteration 174/1000 | Loss: 0.00001051
Iteration 175/1000 | Loss: 0.00001051
Iteration 176/1000 | Loss: 0.00001051
Iteration 177/1000 | Loss: 0.00001051
Iteration 178/1000 | Loss: 0.00001051
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001050
Iteration 182/1000 | Loss: 0.00001050
Iteration 183/1000 | Loss: 0.00001050
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001049
Iteration 188/1000 | Loss: 0.00001049
Iteration 189/1000 | Loss: 0.00001049
Iteration 190/1000 | Loss: 0.00001049
Iteration 191/1000 | Loss: 0.00001049
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001048
Iteration 196/1000 | Loss: 0.00001048
Iteration 197/1000 | Loss: 0.00001048
Iteration 198/1000 | Loss: 0.00001048
Iteration 199/1000 | Loss: 0.00001047
Iteration 200/1000 | Loss: 0.00001047
Iteration 201/1000 | Loss: 0.00001047
Iteration 202/1000 | Loss: 0.00001047
Iteration 203/1000 | Loss: 0.00001047
Iteration 204/1000 | Loss: 0.00001047
Iteration 205/1000 | Loss: 0.00001047
Iteration 206/1000 | Loss: 0.00001047
Iteration 207/1000 | Loss: 0.00001047
Iteration 208/1000 | Loss: 0.00001047
Iteration 209/1000 | Loss: 0.00001047
Iteration 210/1000 | Loss: 0.00001046
Iteration 211/1000 | Loss: 0.00001046
Iteration 212/1000 | Loss: 0.00001046
Iteration 213/1000 | Loss: 0.00001046
Iteration 214/1000 | Loss: 0.00001046
Iteration 215/1000 | Loss: 0.00001046
Iteration 216/1000 | Loss: 0.00001046
Iteration 217/1000 | Loss: 0.00001046
Iteration 218/1000 | Loss: 0.00001046
Iteration 219/1000 | Loss: 0.00001046
Iteration 220/1000 | Loss: 0.00001046
Iteration 221/1000 | Loss: 0.00001046
Iteration 222/1000 | Loss: 0.00001046
Iteration 223/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.0457886673975736e-05, 1.0457886673975736e-05, 1.0457886673975736e-05, 1.0457886673975736e-05, 1.0457886673975736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0457886673975736e-05

Optimization complete. Final v2v error: 2.768951654434204 mm

Highest mean error: 3.0316529273986816 mm for frame 75

Lowest mean error: 2.6269710063934326 mm for frame 183

Saving results

Total time: 47.156594038009644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789465
Iteration 2/25 | Loss: 0.00146868
Iteration 3/25 | Loss: 0.00137995
Iteration 4/25 | Loss: 0.00136856
Iteration 5/25 | Loss: 0.00136511
Iteration 6/25 | Loss: 0.00136511
Iteration 7/25 | Loss: 0.00136511
Iteration 8/25 | Loss: 0.00136511
Iteration 9/25 | Loss: 0.00136511
Iteration 10/25 | Loss: 0.00136511
Iteration 11/25 | Loss: 0.00136511
Iteration 12/25 | Loss: 0.00136511
Iteration 13/25 | Loss: 0.00136511
Iteration 14/25 | Loss: 0.00136511
Iteration 15/25 | Loss: 0.00136511
Iteration 16/25 | Loss: 0.00136511
Iteration 17/25 | Loss: 0.00136511
Iteration 18/25 | Loss: 0.00136511
Iteration 19/25 | Loss: 0.00136511
Iteration 20/25 | Loss: 0.00136511
Iteration 21/25 | Loss: 0.00136511
Iteration 22/25 | Loss: 0.00136511
Iteration 23/25 | Loss: 0.00136511
Iteration 24/25 | Loss: 0.00136511
Iteration 25/25 | Loss: 0.00136511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21496594
Iteration 2/25 | Loss: 0.00119786
Iteration 3/25 | Loss: 0.00119780
Iteration 4/25 | Loss: 0.00119780
Iteration 5/25 | Loss: 0.00119780
Iteration 6/25 | Loss: 0.00119780
Iteration 7/25 | Loss: 0.00119780
Iteration 8/25 | Loss: 0.00119780
Iteration 9/25 | Loss: 0.00119780
Iteration 10/25 | Loss: 0.00119780
Iteration 11/25 | Loss: 0.00119780
Iteration 12/25 | Loss: 0.00119780
Iteration 13/25 | Loss: 0.00119780
Iteration 14/25 | Loss: 0.00119780
Iteration 15/25 | Loss: 0.00119780
Iteration 16/25 | Loss: 0.00119780
Iteration 17/25 | Loss: 0.00119780
Iteration 18/25 | Loss: 0.00119780
Iteration 19/25 | Loss: 0.00119780
Iteration 20/25 | Loss: 0.00119780
Iteration 21/25 | Loss: 0.00119780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011978005059063435, 0.0011978005059063435, 0.0011978005059063435, 0.0011978005059063435, 0.0011978005059063435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011978005059063435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119780
Iteration 2/1000 | Loss: 0.00003877
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002814
Iteration 5/1000 | Loss: 0.00002677
Iteration 6/1000 | Loss: 0.00002589
Iteration 7/1000 | Loss: 0.00002520
Iteration 8/1000 | Loss: 0.00002517
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002423
Iteration 11/1000 | Loss: 0.00002395
Iteration 12/1000 | Loss: 0.00002385
Iteration 13/1000 | Loss: 0.00002358
Iteration 14/1000 | Loss: 0.00002332
Iteration 15/1000 | Loss: 0.00002310
Iteration 16/1000 | Loss: 0.00002305
Iteration 17/1000 | Loss: 0.00002288
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002276
Iteration 20/1000 | Loss: 0.00002276
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002275
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00002274
Iteration 26/1000 | Loss: 0.00002274
Iteration 27/1000 | Loss: 0.00002273
Iteration 28/1000 | Loss: 0.00002273
Iteration 29/1000 | Loss: 0.00002273
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002273
Iteration 32/1000 | Loss: 0.00002273
Iteration 33/1000 | Loss: 0.00002273
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002272
Iteration 36/1000 | Loss: 0.00002271
Iteration 37/1000 | Loss: 0.00002271
Iteration 38/1000 | Loss: 0.00002270
Iteration 39/1000 | Loss: 0.00002270
Iteration 40/1000 | Loss: 0.00002270
Iteration 41/1000 | Loss: 0.00002269
Iteration 42/1000 | Loss: 0.00002269
Iteration 43/1000 | Loss: 0.00002268
Iteration 44/1000 | Loss: 0.00002268
Iteration 45/1000 | Loss: 0.00002267
Iteration 46/1000 | Loss: 0.00002267
Iteration 47/1000 | Loss: 0.00002266
Iteration 48/1000 | Loss: 0.00002266
Iteration 49/1000 | Loss: 0.00002266
Iteration 50/1000 | Loss: 0.00002266
Iteration 51/1000 | Loss: 0.00002266
Iteration 52/1000 | Loss: 0.00002265
Iteration 53/1000 | Loss: 0.00002265
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002263
Iteration 57/1000 | Loss: 0.00002263
Iteration 58/1000 | Loss: 0.00002263
Iteration 59/1000 | Loss: 0.00002263
Iteration 60/1000 | Loss: 0.00002263
Iteration 61/1000 | Loss: 0.00002263
Iteration 62/1000 | Loss: 0.00002263
Iteration 63/1000 | Loss: 0.00002263
Iteration 64/1000 | Loss: 0.00002263
Iteration 65/1000 | Loss: 0.00002262
Iteration 66/1000 | Loss: 0.00002262
Iteration 67/1000 | Loss: 0.00002262
Iteration 68/1000 | Loss: 0.00002262
Iteration 69/1000 | Loss: 0.00002262
Iteration 70/1000 | Loss: 0.00002261
Iteration 71/1000 | Loss: 0.00002261
Iteration 72/1000 | Loss: 0.00002260
Iteration 73/1000 | Loss: 0.00002260
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002259
Iteration 78/1000 | Loss: 0.00002259
Iteration 79/1000 | Loss: 0.00002259
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002259
Iteration 82/1000 | Loss: 0.00002259
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00002258
Iteration 86/1000 | Loss: 0.00002258
Iteration 87/1000 | Loss: 0.00002258
Iteration 88/1000 | Loss: 0.00002258
Iteration 89/1000 | Loss: 0.00002258
Iteration 90/1000 | Loss: 0.00002258
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002255
Iteration 103/1000 | Loss: 0.00002255
Iteration 104/1000 | Loss: 0.00002255
Iteration 105/1000 | Loss: 0.00002255
Iteration 106/1000 | Loss: 0.00002255
Iteration 107/1000 | Loss: 0.00002255
Iteration 108/1000 | Loss: 0.00002255
Iteration 109/1000 | Loss: 0.00002255
Iteration 110/1000 | Loss: 0.00002255
Iteration 111/1000 | Loss: 0.00002254
Iteration 112/1000 | Loss: 0.00002254
Iteration 113/1000 | Loss: 0.00002254
Iteration 114/1000 | Loss: 0.00002254
Iteration 115/1000 | Loss: 0.00002254
Iteration 116/1000 | Loss: 0.00002254
Iteration 117/1000 | Loss: 0.00002254
Iteration 118/1000 | Loss: 0.00002254
Iteration 119/1000 | Loss: 0.00002253
Iteration 120/1000 | Loss: 0.00002253
Iteration 121/1000 | Loss: 0.00002253
Iteration 122/1000 | Loss: 0.00002253
Iteration 123/1000 | Loss: 0.00002253
Iteration 124/1000 | Loss: 0.00002253
Iteration 125/1000 | Loss: 0.00002253
Iteration 126/1000 | Loss: 0.00002253
Iteration 127/1000 | Loss: 0.00002253
Iteration 128/1000 | Loss: 0.00002253
Iteration 129/1000 | Loss: 0.00002253
Iteration 130/1000 | Loss: 0.00002253
Iteration 131/1000 | Loss: 0.00002253
Iteration 132/1000 | Loss: 0.00002253
Iteration 133/1000 | Loss: 0.00002253
Iteration 134/1000 | Loss: 0.00002253
Iteration 135/1000 | Loss: 0.00002253
Iteration 136/1000 | Loss: 0.00002253
Iteration 137/1000 | Loss: 0.00002253
Iteration 138/1000 | Loss: 0.00002253
Iteration 139/1000 | Loss: 0.00002252
Iteration 140/1000 | Loss: 0.00002252
Iteration 141/1000 | Loss: 0.00002252
Iteration 142/1000 | Loss: 0.00002252
Iteration 143/1000 | Loss: 0.00002252
Iteration 144/1000 | Loss: 0.00002252
Iteration 145/1000 | Loss: 0.00002251
Iteration 146/1000 | Loss: 0.00002251
Iteration 147/1000 | Loss: 0.00002251
Iteration 148/1000 | Loss: 0.00002251
Iteration 149/1000 | Loss: 0.00002251
Iteration 150/1000 | Loss: 0.00002251
Iteration 151/1000 | Loss: 0.00002251
Iteration 152/1000 | Loss: 0.00002251
Iteration 153/1000 | Loss: 0.00002251
Iteration 154/1000 | Loss: 0.00002251
Iteration 155/1000 | Loss: 0.00002250
Iteration 156/1000 | Loss: 0.00002250
Iteration 157/1000 | Loss: 0.00002250
Iteration 158/1000 | Loss: 0.00002250
Iteration 159/1000 | Loss: 0.00002250
Iteration 160/1000 | Loss: 0.00002250
Iteration 161/1000 | Loss: 0.00002250
Iteration 162/1000 | Loss: 0.00002250
Iteration 163/1000 | Loss: 0.00002250
Iteration 164/1000 | Loss: 0.00002249
Iteration 165/1000 | Loss: 0.00002249
Iteration 166/1000 | Loss: 0.00002249
Iteration 167/1000 | Loss: 0.00002249
Iteration 168/1000 | Loss: 0.00002248
Iteration 169/1000 | Loss: 0.00002248
Iteration 170/1000 | Loss: 0.00002248
Iteration 171/1000 | Loss: 0.00002248
Iteration 172/1000 | Loss: 0.00002248
Iteration 173/1000 | Loss: 0.00002248
Iteration 174/1000 | Loss: 0.00002248
Iteration 175/1000 | Loss: 0.00002248
Iteration 176/1000 | Loss: 0.00002248
Iteration 177/1000 | Loss: 0.00002248
Iteration 178/1000 | Loss: 0.00002247
Iteration 179/1000 | Loss: 0.00002247
Iteration 180/1000 | Loss: 0.00002247
Iteration 181/1000 | Loss: 0.00002247
Iteration 182/1000 | Loss: 0.00002247
Iteration 183/1000 | Loss: 0.00002247
Iteration 184/1000 | Loss: 0.00002247
Iteration 185/1000 | Loss: 0.00002247
Iteration 186/1000 | Loss: 0.00002247
Iteration 187/1000 | Loss: 0.00002246
Iteration 188/1000 | Loss: 0.00002246
Iteration 189/1000 | Loss: 0.00002246
Iteration 190/1000 | Loss: 0.00002246
Iteration 191/1000 | Loss: 0.00002246
Iteration 192/1000 | Loss: 0.00002246
Iteration 193/1000 | Loss: 0.00002246
Iteration 194/1000 | Loss: 0.00002245
Iteration 195/1000 | Loss: 0.00002245
Iteration 196/1000 | Loss: 0.00002245
Iteration 197/1000 | Loss: 0.00002245
Iteration 198/1000 | Loss: 0.00002245
Iteration 199/1000 | Loss: 0.00002245
Iteration 200/1000 | Loss: 0.00002245
Iteration 201/1000 | Loss: 0.00002245
Iteration 202/1000 | Loss: 0.00002244
Iteration 203/1000 | Loss: 0.00002244
Iteration 204/1000 | Loss: 0.00002244
Iteration 205/1000 | Loss: 0.00002244
Iteration 206/1000 | Loss: 0.00002244
Iteration 207/1000 | Loss: 0.00002244
Iteration 208/1000 | Loss: 0.00002244
Iteration 209/1000 | Loss: 0.00002244
Iteration 210/1000 | Loss: 0.00002244
Iteration 211/1000 | Loss: 0.00002244
Iteration 212/1000 | Loss: 0.00002243
Iteration 213/1000 | Loss: 0.00002243
Iteration 214/1000 | Loss: 0.00002243
Iteration 215/1000 | Loss: 0.00002243
Iteration 216/1000 | Loss: 0.00002243
Iteration 217/1000 | Loss: 0.00002243
Iteration 218/1000 | Loss: 0.00002243
Iteration 219/1000 | Loss: 0.00002243
Iteration 220/1000 | Loss: 0.00002243
Iteration 221/1000 | Loss: 0.00002243
Iteration 222/1000 | Loss: 0.00002243
Iteration 223/1000 | Loss: 0.00002243
Iteration 224/1000 | Loss: 0.00002243
Iteration 225/1000 | Loss: 0.00002243
Iteration 226/1000 | Loss: 0.00002243
Iteration 227/1000 | Loss: 0.00002243
Iteration 228/1000 | Loss: 0.00002243
Iteration 229/1000 | Loss: 0.00002243
Iteration 230/1000 | Loss: 0.00002243
Iteration 231/1000 | Loss: 0.00002243
Iteration 232/1000 | Loss: 0.00002243
Iteration 233/1000 | Loss: 0.00002243
Iteration 234/1000 | Loss: 0.00002242
Iteration 235/1000 | Loss: 0.00002242
Iteration 236/1000 | Loss: 0.00002242
Iteration 237/1000 | Loss: 0.00002242
Iteration 238/1000 | Loss: 0.00002242
Iteration 239/1000 | Loss: 0.00002242
Iteration 240/1000 | Loss: 0.00002242
Iteration 241/1000 | Loss: 0.00002242
Iteration 242/1000 | Loss: 0.00002242
Iteration 243/1000 | Loss: 0.00002242
Iteration 244/1000 | Loss: 0.00002242
Iteration 245/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [2.242481059511192e-05, 2.242481059511192e-05, 2.242481059511192e-05, 2.242481059511192e-05, 2.242481059511192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.242481059511192e-05

Optimization complete. Final v2v error: 4.00032901763916 mm

Highest mean error: 4.203396797180176 mm for frame 74

Lowest mean error: 3.6860663890838623 mm for frame 210

Saving results

Total time: 50.52978229522705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00704385
Iteration 2/25 | Loss: 0.00171337
Iteration 3/25 | Loss: 0.00147377
Iteration 4/25 | Loss: 0.00143904
Iteration 5/25 | Loss: 0.00143038
Iteration 6/25 | Loss: 0.00142672
Iteration 7/25 | Loss: 0.00142539
Iteration 8/25 | Loss: 0.00142585
Iteration 9/25 | Loss: 0.00142462
Iteration 10/25 | Loss: 0.00142336
Iteration 11/25 | Loss: 0.00142132
Iteration 12/25 | Loss: 0.00142093
Iteration 13/25 | Loss: 0.00142074
Iteration 14/25 | Loss: 0.00142070
Iteration 15/25 | Loss: 0.00142070
Iteration 16/25 | Loss: 0.00142070
Iteration 17/25 | Loss: 0.00142070
Iteration 18/25 | Loss: 0.00142070
Iteration 19/25 | Loss: 0.00142069
Iteration 20/25 | Loss: 0.00142069
Iteration 21/25 | Loss: 0.00142069
Iteration 22/25 | Loss: 0.00142069
Iteration 23/25 | Loss: 0.00142069
Iteration 24/25 | Loss: 0.00142069
Iteration 25/25 | Loss: 0.00142069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21544206
Iteration 2/25 | Loss: 0.00168185
Iteration 3/25 | Loss: 0.00168182
Iteration 4/25 | Loss: 0.00168182
Iteration 5/25 | Loss: 0.00168182
Iteration 6/25 | Loss: 0.00168182
Iteration 7/25 | Loss: 0.00168182
Iteration 8/25 | Loss: 0.00168182
Iteration 9/25 | Loss: 0.00168182
Iteration 10/25 | Loss: 0.00168182
Iteration 11/25 | Loss: 0.00168182
Iteration 12/25 | Loss: 0.00168182
Iteration 13/25 | Loss: 0.00168182
Iteration 14/25 | Loss: 0.00168182
Iteration 15/25 | Loss: 0.00168182
Iteration 16/25 | Loss: 0.00168182
Iteration 17/25 | Loss: 0.00168182
Iteration 18/25 | Loss: 0.00168182
Iteration 19/25 | Loss: 0.00168182
Iteration 20/25 | Loss: 0.00168182
Iteration 21/25 | Loss: 0.00168182
Iteration 22/25 | Loss: 0.00168182
Iteration 23/25 | Loss: 0.00168182
Iteration 24/25 | Loss: 0.00168182
Iteration 25/25 | Loss: 0.00168182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168182
Iteration 2/1000 | Loss: 0.00009580
Iteration 3/1000 | Loss: 0.00006596
Iteration 4/1000 | Loss: 0.00005792
Iteration 5/1000 | Loss: 0.00005329
Iteration 6/1000 | Loss: 0.00005119
Iteration 7/1000 | Loss: 0.00004962
Iteration 8/1000 | Loss: 0.00004898
Iteration 9/1000 | Loss: 0.00004829
Iteration 10/1000 | Loss: 0.00004778
Iteration 11/1000 | Loss: 0.00004727
Iteration 12/1000 | Loss: 0.00004687
Iteration 13/1000 | Loss: 0.00004652
Iteration 14/1000 | Loss: 0.00004619
Iteration 15/1000 | Loss: 0.00004597
Iteration 16/1000 | Loss: 0.00004570
Iteration 17/1000 | Loss: 0.00004550
Iteration 18/1000 | Loss: 0.00004533
Iteration 19/1000 | Loss: 0.00004525
Iteration 20/1000 | Loss: 0.00004523
Iteration 21/1000 | Loss: 0.00004520
Iteration 22/1000 | Loss: 0.00004519
Iteration 23/1000 | Loss: 0.00004513
Iteration 24/1000 | Loss: 0.00004510
Iteration 25/1000 | Loss: 0.00004509
Iteration 26/1000 | Loss: 0.00004507
Iteration 27/1000 | Loss: 0.00004506
Iteration 28/1000 | Loss: 0.00004500
Iteration 29/1000 | Loss: 0.00004494
Iteration 30/1000 | Loss: 0.00004494
Iteration 31/1000 | Loss: 0.00004487
Iteration 32/1000 | Loss: 0.00004485
Iteration 33/1000 | Loss: 0.00004485
Iteration 34/1000 | Loss: 0.00004484
Iteration 35/1000 | Loss: 0.00004484
Iteration 36/1000 | Loss: 0.00004480
Iteration 37/1000 | Loss: 0.00004480
Iteration 38/1000 | Loss: 0.00004480
Iteration 39/1000 | Loss: 0.00004480
Iteration 40/1000 | Loss: 0.00004478
Iteration 41/1000 | Loss: 0.00004476
Iteration 42/1000 | Loss: 0.00004476
Iteration 43/1000 | Loss: 0.00004475
Iteration 44/1000 | Loss: 0.00004474
Iteration 45/1000 | Loss: 0.00004473
Iteration 46/1000 | Loss: 0.00004473
Iteration 47/1000 | Loss: 0.00004473
Iteration 48/1000 | Loss: 0.00004473
Iteration 49/1000 | Loss: 0.00004473
Iteration 50/1000 | Loss: 0.00004473
Iteration 51/1000 | Loss: 0.00004473
Iteration 52/1000 | Loss: 0.00004473
Iteration 53/1000 | Loss: 0.00004473
Iteration 54/1000 | Loss: 0.00004473
Iteration 55/1000 | Loss: 0.00004472
Iteration 56/1000 | Loss: 0.00004472
Iteration 57/1000 | Loss: 0.00004472
Iteration 58/1000 | Loss: 0.00004472
Iteration 59/1000 | Loss: 0.00004472
Iteration 60/1000 | Loss: 0.00004471
Iteration 61/1000 | Loss: 0.00004470
Iteration 62/1000 | Loss: 0.00004470
Iteration 63/1000 | Loss: 0.00004470
Iteration 64/1000 | Loss: 0.00004470
Iteration 65/1000 | Loss: 0.00004470
Iteration 66/1000 | Loss: 0.00004470
Iteration 67/1000 | Loss: 0.00004470
Iteration 68/1000 | Loss: 0.00004470
Iteration 69/1000 | Loss: 0.00004470
Iteration 70/1000 | Loss: 0.00004469
Iteration 71/1000 | Loss: 0.00004469
Iteration 72/1000 | Loss: 0.00004469
Iteration 73/1000 | Loss: 0.00004469
Iteration 74/1000 | Loss: 0.00004469
Iteration 75/1000 | Loss: 0.00004469
Iteration 76/1000 | Loss: 0.00004469
Iteration 77/1000 | Loss: 0.00004469
Iteration 78/1000 | Loss: 0.00004468
Iteration 79/1000 | Loss: 0.00004468
Iteration 80/1000 | Loss: 0.00004467
Iteration 81/1000 | Loss: 0.00004467
Iteration 82/1000 | Loss: 0.00004467
Iteration 83/1000 | Loss: 0.00004467
Iteration 84/1000 | Loss: 0.00004467
Iteration 85/1000 | Loss: 0.00004467
Iteration 86/1000 | Loss: 0.00004467
Iteration 87/1000 | Loss: 0.00004466
Iteration 88/1000 | Loss: 0.00004466
Iteration 89/1000 | Loss: 0.00004466
Iteration 90/1000 | Loss: 0.00004466
Iteration 91/1000 | Loss: 0.00004466
Iteration 92/1000 | Loss: 0.00004466
Iteration 93/1000 | Loss: 0.00004466
Iteration 94/1000 | Loss: 0.00004465
Iteration 95/1000 | Loss: 0.00004465
Iteration 96/1000 | Loss: 0.00004465
Iteration 97/1000 | Loss: 0.00004465
Iteration 98/1000 | Loss: 0.00004464
Iteration 99/1000 | Loss: 0.00004464
Iteration 100/1000 | Loss: 0.00004464
Iteration 101/1000 | Loss: 0.00004464
Iteration 102/1000 | Loss: 0.00004464
Iteration 103/1000 | Loss: 0.00004464
Iteration 104/1000 | Loss: 0.00004464
Iteration 105/1000 | Loss: 0.00004464
Iteration 106/1000 | Loss: 0.00004463
Iteration 107/1000 | Loss: 0.00004463
Iteration 108/1000 | Loss: 0.00004463
Iteration 109/1000 | Loss: 0.00004463
Iteration 110/1000 | Loss: 0.00004463
Iteration 111/1000 | Loss: 0.00004462
Iteration 112/1000 | Loss: 0.00004462
Iteration 113/1000 | Loss: 0.00004462
Iteration 114/1000 | Loss: 0.00004462
Iteration 115/1000 | Loss: 0.00004462
Iteration 116/1000 | Loss: 0.00004462
Iteration 117/1000 | Loss: 0.00004461
Iteration 118/1000 | Loss: 0.00004461
Iteration 119/1000 | Loss: 0.00004461
Iteration 120/1000 | Loss: 0.00004461
Iteration 121/1000 | Loss: 0.00004461
Iteration 122/1000 | Loss: 0.00004461
Iteration 123/1000 | Loss: 0.00004460
Iteration 124/1000 | Loss: 0.00004460
Iteration 125/1000 | Loss: 0.00004460
Iteration 126/1000 | Loss: 0.00004460
Iteration 127/1000 | Loss: 0.00004460
Iteration 128/1000 | Loss: 0.00004460
Iteration 129/1000 | Loss: 0.00004459
Iteration 130/1000 | Loss: 0.00004459
Iteration 131/1000 | Loss: 0.00004459
Iteration 132/1000 | Loss: 0.00004459
Iteration 133/1000 | Loss: 0.00004458
Iteration 134/1000 | Loss: 0.00004458
Iteration 135/1000 | Loss: 0.00004458
Iteration 136/1000 | Loss: 0.00004458
Iteration 137/1000 | Loss: 0.00004458
Iteration 138/1000 | Loss: 0.00004457
Iteration 139/1000 | Loss: 0.00004457
Iteration 140/1000 | Loss: 0.00004457
Iteration 141/1000 | Loss: 0.00004457
Iteration 142/1000 | Loss: 0.00004457
Iteration 143/1000 | Loss: 0.00004456
Iteration 144/1000 | Loss: 0.00004456
Iteration 145/1000 | Loss: 0.00004456
Iteration 146/1000 | Loss: 0.00004456
Iteration 147/1000 | Loss: 0.00004456
Iteration 148/1000 | Loss: 0.00004456
Iteration 149/1000 | Loss: 0.00004456
Iteration 150/1000 | Loss: 0.00004456
Iteration 151/1000 | Loss: 0.00004456
Iteration 152/1000 | Loss: 0.00004455
Iteration 153/1000 | Loss: 0.00004455
Iteration 154/1000 | Loss: 0.00004455
Iteration 155/1000 | Loss: 0.00004455
Iteration 156/1000 | Loss: 0.00004455
Iteration 157/1000 | Loss: 0.00004455
Iteration 158/1000 | Loss: 0.00004454
Iteration 159/1000 | Loss: 0.00004454
Iteration 160/1000 | Loss: 0.00004454
Iteration 161/1000 | Loss: 0.00004454
Iteration 162/1000 | Loss: 0.00004453
Iteration 163/1000 | Loss: 0.00004452
Iteration 164/1000 | Loss: 0.00004452
Iteration 165/1000 | Loss: 0.00004452
Iteration 166/1000 | Loss: 0.00004452
Iteration 167/1000 | Loss: 0.00004452
Iteration 168/1000 | Loss: 0.00004451
Iteration 169/1000 | Loss: 0.00004451
Iteration 170/1000 | Loss: 0.00004451
Iteration 171/1000 | Loss: 0.00004451
Iteration 172/1000 | Loss: 0.00004451
Iteration 173/1000 | Loss: 0.00004451
Iteration 174/1000 | Loss: 0.00004450
Iteration 175/1000 | Loss: 0.00004450
Iteration 176/1000 | Loss: 0.00004450
Iteration 177/1000 | Loss: 0.00004449
Iteration 178/1000 | Loss: 0.00004449
Iteration 179/1000 | Loss: 0.00004449
Iteration 180/1000 | Loss: 0.00004449
Iteration 181/1000 | Loss: 0.00004449
Iteration 182/1000 | Loss: 0.00004448
Iteration 183/1000 | Loss: 0.00004448
Iteration 184/1000 | Loss: 0.00004448
Iteration 185/1000 | Loss: 0.00004448
Iteration 186/1000 | Loss: 0.00004447
Iteration 187/1000 | Loss: 0.00004447
Iteration 188/1000 | Loss: 0.00004447
Iteration 189/1000 | Loss: 0.00004447
Iteration 190/1000 | Loss: 0.00004447
Iteration 191/1000 | Loss: 0.00004447
Iteration 192/1000 | Loss: 0.00004446
Iteration 193/1000 | Loss: 0.00004446
Iteration 194/1000 | Loss: 0.00004446
Iteration 195/1000 | Loss: 0.00004445
Iteration 196/1000 | Loss: 0.00004445
Iteration 197/1000 | Loss: 0.00004445
Iteration 198/1000 | Loss: 0.00004444
Iteration 199/1000 | Loss: 0.00004444
Iteration 200/1000 | Loss: 0.00004444
Iteration 201/1000 | Loss: 0.00004443
Iteration 202/1000 | Loss: 0.00004443
Iteration 203/1000 | Loss: 0.00004443
Iteration 204/1000 | Loss: 0.00004443
Iteration 205/1000 | Loss: 0.00004443
Iteration 206/1000 | Loss: 0.00004443
Iteration 207/1000 | Loss: 0.00004443
Iteration 208/1000 | Loss: 0.00004443
Iteration 209/1000 | Loss: 0.00004443
Iteration 210/1000 | Loss: 0.00004442
Iteration 211/1000 | Loss: 0.00004442
Iteration 212/1000 | Loss: 0.00004442
Iteration 213/1000 | Loss: 0.00004442
Iteration 214/1000 | Loss: 0.00004442
Iteration 215/1000 | Loss: 0.00004441
Iteration 216/1000 | Loss: 0.00004441
Iteration 217/1000 | Loss: 0.00004441
Iteration 218/1000 | Loss: 0.00004441
Iteration 219/1000 | Loss: 0.00004441
Iteration 220/1000 | Loss: 0.00004440
Iteration 221/1000 | Loss: 0.00004440
Iteration 222/1000 | Loss: 0.00004440
Iteration 223/1000 | Loss: 0.00004439
Iteration 224/1000 | Loss: 0.00004439
Iteration 225/1000 | Loss: 0.00004439
Iteration 226/1000 | Loss: 0.00004439
Iteration 227/1000 | Loss: 0.00004439
Iteration 228/1000 | Loss: 0.00004439
Iteration 229/1000 | Loss: 0.00004439
Iteration 230/1000 | Loss: 0.00004439
Iteration 231/1000 | Loss: 0.00004438
Iteration 232/1000 | Loss: 0.00004438
Iteration 233/1000 | Loss: 0.00004438
Iteration 234/1000 | Loss: 0.00004438
Iteration 235/1000 | Loss: 0.00004438
Iteration 236/1000 | Loss: 0.00004438
Iteration 237/1000 | Loss: 0.00004438
Iteration 238/1000 | Loss: 0.00004438
Iteration 239/1000 | Loss: 0.00004437
Iteration 240/1000 | Loss: 0.00004437
Iteration 241/1000 | Loss: 0.00004437
Iteration 242/1000 | Loss: 0.00004437
Iteration 243/1000 | Loss: 0.00004437
Iteration 244/1000 | Loss: 0.00004437
Iteration 245/1000 | Loss: 0.00004437
Iteration 246/1000 | Loss: 0.00004437
Iteration 247/1000 | Loss: 0.00004437
Iteration 248/1000 | Loss: 0.00004436
Iteration 249/1000 | Loss: 0.00004436
Iteration 250/1000 | Loss: 0.00004436
Iteration 251/1000 | Loss: 0.00004436
Iteration 252/1000 | Loss: 0.00004436
Iteration 253/1000 | Loss: 0.00004436
Iteration 254/1000 | Loss: 0.00004436
Iteration 255/1000 | Loss: 0.00004436
Iteration 256/1000 | Loss: 0.00004436
Iteration 257/1000 | Loss: 0.00004435
Iteration 258/1000 | Loss: 0.00004435
Iteration 259/1000 | Loss: 0.00004435
Iteration 260/1000 | Loss: 0.00004435
Iteration 261/1000 | Loss: 0.00004435
Iteration 262/1000 | Loss: 0.00004435
Iteration 263/1000 | Loss: 0.00004434
Iteration 264/1000 | Loss: 0.00004434
Iteration 265/1000 | Loss: 0.00004434
Iteration 266/1000 | Loss: 0.00004434
Iteration 267/1000 | Loss: 0.00004433
Iteration 268/1000 | Loss: 0.00004433
Iteration 269/1000 | Loss: 0.00004433
Iteration 270/1000 | Loss: 0.00004433
Iteration 271/1000 | Loss: 0.00004433
Iteration 272/1000 | Loss: 0.00004432
Iteration 273/1000 | Loss: 0.00004432
Iteration 274/1000 | Loss: 0.00004432
Iteration 275/1000 | Loss: 0.00004432
Iteration 276/1000 | Loss: 0.00004432
Iteration 277/1000 | Loss: 0.00004432
Iteration 278/1000 | Loss: 0.00004432
Iteration 279/1000 | Loss: 0.00004432
Iteration 280/1000 | Loss: 0.00004432
Iteration 281/1000 | Loss: 0.00004432
Iteration 282/1000 | Loss: 0.00004432
Iteration 283/1000 | Loss: 0.00004432
Iteration 284/1000 | Loss: 0.00004432
Iteration 285/1000 | Loss: 0.00004432
Iteration 286/1000 | Loss: 0.00004432
Iteration 287/1000 | Loss: 0.00004432
Iteration 288/1000 | Loss: 0.00004432
Iteration 289/1000 | Loss: 0.00004432
Iteration 290/1000 | Loss: 0.00004432
Iteration 291/1000 | Loss: 0.00004432
Iteration 292/1000 | Loss: 0.00004432
Iteration 293/1000 | Loss: 0.00004432
Iteration 294/1000 | Loss: 0.00004432
Iteration 295/1000 | Loss: 0.00004432
Iteration 296/1000 | Loss: 0.00004432
Iteration 297/1000 | Loss: 0.00004432
Iteration 298/1000 | Loss: 0.00004432
Iteration 299/1000 | Loss: 0.00004432
Iteration 300/1000 | Loss: 0.00004432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [4.4323878682916984e-05, 4.4323878682916984e-05, 4.4323878682916984e-05, 4.4323878682916984e-05, 4.4323878682916984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4323878682916984e-05

Optimization complete. Final v2v error: 4.218677520751953 mm

Highest mean error: 12.028701782226562 mm for frame 20

Lowest mean error: 3.067034959793091 mm for frame 230

Saving results

Total time: 84.80345630645752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465493
Iteration 2/25 | Loss: 0.00142428
Iteration 3/25 | Loss: 0.00135396
Iteration 4/25 | Loss: 0.00133888
Iteration 5/25 | Loss: 0.00133499
Iteration 6/25 | Loss: 0.00133499
Iteration 7/25 | Loss: 0.00133499
Iteration 8/25 | Loss: 0.00133499
Iteration 9/25 | Loss: 0.00133499
Iteration 10/25 | Loss: 0.00133499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013349853688850999, 0.0013349853688850999, 0.0013349853688850999, 0.0013349853688850999, 0.0013349853688850999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013349853688850999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30253458
Iteration 2/25 | Loss: 0.00124540
Iteration 3/25 | Loss: 0.00124538
Iteration 4/25 | Loss: 0.00124538
Iteration 5/25 | Loss: 0.00124538
Iteration 6/25 | Loss: 0.00124538
Iteration 7/25 | Loss: 0.00124538
Iteration 8/25 | Loss: 0.00124538
Iteration 9/25 | Loss: 0.00124538
Iteration 10/25 | Loss: 0.00124538
Iteration 11/25 | Loss: 0.00124538
Iteration 12/25 | Loss: 0.00124538
Iteration 13/25 | Loss: 0.00124538
Iteration 14/25 | Loss: 0.00124538
Iteration 15/25 | Loss: 0.00124538
Iteration 16/25 | Loss: 0.00124538
Iteration 17/25 | Loss: 0.00124538
Iteration 18/25 | Loss: 0.00124538
Iteration 19/25 | Loss: 0.00124538
Iteration 20/25 | Loss: 0.00124538
Iteration 21/25 | Loss: 0.00124538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00124537933152169, 0.00124537933152169, 0.00124537933152169, 0.00124537933152169, 0.00124537933152169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124537933152169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124538
Iteration 2/1000 | Loss: 0.00003011
Iteration 3/1000 | Loss: 0.00002174
Iteration 4/1000 | Loss: 0.00001942
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001600
Iteration 14/1000 | Loss: 0.00001584
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001557
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001556
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001542
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001529
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001501
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001500
Iteration 143/1000 | Loss: 0.00001500
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001499
Iteration 149/1000 | Loss: 0.00001499
Iteration 150/1000 | Loss: 0.00001499
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001495
Iteration 169/1000 | Loss: 0.00001495
Iteration 170/1000 | Loss: 0.00001495
Iteration 171/1000 | Loss: 0.00001495
Iteration 172/1000 | Loss: 0.00001495
Iteration 173/1000 | Loss: 0.00001495
Iteration 174/1000 | Loss: 0.00001495
Iteration 175/1000 | Loss: 0.00001495
Iteration 176/1000 | Loss: 0.00001495
Iteration 177/1000 | Loss: 0.00001495
Iteration 178/1000 | Loss: 0.00001495
Iteration 179/1000 | Loss: 0.00001495
Iteration 180/1000 | Loss: 0.00001494
Iteration 181/1000 | Loss: 0.00001494
Iteration 182/1000 | Loss: 0.00001494
Iteration 183/1000 | Loss: 0.00001494
Iteration 184/1000 | Loss: 0.00001494
Iteration 185/1000 | Loss: 0.00001494
Iteration 186/1000 | Loss: 0.00001494
Iteration 187/1000 | Loss: 0.00001494
Iteration 188/1000 | Loss: 0.00001494
Iteration 189/1000 | Loss: 0.00001494
Iteration 190/1000 | Loss: 0.00001494
Iteration 191/1000 | Loss: 0.00001494
Iteration 192/1000 | Loss: 0.00001494
Iteration 193/1000 | Loss: 0.00001494
Iteration 194/1000 | Loss: 0.00001493
Iteration 195/1000 | Loss: 0.00001493
Iteration 196/1000 | Loss: 0.00001493
Iteration 197/1000 | Loss: 0.00001493
Iteration 198/1000 | Loss: 0.00001493
Iteration 199/1000 | Loss: 0.00001493
Iteration 200/1000 | Loss: 0.00001493
Iteration 201/1000 | Loss: 0.00001493
Iteration 202/1000 | Loss: 0.00001493
Iteration 203/1000 | Loss: 0.00001493
Iteration 204/1000 | Loss: 0.00001493
Iteration 205/1000 | Loss: 0.00001493
Iteration 206/1000 | Loss: 0.00001493
Iteration 207/1000 | Loss: 0.00001493
Iteration 208/1000 | Loss: 0.00001493
Iteration 209/1000 | Loss: 0.00001493
Iteration 210/1000 | Loss: 0.00001493
Iteration 211/1000 | Loss: 0.00001493
Iteration 212/1000 | Loss: 0.00001493
Iteration 213/1000 | Loss: 0.00001492
Iteration 214/1000 | Loss: 0.00001492
Iteration 215/1000 | Loss: 0.00001492
Iteration 216/1000 | Loss: 0.00001492
Iteration 217/1000 | Loss: 0.00001492
Iteration 218/1000 | Loss: 0.00001492
Iteration 219/1000 | Loss: 0.00001492
Iteration 220/1000 | Loss: 0.00001492
Iteration 221/1000 | Loss: 0.00001492
Iteration 222/1000 | Loss: 0.00001492
Iteration 223/1000 | Loss: 0.00001492
Iteration 224/1000 | Loss: 0.00001492
Iteration 225/1000 | Loss: 0.00001492
Iteration 226/1000 | Loss: 0.00001492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.4921641195542179e-05, 1.4921641195542179e-05, 1.4921641195542179e-05, 1.4921641195542179e-05, 1.4921641195542179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4921641195542179e-05

Optimization complete. Final v2v error: 3.244178533554077 mm

Highest mean error: 3.5603280067443848 mm for frame 192

Lowest mean error: 3.036031723022461 mm for frame 237

Saving results

Total time: 50.471529483795166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354747
Iteration 2/25 | Loss: 0.00133807
Iteration 3/25 | Loss: 0.00126691
Iteration 4/25 | Loss: 0.00125741
Iteration 5/25 | Loss: 0.00125516
Iteration 6/25 | Loss: 0.00125469
Iteration 7/25 | Loss: 0.00125469
Iteration 8/25 | Loss: 0.00125469
Iteration 9/25 | Loss: 0.00125469
Iteration 10/25 | Loss: 0.00125469
Iteration 11/25 | Loss: 0.00125469
Iteration 12/25 | Loss: 0.00125469
Iteration 13/25 | Loss: 0.00125469
Iteration 14/25 | Loss: 0.00125469
Iteration 15/25 | Loss: 0.00125469
Iteration 16/25 | Loss: 0.00125469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012546881334856153, 0.0012546881334856153, 0.0012546881334856153, 0.0012546881334856153, 0.0012546881334856153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012546881334856153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31308866
Iteration 2/25 | Loss: 0.00163722
Iteration 3/25 | Loss: 0.00163722
Iteration 4/25 | Loss: 0.00163722
Iteration 5/25 | Loss: 0.00163722
Iteration 6/25 | Loss: 0.00163722
Iteration 7/25 | Loss: 0.00163722
Iteration 8/25 | Loss: 0.00163722
Iteration 9/25 | Loss: 0.00163722
Iteration 10/25 | Loss: 0.00163722
Iteration 11/25 | Loss: 0.00163722
Iteration 12/25 | Loss: 0.00163722
Iteration 13/25 | Loss: 0.00163722
Iteration 14/25 | Loss: 0.00163722
Iteration 15/25 | Loss: 0.00163722
Iteration 16/25 | Loss: 0.00163722
Iteration 17/25 | Loss: 0.00163722
Iteration 18/25 | Loss: 0.00163722
Iteration 19/25 | Loss: 0.00163722
Iteration 20/25 | Loss: 0.00163722
Iteration 21/25 | Loss: 0.00163722
Iteration 22/25 | Loss: 0.00163722
Iteration 23/25 | Loss: 0.00163722
Iteration 24/25 | Loss: 0.00163722
Iteration 25/25 | Loss: 0.00163722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163722
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001573
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001290
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001220
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001218
Iteration 122/1000 | Loss: 0.00001218
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001216
Iteration 133/1000 | Loss: 0.00001216
Iteration 134/1000 | Loss: 0.00001216
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001216
Iteration 137/1000 | Loss: 0.00001216
Iteration 138/1000 | Loss: 0.00001215
Iteration 139/1000 | Loss: 0.00001215
Iteration 140/1000 | Loss: 0.00001215
Iteration 141/1000 | Loss: 0.00001215
Iteration 142/1000 | Loss: 0.00001215
Iteration 143/1000 | Loss: 0.00001215
Iteration 144/1000 | Loss: 0.00001215
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001215
Iteration 149/1000 | Loss: 0.00001215
Iteration 150/1000 | Loss: 0.00001215
Iteration 151/1000 | Loss: 0.00001215
Iteration 152/1000 | Loss: 0.00001215
Iteration 153/1000 | Loss: 0.00001215
Iteration 154/1000 | Loss: 0.00001215
Iteration 155/1000 | Loss: 0.00001215
Iteration 156/1000 | Loss: 0.00001215
Iteration 157/1000 | Loss: 0.00001215
Iteration 158/1000 | Loss: 0.00001215
Iteration 159/1000 | Loss: 0.00001215
Iteration 160/1000 | Loss: 0.00001215
Iteration 161/1000 | Loss: 0.00001215
Iteration 162/1000 | Loss: 0.00001215
Iteration 163/1000 | Loss: 0.00001215
Iteration 164/1000 | Loss: 0.00001215
Iteration 165/1000 | Loss: 0.00001215
Iteration 166/1000 | Loss: 0.00001215
Iteration 167/1000 | Loss: 0.00001215
Iteration 168/1000 | Loss: 0.00001215
Iteration 169/1000 | Loss: 0.00001215
Iteration 170/1000 | Loss: 0.00001215
Iteration 171/1000 | Loss: 0.00001215
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001215
Iteration 185/1000 | Loss: 0.00001215
Iteration 186/1000 | Loss: 0.00001215
Iteration 187/1000 | Loss: 0.00001215
Iteration 188/1000 | Loss: 0.00001215
Iteration 189/1000 | Loss: 0.00001215
Iteration 190/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.2152292583778035e-05, 1.2152292583778035e-05, 1.2152292583778035e-05, 1.2152292583778035e-05, 1.2152292583778035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2152292583778035e-05

Optimization complete. Final v2v error: 2.9855175018310547 mm

Highest mean error: 3.720585584640503 mm for frame 10

Lowest mean error: 2.5721919536590576 mm for frame 134

Saving results

Total time: 40.96415328979492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040479
Iteration 2/25 | Loss: 0.00425656
Iteration 3/25 | Loss: 0.00339650
Iteration 4/25 | Loss: 0.00318905
Iteration 5/25 | Loss: 0.00335222
Iteration 6/25 | Loss: 0.00318946
Iteration 7/25 | Loss: 0.00287150
Iteration 8/25 | Loss: 0.00267859
Iteration 9/25 | Loss: 0.00253598
Iteration 10/25 | Loss: 0.00234301
Iteration 11/25 | Loss: 0.00220778
Iteration 12/25 | Loss: 0.00226308
Iteration 13/25 | Loss: 0.00223264
Iteration 14/25 | Loss: 0.00201933
Iteration 15/25 | Loss: 0.00188430
Iteration 16/25 | Loss: 0.00191087
Iteration 17/25 | Loss: 0.00187298
Iteration 18/25 | Loss: 0.00180364
Iteration 19/25 | Loss: 0.00181163
Iteration 20/25 | Loss: 0.00175696
Iteration 21/25 | Loss: 0.00175852
Iteration 22/25 | Loss: 0.00174186
Iteration 23/25 | Loss: 0.00173247
Iteration 24/25 | Loss: 0.00174103
Iteration 25/25 | Loss: 0.00175310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03244424
Iteration 2/25 | Loss: 0.00293827
Iteration 3/25 | Loss: 0.00293826
Iteration 4/25 | Loss: 0.00293826
Iteration 5/25 | Loss: 0.00293826
Iteration 6/25 | Loss: 0.00293826
Iteration 7/25 | Loss: 0.00293826
Iteration 8/25 | Loss: 0.00293826
Iteration 9/25 | Loss: 0.00293826
Iteration 10/25 | Loss: 0.00293826
Iteration 11/25 | Loss: 0.00293826
Iteration 12/25 | Loss: 0.00293826
Iteration 13/25 | Loss: 0.00293826
Iteration 14/25 | Loss: 0.00293826
Iteration 15/25 | Loss: 0.00293826
Iteration 16/25 | Loss: 0.00293826
Iteration 17/25 | Loss: 0.00293826
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002938257297500968, 0.002938257297500968, 0.002938257297500968, 0.002938257297500968, 0.002938257297500968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002938257297500968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293826
Iteration 2/1000 | Loss: 0.00059307
Iteration 3/1000 | Loss: 0.00018520
Iteration 4/1000 | Loss: 0.00015690
Iteration 5/1000 | Loss: 0.00014440
Iteration 6/1000 | Loss: 0.00013624
Iteration 7/1000 | Loss: 0.00028507
Iteration 8/1000 | Loss: 0.00013562
Iteration 9/1000 | Loss: 0.00012411
Iteration 10/1000 | Loss: 0.00272795
Iteration 11/1000 | Loss: 0.00397484
Iteration 12/1000 | Loss: 0.00067558
Iteration 13/1000 | Loss: 0.00017554
Iteration 14/1000 | Loss: 0.00012219
Iteration 15/1000 | Loss: 0.00011531
Iteration 16/1000 | Loss: 0.00009684
Iteration 17/1000 | Loss: 0.00007411
Iteration 18/1000 | Loss: 0.00007533
Iteration 19/1000 | Loss: 0.00006225
Iteration 20/1000 | Loss: 0.00005573
Iteration 21/1000 | Loss: 0.00005140
Iteration 22/1000 | Loss: 0.00004879
Iteration 23/1000 | Loss: 0.00004551
Iteration 24/1000 | Loss: 0.00004368
Iteration 25/1000 | Loss: 0.00004195
Iteration 26/1000 | Loss: 0.00005231
Iteration 27/1000 | Loss: 0.00004933
Iteration 28/1000 | Loss: 0.00004342
Iteration 29/1000 | Loss: 0.00004728
Iteration 30/1000 | Loss: 0.00004644
Iteration 31/1000 | Loss: 0.00005082
Iteration 32/1000 | Loss: 0.00004287
Iteration 33/1000 | Loss: 0.00004936
Iteration 34/1000 | Loss: 0.00005104
Iteration 35/1000 | Loss: 0.00004488
Iteration 36/1000 | Loss: 0.00004246
Iteration 37/1000 | Loss: 0.00004448
Iteration 38/1000 | Loss: 0.00004052
Iteration 39/1000 | Loss: 0.00003945
Iteration 40/1000 | Loss: 0.00003891
Iteration 41/1000 | Loss: 0.00003855
Iteration 42/1000 | Loss: 0.00003803
Iteration 43/1000 | Loss: 0.00003778
Iteration 44/1000 | Loss: 0.00003773
Iteration 45/1000 | Loss: 0.00003770
Iteration 46/1000 | Loss: 0.00003770
Iteration 47/1000 | Loss: 0.00003769
Iteration 48/1000 | Loss: 0.00003769
Iteration 49/1000 | Loss: 0.00003768
Iteration 50/1000 | Loss: 0.00003768
Iteration 51/1000 | Loss: 0.00003768
Iteration 52/1000 | Loss: 0.00003767
Iteration 53/1000 | Loss: 0.00003766
Iteration 54/1000 | Loss: 0.00003766
Iteration 55/1000 | Loss: 0.00003766
Iteration 56/1000 | Loss: 0.00003765
Iteration 57/1000 | Loss: 0.00003765
Iteration 58/1000 | Loss: 0.00003765
Iteration 59/1000 | Loss: 0.00003765
Iteration 60/1000 | Loss: 0.00003764
Iteration 61/1000 | Loss: 0.00003764
Iteration 62/1000 | Loss: 0.00003764
Iteration 63/1000 | Loss: 0.00003763
Iteration 64/1000 | Loss: 0.00003763
Iteration 65/1000 | Loss: 0.00003762
Iteration 66/1000 | Loss: 0.00003762
Iteration 67/1000 | Loss: 0.00003762
Iteration 68/1000 | Loss: 0.00003762
Iteration 69/1000 | Loss: 0.00003762
Iteration 70/1000 | Loss: 0.00003762
Iteration 71/1000 | Loss: 0.00003762
Iteration 72/1000 | Loss: 0.00003761
Iteration 73/1000 | Loss: 0.00003761
Iteration 74/1000 | Loss: 0.00003761
Iteration 75/1000 | Loss: 0.00003761
Iteration 76/1000 | Loss: 0.00003761
Iteration 77/1000 | Loss: 0.00003760
Iteration 78/1000 | Loss: 0.00003759
Iteration 79/1000 | Loss: 0.00003759
Iteration 80/1000 | Loss: 0.00003759
Iteration 81/1000 | Loss: 0.00003759
Iteration 82/1000 | Loss: 0.00003758
Iteration 83/1000 | Loss: 0.00003758
Iteration 84/1000 | Loss: 0.00003758
Iteration 85/1000 | Loss: 0.00003758
Iteration 86/1000 | Loss: 0.00003758
Iteration 87/1000 | Loss: 0.00003758
Iteration 88/1000 | Loss: 0.00003758
Iteration 89/1000 | Loss: 0.00003757
Iteration 90/1000 | Loss: 0.00003757
Iteration 91/1000 | Loss: 0.00003757
Iteration 92/1000 | Loss: 0.00003756
Iteration 93/1000 | Loss: 0.00003755
Iteration 94/1000 | Loss: 0.00003755
Iteration 95/1000 | Loss: 0.00003755
Iteration 96/1000 | Loss: 0.00003755
Iteration 97/1000 | Loss: 0.00003755
Iteration 98/1000 | Loss: 0.00003754
Iteration 99/1000 | Loss: 0.00003754
Iteration 100/1000 | Loss: 0.00003753
Iteration 101/1000 | Loss: 0.00003752
Iteration 102/1000 | Loss: 0.00003751
Iteration 103/1000 | Loss: 0.00003750
Iteration 104/1000 | Loss: 0.00003750
Iteration 105/1000 | Loss: 0.00003749
Iteration 106/1000 | Loss: 0.00003749
Iteration 107/1000 | Loss: 0.00003748
Iteration 108/1000 | Loss: 0.00003748
Iteration 109/1000 | Loss: 0.00003747
Iteration 110/1000 | Loss: 0.00003747
Iteration 111/1000 | Loss: 0.00003733
Iteration 112/1000 | Loss: 0.00003731
Iteration 113/1000 | Loss: 0.00003718
Iteration 114/1000 | Loss: 0.00003715
Iteration 115/1000 | Loss: 0.00003700
Iteration 116/1000 | Loss: 0.00003700
Iteration 117/1000 | Loss: 0.00003698
Iteration 118/1000 | Loss: 0.00003697
Iteration 119/1000 | Loss: 0.00003697
Iteration 120/1000 | Loss: 0.00003697
Iteration 121/1000 | Loss: 0.00003696
Iteration 122/1000 | Loss: 0.00003696
Iteration 123/1000 | Loss: 0.00003695
Iteration 124/1000 | Loss: 0.00003694
Iteration 125/1000 | Loss: 0.00003694
Iteration 126/1000 | Loss: 0.00003694
Iteration 127/1000 | Loss: 0.00003693
Iteration 128/1000 | Loss: 0.00003692
Iteration 129/1000 | Loss: 0.00003692
Iteration 130/1000 | Loss: 0.00003690
Iteration 131/1000 | Loss: 0.00003685
Iteration 132/1000 | Loss: 0.00003682
Iteration 133/1000 | Loss: 0.00003681
Iteration 134/1000 | Loss: 0.00003680
Iteration 135/1000 | Loss: 0.00003676
Iteration 136/1000 | Loss: 0.00003672
Iteration 137/1000 | Loss: 0.00003666
Iteration 138/1000 | Loss: 0.00003666
Iteration 139/1000 | Loss: 0.00003663
Iteration 140/1000 | Loss: 0.00003663
Iteration 141/1000 | Loss: 0.00003662
Iteration 142/1000 | Loss: 0.00003662
Iteration 143/1000 | Loss: 0.00003660
Iteration 144/1000 | Loss: 0.00003658
Iteration 145/1000 | Loss: 0.00003658
Iteration 146/1000 | Loss: 0.00003658
Iteration 147/1000 | Loss: 0.00003658
Iteration 148/1000 | Loss: 0.00003658
Iteration 149/1000 | Loss: 0.00003658
Iteration 150/1000 | Loss: 0.00003657
Iteration 151/1000 | Loss: 0.00003656
Iteration 152/1000 | Loss: 0.00003647
Iteration 153/1000 | Loss: 0.00003647
Iteration 154/1000 | Loss: 0.00003647
Iteration 155/1000 | Loss: 0.00003645
Iteration 156/1000 | Loss: 0.00003644
Iteration 157/1000 | Loss: 0.00003643
Iteration 158/1000 | Loss: 0.00003638
Iteration 159/1000 | Loss: 0.00027388
Iteration 160/1000 | Loss: 0.00013092
Iteration 161/1000 | Loss: 0.00023307
Iteration 162/1000 | Loss: 0.00010248
Iteration 163/1000 | Loss: 0.00003674
Iteration 164/1000 | Loss: 0.00003642
Iteration 165/1000 | Loss: 0.00003640
Iteration 166/1000 | Loss: 0.00003638
Iteration 167/1000 | Loss: 0.00003638
Iteration 168/1000 | Loss: 0.00003638
Iteration 169/1000 | Loss: 0.00003638
Iteration 170/1000 | Loss: 0.00003638
Iteration 171/1000 | Loss: 0.00003638
Iteration 172/1000 | Loss: 0.00003638
Iteration 173/1000 | Loss: 0.00003638
Iteration 174/1000 | Loss: 0.00003638
Iteration 175/1000 | Loss: 0.00003637
Iteration 176/1000 | Loss: 0.00003637
Iteration 177/1000 | Loss: 0.00003637
Iteration 178/1000 | Loss: 0.00003636
Iteration 179/1000 | Loss: 0.00003636
Iteration 180/1000 | Loss: 0.00003636
Iteration 181/1000 | Loss: 0.00003635
Iteration 182/1000 | Loss: 0.00003635
Iteration 183/1000 | Loss: 0.00003635
Iteration 184/1000 | Loss: 0.00003635
Iteration 185/1000 | Loss: 0.00003635
Iteration 186/1000 | Loss: 0.00003635
Iteration 187/1000 | Loss: 0.00003635
Iteration 188/1000 | Loss: 0.00003634
Iteration 189/1000 | Loss: 0.00003634
Iteration 190/1000 | Loss: 0.00003634
Iteration 191/1000 | Loss: 0.00003634
Iteration 192/1000 | Loss: 0.00003633
Iteration 193/1000 | Loss: 0.00003633
Iteration 194/1000 | Loss: 0.00003633
Iteration 195/1000 | Loss: 0.00003632
Iteration 196/1000 | Loss: 0.00003632
Iteration 197/1000 | Loss: 0.00003632
Iteration 198/1000 | Loss: 0.00003631
Iteration 199/1000 | Loss: 0.00003630
Iteration 200/1000 | Loss: 0.00003630
Iteration 201/1000 | Loss: 0.00003630
Iteration 202/1000 | Loss: 0.00003630
Iteration 203/1000 | Loss: 0.00003630
Iteration 204/1000 | Loss: 0.00003630
Iteration 205/1000 | Loss: 0.00003630
Iteration 206/1000 | Loss: 0.00003629
Iteration 207/1000 | Loss: 0.00003629
Iteration 208/1000 | Loss: 0.00003629
Iteration 209/1000 | Loss: 0.00003629
Iteration 210/1000 | Loss: 0.00003628
Iteration 211/1000 | Loss: 0.00003628
Iteration 212/1000 | Loss: 0.00003628
Iteration 213/1000 | Loss: 0.00003627
Iteration 214/1000 | Loss: 0.00003627
Iteration 215/1000 | Loss: 0.00003627
Iteration 216/1000 | Loss: 0.00003626
Iteration 217/1000 | Loss: 0.00003626
Iteration 218/1000 | Loss: 0.00003626
Iteration 219/1000 | Loss: 0.00003625
Iteration 220/1000 | Loss: 0.00027728
Iteration 221/1000 | Loss: 0.00008148
Iteration 222/1000 | Loss: 0.00004149
Iteration 223/1000 | Loss: 0.00025401
Iteration 224/1000 | Loss: 0.00020915
Iteration 225/1000 | Loss: 0.00003908
Iteration 226/1000 | Loss: 0.00003832
Iteration 227/1000 | Loss: 0.00003790
Iteration 228/1000 | Loss: 0.00003749
Iteration 229/1000 | Loss: 0.00003691
Iteration 230/1000 | Loss: 0.00003657
Iteration 231/1000 | Loss: 0.00003629
Iteration 232/1000 | Loss: 0.00003618
Iteration 233/1000 | Loss: 0.00003616
Iteration 234/1000 | Loss: 0.00003615
Iteration 235/1000 | Loss: 0.00003599
Iteration 236/1000 | Loss: 0.00003598
Iteration 237/1000 | Loss: 0.00003596
Iteration 238/1000 | Loss: 0.00003585
Iteration 239/1000 | Loss: 0.00003581
Iteration 240/1000 | Loss: 0.00003581
Iteration 241/1000 | Loss: 0.00003580
Iteration 242/1000 | Loss: 0.00003580
Iteration 243/1000 | Loss: 0.00003580
Iteration 244/1000 | Loss: 0.00003580
Iteration 245/1000 | Loss: 0.00003579
Iteration 246/1000 | Loss: 0.00003576
Iteration 247/1000 | Loss: 0.00003576
Iteration 248/1000 | Loss: 0.00003576
Iteration 249/1000 | Loss: 0.00003576
Iteration 250/1000 | Loss: 0.00003576
Iteration 251/1000 | Loss: 0.00003576
Iteration 252/1000 | Loss: 0.00003576
Iteration 253/1000 | Loss: 0.00003576
Iteration 254/1000 | Loss: 0.00003573
Iteration 255/1000 | Loss: 0.00003573
Iteration 256/1000 | Loss: 0.00003572
Iteration 257/1000 | Loss: 0.00003572
Iteration 258/1000 | Loss: 0.00003572
Iteration 259/1000 | Loss: 0.00003571
Iteration 260/1000 | Loss: 0.00003571
Iteration 261/1000 | Loss: 0.00003571
Iteration 262/1000 | Loss: 0.00003571
Iteration 263/1000 | Loss: 0.00003571
Iteration 264/1000 | Loss: 0.00003570
Iteration 265/1000 | Loss: 0.00003570
Iteration 266/1000 | Loss: 0.00003570
Iteration 267/1000 | Loss: 0.00003570
Iteration 268/1000 | Loss: 0.00003570
Iteration 269/1000 | Loss: 0.00003569
Iteration 270/1000 | Loss: 0.00003568
Iteration 271/1000 | Loss: 0.00003568
Iteration 272/1000 | Loss: 0.00003568
Iteration 273/1000 | Loss: 0.00003568
Iteration 274/1000 | Loss: 0.00003568
Iteration 275/1000 | Loss: 0.00003567
Iteration 276/1000 | Loss: 0.00003567
Iteration 277/1000 | Loss: 0.00003566
Iteration 278/1000 | Loss: 0.00003566
Iteration 279/1000 | Loss: 0.00003566
Iteration 280/1000 | Loss: 0.00003566
Iteration 281/1000 | Loss: 0.00003566
Iteration 282/1000 | Loss: 0.00003565
Iteration 283/1000 | Loss: 0.00003565
Iteration 284/1000 | Loss: 0.00003565
Iteration 285/1000 | Loss: 0.00003565
Iteration 286/1000 | Loss: 0.00003565
Iteration 287/1000 | Loss: 0.00003565
Iteration 288/1000 | Loss: 0.00003565
Iteration 289/1000 | Loss: 0.00003565
Iteration 290/1000 | Loss: 0.00003565
Iteration 291/1000 | Loss: 0.00003565
Iteration 292/1000 | Loss: 0.00003565
Iteration 293/1000 | Loss: 0.00003565
Iteration 294/1000 | Loss: 0.00003565
Iteration 295/1000 | Loss: 0.00003565
Iteration 296/1000 | Loss: 0.00003564
Iteration 297/1000 | Loss: 0.00003564
Iteration 298/1000 | Loss: 0.00003564
Iteration 299/1000 | Loss: 0.00003564
Iteration 300/1000 | Loss: 0.00003564
Iteration 301/1000 | Loss: 0.00003564
Iteration 302/1000 | Loss: 0.00003564
Iteration 303/1000 | Loss: 0.00003564
Iteration 304/1000 | Loss: 0.00003564
Iteration 305/1000 | Loss: 0.00003564
Iteration 306/1000 | Loss: 0.00003564
Iteration 307/1000 | Loss: 0.00003564
Iteration 308/1000 | Loss: 0.00003564
Iteration 309/1000 | Loss: 0.00003564
Iteration 310/1000 | Loss: 0.00003564
Iteration 311/1000 | Loss: 0.00003564
Iteration 312/1000 | Loss: 0.00003564
Iteration 313/1000 | Loss: 0.00003564
Iteration 314/1000 | Loss: 0.00003564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [3.563733116607182e-05, 3.563733116607182e-05, 3.563733116607182e-05, 3.563733116607182e-05, 3.563733116607182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.563733116607182e-05

Optimization complete. Final v2v error: 4.531299114227295 mm

Highest mean error: 6.459848880767822 mm for frame 64

Lowest mean error: 4.14168119430542 mm for frame 162

Saving results

Total time: 169.6618459224701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917673
Iteration 2/25 | Loss: 0.00101183
Iteration 3/25 | Loss: 0.00091849
Iteration 4/25 | Loss: 0.00090176
Iteration 5/25 | Loss: 0.00089563
Iteration 6/25 | Loss: 0.00089455
Iteration 7/25 | Loss: 0.00089447
Iteration 8/25 | Loss: 0.00089447
Iteration 9/25 | Loss: 0.00089447
Iteration 10/25 | Loss: 0.00089447
Iteration 11/25 | Loss: 0.00089447
Iteration 12/25 | Loss: 0.00089447
Iteration 13/25 | Loss: 0.00089447
Iteration 14/25 | Loss: 0.00089447
Iteration 15/25 | Loss: 0.00089447
Iteration 16/25 | Loss: 0.00089447
Iteration 17/25 | Loss: 0.00089447
Iteration 18/25 | Loss: 0.00089447
Iteration 19/25 | Loss: 0.00089447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008944676956161857, 0.0008944676956161857, 0.0008944676956161857, 0.0008944676956161857, 0.0008944676956161857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008944676956161857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64652300
Iteration 2/25 | Loss: 0.00167435
Iteration 3/25 | Loss: 0.00167435
Iteration 4/25 | Loss: 0.00167435
Iteration 5/25 | Loss: 0.00167435
Iteration 6/25 | Loss: 0.00167435
Iteration 7/25 | Loss: 0.00167435
Iteration 8/25 | Loss: 0.00167435
Iteration 9/25 | Loss: 0.00167435
Iteration 10/25 | Loss: 0.00167435
Iteration 11/25 | Loss: 0.00167435
Iteration 12/25 | Loss: 0.00167435
Iteration 13/25 | Loss: 0.00167435
Iteration 14/25 | Loss: 0.00167435
Iteration 15/25 | Loss: 0.00167435
Iteration 16/25 | Loss: 0.00167435
Iteration 17/25 | Loss: 0.00167435
Iteration 18/25 | Loss: 0.00167435
Iteration 19/25 | Loss: 0.00167435
Iteration 20/25 | Loss: 0.00167435
Iteration 21/25 | Loss: 0.00167435
Iteration 22/25 | Loss: 0.00167435
Iteration 23/25 | Loss: 0.00167435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016743497690185905, 0.0016743497690185905, 0.0016743497690185905, 0.0016743497690185905, 0.0016743497690185905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016743497690185905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167435
Iteration 2/1000 | Loss: 0.00003273
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001770
Iteration 8/1000 | Loss: 0.00001736
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001733
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001724
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001719
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00001718
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001712
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.7089900211431086e-05, 1.7089900211431086e-05, 1.7089900211431086e-05, 1.7089900211431086e-05, 1.7089900211431086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7089900211431086e-05

Optimization complete. Final v2v error: 3.524294137954712 mm

Highest mean error: 3.791666030883789 mm for frame 82

Lowest mean error: 3.274012327194214 mm for frame 122

Saving results

Total time: 26.797725915908813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895552
Iteration 2/25 | Loss: 0.00190171
Iteration 3/25 | Loss: 0.00115526
Iteration 4/25 | Loss: 0.00102109
Iteration 5/25 | Loss: 0.00099763
Iteration 6/25 | Loss: 0.00099391
Iteration 7/25 | Loss: 0.00099350
Iteration 8/25 | Loss: 0.00099350
Iteration 9/25 | Loss: 0.00099350
Iteration 10/25 | Loss: 0.00099350
Iteration 11/25 | Loss: 0.00099350
Iteration 12/25 | Loss: 0.00099350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009934983681887388, 0.0009934983681887388, 0.0009934983681887388, 0.0009934983681887388, 0.0009934983681887388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009934983681887388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52859402
Iteration 2/25 | Loss: 0.00148669
Iteration 3/25 | Loss: 0.00148667
Iteration 4/25 | Loss: 0.00148667
Iteration 5/25 | Loss: 0.00148667
Iteration 6/25 | Loss: 0.00148667
Iteration 7/25 | Loss: 0.00148667
Iteration 8/25 | Loss: 0.00148667
Iteration 9/25 | Loss: 0.00148667
Iteration 10/25 | Loss: 0.00148667
Iteration 11/25 | Loss: 0.00148667
Iteration 12/25 | Loss: 0.00148667
Iteration 13/25 | Loss: 0.00148667
Iteration 14/25 | Loss: 0.00148667
Iteration 15/25 | Loss: 0.00148667
Iteration 16/25 | Loss: 0.00148667
Iteration 17/25 | Loss: 0.00148667
Iteration 18/25 | Loss: 0.00148667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014866673154756427, 0.0014866673154756427, 0.0014866673154756427, 0.0014866673154756427, 0.0014866673154756427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014866673154756427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148667
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00002505
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002203
Iteration 6/1000 | Loss: 0.00002139
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002020
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00002006
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002006
Iteration 16/1000 | Loss: 0.00002005
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002002
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001993
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001991
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001991
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001984
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001981
Iteration 79/1000 | Loss: 0.00001981
Iteration 80/1000 | Loss: 0.00001981
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001976
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00001976
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001975
Iteration 94/1000 | Loss: 0.00001975
Iteration 95/1000 | Loss: 0.00001975
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001974
Iteration 99/1000 | Loss: 0.00001974
Iteration 100/1000 | Loss: 0.00001974
Iteration 101/1000 | Loss: 0.00001974
Iteration 102/1000 | Loss: 0.00001974
Iteration 103/1000 | Loss: 0.00001974
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001974
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001973
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001973
Iteration 118/1000 | Loss: 0.00001973
Iteration 119/1000 | Loss: 0.00001973
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001973
Iteration 123/1000 | Loss: 0.00001973
Iteration 124/1000 | Loss: 0.00001973
Iteration 125/1000 | Loss: 0.00001973
Iteration 126/1000 | Loss: 0.00001973
Iteration 127/1000 | Loss: 0.00001973
Iteration 128/1000 | Loss: 0.00001973
Iteration 129/1000 | Loss: 0.00001973
Iteration 130/1000 | Loss: 0.00001973
Iteration 131/1000 | Loss: 0.00001973
Iteration 132/1000 | Loss: 0.00001973
Iteration 133/1000 | Loss: 0.00001973
Iteration 134/1000 | Loss: 0.00001973
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.9729988707695156e-05, 1.9729988707695156e-05, 1.9729988707695156e-05, 1.9729988707695156e-05, 1.9729988707695156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9729988707695156e-05

Optimization complete. Final v2v error: 3.8462700843811035 mm

Highest mean error: 4.058932304382324 mm for frame 122

Lowest mean error: 3.5455284118652344 mm for frame 61

Saving results

Total time: 33.61089825630188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099677
Iteration 2/25 | Loss: 0.00273279
Iteration 3/25 | Loss: 0.00215089
Iteration 4/25 | Loss: 0.00160358
Iteration 5/25 | Loss: 0.00151208
Iteration 6/25 | Loss: 0.00142067
Iteration 7/25 | Loss: 0.00123507
Iteration 8/25 | Loss: 0.00117024
Iteration 9/25 | Loss: 0.00113909
Iteration 10/25 | Loss: 0.00113132
Iteration 11/25 | Loss: 0.00111147
Iteration 12/25 | Loss: 0.00110069
Iteration 13/25 | Loss: 0.00110087
Iteration 14/25 | Loss: 0.00110014
Iteration 15/25 | Loss: 0.00109728
Iteration 16/25 | Loss: 0.00109295
Iteration 17/25 | Loss: 0.00109211
Iteration 18/25 | Loss: 0.00109084
Iteration 19/25 | Loss: 0.00109028
Iteration 20/25 | Loss: 0.00109542
Iteration 21/25 | Loss: 0.00108784
Iteration 22/25 | Loss: 0.00108545
Iteration 23/25 | Loss: 0.00108537
Iteration 24/25 | Loss: 0.00108509
Iteration 25/25 | Loss: 0.00108451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80815601
Iteration 2/25 | Loss: 0.00504985
Iteration 3/25 | Loss: 0.00349144
Iteration 4/25 | Loss: 0.00348885
Iteration 5/25 | Loss: 0.00348885
Iteration 6/25 | Loss: 0.00348885
Iteration 7/25 | Loss: 0.00348885
Iteration 8/25 | Loss: 0.00348885
Iteration 9/25 | Loss: 0.00348885
Iteration 10/25 | Loss: 0.00348885
Iteration 11/25 | Loss: 0.00348885
Iteration 12/25 | Loss: 0.00348884
Iteration 13/25 | Loss: 0.00348884
Iteration 14/25 | Loss: 0.00348884
Iteration 15/25 | Loss: 0.00348884
Iteration 16/25 | Loss: 0.00348884
Iteration 17/25 | Loss: 0.00348884
Iteration 18/25 | Loss: 0.00348884
Iteration 19/25 | Loss: 0.00348884
Iteration 20/25 | Loss: 0.00348884
Iteration 21/25 | Loss: 0.00348884
Iteration 22/25 | Loss: 0.00348884
Iteration 23/25 | Loss: 0.00348884
Iteration 24/25 | Loss: 0.00348884
Iteration 25/25 | Loss: 0.00348884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003488844959065318, 0.003488844959065318, 0.003488844959065318, 0.003488844959065318, 0.003488844959065318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003488844959065318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348884
Iteration 2/1000 | Loss: 0.00202397
Iteration 3/1000 | Loss: 0.00115332
Iteration 4/1000 | Loss: 0.00046924
Iteration 5/1000 | Loss: 0.00028326
Iteration 6/1000 | Loss: 0.00021125
Iteration 7/1000 | Loss: 0.00038512
Iteration 8/1000 | Loss: 0.00023987
Iteration 9/1000 | Loss: 0.00102943
Iteration 10/1000 | Loss: 0.00018610
Iteration 11/1000 | Loss: 0.00011916
Iteration 12/1000 | Loss: 0.00038289
Iteration 13/1000 | Loss: 0.00096624
Iteration 14/1000 | Loss: 0.00042697
Iteration 15/1000 | Loss: 0.00009809
Iteration 16/1000 | Loss: 0.00010717
Iteration 17/1000 | Loss: 0.00008432
Iteration 18/1000 | Loss: 0.00027989
Iteration 19/1000 | Loss: 0.00015389
Iteration 20/1000 | Loss: 0.00040102
Iteration 21/1000 | Loss: 0.00156486
Iteration 22/1000 | Loss: 0.00104523
Iteration 23/1000 | Loss: 0.00029804
Iteration 24/1000 | Loss: 0.00022789
Iteration 25/1000 | Loss: 0.00007499
Iteration 26/1000 | Loss: 0.00006212
Iteration 27/1000 | Loss: 0.00052751
Iteration 28/1000 | Loss: 0.00006788
Iteration 29/1000 | Loss: 0.00005455
Iteration 30/1000 | Loss: 0.00013851
Iteration 31/1000 | Loss: 0.00004882
Iteration 32/1000 | Loss: 0.00004653
Iteration 33/1000 | Loss: 0.00004200
Iteration 34/1000 | Loss: 0.00004101
Iteration 35/1000 | Loss: 0.00004043
Iteration 36/1000 | Loss: 0.00004069
Iteration 37/1000 | Loss: 0.00004036
Iteration 38/1000 | Loss: 0.00003933
Iteration 39/1000 | Loss: 0.00003902
Iteration 40/1000 | Loss: 0.00003902
Iteration 41/1000 | Loss: 0.00003885
Iteration 42/1000 | Loss: 0.00004052
Iteration 43/1000 | Loss: 0.00003967
Iteration 44/1000 | Loss: 0.00003860
Iteration 45/1000 | Loss: 0.00003934
Iteration 46/1000 | Loss: 0.00003897
Iteration 47/1000 | Loss: 0.00004456
Iteration 48/1000 | Loss: 0.00004076
Iteration 49/1000 | Loss: 0.00003944
Iteration 50/1000 | Loss: 0.00004019
Iteration 51/1000 | Loss: 0.00004230
Iteration 52/1000 | Loss: 0.00003970
Iteration 53/1000 | Loss: 0.00004224
Iteration 54/1000 | Loss: 0.00003936
Iteration 55/1000 | Loss: 0.00003863
Iteration 56/1000 | Loss: 0.00004074
Iteration 57/1000 | Loss: 0.00003897
Iteration 58/1000 | Loss: 0.00003846
Iteration 59/1000 | Loss: 0.00004060
Iteration 60/1000 | Loss: 0.00003874
Iteration 61/1000 | Loss: 0.00004069
Iteration 62/1000 | Loss: 0.00003888
Iteration 63/1000 | Loss: 0.00003888
Iteration 64/1000 | Loss: 0.00004118
Iteration 65/1000 | Loss: 0.00003904
Iteration 66/1000 | Loss: 0.00004094
Iteration 67/1000 | Loss: 0.00004058
Iteration 68/1000 | Loss: 0.00003881
Iteration 69/1000 | Loss: 0.00003893
Iteration 70/1000 | Loss: 0.00003878
Iteration 71/1000 | Loss: 0.00003888
Iteration 72/1000 | Loss: 0.00003875
Iteration 73/1000 | Loss: 0.00003880
Iteration 74/1000 | Loss: 0.00003876
Iteration 75/1000 | Loss: 0.00003876
Iteration 76/1000 | Loss: 0.00003873
Iteration 77/1000 | Loss: 0.00003846
Iteration 78/1000 | Loss: 0.00003844
Iteration 79/1000 | Loss: 0.00003843
Iteration 80/1000 | Loss: 0.00003843
Iteration 81/1000 | Loss: 0.00003843
Iteration 82/1000 | Loss: 0.00003843
Iteration 83/1000 | Loss: 0.00003843
Iteration 84/1000 | Loss: 0.00003842
Iteration 85/1000 | Loss: 0.00003842
Iteration 86/1000 | Loss: 0.00003842
Iteration 87/1000 | Loss: 0.00003841
Iteration 88/1000 | Loss: 0.00003841
Iteration 89/1000 | Loss: 0.00003841
Iteration 90/1000 | Loss: 0.00003841
Iteration 91/1000 | Loss: 0.00003841
Iteration 92/1000 | Loss: 0.00003841
Iteration 93/1000 | Loss: 0.00003841
Iteration 94/1000 | Loss: 0.00003841
Iteration 95/1000 | Loss: 0.00003841
Iteration 96/1000 | Loss: 0.00003841
Iteration 97/1000 | Loss: 0.00003841
Iteration 98/1000 | Loss: 0.00003841
Iteration 99/1000 | Loss: 0.00003841
Iteration 100/1000 | Loss: 0.00003841
Iteration 101/1000 | Loss: 0.00003841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [3.8409842090914026e-05, 3.8409842090914026e-05, 3.8409842090914026e-05, 3.8409842090914026e-05, 3.8409842090914026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8409842090914026e-05

Optimization complete. Final v2v error: 3.942138910293579 mm

Highest mean error: 20.056447982788086 mm for frame 146

Lowest mean error: 3.0514461994171143 mm for frame 210

Saving results

Total time: 162.41604161262512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537997
Iteration 2/25 | Loss: 0.00109991
Iteration 3/25 | Loss: 0.00095226
Iteration 4/25 | Loss: 0.00093017
Iteration 5/25 | Loss: 0.00092559
Iteration 6/25 | Loss: 0.00092517
Iteration 7/25 | Loss: 0.00092517
Iteration 8/25 | Loss: 0.00092517
Iteration 9/25 | Loss: 0.00092517
Iteration 10/25 | Loss: 0.00092517
Iteration 11/25 | Loss: 0.00092517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009251724113710225, 0.0009251724113710225, 0.0009251724113710225, 0.0009251724113710225, 0.0009251724113710225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009251724113710225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57191002
Iteration 2/25 | Loss: 0.00155300
Iteration 3/25 | Loss: 0.00155298
Iteration 4/25 | Loss: 0.00155298
Iteration 5/25 | Loss: 0.00155298
Iteration 6/25 | Loss: 0.00155298
Iteration 7/25 | Loss: 0.00155298
Iteration 8/25 | Loss: 0.00155298
Iteration 9/25 | Loss: 0.00155298
Iteration 10/25 | Loss: 0.00155298
Iteration 11/25 | Loss: 0.00155298
Iteration 12/25 | Loss: 0.00155298
Iteration 13/25 | Loss: 0.00155298
Iteration 14/25 | Loss: 0.00155298
Iteration 15/25 | Loss: 0.00155298
Iteration 16/25 | Loss: 0.00155298
Iteration 17/25 | Loss: 0.00155298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001552978646941483, 0.001552978646941483, 0.001552978646941483, 0.001552978646941483, 0.001552978646941483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001552978646941483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155298
Iteration 2/1000 | Loss: 0.00003311
Iteration 3/1000 | Loss: 0.00002622
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001968
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001962
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001950
Iteration 25/1000 | Loss: 0.00001950
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001947
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001944
Iteration 38/1000 | Loss: 0.00001944
Iteration 39/1000 | Loss: 0.00001944
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001943
Iteration 42/1000 | Loss: 0.00001943
Iteration 43/1000 | Loss: 0.00001942
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001942
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001941
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001941
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001940
Iteration 53/1000 | Loss: 0.00001940
Iteration 54/1000 | Loss: 0.00001940
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001939
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001938
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001938
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001936
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.936316220962908e-05, 1.936316220962908e-05, 1.936316220962908e-05, 1.936316220962908e-05, 1.936316220962908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.936316220962908e-05

Optimization complete. Final v2v error: 3.760446548461914 mm

Highest mean error: 4.238335609436035 mm for frame 237

Lowest mean error: 3.484271287918091 mm for frame 223

Saving results

Total time: 33.285677671432495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863739
Iteration 2/25 | Loss: 0.00141888
Iteration 3/25 | Loss: 0.00126596
Iteration 4/25 | Loss: 0.00120928
Iteration 5/25 | Loss: 0.00119883
Iteration 6/25 | Loss: 0.00119947
Iteration 7/25 | Loss: 0.00117365
Iteration 8/25 | Loss: 0.00116175
Iteration 9/25 | Loss: 0.00116031
Iteration 10/25 | Loss: 0.00115996
Iteration 11/25 | Loss: 0.00115979
Iteration 12/25 | Loss: 0.00115976
Iteration 13/25 | Loss: 0.00115976
Iteration 14/25 | Loss: 0.00115976
Iteration 15/25 | Loss: 0.00115976
Iteration 16/25 | Loss: 0.00115976
Iteration 17/25 | Loss: 0.00115976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001159760053269565, 0.001159760053269565, 0.001159760053269565, 0.001159760053269565, 0.001159760053269565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001159760053269565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61995792
Iteration 2/25 | Loss: 0.00287892
Iteration 3/25 | Loss: 0.00287892
Iteration 4/25 | Loss: 0.00287892
Iteration 5/25 | Loss: 0.00287892
Iteration 6/25 | Loss: 0.00287892
Iteration 7/25 | Loss: 0.00287892
Iteration 8/25 | Loss: 0.00287892
Iteration 9/25 | Loss: 0.00287892
Iteration 10/25 | Loss: 0.00287891
Iteration 11/25 | Loss: 0.00287891
Iteration 12/25 | Loss: 0.00287891
Iteration 13/25 | Loss: 0.00287891
Iteration 14/25 | Loss: 0.00287891
Iteration 15/25 | Loss: 0.00287891
Iteration 16/25 | Loss: 0.00287891
Iteration 17/25 | Loss: 0.00287891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002878914587199688, 0.002878914587199688, 0.002878914587199688, 0.002878914587199688, 0.002878914587199688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002878914587199688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287891
Iteration 2/1000 | Loss: 0.00011293
Iteration 3/1000 | Loss: 0.00007718
Iteration 4/1000 | Loss: 0.00006097
Iteration 5/1000 | Loss: 0.00005560
Iteration 6/1000 | Loss: 0.00005228
Iteration 7/1000 | Loss: 0.00005035
Iteration 8/1000 | Loss: 0.00004866
Iteration 9/1000 | Loss: 0.00004744
Iteration 10/1000 | Loss: 0.00004673
Iteration 11/1000 | Loss: 0.00004624
Iteration 12/1000 | Loss: 0.00004580
Iteration 13/1000 | Loss: 0.00004550
Iteration 14/1000 | Loss: 0.00004526
Iteration 15/1000 | Loss: 0.00004508
Iteration 16/1000 | Loss: 0.00004501
Iteration 17/1000 | Loss: 0.00004500
Iteration 18/1000 | Loss: 0.00004499
Iteration 19/1000 | Loss: 0.00004497
Iteration 20/1000 | Loss: 0.00004496
Iteration 21/1000 | Loss: 0.00004495
Iteration 22/1000 | Loss: 0.00004494
Iteration 23/1000 | Loss: 0.00004492
Iteration 24/1000 | Loss: 0.00004489
Iteration 25/1000 | Loss: 0.00004489
Iteration 26/1000 | Loss: 0.00004489
Iteration 27/1000 | Loss: 0.00004489
Iteration 28/1000 | Loss: 0.00004489
Iteration 29/1000 | Loss: 0.00004489
Iteration 30/1000 | Loss: 0.00004489
Iteration 31/1000 | Loss: 0.00004489
Iteration 32/1000 | Loss: 0.00004489
Iteration 33/1000 | Loss: 0.00004488
Iteration 34/1000 | Loss: 0.00004488
Iteration 35/1000 | Loss: 0.00004488
Iteration 36/1000 | Loss: 0.00004487
Iteration 37/1000 | Loss: 0.00004487
Iteration 38/1000 | Loss: 0.00004486
Iteration 39/1000 | Loss: 0.00004486
Iteration 40/1000 | Loss: 0.00004486
Iteration 41/1000 | Loss: 0.00004486
Iteration 42/1000 | Loss: 0.00004485
Iteration 43/1000 | Loss: 0.00004485
Iteration 44/1000 | Loss: 0.00004484
Iteration 45/1000 | Loss: 0.00004484
Iteration 46/1000 | Loss: 0.00004484
Iteration 47/1000 | Loss: 0.00004484
Iteration 48/1000 | Loss: 0.00004483
Iteration 49/1000 | Loss: 0.00004482
Iteration 50/1000 | Loss: 0.00004480
Iteration 51/1000 | Loss: 0.00004479
Iteration 52/1000 | Loss: 0.00004478
Iteration 53/1000 | Loss: 0.00004477
Iteration 54/1000 | Loss: 0.00004477
Iteration 55/1000 | Loss: 0.00004476
Iteration 56/1000 | Loss: 0.00004476
Iteration 57/1000 | Loss: 0.00004476
Iteration 58/1000 | Loss: 0.00004474
Iteration 59/1000 | Loss: 0.00004474
Iteration 60/1000 | Loss: 0.00004474
Iteration 61/1000 | Loss: 0.00004473
Iteration 62/1000 | Loss: 0.00004473
Iteration 63/1000 | Loss: 0.00004473
Iteration 64/1000 | Loss: 0.00004473
Iteration 65/1000 | Loss: 0.00004472
Iteration 66/1000 | Loss: 0.00004472
Iteration 67/1000 | Loss: 0.00004472
Iteration 68/1000 | Loss: 0.00004471
Iteration 69/1000 | Loss: 0.00004471
Iteration 70/1000 | Loss: 0.00004471
Iteration 71/1000 | Loss: 0.00004470
Iteration 72/1000 | Loss: 0.00004470
Iteration 73/1000 | Loss: 0.00004470
Iteration 74/1000 | Loss: 0.00004470
Iteration 75/1000 | Loss: 0.00004470
Iteration 76/1000 | Loss: 0.00004470
Iteration 77/1000 | Loss: 0.00004469
Iteration 78/1000 | Loss: 0.00004469
Iteration 79/1000 | Loss: 0.00004469
Iteration 80/1000 | Loss: 0.00004469
Iteration 81/1000 | Loss: 0.00004469
Iteration 82/1000 | Loss: 0.00004469
Iteration 83/1000 | Loss: 0.00004469
Iteration 84/1000 | Loss: 0.00004469
Iteration 85/1000 | Loss: 0.00004469
Iteration 86/1000 | Loss: 0.00004468
Iteration 87/1000 | Loss: 0.00004468
Iteration 88/1000 | Loss: 0.00004468
Iteration 89/1000 | Loss: 0.00004468
Iteration 90/1000 | Loss: 0.00004467
Iteration 91/1000 | Loss: 0.00004467
Iteration 92/1000 | Loss: 0.00004467
Iteration 93/1000 | Loss: 0.00004467
Iteration 94/1000 | Loss: 0.00004466
Iteration 95/1000 | Loss: 0.00004466
Iteration 96/1000 | Loss: 0.00004466
Iteration 97/1000 | Loss: 0.00004465
Iteration 98/1000 | Loss: 0.00004465
Iteration 99/1000 | Loss: 0.00004465
Iteration 100/1000 | Loss: 0.00004465
Iteration 101/1000 | Loss: 0.00004465
Iteration 102/1000 | Loss: 0.00004465
Iteration 103/1000 | Loss: 0.00004464
Iteration 104/1000 | Loss: 0.00004464
Iteration 105/1000 | Loss: 0.00004464
Iteration 106/1000 | Loss: 0.00004464
Iteration 107/1000 | Loss: 0.00004464
Iteration 108/1000 | Loss: 0.00004464
Iteration 109/1000 | Loss: 0.00004464
Iteration 110/1000 | Loss: 0.00004464
Iteration 111/1000 | Loss: 0.00004464
Iteration 112/1000 | Loss: 0.00004463
Iteration 113/1000 | Loss: 0.00004463
Iteration 114/1000 | Loss: 0.00004462
Iteration 115/1000 | Loss: 0.00004462
Iteration 116/1000 | Loss: 0.00004462
Iteration 117/1000 | Loss: 0.00004462
Iteration 118/1000 | Loss: 0.00004462
Iteration 119/1000 | Loss: 0.00004462
Iteration 120/1000 | Loss: 0.00004462
Iteration 121/1000 | Loss: 0.00004462
Iteration 122/1000 | Loss: 0.00004462
Iteration 123/1000 | Loss: 0.00004462
Iteration 124/1000 | Loss: 0.00004462
Iteration 125/1000 | Loss: 0.00004461
Iteration 126/1000 | Loss: 0.00004461
Iteration 127/1000 | Loss: 0.00004461
Iteration 128/1000 | Loss: 0.00004461
Iteration 129/1000 | Loss: 0.00004461
Iteration 130/1000 | Loss: 0.00004461
Iteration 131/1000 | Loss: 0.00004461
Iteration 132/1000 | Loss: 0.00004461
Iteration 133/1000 | Loss: 0.00004461
Iteration 134/1000 | Loss: 0.00004461
Iteration 135/1000 | Loss: 0.00004461
Iteration 136/1000 | Loss: 0.00004461
Iteration 137/1000 | Loss: 0.00004461
Iteration 138/1000 | Loss: 0.00004460
Iteration 139/1000 | Loss: 0.00004460
Iteration 140/1000 | Loss: 0.00004460
Iteration 141/1000 | Loss: 0.00004460
Iteration 142/1000 | Loss: 0.00004460
Iteration 143/1000 | Loss: 0.00004460
Iteration 144/1000 | Loss: 0.00004460
Iteration 145/1000 | Loss: 0.00004460
Iteration 146/1000 | Loss: 0.00004460
Iteration 147/1000 | Loss: 0.00004459
Iteration 148/1000 | Loss: 0.00004459
Iteration 149/1000 | Loss: 0.00004459
Iteration 150/1000 | Loss: 0.00004459
Iteration 151/1000 | Loss: 0.00004459
Iteration 152/1000 | Loss: 0.00004459
Iteration 153/1000 | Loss: 0.00004458
Iteration 154/1000 | Loss: 0.00004458
Iteration 155/1000 | Loss: 0.00004458
Iteration 156/1000 | Loss: 0.00004458
Iteration 157/1000 | Loss: 0.00004458
Iteration 158/1000 | Loss: 0.00004457
Iteration 159/1000 | Loss: 0.00004457
Iteration 160/1000 | Loss: 0.00004457
Iteration 161/1000 | Loss: 0.00004457
Iteration 162/1000 | Loss: 0.00004457
Iteration 163/1000 | Loss: 0.00004457
Iteration 164/1000 | Loss: 0.00004457
Iteration 165/1000 | Loss: 0.00004456
Iteration 166/1000 | Loss: 0.00004456
Iteration 167/1000 | Loss: 0.00004456
Iteration 168/1000 | Loss: 0.00004456
Iteration 169/1000 | Loss: 0.00004456
Iteration 170/1000 | Loss: 0.00004456
Iteration 171/1000 | Loss: 0.00004455
Iteration 172/1000 | Loss: 0.00004455
Iteration 173/1000 | Loss: 0.00004455
Iteration 174/1000 | Loss: 0.00004455
Iteration 175/1000 | Loss: 0.00004455
Iteration 176/1000 | Loss: 0.00004455
Iteration 177/1000 | Loss: 0.00004455
Iteration 178/1000 | Loss: 0.00004455
Iteration 179/1000 | Loss: 0.00004455
Iteration 180/1000 | Loss: 0.00004455
Iteration 181/1000 | Loss: 0.00004454
Iteration 182/1000 | Loss: 0.00004454
Iteration 183/1000 | Loss: 0.00004454
Iteration 184/1000 | Loss: 0.00004454
Iteration 185/1000 | Loss: 0.00004454
Iteration 186/1000 | Loss: 0.00004454
Iteration 187/1000 | Loss: 0.00004454
Iteration 188/1000 | Loss: 0.00004454
Iteration 189/1000 | Loss: 0.00004454
Iteration 190/1000 | Loss: 0.00004454
Iteration 191/1000 | Loss: 0.00004454
Iteration 192/1000 | Loss: 0.00004454
Iteration 193/1000 | Loss: 0.00004454
Iteration 194/1000 | Loss: 0.00004454
Iteration 195/1000 | Loss: 0.00004454
Iteration 196/1000 | Loss: 0.00004454
Iteration 197/1000 | Loss: 0.00004454
Iteration 198/1000 | Loss: 0.00004454
Iteration 199/1000 | Loss: 0.00004454
Iteration 200/1000 | Loss: 0.00004454
Iteration 201/1000 | Loss: 0.00004454
Iteration 202/1000 | Loss: 0.00004454
Iteration 203/1000 | Loss: 0.00004454
Iteration 204/1000 | Loss: 0.00004454
Iteration 205/1000 | Loss: 0.00004454
Iteration 206/1000 | Loss: 0.00004454
Iteration 207/1000 | Loss: 0.00004454
Iteration 208/1000 | Loss: 0.00004454
Iteration 209/1000 | Loss: 0.00004454
Iteration 210/1000 | Loss: 0.00004454
Iteration 211/1000 | Loss: 0.00004454
Iteration 212/1000 | Loss: 0.00004454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [4.454061127034947e-05, 4.454061127034947e-05, 4.454061127034947e-05, 4.454061127034947e-05, 4.454061127034947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.454061127034947e-05

Optimization complete. Final v2v error: 5.3915839195251465 mm

Highest mean error: 6.092936038970947 mm for frame 6

Lowest mean error: 4.736606597900391 mm for frame 24

Saving results

Total time: 61.82451033592224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071387
Iteration 2/25 | Loss: 0.01071387
Iteration 3/25 | Loss: 0.00483494
Iteration 4/25 | Loss: 0.00298468
Iteration 5/25 | Loss: 0.00249194
Iteration 6/25 | Loss: 0.00243811
Iteration 7/25 | Loss: 0.00218128
Iteration 8/25 | Loss: 0.00208007
Iteration 9/25 | Loss: 0.00211329
Iteration 10/25 | Loss: 0.00197431
Iteration 11/25 | Loss: 0.00187579
Iteration 12/25 | Loss: 0.00180273
Iteration 13/25 | Loss: 0.00180997
Iteration 14/25 | Loss: 0.00173772
Iteration 15/25 | Loss: 0.00172084
Iteration 16/25 | Loss: 0.00170264
Iteration 17/25 | Loss: 0.00169669
Iteration 18/25 | Loss: 0.00167380
Iteration 19/25 | Loss: 0.00166512
Iteration 20/25 | Loss: 0.00165564
Iteration 21/25 | Loss: 0.00163789
Iteration 22/25 | Loss: 0.00161301
Iteration 23/25 | Loss: 0.00159448
Iteration 24/25 | Loss: 0.00158633
Iteration 25/25 | Loss: 0.00157537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70460677
Iteration 2/25 | Loss: 0.01184232
Iteration 3/25 | Loss: 0.00856503
Iteration 4/25 | Loss: 0.00856502
Iteration 5/25 | Loss: 0.00856502
Iteration 6/25 | Loss: 0.00856502
Iteration 7/25 | Loss: 0.00856502
Iteration 8/25 | Loss: 0.00856502
Iteration 9/25 | Loss: 0.00856502
Iteration 10/25 | Loss: 0.00856502
Iteration 11/25 | Loss: 0.00856502
Iteration 12/25 | Loss: 0.00856502
Iteration 13/25 | Loss: 0.00856502
Iteration 14/25 | Loss: 0.00856502
Iteration 15/25 | Loss: 0.00856502
Iteration 16/25 | Loss: 0.00856502
Iteration 17/25 | Loss: 0.00856502
Iteration 18/25 | Loss: 0.00856502
Iteration 19/25 | Loss: 0.00856502
Iteration 20/25 | Loss: 0.00856502
Iteration 21/25 | Loss: 0.00856502
Iteration 22/25 | Loss: 0.00856502
Iteration 23/25 | Loss: 0.00856502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.008565019816160202, 0.008565019816160202, 0.008565019816160202, 0.008565019816160202, 0.008565019816160202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008565019816160202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00856502
Iteration 2/1000 | Loss: 0.00660650
Iteration 3/1000 | Loss: 0.00470127
Iteration 4/1000 | Loss: 0.00221253
Iteration 5/1000 | Loss: 0.00176622
Iteration 6/1000 | Loss: 0.00223523
Iteration 7/1000 | Loss: 0.00541802
Iteration 8/1000 | Loss: 0.00121289
Iteration 9/1000 | Loss: 0.00283904
Iteration 10/1000 | Loss: 0.00153267
Iteration 11/1000 | Loss: 0.00056048
Iteration 12/1000 | Loss: 0.00083871
Iteration 13/1000 | Loss: 0.00141110
Iteration 14/1000 | Loss: 0.00084358
Iteration 15/1000 | Loss: 0.00331595
Iteration 16/1000 | Loss: 0.00533559
Iteration 17/1000 | Loss: 0.00345498
Iteration 18/1000 | Loss: 0.00508417
Iteration 19/1000 | Loss: 0.00208194
Iteration 20/1000 | Loss: 0.00132152
Iteration 21/1000 | Loss: 0.00079489
Iteration 22/1000 | Loss: 0.00033132
Iteration 23/1000 | Loss: 0.00048313
Iteration 24/1000 | Loss: 0.00068230
Iteration 25/1000 | Loss: 0.00067210
Iteration 26/1000 | Loss: 0.00142540
Iteration 27/1000 | Loss: 0.00511318
Iteration 28/1000 | Loss: 0.00038698
Iteration 29/1000 | Loss: 0.00028186
Iteration 30/1000 | Loss: 0.00079493
Iteration 31/1000 | Loss: 0.00043344
Iteration 32/1000 | Loss: 0.00020039
Iteration 33/1000 | Loss: 0.00024294
Iteration 34/1000 | Loss: 0.00088178
Iteration 35/1000 | Loss: 0.00171565
Iteration 36/1000 | Loss: 0.00193595
Iteration 37/1000 | Loss: 0.00216034
Iteration 38/1000 | Loss: 0.00368441
Iteration 39/1000 | Loss: 0.00034330
Iteration 40/1000 | Loss: 0.00122064
Iteration 41/1000 | Loss: 0.00022212
Iteration 42/1000 | Loss: 0.00108752
Iteration 43/1000 | Loss: 0.00150844
Iteration 44/1000 | Loss: 0.00169846
Iteration 45/1000 | Loss: 0.00065598
Iteration 46/1000 | Loss: 0.00059845
Iteration 47/1000 | Loss: 0.00077036
Iteration 48/1000 | Loss: 0.00109320
Iteration 49/1000 | Loss: 0.00019696
Iteration 50/1000 | Loss: 0.00105597
Iteration 51/1000 | Loss: 0.00053287
Iteration 52/1000 | Loss: 0.00035312
Iteration 53/1000 | Loss: 0.00066463
Iteration 54/1000 | Loss: 0.00026796
Iteration 55/1000 | Loss: 0.00012544
Iteration 56/1000 | Loss: 0.00046213
Iteration 57/1000 | Loss: 0.00012608
Iteration 58/1000 | Loss: 0.00011806
Iteration 59/1000 | Loss: 0.00012000
Iteration 60/1000 | Loss: 0.00032855
Iteration 61/1000 | Loss: 0.00126339
Iteration 62/1000 | Loss: 0.00028093
Iteration 63/1000 | Loss: 0.00057194
Iteration 64/1000 | Loss: 0.00010048
Iteration 65/1000 | Loss: 0.00008949
Iteration 66/1000 | Loss: 0.00015978
Iteration 67/1000 | Loss: 0.00010230
Iteration 68/1000 | Loss: 0.00009437
Iteration 69/1000 | Loss: 0.00008888
Iteration 70/1000 | Loss: 0.00027108
Iteration 71/1000 | Loss: 0.00008885
Iteration 72/1000 | Loss: 0.00010132
Iteration 73/1000 | Loss: 0.00016662
Iteration 74/1000 | Loss: 0.00009549
Iteration 75/1000 | Loss: 0.00009949
Iteration 76/1000 | Loss: 0.00010212
Iteration 77/1000 | Loss: 0.00014990
Iteration 78/1000 | Loss: 0.00010620
Iteration 79/1000 | Loss: 0.00008344
Iteration 80/1000 | Loss: 0.00023940
Iteration 81/1000 | Loss: 0.00008918
Iteration 82/1000 | Loss: 0.00007972
Iteration 83/1000 | Loss: 0.00007917
Iteration 84/1000 | Loss: 0.00007891
Iteration 85/1000 | Loss: 0.00007858
Iteration 86/1000 | Loss: 0.00079197
Iteration 87/1000 | Loss: 0.00069655
Iteration 88/1000 | Loss: 0.00178773
Iteration 89/1000 | Loss: 0.00068648
Iteration 90/1000 | Loss: 0.00047531
Iteration 91/1000 | Loss: 0.00067222
Iteration 92/1000 | Loss: 0.00042243
Iteration 93/1000 | Loss: 0.00019796
Iteration 94/1000 | Loss: 0.00011689
Iteration 95/1000 | Loss: 0.00038966
Iteration 96/1000 | Loss: 0.00031227
Iteration 97/1000 | Loss: 0.00029229
Iteration 98/1000 | Loss: 0.00009406
Iteration 99/1000 | Loss: 0.00030889
Iteration 100/1000 | Loss: 0.00029049
Iteration 101/1000 | Loss: 0.00081086
Iteration 102/1000 | Loss: 0.00044751
Iteration 103/1000 | Loss: 0.00011103
Iteration 104/1000 | Loss: 0.00010735
Iteration 105/1000 | Loss: 0.00016582
Iteration 106/1000 | Loss: 0.00008077
Iteration 107/1000 | Loss: 0.00007789
Iteration 108/1000 | Loss: 0.00101639
Iteration 109/1000 | Loss: 0.00035474
Iteration 110/1000 | Loss: 0.00070627
Iteration 111/1000 | Loss: 0.00138322
Iteration 112/1000 | Loss: 0.00034022
Iteration 113/1000 | Loss: 0.00030013
Iteration 114/1000 | Loss: 0.00084344
Iteration 115/1000 | Loss: 0.00008517
Iteration 116/1000 | Loss: 0.00047405
Iteration 117/1000 | Loss: 0.00101020
Iteration 118/1000 | Loss: 0.00053373
Iteration 119/1000 | Loss: 0.00007920
Iteration 120/1000 | Loss: 0.00060096
Iteration 121/1000 | Loss: 0.00167075
Iteration 122/1000 | Loss: 0.00023926
Iteration 123/1000 | Loss: 0.00009532
Iteration 124/1000 | Loss: 0.00007494
Iteration 125/1000 | Loss: 0.00014808
Iteration 126/1000 | Loss: 0.00022557
Iteration 127/1000 | Loss: 0.00006832
Iteration 128/1000 | Loss: 0.00009526
Iteration 129/1000 | Loss: 0.00006611
Iteration 130/1000 | Loss: 0.00006468
Iteration 131/1000 | Loss: 0.00017960
Iteration 132/1000 | Loss: 0.00020639
Iteration 133/1000 | Loss: 0.00035470
Iteration 134/1000 | Loss: 0.00012280
Iteration 135/1000 | Loss: 0.00006425
Iteration 136/1000 | Loss: 0.00006416
Iteration 137/1000 | Loss: 0.00018558
Iteration 138/1000 | Loss: 0.00009254
Iteration 139/1000 | Loss: 0.00007628
Iteration 140/1000 | Loss: 0.00006401
Iteration 141/1000 | Loss: 0.00006378
Iteration 142/1000 | Loss: 0.00006376
Iteration 143/1000 | Loss: 0.00006375
Iteration 144/1000 | Loss: 0.00006375
Iteration 145/1000 | Loss: 0.00006361
Iteration 146/1000 | Loss: 0.00006360
Iteration 147/1000 | Loss: 0.00006360
Iteration 148/1000 | Loss: 0.00006359
Iteration 149/1000 | Loss: 0.00006358
Iteration 150/1000 | Loss: 0.00006358
Iteration 151/1000 | Loss: 0.00006358
Iteration 152/1000 | Loss: 0.00006357
Iteration 153/1000 | Loss: 0.00006357
Iteration 154/1000 | Loss: 0.00006357
Iteration 155/1000 | Loss: 0.00006357
Iteration 156/1000 | Loss: 0.00006357
Iteration 157/1000 | Loss: 0.00006357
Iteration 158/1000 | Loss: 0.00006357
Iteration 159/1000 | Loss: 0.00006356
Iteration 160/1000 | Loss: 0.00006356
Iteration 161/1000 | Loss: 0.00006356
Iteration 162/1000 | Loss: 0.00006356
Iteration 163/1000 | Loss: 0.00006355
Iteration 164/1000 | Loss: 0.00006355
Iteration 165/1000 | Loss: 0.00006354
Iteration 166/1000 | Loss: 0.00006354
Iteration 167/1000 | Loss: 0.00006354
Iteration 168/1000 | Loss: 0.00006354
Iteration 169/1000 | Loss: 0.00006353
Iteration 170/1000 | Loss: 0.00006353
Iteration 171/1000 | Loss: 0.00006353
Iteration 172/1000 | Loss: 0.00006353
Iteration 173/1000 | Loss: 0.00006353
Iteration 174/1000 | Loss: 0.00006353
Iteration 175/1000 | Loss: 0.00006353
Iteration 176/1000 | Loss: 0.00006353
Iteration 177/1000 | Loss: 0.00006353
Iteration 178/1000 | Loss: 0.00006353
Iteration 179/1000 | Loss: 0.00006353
Iteration 180/1000 | Loss: 0.00006353
Iteration 181/1000 | Loss: 0.00006353
Iteration 182/1000 | Loss: 0.00006352
Iteration 183/1000 | Loss: 0.00006352
Iteration 184/1000 | Loss: 0.00006352
Iteration 185/1000 | Loss: 0.00006352
Iteration 186/1000 | Loss: 0.00006351
Iteration 187/1000 | Loss: 0.00006351
Iteration 188/1000 | Loss: 0.00006351
Iteration 189/1000 | Loss: 0.00006351
Iteration 190/1000 | Loss: 0.00006351
Iteration 191/1000 | Loss: 0.00006351
Iteration 192/1000 | Loss: 0.00006351
Iteration 193/1000 | Loss: 0.00006351
Iteration 194/1000 | Loss: 0.00006350
Iteration 195/1000 | Loss: 0.00006350
Iteration 196/1000 | Loss: 0.00006350
Iteration 197/1000 | Loss: 0.00006350
Iteration 198/1000 | Loss: 0.00006350
Iteration 199/1000 | Loss: 0.00006349
Iteration 200/1000 | Loss: 0.00006349
Iteration 201/1000 | Loss: 0.00006349
Iteration 202/1000 | Loss: 0.00006349
Iteration 203/1000 | Loss: 0.00006348
Iteration 204/1000 | Loss: 0.00006348
Iteration 205/1000 | Loss: 0.00006348
Iteration 206/1000 | Loss: 0.00006348
Iteration 207/1000 | Loss: 0.00006348
Iteration 208/1000 | Loss: 0.00006348
Iteration 209/1000 | Loss: 0.00006348
Iteration 210/1000 | Loss: 0.00006348
Iteration 211/1000 | Loss: 0.00006347
Iteration 212/1000 | Loss: 0.00006347
Iteration 213/1000 | Loss: 0.00006347
Iteration 214/1000 | Loss: 0.00006347
Iteration 215/1000 | Loss: 0.00006347
Iteration 216/1000 | Loss: 0.00006347
Iteration 217/1000 | Loss: 0.00006347
Iteration 218/1000 | Loss: 0.00006346
Iteration 219/1000 | Loss: 0.00006346
Iteration 220/1000 | Loss: 0.00006346
Iteration 221/1000 | Loss: 0.00006346
Iteration 222/1000 | Loss: 0.00006346
Iteration 223/1000 | Loss: 0.00006345
Iteration 224/1000 | Loss: 0.00006345
Iteration 225/1000 | Loss: 0.00006345
Iteration 226/1000 | Loss: 0.00006344
Iteration 227/1000 | Loss: 0.00006344
Iteration 228/1000 | Loss: 0.00006344
Iteration 229/1000 | Loss: 0.00006344
Iteration 230/1000 | Loss: 0.00006344
Iteration 231/1000 | Loss: 0.00006344
Iteration 232/1000 | Loss: 0.00006344
Iteration 233/1000 | Loss: 0.00006344
Iteration 234/1000 | Loss: 0.00006344
Iteration 235/1000 | Loss: 0.00006344
Iteration 236/1000 | Loss: 0.00006344
Iteration 237/1000 | Loss: 0.00006343
Iteration 238/1000 | Loss: 0.00006343
Iteration 239/1000 | Loss: 0.00006343
Iteration 240/1000 | Loss: 0.00006343
Iteration 241/1000 | Loss: 0.00006343
Iteration 242/1000 | Loss: 0.00006343
Iteration 243/1000 | Loss: 0.00006343
Iteration 244/1000 | Loss: 0.00006343
Iteration 245/1000 | Loss: 0.00006343
Iteration 246/1000 | Loss: 0.00006343
Iteration 247/1000 | Loss: 0.00006343
Iteration 248/1000 | Loss: 0.00006343
Iteration 249/1000 | Loss: 0.00006343
Iteration 250/1000 | Loss: 0.00006343
Iteration 251/1000 | Loss: 0.00006343
Iteration 252/1000 | Loss: 0.00006343
Iteration 253/1000 | Loss: 0.00006343
Iteration 254/1000 | Loss: 0.00006343
Iteration 255/1000 | Loss: 0.00006343
Iteration 256/1000 | Loss: 0.00006342
Iteration 257/1000 | Loss: 0.00006342
Iteration 258/1000 | Loss: 0.00006342
Iteration 259/1000 | Loss: 0.00006342
Iteration 260/1000 | Loss: 0.00006342
Iteration 261/1000 | Loss: 0.00006342
Iteration 262/1000 | Loss: 0.00006342
Iteration 263/1000 | Loss: 0.00006342
Iteration 264/1000 | Loss: 0.00006342
Iteration 265/1000 | Loss: 0.00006342
Iteration 266/1000 | Loss: 0.00006342
Iteration 267/1000 | Loss: 0.00006342
Iteration 268/1000 | Loss: 0.00006342
Iteration 269/1000 | Loss: 0.00006341
Iteration 270/1000 | Loss: 0.00006341
Iteration 271/1000 | Loss: 0.00006341
Iteration 272/1000 | Loss: 0.00006341
Iteration 273/1000 | Loss: 0.00006341
Iteration 274/1000 | Loss: 0.00006341
Iteration 275/1000 | Loss: 0.00006341
Iteration 276/1000 | Loss: 0.00006341
Iteration 277/1000 | Loss: 0.00006341
Iteration 278/1000 | Loss: 0.00006341
Iteration 279/1000 | Loss: 0.00006341
Iteration 280/1000 | Loss: 0.00006341
Iteration 281/1000 | Loss: 0.00006341
Iteration 282/1000 | Loss: 0.00006341
Iteration 283/1000 | Loss: 0.00006341
Iteration 284/1000 | Loss: 0.00006341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [6.341333937598392e-05, 6.341333937598392e-05, 6.341333937598392e-05, 6.341333937598392e-05, 6.341333937598392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.341333937598392e-05

Optimization complete. Final v2v error: 4.744394779205322 mm

Highest mean error: 13.807689666748047 mm for frame 109

Lowest mean error: 3.5802433490753174 mm for frame 173

Saving results

Total time: 287.70205187797546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112513
Iteration 2/25 | Loss: 0.00331742
Iteration 3/25 | Loss: 0.00200482
Iteration 4/25 | Loss: 0.00172834
Iteration 5/25 | Loss: 0.00169919
Iteration 6/25 | Loss: 0.00147875
Iteration 7/25 | Loss: 0.00130701
Iteration 8/25 | Loss: 0.00117849
Iteration 9/25 | Loss: 0.00106925
Iteration 10/25 | Loss: 0.00102140
Iteration 11/25 | Loss: 0.00101135
Iteration 12/25 | Loss: 0.00099167
Iteration 13/25 | Loss: 0.00097492
Iteration 14/25 | Loss: 0.00096367
Iteration 15/25 | Loss: 0.00095980
Iteration 16/25 | Loss: 0.00095833
Iteration 17/25 | Loss: 0.00095783
Iteration 18/25 | Loss: 0.00095768
Iteration 19/25 | Loss: 0.00095767
Iteration 20/25 | Loss: 0.00095766
Iteration 21/25 | Loss: 0.00095765
Iteration 22/25 | Loss: 0.00095764
Iteration 23/25 | Loss: 0.00095763
Iteration 24/25 | Loss: 0.00095762
Iteration 25/25 | Loss: 0.00095762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56750631
Iteration 2/25 | Loss: 0.00204709
Iteration 3/25 | Loss: 0.00204709
Iteration 4/25 | Loss: 0.00205425
Iteration 5/25 | Loss: 0.00204709
Iteration 6/25 | Loss: 0.00204709
Iteration 7/25 | Loss: 0.00204709
Iteration 8/25 | Loss: 0.00204709
Iteration 9/25 | Loss: 0.00204709
Iteration 10/25 | Loss: 0.00204709
Iteration 11/25 | Loss: 0.00204709
Iteration 12/25 | Loss: 0.00204709
Iteration 13/25 | Loss: 0.00204709
Iteration 14/25 | Loss: 0.00204709
Iteration 15/25 | Loss: 0.00204709
Iteration 16/25 | Loss: 0.00204709
Iteration 17/25 | Loss: 0.00204709
Iteration 18/25 | Loss: 0.00204709
Iteration 19/25 | Loss: 0.00204709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020470882300287485, 0.0020470882300287485, 0.0020470882300287485, 0.0020470882300287485, 0.0020470882300287485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020470882300287485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204709
Iteration 2/1000 | Loss: 0.00005453
Iteration 3/1000 | Loss: 0.00005147
Iteration 4/1000 | Loss: 0.00003795
Iteration 5/1000 | Loss: 0.00004377
Iteration 6/1000 | Loss: 0.00003981
Iteration 7/1000 | Loss: 0.00003543
Iteration 8/1000 | Loss: 0.00147205
Iteration 9/1000 | Loss: 0.00022868
Iteration 10/1000 | Loss: 0.00024973
Iteration 11/1000 | Loss: 0.00005442
Iteration 12/1000 | Loss: 0.00016091
Iteration 13/1000 | Loss: 0.00004356
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00006335
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002043
Iteration 19/1000 | Loss: 0.00002012
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001945
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001922
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001920
Iteration 76/1000 | Loss: 0.00001919
Iteration 77/1000 | Loss: 0.00001919
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001919
Iteration 80/1000 | Loss: 0.00001919
Iteration 81/1000 | Loss: 0.00001919
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001918
Iteration 86/1000 | Loss: 0.00001918
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001918
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001918
Iteration 94/1000 | Loss: 0.00001918
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001918
Iteration 97/1000 | Loss: 0.00001918
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001917
Iteration 100/1000 | Loss: 0.00001917
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001917
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001917
Iteration 110/1000 | Loss: 0.00001917
Iteration 111/1000 | Loss: 0.00001917
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001916
Iteration 119/1000 | Loss: 0.00001916
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.916415749292355e-05, 1.916415749292355e-05, 1.916415749292355e-05, 1.916415749292355e-05, 1.916415749292355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.916415749292355e-05

Optimization complete. Final v2v error: 3.7408275604248047 mm

Highest mean error: 4.782817363739014 mm for frame 45

Lowest mean error: 2.9078617095947266 mm for frame 124

Saving results

Total time: 71.3364942073822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934035
Iteration 2/25 | Loss: 0.00203470
Iteration 3/25 | Loss: 0.00142567
Iteration 4/25 | Loss: 0.00118311
Iteration 5/25 | Loss: 0.00114698
Iteration 6/25 | Loss: 0.00114188
Iteration 7/25 | Loss: 0.00114058
Iteration 8/25 | Loss: 0.00114057
Iteration 9/25 | Loss: 0.00114057
Iteration 10/25 | Loss: 0.00114057
Iteration 11/25 | Loss: 0.00114057
Iteration 12/25 | Loss: 0.00114057
Iteration 13/25 | Loss: 0.00114057
Iteration 14/25 | Loss: 0.00114057
Iteration 15/25 | Loss: 0.00114057
Iteration 16/25 | Loss: 0.00114057
Iteration 17/25 | Loss: 0.00114057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011405678233131766, 0.0011405678233131766, 0.0011405678233131766, 0.0011405678233131766, 0.0011405678233131766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011405678233131766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66508484
Iteration 2/25 | Loss: 0.00232034
Iteration 3/25 | Loss: 0.00232034
Iteration 4/25 | Loss: 0.00232034
Iteration 5/25 | Loss: 0.00232034
Iteration 6/25 | Loss: 0.00232034
Iteration 7/25 | Loss: 0.00232034
Iteration 8/25 | Loss: 0.00232034
Iteration 9/25 | Loss: 0.00232034
Iteration 10/25 | Loss: 0.00232034
Iteration 11/25 | Loss: 0.00232034
Iteration 12/25 | Loss: 0.00232034
Iteration 13/25 | Loss: 0.00232034
Iteration 14/25 | Loss: 0.00232034
Iteration 15/25 | Loss: 0.00232034
Iteration 16/25 | Loss: 0.00232034
Iteration 17/25 | Loss: 0.00232034
Iteration 18/25 | Loss: 0.00232034
Iteration 19/25 | Loss: 0.00232034
Iteration 20/25 | Loss: 0.00232034
Iteration 21/25 | Loss: 0.00232034
Iteration 22/25 | Loss: 0.00232034
Iteration 23/25 | Loss: 0.00232034
Iteration 24/25 | Loss: 0.00232034
Iteration 25/25 | Loss: 0.00232034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232034
Iteration 2/1000 | Loss: 0.00005774
Iteration 3/1000 | Loss: 0.00003754
Iteration 4/1000 | Loss: 0.00003338
Iteration 5/1000 | Loss: 0.00003149
Iteration 6/1000 | Loss: 0.00003015
Iteration 7/1000 | Loss: 0.00002921
Iteration 8/1000 | Loss: 0.00002859
Iteration 9/1000 | Loss: 0.00002826
Iteration 10/1000 | Loss: 0.00002792
Iteration 11/1000 | Loss: 0.00002770
Iteration 12/1000 | Loss: 0.00002749
Iteration 13/1000 | Loss: 0.00002748
Iteration 14/1000 | Loss: 0.00002747
Iteration 15/1000 | Loss: 0.00002741
Iteration 16/1000 | Loss: 0.00002738
Iteration 17/1000 | Loss: 0.00002738
Iteration 18/1000 | Loss: 0.00002738
Iteration 19/1000 | Loss: 0.00002738
Iteration 20/1000 | Loss: 0.00002737
Iteration 21/1000 | Loss: 0.00002737
Iteration 22/1000 | Loss: 0.00002737
Iteration 23/1000 | Loss: 0.00002737
Iteration 24/1000 | Loss: 0.00002733
Iteration 25/1000 | Loss: 0.00002731
Iteration 26/1000 | Loss: 0.00002731
Iteration 27/1000 | Loss: 0.00002731
Iteration 28/1000 | Loss: 0.00002731
Iteration 29/1000 | Loss: 0.00002731
Iteration 30/1000 | Loss: 0.00002731
Iteration 31/1000 | Loss: 0.00002731
Iteration 32/1000 | Loss: 0.00002730
Iteration 33/1000 | Loss: 0.00002730
Iteration 34/1000 | Loss: 0.00002730
Iteration 35/1000 | Loss: 0.00002730
Iteration 36/1000 | Loss: 0.00002730
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002729
Iteration 40/1000 | Loss: 0.00002729
Iteration 41/1000 | Loss: 0.00002728
Iteration 42/1000 | Loss: 0.00002728
Iteration 43/1000 | Loss: 0.00002727
Iteration 44/1000 | Loss: 0.00002727
Iteration 45/1000 | Loss: 0.00002727
Iteration 46/1000 | Loss: 0.00002727
Iteration 47/1000 | Loss: 0.00002726
Iteration 48/1000 | Loss: 0.00002726
Iteration 49/1000 | Loss: 0.00002726
Iteration 50/1000 | Loss: 0.00002725
Iteration 51/1000 | Loss: 0.00002725
Iteration 52/1000 | Loss: 0.00002725
Iteration 53/1000 | Loss: 0.00002724
Iteration 54/1000 | Loss: 0.00002724
Iteration 55/1000 | Loss: 0.00002724
Iteration 56/1000 | Loss: 0.00002724
Iteration 57/1000 | Loss: 0.00002724
Iteration 58/1000 | Loss: 0.00002724
Iteration 59/1000 | Loss: 0.00002724
Iteration 60/1000 | Loss: 0.00002724
Iteration 61/1000 | Loss: 0.00002724
Iteration 62/1000 | Loss: 0.00002724
Iteration 63/1000 | Loss: 0.00002724
Iteration 64/1000 | Loss: 0.00002723
Iteration 65/1000 | Loss: 0.00002723
Iteration 66/1000 | Loss: 0.00002723
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002723
Iteration 69/1000 | Loss: 0.00002723
Iteration 70/1000 | Loss: 0.00002723
Iteration 71/1000 | Loss: 0.00002723
Iteration 72/1000 | Loss: 0.00002723
Iteration 73/1000 | Loss: 0.00002723
Iteration 74/1000 | Loss: 0.00002723
Iteration 75/1000 | Loss: 0.00002723
Iteration 76/1000 | Loss: 0.00002722
Iteration 77/1000 | Loss: 0.00002722
Iteration 78/1000 | Loss: 0.00002722
Iteration 79/1000 | Loss: 0.00002722
Iteration 80/1000 | Loss: 0.00002722
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002721
Iteration 83/1000 | Loss: 0.00002721
Iteration 84/1000 | Loss: 0.00002721
Iteration 85/1000 | Loss: 0.00002721
Iteration 86/1000 | Loss: 0.00002721
Iteration 87/1000 | Loss: 0.00002721
Iteration 88/1000 | Loss: 0.00002721
Iteration 89/1000 | Loss: 0.00002721
Iteration 90/1000 | Loss: 0.00002720
Iteration 91/1000 | Loss: 0.00002720
Iteration 92/1000 | Loss: 0.00002720
Iteration 93/1000 | Loss: 0.00002720
Iteration 94/1000 | Loss: 0.00002719
Iteration 95/1000 | Loss: 0.00002719
Iteration 96/1000 | Loss: 0.00002719
Iteration 97/1000 | Loss: 0.00002719
Iteration 98/1000 | Loss: 0.00002719
Iteration 99/1000 | Loss: 0.00002719
Iteration 100/1000 | Loss: 0.00002719
Iteration 101/1000 | Loss: 0.00002719
Iteration 102/1000 | Loss: 0.00002719
Iteration 103/1000 | Loss: 0.00002719
Iteration 104/1000 | Loss: 0.00002718
Iteration 105/1000 | Loss: 0.00002718
Iteration 106/1000 | Loss: 0.00002718
Iteration 107/1000 | Loss: 0.00002718
Iteration 108/1000 | Loss: 0.00002718
Iteration 109/1000 | Loss: 0.00002718
Iteration 110/1000 | Loss: 0.00002718
Iteration 111/1000 | Loss: 0.00002718
Iteration 112/1000 | Loss: 0.00002718
Iteration 113/1000 | Loss: 0.00002718
Iteration 114/1000 | Loss: 0.00002718
Iteration 115/1000 | Loss: 0.00002718
Iteration 116/1000 | Loss: 0.00002718
Iteration 117/1000 | Loss: 0.00002718
Iteration 118/1000 | Loss: 0.00002718
Iteration 119/1000 | Loss: 0.00002717
Iteration 120/1000 | Loss: 0.00002717
Iteration 121/1000 | Loss: 0.00002717
Iteration 122/1000 | Loss: 0.00002717
Iteration 123/1000 | Loss: 0.00002717
Iteration 124/1000 | Loss: 0.00002717
Iteration 125/1000 | Loss: 0.00002717
Iteration 126/1000 | Loss: 0.00002717
Iteration 127/1000 | Loss: 0.00002717
Iteration 128/1000 | Loss: 0.00002717
Iteration 129/1000 | Loss: 0.00002717
Iteration 130/1000 | Loss: 0.00002717
Iteration 131/1000 | Loss: 0.00002717
Iteration 132/1000 | Loss: 0.00002717
Iteration 133/1000 | Loss: 0.00002717
Iteration 134/1000 | Loss: 0.00002716
Iteration 135/1000 | Loss: 0.00002716
Iteration 136/1000 | Loss: 0.00002716
Iteration 137/1000 | Loss: 0.00002716
Iteration 138/1000 | Loss: 0.00002716
Iteration 139/1000 | Loss: 0.00002716
Iteration 140/1000 | Loss: 0.00002716
Iteration 141/1000 | Loss: 0.00002716
Iteration 142/1000 | Loss: 0.00002716
Iteration 143/1000 | Loss: 0.00002716
Iteration 144/1000 | Loss: 0.00002716
Iteration 145/1000 | Loss: 0.00002716
Iteration 146/1000 | Loss: 0.00002716
Iteration 147/1000 | Loss: 0.00002716
Iteration 148/1000 | Loss: 0.00002716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.715678419917822e-05, 2.715678419917822e-05, 2.715678419917822e-05, 2.715678419917822e-05, 2.715678419917822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.715678419917822e-05

Optimization complete. Final v2v error: 4.349004745483398 mm

Highest mean error: 4.85719108581543 mm for frame 76

Lowest mean error: 3.836376905441284 mm for frame 169

Saving results

Total time: 37.760353565216064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781710
Iteration 2/25 | Loss: 0.00194623
Iteration 3/25 | Loss: 0.00133695
Iteration 4/25 | Loss: 0.00121934
Iteration 5/25 | Loss: 0.00121847
Iteration 6/25 | Loss: 0.00116771
Iteration 7/25 | Loss: 0.00116167
Iteration 8/25 | Loss: 0.00115742
Iteration 9/25 | Loss: 0.00115369
Iteration 10/25 | Loss: 0.00116036
Iteration 11/25 | Loss: 0.00115165
Iteration 12/25 | Loss: 0.00114913
Iteration 13/25 | Loss: 0.00114869
Iteration 14/25 | Loss: 0.00115477
Iteration 15/25 | Loss: 0.00115172
Iteration 16/25 | Loss: 0.00115176
Iteration 17/25 | Loss: 0.00114857
Iteration 18/25 | Loss: 0.00114470
Iteration 19/25 | Loss: 0.00114752
Iteration 20/25 | Loss: 0.00114482
Iteration 21/25 | Loss: 0.00114235
Iteration 22/25 | Loss: 0.00114388
Iteration 23/25 | Loss: 0.00114755
Iteration 24/25 | Loss: 0.00114092
Iteration 25/25 | Loss: 0.00113718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.36341190
Iteration 2/25 | Loss: 0.00371611
Iteration 3/25 | Loss: 0.00371609
Iteration 4/25 | Loss: 0.00371609
Iteration 5/25 | Loss: 0.00371609
Iteration 6/25 | Loss: 0.00371609
Iteration 7/25 | Loss: 0.00371609
Iteration 8/25 | Loss: 0.00371609
Iteration 9/25 | Loss: 0.00371609
Iteration 10/25 | Loss: 0.00371609
Iteration 11/25 | Loss: 0.00371609
Iteration 12/25 | Loss: 0.00371609
Iteration 13/25 | Loss: 0.00371609
Iteration 14/25 | Loss: 0.00371609
Iteration 15/25 | Loss: 0.00371609
Iteration 16/25 | Loss: 0.00371609
Iteration 17/25 | Loss: 0.00371609
Iteration 18/25 | Loss: 0.00371609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0037160858046263456, 0.0037160858046263456, 0.0037160858046263456, 0.0037160858046263456, 0.0037160858046263456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037160858046263456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371609
Iteration 2/1000 | Loss: 0.00033309
Iteration 3/1000 | Loss: 0.00065616
Iteration 4/1000 | Loss: 0.00291135
Iteration 5/1000 | Loss: 0.00061537
Iteration 6/1000 | Loss: 0.00046796
Iteration 7/1000 | Loss: 0.00054216
Iteration 8/1000 | Loss: 0.00054761
Iteration 9/1000 | Loss: 0.00075687
Iteration 10/1000 | Loss: 0.00071082
Iteration 11/1000 | Loss: 0.00080133
Iteration 12/1000 | Loss: 0.00085424
Iteration 13/1000 | Loss: 0.00065064
Iteration 14/1000 | Loss: 0.00108384
Iteration 15/1000 | Loss: 0.00135687
Iteration 16/1000 | Loss: 0.00136146
Iteration 17/1000 | Loss: 0.00076883
Iteration 18/1000 | Loss: 0.00067781
Iteration 19/1000 | Loss: 0.00084718
Iteration 20/1000 | Loss: 0.00163838
Iteration 21/1000 | Loss: 0.00117014
Iteration 22/1000 | Loss: 0.00147409
Iteration 23/1000 | Loss: 0.00135406
Iteration 24/1000 | Loss: 0.00154856
Iteration 25/1000 | Loss: 0.00057418
Iteration 26/1000 | Loss: 0.00063524
Iteration 27/1000 | Loss: 0.00074372
Iteration 28/1000 | Loss: 0.00147801
Iteration 29/1000 | Loss: 0.00174375
Iteration 30/1000 | Loss: 0.00151383
Iteration 31/1000 | Loss: 0.00146754
Iteration 32/1000 | Loss: 0.00249158
Iteration 33/1000 | Loss: 0.00225221
Iteration 34/1000 | Loss: 0.00141109
Iteration 35/1000 | Loss: 0.00134422
Iteration 36/1000 | Loss: 0.00183037
Iteration 37/1000 | Loss: 0.00222311
Iteration 38/1000 | Loss: 0.00174121
Iteration 39/1000 | Loss: 0.00124103
Iteration 40/1000 | Loss: 0.00170371
Iteration 41/1000 | Loss: 0.00123328
Iteration 42/1000 | Loss: 0.00093465
Iteration 43/1000 | Loss: 0.00104767
Iteration 44/1000 | Loss: 0.00116645
Iteration 45/1000 | Loss: 0.00090586
Iteration 46/1000 | Loss: 0.00125690
Iteration 47/1000 | Loss: 0.00074073
Iteration 48/1000 | Loss: 0.00109990
Iteration 49/1000 | Loss: 0.00091037
Iteration 50/1000 | Loss: 0.00059822
Iteration 51/1000 | Loss: 0.00026635
Iteration 52/1000 | Loss: 0.00034080
Iteration 53/1000 | Loss: 0.00036337
Iteration 54/1000 | Loss: 0.00052247
Iteration 55/1000 | Loss: 0.00025771
Iteration 56/1000 | Loss: 0.00041305
Iteration 57/1000 | Loss: 0.00025215
Iteration 58/1000 | Loss: 0.00037032
Iteration 59/1000 | Loss: 0.00022043
Iteration 60/1000 | Loss: 0.00056229
Iteration 61/1000 | Loss: 0.00109191
Iteration 62/1000 | Loss: 0.00052455
Iteration 63/1000 | Loss: 0.00008047
Iteration 64/1000 | Loss: 0.00009001
Iteration 65/1000 | Loss: 0.00017160
Iteration 66/1000 | Loss: 0.00056732
Iteration 67/1000 | Loss: 0.00074540
Iteration 68/1000 | Loss: 0.00178488
Iteration 69/1000 | Loss: 0.00161613
Iteration 70/1000 | Loss: 0.00137545
Iteration 71/1000 | Loss: 0.00214856
Iteration 72/1000 | Loss: 0.00150592
Iteration 73/1000 | Loss: 0.00041553
Iteration 74/1000 | Loss: 0.00089318
Iteration 75/1000 | Loss: 0.00097364
Iteration 76/1000 | Loss: 0.00079421
Iteration 77/1000 | Loss: 0.00056270
Iteration 78/1000 | Loss: 0.00067445
Iteration 79/1000 | Loss: 0.00070564
Iteration 80/1000 | Loss: 0.00031728
Iteration 81/1000 | Loss: 0.00027125
Iteration 82/1000 | Loss: 0.00022242
Iteration 83/1000 | Loss: 0.00056951
Iteration 84/1000 | Loss: 0.00044812
Iteration 85/1000 | Loss: 0.00015867
Iteration 86/1000 | Loss: 0.00071613
Iteration 87/1000 | Loss: 0.00025286
Iteration 88/1000 | Loss: 0.00049169
Iteration 89/1000 | Loss: 0.00021259
Iteration 90/1000 | Loss: 0.00092095
Iteration 91/1000 | Loss: 0.00050413
Iteration 92/1000 | Loss: 0.00103227
Iteration 93/1000 | Loss: 0.00052997
Iteration 94/1000 | Loss: 0.00128035
Iteration 95/1000 | Loss: 0.00064226
Iteration 96/1000 | Loss: 0.00041679
Iteration 97/1000 | Loss: 0.00072297
Iteration 98/1000 | Loss: 0.00034629
Iteration 99/1000 | Loss: 0.00010232
Iteration 100/1000 | Loss: 0.00007881
Iteration 101/1000 | Loss: 0.00093906
Iteration 102/1000 | Loss: 0.00080878
Iteration 103/1000 | Loss: 0.00174839
Iteration 104/1000 | Loss: 0.00018942
Iteration 105/1000 | Loss: 0.00037398
Iteration 106/1000 | Loss: 0.00036397
Iteration 107/1000 | Loss: 0.00041842
Iteration 108/1000 | Loss: 0.00038907
Iteration 109/1000 | Loss: 0.00039372
Iteration 110/1000 | Loss: 0.00035061
Iteration 111/1000 | Loss: 0.00038003
Iteration 112/1000 | Loss: 0.00065149
Iteration 113/1000 | Loss: 0.00046942
Iteration 114/1000 | Loss: 0.00032880
Iteration 115/1000 | Loss: 0.00018246
Iteration 116/1000 | Loss: 0.00032603
Iteration 117/1000 | Loss: 0.00039635
Iteration 118/1000 | Loss: 0.00032241
Iteration 119/1000 | Loss: 0.00034172
Iteration 120/1000 | Loss: 0.00029134
Iteration 121/1000 | Loss: 0.00034703
Iteration 122/1000 | Loss: 0.00029177
Iteration 123/1000 | Loss: 0.00008016
Iteration 124/1000 | Loss: 0.00015035
Iteration 125/1000 | Loss: 0.00007202
Iteration 126/1000 | Loss: 0.00006777
Iteration 127/1000 | Loss: 0.00005689
Iteration 128/1000 | Loss: 0.00040566
Iteration 129/1000 | Loss: 0.00013166
Iteration 130/1000 | Loss: 0.00041981
Iteration 131/1000 | Loss: 0.00013078
Iteration 132/1000 | Loss: 0.00036015
Iteration 133/1000 | Loss: 0.00044648
Iteration 134/1000 | Loss: 0.00042640
Iteration 135/1000 | Loss: 0.00053819
Iteration 136/1000 | Loss: 0.00057666
Iteration 137/1000 | Loss: 0.00010622
Iteration 138/1000 | Loss: 0.00091033
Iteration 139/1000 | Loss: 0.00025073
Iteration 140/1000 | Loss: 0.00007228
Iteration 141/1000 | Loss: 0.00006883
Iteration 142/1000 | Loss: 0.00008269
Iteration 143/1000 | Loss: 0.00007684
Iteration 144/1000 | Loss: 0.00058593
Iteration 145/1000 | Loss: 0.00011263
Iteration 146/1000 | Loss: 0.00006360
Iteration 147/1000 | Loss: 0.00005578
Iteration 148/1000 | Loss: 0.00005190
Iteration 149/1000 | Loss: 0.00004928
Iteration 150/1000 | Loss: 0.00077273
Iteration 151/1000 | Loss: 0.00059611
Iteration 152/1000 | Loss: 0.00057944
Iteration 153/1000 | Loss: 0.00004880
Iteration 154/1000 | Loss: 0.00004321
Iteration 155/1000 | Loss: 0.00004140
Iteration 156/1000 | Loss: 0.00004002
Iteration 157/1000 | Loss: 0.00003874
Iteration 158/1000 | Loss: 0.00003783
Iteration 159/1000 | Loss: 0.00003718
Iteration 160/1000 | Loss: 0.00110587
Iteration 161/1000 | Loss: 0.00024371
Iteration 162/1000 | Loss: 0.00003743
Iteration 163/1000 | Loss: 0.00003647
Iteration 164/1000 | Loss: 0.00003617
Iteration 165/1000 | Loss: 0.00003597
Iteration 166/1000 | Loss: 0.00003570
Iteration 167/1000 | Loss: 0.00003545
Iteration 168/1000 | Loss: 0.00003524
Iteration 169/1000 | Loss: 0.00003521
Iteration 170/1000 | Loss: 0.00003519
Iteration 171/1000 | Loss: 0.00003519
Iteration 172/1000 | Loss: 0.00003518
Iteration 173/1000 | Loss: 0.00003516
Iteration 174/1000 | Loss: 0.00003510
Iteration 175/1000 | Loss: 0.00003505
Iteration 176/1000 | Loss: 0.00005736
Iteration 177/1000 | Loss: 0.00004465
Iteration 178/1000 | Loss: 0.00005397
Iteration 179/1000 | Loss: 0.00004711
Iteration 180/1000 | Loss: 0.00003527
Iteration 181/1000 | Loss: 0.00005245
Iteration 182/1000 | Loss: 0.00004689
Iteration 183/1000 | Loss: 0.00006299
Iteration 184/1000 | Loss: 0.00006450
Iteration 185/1000 | Loss: 0.00003509
Iteration 186/1000 | Loss: 0.00003488
Iteration 187/1000 | Loss: 0.00003487
Iteration 188/1000 | Loss: 0.00003487
Iteration 189/1000 | Loss: 0.00003487
Iteration 190/1000 | Loss: 0.00003486
Iteration 191/1000 | Loss: 0.00003486
Iteration 192/1000 | Loss: 0.00003485
Iteration 193/1000 | Loss: 0.00003485
Iteration 194/1000 | Loss: 0.00003485
Iteration 195/1000 | Loss: 0.00003484
Iteration 196/1000 | Loss: 0.00003484
Iteration 197/1000 | Loss: 0.00003484
Iteration 198/1000 | Loss: 0.00003484
Iteration 199/1000 | Loss: 0.00003484
Iteration 200/1000 | Loss: 0.00003484
Iteration 201/1000 | Loss: 0.00003484
Iteration 202/1000 | Loss: 0.00003484
Iteration 203/1000 | Loss: 0.00003484
Iteration 204/1000 | Loss: 0.00003484
Iteration 205/1000 | Loss: 0.00003484
Iteration 206/1000 | Loss: 0.00003483
Iteration 207/1000 | Loss: 0.00003483
Iteration 208/1000 | Loss: 0.00003483
Iteration 209/1000 | Loss: 0.00003483
Iteration 210/1000 | Loss: 0.00003483
Iteration 211/1000 | Loss: 0.00003483
Iteration 212/1000 | Loss: 0.00003482
Iteration 213/1000 | Loss: 0.00003482
Iteration 214/1000 | Loss: 0.00003482
Iteration 215/1000 | Loss: 0.00003482
Iteration 216/1000 | Loss: 0.00003482
Iteration 217/1000 | Loss: 0.00003482
Iteration 218/1000 | Loss: 0.00003482
Iteration 219/1000 | Loss: 0.00003482
Iteration 220/1000 | Loss: 0.00003482
Iteration 221/1000 | Loss: 0.00003482
Iteration 222/1000 | Loss: 0.00003482
Iteration 223/1000 | Loss: 0.00003481
Iteration 224/1000 | Loss: 0.00003481
Iteration 225/1000 | Loss: 0.00003481
Iteration 226/1000 | Loss: 0.00003481
Iteration 227/1000 | Loss: 0.00003480
Iteration 228/1000 | Loss: 0.00003480
Iteration 229/1000 | Loss: 0.00003480
Iteration 230/1000 | Loss: 0.00003480
Iteration 231/1000 | Loss: 0.00003479
Iteration 232/1000 | Loss: 0.00003479
Iteration 233/1000 | Loss: 0.00003479
Iteration 234/1000 | Loss: 0.00003479
Iteration 235/1000 | Loss: 0.00003479
Iteration 236/1000 | Loss: 0.00003478
Iteration 237/1000 | Loss: 0.00003478
Iteration 238/1000 | Loss: 0.00003478
Iteration 239/1000 | Loss: 0.00003478
Iteration 240/1000 | Loss: 0.00003478
Iteration 241/1000 | Loss: 0.00003477
Iteration 242/1000 | Loss: 0.00003477
Iteration 243/1000 | Loss: 0.00003477
Iteration 244/1000 | Loss: 0.00003477
Iteration 245/1000 | Loss: 0.00003477
Iteration 246/1000 | Loss: 0.00003477
Iteration 247/1000 | Loss: 0.00003477
Iteration 248/1000 | Loss: 0.00003477
Iteration 249/1000 | Loss: 0.00003476
Iteration 250/1000 | Loss: 0.00003476
Iteration 251/1000 | Loss: 0.00003476
Iteration 252/1000 | Loss: 0.00003475
Iteration 253/1000 | Loss: 0.00003475
Iteration 254/1000 | Loss: 0.00003475
Iteration 255/1000 | Loss: 0.00003475
Iteration 256/1000 | Loss: 0.00003475
Iteration 257/1000 | Loss: 0.00003474
Iteration 258/1000 | Loss: 0.00003474
Iteration 259/1000 | Loss: 0.00003474
Iteration 260/1000 | Loss: 0.00003471
Iteration 261/1000 | Loss: 0.00003471
Iteration 262/1000 | Loss: 0.00003471
Iteration 263/1000 | Loss: 0.00003469
Iteration 264/1000 | Loss: 0.00003469
Iteration 265/1000 | Loss: 0.00003469
Iteration 266/1000 | Loss: 0.00003468
Iteration 267/1000 | Loss: 0.00003468
Iteration 268/1000 | Loss: 0.00003468
Iteration 269/1000 | Loss: 0.00003468
Iteration 270/1000 | Loss: 0.00003467
Iteration 271/1000 | Loss: 0.00003467
Iteration 272/1000 | Loss: 0.00003467
Iteration 273/1000 | Loss: 0.00003467
Iteration 274/1000 | Loss: 0.00003467
Iteration 275/1000 | Loss: 0.00003467
Iteration 276/1000 | Loss: 0.00003467
Iteration 277/1000 | Loss: 0.00003467
Iteration 278/1000 | Loss: 0.00003466
Iteration 279/1000 | Loss: 0.00003466
Iteration 280/1000 | Loss: 0.00003466
Iteration 281/1000 | Loss: 0.00003466
Iteration 282/1000 | Loss: 0.00003466
Iteration 283/1000 | Loss: 0.00003466
Iteration 284/1000 | Loss: 0.00003466
Iteration 285/1000 | Loss: 0.00003466
Iteration 286/1000 | Loss: 0.00003466
Iteration 287/1000 | Loss: 0.00003466
Iteration 288/1000 | Loss: 0.00003466
Iteration 289/1000 | Loss: 0.00003466
Iteration 290/1000 | Loss: 0.00003466
Iteration 291/1000 | Loss: 0.00003466
Iteration 292/1000 | Loss: 0.00003466
Iteration 293/1000 | Loss: 0.00003466
Iteration 294/1000 | Loss: 0.00003466
Iteration 295/1000 | Loss: 0.00003466
Iteration 296/1000 | Loss: 0.00003466
Iteration 297/1000 | Loss: 0.00003466
Iteration 298/1000 | Loss: 0.00003466
Iteration 299/1000 | Loss: 0.00003466
Iteration 300/1000 | Loss: 0.00003466
Iteration 301/1000 | Loss: 0.00003466
Iteration 302/1000 | Loss: 0.00003466
Iteration 303/1000 | Loss: 0.00003466
Iteration 304/1000 | Loss: 0.00003466
Iteration 305/1000 | Loss: 0.00003466
Iteration 306/1000 | Loss: 0.00003466
Iteration 307/1000 | Loss: 0.00003466
Iteration 308/1000 | Loss: 0.00003466
Iteration 309/1000 | Loss: 0.00003466
Iteration 310/1000 | Loss: 0.00003466
Iteration 311/1000 | Loss: 0.00003466
Iteration 312/1000 | Loss: 0.00003466
Iteration 313/1000 | Loss: 0.00003466
Iteration 314/1000 | Loss: 0.00003466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [3.46602282661479e-05, 3.46602282661479e-05, 3.46602282661479e-05, 3.46602282661479e-05, 3.46602282661479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.46602282661479e-05

Optimization complete. Final v2v error: 4.733778953552246 mm

Highest mean error: 12.495911598205566 mm for frame 43

Lowest mean error: 3.54990291595459 mm for frame 1

Saving results

Total time: 314.941792011261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814828
Iteration 2/25 | Loss: 0.00147400
Iteration 3/25 | Loss: 0.00109955
Iteration 4/25 | Loss: 0.00103704
Iteration 5/25 | Loss: 0.00102217
Iteration 6/25 | Loss: 0.00101994
Iteration 7/25 | Loss: 0.00101994
Iteration 8/25 | Loss: 0.00101994
Iteration 9/25 | Loss: 0.00101994
Iteration 10/25 | Loss: 0.00101994
Iteration 11/25 | Loss: 0.00101994
Iteration 12/25 | Loss: 0.00101994
Iteration 13/25 | Loss: 0.00101994
Iteration 14/25 | Loss: 0.00101994
Iteration 15/25 | Loss: 0.00101994
Iteration 16/25 | Loss: 0.00101994
Iteration 17/25 | Loss: 0.00101994
Iteration 18/25 | Loss: 0.00101994
Iteration 19/25 | Loss: 0.00101994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010199375683441758, 0.0010199375683441758, 0.0010199375683441758, 0.0010199375683441758, 0.0010199375683441758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010199375683441758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57378387
Iteration 2/25 | Loss: 0.00157908
Iteration 3/25 | Loss: 0.00157907
Iteration 4/25 | Loss: 0.00157907
Iteration 5/25 | Loss: 0.00157907
Iteration 6/25 | Loss: 0.00157907
Iteration 7/25 | Loss: 0.00157907
Iteration 8/25 | Loss: 0.00157907
Iteration 9/25 | Loss: 0.00157907
Iteration 10/25 | Loss: 0.00157907
Iteration 11/25 | Loss: 0.00157907
Iteration 12/25 | Loss: 0.00157907
Iteration 13/25 | Loss: 0.00157907
Iteration 14/25 | Loss: 0.00157907
Iteration 15/25 | Loss: 0.00157907
Iteration 16/25 | Loss: 0.00157907
Iteration 17/25 | Loss: 0.00157907
Iteration 18/25 | Loss: 0.00157907
Iteration 19/25 | Loss: 0.00157907
Iteration 20/25 | Loss: 0.00157907
Iteration 21/25 | Loss: 0.00157907
Iteration 22/25 | Loss: 0.00157907
Iteration 23/25 | Loss: 0.00157907
Iteration 24/25 | Loss: 0.00157907
Iteration 25/25 | Loss: 0.00157907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157907
Iteration 2/1000 | Loss: 0.00005848
Iteration 3/1000 | Loss: 0.00004226
Iteration 4/1000 | Loss: 0.00003692
Iteration 5/1000 | Loss: 0.00003375
Iteration 6/1000 | Loss: 0.00003161
Iteration 7/1000 | Loss: 0.00003031
Iteration 8/1000 | Loss: 0.00002945
Iteration 9/1000 | Loss: 0.00002906
Iteration 10/1000 | Loss: 0.00002875
Iteration 11/1000 | Loss: 0.00002850
Iteration 12/1000 | Loss: 0.00002848
Iteration 13/1000 | Loss: 0.00002842
Iteration 14/1000 | Loss: 0.00002840
Iteration 15/1000 | Loss: 0.00002840
Iteration 16/1000 | Loss: 0.00002839
Iteration 17/1000 | Loss: 0.00002837
Iteration 18/1000 | Loss: 0.00002836
Iteration 19/1000 | Loss: 0.00002836
Iteration 20/1000 | Loss: 0.00002831
Iteration 21/1000 | Loss: 0.00002828
Iteration 22/1000 | Loss: 0.00002826
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002825
Iteration 25/1000 | Loss: 0.00002825
Iteration 26/1000 | Loss: 0.00002825
Iteration 27/1000 | Loss: 0.00002825
Iteration 28/1000 | Loss: 0.00002825
Iteration 29/1000 | Loss: 0.00002825
Iteration 30/1000 | Loss: 0.00002825
Iteration 31/1000 | Loss: 0.00002825
Iteration 32/1000 | Loss: 0.00002825
Iteration 33/1000 | Loss: 0.00002825
Iteration 34/1000 | Loss: 0.00002825
Iteration 35/1000 | Loss: 0.00002825
Iteration 36/1000 | Loss: 0.00002825
Iteration 37/1000 | Loss: 0.00002825
Iteration 38/1000 | Loss: 0.00002825
Iteration 39/1000 | Loss: 0.00002825
Iteration 40/1000 | Loss: 0.00002825
Iteration 41/1000 | Loss: 0.00002825
Iteration 42/1000 | Loss: 0.00002825
Iteration 43/1000 | Loss: 0.00002825
Iteration 44/1000 | Loss: 0.00002825
Iteration 45/1000 | Loss: 0.00002825
Iteration 46/1000 | Loss: 0.00002825
Iteration 47/1000 | Loss: 0.00002825
Iteration 48/1000 | Loss: 0.00002825
Iteration 49/1000 | Loss: 0.00002825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [2.824531838996336e-05, 2.824531838996336e-05, 2.824531838996336e-05, 2.824531838996336e-05, 2.824531838996336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.824531838996336e-05

Optimization complete. Final v2v error: 4.408526420593262 mm

Highest mean error: 4.9084649085998535 mm for frame 229

Lowest mean error: 3.8765859603881836 mm for frame 78

Saving results

Total time: 32.176421880722046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397852
Iteration 2/25 | Loss: 0.00099391
Iteration 3/25 | Loss: 0.00092677
Iteration 4/25 | Loss: 0.00091716
Iteration 5/25 | Loss: 0.00091311
Iteration 6/25 | Loss: 0.00091218
Iteration 7/25 | Loss: 0.00091218
Iteration 8/25 | Loss: 0.00091218
Iteration 9/25 | Loss: 0.00091218
Iteration 10/25 | Loss: 0.00091218
Iteration 11/25 | Loss: 0.00091218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009121777256950736, 0.0009121777256950736, 0.0009121777256950736, 0.0009121777256950736, 0.0009121777256950736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009121777256950736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09936857
Iteration 2/25 | Loss: 0.00181847
Iteration 3/25 | Loss: 0.00181847
Iteration 4/25 | Loss: 0.00181847
Iteration 5/25 | Loss: 0.00181847
Iteration 6/25 | Loss: 0.00181847
Iteration 7/25 | Loss: 0.00181847
Iteration 8/25 | Loss: 0.00181847
Iteration 9/25 | Loss: 0.00181847
Iteration 10/25 | Loss: 0.00181847
Iteration 11/25 | Loss: 0.00181847
Iteration 12/25 | Loss: 0.00181847
Iteration 13/25 | Loss: 0.00181847
Iteration 14/25 | Loss: 0.00181847
Iteration 15/25 | Loss: 0.00181847
Iteration 16/25 | Loss: 0.00181847
Iteration 17/25 | Loss: 0.00181847
Iteration 18/25 | Loss: 0.00181847
Iteration 19/25 | Loss: 0.00181847
Iteration 20/25 | Loss: 0.00181847
Iteration 21/25 | Loss: 0.00181847
Iteration 22/25 | Loss: 0.00181847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018184662330895662, 0.0018184662330895662, 0.0018184662330895662, 0.0018184662330895662, 0.0018184662330895662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018184662330895662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181847
Iteration 2/1000 | Loss: 0.00003345
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002231
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002086
Iteration 8/1000 | Loss: 0.00002054
Iteration 9/1000 | Loss: 0.00002038
Iteration 10/1000 | Loss: 0.00002026
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002023
Iteration 13/1000 | Loss: 0.00002023
Iteration 14/1000 | Loss: 0.00002021
Iteration 15/1000 | Loss: 0.00002020
Iteration 16/1000 | Loss: 0.00002020
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002019
Iteration 19/1000 | Loss: 0.00002017
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002016
Iteration 25/1000 | Loss: 0.00002015
Iteration 26/1000 | Loss: 0.00002015
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002015
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002011
Iteration 34/1000 | Loss: 0.00002011
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002010
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002007
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00002007
Iteration 46/1000 | Loss: 0.00002007
Iteration 47/1000 | Loss: 0.00002007
Iteration 48/1000 | Loss: 0.00002007
Iteration 49/1000 | Loss: 0.00002007
Iteration 50/1000 | Loss: 0.00002007
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002006
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002006
Iteration 56/1000 | Loss: 0.00002006
Iteration 57/1000 | Loss: 0.00002005
Iteration 58/1000 | Loss: 0.00002005
Iteration 59/1000 | Loss: 0.00002005
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002004
Iteration 62/1000 | Loss: 0.00002004
Iteration 63/1000 | Loss: 0.00002004
Iteration 64/1000 | Loss: 0.00002004
Iteration 65/1000 | Loss: 0.00002004
Iteration 66/1000 | Loss: 0.00002003
Iteration 67/1000 | Loss: 0.00002003
Iteration 68/1000 | Loss: 0.00002003
Iteration 69/1000 | Loss: 0.00002003
Iteration 70/1000 | Loss: 0.00002003
Iteration 71/1000 | Loss: 0.00002003
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002002
Iteration 74/1000 | Loss: 0.00002002
Iteration 75/1000 | Loss: 0.00002002
Iteration 76/1000 | Loss: 0.00002002
Iteration 77/1000 | Loss: 0.00002002
Iteration 78/1000 | Loss: 0.00002002
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002001
Iteration 81/1000 | Loss: 0.00002001
Iteration 82/1000 | Loss: 0.00002001
Iteration 83/1000 | Loss: 0.00002001
Iteration 84/1000 | Loss: 0.00002001
Iteration 85/1000 | Loss: 0.00002001
Iteration 86/1000 | Loss: 0.00002001
Iteration 87/1000 | Loss: 0.00002001
Iteration 88/1000 | Loss: 0.00002001
Iteration 89/1000 | Loss: 0.00002001
Iteration 90/1000 | Loss: 0.00002001
Iteration 91/1000 | Loss: 0.00002001
Iteration 92/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.0011641026940197e-05, 2.0011641026940197e-05, 2.0011641026940197e-05, 2.0011641026940197e-05, 2.0011641026940197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0011641026940197e-05

Optimization complete. Final v2v error: 3.7601478099823 mm

Highest mean error: 4.091792583465576 mm for frame 122

Lowest mean error: 3.4407033920288086 mm for frame 152

Saving results

Total time: 30.89247155189514
