Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=251, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14056-14111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384152
Iteration 2/25 | Loss: 0.00087827
Iteration 3/25 | Loss: 0.00076353
Iteration 4/25 | Loss: 0.00075075
Iteration 5/25 | Loss: 0.00074704
Iteration 6/25 | Loss: 0.00074666
Iteration 7/25 | Loss: 0.00074666
Iteration 8/25 | Loss: 0.00074666
Iteration 9/25 | Loss: 0.00074666
Iteration 10/25 | Loss: 0.00074666
Iteration 11/25 | Loss: 0.00074666
Iteration 12/25 | Loss: 0.00074666
Iteration 13/25 | Loss: 0.00074666
Iteration 14/25 | Loss: 0.00074666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007466567913070321, 0.0007466567913070321, 0.0007466567913070321, 0.0007466567913070321, 0.0007466567913070321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007466567913070321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78995430
Iteration 2/25 | Loss: 0.00054778
Iteration 3/25 | Loss: 0.00054774
Iteration 4/25 | Loss: 0.00054774
Iteration 5/25 | Loss: 0.00054774
Iteration 6/25 | Loss: 0.00054774
Iteration 7/25 | Loss: 0.00054773
Iteration 8/25 | Loss: 0.00054773
Iteration 9/25 | Loss: 0.00054773
Iteration 10/25 | Loss: 0.00054773
Iteration 11/25 | Loss: 0.00054773
Iteration 12/25 | Loss: 0.00054773
Iteration 13/25 | Loss: 0.00054773
Iteration 14/25 | Loss: 0.00054773
Iteration 15/25 | Loss: 0.00054773
Iteration 16/25 | Loss: 0.00054773
Iteration 17/25 | Loss: 0.00054773
Iteration 18/25 | Loss: 0.00054773
Iteration 19/25 | Loss: 0.00054773
Iteration 20/25 | Loss: 0.00054773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005477339145727456, 0.0005477339145727456, 0.0005477339145727456, 0.0005477339145727456, 0.0005477339145727456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005477339145727456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054773
Iteration 2/1000 | Loss: 0.00002616
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001260
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.186675945064053e-05, 1.186675945064053e-05, 1.186675945064053e-05, 1.186675945064053e-05, 1.186675945064053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.186675945064053e-05

Optimization complete. Final v2v error: 2.9380452632904053 mm

Highest mean error: 3.2034714221954346 mm for frame 101

Lowest mean error: 2.761380910873413 mm for frame 120

Saving results

Total time: 37.83641576766968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431729
Iteration 2/25 | Loss: 0.00112507
Iteration 3/25 | Loss: 0.00083605
Iteration 4/25 | Loss: 0.00081344
Iteration 5/25 | Loss: 0.00080874
Iteration 6/25 | Loss: 0.00080683
Iteration 7/25 | Loss: 0.00080658
Iteration 8/25 | Loss: 0.00080658
Iteration 9/25 | Loss: 0.00080658
Iteration 10/25 | Loss: 0.00080658
Iteration 11/25 | Loss: 0.00080658
Iteration 12/25 | Loss: 0.00080658
Iteration 13/25 | Loss: 0.00080658
Iteration 14/25 | Loss: 0.00080658
Iteration 15/25 | Loss: 0.00080658
Iteration 16/25 | Loss: 0.00080658
Iteration 17/25 | Loss: 0.00080658
Iteration 18/25 | Loss: 0.00080658
Iteration 19/25 | Loss: 0.00080658
Iteration 20/25 | Loss: 0.00080658
Iteration 21/25 | Loss: 0.00080658
Iteration 22/25 | Loss: 0.00080658
Iteration 23/25 | Loss: 0.00080658
Iteration 24/25 | Loss: 0.00080658
Iteration 25/25 | Loss: 0.00080658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45178938
Iteration 2/25 | Loss: 0.00054051
Iteration 3/25 | Loss: 0.00054047
Iteration 4/25 | Loss: 0.00054047
Iteration 5/25 | Loss: 0.00054047
Iteration 6/25 | Loss: 0.00054047
Iteration 7/25 | Loss: 0.00054047
Iteration 8/25 | Loss: 0.00054047
Iteration 9/25 | Loss: 0.00054047
Iteration 10/25 | Loss: 0.00054047
Iteration 11/25 | Loss: 0.00054047
Iteration 12/25 | Loss: 0.00054047
Iteration 13/25 | Loss: 0.00054047
Iteration 14/25 | Loss: 0.00054047
Iteration 15/25 | Loss: 0.00054047
Iteration 16/25 | Loss: 0.00054047
Iteration 17/25 | Loss: 0.00054047
Iteration 18/25 | Loss: 0.00054047
Iteration 19/25 | Loss: 0.00054047
Iteration 20/25 | Loss: 0.00054047
Iteration 21/25 | Loss: 0.00054047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005404700059443712, 0.0005404700059443712, 0.0005404700059443712, 0.0005404700059443712, 0.0005404700059443712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005404700059443712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054047
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001965
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001859
Iteration 8/1000 | Loss: 0.00001837
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001769
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001765
Iteration 121/1000 | Loss: 0.00001765
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001764
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001763
Iteration 144/1000 | Loss: 0.00001763
Iteration 145/1000 | Loss: 0.00001763
Iteration 146/1000 | Loss: 0.00001763
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001762
Iteration 150/1000 | Loss: 0.00001762
Iteration 151/1000 | Loss: 0.00001762
Iteration 152/1000 | Loss: 0.00001762
Iteration 153/1000 | Loss: 0.00001762
Iteration 154/1000 | Loss: 0.00001762
Iteration 155/1000 | Loss: 0.00001762
Iteration 156/1000 | Loss: 0.00001762
Iteration 157/1000 | Loss: 0.00001762
Iteration 158/1000 | Loss: 0.00001762
Iteration 159/1000 | Loss: 0.00001762
Iteration 160/1000 | Loss: 0.00001762
Iteration 161/1000 | Loss: 0.00001762
Iteration 162/1000 | Loss: 0.00001762
Iteration 163/1000 | Loss: 0.00001762
Iteration 164/1000 | Loss: 0.00001762
Iteration 165/1000 | Loss: 0.00001762
Iteration 166/1000 | Loss: 0.00001762
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001762
Iteration 169/1000 | Loss: 0.00001762
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001762
Iteration 172/1000 | Loss: 0.00001762
Iteration 173/1000 | Loss: 0.00001762
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001762
Iteration 182/1000 | Loss: 0.00001762
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.761521525622811e-05, 1.761521525622811e-05, 1.761521525622811e-05, 1.761521525622811e-05, 1.761521525622811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.761521525622811e-05

Optimization complete. Final v2v error: 3.59027361869812 mm

Highest mean error: 3.8529467582702637 mm for frame 101

Lowest mean error: 3.3534724712371826 mm for frame 130

Saving results

Total time: 37.83109521865845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568717
Iteration 2/25 | Loss: 0.00096391
Iteration 3/25 | Loss: 0.00081676
Iteration 4/25 | Loss: 0.00079288
Iteration 5/25 | Loss: 0.00078436
Iteration 6/25 | Loss: 0.00078240
Iteration 7/25 | Loss: 0.00078225
Iteration 8/25 | Loss: 0.00078225
Iteration 9/25 | Loss: 0.00078225
Iteration 10/25 | Loss: 0.00078225
Iteration 11/25 | Loss: 0.00078225
Iteration 12/25 | Loss: 0.00078225
Iteration 13/25 | Loss: 0.00078225
Iteration 14/25 | Loss: 0.00078225
Iteration 15/25 | Loss: 0.00078225
Iteration 16/25 | Loss: 0.00078225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000782251765485853, 0.000782251765485853, 0.000782251765485853, 0.000782251765485853, 0.000782251765485853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000782251765485853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.91560555
Iteration 2/25 | Loss: 0.00038548
Iteration 3/25 | Loss: 0.00038539
Iteration 4/25 | Loss: 0.00038539
Iteration 5/25 | Loss: 0.00038539
Iteration 6/25 | Loss: 0.00038539
Iteration 7/25 | Loss: 0.00038539
Iteration 8/25 | Loss: 0.00038539
Iteration 9/25 | Loss: 0.00038539
Iteration 10/25 | Loss: 0.00038539
Iteration 11/25 | Loss: 0.00038539
Iteration 12/25 | Loss: 0.00038539
Iteration 13/25 | Loss: 0.00038539
Iteration 14/25 | Loss: 0.00038539
Iteration 15/25 | Loss: 0.00038539
Iteration 16/25 | Loss: 0.00038539
Iteration 17/25 | Loss: 0.00038539
Iteration 18/25 | Loss: 0.00038539
Iteration 19/25 | Loss: 0.00038539
Iteration 20/25 | Loss: 0.00038539
Iteration 21/25 | Loss: 0.00038539
Iteration 22/25 | Loss: 0.00038539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003853921080008149, 0.0003853921080008149, 0.0003853921080008149, 0.0003853921080008149, 0.0003853921080008149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003853921080008149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038539
Iteration 2/1000 | Loss: 0.00002868
Iteration 3/1000 | Loss: 0.00001898
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001640
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001572
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00001537
Iteration 15/1000 | Loss: 0.00001536
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001529
Iteration 19/1000 | Loss: 0.00001529
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001522
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001516
Iteration 59/1000 | Loss: 0.00001516
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001515
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001515
Iteration 68/1000 | Loss: 0.00001514
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.5137545233301353e-05, 1.5137545233301353e-05, 1.5137545233301353e-05, 1.5137545233301353e-05, 1.5137545233301353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5137545233301353e-05

Optimization complete. Final v2v error: 3.351914167404175 mm

Highest mean error: 3.608264446258545 mm for frame 233

Lowest mean error: 3.18239688873291 mm for frame 124

Saving results

Total time: 33.06606316566467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804065
Iteration 2/25 | Loss: 0.00137524
Iteration 3/25 | Loss: 0.00095642
Iteration 4/25 | Loss: 0.00086886
Iteration 5/25 | Loss: 0.00084631
Iteration 6/25 | Loss: 0.00084123
Iteration 7/25 | Loss: 0.00083903
Iteration 8/25 | Loss: 0.00083865
Iteration 9/25 | Loss: 0.00083858
Iteration 10/25 | Loss: 0.00083858
Iteration 11/25 | Loss: 0.00083858
Iteration 12/25 | Loss: 0.00083858
Iteration 13/25 | Loss: 0.00083858
Iteration 14/25 | Loss: 0.00083858
Iteration 15/25 | Loss: 0.00083858
Iteration 16/25 | Loss: 0.00083858
Iteration 17/25 | Loss: 0.00083857
Iteration 18/25 | Loss: 0.00083857
Iteration 19/25 | Loss: 0.00083857
Iteration 20/25 | Loss: 0.00083857
Iteration 21/25 | Loss: 0.00083857
Iteration 22/25 | Loss: 0.00083857
Iteration 23/25 | Loss: 0.00083857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008385734399780631, 0.0008385734399780631, 0.0008385734399780631, 0.0008385734399780631, 0.0008385734399780631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008385734399780631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42797637
Iteration 2/25 | Loss: 0.00057522
Iteration 3/25 | Loss: 0.00057522
Iteration 4/25 | Loss: 0.00057522
Iteration 5/25 | Loss: 0.00057522
Iteration 6/25 | Loss: 0.00057522
Iteration 7/25 | Loss: 0.00057522
Iteration 8/25 | Loss: 0.00057522
Iteration 9/25 | Loss: 0.00057522
Iteration 10/25 | Loss: 0.00057522
Iteration 11/25 | Loss: 0.00057522
Iteration 12/25 | Loss: 0.00057522
Iteration 13/25 | Loss: 0.00057522
Iteration 14/25 | Loss: 0.00057522
Iteration 15/25 | Loss: 0.00057522
Iteration 16/25 | Loss: 0.00057522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005752192228101194, 0.0005752192228101194, 0.0005752192228101194, 0.0005752192228101194, 0.0005752192228101194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005752192228101194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057522
Iteration 2/1000 | Loss: 0.00004720
Iteration 3/1000 | Loss: 0.00003404
Iteration 4/1000 | Loss: 0.00002983
Iteration 5/1000 | Loss: 0.00002816
Iteration 6/1000 | Loss: 0.00002696
Iteration 7/1000 | Loss: 0.00002613
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002475
Iteration 10/1000 | Loss: 0.00002439
Iteration 11/1000 | Loss: 0.00002411
Iteration 12/1000 | Loss: 0.00002388
Iteration 13/1000 | Loss: 0.00002371
Iteration 14/1000 | Loss: 0.00002359
Iteration 15/1000 | Loss: 0.00002343
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002332
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002330
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002325
Iteration 23/1000 | Loss: 0.00002324
Iteration 24/1000 | Loss: 0.00024302
Iteration 25/1000 | Loss: 0.00002994
Iteration 26/1000 | Loss: 0.00002775
Iteration 27/1000 | Loss: 0.00002511
Iteration 28/1000 | Loss: 0.00002381
Iteration 29/1000 | Loss: 0.00002322
Iteration 30/1000 | Loss: 0.00002291
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002266
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002254
Iteration 37/1000 | Loss: 0.00002254
Iteration 38/1000 | Loss: 0.00002253
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002251
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002250
Iteration 45/1000 | Loss: 0.00002250
Iteration 46/1000 | Loss: 0.00002250
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002248
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002248
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00002248
Iteration 62/1000 | Loss: 0.00002247
Iteration 63/1000 | Loss: 0.00002247
Iteration 64/1000 | Loss: 0.00002247
Iteration 65/1000 | Loss: 0.00002247
Iteration 66/1000 | Loss: 0.00002247
Iteration 67/1000 | Loss: 0.00002247
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002247
Iteration 71/1000 | Loss: 0.00002247
Iteration 72/1000 | Loss: 0.00002247
Iteration 73/1000 | Loss: 0.00002247
Iteration 74/1000 | Loss: 0.00002247
Iteration 75/1000 | Loss: 0.00002247
Iteration 76/1000 | Loss: 0.00002247
Iteration 77/1000 | Loss: 0.00002247
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002246
Iteration 80/1000 | Loss: 0.00002246
Iteration 81/1000 | Loss: 0.00002246
Iteration 82/1000 | Loss: 0.00002246
Iteration 83/1000 | Loss: 0.00002246
Iteration 84/1000 | Loss: 0.00002246
Iteration 85/1000 | Loss: 0.00002246
Iteration 86/1000 | Loss: 0.00002246
Iteration 87/1000 | Loss: 0.00002246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.246353506052401e-05, 2.246353506052401e-05, 2.246353506052401e-05, 2.246353506052401e-05, 2.246353506052401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.246353506052401e-05

Optimization complete. Final v2v error: 3.9876489639282227 mm

Highest mean error: 5.234535217285156 mm for frame 65

Lowest mean error: 3.2512686252593994 mm for frame 189

Saving results

Total time: 60.026694774627686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083210
Iteration 2/25 | Loss: 0.01083210
Iteration 3/25 | Loss: 0.01083210
Iteration 4/25 | Loss: 0.00343518
Iteration 5/25 | Loss: 0.00195983
Iteration 6/25 | Loss: 0.00170161
Iteration 7/25 | Loss: 0.00157465
Iteration 8/25 | Loss: 0.00155800
Iteration 9/25 | Loss: 0.00156539
Iteration 10/25 | Loss: 0.00150005
Iteration 11/25 | Loss: 0.00142274
Iteration 12/25 | Loss: 0.00138047
Iteration 13/25 | Loss: 0.00126238
Iteration 14/25 | Loss: 0.00128298
Iteration 15/25 | Loss: 0.00125907
Iteration 16/25 | Loss: 0.00125523
Iteration 17/25 | Loss: 0.00123171
Iteration 18/25 | Loss: 0.00122834
Iteration 19/25 | Loss: 0.00122961
Iteration 20/25 | Loss: 0.00121340
Iteration 21/25 | Loss: 0.00120832
Iteration 22/25 | Loss: 0.00120893
Iteration 23/25 | Loss: 0.00120010
Iteration 24/25 | Loss: 0.00120271
Iteration 25/25 | Loss: 0.00118604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44350874
Iteration 2/25 | Loss: 0.00770609
Iteration 3/25 | Loss: 0.00419910
Iteration 4/25 | Loss: 0.00419910
Iteration 5/25 | Loss: 0.00419910
Iteration 6/25 | Loss: 0.00419910
Iteration 7/25 | Loss: 0.00419910
Iteration 8/25 | Loss: 0.00419910
Iteration 9/25 | Loss: 0.00419910
Iteration 10/25 | Loss: 0.00419910
Iteration 11/25 | Loss: 0.00419910
Iteration 12/25 | Loss: 0.00419910
Iteration 13/25 | Loss: 0.00419910
Iteration 14/25 | Loss: 0.00419910
Iteration 15/25 | Loss: 0.00419910
Iteration 16/25 | Loss: 0.00419910
Iteration 17/25 | Loss: 0.00419910
Iteration 18/25 | Loss: 0.00419910
Iteration 19/25 | Loss: 0.00419910
Iteration 20/25 | Loss: 0.00419910
Iteration 21/25 | Loss: 0.00419910
Iteration 22/25 | Loss: 0.00419910
Iteration 23/25 | Loss: 0.00419910
Iteration 24/25 | Loss: 0.00419910
Iteration 25/25 | Loss: 0.00419910

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419910
Iteration 2/1000 | Loss: 0.00316150
Iteration 3/1000 | Loss: 0.00373224
Iteration 4/1000 | Loss: 0.00266916
Iteration 5/1000 | Loss: 0.00170903
Iteration 6/1000 | Loss: 0.00298449
Iteration 7/1000 | Loss: 0.00889354
Iteration 8/1000 | Loss: 0.00161413
Iteration 9/1000 | Loss: 0.00524605
Iteration 10/1000 | Loss: 0.00298943
Iteration 11/1000 | Loss: 0.00094177
Iteration 12/1000 | Loss: 0.00106116
Iteration 13/1000 | Loss: 0.00355629
Iteration 14/1000 | Loss: 0.00133874
Iteration 15/1000 | Loss: 0.00501465
Iteration 16/1000 | Loss: 0.00124658
Iteration 17/1000 | Loss: 0.00304651
Iteration 18/1000 | Loss: 0.00165023
Iteration 19/1000 | Loss: 0.00325086
Iteration 20/1000 | Loss: 0.00254932
Iteration 21/1000 | Loss: 0.00658606
Iteration 22/1000 | Loss: 0.00380129
Iteration 23/1000 | Loss: 0.00386580
Iteration 24/1000 | Loss: 0.00468682
Iteration 25/1000 | Loss: 0.00527305
Iteration 26/1000 | Loss: 0.00466242
Iteration 27/1000 | Loss: 0.00626277
Iteration 28/1000 | Loss: 0.00274389
Iteration 29/1000 | Loss: 0.00319202
Iteration 30/1000 | Loss: 0.00253787
Iteration 31/1000 | Loss: 0.00367616
Iteration 32/1000 | Loss: 0.00216188
Iteration 33/1000 | Loss: 0.00160725
Iteration 34/1000 | Loss: 0.00213249
Iteration 35/1000 | Loss: 0.00277545
Iteration 36/1000 | Loss: 0.00146377
Iteration 37/1000 | Loss: 0.00143680
Iteration 38/1000 | Loss: 0.00251252
Iteration 39/1000 | Loss: 0.00154647
Iteration 40/1000 | Loss: 0.00313541
Iteration 41/1000 | Loss: 0.00623656
Iteration 42/1000 | Loss: 0.00217052
Iteration 43/1000 | Loss: 0.00190383
Iteration 44/1000 | Loss: 0.00246218
Iteration 45/1000 | Loss: 0.00260573
Iteration 46/1000 | Loss: 0.00095953
Iteration 47/1000 | Loss: 0.00170467
Iteration 48/1000 | Loss: 0.00187180
Iteration 49/1000 | Loss: 0.00206710
Iteration 50/1000 | Loss: 0.00204664
Iteration 51/1000 | Loss: 0.00075391
Iteration 52/1000 | Loss: 0.00089583
Iteration 53/1000 | Loss: 0.00087803
Iteration 54/1000 | Loss: 0.00170119
Iteration 55/1000 | Loss: 0.00164843
Iteration 56/1000 | Loss: 0.00078535
Iteration 57/1000 | Loss: 0.00119065
Iteration 58/1000 | Loss: 0.00141386
Iteration 59/1000 | Loss: 0.00152034
Iteration 60/1000 | Loss: 0.00158812
Iteration 61/1000 | Loss: 0.00100419
Iteration 62/1000 | Loss: 0.00080964
Iteration 63/1000 | Loss: 0.00147293
Iteration 64/1000 | Loss: 0.00078493
Iteration 65/1000 | Loss: 0.00104298
Iteration 66/1000 | Loss: 0.00092064
Iteration 67/1000 | Loss: 0.00133935
Iteration 68/1000 | Loss: 0.00157894
Iteration 69/1000 | Loss: 0.00103017
Iteration 70/1000 | Loss: 0.00132164
Iteration 71/1000 | Loss: 0.00099524
Iteration 72/1000 | Loss: 0.00085367
Iteration 73/1000 | Loss: 0.00104073
Iteration 74/1000 | Loss: 0.00116785
Iteration 75/1000 | Loss: 0.00120878
Iteration 76/1000 | Loss: 0.00076430
Iteration 77/1000 | Loss: 0.00067533
Iteration 78/1000 | Loss: 0.00056910
Iteration 79/1000 | Loss: 0.00059357
Iteration 80/1000 | Loss: 0.00090382
Iteration 81/1000 | Loss: 0.00116711
Iteration 82/1000 | Loss: 0.00206225
Iteration 83/1000 | Loss: 0.00070451
Iteration 84/1000 | Loss: 0.00109281
Iteration 85/1000 | Loss: 0.00014026
Iteration 86/1000 | Loss: 0.00046872
Iteration 87/1000 | Loss: 0.00026460
Iteration 88/1000 | Loss: 0.00021510
Iteration 89/1000 | Loss: 0.00039424
Iteration 90/1000 | Loss: 0.00049862
Iteration 91/1000 | Loss: 0.00050165
Iteration 92/1000 | Loss: 0.00041892
Iteration 93/1000 | Loss: 0.00242317
Iteration 94/1000 | Loss: 0.00099764
Iteration 95/1000 | Loss: 0.00154771
Iteration 96/1000 | Loss: 0.00071880
Iteration 97/1000 | Loss: 0.00055947
Iteration 98/1000 | Loss: 0.00050084
Iteration 99/1000 | Loss: 0.00057461
Iteration 100/1000 | Loss: 0.00072065
Iteration 101/1000 | Loss: 0.00071401
Iteration 102/1000 | Loss: 0.00060801
Iteration 103/1000 | Loss: 0.00087707
Iteration 104/1000 | Loss: 0.00067778
Iteration 105/1000 | Loss: 0.00008585
Iteration 106/1000 | Loss: 0.00102493
Iteration 107/1000 | Loss: 0.00057188
Iteration 108/1000 | Loss: 0.00157628
Iteration 109/1000 | Loss: 0.00014568
Iteration 110/1000 | Loss: 0.00008972
Iteration 111/1000 | Loss: 0.00023083
Iteration 112/1000 | Loss: 0.00022326
Iteration 113/1000 | Loss: 0.00019099
Iteration 114/1000 | Loss: 0.00012531
Iteration 115/1000 | Loss: 0.00015726
Iteration 116/1000 | Loss: 0.00009015
Iteration 117/1000 | Loss: 0.00084707
Iteration 118/1000 | Loss: 0.00032163
Iteration 119/1000 | Loss: 0.00079285
Iteration 120/1000 | Loss: 0.00031719
Iteration 121/1000 | Loss: 0.00037881
Iteration 122/1000 | Loss: 0.00031702
Iteration 123/1000 | Loss: 0.00045838
Iteration 124/1000 | Loss: 0.00030428
Iteration 125/1000 | Loss: 0.00029663
Iteration 126/1000 | Loss: 0.00043850
Iteration 127/1000 | Loss: 0.00066644
Iteration 128/1000 | Loss: 0.00076012
Iteration 129/1000 | Loss: 0.00228097
Iteration 130/1000 | Loss: 0.00116204
Iteration 131/1000 | Loss: 0.00133939
Iteration 132/1000 | Loss: 0.00107124
Iteration 133/1000 | Loss: 0.00059661
Iteration 134/1000 | Loss: 0.00065756
Iteration 135/1000 | Loss: 0.00051101
Iteration 136/1000 | Loss: 0.00025733
Iteration 137/1000 | Loss: 0.00043072
Iteration 138/1000 | Loss: 0.00049202
Iteration 139/1000 | Loss: 0.00024244
Iteration 140/1000 | Loss: 0.00032260
Iteration 141/1000 | Loss: 0.00055240
Iteration 142/1000 | Loss: 0.00031696
Iteration 143/1000 | Loss: 0.00023550
Iteration 144/1000 | Loss: 0.00034183
Iteration 145/1000 | Loss: 0.00040018
Iteration 146/1000 | Loss: 0.00024462
Iteration 147/1000 | Loss: 0.00065174
Iteration 148/1000 | Loss: 0.00080200
Iteration 149/1000 | Loss: 0.00011378
Iteration 150/1000 | Loss: 0.00016420
Iteration 151/1000 | Loss: 0.00009510
Iteration 152/1000 | Loss: 0.00017292
Iteration 153/1000 | Loss: 0.00123055
Iteration 154/1000 | Loss: 0.00040048
Iteration 155/1000 | Loss: 0.00013237
Iteration 156/1000 | Loss: 0.00021209
Iteration 157/1000 | Loss: 0.00015990
Iteration 158/1000 | Loss: 0.00015762
Iteration 159/1000 | Loss: 0.00014345
Iteration 160/1000 | Loss: 0.00083712
Iteration 161/1000 | Loss: 0.00018221
Iteration 162/1000 | Loss: 0.00013122
Iteration 163/1000 | Loss: 0.00018167
Iteration 164/1000 | Loss: 0.00024639
Iteration 165/1000 | Loss: 0.00014309
Iteration 166/1000 | Loss: 0.00021120
Iteration 167/1000 | Loss: 0.00016659
Iteration 168/1000 | Loss: 0.00019471
Iteration 169/1000 | Loss: 0.00016477
Iteration 170/1000 | Loss: 0.00015704
Iteration 171/1000 | Loss: 0.00014866
Iteration 172/1000 | Loss: 0.00060432
Iteration 173/1000 | Loss: 0.00027675
Iteration 174/1000 | Loss: 0.00009755
Iteration 175/1000 | Loss: 0.00024600
Iteration 176/1000 | Loss: 0.00015935
Iteration 177/1000 | Loss: 0.00024667
Iteration 178/1000 | Loss: 0.00024784
Iteration 179/1000 | Loss: 0.00035075
Iteration 180/1000 | Loss: 0.00016773
Iteration 181/1000 | Loss: 0.00012852
Iteration 182/1000 | Loss: 0.00011358
Iteration 183/1000 | Loss: 0.00013007
Iteration 184/1000 | Loss: 0.00025901
Iteration 185/1000 | Loss: 0.00012170
Iteration 186/1000 | Loss: 0.00018869
Iteration 187/1000 | Loss: 0.00028299
Iteration 188/1000 | Loss: 0.00037221
Iteration 189/1000 | Loss: 0.00017716
Iteration 190/1000 | Loss: 0.00008530
Iteration 191/1000 | Loss: 0.00004157
Iteration 192/1000 | Loss: 0.00004786
Iteration 193/1000 | Loss: 0.00003774
Iteration 194/1000 | Loss: 0.00041225
Iteration 195/1000 | Loss: 0.00003689
Iteration 196/1000 | Loss: 0.00003554
Iteration 197/1000 | Loss: 0.00005678
Iteration 198/1000 | Loss: 0.00042731
Iteration 199/1000 | Loss: 0.00061589
Iteration 200/1000 | Loss: 0.00005439
Iteration 201/1000 | Loss: 0.00003636
Iteration 202/1000 | Loss: 0.00049627
Iteration 203/1000 | Loss: 0.00006778
Iteration 204/1000 | Loss: 0.00007106
Iteration 205/1000 | Loss: 0.00004369
Iteration 206/1000 | Loss: 0.00004546
Iteration 207/1000 | Loss: 0.00003750
Iteration 208/1000 | Loss: 0.00005307
Iteration 209/1000 | Loss: 0.00007491
Iteration 210/1000 | Loss: 0.00010200
Iteration 211/1000 | Loss: 0.00003522
Iteration 212/1000 | Loss: 0.00008410
Iteration 213/1000 | Loss: 0.00003992
Iteration 214/1000 | Loss: 0.00006296
Iteration 215/1000 | Loss: 0.00006501
Iteration 216/1000 | Loss: 0.00004218
Iteration 217/1000 | Loss: 0.00004321
Iteration 218/1000 | Loss: 0.00004527
Iteration 219/1000 | Loss: 0.00003386
Iteration 220/1000 | Loss: 0.00003928
Iteration 221/1000 | Loss: 0.00003649
Iteration 222/1000 | Loss: 0.00003265
Iteration 223/1000 | Loss: 0.00004439
Iteration 224/1000 | Loss: 0.00005163
Iteration 225/1000 | Loss: 0.00004870
Iteration 226/1000 | Loss: 0.00004225
Iteration 227/1000 | Loss: 0.00004438
Iteration 228/1000 | Loss: 0.00008146
Iteration 229/1000 | Loss: 0.00003699
Iteration 230/1000 | Loss: 0.00004467
Iteration 231/1000 | Loss: 0.00014076
Iteration 232/1000 | Loss: 0.00040419
Iteration 233/1000 | Loss: 0.00176825
Iteration 234/1000 | Loss: 0.00084693
Iteration 235/1000 | Loss: 0.00031689
Iteration 236/1000 | Loss: 0.00021111
Iteration 237/1000 | Loss: 0.00044933
Iteration 238/1000 | Loss: 0.00019446
Iteration 239/1000 | Loss: 0.00005254
Iteration 240/1000 | Loss: 0.00005278
Iteration 241/1000 | Loss: 0.00005223
Iteration 242/1000 | Loss: 0.00005471
Iteration 243/1000 | Loss: 0.00003822
Iteration 244/1000 | Loss: 0.00011397
Iteration 245/1000 | Loss: 0.00005341
Iteration 246/1000 | Loss: 0.00003607
Iteration 247/1000 | Loss: 0.00004404
Iteration 248/1000 | Loss: 0.00003346
Iteration 249/1000 | Loss: 0.00003353
Iteration 250/1000 | Loss: 0.00005015
Iteration 251/1000 | Loss: 0.00004180
Iteration 252/1000 | Loss: 0.00009009
Iteration 253/1000 | Loss: 0.00007443
Iteration 254/1000 | Loss: 0.00004289
Iteration 255/1000 | Loss: 0.00004778
Iteration 256/1000 | Loss: 0.00005174
Iteration 257/1000 | Loss: 0.00005005
Iteration 258/1000 | Loss: 0.00005105
Iteration 259/1000 | Loss: 0.00005052
Iteration 260/1000 | Loss: 0.00007459
Iteration 261/1000 | Loss: 0.00005102
Iteration 262/1000 | Loss: 0.00005215
Iteration 263/1000 | Loss: 0.00005072
Iteration 264/1000 | Loss: 0.00007886
Iteration 265/1000 | Loss: 0.00005385
Iteration 266/1000 | Loss: 0.00007547
Iteration 267/1000 | Loss: 0.00005301
Iteration 268/1000 | Loss: 0.00004776
Iteration 269/1000 | Loss: 0.00004875
Iteration 270/1000 | Loss: 0.00004923
Iteration 271/1000 | Loss: 0.00003213
Iteration 272/1000 | Loss: 0.00005179
Iteration 273/1000 | Loss: 0.00004603
Iteration 274/1000 | Loss: 0.00006588
Iteration 275/1000 | Loss: 0.00004703
Iteration 276/1000 | Loss: 0.00004285
Iteration 277/1000 | Loss: 0.00003857
Iteration 278/1000 | Loss: 0.00009676
Iteration 279/1000 | Loss: 0.00057881
Iteration 280/1000 | Loss: 0.00039327
Iteration 281/1000 | Loss: 0.00044680
Iteration 282/1000 | Loss: 0.00022962
Iteration 283/1000 | Loss: 0.00017387
Iteration 284/1000 | Loss: 0.00004174
Iteration 285/1000 | Loss: 0.00003662
Iteration 286/1000 | Loss: 0.00004383
Iteration 287/1000 | Loss: 0.00004036
Iteration 288/1000 | Loss: 0.00008153
Iteration 289/1000 | Loss: 0.00028597
Iteration 290/1000 | Loss: 0.00009324
Iteration 291/1000 | Loss: 0.00012491
Iteration 292/1000 | Loss: 0.00021479
Iteration 293/1000 | Loss: 0.00038264
Iteration 294/1000 | Loss: 0.00024232
Iteration 295/1000 | Loss: 0.00029320
Iteration 296/1000 | Loss: 0.00016395
Iteration 297/1000 | Loss: 0.00003919
Iteration 298/1000 | Loss: 0.00003770
Iteration 299/1000 | Loss: 0.00026190
Iteration 300/1000 | Loss: 0.00092515
Iteration 301/1000 | Loss: 0.00034557
Iteration 302/1000 | Loss: 0.00004284
Iteration 303/1000 | Loss: 0.00021676
Iteration 304/1000 | Loss: 0.00022604
Iteration 305/1000 | Loss: 0.00016326
Iteration 306/1000 | Loss: 0.00023008
Iteration 307/1000 | Loss: 0.00016032
Iteration 308/1000 | Loss: 0.00028023
Iteration 309/1000 | Loss: 0.00017270
Iteration 310/1000 | Loss: 0.00027526
Iteration 311/1000 | Loss: 0.00009959
Iteration 312/1000 | Loss: 0.00004745
Iteration 313/1000 | Loss: 0.00004434
Iteration 314/1000 | Loss: 0.00010334
Iteration 315/1000 | Loss: 0.00003713
Iteration 316/1000 | Loss: 0.00019361
Iteration 317/1000 | Loss: 0.00014906
Iteration 318/1000 | Loss: 0.00028775
Iteration 319/1000 | Loss: 0.00016018
Iteration 320/1000 | Loss: 0.00004362
Iteration 321/1000 | Loss: 0.00003770
Iteration 322/1000 | Loss: 0.00003242
Iteration 323/1000 | Loss: 0.00002726
Iteration 324/1000 | Loss: 0.00004255
Iteration 325/1000 | Loss: 0.00004108
Iteration 326/1000 | Loss: 0.00004395
Iteration 327/1000 | Loss: 0.00004112
Iteration 328/1000 | Loss: 0.00003591
Iteration 329/1000 | Loss: 0.00003906
Iteration 330/1000 | Loss: 0.00006461
Iteration 331/1000 | Loss: 0.00005373
Iteration 332/1000 | Loss: 0.00003775
Iteration 333/1000 | Loss: 0.00003793
Iteration 334/1000 | Loss: 0.00003974
Iteration 335/1000 | Loss: 0.00005133
Iteration 336/1000 | Loss: 0.00003104
Iteration 337/1000 | Loss: 0.00004274
Iteration 338/1000 | Loss: 0.00003270
Iteration 339/1000 | Loss: 0.00003912
Iteration 340/1000 | Loss: 0.00004255
Iteration 341/1000 | Loss: 0.00003918
Iteration 342/1000 | Loss: 0.00004755
Iteration 343/1000 | Loss: 0.00004916
Iteration 344/1000 | Loss: 0.00008213
Iteration 345/1000 | Loss: 0.00007155
Iteration 346/1000 | Loss: 0.00004778
Iteration 347/1000 | Loss: 0.00003238
Iteration 348/1000 | Loss: 0.00005793
Iteration 349/1000 | Loss: 0.00004650
Iteration 350/1000 | Loss: 0.00003473
Iteration 351/1000 | Loss: 0.00003134
Iteration 352/1000 | Loss: 0.00002822
Iteration 353/1000 | Loss: 0.00003045
Iteration 354/1000 | Loss: 0.00002447
Iteration 355/1000 | Loss: 0.00004730
Iteration 356/1000 | Loss: 0.00004248
Iteration 357/1000 | Loss: 0.00004375
Iteration 358/1000 | Loss: 0.00003606
Iteration 359/1000 | Loss: 0.00003883
Iteration 360/1000 | Loss: 0.00005205
Iteration 361/1000 | Loss: 0.00003618
Iteration 362/1000 | Loss: 0.00012461
Iteration 363/1000 | Loss: 0.00003396
Iteration 364/1000 | Loss: 0.00003621
Iteration 365/1000 | Loss: 0.00003811
Iteration 366/1000 | Loss: 0.00003459
Iteration 367/1000 | Loss: 0.00006031
Iteration 368/1000 | Loss: 0.00015824
Iteration 369/1000 | Loss: 0.00003021
Iteration 370/1000 | Loss: 0.00002544
Iteration 371/1000 | Loss: 0.00002387
Iteration 372/1000 | Loss: 0.00003206
Iteration 373/1000 | Loss: 0.00002311
Iteration 374/1000 | Loss: 0.00002987
Iteration 375/1000 | Loss: 0.00002288
Iteration 376/1000 | Loss: 0.00002274
Iteration 377/1000 | Loss: 0.00002273
Iteration 378/1000 | Loss: 0.00002277
Iteration 379/1000 | Loss: 0.00002276
Iteration 380/1000 | Loss: 0.00002276
Iteration 381/1000 | Loss: 0.00002276
Iteration 382/1000 | Loss: 0.00002266
Iteration 383/1000 | Loss: 0.00002266
Iteration 384/1000 | Loss: 0.00002263
Iteration 385/1000 | Loss: 0.00002685
Iteration 386/1000 | Loss: 0.00002805
Iteration 387/1000 | Loss: 0.00002421
Iteration 388/1000 | Loss: 0.00002257
Iteration 389/1000 | Loss: 0.00002252
Iteration 390/1000 | Loss: 0.00002252
Iteration 391/1000 | Loss: 0.00002252
Iteration 392/1000 | Loss: 0.00002252
Iteration 393/1000 | Loss: 0.00002252
Iteration 394/1000 | Loss: 0.00002252
Iteration 395/1000 | Loss: 0.00002250
Iteration 396/1000 | Loss: 0.00002250
Iteration 397/1000 | Loss: 0.00002250
Iteration 398/1000 | Loss: 0.00002250
Iteration 399/1000 | Loss: 0.00002250
Iteration 400/1000 | Loss: 0.00002249
Iteration 401/1000 | Loss: 0.00002249
Iteration 402/1000 | Loss: 0.00002249
Iteration 403/1000 | Loss: 0.00002249
Iteration 404/1000 | Loss: 0.00002249
Iteration 405/1000 | Loss: 0.00002249
Iteration 406/1000 | Loss: 0.00002249
Iteration 407/1000 | Loss: 0.00002249
Iteration 408/1000 | Loss: 0.00002249
Iteration 409/1000 | Loss: 0.00002249
Iteration 410/1000 | Loss: 0.00002249
Iteration 411/1000 | Loss: 0.00002248
Iteration 412/1000 | Loss: 0.00002248
Iteration 413/1000 | Loss: 0.00002248
Iteration 414/1000 | Loss: 0.00002248
Iteration 415/1000 | Loss: 0.00002248
Iteration 416/1000 | Loss: 0.00002248
Iteration 417/1000 | Loss: 0.00002248
Iteration 418/1000 | Loss: 0.00002248
Iteration 419/1000 | Loss: 0.00002248
Iteration 420/1000 | Loss: 0.00002248
Iteration 421/1000 | Loss: 0.00002248
Iteration 422/1000 | Loss: 0.00002248
Iteration 423/1000 | Loss: 0.00002248
Iteration 424/1000 | Loss: 0.00002248
Iteration 425/1000 | Loss: 0.00002248
Iteration 426/1000 | Loss: 0.00002248
Iteration 427/1000 | Loss: 0.00002248
Iteration 428/1000 | Loss: 0.00002248
Iteration 429/1000 | Loss: 0.00002248
Iteration 430/1000 | Loss: 0.00002248
Iteration 431/1000 | Loss: 0.00002248
Iteration 432/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 432. Stopping optimization.
Last 5 losses: [2.2481211999547668e-05, 2.2481211999547668e-05, 2.2481211999547668e-05, 2.2481211999547668e-05, 2.2481211999547668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2481211999547668e-05

Optimization complete. Final v2v error: 3.8224592208862305 mm

Highest mean error: 6.658236503601074 mm for frame 68

Lowest mean error: 3.2388992309570312 mm for frame 86

Saving results

Total time: 656.3837080001831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847670
Iteration 2/25 | Loss: 0.00110605
Iteration 3/25 | Loss: 0.00088619
Iteration 4/25 | Loss: 0.00083539
Iteration 5/25 | Loss: 0.00080911
Iteration 6/25 | Loss: 0.00080326
Iteration 7/25 | Loss: 0.00080126
Iteration 8/25 | Loss: 0.00080046
Iteration 9/25 | Loss: 0.00080046
Iteration 10/25 | Loss: 0.00080046
Iteration 11/25 | Loss: 0.00080046
Iteration 12/25 | Loss: 0.00080046
Iteration 13/25 | Loss: 0.00080046
Iteration 14/25 | Loss: 0.00080046
Iteration 15/25 | Loss: 0.00080046
Iteration 16/25 | Loss: 0.00080046
Iteration 17/25 | Loss: 0.00080046
Iteration 18/25 | Loss: 0.00080046
Iteration 19/25 | Loss: 0.00080046
Iteration 20/25 | Loss: 0.00080046
Iteration 21/25 | Loss: 0.00080046
Iteration 22/25 | Loss: 0.00080046
Iteration 23/25 | Loss: 0.00080046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008004645351320505, 0.0008004645351320505, 0.0008004645351320505, 0.0008004645351320505, 0.0008004645351320505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008004645351320505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66127658
Iteration 2/25 | Loss: 0.00069706
Iteration 3/25 | Loss: 0.00069706
Iteration 4/25 | Loss: 0.00069706
Iteration 5/25 | Loss: 0.00069706
Iteration 6/25 | Loss: 0.00069706
Iteration 7/25 | Loss: 0.00069706
Iteration 8/25 | Loss: 0.00069706
Iteration 9/25 | Loss: 0.00069706
Iteration 10/25 | Loss: 0.00069706
Iteration 11/25 | Loss: 0.00069706
Iteration 12/25 | Loss: 0.00069706
Iteration 13/25 | Loss: 0.00069706
Iteration 14/25 | Loss: 0.00069706
Iteration 15/25 | Loss: 0.00069706
Iteration 16/25 | Loss: 0.00069706
Iteration 17/25 | Loss: 0.00069706
Iteration 18/25 | Loss: 0.00069706
Iteration 19/25 | Loss: 0.00069706
Iteration 20/25 | Loss: 0.00069706
Iteration 21/25 | Loss: 0.00069706
Iteration 22/25 | Loss: 0.00069706
Iteration 23/25 | Loss: 0.00069706
Iteration 24/25 | Loss: 0.00069706
Iteration 25/25 | Loss: 0.00069706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000697055016644299, 0.000697055016644299, 0.000697055016644299, 0.000697055016644299, 0.000697055016644299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000697055016644299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069706
Iteration 2/1000 | Loss: 0.00004953
Iteration 3/1000 | Loss: 0.00003826
Iteration 4/1000 | Loss: 0.00003164
Iteration 5/1000 | Loss: 0.00002943
Iteration 6/1000 | Loss: 0.00002821
Iteration 7/1000 | Loss: 0.00002724
Iteration 8/1000 | Loss: 0.00002655
Iteration 9/1000 | Loss: 0.00002570
Iteration 10/1000 | Loss: 0.00002521
Iteration 11/1000 | Loss: 0.00002492
Iteration 12/1000 | Loss: 0.00002465
Iteration 13/1000 | Loss: 0.00002442
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002416
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002407
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002406
Iteration 20/1000 | Loss: 0.00002406
Iteration 21/1000 | Loss: 0.00002403
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002399
Iteration 25/1000 | Loss: 0.00002399
Iteration 26/1000 | Loss: 0.00002398
Iteration 27/1000 | Loss: 0.00002398
Iteration 28/1000 | Loss: 0.00002398
Iteration 29/1000 | Loss: 0.00002398
Iteration 30/1000 | Loss: 0.00002397
Iteration 31/1000 | Loss: 0.00002397
Iteration 32/1000 | Loss: 0.00002397
Iteration 33/1000 | Loss: 0.00002397
Iteration 34/1000 | Loss: 0.00002397
Iteration 35/1000 | Loss: 0.00002397
Iteration 36/1000 | Loss: 0.00002397
Iteration 37/1000 | Loss: 0.00002396
Iteration 38/1000 | Loss: 0.00002396
Iteration 39/1000 | Loss: 0.00002395
Iteration 40/1000 | Loss: 0.00002395
Iteration 41/1000 | Loss: 0.00002394
Iteration 42/1000 | Loss: 0.00002394
Iteration 43/1000 | Loss: 0.00002393
Iteration 44/1000 | Loss: 0.00002393
Iteration 45/1000 | Loss: 0.00002393
Iteration 46/1000 | Loss: 0.00002392
Iteration 47/1000 | Loss: 0.00002392
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002392
Iteration 50/1000 | Loss: 0.00002391
Iteration 51/1000 | Loss: 0.00002390
Iteration 52/1000 | Loss: 0.00002390
Iteration 53/1000 | Loss: 0.00002390
Iteration 54/1000 | Loss: 0.00002389
Iteration 55/1000 | Loss: 0.00002389
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002385
Iteration 68/1000 | Loss: 0.00002385
Iteration 69/1000 | Loss: 0.00002384
Iteration 70/1000 | Loss: 0.00002384
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002384
Iteration 73/1000 | Loss: 0.00002383
Iteration 74/1000 | Loss: 0.00002383
Iteration 75/1000 | Loss: 0.00002383
Iteration 76/1000 | Loss: 0.00002382
Iteration 77/1000 | Loss: 0.00002382
Iteration 78/1000 | Loss: 0.00002382
Iteration 79/1000 | Loss: 0.00002382
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00002382
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002382
Iteration 86/1000 | Loss: 0.00002381
Iteration 87/1000 | Loss: 0.00002381
Iteration 88/1000 | Loss: 0.00002381
Iteration 89/1000 | Loss: 0.00002381
Iteration 90/1000 | Loss: 0.00002381
Iteration 91/1000 | Loss: 0.00002381
Iteration 92/1000 | Loss: 0.00002381
Iteration 93/1000 | Loss: 0.00002381
Iteration 94/1000 | Loss: 0.00002381
Iteration 95/1000 | Loss: 0.00002381
Iteration 96/1000 | Loss: 0.00002381
Iteration 97/1000 | Loss: 0.00002381
Iteration 98/1000 | Loss: 0.00002381
Iteration 99/1000 | Loss: 0.00002381
Iteration 100/1000 | Loss: 0.00002380
Iteration 101/1000 | Loss: 0.00002380
Iteration 102/1000 | Loss: 0.00002380
Iteration 103/1000 | Loss: 0.00002380
Iteration 104/1000 | Loss: 0.00002379
Iteration 105/1000 | Loss: 0.00002379
Iteration 106/1000 | Loss: 0.00002379
Iteration 107/1000 | Loss: 0.00002379
Iteration 108/1000 | Loss: 0.00002379
Iteration 109/1000 | Loss: 0.00002378
Iteration 110/1000 | Loss: 0.00002378
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002378
Iteration 113/1000 | Loss: 0.00002378
Iteration 114/1000 | Loss: 0.00002378
Iteration 115/1000 | Loss: 0.00002378
Iteration 116/1000 | Loss: 0.00002378
Iteration 117/1000 | Loss: 0.00002378
Iteration 118/1000 | Loss: 0.00002377
Iteration 119/1000 | Loss: 0.00002377
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002377
Iteration 122/1000 | Loss: 0.00002377
Iteration 123/1000 | Loss: 0.00002377
Iteration 124/1000 | Loss: 0.00002377
Iteration 125/1000 | Loss: 0.00002377
Iteration 126/1000 | Loss: 0.00002377
Iteration 127/1000 | Loss: 0.00002377
Iteration 128/1000 | Loss: 0.00002377
Iteration 129/1000 | Loss: 0.00002377
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002377
Iteration 132/1000 | Loss: 0.00002376
Iteration 133/1000 | Loss: 0.00002376
Iteration 134/1000 | Loss: 0.00002376
Iteration 135/1000 | Loss: 0.00002376
Iteration 136/1000 | Loss: 0.00002376
Iteration 137/1000 | Loss: 0.00002376
Iteration 138/1000 | Loss: 0.00002376
Iteration 139/1000 | Loss: 0.00002376
Iteration 140/1000 | Loss: 0.00002376
Iteration 141/1000 | Loss: 0.00002376
Iteration 142/1000 | Loss: 0.00002375
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002375
Iteration 148/1000 | Loss: 0.00002375
Iteration 149/1000 | Loss: 0.00002375
Iteration 150/1000 | Loss: 0.00002375
Iteration 151/1000 | Loss: 0.00002375
Iteration 152/1000 | Loss: 0.00002375
Iteration 153/1000 | Loss: 0.00002375
Iteration 154/1000 | Loss: 0.00002375
Iteration 155/1000 | Loss: 0.00002375
Iteration 156/1000 | Loss: 0.00002374
Iteration 157/1000 | Loss: 0.00002374
Iteration 158/1000 | Loss: 0.00002374
Iteration 159/1000 | Loss: 0.00002374
Iteration 160/1000 | Loss: 0.00002374
Iteration 161/1000 | Loss: 0.00002374
Iteration 162/1000 | Loss: 0.00002374
Iteration 163/1000 | Loss: 0.00002374
Iteration 164/1000 | Loss: 0.00002374
Iteration 165/1000 | Loss: 0.00002374
Iteration 166/1000 | Loss: 0.00002374
Iteration 167/1000 | Loss: 0.00002374
Iteration 168/1000 | Loss: 0.00002373
Iteration 169/1000 | Loss: 0.00002373
Iteration 170/1000 | Loss: 0.00002373
Iteration 171/1000 | Loss: 0.00002373
Iteration 172/1000 | Loss: 0.00002373
Iteration 173/1000 | Loss: 0.00002373
Iteration 174/1000 | Loss: 0.00002373
Iteration 175/1000 | Loss: 0.00002373
Iteration 176/1000 | Loss: 0.00002373
Iteration 177/1000 | Loss: 0.00002373
Iteration 178/1000 | Loss: 0.00002373
Iteration 179/1000 | Loss: 0.00002373
Iteration 180/1000 | Loss: 0.00002372
Iteration 181/1000 | Loss: 0.00002372
Iteration 182/1000 | Loss: 0.00002372
Iteration 183/1000 | Loss: 0.00002372
Iteration 184/1000 | Loss: 0.00002372
Iteration 185/1000 | Loss: 0.00002372
Iteration 186/1000 | Loss: 0.00002372
Iteration 187/1000 | Loss: 0.00002371
Iteration 188/1000 | Loss: 0.00002371
Iteration 189/1000 | Loss: 0.00002371
Iteration 190/1000 | Loss: 0.00002371
Iteration 191/1000 | Loss: 0.00002371
Iteration 192/1000 | Loss: 0.00002371
Iteration 193/1000 | Loss: 0.00002371
Iteration 194/1000 | Loss: 0.00002371
Iteration 195/1000 | Loss: 0.00002371
Iteration 196/1000 | Loss: 0.00002371
Iteration 197/1000 | Loss: 0.00002371
Iteration 198/1000 | Loss: 0.00002371
Iteration 199/1000 | Loss: 0.00002371
Iteration 200/1000 | Loss: 0.00002371
Iteration 201/1000 | Loss: 0.00002371
Iteration 202/1000 | Loss: 0.00002371
Iteration 203/1000 | Loss: 0.00002371
Iteration 204/1000 | Loss: 0.00002371
Iteration 205/1000 | Loss: 0.00002371
Iteration 206/1000 | Loss: 0.00002371
Iteration 207/1000 | Loss: 0.00002371
Iteration 208/1000 | Loss: 0.00002371
Iteration 209/1000 | Loss: 0.00002371
Iteration 210/1000 | Loss: 0.00002371
Iteration 211/1000 | Loss: 0.00002371
Iteration 212/1000 | Loss: 0.00002371
Iteration 213/1000 | Loss: 0.00002371
Iteration 214/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.3712796973995864e-05, 2.3712796973995864e-05, 2.3712796973995864e-05, 2.3712796973995864e-05, 2.3712796973995864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3712796973995864e-05

Optimization complete. Final v2v error: 3.978269100189209 mm

Highest mean error: 6.185430526733398 mm for frame 123

Lowest mean error: 2.8144569396972656 mm for frame 1

Saving results

Total time: 47.38480544090271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079747
Iteration 2/25 | Loss: 0.00302362
Iteration 3/25 | Loss: 0.00169685
Iteration 4/25 | Loss: 0.00150779
Iteration 5/25 | Loss: 0.00148152
Iteration 6/25 | Loss: 0.00141765
Iteration 7/25 | Loss: 0.00133888
Iteration 8/25 | Loss: 0.00127032
Iteration 9/25 | Loss: 0.00121840
Iteration 10/25 | Loss: 0.00118028
Iteration 11/25 | Loss: 0.00116437
Iteration 12/25 | Loss: 0.00116415
Iteration 13/25 | Loss: 0.00115246
Iteration 14/25 | Loss: 0.00115297
Iteration 15/25 | Loss: 0.00113972
Iteration 16/25 | Loss: 0.00113205
Iteration 17/25 | Loss: 0.00113517
Iteration 18/25 | Loss: 0.00112791
Iteration 19/25 | Loss: 0.00113126
Iteration 20/25 | Loss: 0.00113437
Iteration 21/25 | Loss: 0.00112541
Iteration 22/25 | Loss: 0.00112593
Iteration 23/25 | Loss: 0.00113068
Iteration 24/25 | Loss: 0.00112930
Iteration 25/25 | Loss: 0.00112596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42916715
Iteration 2/25 | Loss: 0.00354532
Iteration 3/25 | Loss: 0.00342966
Iteration 4/25 | Loss: 0.00342966
Iteration 5/25 | Loss: 0.00342966
Iteration 6/25 | Loss: 0.00342966
Iteration 7/25 | Loss: 0.00342966
Iteration 8/25 | Loss: 0.00342966
Iteration 9/25 | Loss: 0.00342966
Iteration 10/25 | Loss: 0.00342966
Iteration 11/25 | Loss: 0.00342966
Iteration 12/25 | Loss: 0.00342966
Iteration 13/25 | Loss: 0.00342966
Iteration 14/25 | Loss: 0.00342966
Iteration 15/25 | Loss: 0.00342966
Iteration 16/25 | Loss: 0.00342966
Iteration 17/25 | Loss: 0.00342966
Iteration 18/25 | Loss: 0.00342966
Iteration 19/25 | Loss: 0.00342966
Iteration 20/25 | Loss: 0.00342966
Iteration 21/25 | Loss: 0.00342966
Iteration 22/25 | Loss: 0.00342966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0034296559169888496, 0.0034296559169888496, 0.0034296559169888496, 0.0034296559169888496, 0.0034296559169888496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034296559169888496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342966
Iteration 2/1000 | Loss: 0.00473109
Iteration 3/1000 | Loss: 0.00603804
Iteration 4/1000 | Loss: 0.00215039
Iteration 5/1000 | Loss: 0.00197679
Iteration 6/1000 | Loss: 0.00132938
Iteration 7/1000 | Loss: 0.00234189
Iteration 8/1000 | Loss: 0.00463991
Iteration 9/1000 | Loss: 0.00263845
Iteration 10/1000 | Loss: 0.00639705
Iteration 11/1000 | Loss: 0.00101922
Iteration 12/1000 | Loss: 0.00191379
Iteration 13/1000 | Loss: 0.00342673
Iteration 14/1000 | Loss: 0.00128155
Iteration 15/1000 | Loss: 0.00175225
Iteration 16/1000 | Loss: 0.00097929
Iteration 17/1000 | Loss: 0.00164656
Iteration 18/1000 | Loss: 0.00244672
Iteration 19/1000 | Loss: 0.00178611
Iteration 20/1000 | Loss: 0.00241927
Iteration 21/1000 | Loss: 0.00167384
Iteration 22/1000 | Loss: 0.00243007
Iteration 23/1000 | Loss: 0.00412105
Iteration 24/1000 | Loss: 0.00550883
Iteration 25/1000 | Loss: 0.00354836
Iteration 26/1000 | Loss: 0.00563926
Iteration 27/1000 | Loss: 0.00361676
Iteration 28/1000 | Loss: 0.00669453
Iteration 29/1000 | Loss: 0.00784977
Iteration 30/1000 | Loss: 0.00346490
Iteration 31/1000 | Loss: 0.00338382
Iteration 32/1000 | Loss: 0.00251677
Iteration 33/1000 | Loss: 0.00183549
Iteration 34/1000 | Loss: 0.00125500
Iteration 35/1000 | Loss: 0.00117908
Iteration 36/1000 | Loss: 0.00106628
Iteration 37/1000 | Loss: 0.00106122
Iteration 38/1000 | Loss: 0.00096139
Iteration 39/1000 | Loss: 0.00134668
Iteration 40/1000 | Loss: 0.00239190
Iteration 41/1000 | Loss: 0.00128902
Iteration 42/1000 | Loss: 0.00022068
Iteration 43/1000 | Loss: 0.00178782
Iteration 44/1000 | Loss: 0.00160814
Iteration 45/1000 | Loss: 0.00040509
Iteration 46/1000 | Loss: 0.00107481
Iteration 47/1000 | Loss: 0.00014176
Iteration 48/1000 | Loss: 0.00029302
Iteration 49/1000 | Loss: 0.00082195
Iteration 50/1000 | Loss: 0.00063671
Iteration 51/1000 | Loss: 0.00010663
Iteration 52/1000 | Loss: 0.00119591
Iteration 53/1000 | Loss: 0.00100214
Iteration 54/1000 | Loss: 0.00042017
Iteration 55/1000 | Loss: 0.00083654
Iteration 56/1000 | Loss: 0.00192380
Iteration 57/1000 | Loss: 0.00234018
Iteration 58/1000 | Loss: 0.00072400
Iteration 59/1000 | Loss: 0.00106825
Iteration 60/1000 | Loss: 0.00017315
Iteration 61/1000 | Loss: 0.00055595
Iteration 62/1000 | Loss: 0.00218779
Iteration 63/1000 | Loss: 0.00011661
Iteration 64/1000 | Loss: 0.00041609
Iteration 65/1000 | Loss: 0.00035272
Iteration 66/1000 | Loss: 0.00039836
Iteration 67/1000 | Loss: 0.00009601
Iteration 68/1000 | Loss: 0.00010670
Iteration 69/1000 | Loss: 0.00027108
Iteration 70/1000 | Loss: 0.00047679
Iteration 71/1000 | Loss: 0.00027136
Iteration 72/1000 | Loss: 0.00159614
Iteration 73/1000 | Loss: 0.00055248
Iteration 74/1000 | Loss: 0.00049107
Iteration 75/1000 | Loss: 0.00031579
Iteration 76/1000 | Loss: 0.00064301
Iteration 77/1000 | Loss: 0.00048526
Iteration 78/1000 | Loss: 0.00086966
Iteration 79/1000 | Loss: 0.00069247
Iteration 80/1000 | Loss: 0.00061599
Iteration 81/1000 | Loss: 0.00030350
Iteration 82/1000 | Loss: 0.00036418
Iteration 83/1000 | Loss: 0.00040008
Iteration 84/1000 | Loss: 0.00052248
Iteration 85/1000 | Loss: 0.00025690
Iteration 86/1000 | Loss: 0.00030743
Iteration 87/1000 | Loss: 0.00051134
Iteration 88/1000 | Loss: 0.00063347
Iteration 89/1000 | Loss: 0.00049566
Iteration 90/1000 | Loss: 0.00045313
Iteration 91/1000 | Loss: 0.00104754
Iteration 92/1000 | Loss: 0.00031769
Iteration 93/1000 | Loss: 0.00042022
Iteration 94/1000 | Loss: 0.00034435
Iteration 95/1000 | Loss: 0.00030968
Iteration 96/1000 | Loss: 0.00012930
Iteration 97/1000 | Loss: 0.00011365
Iteration 98/1000 | Loss: 0.00005634
Iteration 99/1000 | Loss: 0.00005780
Iteration 100/1000 | Loss: 0.00006050
Iteration 101/1000 | Loss: 0.00005010
Iteration 102/1000 | Loss: 0.00005187
Iteration 103/1000 | Loss: 0.00010732
Iteration 104/1000 | Loss: 0.00005799
Iteration 105/1000 | Loss: 0.00003932
Iteration 106/1000 | Loss: 0.00006198
Iteration 107/1000 | Loss: 0.00006608
Iteration 108/1000 | Loss: 0.00105356
Iteration 109/1000 | Loss: 0.00068955
Iteration 110/1000 | Loss: 0.00117040
Iteration 111/1000 | Loss: 0.00118080
Iteration 112/1000 | Loss: 0.00042445
Iteration 113/1000 | Loss: 0.00073604
Iteration 114/1000 | Loss: 0.00129667
Iteration 115/1000 | Loss: 0.00013541
Iteration 116/1000 | Loss: 0.00003557
Iteration 117/1000 | Loss: 0.00005206
Iteration 118/1000 | Loss: 0.00085278
Iteration 119/1000 | Loss: 0.00064737
Iteration 120/1000 | Loss: 0.00054970
Iteration 121/1000 | Loss: 0.00021709
Iteration 122/1000 | Loss: 0.00030201
Iteration 123/1000 | Loss: 0.00021485
Iteration 124/1000 | Loss: 0.00063759
Iteration 125/1000 | Loss: 0.00038553
Iteration 126/1000 | Loss: 0.00021350
Iteration 127/1000 | Loss: 0.00026687
Iteration 128/1000 | Loss: 0.00028321
Iteration 129/1000 | Loss: 0.00016312
Iteration 130/1000 | Loss: 0.00028429
Iteration 131/1000 | Loss: 0.00026578
Iteration 132/1000 | Loss: 0.00042333
Iteration 133/1000 | Loss: 0.00025836
Iteration 134/1000 | Loss: 0.00039507
Iteration 135/1000 | Loss: 0.00025999
Iteration 136/1000 | Loss: 0.00006517
Iteration 137/1000 | Loss: 0.00032257
Iteration 138/1000 | Loss: 0.00010625
Iteration 139/1000 | Loss: 0.00004173
Iteration 140/1000 | Loss: 0.00027267
Iteration 141/1000 | Loss: 0.00014876
Iteration 142/1000 | Loss: 0.00014124
Iteration 143/1000 | Loss: 0.00004367
Iteration 144/1000 | Loss: 0.00004068
Iteration 145/1000 | Loss: 0.00010542
Iteration 146/1000 | Loss: 0.00004406
Iteration 147/1000 | Loss: 0.00004078
Iteration 148/1000 | Loss: 0.00005545
Iteration 149/1000 | Loss: 0.00004103
Iteration 150/1000 | Loss: 0.00005762
Iteration 151/1000 | Loss: 0.00003576
Iteration 152/1000 | Loss: 0.00005830
Iteration 153/1000 | Loss: 0.00003863
Iteration 154/1000 | Loss: 0.00004364
Iteration 155/1000 | Loss: 0.00003622
Iteration 156/1000 | Loss: 0.00009110
Iteration 157/1000 | Loss: 0.00004977
Iteration 158/1000 | Loss: 0.00006997
Iteration 159/1000 | Loss: 0.00005347
Iteration 160/1000 | Loss: 0.00005130
Iteration 161/1000 | Loss: 0.00004415
Iteration 162/1000 | Loss: 0.00004266
Iteration 163/1000 | Loss: 0.00004436
Iteration 164/1000 | Loss: 0.00004224
Iteration 165/1000 | Loss: 0.00016247
Iteration 166/1000 | Loss: 0.00003804
Iteration 167/1000 | Loss: 0.00003385
Iteration 168/1000 | Loss: 0.00004146
Iteration 169/1000 | Loss: 0.00006024
Iteration 170/1000 | Loss: 0.00003734
Iteration 171/1000 | Loss: 0.00004115
Iteration 172/1000 | Loss: 0.00004070
Iteration 173/1000 | Loss: 0.00004467
Iteration 174/1000 | Loss: 0.00004589
Iteration 175/1000 | Loss: 0.00004475
Iteration 176/1000 | Loss: 0.00004819
Iteration 177/1000 | Loss: 0.00004199
Iteration 178/1000 | Loss: 0.00004084
Iteration 179/1000 | Loss: 0.00004086
Iteration 180/1000 | Loss: 0.00004198
Iteration 181/1000 | Loss: 0.00004239
Iteration 182/1000 | Loss: 0.00004241
Iteration 183/1000 | Loss: 0.00003853
Iteration 184/1000 | Loss: 0.00003833
Iteration 185/1000 | Loss: 0.00004912
Iteration 186/1000 | Loss: 0.00004140
Iteration 187/1000 | Loss: 0.00003698
Iteration 188/1000 | Loss: 0.00003809
Iteration 189/1000 | Loss: 0.00003739
Iteration 190/1000 | Loss: 0.00003824
Iteration 191/1000 | Loss: 0.00006450
Iteration 192/1000 | Loss: 0.00002648
Iteration 193/1000 | Loss: 0.00004370
Iteration 194/1000 | Loss: 0.00003646
Iteration 195/1000 | Loss: 0.00003861
Iteration 196/1000 | Loss: 0.00005559
Iteration 197/1000 | Loss: 0.00004756
Iteration 198/1000 | Loss: 0.00004193
Iteration 199/1000 | Loss: 0.00004061
Iteration 200/1000 | Loss: 0.00004044
Iteration 201/1000 | Loss: 0.00005077
Iteration 202/1000 | Loss: 0.00004079
Iteration 203/1000 | Loss: 0.00004698
Iteration 204/1000 | Loss: 0.00004126
Iteration 205/1000 | Loss: 0.00004028
Iteration 206/1000 | Loss: 0.00004048
Iteration 207/1000 | Loss: 0.00003643
Iteration 208/1000 | Loss: 0.00004077
Iteration 209/1000 | Loss: 0.00006362
Iteration 210/1000 | Loss: 0.00003909
Iteration 211/1000 | Loss: 0.00003993
Iteration 212/1000 | Loss: 0.00004012
Iteration 213/1000 | Loss: 0.00003421
Iteration 214/1000 | Loss: 0.00004194
Iteration 215/1000 | Loss: 0.00004127
Iteration 216/1000 | Loss: 0.00004059
Iteration 217/1000 | Loss: 0.00004011
Iteration 218/1000 | Loss: 0.00004034
Iteration 219/1000 | Loss: 0.00004236
Iteration 220/1000 | Loss: 0.00004017
Iteration 221/1000 | Loss: 0.00004087
Iteration 222/1000 | Loss: 0.00004087
Iteration 223/1000 | Loss: 0.00002946
Iteration 224/1000 | Loss: 0.00002472
Iteration 225/1000 | Loss: 0.00002844
Iteration 226/1000 | Loss: 0.00002365
Iteration 227/1000 | Loss: 0.00002316
Iteration 228/1000 | Loss: 0.00004676
Iteration 229/1000 | Loss: 0.00002223
Iteration 230/1000 | Loss: 0.00003634
Iteration 231/1000 | Loss: 0.00011983
Iteration 232/1000 | Loss: 0.00008805
Iteration 233/1000 | Loss: 0.00002310
Iteration 234/1000 | Loss: 0.00015477
Iteration 235/1000 | Loss: 0.00003019
Iteration 236/1000 | Loss: 0.00002168
Iteration 237/1000 | Loss: 0.00002165
Iteration 238/1000 | Loss: 0.00002162
Iteration 239/1000 | Loss: 0.00002161
Iteration 240/1000 | Loss: 0.00002159
Iteration 241/1000 | Loss: 0.00002159
Iteration 242/1000 | Loss: 0.00002157
Iteration 243/1000 | Loss: 0.00002156
Iteration 244/1000 | Loss: 0.00002156
Iteration 245/1000 | Loss: 0.00002156
Iteration 246/1000 | Loss: 0.00002155
Iteration 247/1000 | Loss: 0.00002155
Iteration 248/1000 | Loss: 0.00002155
Iteration 249/1000 | Loss: 0.00002154
Iteration 250/1000 | Loss: 0.00002154
Iteration 251/1000 | Loss: 0.00002153
Iteration 252/1000 | Loss: 0.00002153
Iteration 253/1000 | Loss: 0.00002153
Iteration 254/1000 | Loss: 0.00002152
Iteration 255/1000 | Loss: 0.00002151
Iteration 256/1000 | Loss: 0.00002151
Iteration 257/1000 | Loss: 0.00002151
Iteration 258/1000 | Loss: 0.00002150
Iteration 259/1000 | Loss: 0.00002150
Iteration 260/1000 | Loss: 0.00002150
Iteration 261/1000 | Loss: 0.00002149
Iteration 262/1000 | Loss: 0.00002149
Iteration 263/1000 | Loss: 0.00002148
Iteration 264/1000 | Loss: 0.00002147
Iteration 265/1000 | Loss: 0.00002144
Iteration 266/1000 | Loss: 0.00002142
Iteration 267/1000 | Loss: 0.00002140
Iteration 268/1000 | Loss: 0.00002140
Iteration 269/1000 | Loss: 0.00002140
Iteration 270/1000 | Loss: 0.00002140
Iteration 271/1000 | Loss: 0.00002139
Iteration 272/1000 | Loss: 0.00002138
Iteration 273/1000 | Loss: 0.00002137
Iteration 274/1000 | Loss: 0.00002136
Iteration 275/1000 | Loss: 0.00002135
Iteration 276/1000 | Loss: 0.00002135
Iteration 277/1000 | Loss: 0.00002132
Iteration 278/1000 | Loss: 0.00002129
Iteration 279/1000 | Loss: 0.00002129
Iteration 280/1000 | Loss: 0.00002129
Iteration 281/1000 | Loss: 0.00002129
Iteration 282/1000 | Loss: 0.00002129
Iteration 283/1000 | Loss: 0.00002128
Iteration 284/1000 | Loss: 0.00002128
Iteration 285/1000 | Loss: 0.00002128
Iteration 286/1000 | Loss: 0.00002128
Iteration 287/1000 | Loss: 0.00002128
Iteration 288/1000 | Loss: 0.00002128
Iteration 289/1000 | Loss: 0.00002128
Iteration 290/1000 | Loss: 0.00002128
Iteration 291/1000 | Loss: 0.00002127
Iteration 292/1000 | Loss: 0.00002127
Iteration 293/1000 | Loss: 0.00002127
Iteration 294/1000 | Loss: 0.00002127
Iteration 295/1000 | Loss: 0.00002127
Iteration 296/1000 | Loss: 0.00002127
Iteration 297/1000 | Loss: 0.00002127
Iteration 298/1000 | Loss: 0.00002127
Iteration 299/1000 | Loss: 0.00002126
Iteration 300/1000 | Loss: 0.00002126
Iteration 301/1000 | Loss: 0.00002126
Iteration 302/1000 | Loss: 0.00002126
Iteration 303/1000 | Loss: 0.00002126
Iteration 304/1000 | Loss: 0.00002126
Iteration 305/1000 | Loss: 0.00002126
Iteration 306/1000 | Loss: 0.00002125
Iteration 307/1000 | Loss: 0.00002125
Iteration 308/1000 | Loss: 0.00002125
Iteration 309/1000 | Loss: 0.00002125
Iteration 310/1000 | Loss: 0.00002125
Iteration 311/1000 | Loss: 0.00002125
Iteration 312/1000 | Loss: 0.00002125
Iteration 313/1000 | Loss: 0.00002125
Iteration 314/1000 | Loss: 0.00002125
Iteration 315/1000 | Loss: 0.00002125
Iteration 316/1000 | Loss: 0.00002124
Iteration 317/1000 | Loss: 0.00002124
Iteration 318/1000 | Loss: 0.00002124
Iteration 319/1000 | Loss: 0.00002124
Iteration 320/1000 | Loss: 0.00002124
Iteration 321/1000 | Loss: 0.00002124
Iteration 322/1000 | Loss: 0.00002124
Iteration 323/1000 | Loss: 0.00002124
Iteration 324/1000 | Loss: 0.00002124
Iteration 325/1000 | Loss: 0.00002124
Iteration 326/1000 | Loss: 0.00002124
Iteration 327/1000 | Loss: 0.00002123
Iteration 328/1000 | Loss: 0.00002123
Iteration 329/1000 | Loss: 0.00002123
Iteration 330/1000 | Loss: 0.00002123
Iteration 331/1000 | Loss: 0.00002123
Iteration 332/1000 | Loss: 0.00002123
Iteration 333/1000 | Loss: 0.00002123
Iteration 334/1000 | Loss: 0.00002123
Iteration 335/1000 | Loss: 0.00002123
Iteration 336/1000 | Loss: 0.00002123
Iteration 337/1000 | Loss: 0.00002123
Iteration 338/1000 | Loss: 0.00002122
Iteration 339/1000 | Loss: 0.00002122
Iteration 340/1000 | Loss: 0.00002122
Iteration 341/1000 | Loss: 0.00002122
Iteration 342/1000 | Loss: 0.00002122
Iteration 343/1000 | Loss: 0.00002122
Iteration 344/1000 | Loss: 0.00002122
Iteration 345/1000 | Loss: 0.00002122
Iteration 346/1000 | Loss: 0.00002122
Iteration 347/1000 | Loss: 0.00002122
Iteration 348/1000 | Loss: 0.00002121
Iteration 349/1000 | Loss: 0.00002121
Iteration 350/1000 | Loss: 0.00002121
Iteration 351/1000 | Loss: 0.00002121
Iteration 352/1000 | Loss: 0.00002121
Iteration 353/1000 | Loss: 0.00002121
Iteration 354/1000 | Loss: 0.00002121
Iteration 355/1000 | Loss: 0.00002121
Iteration 356/1000 | Loss: 0.00002121
Iteration 357/1000 | Loss: 0.00002121
Iteration 358/1000 | Loss: 0.00002121
Iteration 359/1000 | Loss: 0.00002121
Iteration 360/1000 | Loss: 0.00002121
Iteration 361/1000 | Loss: 0.00002121
Iteration 362/1000 | Loss: 0.00002121
Iteration 363/1000 | Loss: 0.00002120
Iteration 364/1000 | Loss: 0.00002120
Iteration 365/1000 | Loss: 0.00002120
Iteration 366/1000 | Loss: 0.00002120
Iteration 367/1000 | Loss: 0.00002120
Iteration 368/1000 | Loss: 0.00002120
Iteration 369/1000 | Loss: 0.00002120
Iteration 370/1000 | Loss: 0.00002120
Iteration 371/1000 | Loss: 0.00002120
Iteration 372/1000 | Loss: 0.00002120
Iteration 373/1000 | Loss: 0.00002120
Iteration 374/1000 | Loss: 0.00002120
Iteration 375/1000 | Loss: 0.00002120
Iteration 376/1000 | Loss: 0.00002120
Iteration 377/1000 | Loss: 0.00002120
Iteration 378/1000 | Loss: 0.00002120
Iteration 379/1000 | Loss: 0.00002120
Iteration 380/1000 | Loss: 0.00002120
Iteration 381/1000 | Loss: 0.00002120
Iteration 382/1000 | Loss: 0.00002120
Iteration 383/1000 | Loss: 0.00002120
Iteration 384/1000 | Loss: 0.00002120
Iteration 385/1000 | Loss: 0.00002120
Iteration 386/1000 | Loss: 0.00002119
Iteration 387/1000 | Loss: 0.00002119
Iteration 388/1000 | Loss: 0.00002119
Iteration 389/1000 | Loss: 0.00002119
Iteration 390/1000 | Loss: 0.00002119
Iteration 391/1000 | Loss: 0.00002119
Iteration 392/1000 | Loss: 0.00002119
Iteration 393/1000 | Loss: 0.00002119
Iteration 394/1000 | Loss: 0.00002119
Iteration 395/1000 | Loss: 0.00002119
Iteration 396/1000 | Loss: 0.00002119
Iteration 397/1000 | Loss: 0.00002119
Iteration 398/1000 | Loss: 0.00002119
Iteration 399/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 399. Stopping optimization.
Last 5 losses: [2.1193160137045197e-05, 2.1193160137045197e-05, 2.1193160137045197e-05, 2.1193160137045197e-05, 2.1193160137045197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1193160137045197e-05

Optimization complete. Final v2v error: 3.742687225341797 mm

Highest mean error: 5.861500263214111 mm for frame 50

Lowest mean error: 3.0601351261138916 mm for frame 69

Saving results

Total time: 406.9676294326782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416124
Iteration 2/25 | Loss: 0.00096613
Iteration 3/25 | Loss: 0.00080042
Iteration 4/25 | Loss: 0.00077685
Iteration 5/25 | Loss: 0.00077113
Iteration 6/25 | Loss: 0.00076939
Iteration 7/25 | Loss: 0.00076857
Iteration 8/25 | Loss: 0.00076851
Iteration 9/25 | Loss: 0.00076851
Iteration 10/25 | Loss: 0.00076851
Iteration 11/25 | Loss: 0.00076851
Iteration 12/25 | Loss: 0.00076851
Iteration 13/25 | Loss: 0.00076851
Iteration 14/25 | Loss: 0.00076851
Iteration 15/25 | Loss: 0.00076851
Iteration 16/25 | Loss: 0.00076851
Iteration 17/25 | Loss: 0.00076851
Iteration 18/25 | Loss: 0.00076851
Iteration 19/25 | Loss: 0.00076851
Iteration 20/25 | Loss: 0.00076851
Iteration 21/25 | Loss: 0.00076851
Iteration 22/25 | Loss: 0.00076851
Iteration 23/25 | Loss: 0.00076851
Iteration 24/25 | Loss: 0.00076851
Iteration 25/25 | Loss: 0.00076851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51155615
Iteration 2/25 | Loss: 0.00050403
Iteration 3/25 | Loss: 0.00050403
Iteration 4/25 | Loss: 0.00050403
Iteration 5/25 | Loss: 0.00050403
Iteration 6/25 | Loss: 0.00050403
Iteration 7/25 | Loss: 0.00050403
Iteration 8/25 | Loss: 0.00050403
Iteration 9/25 | Loss: 0.00050403
Iteration 10/25 | Loss: 0.00050403
Iteration 11/25 | Loss: 0.00050403
Iteration 12/25 | Loss: 0.00050403
Iteration 13/25 | Loss: 0.00050403
Iteration 14/25 | Loss: 0.00050403
Iteration 15/25 | Loss: 0.00050403
Iteration 16/25 | Loss: 0.00050403
Iteration 17/25 | Loss: 0.00050403
Iteration 18/25 | Loss: 0.00050403
Iteration 19/25 | Loss: 0.00050403
Iteration 20/25 | Loss: 0.00050403
Iteration 21/25 | Loss: 0.00050403
Iteration 22/25 | Loss: 0.00050403
Iteration 23/25 | Loss: 0.00050403
Iteration 24/25 | Loss: 0.00050403
Iteration 25/25 | Loss: 0.00050403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050403
Iteration 2/1000 | Loss: 0.00002585
Iteration 3/1000 | Loss: 0.00001926
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001655
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001631
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001631
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001630
Iteration 18/1000 | Loss: 0.00001629
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00001626
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001618
Iteration 34/1000 | Loss: 0.00001617
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001615
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001614
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001606
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001604
Iteration 62/1000 | Loss: 0.00001604
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001592
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001592
Iteration 83/1000 | Loss: 0.00001592
Iteration 84/1000 | Loss: 0.00001592
Iteration 85/1000 | Loss: 0.00001592
Iteration 86/1000 | Loss: 0.00001592
Iteration 87/1000 | Loss: 0.00001592
Iteration 88/1000 | Loss: 0.00001591
Iteration 89/1000 | Loss: 0.00001591
Iteration 90/1000 | Loss: 0.00001591
Iteration 91/1000 | Loss: 0.00001591
Iteration 92/1000 | Loss: 0.00001590
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001590
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001589
Iteration 97/1000 | Loss: 0.00001589
Iteration 98/1000 | Loss: 0.00001589
Iteration 99/1000 | Loss: 0.00001589
Iteration 100/1000 | Loss: 0.00001589
Iteration 101/1000 | Loss: 0.00001589
Iteration 102/1000 | Loss: 0.00001589
Iteration 103/1000 | Loss: 0.00001589
Iteration 104/1000 | Loss: 0.00001588
Iteration 105/1000 | Loss: 0.00001588
Iteration 106/1000 | Loss: 0.00001588
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001588
Iteration 109/1000 | Loss: 0.00001588
Iteration 110/1000 | Loss: 0.00001588
Iteration 111/1000 | Loss: 0.00001588
Iteration 112/1000 | Loss: 0.00001587
Iteration 113/1000 | Loss: 0.00001587
Iteration 114/1000 | Loss: 0.00001587
Iteration 115/1000 | Loss: 0.00001587
Iteration 116/1000 | Loss: 0.00001587
Iteration 117/1000 | Loss: 0.00001587
Iteration 118/1000 | Loss: 0.00001587
Iteration 119/1000 | Loss: 0.00001587
Iteration 120/1000 | Loss: 0.00001587
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001586
Iteration 128/1000 | Loss: 0.00001586
Iteration 129/1000 | Loss: 0.00001586
Iteration 130/1000 | Loss: 0.00001586
Iteration 131/1000 | Loss: 0.00001586
Iteration 132/1000 | Loss: 0.00001586
Iteration 133/1000 | Loss: 0.00001586
Iteration 134/1000 | Loss: 0.00001586
Iteration 135/1000 | Loss: 0.00001586
Iteration 136/1000 | Loss: 0.00001586
Iteration 137/1000 | Loss: 0.00001586
Iteration 138/1000 | Loss: 0.00001586
Iteration 139/1000 | Loss: 0.00001586
Iteration 140/1000 | Loss: 0.00001586
Iteration 141/1000 | Loss: 0.00001586
Iteration 142/1000 | Loss: 0.00001586
Iteration 143/1000 | Loss: 0.00001586
Iteration 144/1000 | Loss: 0.00001586
Iteration 145/1000 | Loss: 0.00001586
Iteration 146/1000 | Loss: 0.00001586
Iteration 147/1000 | Loss: 0.00001586
Iteration 148/1000 | Loss: 0.00001586
Iteration 149/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.58600141730858e-05, 1.58600141730858e-05, 1.58600141730858e-05, 1.58600141730858e-05, 1.58600141730858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.58600141730858e-05

Optimization complete. Final v2v error: 3.3530030250549316 mm

Highest mean error: 3.7393085956573486 mm for frame 21

Lowest mean error: 3.1949219703674316 mm for frame 79

Saving results

Total time: 36.6000120639801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428298
Iteration 2/25 | Loss: 0.00097753
Iteration 3/25 | Loss: 0.00079381
Iteration 4/25 | Loss: 0.00076978
Iteration 5/25 | Loss: 0.00076494
Iteration 6/25 | Loss: 0.00076336
Iteration 7/25 | Loss: 0.00076336
Iteration 8/25 | Loss: 0.00076336
Iteration 9/25 | Loss: 0.00076336
Iteration 10/25 | Loss: 0.00076336
Iteration 11/25 | Loss: 0.00076336
Iteration 12/25 | Loss: 0.00076336
Iteration 13/25 | Loss: 0.00076336
Iteration 14/25 | Loss: 0.00076336
Iteration 15/25 | Loss: 0.00076336
Iteration 16/25 | Loss: 0.00076336
Iteration 17/25 | Loss: 0.00076336
Iteration 18/25 | Loss: 0.00076336
Iteration 19/25 | Loss: 0.00076336
Iteration 20/25 | Loss: 0.00076336
Iteration 21/25 | Loss: 0.00076336
Iteration 22/25 | Loss: 0.00076336
Iteration 23/25 | Loss: 0.00076336
Iteration 24/25 | Loss: 0.00076336
Iteration 25/25 | Loss: 0.00076336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48962104
Iteration 2/25 | Loss: 0.00056094
Iteration 3/25 | Loss: 0.00056094
Iteration 4/25 | Loss: 0.00056094
Iteration 5/25 | Loss: 0.00056094
Iteration 6/25 | Loss: 0.00056094
Iteration 7/25 | Loss: 0.00056094
Iteration 8/25 | Loss: 0.00056094
Iteration 9/25 | Loss: 0.00056094
Iteration 10/25 | Loss: 0.00056094
Iteration 11/25 | Loss: 0.00056094
Iteration 12/25 | Loss: 0.00056094
Iteration 13/25 | Loss: 0.00056094
Iteration 14/25 | Loss: 0.00056094
Iteration 15/25 | Loss: 0.00056094
Iteration 16/25 | Loss: 0.00056094
Iteration 17/25 | Loss: 0.00056094
Iteration 18/25 | Loss: 0.00056094
Iteration 19/25 | Loss: 0.00056094
Iteration 20/25 | Loss: 0.00056094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005609404761344194, 0.0005609404761344194, 0.0005609404761344194, 0.0005609404761344194, 0.0005609404761344194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005609404761344194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056094
Iteration 2/1000 | Loss: 0.00002419
Iteration 3/1000 | Loss: 0.00001655
Iteration 4/1000 | Loss: 0.00001531
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001341
Iteration 96/1000 | Loss: 0.00001341
Iteration 97/1000 | Loss: 0.00001341
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001340
Iteration 101/1000 | Loss: 0.00001340
Iteration 102/1000 | Loss: 0.00001340
Iteration 103/1000 | Loss: 0.00001340
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001338
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001337
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001337
Iteration 143/1000 | Loss: 0.00001337
Iteration 144/1000 | Loss: 0.00001337
Iteration 145/1000 | Loss: 0.00001337
Iteration 146/1000 | Loss: 0.00001337
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001337
Iteration 149/1000 | Loss: 0.00001337
Iteration 150/1000 | Loss: 0.00001337
Iteration 151/1000 | Loss: 0.00001337
Iteration 152/1000 | Loss: 0.00001337
Iteration 153/1000 | Loss: 0.00001337
Iteration 154/1000 | Loss: 0.00001337
Iteration 155/1000 | Loss: 0.00001337
Iteration 156/1000 | Loss: 0.00001337
Iteration 157/1000 | Loss: 0.00001337
Iteration 158/1000 | Loss: 0.00001337
Iteration 159/1000 | Loss: 0.00001337
Iteration 160/1000 | Loss: 0.00001337
Iteration 161/1000 | Loss: 0.00001337
Iteration 162/1000 | Loss: 0.00001337
Iteration 163/1000 | Loss: 0.00001337
Iteration 164/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3373937690630555e-05, 1.3373937690630555e-05, 1.3373937690630555e-05, 1.3373937690630555e-05, 1.3373937690630555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3373937690630555e-05

Optimization complete. Final v2v error: 3.0814621448516846 mm

Highest mean error: 3.583669424057007 mm for frame 9

Lowest mean error: 2.8657591342926025 mm for frame 20

Saving results

Total time: 34.58983278274536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784093
Iteration 2/25 | Loss: 0.00108210
Iteration 3/25 | Loss: 0.00085949
Iteration 4/25 | Loss: 0.00081950
Iteration 5/25 | Loss: 0.00081002
Iteration 6/25 | Loss: 0.00080939
Iteration 7/25 | Loss: 0.00080939
Iteration 8/25 | Loss: 0.00080939
Iteration 9/25 | Loss: 0.00080939
Iteration 10/25 | Loss: 0.00080939
Iteration 11/25 | Loss: 0.00080939
Iteration 12/25 | Loss: 0.00080939
Iteration 13/25 | Loss: 0.00080939
Iteration 14/25 | Loss: 0.00080939
Iteration 15/25 | Loss: 0.00080939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008093888172879815, 0.0008093888172879815, 0.0008093888172879815, 0.0008093888172879815, 0.0008093888172879815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008093888172879815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57503223
Iteration 2/25 | Loss: 0.00048646
Iteration 3/25 | Loss: 0.00048646
Iteration 4/25 | Loss: 0.00048646
Iteration 5/25 | Loss: 0.00048646
Iteration 6/25 | Loss: 0.00048646
Iteration 7/25 | Loss: 0.00048646
Iteration 8/25 | Loss: 0.00048646
Iteration 9/25 | Loss: 0.00048646
Iteration 10/25 | Loss: 0.00048646
Iteration 11/25 | Loss: 0.00048646
Iteration 12/25 | Loss: 0.00048646
Iteration 13/25 | Loss: 0.00048646
Iteration 14/25 | Loss: 0.00048646
Iteration 15/25 | Loss: 0.00048646
Iteration 16/25 | Loss: 0.00048646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004864595248363912, 0.0004864595248363912, 0.0004864595248363912, 0.0004864595248363912, 0.0004864595248363912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004864595248363912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048646
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002727
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002305
Iteration 7/1000 | Loss: 0.00002244
Iteration 8/1000 | Loss: 0.00002183
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002111
Iteration 11/1000 | Loss: 0.00002083
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002039
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002035
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002030
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002029
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00002029
Iteration 35/1000 | Loss: 0.00002029
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002029
Iteration 38/1000 | Loss: 0.00002028
Iteration 39/1000 | Loss: 0.00002028
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002028
Iteration 42/1000 | Loss: 0.00002027
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002027
Iteration 45/1000 | Loss: 0.00002027
Iteration 46/1000 | Loss: 0.00002027
Iteration 47/1000 | Loss: 0.00002027
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002026
Iteration 50/1000 | Loss: 0.00002026
Iteration 51/1000 | Loss: 0.00002026
Iteration 52/1000 | Loss: 0.00002026
Iteration 53/1000 | Loss: 0.00002026
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00002026
Iteration 57/1000 | Loss: 0.00002026
Iteration 58/1000 | Loss: 0.00002026
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002025
Iteration 61/1000 | Loss: 0.00002025
Iteration 62/1000 | Loss: 0.00002025
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002025
Iteration 66/1000 | Loss: 0.00002025
Iteration 67/1000 | Loss: 0.00002025
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002025
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00002024
Iteration 77/1000 | Loss: 0.00002024
Iteration 78/1000 | Loss: 0.00002024
Iteration 79/1000 | Loss: 0.00002024
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002024
Iteration 82/1000 | Loss: 0.00002024
Iteration 83/1000 | Loss: 0.00002024
Iteration 84/1000 | Loss: 0.00002024
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002022
Iteration 102/1000 | Loss: 0.00002022
Iteration 103/1000 | Loss: 0.00002022
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002021
Iteration 106/1000 | Loss: 0.00002021
Iteration 107/1000 | Loss: 0.00002021
Iteration 108/1000 | Loss: 0.00002021
Iteration 109/1000 | Loss: 0.00002020
Iteration 110/1000 | Loss: 0.00002020
Iteration 111/1000 | Loss: 0.00002020
Iteration 112/1000 | Loss: 0.00002020
Iteration 113/1000 | Loss: 0.00002020
Iteration 114/1000 | Loss: 0.00002020
Iteration 115/1000 | Loss: 0.00002020
Iteration 116/1000 | Loss: 0.00002020
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002020
Iteration 119/1000 | Loss: 0.00002020
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002020
Iteration 122/1000 | Loss: 0.00002020
Iteration 123/1000 | Loss: 0.00002020
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002020
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002020
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.019828389165923e-05, 2.019828389165923e-05, 2.019828389165923e-05, 2.019828389165923e-05, 2.019828389165923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.019828389165923e-05

Optimization complete. Final v2v error: 3.693659782409668 mm

Highest mean error: 4.203643798828125 mm for frame 24

Lowest mean error: 2.958080768585205 mm for frame 172

Saving results

Total time: 39.59888505935669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058610
Iteration 2/25 | Loss: 0.00273348
Iteration 3/25 | Loss: 0.00154785
Iteration 4/25 | Loss: 0.00130146
Iteration 5/25 | Loss: 0.00123161
Iteration 6/25 | Loss: 0.00111271
Iteration 7/25 | Loss: 0.00106291
Iteration 8/25 | Loss: 0.00102602
Iteration 9/25 | Loss: 0.00098114
Iteration 10/25 | Loss: 0.00095362
Iteration 11/25 | Loss: 0.00095805
Iteration 12/25 | Loss: 0.00091916
Iteration 13/25 | Loss: 0.00089385
Iteration 14/25 | Loss: 0.00087377
Iteration 15/25 | Loss: 0.00085535
Iteration 16/25 | Loss: 0.00085615
Iteration 17/25 | Loss: 0.00085507
Iteration 18/25 | Loss: 0.00086520
Iteration 19/25 | Loss: 0.00087277
Iteration 20/25 | Loss: 0.00088056
Iteration 21/25 | Loss: 0.00087449
Iteration 22/25 | Loss: 0.00088995
Iteration 23/25 | Loss: 0.00088997
Iteration 24/25 | Loss: 0.00090980
Iteration 25/25 | Loss: 0.00089920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.57603025
Iteration 2/25 | Loss: 0.00221095
Iteration 3/25 | Loss: 0.00197255
Iteration 4/25 | Loss: 0.00197255
Iteration 5/25 | Loss: 0.00197255
Iteration 6/25 | Loss: 0.00197255
Iteration 7/25 | Loss: 0.00197255
Iteration 8/25 | Loss: 0.00197255
Iteration 9/25 | Loss: 0.00197255
Iteration 10/25 | Loss: 0.00197255
Iteration 11/25 | Loss: 0.00197255
Iteration 12/25 | Loss: 0.00197255
Iteration 13/25 | Loss: 0.00197255
Iteration 14/25 | Loss: 0.00197255
Iteration 15/25 | Loss: 0.00197255
Iteration 16/25 | Loss: 0.00197255
Iteration 17/25 | Loss: 0.00197255
Iteration 18/25 | Loss: 0.00197255
Iteration 19/25 | Loss: 0.00197255
Iteration 20/25 | Loss: 0.00197255
Iteration 21/25 | Loss: 0.00197255
Iteration 22/25 | Loss: 0.00197255
Iteration 23/25 | Loss: 0.00197255
Iteration 24/25 | Loss: 0.00197255
Iteration 25/25 | Loss: 0.00197255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197255
Iteration 2/1000 | Loss: 0.00142485
Iteration 3/1000 | Loss: 0.00254655
Iteration 4/1000 | Loss: 0.00202173
Iteration 5/1000 | Loss: 0.00222067
Iteration 6/1000 | Loss: 0.00283125
Iteration 7/1000 | Loss: 0.00122985
Iteration 8/1000 | Loss: 0.00121752
Iteration 9/1000 | Loss: 0.00140544
Iteration 10/1000 | Loss: 0.00129622
Iteration 11/1000 | Loss: 0.00131699
Iteration 12/1000 | Loss: 0.00148139
Iteration 13/1000 | Loss: 0.00110713
Iteration 14/1000 | Loss: 0.00158487
Iteration 15/1000 | Loss: 0.00139377
Iteration 16/1000 | Loss: 0.00143417
Iteration 17/1000 | Loss: 0.00169144
Iteration 18/1000 | Loss: 0.00114127
Iteration 19/1000 | Loss: 0.00093568
Iteration 20/1000 | Loss: 0.00103870
Iteration 21/1000 | Loss: 0.00097626
Iteration 22/1000 | Loss: 0.00151142
Iteration 23/1000 | Loss: 0.00112772
Iteration 24/1000 | Loss: 0.00144725
Iteration 25/1000 | Loss: 0.00102685
Iteration 26/1000 | Loss: 0.00121948
Iteration 27/1000 | Loss: 0.00146560
Iteration 28/1000 | Loss: 0.00101631
Iteration 29/1000 | Loss: 0.00090989
Iteration 30/1000 | Loss: 0.00090204
Iteration 31/1000 | Loss: 0.00097456
Iteration 32/1000 | Loss: 0.00124845
Iteration 33/1000 | Loss: 0.00147148
Iteration 34/1000 | Loss: 0.00100266
Iteration 35/1000 | Loss: 0.00046735
Iteration 36/1000 | Loss: 0.00077877
Iteration 37/1000 | Loss: 0.00120611
Iteration 38/1000 | Loss: 0.00127030
Iteration 39/1000 | Loss: 0.00107301
Iteration 40/1000 | Loss: 0.00098708
Iteration 41/1000 | Loss: 0.00099998
Iteration 42/1000 | Loss: 0.00086846
Iteration 43/1000 | Loss: 0.00073376
Iteration 44/1000 | Loss: 0.00182329
Iteration 45/1000 | Loss: 0.00087834
Iteration 46/1000 | Loss: 0.00100663
Iteration 47/1000 | Loss: 0.00053838
Iteration 48/1000 | Loss: 0.00045909
Iteration 49/1000 | Loss: 0.00082232
Iteration 50/1000 | Loss: 0.00133795
Iteration 51/1000 | Loss: 0.00038456
Iteration 52/1000 | Loss: 0.00037880
Iteration 53/1000 | Loss: 0.00027936
Iteration 54/1000 | Loss: 0.00081322
Iteration 55/1000 | Loss: 0.00039748
Iteration 56/1000 | Loss: 0.00020128
Iteration 57/1000 | Loss: 0.00031319
Iteration 58/1000 | Loss: 0.00050017
Iteration 59/1000 | Loss: 0.00085824
Iteration 60/1000 | Loss: 0.00071350
Iteration 61/1000 | Loss: 0.00036052
Iteration 62/1000 | Loss: 0.00034704
Iteration 63/1000 | Loss: 0.00049806
Iteration 64/1000 | Loss: 0.00053183
Iteration 65/1000 | Loss: 0.00046975
Iteration 66/1000 | Loss: 0.00025149
Iteration 67/1000 | Loss: 0.00049036
Iteration 68/1000 | Loss: 0.00024226
Iteration 69/1000 | Loss: 0.00035008
Iteration 70/1000 | Loss: 0.00055215
Iteration 71/1000 | Loss: 0.00045810
Iteration 72/1000 | Loss: 0.00039336
Iteration 73/1000 | Loss: 0.00038140
Iteration 74/1000 | Loss: 0.00082269
Iteration 75/1000 | Loss: 0.00116706
Iteration 76/1000 | Loss: 0.00075841
Iteration 77/1000 | Loss: 0.00054932
Iteration 78/1000 | Loss: 0.00094993
Iteration 79/1000 | Loss: 0.00068818
Iteration 80/1000 | Loss: 0.00106301
Iteration 81/1000 | Loss: 0.00044636
Iteration 82/1000 | Loss: 0.00044200
Iteration 83/1000 | Loss: 0.00063737
Iteration 84/1000 | Loss: 0.00135652
Iteration 85/1000 | Loss: 0.00056746
Iteration 86/1000 | Loss: 0.00076927
Iteration 87/1000 | Loss: 0.00046004
Iteration 88/1000 | Loss: 0.00027479
Iteration 89/1000 | Loss: 0.00081250
Iteration 90/1000 | Loss: 0.00089224
Iteration 91/1000 | Loss: 0.00026256
Iteration 92/1000 | Loss: 0.00028590
Iteration 93/1000 | Loss: 0.00010383
Iteration 94/1000 | Loss: 0.00007525
Iteration 95/1000 | Loss: 0.00012410
Iteration 96/1000 | Loss: 0.00013100
Iteration 97/1000 | Loss: 0.00021501
Iteration 98/1000 | Loss: 0.00018749
Iteration 99/1000 | Loss: 0.00071845
Iteration 100/1000 | Loss: 0.00024993
Iteration 101/1000 | Loss: 0.00024744
Iteration 102/1000 | Loss: 0.00023023
Iteration 103/1000 | Loss: 0.00017905
Iteration 104/1000 | Loss: 0.00022108
Iteration 105/1000 | Loss: 0.00036755
Iteration 106/1000 | Loss: 0.00012949
Iteration 107/1000 | Loss: 0.00009483
Iteration 108/1000 | Loss: 0.00022255
Iteration 109/1000 | Loss: 0.00016643
Iteration 110/1000 | Loss: 0.00058531
Iteration 111/1000 | Loss: 0.00031092
Iteration 112/1000 | Loss: 0.00034350
Iteration 113/1000 | Loss: 0.00014730
Iteration 114/1000 | Loss: 0.00013126
Iteration 115/1000 | Loss: 0.00105165
Iteration 116/1000 | Loss: 0.00047892
Iteration 117/1000 | Loss: 0.00016908
Iteration 118/1000 | Loss: 0.00026182
Iteration 119/1000 | Loss: 0.00007734
Iteration 120/1000 | Loss: 0.00044075
Iteration 121/1000 | Loss: 0.00067573
Iteration 122/1000 | Loss: 0.00051589
Iteration 123/1000 | Loss: 0.00033915
Iteration 124/1000 | Loss: 0.00026443
Iteration 125/1000 | Loss: 0.00028118
Iteration 126/1000 | Loss: 0.00032412
Iteration 127/1000 | Loss: 0.00053608
Iteration 128/1000 | Loss: 0.00027869
Iteration 129/1000 | Loss: 0.00031045
Iteration 130/1000 | Loss: 0.00011769
Iteration 131/1000 | Loss: 0.00012053
Iteration 132/1000 | Loss: 0.00019673
Iteration 133/1000 | Loss: 0.00005030
Iteration 134/1000 | Loss: 0.00007045
Iteration 135/1000 | Loss: 0.00020067
Iteration 136/1000 | Loss: 0.00024926
Iteration 137/1000 | Loss: 0.00041316
Iteration 138/1000 | Loss: 0.00034804
Iteration 139/1000 | Loss: 0.00007931
Iteration 140/1000 | Loss: 0.00018036
Iteration 141/1000 | Loss: 0.00046478
Iteration 142/1000 | Loss: 0.00009522
Iteration 143/1000 | Loss: 0.00009037
Iteration 144/1000 | Loss: 0.00029274
Iteration 145/1000 | Loss: 0.00045519
Iteration 146/1000 | Loss: 0.00030855
Iteration 147/1000 | Loss: 0.00029340
Iteration 148/1000 | Loss: 0.00028092
Iteration 149/1000 | Loss: 0.00067505
Iteration 150/1000 | Loss: 0.00027666
Iteration 151/1000 | Loss: 0.00063278
Iteration 152/1000 | Loss: 0.00120873
Iteration 153/1000 | Loss: 0.00021096
Iteration 154/1000 | Loss: 0.00020322
Iteration 155/1000 | Loss: 0.00073458
Iteration 156/1000 | Loss: 0.00114389
Iteration 157/1000 | Loss: 0.00056810
Iteration 158/1000 | Loss: 0.00037870
Iteration 159/1000 | Loss: 0.00053844
Iteration 160/1000 | Loss: 0.00061510
Iteration 161/1000 | Loss: 0.00013980
Iteration 162/1000 | Loss: 0.00017299
Iteration 163/1000 | Loss: 0.00010051
Iteration 164/1000 | Loss: 0.00005725
Iteration 165/1000 | Loss: 0.00006705
Iteration 166/1000 | Loss: 0.00032934
Iteration 167/1000 | Loss: 0.00031650
Iteration 168/1000 | Loss: 0.00107383
Iteration 169/1000 | Loss: 0.00037469
Iteration 170/1000 | Loss: 0.00025560
Iteration 171/1000 | Loss: 0.00034377
Iteration 172/1000 | Loss: 0.00026987
Iteration 173/1000 | Loss: 0.00012595
Iteration 174/1000 | Loss: 0.00019303
Iteration 175/1000 | Loss: 0.00016784
Iteration 176/1000 | Loss: 0.00009855
Iteration 177/1000 | Loss: 0.00015350
Iteration 178/1000 | Loss: 0.00014516
Iteration 179/1000 | Loss: 0.00007988
Iteration 180/1000 | Loss: 0.00003210
Iteration 181/1000 | Loss: 0.00022064
Iteration 182/1000 | Loss: 0.00028346
Iteration 183/1000 | Loss: 0.00017475
Iteration 184/1000 | Loss: 0.00021813
Iteration 185/1000 | Loss: 0.00037921
Iteration 186/1000 | Loss: 0.00023119
Iteration 187/1000 | Loss: 0.00021282
Iteration 188/1000 | Loss: 0.00017345
Iteration 189/1000 | Loss: 0.00029291
Iteration 190/1000 | Loss: 0.00026804
Iteration 191/1000 | Loss: 0.00010574
Iteration 192/1000 | Loss: 0.00016110
Iteration 193/1000 | Loss: 0.00018428
Iteration 194/1000 | Loss: 0.00003937
Iteration 195/1000 | Loss: 0.00004718
Iteration 196/1000 | Loss: 0.00003767
Iteration 197/1000 | Loss: 0.00023431
Iteration 198/1000 | Loss: 0.00065930
Iteration 199/1000 | Loss: 0.00058767
Iteration 200/1000 | Loss: 0.00068629
Iteration 201/1000 | Loss: 0.00028742
Iteration 202/1000 | Loss: 0.00059050
Iteration 203/1000 | Loss: 0.00030441
Iteration 204/1000 | Loss: 0.00012484
Iteration 205/1000 | Loss: 0.00019949
Iteration 206/1000 | Loss: 0.00044406
Iteration 207/1000 | Loss: 0.00061104
Iteration 208/1000 | Loss: 0.00027746
Iteration 209/1000 | Loss: 0.00022824
Iteration 210/1000 | Loss: 0.00009119
Iteration 211/1000 | Loss: 0.00019168
Iteration 212/1000 | Loss: 0.00051025
Iteration 213/1000 | Loss: 0.00035287
Iteration 214/1000 | Loss: 0.00022799
Iteration 215/1000 | Loss: 0.00025725
Iteration 216/1000 | Loss: 0.00025250
Iteration 217/1000 | Loss: 0.00024245
Iteration 218/1000 | Loss: 0.00023430
Iteration 219/1000 | Loss: 0.00020464
Iteration 220/1000 | Loss: 0.00023862
Iteration 221/1000 | Loss: 0.00022907
Iteration 222/1000 | Loss: 0.00004262
Iteration 223/1000 | Loss: 0.00002930
Iteration 224/1000 | Loss: 0.00008193
Iteration 225/1000 | Loss: 0.00024453
Iteration 226/1000 | Loss: 0.00003150
Iteration 227/1000 | Loss: 0.00002825
Iteration 228/1000 | Loss: 0.00037709
Iteration 229/1000 | Loss: 0.00023219
Iteration 230/1000 | Loss: 0.00005029
Iteration 231/1000 | Loss: 0.00015456
Iteration 232/1000 | Loss: 0.00002797
Iteration 233/1000 | Loss: 0.00002990
Iteration 234/1000 | Loss: 0.00002526
Iteration 235/1000 | Loss: 0.00008769
Iteration 236/1000 | Loss: 0.00022064
Iteration 237/1000 | Loss: 0.00036869
Iteration 238/1000 | Loss: 0.00006317
Iteration 239/1000 | Loss: 0.00003019
Iteration 240/1000 | Loss: 0.00005738
Iteration 241/1000 | Loss: 0.00009961
Iteration 242/1000 | Loss: 0.00004295
Iteration 243/1000 | Loss: 0.00002570
Iteration 244/1000 | Loss: 0.00005738
Iteration 245/1000 | Loss: 0.00012568
Iteration 246/1000 | Loss: 0.00010468
Iteration 247/1000 | Loss: 0.00002504
Iteration 248/1000 | Loss: 0.00014556
Iteration 249/1000 | Loss: 0.00007936
Iteration 250/1000 | Loss: 0.00026857
Iteration 251/1000 | Loss: 0.00008634
Iteration 252/1000 | Loss: 0.00018375
Iteration 253/1000 | Loss: 0.00028129
Iteration 254/1000 | Loss: 0.00036978
Iteration 255/1000 | Loss: 0.00022524
Iteration 256/1000 | Loss: 0.00007690
Iteration 257/1000 | Loss: 0.00072553
Iteration 258/1000 | Loss: 0.00012165
Iteration 259/1000 | Loss: 0.00003927
Iteration 260/1000 | Loss: 0.00038081
Iteration 261/1000 | Loss: 0.00062280
Iteration 262/1000 | Loss: 0.00082255
Iteration 263/1000 | Loss: 0.00042954
Iteration 264/1000 | Loss: 0.00046313
Iteration 265/1000 | Loss: 0.00038002
Iteration 266/1000 | Loss: 0.00029585
Iteration 267/1000 | Loss: 0.00040010
Iteration 268/1000 | Loss: 0.00044220
Iteration 269/1000 | Loss: 0.00046873
Iteration 270/1000 | Loss: 0.00030597
Iteration 271/1000 | Loss: 0.00026288
Iteration 272/1000 | Loss: 0.00015232
Iteration 273/1000 | Loss: 0.00032658
Iteration 274/1000 | Loss: 0.00016957
Iteration 275/1000 | Loss: 0.00003979
Iteration 276/1000 | Loss: 0.00016899
Iteration 277/1000 | Loss: 0.00011122
Iteration 278/1000 | Loss: 0.00010747
Iteration 279/1000 | Loss: 0.00028827
Iteration 280/1000 | Loss: 0.00020340
Iteration 281/1000 | Loss: 0.00060522
Iteration 282/1000 | Loss: 0.00029656
Iteration 283/1000 | Loss: 0.00011260
Iteration 284/1000 | Loss: 0.00010507
Iteration 285/1000 | Loss: 0.00009615
Iteration 286/1000 | Loss: 0.00009770
Iteration 287/1000 | Loss: 0.00006907
Iteration 288/1000 | Loss: 0.00007918
Iteration 289/1000 | Loss: 0.00005785
Iteration 290/1000 | Loss: 0.00008908
Iteration 291/1000 | Loss: 0.00005525
Iteration 292/1000 | Loss: 0.00012646
Iteration 293/1000 | Loss: 0.00005036
Iteration 294/1000 | Loss: 0.00070018
Iteration 295/1000 | Loss: 0.00033884
Iteration 296/1000 | Loss: 0.00005198
Iteration 297/1000 | Loss: 0.00023018
Iteration 298/1000 | Loss: 0.00041267
Iteration 299/1000 | Loss: 0.00009801
Iteration 300/1000 | Loss: 0.00005885
Iteration 301/1000 | Loss: 0.00055949
Iteration 302/1000 | Loss: 0.00029329
Iteration 303/1000 | Loss: 0.00024182
Iteration 304/1000 | Loss: 0.00028937
Iteration 305/1000 | Loss: 0.00025210
Iteration 306/1000 | Loss: 0.00011351
Iteration 307/1000 | Loss: 0.00011619
Iteration 308/1000 | Loss: 0.00028101
Iteration 309/1000 | Loss: 0.00029310
Iteration 310/1000 | Loss: 0.00011155
Iteration 311/1000 | Loss: 0.00042920
Iteration 312/1000 | Loss: 0.00025278
Iteration 313/1000 | Loss: 0.00017758
Iteration 314/1000 | Loss: 0.00006973
Iteration 315/1000 | Loss: 0.00013405
Iteration 316/1000 | Loss: 0.00010199
Iteration 317/1000 | Loss: 0.00016332
Iteration 318/1000 | Loss: 0.00035583
Iteration 319/1000 | Loss: 0.00017699
Iteration 320/1000 | Loss: 0.00006613
Iteration 321/1000 | Loss: 0.00020087
Iteration 322/1000 | Loss: 0.00025559
Iteration 323/1000 | Loss: 0.00008429
Iteration 324/1000 | Loss: 0.00027085
Iteration 325/1000 | Loss: 0.00022800
Iteration 326/1000 | Loss: 0.00023136
Iteration 327/1000 | Loss: 0.00016638
Iteration 328/1000 | Loss: 0.00028090
Iteration 329/1000 | Loss: 0.00055118
Iteration 330/1000 | Loss: 0.00029382
Iteration 331/1000 | Loss: 0.00033585
Iteration 332/1000 | Loss: 0.00017015
Iteration 333/1000 | Loss: 0.00025410
Iteration 334/1000 | Loss: 0.00034210
Iteration 335/1000 | Loss: 0.00045600
Iteration 336/1000 | Loss: 0.00034397
Iteration 337/1000 | Loss: 0.00054401
Iteration 338/1000 | Loss: 0.00063254
Iteration 339/1000 | Loss: 0.00041197
Iteration 340/1000 | Loss: 0.00035001
Iteration 341/1000 | Loss: 0.00030864
Iteration 342/1000 | Loss: 0.00026219
Iteration 343/1000 | Loss: 0.00052101
Iteration 344/1000 | Loss: 0.00058389
Iteration 345/1000 | Loss: 0.00064348
Iteration 346/1000 | Loss: 0.00046655
Iteration 347/1000 | Loss: 0.00039309
Iteration 348/1000 | Loss: 0.00037387
Iteration 349/1000 | Loss: 0.00027850
Iteration 350/1000 | Loss: 0.00067006
Iteration 351/1000 | Loss: 0.00037135
Iteration 352/1000 | Loss: 0.00013410
Iteration 353/1000 | Loss: 0.00025266
Iteration 354/1000 | Loss: 0.00029614
Iteration 355/1000 | Loss: 0.00028857
Iteration 356/1000 | Loss: 0.00018469
Iteration 357/1000 | Loss: 0.00020753
Iteration 358/1000 | Loss: 0.00010436
Iteration 359/1000 | Loss: 0.00008206
Iteration 360/1000 | Loss: 0.00026736
Iteration 361/1000 | Loss: 0.00005582
Iteration 362/1000 | Loss: 0.00011243
Iteration 363/1000 | Loss: 0.00049665
Iteration 364/1000 | Loss: 0.00011839
Iteration 365/1000 | Loss: 0.00008237
Iteration 366/1000 | Loss: 0.00019142
Iteration 367/1000 | Loss: 0.00015721
Iteration 368/1000 | Loss: 0.00016692
Iteration 369/1000 | Loss: 0.00032844
Iteration 370/1000 | Loss: 0.00014464
Iteration 371/1000 | Loss: 0.00014821
Iteration 372/1000 | Loss: 0.00018574
Iteration 373/1000 | Loss: 0.00014857
Iteration 374/1000 | Loss: 0.00027500
Iteration 375/1000 | Loss: 0.00008918
Iteration 376/1000 | Loss: 0.00013435
Iteration 377/1000 | Loss: 0.00017316
Iteration 378/1000 | Loss: 0.00011329
Iteration 379/1000 | Loss: 0.00017100
Iteration 380/1000 | Loss: 0.00011368
Iteration 381/1000 | Loss: 0.00013674
Iteration 382/1000 | Loss: 0.00020894
Iteration 383/1000 | Loss: 0.00013238
Iteration 384/1000 | Loss: 0.00015360
Iteration 385/1000 | Loss: 0.00010941
Iteration 386/1000 | Loss: 0.00012345
Iteration 387/1000 | Loss: 0.00015353
Iteration 388/1000 | Loss: 0.00005125
Iteration 389/1000 | Loss: 0.00005418
Iteration 390/1000 | Loss: 0.00005919
Iteration 391/1000 | Loss: 0.00029415
Iteration 392/1000 | Loss: 0.00022167
Iteration 393/1000 | Loss: 0.00015940
Iteration 394/1000 | Loss: 0.00007212
Iteration 395/1000 | Loss: 0.00009539
Iteration 396/1000 | Loss: 0.00002927
Iteration 397/1000 | Loss: 0.00019132
Iteration 398/1000 | Loss: 0.00043867
Iteration 399/1000 | Loss: 0.00030129
Iteration 400/1000 | Loss: 0.00012339
Iteration 401/1000 | Loss: 0.00042927
Iteration 402/1000 | Loss: 0.00048352
Iteration 403/1000 | Loss: 0.00052372
Iteration 404/1000 | Loss: 0.00009834
Iteration 405/1000 | Loss: 0.00013194
Iteration 406/1000 | Loss: 0.00012555
Iteration 407/1000 | Loss: 0.00011627
Iteration 408/1000 | Loss: 0.00014247
Iteration 409/1000 | Loss: 0.00012369
Iteration 410/1000 | Loss: 0.00074880
Iteration 411/1000 | Loss: 0.00009159
Iteration 412/1000 | Loss: 0.00017702
Iteration 413/1000 | Loss: 0.00019634
Iteration 414/1000 | Loss: 0.00016955
Iteration 415/1000 | Loss: 0.00014965
Iteration 416/1000 | Loss: 0.00002931
Iteration 417/1000 | Loss: 0.00002785
Iteration 418/1000 | Loss: 0.00020861
Iteration 419/1000 | Loss: 0.00019303
Iteration 420/1000 | Loss: 0.00017611
Iteration 421/1000 | Loss: 0.00020523
Iteration 422/1000 | Loss: 0.00012075
Iteration 423/1000 | Loss: 0.00022839
Iteration 424/1000 | Loss: 0.00031940
Iteration 425/1000 | Loss: 0.00027513
Iteration 426/1000 | Loss: 0.00026676
Iteration 427/1000 | Loss: 0.00044647
Iteration 428/1000 | Loss: 0.00014547
Iteration 429/1000 | Loss: 0.00030900
Iteration 430/1000 | Loss: 0.00014382
Iteration 431/1000 | Loss: 0.00011985
Iteration 432/1000 | Loss: 0.00012756
Iteration 433/1000 | Loss: 0.00024802
Iteration 434/1000 | Loss: 0.00016507
Iteration 435/1000 | Loss: 0.00030164
Iteration 436/1000 | Loss: 0.00019883
Iteration 437/1000 | Loss: 0.00018119
Iteration 438/1000 | Loss: 0.00027605
Iteration 439/1000 | Loss: 0.00017809
Iteration 440/1000 | Loss: 0.00008965
Iteration 441/1000 | Loss: 0.00026998
Iteration 442/1000 | Loss: 0.00090115
Iteration 443/1000 | Loss: 0.00041608
Iteration 444/1000 | Loss: 0.00025874
Iteration 445/1000 | Loss: 0.00040310
Iteration 446/1000 | Loss: 0.00016167
Iteration 447/1000 | Loss: 0.00020426
Iteration 448/1000 | Loss: 0.00026101
Iteration 449/1000 | Loss: 0.00029670
Iteration 450/1000 | Loss: 0.00039656
Iteration 451/1000 | Loss: 0.00022562
Iteration 452/1000 | Loss: 0.00025044
Iteration 453/1000 | Loss: 0.00025017
Iteration 454/1000 | Loss: 0.00036206
Iteration 455/1000 | Loss: 0.00024198
Iteration 456/1000 | Loss: 0.00040722
Iteration 457/1000 | Loss: 0.00034583
Iteration 458/1000 | Loss: 0.00026688
Iteration 459/1000 | Loss: 0.00030584
Iteration 460/1000 | Loss: 0.00033826
Iteration 461/1000 | Loss: 0.00034059
Iteration 462/1000 | Loss: 0.00038733
Iteration 463/1000 | Loss: 0.00032013
Iteration 464/1000 | Loss: 0.00020441
Iteration 465/1000 | Loss: 0.00027399
Iteration 466/1000 | Loss: 0.00019031
Iteration 467/1000 | Loss: 0.00016564
Iteration 468/1000 | Loss: 0.00040355
Iteration 469/1000 | Loss: 0.00033916
Iteration 470/1000 | Loss: 0.00048995
Iteration 471/1000 | Loss: 0.00054883
Iteration 472/1000 | Loss: 0.00032298
Iteration 473/1000 | Loss: 0.00019202
Iteration 474/1000 | Loss: 0.00008195
Iteration 475/1000 | Loss: 0.00005930
Iteration 476/1000 | Loss: 0.00013087
Iteration 477/1000 | Loss: 0.00015854
Iteration 478/1000 | Loss: 0.00025066
Iteration 479/1000 | Loss: 0.00017956
Iteration 480/1000 | Loss: 0.00018410
Iteration 481/1000 | Loss: 0.00021350
Iteration 482/1000 | Loss: 0.00033988
Iteration 483/1000 | Loss: 0.00032231
Iteration 484/1000 | Loss: 0.00035257
Iteration 485/1000 | Loss: 0.00028896
Iteration 486/1000 | Loss: 0.00038912
Iteration 487/1000 | Loss: 0.00034218
Iteration 488/1000 | Loss: 0.00017773
Iteration 489/1000 | Loss: 0.00038824
Iteration 490/1000 | Loss: 0.00065890
Iteration 491/1000 | Loss: 0.00083205
Iteration 492/1000 | Loss: 0.00063402
Iteration 493/1000 | Loss: 0.00073810
Iteration 494/1000 | Loss: 0.00112894
Iteration 495/1000 | Loss: 0.00025864
Iteration 496/1000 | Loss: 0.00020069
Iteration 497/1000 | Loss: 0.00037802
Iteration 498/1000 | Loss: 0.00010161
Iteration 499/1000 | Loss: 0.00048801
Iteration 500/1000 | Loss: 0.00021633
Iteration 501/1000 | Loss: 0.00026582
Iteration 502/1000 | Loss: 0.00021441
Iteration 503/1000 | Loss: 0.00028102
Iteration 504/1000 | Loss: 0.00020914
Iteration 505/1000 | Loss: 0.00020303
Iteration 506/1000 | Loss: 0.00021895
Iteration 507/1000 | Loss: 0.00024445
Iteration 508/1000 | Loss: 0.00019416
Iteration 509/1000 | Loss: 0.00014149
Iteration 510/1000 | Loss: 0.00024011
Iteration 511/1000 | Loss: 0.00019092
Iteration 512/1000 | Loss: 0.00011401
Iteration 513/1000 | Loss: 0.00023526
Iteration 514/1000 | Loss: 0.00043792
Iteration 515/1000 | Loss: 0.00009927
Iteration 516/1000 | Loss: 0.00017843
Iteration 517/1000 | Loss: 0.00013183
Iteration 518/1000 | Loss: 0.00010880
Iteration 519/1000 | Loss: 0.00022009
Iteration 520/1000 | Loss: 0.00024763
Iteration 521/1000 | Loss: 0.00021265
Iteration 522/1000 | Loss: 0.00022372
Iteration 523/1000 | Loss: 0.00020579
Iteration 524/1000 | Loss: 0.00019859
Iteration 525/1000 | Loss: 0.00022002
Iteration 526/1000 | Loss: 0.00029589
Iteration 527/1000 | Loss: 0.00019487
Iteration 528/1000 | Loss: 0.00011765
Iteration 529/1000 | Loss: 0.00015428
Iteration 530/1000 | Loss: 0.00034150
Iteration 531/1000 | Loss: 0.00019744
Iteration 532/1000 | Loss: 0.00058302
Iteration 533/1000 | Loss: 0.00031823
Iteration 534/1000 | Loss: 0.00043801
Iteration 535/1000 | Loss: 0.00028798
Iteration 536/1000 | Loss: 0.00033370
Iteration 537/1000 | Loss: 0.00055260
Iteration 538/1000 | Loss: 0.00027213
Iteration 539/1000 | Loss: 0.00021434
Iteration 540/1000 | Loss: 0.00039220
Iteration 541/1000 | Loss: 0.00035133
Iteration 542/1000 | Loss: 0.00016678
Iteration 543/1000 | Loss: 0.00010735
Iteration 544/1000 | Loss: 0.00043817
Iteration 545/1000 | Loss: 0.00065168
Iteration 546/1000 | Loss: 0.00099648
Iteration 547/1000 | Loss: 0.00032725
Iteration 548/1000 | Loss: 0.00013383
Iteration 549/1000 | Loss: 0.00006061
Iteration 550/1000 | Loss: 0.00003436
Iteration 551/1000 | Loss: 0.00004378
Iteration 552/1000 | Loss: 0.00036003
Iteration 553/1000 | Loss: 0.00015497
Iteration 554/1000 | Loss: 0.00014587
Iteration 555/1000 | Loss: 0.00004029
Iteration 556/1000 | Loss: 0.00010905
Iteration 557/1000 | Loss: 0.00016025
Iteration 558/1000 | Loss: 0.00016000
Iteration 559/1000 | Loss: 0.00010588
Iteration 560/1000 | Loss: 0.00004857
Iteration 561/1000 | Loss: 0.00006152
Iteration 562/1000 | Loss: 0.00004761
Iteration 563/1000 | Loss: 0.00002635
Iteration 564/1000 | Loss: 0.00003383
Iteration 565/1000 | Loss: 0.00013712
Iteration 566/1000 | Loss: 0.00020558
Iteration 567/1000 | Loss: 0.00042068
Iteration 568/1000 | Loss: 0.00031754
Iteration 569/1000 | Loss: 0.00014948
Iteration 570/1000 | Loss: 0.00006389
Iteration 571/1000 | Loss: 0.00015221
Iteration 572/1000 | Loss: 0.00004069
Iteration 573/1000 | Loss: 0.00003032
Iteration 574/1000 | Loss: 0.00008151
Iteration 575/1000 | Loss: 0.00016628
Iteration 576/1000 | Loss: 0.00014683
Iteration 577/1000 | Loss: 0.00014278
Iteration 578/1000 | Loss: 0.00021109
Iteration 579/1000 | Loss: 0.00012587
Iteration 580/1000 | Loss: 0.00018812
Iteration 581/1000 | Loss: 0.00013316
Iteration 582/1000 | Loss: 0.00013748
Iteration 583/1000 | Loss: 0.00052074
Iteration 584/1000 | Loss: 0.00019749
Iteration 585/1000 | Loss: 0.00011382
Iteration 586/1000 | Loss: 0.00013768
Iteration 587/1000 | Loss: 0.00012774
Iteration 588/1000 | Loss: 0.00011901
Iteration 589/1000 | Loss: 0.00027588
Iteration 590/1000 | Loss: 0.00020818
Iteration 591/1000 | Loss: 0.00019628
Iteration 592/1000 | Loss: 0.00014113
Iteration 593/1000 | Loss: 0.00008935
Iteration 594/1000 | Loss: 0.00003074
Iteration 595/1000 | Loss: 0.00010548
Iteration 596/1000 | Loss: 0.00004763
Iteration 597/1000 | Loss: 0.00021911
Iteration 598/1000 | Loss: 0.00029129
Iteration 599/1000 | Loss: 0.00008234
Iteration 600/1000 | Loss: 0.00046852
Iteration 601/1000 | Loss: 0.00010854
Iteration 602/1000 | Loss: 0.00016385
Iteration 603/1000 | Loss: 0.00015168
Iteration 604/1000 | Loss: 0.00007234
Iteration 605/1000 | Loss: 0.00006386
Iteration 606/1000 | Loss: 0.00013812
Iteration 607/1000 | Loss: 0.00009317
Iteration 608/1000 | Loss: 0.00004817
Iteration 609/1000 | Loss: 0.00003199
Iteration 610/1000 | Loss: 0.00014899
Iteration 611/1000 | Loss: 0.00009425
Iteration 612/1000 | Loss: 0.00011270
Iteration 613/1000 | Loss: 0.00009593
Iteration 614/1000 | Loss: 0.00010559
Iteration 615/1000 | Loss: 0.00005511
Iteration 616/1000 | Loss: 0.00009541
Iteration 617/1000 | Loss: 0.00011124
Iteration 618/1000 | Loss: 0.00018962
Iteration 619/1000 | Loss: 0.00014105
Iteration 620/1000 | Loss: 0.00044868
Iteration 621/1000 | Loss: 0.00013526
Iteration 622/1000 | Loss: 0.00006814
Iteration 623/1000 | Loss: 0.00039768
Iteration 624/1000 | Loss: 0.00026307
Iteration 625/1000 | Loss: 0.00013444
Iteration 626/1000 | Loss: 0.00010526
Iteration 627/1000 | Loss: 0.00026413
Iteration 628/1000 | Loss: 0.00006760
Iteration 629/1000 | Loss: 0.00006963
Iteration 630/1000 | Loss: 0.00004641
Iteration 631/1000 | Loss: 0.00007857
Iteration 632/1000 | Loss: 0.00005068
Iteration 633/1000 | Loss: 0.00002647
Iteration 634/1000 | Loss: 0.00006331
Iteration 635/1000 | Loss: 0.00002468
Iteration 636/1000 | Loss: 0.00002425
Iteration 637/1000 | Loss: 0.00038380
Iteration 638/1000 | Loss: 0.00073162
Iteration 639/1000 | Loss: 0.00091983
Iteration 640/1000 | Loss: 0.00050366
Iteration 641/1000 | Loss: 0.00019533
Iteration 642/1000 | Loss: 0.00007009
Iteration 643/1000 | Loss: 0.00009826
Iteration 644/1000 | Loss: 0.00002543
Iteration 645/1000 | Loss: 0.00042314
Iteration 646/1000 | Loss: 0.00045422
Iteration 647/1000 | Loss: 0.00070072
Iteration 648/1000 | Loss: 0.00017596
Iteration 649/1000 | Loss: 0.00009790
Iteration 650/1000 | Loss: 0.00002670
Iteration 651/1000 | Loss: 0.00023591
Iteration 652/1000 | Loss: 0.00053959
Iteration 653/1000 | Loss: 0.00039232
Iteration 654/1000 | Loss: 0.00037082
Iteration 655/1000 | Loss: 0.00036818
Iteration 656/1000 | Loss: 0.00018114
Iteration 657/1000 | Loss: 0.00013765
Iteration 658/1000 | Loss: 0.00013168
Iteration 659/1000 | Loss: 0.00019808
Iteration 660/1000 | Loss: 0.00038912
Iteration 661/1000 | Loss: 0.00023966
Iteration 662/1000 | Loss: 0.00017185
Iteration 663/1000 | Loss: 0.00019496
Iteration 664/1000 | Loss: 0.00007688
Iteration 665/1000 | Loss: 0.00006727
Iteration 666/1000 | Loss: 0.00006023
Iteration 667/1000 | Loss: 0.00017196
Iteration 668/1000 | Loss: 0.00005792
Iteration 669/1000 | Loss: 0.00002717
Iteration 670/1000 | Loss: 0.00004250
Iteration 671/1000 | Loss: 0.00002533
Iteration 672/1000 | Loss: 0.00013829
Iteration 673/1000 | Loss: 0.00007524
Iteration 674/1000 | Loss: 0.00007837
Iteration 675/1000 | Loss: 0.00003362
Iteration 676/1000 | Loss: 0.00003825
Iteration 677/1000 | Loss: 0.00013491
Iteration 678/1000 | Loss: 0.00008550
Iteration 679/1000 | Loss: 0.00006239
Iteration 680/1000 | Loss: 0.00017413
Iteration 681/1000 | Loss: 0.00004211
Iteration 682/1000 | Loss: 0.00013937
Iteration 683/1000 | Loss: 0.00003059
Iteration 684/1000 | Loss: 0.00013399
Iteration 685/1000 | Loss: 0.00003852
Iteration 686/1000 | Loss: 0.00013621
Iteration 687/1000 | Loss: 0.00043953
Iteration 688/1000 | Loss: 0.00013981
Iteration 689/1000 | Loss: 0.00003125
Iteration 690/1000 | Loss: 0.00013891
Iteration 691/1000 | Loss: 0.00010180
Iteration 692/1000 | Loss: 0.00013175
Iteration 693/1000 | Loss: 0.00007446
Iteration 694/1000 | Loss: 0.00015948
Iteration 695/1000 | Loss: 0.00002708
Iteration 696/1000 | Loss: 0.00008042
Iteration 697/1000 | Loss: 0.00002656
Iteration 698/1000 | Loss: 0.00006487
Iteration 699/1000 | Loss: 0.00019333
Iteration 700/1000 | Loss: 0.00041645
Iteration 701/1000 | Loss: 0.00016818
Iteration 702/1000 | Loss: 0.00016234
Iteration 703/1000 | Loss: 0.00010777
Iteration 704/1000 | Loss: 0.00030230
Iteration 705/1000 | Loss: 0.00028594
Iteration 706/1000 | Loss: 0.00007670
Iteration 707/1000 | Loss: 0.00031696
Iteration 708/1000 | Loss: 0.00044895
Iteration 709/1000 | Loss: 0.00028072
Iteration 710/1000 | Loss: 0.00025394
Iteration 711/1000 | Loss: 0.00054089
Iteration 712/1000 | Loss: 0.00044710
Iteration 713/1000 | Loss: 0.00026067
Iteration 714/1000 | Loss: 0.00031977
Iteration 715/1000 | Loss: 0.00022409
Iteration 716/1000 | Loss: 0.00019466
Iteration 717/1000 | Loss: 0.00021753
Iteration 718/1000 | Loss: 0.00021274
Iteration 719/1000 | Loss: 0.00022100
Iteration 720/1000 | Loss: 0.00015406
Iteration 721/1000 | Loss: 0.00019149
Iteration 722/1000 | Loss: 0.00012754
Iteration 723/1000 | Loss: 0.00065923
Iteration 724/1000 | Loss: 0.00002663
Iteration 725/1000 | Loss: 0.00002596
Iteration 726/1000 | Loss: 0.00003035
Iteration 727/1000 | Loss: 0.00002169
Iteration 728/1000 | Loss: 0.00002096
Iteration 729/1000 | Loss: 0.00003246
Iteration 730/1000 | Loss: 0.00010080
Iteration 731/1000 | Loss: 0.00002756
Iteration 732/1000 | Loss: 0.00010888
Iteration 733/1000 | Loss: 0.00010266
Iteration 734/1000 | Loss: 0.00002309
Iteration 735/1000 | Loss: 0.00021613
Iteration 736/1000 | Loss: 0.00034896
Iteration 737/1000 | Loss: 0.00007973
Iteration 738/1000 | Loss: 0.00021380
Iteration 739/1000 | Loss: 0.00030467
Iteration 740/1000 | Loss: 0.00029847
Iteration 741/1000 | Loss: 0.00068369
Iteration 742/1000 | Loss: 0.00016911
Iteration 743/1000 | Loss: 0.00058780
Iteration 744/1000 | Loss: 0.00022279
Iteration 745/1000 | Loss: 0.00047826
Iteration 746/1000 | Loss: 0.00007690
Iteration 747/1000 | Loss: 0.00016500
Iteration 748/1000 | Loss: 0.00045608
Iteration 749/1000 | Loss: 0.00027962
Iteration 750/1000 | Loss: 0.00032272
Iteration 751/1000 | Loss: 0.00011537
Iteration 752/1000 | Loss: 0.00004151
Iteration 753/1000 | Loss: 0.00015925
Iteration 754/1000 | Loss: 0.00038842
Iteration 755/1000 | Loss: 0.00016779
Iteration 756/1000 | Loss: 0.00010825
Iteration 757/1000 | Loss: 0.00010014
Iteration 758/1000 | Loss: 0.00006918
Iteration 759/1000 | Loss: 0.00007247
Iteration 760/1000 | Loss: 0.00002270
Iteration 761/1000 | Loss: 0.00010216
Iteration 762/1000 | Loss: 0.00005276
Iteration 763/1000 | Loss: 0.00005877
Iteration 764/1000 | Loss: 0.00007026
Iteration 765/1000 | Loss: 0.00006923
Iteration 766/1000 | Loss: 0.00002084
Iteration 767/1000 | Loss: 0.00006839
Iteration 768/1000 | Loss: 0.00008440
Iteration 769/1000 | Loss: 0.00002091
Iteration 770/1000 | Loss: 0.00002381
Iteration 771/1000 | Loss: 0.00009943
Iteration 772/1000 | Loss: 0.00007139
Iteration 773/1000 | Loss: 0.00009947
Iteration 774/1000 | Loss: 0.00008039
Iteration 775/1000 | Loss: 0.00015824
Iteration 776/1000 | Loss: 0.00011904
Iteration 777/1000 | Loss: 0.00017721
Iteration 778/1000 | Loss: 0.00029001
Iteration 779/1000 | Loss: 0.00019682
Iteration 780/1000 | Loss: 0.00024637
Iteration 781/1000 | Loss: 0.00021208
Iteration 782/1000 | Loss: 0.00018449
Iteration 783/1000 | Loss: 0.00019360
Iteration 784/1000 | Loss: 0.00020512
Iteration 785/1000 | Loss: 0.00001905
Iteration 786/1000 | Loss: 0.00001754
Iteration 787/1000 | Loss: 0.00001694
Iteration 788/1000 | Loss: 0.00001665
Iteration 789/1000 | Loss: 0.00001625
Iteration 790/1000 | Loss: 0.00001606
Iteration 791/1000 | Loss: 0.00001599
Iteration 792/1000 | Loss: 0.00001599
Iteration 793/1000 | Loss: 0.00001599
Iteration 794/1000 | Loss: 0.00001599
Iteration 795/1000 | Loss: 0.00001599
Iteration 796/1000 | Loss: 0.00001599
Iteration 797/1000 | Loss: 0.00001599
Iteration 798/1000 | Loss: 0.00001599
Iteration 799/1000 | Loss: 0.00001599
Iteration 800/1000 | Loss: 0.00001599
Iteration 801/1000 | Loss: 0.00001599
Iteration 802/1000 | Loss: 0.00001599
Iteration 803/1000 | Loss: 0.00001599
Iteration 804/1000 | Loss: 0.00001598
Iteration 805/1000 | Loss: 0.00001598
Iteration 806/1000 | Loss: 0.00001597
Iteration 807/1000 | Loss: 0.00001597
Iteration 808/1000 | Loss: 0.00001597
Iteration 809/1000 | Loss: 0.00001597
Iteration 810/1000 | Loss: 0.00001596
Iteration 811/1000 | Loss: 0.00001596
Iteration 812/1000 | Loss: 0.00001596
Iteration 813/1000 | Loss: 0.00001596
Iteration 814/1000 | Loss: 0.00001595
Iteration 815/1000 | Loss: 0.00001595
Iteration 816/1000 | Loss: 0.00001595
Iteration 817/1000 | Loss: 0.00001595
Iteration 818/1000 | Loss: 0.00001595
Iteration 819/1000 | Loss: 0.00001594
Iteration 820/1000 | Loss: 0.00001592
Iteration 821/1000 | Loss: 0.00001592
Iteration 822/1000 | Loss: 0.00001590
Iteration 823/1000 | Loss: 0.00001590
Iteration 824/1000 | Loss: 0.00001589
Iteration 825/1000 | Loss: 0.00001589
Iteration 826/1000 | Loss: 0.00001589
Iteration 827/1000 | Loss: 0.00001589
Iteration 828/1000 | Loss: 0.00001589
Iteration 829/1000 | Loss: 0.00001589
Iteration 830/1000 | Loss: 0.00001589
Iteration 831/1000 | Loss: 0.00001589
Iteration 832/1000 | Loss: 0.00001588
Iteration 833/1000 | Loss: 0.00001588
Iteration 834/1000 | Loss: 0.00001588
Iteration 835/1000 | Loss: 0.00001588
Iteration 836/1000 | Loss: 0.00001588
Iteration 837/1000 | Loss: 0.00001588
Iteration 838/1000 | Loss: 0.00001588
Iteration 839/1000 | Loss: 0.00001587
Iteration 840/1000 | Loss: 0.00001587
Iteration 841/1000 | Loss: 0.00001587
Iteration 842/1000 | Loss: 0.00001587
Iteration 843/1000 | Loss: 0.00001587
Iteration 844/1000 | Loss: 0.00001587
Iteration 845/1000 | Loss: 0.00001586
Iteration 846/1000 | Loss: 0.00001586
Iteration 847/1000 | Loss: 0.00001586
Iteration 848/1000 | Loss: 0.00001586
Iteration 849/1000 | Loss: 0.00001586
Iteration 850/1000 | Loss: 0.00001586
Iteration 851/1000 | Loss: 0.00001586
Iteration 852/1000 | Loss: 0.00001585
Iteration 853/1000 | Loss: 0.00001585
Iteration 854/1000 | Loss: 0.00001585
Iteration 855/1000 | Loss: 0.00001585
Iteration 856/1000 | Loss: 0.00001585
Iteration 857/1000 | Loss: 0.00001585
Iteration 858/1000 | Loss: 0.00001585
Iteration 859/1000 | Loss: 0.00001585
Iteration 860/1000 | Loss: 0.00001584
Iteration 861/1000 | Loss: 0.00001583
Iteration 862/1000 | Loss: 0.00001583
Iteration 863/1000 | Loss: 0.00001582
Iteration 864/1000 | Loss: 0.00001582
Iteration 865/1000 | Loss: 0.00001582
Iteration 866/1000 | Loss: 0.00001582
Iteration 867/1000 | Loss: 0.00001582
Iteration 868/1000 | Loss: 0.00001581
Iteration 869/1000 | Loss: 0.00001581
Iteration 870/1000 | Loss: 0.00001581
Iteration 871/1000 | Loss: 0.00001581
Iteration 872/1000 | Loss: 0.00001581
Iteration 873/1000 | Loss: 0.00001580
Iteration 874/1000 | Loss: 0.00001580
Iteration 875/1000 | Loss: 0.00001580
Iteration 876/1000 | Loss: 0.00001580
Iteration 877/1000 | Loss: 0.00001580
Iteration 878/1000 | Loss: 0.00001579
Iteration 879/1000 | Loss: 0.00001579
Iteration 880/1000 | Loss: 0.00001579
Iteration 881/1000 | Loss: 0.00001579
Iteration 882/1000 | Loss: 0.00001578
Iteration 883/1000 | Loss: 0.00001578
Iteration 884/1000 | Loss: 0.00001578
Iteration 885/1000 | Loss: 0.00001578
Iteration 886/1000 | Loss: 0.00001578
Iteration 887/1000 | Loss: 0.00001578
Iteration 888/1000 | Loss: 0.00001578
Iteration 889/1000 | Loss: 0.00001578
Iteration 890/1000 | Loss: 0.00001578
Iteration 891/1000 | Loss: 0.00001577
Iteration 892/1000 | Loss: 0.00001577
Iteration 893/1000 | Loss: 0.00001577
Iteration 894/1000 | Loss: 0.00001577
Iteration 895/1000 | Loss: 0.00001577
Iteration 896/1000 | Loss: 0.00001577
Iteration 897/1000 | Loss: 0.00001577
Iteration 898/1000 | Loss: 0.00001577
Iteration 899/1000 | Loss: 0.00001577
Iteration 900/1000 | Loss: 0.00001577
Iteration 901/1000 | Loss: 0.00001577
Iteration 902/1000 | Loss: 0.00001577
Iteration 903/1000 | Loss: 0.00001577
Iteration 904/1000 | Loss: 0.00001577
Iteration 905/1000 | Loss: 0.00001577
Iteration 906/1000 | Loss: 0.00001577
Iteration 907/1000 | Loss: 0.00001577
Iteration 908/1000 | Loss: 0.00001577
Iteration 909/1000 | Loss: 0.00001577
Iteration 910/1000 | Loss: 0.00001576
Iteration 911/1000 | Loss: 0.00001576
Iteration 912/1000 | Loss: 0.00001576
Iteration 913/1000 | Loss: 0.00001576
Iteration 914/1000 | Loss: 0.00001576
Iteration 915/1000 | Loss: 0.00001576
Iteration 916/1000 | Loss: 0.00001576
Iteration 917/1000 | Loss: 0.00001576
Iteration 918/1000 | Loss: 0.00001576
Iteration 919/1000 | Loss: 0.00001576
Iteration 920/1000 | Loss: 0.00001576
Iteration 921/1000 | Loss: 0.00001576
Iteration 922/1000 | Loss: 0.00001576
Iteration 923/1000 | Loss: 0.00001576
Iteration 924/1000 | Loss: 0.00001576
Iteration 925/1000 | Loss: 0.00001576
Iteration 926/1000 | Loss: 0.00001576
Iteration 927/1000 | Loss: 0.00001576
Iteration 928/1000 | Loss: 0.00001576
Iteration 929/1000 | Loss: 0.00001576
Iteration 930/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 930. Stopping optimization.
Last 5 losses: [1.575727765157353e-05, 1.575727765157353e-05, 1.575727765157353e-05, 1.575727765157353e-05, 1.575727765157353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.575727765157353e-05

Optimization complete. Final v2v error: 3.2858898639678955 mm

Highest mean error: 7.90285587310791 mm for frame 69

Lowest mean error: 2.6481130123138428 mm for frame 132

Saving results

Total time: 1347.2881321907043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818007
Iteration 2/25 | Loss: 0.00105681
Iteration 3/25 | Loss: 0.00080135
Iteration 4/25 | Loss: 0.00076194
Iteration 5/25 | Loss: 0.00075132
Iteration 6/25 | Loss: 0.00074994
Iteration 7/25 | Loss: 0.00074994
Iteration 8/25 | Loss: 0.00074994
Iteration 9/25 | Loss: 0.00074994
Iteration 10/25 | Loss: 0.00074994
Iteration 11/25 | Loss: 0.00074994
Iteration 12/25 | Loss: 0.00074994
Iteration 13/25 | Loss: 0.00074994
Iteration 14/25 | Loss: 0.00074994
Iteration 15/25 | Loss: 0.00074994
Iteration 16/25 | Loss: 0.00074994
Iteration 17/25 | Loss: 0.00074994
Iteration 18/25 | Loss: 0.00074994
Iteration 19/25 | Loss: 0.00074994
Iteration 20/25 | Loss: 0.00074994
Iteration 21/25 | Loss: 0.00074994
Iteration 22/25 | Loss: 0.00074994
Iteration 23/25 | Loss: 0.00074994
Iteration 24/25 | Loss: 0.00074994
Iteration 25/25 | Loss: 0.00074994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89776897
Iteration 2/25 | Loss: 0.00057477
Iteration 3/25 | Loss: 0.00057470
Iteration 4/25 | Loss: 0.00057470
Iteration 5/25 | Loss: 0.00057470
Iteration 6/25 | Loss: 0.00057470
Iteration 7/25 | Loss: 0.00057470
Iteration 8/25 | Loss: 0.00057470
Iteration 9/25 | Loss: 0.00057470
Iteration 10/25 | Loss: 0.00057470
Iteration 11/25 | Loss: 0.00057470
Iteration 12/25 | Loss: 0.00057470
Iteration 13/25 | Loss: 0.00057470
Iteration 14/25 | Loss: 0.00057470
Iteration 15/25 | Loss: 0.00057470
Iteration 16/25 | Loss: 0.00057470
Iteration 17/25 | Loss: 0.00057470
Iteration 18/25 | Loss: 0.00057470
Iteration 19/25 | Loss: 0.00057470
Iteration 20/25 | Loss: 0.00057470
Iteration 21/25 | Loss: 0.00057470
Iteration 22/25 | Loss: 0.00057470
Iteration 23/25 | Loss: 0.00057470
Iteration 24/25 | Loss: 0.00057470
Iteration 25/25 | Loss: 0.00057470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057470
Iteration 2/1000 | Loss: 0.00004045
Iteration 3/1000 | Loss: 0.00002738
Iteration 4/1000 | Loss: 0.00002426
Iteration 5/1000 | Loss: 0.00002245
Iteration 6/1000 | Loss: 0.00002124
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00001998
Iteration 9/1000 | Loss: 0.00001959
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001891
Iteration 15/1000 | Loss: 0.00001875
Iteration 16/1000 | Loss: 0.00001874
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001840
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001831
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001827
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001822
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001817
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001817
Iteration 120/1000 | Loss: 0.00001817
Iteration 121/1000 | Loss: 0.00001817
Iteration 122/1000 | Loss: 0.00001817
Iteration 123/1000 | Loss: 0.00001817
Iteration 124/1000 | Loss: 0.00001817
Iteration 125/1000 | Loss: 0.00001817
Iteration 126/1000 | Loss: 0.00001817
Iteration 127/1000 | Loss: 0.00001816
Iteration 128/1000 | Loss: 0.00001816
Iteration 129/1000 | Loss: 0.00001816
Iteration 130/1000 | Loss: 0.00001816
Iteration 131/1000 | Loss: 0.00001816
Iteration 132/1000 | Loss: 0.00001816
Iteration 133/1000 | Loss: 0.00001816
Iteration 134/1000 | Loss: 0.00001816
Iteration 135/1000 | Loss: 0.00001816
Iteration 136/1000 | Loss: 0.00001816
Iteration 137/1000 | Loss: 0.00001816
Iteration 138/1000 | Loss: 0.00001816
Iteration 139/1000 | Loss: 0.00001815
Iteration 140/1000 | Loss: 0.00001815
Iteration 141/1000 | Loss: 0.00001815
Iteration 142/1000 | Loss: 0.00001815
Iteration 143/1000 | Loss: 0.00001815
Iteration 144/1000 | Loss: 0.00001815
Iteration 145/1000 | Loss: 0.00001815
Iteration 146/1000 | Loss: 0.00001815
Iteration 147/1000 | Loss: 0.00001815
Iteration 148/1000 | Loss: 0.00001814
Iteration 149/1000 | Loss: 0.00001814
Iteration 150/1000 | Loss: 0.00001814
Iteration 151/1000 | Loss: 0.00001814
Iteration 152/1000 | Loss: 0.00001814
Iteration 153/1000 | Loss: 0.00001814
Iteration 154/1000 | Loss: 0.00001814
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001813
Iteration 157/1000 | Loss: 0.00001813
Iteration 158/1000 | Loss: 0.00001813
Iteration 159/1000 | Loss: 0.00001813
Iteration 160/1000 | Loss: 0.00001813
Iteration 161/1000 | Loss: 0.00001813
Iteration 162/1000 | Loss: 0.00001813
Iteration 163/1000 | Loss: 0.00001813
Iteration 164/1000 | Loss: 0.00001813
Iteration 165/1000 | Loss: 0.00001813
Iteration 166/1000 | Loss: 0.00001812
Iteration 167/1000 | Loss: 0.00001812
Iteration 168/1000 | Loss: 0.00001812
Iteration 169/1000 | Loss: 0.00001812
Iteration 170/1000 | Loss: 0.00001812
Iteration 171/1000 | Loss: 0.00001812
Iteration 172/1000 | Loss: 0.00001812
Iteration 173/1000 | Loss: 0.00001812
Iteration 174/1000 | Loss: 0.00001812
Iteration 175/1000 | Loss: 0.00001812
Iteration 176/1000 | Loss: 0.00001812
Iteration 177/1000 | Loss: 0.00001812
Iteration 178/1000 | Loss: 0.00001812
Iteration 179/1000 | Loss: 0.00001812
Iteration 180/1000 | Loss: 0.00001812
Iteration 181/1000 | Loss: 0.00001812
Iteration 182/1000 | Loss: 0.00001812
Iteration 183/1000 | Loss: 0.00001812
Iteration 184/1000 | Loss: 0.00001812
Iteration 185/1000 | Loss: 0.00001812
Iteration 186/1000 | Loss: 0.00001812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.811564106901642e-05, 1.811564106901642e-05, 1.811564106901642e-05, 1.811564106901642e-05, 1.811564106901642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.811564106901642e-05

Optimization complete. Final v2v error: 3.5805625915527344 mm

Highest mean error: 4.769132137298584 mm for frame 223

Lowest mean error: 2.9223525524139404 mm for frame 41

Saving results

Total time: 51.07720685005188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395652
Iteration 2/25 | Loss: 0.00087898
Iteration 3/25 | Loss: 0.00076483
Iteration 4/25 | Loss: 0.00075636
Iteration 5/25 | Loss: 0.00075334
Iteration 6/25 | Loss: 0.00075244
Iteration 7/25 | Loss: 0.00075244
Iteration 8/25 | Loss: 0.00075244
Iteration 9/25 | Loss: 0.00075244
Iteration 10/25 | Loss: 0.00075244
Iteration 11/25 | Loss: 0.00075244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007524436805397272, 0.0007524436805397272, 0.0007524436805397272, 0.0007524436805397272, 0.0007524436805397272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007524436805397272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49172473
Iteration 2/25 | Loss: 0.00058714
Iteration 3/25 | Loss: 0.00058714
Iteration 4/25 | Loss: 0.00058714
Iteration 5/25 | Loss: 0.00058714
Iteration 6/25 | Loss: 0.00058714
Iteration 7/25 | Loss: 0.00058714
Iteration 8/25 | Loss: 0.00058714
Iteration 9/25 | Loss: 0.00058714
Iteration 10/25 | Loss: 0.00058714
Iteration 11/25 | Loss: 0.00058714
Iteration 12/25 | Loss: 0.00058714
Iteration 13/25 | Loss: 0.00058714
Iteration 14/25 | Loss: 0.00058714
Iteration 15/25 | Loss: 0.00058714
Iteration 16/25 | Loss: 0.00058714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005871359026059508, 0.0005871359026059508, 0.0005871359026059508, 0.0005871359026059508, 0.0005871359026059508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005871359026059508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058714
Iteration 2/1000 | Loss: 0.00002911
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001211
Iteration 6/1000 | Loss: 0.00001164
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001114
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001114
Iteration 14/1000 | Loss: 0.00001114
Iteration 15/1000 | Loss: 0.00001113
Iteration 16/1000 | Loss: 0.00001113
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001108
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001092
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001086
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001085
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001083
Iteration 65/1000 | Loss: 0.00001083
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00001079
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001078
Iteration 96/1000 | Loss: 0.00001078
Iteration 97/1000 | Loss: 0.00001078
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001077
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001072
Iteration 155/1000 | Loss: 0.00001072
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.0717101758928038e-05, 1.0717101758928038e-05, 1.0717101758928038e-05, 1.0717101758928038e-05, 1.0717101758928038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0717101758928038e-05

Optimization complete. Final v2v error: 2.761003255844116 mm

Highest mean error: 3.0884039402008057 mm for frame 47

Lowest mean error: 2.610856533050537 mm for frame 62

Saving results

Total time: 35.5753858089447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038609
Iteration 2/25 | Loss: 0.00169660
Iteration 3/25 | Loss: 0.00121419
Iteration 4/25 | Loss: 0.00118882
Iteration 5/25 | Loss: 0.00109302
Iteration 6/25 | Loss: 0.00110407
Iteration 7/25 | Loss: 0.00109756
Iteration 8/25 | Loss: 0.00108885
Iteration 9/25 | Loss: 0.00107278
Iteration 10/25 | Loss: 0.00106809
Iteration 11/25 | Loss: 0.00106727
Iteration 12/25 | Loss: 0.00107535
Iteration 13/25 | Loss: 0.00107522
Iteration 14/25 | Loss: 0.00110180
Iteration 15/25 | Loss: 0.00110894
Iteration 16/25 | Loss: 0.00107634
Iteration 17/25 | Loss: 0.00104949
Iteration 18/25 | Loss: 0.00105899
Iteration 19/25 | Loss: 0.00105390
Iteration 20/25 | Loss: 0.00103372
Iteration 21/25 | Loss: 0.00102904
Iteration 22/25 | Loss: 0.00102813
Iteration 23/25 | Loss: 0.00101134
Iteration 24/25 | Loss: 0.00101696
Iteration 25/25 | Loss: 0.00101086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14079380
Iteration 2/25 | Loss: 0.00757049
Iteration 3/25 | Loss: 0.00813921
Iteration 4/25 | Loss: 0.00452794
Iteration 5/25 | Loss: 0.00245004
Iteration 6/25 | Loss: 0.00245004
Iteration 7/25 | Loss: 0.00245004
Iteration 8/25 | Loss: 0.00245004
Iteration 9/25 | Loss: 0.00245004
Iteration 10/25 | Loss: 0.00245004
Iteration 11/25 | Loss: 0.00245004
Iteration 12/25 | Loss: 0.00245004
Iteration 13/25 | Loss: 0.00245004
Iteration 14/25 | Loss: 0.00245004
Iteration 15/25 | Loss: 0.00245004
Iteration 16/25 | Loss: 0.00245004
Iteration 17/25 | Loss: 0.00245004
Iteration 18/25 | Loss: 0.00245004
Iteration 19/25 | Loss: 0.00245004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002450039377436042, 0.002450039377436042, 0.002450039377436042, 0.002450039377436042, 0.002450039377436042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002450039377436042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245004
Iteration 2/1000 | Loss: 0.00293647
Iteration 3/1000 | Loss: 0.00189439
Iteration 4/1000 | Loss: 0.00183099
Iteration 5/1000 | Loss: 0.00612612
Iteration 6/1000 | Loss: 0.00517573
Iteration 7/1000 | Loss: 0.00211216
Iteration 8/1000 | Loss: 0.00269692
Iteration 9/1000 | Loss: 0.00415878
Iteration 10/1000 | Loss: 0.00304090
Iteration 11/1000 | Loss: 0.00350325
Iteration 12/1000 | Loss: 0.00229082
Iteration 13/1000 | Loss: 0.00222633
Iteration 14/1000 | Loss: 0.00271722
Iteration 15/1000 | Loss: 0.00514915
Iteration 16/1000 | Loss: 0.00180391
Iteration 17/1000 | Loss: 0.00221253
Iteration 18/1000 | Loss: 0.00251686
Iteration 19/1000 | Loss: 0.00236573
Iteration 20/1000 | Loss: 0.00437902
Iteration 21/1000 | Loss: 0.00204204
Iteration 22/1000 | Loss: 0.00291701
Iteration 23/1000 | Loss: 0.00159374
Iteration 24/1000 | Loss: 0.00217687
Iteration 25/1000 | Loss: 0.00202966
Iteration 26/1000 | Loss: 0.00135262
Iteration 27/1000 | Loss: 0.00198733
Iteration 28/1000 | Loss: 0.00251652
Iteration 29/1000 | Loss: 0.00256346
Iteration 30/1000 | Loss: 0.00298272
Iteration 31/1000 | Loss: 0.00243848
Iteration 32/1000 | Loss: 0.00160708
Iteration 33/1000 | Loss: 0.00190459
Iteration 34/1000 | Loss: 0.00189709
Iteration 35/1000 | Loss: 0.00405667
Iteration 36/1000 | Loss: 0.00171966
Iteration 37/1000 | Loss: 0.00145256
Iteration 38/1000 | Loss: 0.00187142
Iteration 39/1000 | Loss: 0.00281185
Iteration 40/1000 | Loss: 0.00178611
Iteration 41/1000 | Loss: 0.00171858
Iteration 42/1000 | Loss: 0.00213043
Iteration 43/1000 | Loss: 0.00215758
Iteration 44/1000 | Loss: 0.00376672
Iteration 45/1000 | Loss: 0.00194398
Iteration 46/1000 | Loss: 0.00152374
Iteration 47/1000 | Loss: 0.00180326
Iteration 48/1000 | Loss: 0.00196359
Iteration 49/1000 | Loss: 0.00172495
Iteration 50/1000 | Loss: 0.00158180
Iteration 51/1000 | Loss: 0.00163269
Iteration 52/1000 | Loss: 0.00151921
Iteration 53/1000 | Loss: 0.00108584
Iteration 54/1000 | Loss: 0.00117215
Iteration 55/1000 | Loss: 0.00205164
Iteration 56/1000 | Loss: 0.00179204
Iteration 57/1000 | Loss: 0.00197213
Iteration 58/1000 | Loss: 0.00250874
Iteration 59/1000 | Loss: 0.00228686
Iteration 60/1000 | Loss: 0.00136311
Iteration 61/1000 | Loss: 0.00150304
Iteration 62/1000 | Loss: 0.00134191
Iteration 63/1000 | Loss: 0.00141746
Iteration 64/1000 | Loss: 0.00138743
Iteration 65/1000 | Loss: 0.00131111
Iteration 66/1000 | Loss: 0.00324991
Iteration 67/1000 | Loss: 0.00163736
Iteration 68/1000 | Loss: 0.00134742
Iteration 69/1000 | Loss: 0.00200348
Iteration 70/1000 | Loss: 0.00136511
Iteration 71/1000 | Loss: 0.00131427
Iteration 72/1000 | Loss: 0.00173848
Iteration 73/1000 | Loss: 0.00168065
Iteration 74/1000 | Loss: 0.00160568
Iteration 75/1000 | Loss: 0.00228280
Iteration 76/1000 | Loss: 0.00157378
Iteration 77/1000 | Loss: 0.00109720
Iteration 78/1000 | Loss: 0.00134902
Iteration 79/1000 | Loss: 0.00254482
Iteration 80/1000 | Loss: 0.00157490
Iteration 81/1000 | Loss: 0.00109429
Iteration 82/1000 | Loss: 0.00089295
Iteration 83/1000 | Loss: 0.00092140
Iteration 84/1000 | Loss: 0.00138457
Iteration 85/1000 | Loss: 0.00184647
Iteration 86/1000 | Loss: 0.00122980
Iteration 87/1000 | Loss: 0.00138547
Iteration 88/1000 | Loss: 0.00142905
Iteration 89/1000 | Loss: 0.00114665
Iteration 90/1000 | Loss: 0.00108926
Iteration 91/1000 | Loss: 0.00121177
Iteration 92/1000 | Loss: 0.00108546
Iteration 93/1000 | Loss: 0.00093646
Iteration 94/1000 | Loss: 0.00128675
Iteration 95/1000 | Loss: 0.00281877
Iteration 96/1000 | Loss: 0.00117600
Iteration 97/1000 | Loss: 0.00233841
Iteration 98/1000 | Loss: 0.00109140
Iteration 99/1000 | Loss: 0.00129486
Iteration 100/1000 | Loss: 0.00567245
Iteration 101/1000 | Loss: 0.00169522
Iteration 102/1000 | Loss: 0.00172635
Iteration 103/1000 | Loss: 0.00292339
Iteration 104/1000 | Loss: 0.00122830
Iteration 105/1000 | Loss: 0.00135148
Iteration 106/1000 | Loss: 0.00146474
Iteration 107/1000 | Loss: 0.00141313
Iteration 108/1000 | Loss: 0.00139960
Iteration 109/1000 | Loss: 0.00242547
Iteration 110/1000 | Loss: 0.00160904
Iteration 111/1000 | Loss: 0.00160759
Iteration 112/1000 | Loss: 0.00159186
Iteration 113/1000 | Loss: 0.00132013
Iteration 114/1000 | Loss: 0.00133468
Iteration 115/1000 | Loss: 0.00144541
Iteration 116/1000 | Loss: 0.00169695
Iteration 117/1000 | Loss: 0.00151888
Iteration 118/1000 | Loss: 0.00415591
Iteration 119/1000 | Loss: 0.00362730
Iteration 120/1000 | Loss: 0.00204934
Iteration 121/1000 | Loss: 0.00251172
Iteration 122/1000 | Loss: 0.00212085
Iteration 123/1000 | Loss: 0.00202615
Iteration 124/1000 | Loss: 0.00190512
Iteration 125/1000 | Loss: 0.00530473
Iteration 126/1000 | Loss: 0.00207779
Iteration 127/1000 | Loss: 0.00404096
Iteration 128/1000 | Loss: 0.00102418
Iteration 129/1000 | Loss: 0.00179322
Iteration 130/1000 | Loss: 0.00149680
Iteration 131/1000 | Loss: 0.00141537
Iteration 132/1000 | Loss: 0.00084461
Iteration 133/1000 | Loss: 0.00113106
Iteration 134/1000 | Loss: 0.00153068
Iteration 135/1000 | Loss: 0.00367606
Iteration 136/1000 | Loss: 0.00127738
Iteration 137/1000 | Loss: 0.00092100
Iteration 138/1000 | Loss: 0.00265012
Iteration 139/1000 | Loss: 0.00145166
Iteration 140/1000 | Loss: 0.00116335
Iteration 141/1000 | Loss: 0.00127626
Iteration 142/1000 | Loss: 0.00132066
Iteration 143/1000 | Loss: 0.00141905
Iteration 144/1000 | Loss: 0.00147527
Iteration 145/1000 | Loss: 0.00148324
Iteration 146/1000 | Loss: 0.00129549
Iteration 147/1000 | Loss: 0.00421411
Iteration 148/1000 | Loss: 0.00161082
Iteration 149/1000 | Loss: 0.00484865
Iteration 150/1000 | Loss: 0.00547693
Iteration 151/1000 | Loss: 0.00097417
Iteration 152/1000 | Loss: 0.00121563
Iteration 153/1000 | Loss: 0.00147687
Iteration 154/1000 | Loss: 0.00141983
Iteration 155/1000 | Loss: 0.00283747
Iteration 156/1000 | Loss: 0.00145245
Iteration 157/1000 | Loss: 0.00126235
Iteration 158/1000 | Loss: 0.00334841
Iteration 159/1000 | Loss: 0.00079893
Iteration 160/1000 | Loss: 0.00095992
Iteration 161/1000 | Loss: 0.00248414
Iteration 162/1000 | Loss: 0.00083013
Iteration 163/1000 | Loss: 0.00108270
Iteration 164/1000 | Loss: 0.00154887
Iteration 165/1000 | Loss: 0.00085818
Iteration 166/1000 | Loss: 0.00109552
Iteration 167/1000 | Loss: 0.00084675
Iteration 168/1000 | Loss: 0.00089650
Iteration 169/1000 | Loss: 0.00091807
Iteration 170/1000 | Loss: 0.00100729
Iteration 171/1000 | Loss: 0.00098881
Iteration 172/1000 | Loss: 0.00142269
Iteration 173/1000 | Loss: 0.00416226
Iteration 174/1000 | Loss: 0.00549498
Iteration 175/1000 | Loss: 0.01077020
Iteration 176/1000 | Loss: 0.00845245
Iteration 177/1000 | Loss: 0.00210225
Iteration 178/1000 | Loss: 0.00116709
Iteration 179/1000 | Loss: 0.00090730
Iteration 180/1000 | Loss: 0.00188931
Iteration 181/1000 | Loss: 0.00063057
Iteration 182/1000 | Loss: 0.00135374
Iteration 183/1000 | Loss: 0.00085940
Iteration 184/1000 | Loss: 0.00188006
Iteration 185/1000 | Loss: 0.00094891
Iteration 186/1000 | Loss: 0.00074263
Iteration 187/1000 | Loss: 0.00082036
Iteration 188/1000 | Loss: 0.00148683
Iteration 189/1000 | Loss: 0.00162071
Iteration 190/1000 | Loss: 0.00211469
Iteration 191/1000 | Loss: 0.00368812
Iteration 192/1000 | Loss: 0.00114290
Iteration 193/1000 | Loss: 0.00230337
Iteration 194/1000 | Loss: 0.00201954
Iteration 195/1000 | Loss: 0.00128234
Iteration 196/1000 | Loss: 0.00130239
Iteration 197/1000 | Loss: 0.00192467
Iteration 198/1000 | Loss: 0.00075593
Iteration 199/1000 | Loss: 0.00073775
Iteration 200/1000 | Loss: 0.00074627
Iteration 201/1000 | Loss: 0.00075310
Iteration 202/1000 | Loss: 0.00183844
Iteration 203/1000 | Loss: 0.00113153
Iteration 204/1000 | Loss: 0.00377274
Iteration 205/1000 | Loss: 0.00251368
Iteration 206/1000 | Loss: 0.00102955
Iteration 207/1000 | Loss: 0.00077562
Iteration 208/1000 | Loss: 0.00078514
Iteration 209/1000 | Loss: 0.00082697
Iteration 210/1000 | Loss: 0.00102450
Iteration 211/1000 | Loss: 0.00104853
Iteration 212/1000 | Loss: 0.00089797
Iteration 213/1000 | Loss: 0.00067449
Iteration 214/1000 | Loss: 0.00090970
Iteration 215/1000 | Loss: 0.00090520
Iteration 216/1000 | Loss: 0.00103040
Iteration 217/1000 | Loss: 0.00112074
Iteration 218/1000 | Loss: 0.00125567
Iteration 219/1000 | Loss: 0.00196310
Iteration 220/1000 | Loss: 0.00141206
Iteration 221/1000 | Loss: 0.00100505
Iteration 222/1000 | Loss: 0.00117674
Iteration 223/1000 | Loss: 0.00097178
Iteration 224/1000 | Loss: 0.00078548
Iteration 225/1000 | Loss: 0.00119996
Iteration 226/1000 | Loss: 0.00093952
Iteration 227/1000 | Loss: 0.00113842
Iteration 228/1000 | Loss: 0.00100583
Iteration 229/1000 | Loss: 0.00121193
Iteration 230/1000 | Loss: 0.00117994
Iteration 231/1000 | Loss: 0.00157392
Iteration 232/1000 | Loss: 0.00093620
Iteration 233/1000 | Loss: 0.00086375
Iteration 234/1000 | Loss: 0.00117615
Iteration 235/1000 | Loss: 0.00086238
Iteration 236/1000 | Loss: 0.00145717
Iteration 237/1000 | Loss: 0.00058702
Iteration 238/1000 | Loss: 0.00098476
Iteration 239/1000 | Loss: 0.00111136
Iteration 240/1000 | Loss: 0.00114214
Iteration 241/1000 | Loss: 0.00146674
Iteration 242/1000 | Loss: 0.00084280
Iteration 243/1000 | Loss: 0.00093246
Iteration 244/1000 | Loss: 0.00086079
Iteration 245/1000 | Loss: 0.00098149
Iteration 246/1000 | Loss: 0.00096493
Iteration 247/1000 | Loss: 0.00099910
Iteration 248/1000 | Loss: 0.00098952
Iteration 249/1000 | Loss: 0.00133101
Iteration 250/1000 | Loss: 0.00106238
Iteration 251/1000 | Loss: 0.00141309
Iteration 252/1000 | Loss: 0.00109846
Iteration 253/1000 | Loss: 0.00096153
Iteration 254/1000 | Loss: 0.00094026
Iteration 255/1000 | Loss: 0.00086330
Iteration 256/1000 | Loss: 0.00125655
Iteration 257/1000 | Loss: 0.00058946
Iteration 258/1000 | Loss: 0.00076674
Iteration 259/1000 | Loss: 0.00086318
Iteration 260/1000 | Loss: 0.00112832
Iteration 261/1000 | Loss: 0.00168215
Iteration 262/1000 | Loss: 0.00228155
Iteration 263/1000 | Loss: 0.00235832
Iteration 264/1000 | Loss: 0.00097034
Iteration 265/1000 | Loss: 0.00075469
Iteration 266/1000 | Loss: 0.00087246
Iteration 267/1000 | Loss: 0.00129632
Iteration 268/1000 | Loss: 0.00090392
Iteration 269/1000 | Loss: 0.00100307
Iteration 270/1000 | Loss: 0.00072330
Iteration 271/1000 | Loss: 0.00073402
Iteration 272/1000 | Loss: 0.00088060
Iteration 273/1000 | Loss: 0.00062303
Iteration 274/1000 | Loss: 0.00076198
Iteration 275/1000 | Loss: 0.00102715
Iteration 276/1000 | Loss: 0.00081636
Iteration 277/1000 | Loss: 0.00073145
Iteration 278/1000 | Loss: 0.00076867
Iteration 279/1000 | Loss: 0.00070676
Iteration 280/1000 | Loss: 0.00055525
Iteration 281/1000 | Loss: 0.00141039
Iteration 282/1000 | Loss: 0.00159741
Iteration 283/1000 | Loss: 0.00069998
Iteration 284/1000 | Loss: 0.00056947
Iteration 285/1000 | Loss: 0.00208065
Iteration 286/1000 | Loss: 0.00046835
Iteration 287/1000 | Loss: 0.00038079
Iteration 288/1000 | Loss: 0.00039388
Iteration 289/1000 | Loss: 0.00028594
Iteration 290/1000 | Loss: 0.00034149
Iteration 291/1000 | Loss: 0.00043618
Iteration 292/1000 | Loss: 0.00032824
Iteration 293/1000 | Loss: 0.00037578
Iteration 294/1000 | Loss: 0.00023865
Iteration 295/1000 | Loss: 0.00044961
Iteration 296/1000 | Loss: 0.00045556
Iteration 297/1000 | Loss: 0.00058206
Iteration 298/1000 | Loss: 0.00123992
Iteration 299/1000 | Loss: 0.00332087
Iteration 300/1000 | Loss: 0.00124616
Iteration 301/1000 | Loss: 0.00047779
Iteration 302/1000 | Loss: 0.00055074
Iteration 303/1000 | Loss: 0.00080672
Iteration 304/1000 | Loss: 0.00057321
Iteration 305/1000 | Loss: 0.00119642
Iteration 306/1000 | Loss: 0.00050616
Iteration 307/1000 | Loss: 0.00067056
Iteration 308/1000 | Loss: 0.00403893
Iteration 309/1000 | Loss: 0.00156435
Iteration 310/1000 | Loss: 0.00068312
Iteration 311/1000 | Loss: 0.00071283
Iteration 312/1000 | Loss: 0.00208242
Iteration 313/1000 | Loss: 0.00188399
Iteration 314/1000 | Loss: 0.00098889
Iteration 315/1000 | Loss: 0.00072715
Iteration 316/1000 | Loss: 0.00062639
Iteration 317/1000 | Loss: 0.00150476
Iteration 318/1000 | Loss: 0.00304730
Iteration 319/1000 | Loss: 0.00121910
Iteration 320/1000 | Loss: 0.00059355
Iteration 321/1000 | Loss: 0.00104128
Iteration 322/1000 | Loss: 0.00172905
Iteration 323/1000 | Loss: 0.00081971
Iteration 324/1000 | Loss: 0.00115031
Iteration 325/1000 | Loss: 0.00043370
Iteration 326/1000 | Loss: 0.00070973
Iteration 327/1000 | Loss: 0.00067136
Iteration 328/1000 | Loss: 0.00082830
Iteration 329/1000 | Loss: 0.00070850
Iteration 330/1000 | Loss: 0.00138439
Iteration 331/1000 | Loss: 0.00100828
Iteration 332/1000 | Loss: 0.00226214
Iteration 333/1000 | Loss: 0.00093147
Iteration 334/1000 | Loss: 0.00072209
Iteration 335/1000 | Loss: 0.00057109
Iteration 336/1000 | Loss: 0.00130667
Iteration 337/1000 | Loss: 0.00117988
Iteration 338/1000 | Loss: 0.00075984
Iteration 339/1000 | Loss: 0.00132760
Iteration 340/1000 | Loss: 0.00068689
Iteration 341/1000 | Loss: 0.00066664
Iteration 342/1000 | Loss: 0.00119939
Iteration 343/1000 | Loss: 0.00189212
Iteration 344/1000 | Loss: 0.00111072
Iteration 345/1000 | Loss: 0.00139336
Iteration 346/1000 | Loss: 0.00107232
Iteration 347/1000 | Loss: 0.00127722
Iteration 348/1000 | Loss: 0.00074273
Iteration 349/1000 | Loss: 0.00093110
Iteration 350/1000 | Loss: 0.00077194
Iteration 351/1000 | Loss: 0.00072037
Iteration 352/1000 | Loss: 0.00061736
Iteration 353/1000 | Loss: 0.00066905
Iteration 354/1000 | Loss: 0.00049869
Iteration 355/1000 | Loss: 0.00125212
Iteration 356/1000 | Loss: 0.00063114
Iteration 357/1000 | Loss: 0.00044936
Iteration 358/1000 | Loss: 0.00077735
Iteration 359/1000 | Loss: 0.00052984
Iteration 360/1000 | Loss: 0.00085044
Iteration 361/1000 | Loss: 0.00329157
Iteration 362/1000 | Loss: 0.00076158
Iteration 363/1000 | Loss: 0.00075953
Iteration 364/1000 | Loss: 0.00128635
Iteration 365/1000 | Loss: 0.00291402
Iteration 366/1000 | Loss: 0.00152334
Iteration 367/1000 | Loss: 0.00129153
Iteration 368/1000 | Loss: 0.00066254
Iteration 369/1000 | Loss: 0.00079518
Iteration 370/1000 | Loss: 0.00197554
Iteration 371/1000 | Loss: 0.00080659
Iteration 372/1000 | Loss: 0.00124787
Iteration 373/1000 | Loss: 0.00053640
Iteration 374/1000 | Loss: 0.00145661
Iteration 375/1000 | Loss: 0.00100664
Iteration 376/1000 | Loss: 0.00046596
Iteration 377/1000 | Loss: 0.00057657
Iteration 378/1000 | Loss: 0.00023231
Iteration 379/1000 | Loss: 0.00072200
Iteration 380/1000 | Loss: 0.00069809
Iteration 381/1000 | Loss: 0.00038672
Iteration 382/1000 | Loss: 0.00136152
Iteration 383/1000 | Loss: 0.00065766
Iteration 384/1000 | Loss: 0.00052539
Iteration 385/1000 | Loss: 0.00129681
Iteration 386/1000 | Loss: 0.00063753
Iteration 387/1000 | Loss: 0.00046593
Iteration 388/1000 | Loss: 0.00070070
Iteration 389/1000 | Loss: 0.00098482
Iteration 390/1000 | Loss: 0.00060027
Iteration 391/1000 | Loss: 0.00061578
Iteration 392/1000 | Loss: 0.00063540
Iteration 393/1000 | Loss: 0.00052255
Iteration 394/1000 | Loss: 0.00046978
Iteration 395/1000 | Loss: 0.00055777
Iteration 396/1000 | Loss: 0.00057249
Iteration 397/1000 | Loss: 0.00050732
Iteration 398/1000 | Loss: 0.00179882
Iteration 399/1000 | Loss: 0.00066391
Iteration 400/1000 | Loss: 0.00093760
Iteration 401/1000 | Loss: 0.00084361
Iteration 402/1000 | Loss: 0.00125581
Iteration 403/1000 | Loss: 0.00097020
Iteration 404/1000 | Loss: 0.00072280
Iteration 405/1000 | Loss: 0.00074672
Iteration 406/1000 | Loss: 0.00131052
Iteration 407/1000 | Loss: 0.00076196
Iteration 408/1000 | Loss: 0.00177589
Iteration 409/1000 | Loss: 0.00130925
Iteration 410/1000 | Loss: 0.00267589
Iteration 411/1000 | Loss: 0.00182304
Iteration 412/1000 | Loss: 0.00117856
Iteration 413/1000 | Loss: 0.00057435
Iteration 414/1000 | Loss: 0.00063085
Iteration 415/1000 | Loss: 0.00061621
Iteration 416/1000 | Loss: 0.00088131
Iteration 417/1000 | Loss: 0.00089106
Iteration 418/1000 | Loss: 0.00074204
Iteration 419/1000 | Loss: 0.00106918
Iteration 420/1000 | Loss: 0.00024387
Iteration 421/1000 | Loss: 0.00071929
Iteration 422/1000 | Loss: 0.00072124
Iteration 423/1000 | Loss: 0.00088621
Iteration 424/1000 | Loss: 0.00063432
Iteration 425/1000 | Loss: 0.00053012
Iteration 426/1000 | Loss: 0.00048941
Iteration 427/1000 | Loss: 0.00162996
Iteration 428/1000 | Loss: 0.00033059
Iteration 429/1000 | Loss: 0.00043899
Iteration 430/1000 | Loss: 0.00041916
Iteration 431/1000 | Loss: 0.00044723
Iteration 432/1000 | Loss: 0.00031829
Iteration 433/1000 | Loss: 0.00070175
Iteration 434/1000 | Loss: 0.00101949
Iteration 435/1000 | Loss: 0.00034483
Iteration 436/1000 | Loss: 0.00047576
Iteration 437/1000 | Loss: 0.00046892
Iteration 438/1000 | Loss: 0.00044064
Iteration 439/1000 | Loss: 0.00094747
Iteration 440/1000 | Loss: 0.00071876
Iteration 441/1000 | Loss: 0.00049331
Iteration 442/1000 | Loss: 0.00035367
Iteration 443/1000 | Loss: 0.00242484
Iteration 444/1000 | Loss: 0.00071342
Iteration 445/1000 | Loss: 0.00075615
Iteration 446/1000 | Loss: 0.00041590
Iteration 447/1000 | Loss: 0.00040511
Iteration 448/1000 | Loss: 0.00029379
Iteration 449/1000 | Loss: 0.00086441
Iteration 450/1000 | Loss: 0.00064165
Iteration 451/1000 | Loss: 0.00044093
Iteration 452/1000 | Loss: 0.00039521
Iteration 453/1000 | Loss: 0.00037457
Iteration 454/1000 | Loss: 0.00044487
Iteration 455/1000 | Loss: 0.00043102
Iteration 456/1000 | Loss: 0.00082951
Iteration 457/1000 | Loss: 0.00138733
Iteration 458/1000 | Loss: 0.00051408
Iteration 459/1000 | Loss: 0.00035656
Iteration 460/1000 | Loss: 0.00042561
Iteration 461/1000 | Loss: 0.00043308
Iteration 462/1000 | Loss: 0.00329876
Iteration 463/1000 | Loss: 0.00064318
Iteration 464/1000 | Loss: 0.00060940
Iteration 465/1000 | Loss: 0.00084098
Iteration 466/1000 | Loss: 0.00110351
Iteration 467/1000 | Loss: 0.00034033
Iteration 468/1000 | Loss: 0.00044167
Iteration 469/1000 | Loss: 0.00108053
Iteration 470/1000 | Loss: 0.00069248
Iteration 471/1000 | Loss: 0.00190020
Iteration 472/1000 | Loss: 0.00074661
Iteration 473/1000 | Loss: 0.00135341
Iteration 474/1000 | Loss: 0.00029524
Iteration 475/1000 | Loss: 0.00035868
Iteration 476/1000 | Loss: 0.00049599
Iteration 477/1000 | Loss: 0.00051645
Iteration 478/1000 | Loss: 0.00048073
Iteration 479/1000 | Loss: 0.00052457
Iteration 480/1000 | Loss: 0.00046596
Iteration 481/1000 | Loss: 0.00048280
Iteration 482/1000 | Loss: 0.00048810
Iteration 483/1000 | Loss: 0.00058003
Iteration 484/1000 | Loss: 0.00046016
Iteration 485/1000 | Loss: 0.00053463
Iteration 486/1000 | Loss: 0.00070553
Iteration 487/1000 | Loss: 0.00170648
Iteration 488/1000 | Loss: 0.00041537
Iteration 489/1000 | Loss: 0.00044489
Iteration 490/1000 | Loss: 0.00196879
Iteration 491/1000 | Loss: 0.00233031
Iteration 492/1000 | Loss: 0.00245803
Iteration 493/1000 | Loss: 0.00047769
Iteration 494/1000 | Loss: 0.00030740
Iteration 495/1000 | Loss: 0.00037451
Iteration 496/1000 | Loss: 0.00080968
Iteration 497/1000 | Loss: 0.00110311
Iteration 498/1000 | Loss: 0.00057384
Iteration 499/1000 | Loss: 0.00043266
Iteration 500/1000 | Loss: 0.00042461
Iteration 501/1000 | Loss: 0.00065515
Iteration 502/1000 | Loss: 0.00099395
Iteration 503/1000 | Loss: 0.00113680
Iteration 504/1000 | Loss: 0.00043447
Iteration 505/1000 | Loss: 0.00041392
Iteration 506/1000 | Loss: 0.00111313
Iteration 507/1000 | Loss: 0.00041623
Iteration 508/1000 | Loss: 0.00084915
Iteration 509/1000 | Loss: 0.00043108
Iteration 510/1000 | Loss: 0.00044979
Iteration 511/1000 | Loss: 0.00037958
Iteration 512/1000 | Loss: 0.00047390
Iteration 513/1000 | Loss: 0.00049054
Iteration 514/1000 | Loss: 0.00046689
Iteration 515/1000 | Loss: 0.00050803
Iteration 516/1000 | Loss: 0.00054633
Iteration 517/1000 | Loss: 0.00042085
Iteration 518/1000 | Loss: 0.00049966
Iteration 519/1000 | Loss: 0.00022296
Iteration 520/1000 | Loss: 0.00056805
Iteration 521/1000 | Loss: 0.00055750
Iteration 522/1000 | Loss: 0.00068797
Iteration 523/1000 | Loss: 0.00060455
Iteration 524/1000 | Loss: 0.00063056
Iteration 525/1000 | Loss: 0.00059525
Iteration 526/1000 | Loss: 0.00064970
Iteration 527/1000 | Loss: 0.00168044
Iteration 528/1000 | Loss: 0.00078733
Iteration 529/1000 | Loss: 0.00063646
Iteration 530/1000 | Loss: 0.00057041
Iteration 531/1000 | Loss: 0.00029244
Iteration 532/1000 | Loss: 0.00030076
Iteration 533/1000 | Loss: 0.00050793
Iteration 534/1000 | Loss: 0.00045697
Iteration 535/1000 | Loss: 0.00106324
Iteration 536/1000 | Loss: 0.00070800
Iteration 537/1000 | Loss: 0.00163219
Iteration 538/1000 | Loss: 0.00080815
Iteration 539/1000 | Loss: 0.00196517
Iteration 540/1000 | Loss: 0.00061658
Iteration 541/1000 | Loss: 0.00111252
Iteration 542/1000 | Loss: 0.00159375
Iteration 543/1000 | Loss: 0.00094848
Iteration 544/1000 | Loss: 0.00130610
Iteration 545/1000 | Loss: 0.00057966
Iteration 546/1000 | Loss: 0.00062251
Iteration 547/1000 | Loss: 0.00097278
Iteration 548/1000 | Loss: 0.00154471
Iteration 549/1000 | Loss: 0.00147253
Iteration 550/1000 | Loss: 0.00043970
Iteration 551/1000 | Loss: 0.00058678
Iteration 552/1000 | Loss: 0.00034394
Iteration 553/1000 | Loss: 0.00036362
Iteration 554/1000 | Loss: 0.00033422
Iteration 555/1000 | Loss: 0.00028125
Iteration 556/1000 | Loss: 0.00031785
Iteration 557/1000 | Loss: 0.00035024
Iteration 558/1000 | Loss: 0.00113239
Iteration 559/1000 | Loss: 0.00065922
Iteration 560/1000 | Loss: 0.00051417
Iteration 561/1000 | Loss: 0.00056782
Iteration 562/1000 | Loss: 0.00056972
Iteration 563/1000 | Loss: 0.00025149
Iteration 564/1000 | Loss: 0.00050856
Iteration 565/1000 | Loss: 0.00040370
Iteration 566/1000 | Loss: 0.00062230
Iteration 567/1000 | Loss: 0.00032818
Iteration 568/1000 | Loss: 0.00068837
Iteration 569/1000 | Loss: 0.00057778
Iteration 570/1000 | Loss: 0.00030298
Iteration 571/1000 | Loss: 0.00095391
Iteration 572/1000 | Loss: 0.00014669
Iteration 573/1000 | Loss: 0.00040228
Iteration 574/1000 | Loss: 0.00046514
Iteration 575/1000 | Loss: 0.00014771
Iteration 576/1000 | Loss: 0.00044276
Iteration 577/1000 | Loss: 0.00036884
Iteration 578/1000 | Loss: 0.00058204
Iteration 579/1000 | Loss: 0.00036266
Iteration 580/1000 | Loss: 0.00030675
Iteration 581/1000 | Loss: 0.00127537
Iteration 582/1000 | Loss: 0.00036588
Iteration 583/1000 | Loss: 0.00047557
Iteration 584/1000 | Loss: 0.00043189
Iteration 585/1000 | Loss: 0.00017022
Iteration 586/1000 | Loss: 0.00037791
Iteration 587/1000 | Loss: 0.00043538
Iteration 588/1000 | Loss: 0.00024607
Iteration 589/1000 | Loss: 0.00108286
Iteration 590/1000 | Loss: 0.00050566
Iteration 591/1000 | Loss: 0.00090301
Iteration 592/1000 | Loss: 0.00079813
Iteration 593/1000 | Loss: 0.00093904
Iteration 594/1000 | Loss: 0.00053444
Iteration 595/1000 | Loss: 0.00035552
Iteration 596/1000 | Loss: 0.00039730
Iteration 597/1000 | Loss: 0.00047963
Iteration 598/1000 | Loss: 0.00035614
Iteration 599/1000 | Loss: 0.00032241
Iteration 600/1000 | Loss: 0.00038008
Iteration 601/1000 | Loss: 0.00104369
Iteration 602/1000 | Loss: 0.00053419
Iteration 603/1000 | Loss: 0.00032056
Iteration 604/1000 | Loss: 0.00061882
Iteration 605/1000 | Loss: 0.00064191
Iteration 606/1000 | Loss: 0.00053255
Iteration 607/1000 | Loss: 0.00044645
Iteration 608/1000 | Loss: 0.00053936
Iteration 609/1000 | Loss: 0.00045482
Iteration 610/1000 | Loss: 0.00053880
Iteration 611/1000 | Loss: 0.00195759
Iteration 612/1000 | Loss: 0.00086143
Iteration 613/1000 | Loss: 0.00052875
Iteration 614/1000 | Loss: 0.00044366
Iteration 615/1000 | Loss: 0.00035834
Iteration 616/1000 | Loss: 0.00031933
Iteration 617/1000 | Loss: 0.00034047
Iteration 618/1000 | Loss: 0.00036650
Iteration 619/1000 | Loss: 0.00043253
Iteration 620/1000 | Loss: 0.00043798
Iteration 621/1000 | Loss: 0.00040088
Iteration 622/1000 | Loss: 0.00050956
Iteration 623/1000 | Loss: 0.00047212
Iteration 624/1000 | Loss: 0.00166995
Iteration 625/1000 | Loss: 0.00054188
Iteration 626/1000 | Loss: 0.00076235
Iteration 627/1000 | Loss: 0.00060478
Iteration 628/1000 | Loss: 0.00038535
Iteration 629/1000 | Loss: 0.00052339
Iteration 630/1000 | Loss: 0.00055036
Iteration 631/1000 | Loss: 0.00050601
Iteration 632/1000 | Loss: 0.00105869
Iteration 633/1000 | Loss: 0.00056011
Iteration 634/1000 | Loss: 0.00133163
Iteration 635/1000 | Loss: 0.00057846
Iteration 636/1000 | Loss: 0.00055014
Iteration 637/1000 | Loss: 0.00058844
Iteration 638/1000 | Loss: 0.00055180
Iteration 639/1000 | Loss: 0.00056071
Iteration 640/1000 | Loss: 0.00050315
Iteration 641/1000 | Loss: 0.00114839
Iteration 642/1000 | Loss: 0.00048986
Iteration 643/1000 | Loss: 0.00033990
Iteration 644/1000 | Loss: 0.00040482
Iteration 645/1000 | Loss: 0.00054409
Iteration 646/1000 | Loss: 0.00049488
Iteration 647/1000 | Loss: 0.00049319
Iteration 648/1000 | Loss: 0.00045832
Iteration 649/1000 | Loss: 0.00065461
Iteration 650/1000 | Loss: 0.00053842
Iteration 651/1000 | Loss: 0.00058554
Iteration 652/1000 | Loss: 0.00076533
Iteration 653/1000 | Loss: 0.00030163
Iteration 654/1000 | Loss: 0.00042756
Iteration 655/1000 | Loss: 0.00041578
Iteration 656/1000 | Loss: 0.00040982
Iteration 657/1000 | Loss: 0.00044752
Iteration 658/1000 | Loss: 0.00044647
Iteration 659/1000 | Loss: 0.00022974
Iteration 660/1000 | Loss: 0.00035214
Iteration 661/1000 | Loss: 0.00038099
Iteration 662/1000 | Loss: 0.00032697
Iteration 663/1000 | Loss: 0.00039252
Iteration 664/1000 | Loss: 0.00036135
Iteration 665/1000 | Loss: 0.00048977
Iteration 666/1000 | Loss: 0.00032134
Iteration 667/1000 | Loss: 0.00100181
Iteration 668/1000 | Loss: 0.00042341
Iteration 669/1000 | Loss: 0.00046126
Iteration 670/1000 | Loss: 0.00114794
Iteration 671/1000 | Loss: 0.00045281
Iteration 672/1000 | Loss: 0.00042158
Iteration 673/1000 | Loss: 0.00048258
Iteration 674/1000 | Loss: 0.00047350
Iteration 675/1000 | Loss: 0.00040096
Iteration 676/1000 | Loss: 0.00053732
Iteration 677/1000 | Loss: 0.00046452
Iteration 678/1000 | Loss: 0.00045246
Iteration 679/1000 | Loss: 0.00127800
Iteration 680/1000 | Loss: 0.00053464
Iteration 681/1000 | Loss: 0.00031484
Iteration 682/1000 | Loss: 0.00035486
Iteration 683/1000 | Loss: 0.00038092
Iteration 684/1000 | Loss: 0.00044187
Iteration 685/1000 | Loss: 0.00045620
Iteration 686/1000 | Loss: 0.00292570
Iteration 687/1000 | Loss: 0.00071673
Iteration 688/1000 | Loss: 0.00057735
Iteration 689/1000 | Loss: 0.00031484
Iteration 690/1000 | Loss: 0.00036526
Iteration 691/1000 | Loss: 0.00036845
Iteration 692/1000 | Loss: 0.00039725
Iteration 693/1000 | Loss: 0.00046500
Iteration 694/1000 | Loss: 0.00088598
Iteration 695/1000 | Loss: 0.00045979
Iteration 696/1000 | Loss: 0.00042979
Iteration 697/1000 | Loss: 0.00020026
Iteration 698/1000 | Loss: 0.00020581
Iteration 699/1000 | Loss: 0.00028769
Iteration 700/1000 | Loss: 0.00026694
Iteration 701/1000 | Loss: 0.00185621
Iteration 702/1000 | Loss: 0.00112795
Iteration 703/1000 | Loss: 0.00070656
Iteration 704/1000 | Loss: 0.00073974
Iteration 705/1000 | Loss: 0.00201819
Iteration 706/1000 | Loss: 0.00130083
Iteration 707/1000 | Loss: 0.00062323
Iteration 708/1000 | Loss: 0.00046503
Iteration 709/1000 | Loss: 0.00038952
Iteration 710/1000 | Loss: 0.00138424
Iteration 711/1000 | Loss: 0.00020615
Iteration 712/1000 | Loss: 0.00016535
Iteration 713/1000 | Loss: 0.00027525
Iteration 714/1000 | Loss: 0.00027820
Iteration 715/1000 | Loss: 0.00017520
Iteration 716/1000 | Loss: 0.00039577
Iteration 717/1000 | Loss: 0.00033037
Iteration 718/1000 | Loss: 0.00030908
Iteration 719/1000 | Loss: 0.00030446
Iteration 720/1000 | Loss: 0.00029243
Iteration 721/1000 | Loss: 0.00014405
Iteration 722/1000 | Loss: 0.00021091
Iteration 723/1000 | Loss: 0.00018413
Iteration 724/1000 | Loss: 0.00015515
Iteration 725/1000 | Loss: 0.00013679
Iteration 726/1000 | Loss: 0.00011834
Iteration 727/1000 | Loss: 0.00020352
Iteration 728/1000 | Loss: 0.00027187
Iteration 729/1000 | Loss: 0.00035250
Iteration 730/1000 | Loss: 0.00041662
Iteration 731/1000 | Loss: 0.00028424
Iteration 732/1000 | Loss: 0.00098407
Iteration 733/1000 | Loss: 0.00069048
Iteration 734/1000 | Loss: 0.00075893
Iteration 735/1000 | Loss: 0.00032125
Iteration 736/1000 | Loss: 0.00063400
Iteration 737/1000 | Loss: 0.00060046
Iteration 738/1000 | Loss: 0.00111049
Iteration 739/1000 | Loss: 0.00008348
Iteration 740/1000 | Loss: 0.00006100
Iteration 741/1000 | Loss: 0.00015657
Iteration 742/1000 | Loss: 0.00009403
Iteration 743/1000 | Loss: 0.00008656
Iteration 744/1000 | Loss: 0.00012063
Iteration 745/1000 | Loss: 0.00020301
Iteration 746/1000 | Loss: 0.00116791
Iteration 747/1000 | Loss: 0.00044227
Iteration 748/1000 | Loss: 0.00025606
Iteration 749/1000 | Loss: 0.00018938
Iteration 750/1000 | Loss: 0.00024840
Iteration 751/1000 | Loss: 0.00020363
Iteration 752/1000 | Loss: 0.00229166
Iteration 753/1000 | Loss: 0.00029063
Iteration 754/1000 | Loss: 0.00020520
Iteration 755/1000 | Loss: 0.00014373
Iteration 756/1000 | Loss: 0.00013845
Iteration 757/1000 | Loss: 0.00013766
Iteration 758/1000 | Loss: 0.00013054
Iteration 759/1000 | Loss: 0.00014625
Iteration 760/1000 | Loss: 0.00072680
Iteration 761/1000 | Loss: 0.00022299
Iteration 762/1000 | Loss: 0.00018704
Iteration 763/1000 | Loss: 0.00017504
Iteration 764/1000 | Loss: 0.00012870
Iteration 765/1000 | Loss: 0.00017919
Iteration 766/1000 | Loss: 0.00020321
Iteration 767/1000 | Loss: 0.00020680
Iteration 768/1000 | Loss: 0.00027296
Iteration 769/1000 | Loss: 0.00016248
Iteration 770/1000 | Loss: 0.00018455
Iteration 771/1000 | Loss: 0.00019755
Iteration 772/1000 | Loss: 0.00021380
Iteration 773/1000 | Loss: 0.00018313
Iteration 774/1000 | Loss: 0.00018576
Iteration 775/1000 | Loss: 0.00016974
Iteration 776/1000 | Loss: 0.00021658
Iteration 777/1000 | Loss: 0.00013038
Iteration 778/1000 | Loss: 0.00017932
Iteration 779/1000 | Loss: 0.00021856
Iteration 780/1000 | Loss: 0.00017410
Iteration 781/1000 | Loss: 0.00032355
Iteration 782/1000 | Loss: 0.00033406
Iteration 783/1000 | Loss: 0.00035626
Iteration 784/1000 | Loss: 0.00030410
Iteration 785/1000 | Loss: 0.00025455
Iteration 786/1000 | Loss: 0.00122688
Iteration 787/1000 | Loss: 0.00032669
Iteration 788/1000 | Loss: 0.00026713
Iteration 789/1000 | Loss: 0.00024854
Iteration 790/1000 | Loss: 0.00023055
Iteration 791/1000 | Loss: 0.00071155
Iteration 792/1000 | Loss: 0.00009065
Iteration 793/1000 | Loss: 0.00034199
Iteration 794/1000 | Loss: 0.00114895
Iteration 795/1000 | Loss: 0.00031434
Iteration 796/1000 | Loss: 0.00034465
Iteration 797/1000 | Loss: 0.00025786
Iteration 798/1000 | Loss: 0.00032515
Iteration 799/1000 | Loss: 0.00047122
Iteration 800/1000 | Loss: 0.00041934
Iteration 801/1000 | Loss: 0.00025749
Iteration 802/1000 | Loss: 0.00045145
Iteration 803/1000 | Loss: 0.00029805
Iteration 804/1000 | Loss: 0.00036843
Iteration 805/1000 | Loss: 0.00042009
Iteration 806/1000 | Loss: 0.00041395
Iteration 807/1000 | Loss: 0.00020599
Iteration 808/1000 | Loss: 0.00038040
Iteration 809/1000 | Loss: 0.00045203
Iteration 810/1000 | Loss: 0.00033708
Iteration 811/1000 | Loss: 0.00032335
Iteration 812/1000 | Loss: 0.00046953
Iteration 813/1000 | Loss: 0.00031771
Iteration 814/1000 | Loss: 0.00032632
Iteration 815/1000 | Loss: 0.00041953
Iteration 816/1000 | Loss: 0.00018550
Iteration 817/1000 | Loss: 0.00040773
Iteration 818/1000 | Loss: 0.00046227
Iteration 819/1000 | Loss: 0.00035801
Iteration 820/1000 | Loss: 0.00037353
Iteration 821/1000 | Loss: 0.00016757
Iteration 822/1000 | Loss: 0.00027395
Iteration 823/1000 | Loss: 0.00016629
Iteration 824/1000 | Loss: 0.00012883
Iteration 825/1000 | Loss: 0.00041799
Iteration 826/1000 | Loss: 0.00017937
Iteration 827/1000 | Loss: 0.00009333
Iteration 828/1000 | Loss: 0.00019767
Iteration 829/1000 | Loss: 0.00019201
Iteration 830/1000 | Loss: 0.00037386
Iteration 831/1000 | Loss: 0.00015183
Iteration 832/1000 | Loss: 0.00030613
Iteration 833/1000 | Loss: 0.00031557
Iteration 834/1000 | Loss: 0.00031006
Iteration 835/1000 | Loss: 0.00019542
Iteration 836/1000 | Loss: 0.00029778
Iteration 837/1000 | Loss: 0.00015380
Iteration 838/1000 | Loss: 0.00018645
Iteration 839/1000 | Loss: 0.00015660
Iteration 840/1000 | Loss: 0.00006593
Iteration 841/1000 | Loss: 0.00022435
Iteration 842/1000 | Loss: 0.00021794
Iteration 843/1000 | Loss: 0.00023532
Iteration 844/1000 | Loss: 0.00019522
Iteration 845/1000 | Loss: 0.00008012
Iteration 846/1000 | Loss: 0.00014862
Iteration 847/1000 | Loss: 0.00016887
Iteration 848/1000 | Loss: 0.00018218
Iteration 849/1000 | Loss: 0.00016677
Iteration 850/1000 | Loss: 0.00017495
Iteration 851/1000 | Loss: 0.00017677
Iteration 852/1000 | Loss: 0.00020037
Iteration 853/1000 | Loss: 0.00016221
Iteration 854/1000 | Loss: 0.00031719
Iteration 855/1000 | Loss: 0.00011468
Iteration 856/1000 | Loss: 0.00041064
Iteration 857/1000 | Loss: 0.00018813
Iteration 858/1000 | Loss: 0.00007223
Iteration 859/1000 | Loss: 0.00020394
Iteration 860/1000 | Loss: 0.00009280
Iteration 861/1000 | Loss: 0.00006651
Iteration 862/1000 | Loss: 0.00043561
Iteration 863/1000 | Loss: 0.00029983
Iteration 864/1000 | Loss: 0.00040020
Iteration 865/1000 | Loss: 0.00029864
Iteration 866/1000 | Loss: 0.00036165
Iteration 867/1000 | Loss: 0.00029938
Iteration 868/1000 | Loss: 0.00017702
Iteration 869/1000 | Loss: 0.00022496
Iteration 870/1000 | Loss: 0.00011203
Iteration 871/1000 | Loss: 0.00045596
Iteration 872/1000 | Loss: 0.00064847
Iteration 873/1000 | Loss: 0.00045688
Iteration 874/1000 | Loss: 0.00049133
Iteration 875/1000 | Loss: 0.00030188
Iteration 876/1000 | Loss: 0.00046674
Iteration 877/1000 | Loss: 0.00026398
Iteration 878/1000 | Loss: 0.00063701
Iteration 879/1000 | Loss: 0.00024981
Iteration 880/1000 | Loss: 0.00039152
Iteration 881/1000 | Loss: 0.00025082
Iteration 882/1000 | Loss: 0.00021068
Iteration 883/1000 | Loss: 0.00011615
Iteration 884/1000 | Loss: 0.00012733
Iteration 885/1000 | Loss: 0.00013986
Iteration 886/1000 | Loss: 0.00015706
Iteration 887/1000 | Loss: 0.00014066
Iteration 888/1000 | Loss: 0.00017972
Iteration 889/1000 | Loss: 0.00011851
Iteration 890/1000 | Loss: 0.00011609
Iteration 891/1000 | Loss: 0.00012179
Iteration 892/1000 | Loss: 0.00017198
Iteration 893/1000 | Loss: 0.00018621
Iteration 894/1000 | Loss: 0.00018928
Iteration 895/1000 | Loss: 0.00019353
Iteration 896/1000 | Loss: 0.00018200
Iteration 897/1000 | Loss: 0.00017500
Iteration 898/1000 | Loss: 0.00019178
Iteration 899/1000 | Loss: 0.00018666
Iteration 900/1000 | Loss: 0.00017951
Iteration 901/1000 | Loss: 0.00018556
Iteration 902/1000 | Loss: 0.00019495
Iteration 903/1000 | Loss: 0.00018197
Iteration 904/1000 | Loss: 0.00018694
Iteration 905/1000 | Loss: 0.00011113
Iteration 906/1000 | Loss: 0.00013428
Iteration 907/1000 | Loss: 0.00018631
Iteration 908/1000 | Loss: 0.00017470
Iteration 909/1000 | Loss: 0.00016976
Iteration 910/1000 | Loss: 0.00018101
Iteration 911/1000 | Loss: 0.00011260
Iteration 912/1000 | Loss: 0.00021114
Iteration 913/1000 | Loss: 0.00014864
Iteration 914/1000 | Loss: 0.00055790
Iteration 915/1000 | Loss: 0.00041315
Iteration 916/1000 | Loss: 0.00013614
Iteration 917/1000 | Loss: 0.00012657
Iteration 918/1000 | Loss: 0.00011979
Iteration 919/1000 | Loss: 0.00010254
Iteration 920/1000 | Loss: 0.00010743
Iteration 921/1000 | Loss: 0.00018645
Iteration 922/1000 | Loss: 0.00015299
Iteration 923/1000 | Loss: 0.00013618
Iteration 924/1000 | Loss: 0.00007298
Iteration 925/1000 | Loss: 0.00014923
Iteration 926/1000 | Loss: 0.00017219
Iteration 927/1000 | Loss: 0.00014255
Iteration 928/1000 | Loss: 0.00016685
Iteration 929/1000 | Loss: 0.00010160
Iteration 930/1000 | Loss: 0.00014673
Iteration 931/1000 | Loss: 0.00017217
Iteration 932/1000 | Loss: 0.00023007
Iteration 933/1000 | Loss: 0.00015231
Iteration 934/1000 | Loss: 0.00020141
Iteration 935/1000 | Loss: 0.00018254
Iteration 936/1000 | Loss: 0.00020357
Iteration 937/1000 | Loss: 0.00017663
Iteration 938/1000 | Loss: 0.00021516
Iteration 939/1000 | Loss: 0.00005504
Iteration 940/1000 | Loss: 0.00017146
Iteration 941/1000 | Loss: 0.00004890
Iteration 942/1000 | Loss: 0.00008131
Iteration 943/1000 | Loss: 0.00013342
Iteration 944/1000 | Loss: 0.00012090
Iteration 945/1000 | Loss: 0.00003790
Iteration 946/1000 | Loss: 0.00019441
Iteration 947/1000 | Loss: 0.00018739
Iteration 948/1000 | Loss: 0.00008454
Iteration 949/1000 | Loss: 0.00012303
Iteration 950/1000 | Loss: 0.00010741
Iteration 951/1000 | Loss: 0.00013313
Iteration 952/1000 | Loss: 0.00010644
Iteration 953/1000 | Loss: 0.00018240
Iteration 954/1000 | Loss: 0.00005759
Iteration 955/1000 | Loss: 0.00007733
Iteration 956/1000 | Loss: 0.00010094
Iteration 957/1000 | Loss: 0.00008931
Iteration 958/1000 | Loss: 0.00009278
Iteration 959/1000 | Loss: 0.00015619
Iteration 960/1000 | Loss: 0.00013172
Iteration 961/1000 | Loss: 0.00005827
Iteration 962/1000 | Loss: 0.00008805
Iteration 963/1000 | Loss: 0.00023991
Iteration 964/1000 | Loss: 0.00013396
Iteration 965/1000 | Loss: 0.00007754
Iteration 966/1000 | Loss: 0.00014785
Iteration 967/1000 | Loss: 0.00022235
Iteration 968/1000 | Loss: 0.00021585
Iteration 969/1000 | Loss: 0.00017518
Iteration 970/1000 | Loss: 0.00006961
Iteration 971/1000 | Loss: 0.00005295
Iteration 972/1000 | Loss: 0.00003263
Iteration 973/1000 | Loss: 0.00002970
Iteration 974/1000 | Loss: 0.00002845
Iteration 975/1000 | Loss: 0.00017524
Iteration 976/1000 | Loss: 0.00011814
Iteration 977/1000 | Loss: 0.00012429
Iteration 978/1000 | Loss: 0.00010062
Iteration 979/1000 | Loss: 0.00016374
Iteration 980/1000 | Loss: 0.00013009
Iteration 981/1000 | Loss: 0.00014619
Iteration 982/1000 | Loss: 0.00011257
Iteration 983/1000 | Loss: 0.00027555
Iteration 984/1000 | Loss: 0.00020634
Iteration 985/1000 | Loss: 0.00061089
Iteration 986/1000 | Loss: 0.00019536
Iteration 987/1000 | Loss: 0.00002991
Iteration 988/1000 | Loss: 0.00002735
Iteration 989/1000 | Loss: 0.00006135
Iteration 990/1000 | Loss: 0.00043761
Iteration 991/1000 | Loss: 0.00015950
Iteration 992/1000 | Loss: 0.00123296
Iteration 993/1000 | Loss: 0.00061998
Iteration 994/1000 | Loss: 0.00116625
Iteration 995/1000 | Loss: 0.00006474
Iteration 996/1000 | Loss: 0.00004044
Iteration 997/1000 | Loss: 0.00002883
Iteration 998/1000 | Loss: 0.00002561
Iteration 999/1000 | Loss: 0.00002438
Iteration 1000/1000 | Loss: 0.00002401

Optimization complete. Final v2v error: 3.8915717601776123 mm

Highest mean error: 12.270903587341309 mm for frame 178

Lowest mean error: 3.3901801109313965 mm for frame 0

Saving results

Total time: 1681.5303831100464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026571
Iteration 2/25 | Loss: 0.00230490
Iteration 3/25 | Loss: 0.00146163
Iteration 4/25 | Loss: 0.00121281
Iteration 5/25 | Loss: 0.00115934
Iteration 6/25 | Loss: 0.00119413
Iteration 7/25 | Loss: 0.00107391
Iteration 8/25 | Loss: 0.00096185
Iteration 9/25 | Loss: 0.00090306
Iteration 10/25 | Loss: 0.00085986
Iteration 11/25 | Loss: 0.00083490
Iteration 12/25 | Loss: 0.00082017
Iteration 13/25 | Loss: 0.00080976
Iteration 14/25 | Loss: 0.00080656
Iteration 15/25 | Loss: 0.00080264
Iteration 16/25 | Loss: 0.00079849
Iteration 17/25 | Loss: 0.00079753
Iteration 18/25 | Loss: 0.00079689
Iteration 19/25 | Loss: 0.00080439
Iteration 20/25 | Loss: 0.00080731
Iteration 21/25 | Loss: 0.00080167
Iteration 22/25 | Loss: 0.00079727
Iteration 23/25 | Loss: 0.00079412
Iteration 24/25 | Loss: 0.00079289
Iteration 25/25 | Loss: 0.00079611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55291784
Iteration 2/25 | Loss: 0.00075942
Iteration 3/25 | Loss: 0.00082746
Iteration 4/25 | Loss: 0.00067267
Iteration 5/25 | Loss: 0.00067267
Iteration 6/25 | Loss: 0.00067266
Iteration 7/25 | Loss: 0.00067266
Iteration 8/25 | Loss: 0.00067266
Iteration 9/25 | Loss: 0.00067266
Iteration 10/25 | Loss: 0.00067266
Iteration 11/25 | Loss: 0.00067266
Iteration 12/25 | Loss: 0.00067266
Iteration 13/25 | Loss: 0.00067266
Iteration 14/25 | Loss: 0.00067266
Iteration 15/25 | Loss: 0.00067266
Iteration 16/25 | Loss: 0.00067266
Iteration 17/25 | Loss: 0.00067266
Iteration 18/25 | Loss: 0.00067266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006726632709614933, 0.0006726632709614933, 0.0006726632709614933, 0.0006726632709614933, 0.0006726632709614933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006726632709614933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067266
Iteration 2/1000 | Loss: 0.00021637
Iteration 3/1000 | Loss: 0.00003714
Iteration 4/1000 | Loss: 0.00003024
Iteration 5/1000 | Loss: 0.00021446
Iteration 6/1000 | Loss: 0.00003346
Iteration 7/1000 | Loss: 0.00002832
Iteration 8/1000 | Loss: 0.00014514
Iteration 9/1000 | Loss: 0.00043901
Iteration 10/1000 | Loss: 0.00038600
Iteration 11/1000 | Loss: 0.00004243
Iteration 12/1000 | Loss: 0.00003289
Iteration 13/1000 | Loss: 0.00002564
Iteration 14/1000 | Loss: 0.00002272
Iteration 15/1000 | Loss: 0.00004262
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00003482
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00002841
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00001609
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00004908
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001518
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001498
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00004480
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001480
Iteration 107/1000 | Loss: 0.00001480
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001477
Iteration 127/1000 | Loss: 0.00001477
Iteration 128/1000 | Loss: 0.00001477
Iteration 129/1000 | Loss: 0.00001477
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001476
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00003874
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4724501852469984e-05, 1.4724501852469984e-05, 1.4724501852469984e-05, 1.4724501852469984e-05, 1.4724501852469984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4724501852469984e-05

Optimization complete. Final v2v error: 3.1370561122894287 mm

Highest mean error: 5.338541507720947 mm for frame 66

Lowest mean error: 2.563913583755493 mm for frame 147

Saving results

Total time: 103.87483429908752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770628
Iteration 2/25 | Loss: 0.00089709
Iteration 3/25 | Loss: 0.00078208
Iteration 4/25 | Loss: 0.00075014
Iteration 5/25 | Loss: 0.00074358
Iteration 6/25 | Loss: 0.00074186
Iteration 7/25 | Loss: 0.00074161
Iteration 8/25 | Loss: 0.00074161
Iteration 9/25 | Loss: 0.00074161
Iteration 10/25 | Loss: 0.00074161
Iteration 11/25 | Loss: 0.00074161
Iteration 12/25 | Loss: 0.00074161
Iteration 13/25 | Loss: 0.00074161
Iteration 14/25 | Loss: 0.00074161
Iteration 15/25 | Loss: 0.00074161
Iteration 16/25 | Loss: 0.00074161
Iteration 17/25 | Loss: 0.00074161
Iteration 18/25 | Loss: 0.00074161
Iteration 19/25 | Loss: 0.00074161
Iteration 20/25 | Loss: 0.00074161
Iteration 21/25 | Loss: 0.00074161
Iteration 22/25 | Loss: 0.00074161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007416067528538406, 0.0007416067528538406, 0.0007416067528538406, 0.0007416067528538406, 0.0007416067528538406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007416067528538406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35205543
Iteration 2/25 | Loss: 0.00039701
Iteration 3/25 | Loss: 0.00039694
Iteration 4/25 | Loss: 0.00039694
Iteration 5/25 | Loss: 0.00039694
Iteration 6/25 | Loss: 0.00039694
Iteration 7/25 | Loss: 0.00039694
Iteration 8/25 | Loss: 0.00039694
Iteration 9/25 | Loss: 0.00039694
Iteration 10/25 | Loss: 0.00039694
Iteration 11/25 | Loss: 0.00039694
Iteration 12/25 | Loss: 0.00039694
Iteration 13/25 | Loss: 0.00039694
Iteration 14/25 | Loss: 0.00039694
Iteration 15/25 | Loss: 0.00039694
Iteration 16/25 | Loss: 0.00039694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00039693593862466514, 0.00039693593862466514, 0.00039693593862466514, 0.00039693593862466514, 0.00039693593862466514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039693593862466514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039694
Iteration 2/1000 | Loss: 0.00002969
Iteration 3/1000 | Loss: 0.00002147
Iteration 4/1000 | Loss: 0.00001977
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001754
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001604
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001593
Iteration 18/1000 | Loss: 0.00001591
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001556
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5557581718894653e-05, 1.5557581718894653e-05, 1.5557581718894653e-05, 1.5557581718894653e-05, 1.5557581718894653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5557581718894653e-05

Optimization complete. Final v2v error: 3.3299026489257812 mm

Highest mean error: 4.448796272277832 mm for frame 5

Lowest mean error: 2.980346441268921 mm for frame 100

Saving results

Total time: 40.978333711624146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037998
Iteration 2/25 | Loss: 0.00149718
Iteration 3/25 | Loss: 0.00097155
Iteration 4/25 | Loss: 0.00087470
Iteration 5/25 | Loss: 0.00084818
Iteration 6/25 | Loss: 0.00084192
Iteration 7/25 | Loss: 0.00083958
Iteration 8/25 | Loss: 0.00083516
Iteration 9/25 | Loss: 0.00082863
Iteration 10/25 | Loss: 0.00082676
Iteration 11/25 | Loss: 0.00082596
Iteration 12/25 | Loss: 0.00082585
Iteration 13/25 | Loss: 0.00082585
Iteration 14/25 | Loss: 0.00082585
Iteration 15/25 | Loss: 0.00082583
Iteration 16/25 | Loss: 0.00082582
Iteration 17/25 | Loss: 0.00082582
Iteration 18/25 | Loss: 0.00082582
Iteration 19/25 | Loss: 0.00082582
Iteration 20/25 | Loss: 0.00082582
Iteration 21/25 | Loss: 0.00082582
Iteration 22/25 | Loss: 0.00082582
Iteration 23/25 | Loss: 0.00082582
Iteration 24/25 | Loss: 0.00082582
Iteration 25/25 | Loss: 0.00082582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49973190
Iteration 2/25 | Loss: 0.00038767
Iteration 3/25 | Loss: 0.00038767
Iteration 4/25 | Loss: 0.00038767
Iteration 5/25 | Loss: 0.00038767
Iteration 6/25 | Loss: 0.00038767
Iteration 7/25 | Loss: 0.00038767
Iteration 8/25 | Loss: 0.00038767
Iteration 9/25 | Loss: 0.00038767
Iteration 10/25 | Loss: 0.00038767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0003876679402310401, 0.0003876679402310401, 0.0003876679402310401, 0.0003876679402310401, 0.0003876679402310401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003876679402310401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038767
Iteration 2/1000 | Loss: 0.00003498
Iteration 3/1000 | Loss: 0.00002556
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00002278
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002219
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002218
Iteration 11/1000 | Loss: 0.00002206
Iteration 12/1000 | Loss: 0.00002187
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00002170
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002164
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002159
Iteration 20/1000 | Loss: 0.00002155
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002154
Iteration 23/1000 | Loss: 0.00002154
Iteration 24/1000 | Loss: 0.00002153
Iteration 25/1000 | Loss: 0.00002153
Iteration 26/1000 | Loss: 0.00002151
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002150
Iteration 31/1000 | Loss: 0.00002150
Iteration 32/1000 | Loss: 0.00002150
Iteration 33/1000 | Loss: 0.00002150
Iteration 34/1000 | Loss: 0.00002150
Iteration 35/1000 | Loss: 0.00002149
Iteration 36/1000 | Loss: 0.00002148
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002148
Iteration 40/1000 | Loss: 0.00002148
Iteration 41/1000 | Loss: 0.00002148
Iteration 42/1000 | Loss: 0.00002148
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00002147
Iteration 47/1000 | Loss: 0.00002147
Iteration 48/1000 | Loss: 0.00002147
Iteration 49/1000 | Loss: 0.00002147
Iteration 50/1000 | Loss: 0.00002147
Iteration 51/1000 | Loss: 0.00002147
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002147
Iteration 54/1000 | Loss: 0.00002147
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002143
Iteration 58/1000 | Loss: 0.00002143
Iteration 59/1000 | Loss: 0.00002143
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002142
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002141
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002141
Iteration 72/1000 | Loss: 0.00002141
Iteration 73/1000 | Loss: 0.00002141
Iteration 74/1000 | Loss: 0.00002141
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002140
Iteration 77/1000 | Loss: 0.00002140
Iteration 78/1000 | Loss: 0.00002139
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002136
Iteration 83/1000 | Loss: 0.00002136
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002135
Iteration 86/1000 | Loss: 0.00002135
Iteration 87/1000 | Loss: 0.00002135
Iteration 88/1000 | Loss: 0.00002135
Iteration 89/1000 | Loss: 0.00002135
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002133
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00002133
Iteration 112/1000 | Loss: 0.00002133
Iteration 113/1000 | Loss: 0.00002133
Iteration 114/1000 | Loss: 0.00002133
Iteration 115/1000 | Loss: 0.00002133
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00002133
Iteration 118/1000 | Loss: 0.00002133
Iteration 119/1000 | Loss: 0.00002133
Iteration 120/1000 | Loss: 0.00002133
Iteration 121/1000 | Loss: 0.00002133
Iteration 122/1000 | Loss: 0.00002133
Iteration 123/1000 | Loss: 0.00002133
Iteration 124/1000 | Loss: 0.00002133
Iteration 125/1000 | Loss: 0.00002133
Iteration 126/1000 | Loss: 0.00002133
Iteration 127/1000 | Loss: 0.00002133
Iteration 128/1000 | Loss: 0.00002133
Iteration 129/1000 | Loss: 0.00002133
Iteration 130/1000 | Loss: 0.00002133
Iteration 131/1000 | Loss: 0.00002133
Iteration 132/1000 | Loss: 0.00002133
Iteration 133/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.1331146854208782e-05, 2.1331146854208782e-05, 2.1331146854208782e-05, 2.1331146854208782e-05, 2.1331146854208782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1331146854208782e-05

Optimization complete. Final v2v error: 3.9384965896606445 mm

Highest mean error: 4.0432305335998535 mm for frame 55

Lowest mean error: 3.7608675956726074 mm for frame 1

Saving results

Total time: 44.96408486366272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005804
Iteration 2/25 | Loss: 0.00312515
Iteration 3/25 | Loss: 0.00198875
Iteration 4/25 | Loss: 0.00169927
Iteration 5/25 | Loss: 0.00138421
Iteration 6/25 | Loss: 0.00125462
Iteration 7/25 | Loss: 0.00122620
Iteration 8/25 | Loss: 0.00120537
Iteration 9/25 | Loss: 0.00123201
Iteration 10/25 | Loss: 0.00113671
Iteration 11/25 | Loss: 0.00110055
Iteration 12/25 | Loss: 0.00108122
Iteration 13/25 | Loss: 0.00103904
Iteration 14/25 | Loss: 0.00098907
Iteration 15/25 | Loss: 0.00097410
Iteration 16/25 | Loss: 0.00095659
Iteration 17/25 | Loss: 0.00094266
Iteration 18/25 | Loss: 0.00094176
Iteration 19/25 | Loss: 0.00093781
Iteration 20/25 | Loss: 0.00092976
Iteration 21/25 | Loss: 0.00092379
Iteration 22/25 | Loss: 0.00092768
Iteration 23/25 | Loss: 0.00092121
Iteration 24/25 | Loss: 0.00091761
Iteration 25/25 | Loss: 0.00091640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50465274
Iteration 2/25 | Loss: 0.00103864
Iteration 3/25 | Loss: 0.00100266
Iteration 4/25 | Loss: 0.00100266
Iteration 5/25 | Loss: 0.00100266
Iteration 6/25 | Loss: 0.00100266
Iteration 7/25 | Loss: 0.00100266
Iteration 8/25 | Loss: 0.00100266
Iteration 9/25 | Loss: 0.00100266
Iteration 10/25 | Loss: 0.00100266
Iteration 11/25 | Loss: 0.00100266
Iteration 12/25 | Loss: 0.00100266
Iteration 13/25 | Loss: 0.00100266
Iteration 14/25 | Loss: 0.00100266
Iteration 15/25 | Loss: 0.00100266
Iteration 16/25 | Loss: 0.00100266
Iteration 17/25 | Loss: 0.00100266
Iteration 18/25 | Loss: 0.00100266
Iteration 19/25 | Loss: 0.00100266
Iteration 20/25 | Loss: 0.00100266
Iteration 21/25 | Loss: 0.00100266
Iteration 22/25 | Loss: 0.00100266
Iteration 23/25 | Loss: 0.00100266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010026601376011968, 0.0010026601376011968, 0.0010026601376011968, 0.0010026601376011968, 0.0010026601376011968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010026601376011968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100266
Iteration 2/1000 | Loss: 0.00017456
Iteration 3/1000 | Loss: 0.00015665
Iteration 4/1000 | Loss: 0.00010124
Iteration 5/1000 | Loss: 0.00008764
Iteration 6/1000 | Loss: 0.00007720
Iteration 7/1000 | Loss: 0.00034828
Iteration 8/1000 | Loss: 0.00007986
Iteration 9/1000 | Loss: 0.00006832
Iteration 10/1000 | Loss: 0.00006509
Iteration 11/1000 | Loss: 0.00006351
Iteration 12/1000 | Loss: 0.00006220
Iteration 13/1000 | Loss: 0.00006126
Iteration 14/1000 | Loss: 0.00006025
Iteration 15/1000 | Loss: 0.00005926
Iteration 16/1000 | Loss: 0.00005812
Iteration 17/1000 | Loss: 0.00450396
Iteration 18/1000 | Loss: 0.00363581
Iteration 19/1000 | Loss: 0.00048719
Iteration 20/1000 | Loss: 0.00052108
Iteration 21/1000 | Loss: 0.00008357
Iteration 22/1000 | Loss: 0.00013430
Iteration 23/1000 | Loss: 0.00006888
Iteration 24/1000 | Loss: 0.00005049
Iteration 25/1000 | Loss: 0.00011729
Iteration 26/1000 | Loss: 0.00004715
Iteration 27/1000 | Loss: 0.00008692
Iteration 28/1000 | Loss: 0.00036200
Iteration 29/1000 | Loss: 0.00004427
Iteration 30/1000 | Loss: 0.00003747
Iteration 31/1000 | Loss: 0.00004894
Iteration 32/1000 | Loss: 0.00007662
Iteration 33/1000 | Loss: 0.00006799
Iteration 34/1000 | Loss: 0.00003563
Iteration 35/1000 | Loss: 0.00003453
Iteration 36/1000 | Loss: 0.00004517
Iteration 37/1000 | Loss: 0.00003361
Iteration 38/1000 | Loss: 0.00003745
Iteration 39/1000 | Loss: 0.00003300
Iteration 40/1000 | Loss: 0.00004731
Iteration 41/1000 | Loss: 0.00003273
Iteration 42/1000 | Loss: 0.00003259
Iteration 43/1000 | Loss: 0.00003254
Iteration 44/1000 | Loss: 0.00003243
Iteration 45/1000 | Loss: 0.00003237
Iteration 46/1000 | Loss: 0.00004137
Iteration 47/1000 | Loss: 0.00006617
Iteration 48/1000 | Loss: 0.00004094
Iteration 49/1000 | Loss: 0.00003215
Iteration 50/1000 | Loss: 0.00003195
Iteration 51/1000 | Loss: 0.00003185
Iteration 52/1000 | Loss: 0.00003174
Iteration 53/1000 | Loss: 0.00003174
Iteration 54/1000 | Loss: 0.00003173
Iteration 55/1000 | Loss: 0.00084215
Iteration 56/1000 | Loss: 0.00015094
Iteration 57/1000 | Loss: 0.00005619
Iteration 58/1000 | Loss: 0.00003480
Iteration 59/1000 | Loss: 0.00004173
Iteration 60/1000 | Loss: 0.00003129
Iteration 61/1000 | Loss: 0.00003048
Iteration 62/1000 | Loss: 0.00002992
Iteration 63/1000 | Loss: 0.00002962
Iteration 64/1000 | Loss: 0.00002933
Iteration 65/1000 | Loss: 0.00005926
Iteration 66/1000 | Loss: 0.00002901
Iteration 67/1000 | Loss: 0.00002894
Iteration 68/1000 | Loss: 0.00002893
Iteration 69/1000 | Loss: 0.00002893
Iteration 70/1000 | Loss: 0.00002890
Iteration 71/1000 | Loss: 0.00002890
Iteration 72/1000 | Loss: 0.00002885
Iteration 73/1000 | Loss: 0.00002885
Iteration 74/1000 | Loss: 0.00002883
Iteration 75/1000 | Loss: 0.00002883
Iteration 76/1000 | Loss: 0.00002882
Iteration 77/1000 | Loss: 0.00002882
Iteration 78/1000 | Loss: 0.00002882
Iteration 79/1000 | Loss: 0.00002881
Iteration 80/1000 | Loss: 0.00002880
Iteration 81/1000 | Loss: 0.00002879
Iteration 82/1000 | Loss: 0.00002877
Iteration 83/1000 | Loss: 0.00003231
Iteration 84/1000 | Loss: 0.00002873
Iteration 85/1000 | Loss: 0.00002872
Iteration 86/1000 | Loss: 0.00002871
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002871
Iteration 89/1000 | Loss: 0.00002870
Iteration 90/1000 | Loss: 0.00002870
Iteration 91/1000 | Loss: 0.00002870
Iteration 92/1000 | Loss: 0.00002870
Iteration 93/1000 | Loss: 0.00002870
Iteration 94/1000 | Loss: 0.00003036
Iteration 95/1000 | Loss: 0.00002870
Iteration 96/1000 | Loss: 0.00002870
Iteration 97/1000 | Loss: 0.00002869
Iteration 98/1000 | Loss: 0.00002869
Iteration 99/1000 | Loss: 0.00002869
Iteration 100/1000 | Loss: 0.00002869
Iteration 101/1000 | Loss: 0.00002869
Iteration 102/1000 | Loss: 0.00002869
Iteration 103/1000 | Loss: 0.00002869
Iteration 104/1000 | Loss: 0.00002869
Iteration 105/1000 | Loss: 0.00002869
Iteration 106/1000 | Loss: 0.00002869
Iteration 107/1000 | Loss: 0.00002868
Iteration 108/1000 | Loss: 0.00002868
Iteration 109/1000 | Loss: 0.00002867
Iteration 110/1000 | Loss: 0.00002867
Iteration 111/1000 | Loss: 0.00002867
Iteration 112/1000 | Loss: 0.00002867
Iteration 113/1000 | Loss: 0.00002867
Iteration 114/1000 | Loss: 0.00002867
Iteration 115/1000 | Loss: 0.00002866
Iteration 116/1000 | Loss: 0.00002866
Iteration 117/1000 | Loss: 0.00002866
Iteration 118/1000 | Loss: 0.00002866
Iteration 119/1000 | Loss: 0.00002866
Iteration 120/1000 | Loss: 0.00002866
Iteration 121/1000 | Loss: 0.00002865
Iteration 122/1000 | Loss: 0.00002865
Iteration 123/1000 | Loss: 0.00002864
Iteration 124/1000 | Loss: 0.00002864
Iteration 125/1000 | Loss: 0.00003649
Iteration 126/1000 | Loss: 0.00003190
Iteration 127/1000 | Loss: 0.00002915
Iteration 128/1000 | Loss: 0.00002859
Iteration 129/1000 | Loss: 0.00002859
Iteration 130/1000 | Loss: 0.00002859
Iteration 131/1000 | Loss: 0.00002859
Iteration 132/1000 | Loss: 0.00002859
Iteration 133/1000 | Loss: 0.00002859
Iteration 134/1000 | Loss: 0.00002859
Iteration 135/1000 | Loss: 0.00002859
Iteration 136/1000 | Loss: 0.00002859
Iteration 137/1000 | Loss: 0.00002859
Iteration 138/1000 | Loss: 0.00002859
Iteration 139/1000 | Loss: 0.00002859
Iteration 140/1000 | Loss: 0.00002858
Iteration 141/1000 | Loss: 0.00002858
Iteration 142/1000 | Loss: 0.00002858
Iteration 143/1000 | Loss: 0.00002858
Iteration 144/1000 | Loss: 0.00002858
Iteration 145/1000 | Loss: 0.00002858
Iteration 146/1000 | Loss: 0.00002858
Iteration 147/1000 | Loss: 0.00002858
Iteration 148/1000 | Loss: 0.00002858
Iteration 149/1000 | Loss: 0.00002858
Iteration 150/1000 | Loss: 0.00002858
Iteration 151/1000 | Loss: 0.00002858
Iteration 152/1000 | Loss: 0.00002858
Iteration 153/1000 | Loss: 0.00002858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.8580370781128295e-05, 2.8580370781128295e-05, 2.8580370781128295e-05, 2.8580370781128295e-05, 2.8580370781128295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8580370781128295e-05

Optimization complete. Final v2v error: 4.279546737670898 mm

Highest mean error: 6.036733150482178 mm for frame 98

Lowest mean error: 3.0955991744995117 mm for frame 29

Saving results

Total time: 148.94726490974426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437718
Iteration 2/25 | Loss: 0.00087136
Iteration 3/25 | Loss: 0.00077951
Iteration 4/25 | Loss: 0.00075976
Iteration 5/25 | Loss: 0.00075249
Iteration 6/25 | Loss: 0.00075102
Iteration 7/25 | Loss: 0.00075076
Iteration 8/25 | Loss: 0.00075076
Iteration 9/25 | Loss: 0.00075076
Iteration 10/25 | Loss: 0.00075076
Iteration 11/25 | Loss: 0.00075076
Iteration 12/25 | Loss: 0.00075076
Iteration 13/25 | Loss: 0.00075076
Iteration 14/25 | Loss: 0.00075076
Iteration 15/25 | Loss: 0.00075076
Iteration 16/25 | Loss: 0.00075076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007507624104619026, 0.0007507624104619026, 0.0007507624104619026, 0.0007507624104619026, 0.0007507624104619026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007507624104619026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50992942
Iteration 2/25 | Loss: 0.00047785
Iteration 3/25 | Loss: 0.00047784
Iteration 4/25 | Loss: 0.00047784
Iteration 5/25 | Loss: 0.00047784
Iteration 6/25 | Loss: 0.00047784
Iteration 7/25 | Loss: 0.00047784
Iteration 8/25 | Loss: 0.00047784
Iteration 9/25 | Loss: 0.00047784
Iteration 10/25 | Loss: 0.00047784
Iteration 11/25 | Loss: 0.00047784
Iteration 12/25 | Loss: 0.00047784
Iteration 13/25 | Loss: 0.00047784
Iteration 14/25 | Loss: 0.00047784
Iteration 15/25 | Loss: 0.00047784
Iteration 16/25 | Loss: 0.00047784
Iteration 17/25 | Loss: 0.00047784
Iteration 18/25 | Loss: 0.00047784
Iteration 19/25 | Loss: 0.00047784
Iteration 20/25 | Loss: 0.00047784
Iteration 21/25 | Loss: 0.00047784
Iteration 22/25 | Loss: 0.00047784
Iteration 23/25 | Loss: 0.00047784
Iteration 24/25 | Loss: 0.00047784
Iteration 25/25 | Loss: 0.00047784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047784
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001632
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001473
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001441
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001432
Iteration 24/1000 | Loss: 0.00001432
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001430
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001420
Iteration 47/1000 | Loss: 0.00001420
Iteration 48/1000 | Loss: 0.00001419
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001404
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.4014412954566069e-05, 1.4014412954566069e-05, 1.4014412954566069e-05, 1.4014412954566069e-05, 1.4014412954566069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4014412954566069e-05

Optimization complete. Final v2v error: 3.2095072269439697 mm

Highest mean error: 3.319282293319702 mm for frame 97

Lowest mean error: 3.0540127754211426 mm for frame 191

Saving results

Total time: 32.74649453163147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412061
Iteration 2/25 | Loss: 0.00131151
Iteration 3/25 | Loss: 0.00105433
Iteration 4/25 | Loss: 0.00100745
Iteration 5/25 | Loss: 0.00099698
Iteration 6/25 | Loss: 0.00099357
Iteration 7/25 | Loss: 0.00099229
Iteration 8/25 | Loss: 0.00099229
Iteration 9/25 | Loss: 0.00099229
Iteration 10/25 | Loss: 0.00099229
Iteration 11/25 | Loss: 0.00099229
Iteration 12/25 | Loss: 0.00099229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000992294866591692, 0.000992294866591692, 0.000992294866591692, 0.000992294866591692, 0.000992294866591692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000992294866591692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35745454
Iteration 2/25 | Loss: 0.00048104
Iteration 3/25 | Loss: 0.00048104
Iteration 4/25 | Loss: 0.00048104
Iteration 5/25 | Loss: 0.00048104
Iteration 6/25 | Loss: 0.00048104
Iteration 7/25 | Loss: 0.00048104
Iteration 8/25 | Loss: 0.00048104
Iteration 9/25 | Loss: 0.00048104
Iteration 10/25 | Loss: 0.00048104
Iteration 11/25 | Loss: 0.00048103
Iteration 12/25 | Loss: 0.00048103
Iteration 13/25 | Loss: 0.00048103
Iteration 14/25 | Loss: 0.00048103
Iteration 15/25 | Loss: 0.00048103
Iteration 16/25 | Loss: 0.00048103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00048103492008522153, 0.00048103492008522153, 0.00048103492008522153, 0.00048103492008522153, 0.00048103492008522153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048103492008522153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048103
Iteration 2/1000 | Loss: 0.00003223
Iteration 3/1000 | Loss: 0.00002206
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001718
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001636
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001629
Iteration 17/1000 | Loss: 0.00001626
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001623
Iteration 21/1000 | Loss: 0.00001623
Iteration 22/1000 | Loss: 0.00001622
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001609
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001599
Iteration 110/1000 | Loss: 0.00001599
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001599
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001598
Iteration 115/1000 | Loss: 0.00001598
Iteration 116/1000 | Loss: 0.00001598
Iteration 117/1000 | Loss: 0.00001598
Iteration 118/1000 | Loss: 0.00001598
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001597
Iteration 127/1000 | Loss: 0.00001597
Iteration 128/1000 | Loss: 0.00001597
Iteration 129/1000 | Loss: 0.00001597
Iteration 130/1000 | Loss: 0.00001597
Iteration 131/1000 | Loss: 0.00001597
Iteration 132/1000 | Loss: 0.00001597
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001596
Iteration 138/1000 | Loss: 0.00001596
Iteration 139/1000 | Loss: 0.00001596
Iteration 140/1000 | Loss: 0.00001596
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001596
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001596
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001595
Iteration 154/1000 | Loss: 0.00001595
Iteration 155/1000 | Loss: 0.00001595
Iteration 156/1000 | Loss: 0.00001595
Iteration 157/1000 | Loss: 0.00001595
Iteration 158/1000 | Loss: 0.00001595
Iteration 159/1000 | Loss: 0.00001595
Iteration 160/1000 | Loss: 0.00001595
Iteration 161/1000 | Loss: 0.00001595
Iteration 162/1000 | Loss: 0.00001595
Iteration 163/1000 | Loss: 0.00001595
Iteration 164/1000 | Loss: 0.00001595
Iteration 165/1000 | Loss: 0.00001595
Iteration 166/1000 | Loss: 0.00001595
Iteration 167/1000 | Loss: 0.00001595
Iteration 168/1000 | Loss: 0.00001595
Iteration 169/1000 | Loss: 0.00001595
Iteration 170/1000 | Loss: 0.00001595
Iteration 171/1000 | Loss: 0.00001595
Iteration 172/1000 | Loss: 0.00001595
Iteration 173/1000 | Loss: 0.00001595
Iteration 174/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.5946017811074853e-05, 1.5946017811074853e-05, 1.5946017811074853e-05, 1.5946017811074853e-05, 1.5946017811074853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5946017811074853e-05

Optimization complete. Final v2v error: 3.459691047668457 mm

Highest mean error: 4.026778697967529 mm for frame 82

Lowest mean error: 3.148111343383789 mm for frame 196

Saving results

Total time: 43.37903046607971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01218413
Iteration 2/25 | Loss: 0.00217907
Iteration 3/25 | Loss: 0.00141914
Iteration 4/25 | Loss: 0.00132541
Iteration 5/25 | Loss: 0.00130957
Iteration 6/25 | Loss: 0.00129515
Iteration 7/25 | Loss: 0.00129545
Iteration 8/25 | Loss: 0.00128352
Iteration 9/25 | Loss: 0.00128516
Iteration 10/25 | Loss: 0.00127834
Iteration 11/25 | Loss: 0.00127329
Iteration 12/25 | Loss: 0.00127602
Iteration 13/25 | Loss: 0.00127150
Iteration 14/25 | Loss: 0.00127198
Iteration 15/25 | Loss: 0.00126610
Iteration 16/25 | Loss: 0.00126444
Iteration 17/25 | Loss: 0.00125831
Iteration 18/25 | Loss: 0.00126042
Iteration 19/25 | Loss: 0.00125852
Iteration 20/25 | Loss: 0.00125887
Iteration 21/25 | Loss: 0.00126022
Iteration 22/25 | Loss: 0.00126719
Iteration 23/25 | Loss: 0.00126548
Iteration 24/25 | Loss: 0.00127016
Iteration 25/25 | Loss: 0.00126090

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07914090
Iteration 2/25 | Loss: 0.00078595
Iteration 3/25 | Loss: 0.00078593
Iteration 4/25 | Loss: 0.00078593
Iteration 5/25 | Loss: 0.00078593
Iteration 6/25 | Loss: 0.00078593
Iteration 7/25 | Loss: 0.00078593
Iteration 8/25 | Loss: 0.00078593
Iteration 9/25 | Loss: 0.00078593
Iteration 10/25 | Loss: 0.00078593
Iteration 11/25 | Loss: 0.00078593
Iteration 12/25 | Loss: 0.00078593
Iteration 13/25 | Loss: 0.00078593
Iteration 14/25 | Loss: 0.00078593
Iteration 15/25 | Loss: 0.00078593
Iteration 16/25 | Loss: 0.00078593
Iteration 17/25 | Loss: 0.00078593
Iteration 18/25 | Loss: 0.00078593
Iteration 19/25 | Loss: 0.00078593
Iteration 20/25 | Loss: 0.00078593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007859260076656938, 0.0007859260076656938, 0.0007859260076656938, 0.0007859260076656938, 0.0007859260076656938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007859260076656938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078593
Iteration 2/1000 | Loss: 0.00067621
Iteration 3/1000 | Loss: 0.00075594
Iteration 4/1000 | Loss: 0.00078589
Iteration 5/1000 | Loss: 0.00079717
Iteration 6/1000 | Loss: 0.00049046
Iteration 7/1000 | Loss: 0.00044349
Iteration 8/1000 | Loss: 0.00022173
Iteration 9/1000 | Loss: 0.00040757
Iteration 10/1000 | Loss: 0.00040479
Iteration 11/1000 | Loss: 0.00044946
Iteration 12/1000 | Loss: 0.00044336
Iteration 13/1000 | Loss: 0.00029074
Iteration 14/1000 | Loss: 0.00037011
Iteration 15/1000 | Loss: 0.00022087
Iteration 16/1000 | Loss: 0.00032869
Iteration 17/1000 | Loss: 0.00037081
Iteration 18/1000 | Loss: 0.00039576
Iteration 19/1000 | Loss: 0.00039780
Iteration 20/1000 | Loss: 0.00049274
Iteration 21/1000 | Loss: 0.00039507
Iteration 22/1000 | Loss: 0.00032094
Iteration 23/1000 | Loss: 0.00047716
Iteration 24/1000 | Loss: 0.00033199
Iteration 25/1000 | Loss: 0.00035369
Iteration 26/1000 | Loss: 0.00040519
Iteration 27/1000 | Loss: 0.00042867
Iteration 28/1000 | Loss: 0.00047046
Iteration 29/1000 | Loss: 0.00056928
Iteration 30/1000 | Loss: 0.00062915
Iteration 31/1000 | Loss: 0.00039129
Iteration 32/1000 | Loss: 0.00049002
Iteration 33/1000 | Loss: 0.00049235
Iteration 34/1000 | Loss: 0.00043885
Iteration 35/1000 | Loss: 0.00040049
Iteration 36/1000 | Loss: 0.00046050
Iteration 37/1000 | Loss: 0.00049305
Iteration 38/1000 | Loss: 0.00037200
Iteration 39/1000 | Loss: 0.00040621
Iteration 40/1000 | Loss: 0.00048348
Iteration 41/1000 | Loss: 0.00049227
Iteration 42/1000 | Loss: 0.00042040
Iteration 43/1000 | Loss: 0.00041798
Iteration 44/1000 | Loss: 0.00040545
Iteration 45/1000 | Loss: 0.00040087
Iteration 46/1000 | Loss: 0.00043775
Iteration 47/1000 | Loss: 0.00051368
Iteration 48/1000 | Loss: 0.00062666
Iteration 49/1000 | Loss: 0.00043417
Iteration 50/1000 | Loss: 0.00037332
Iteration 51/1000 | Loss: 0.00044551
Iteration 52/1000 | Loss: 0.00044303
Iteration 53/1000 | Loss: 0.00043943
Iteration 54/1000 | Loss: 0.00040519
Iteration 55/1000 | Loss: 0.00038675
Iteration 56/1000 | Loss: 0.00044172
Iteration 57/1000 | Loss: 0.00042761
Iteration 58/1000 | Loss: 0.00044450
Iteration 59/1000 | Loss: 0.00035948
Iteration 60/1000 | Loss: 0.00033913
Iteration 61/1000 | Loss: 0.00038145
Iteration 62/1000 | Loss: 0.00085390
Iteration 63/1000 | Loss: 0.00039456
Iteration 64/1000 | Loss: 0.00056062
Iteration 65/1000 | Loss: 0.00056794
Iteration 66/1000 | Loss: 0.00043403
Iteration 67/1000 | Loss: 0.00029446
Iteration 68/1000 | Loss: 0.00056423
Iteration 69/1000 | Loss: 0.00048660
Iteration 70/1000 | Loss: 0.00041389
Iteration 71/1000 | Loss: 0.00044821
Iteration 72/1000 | Loss: 0.00046055
Iteration 73/1000 | Loss: 0.00039548
Iteration 74/1000 | Loss: 0.00041024
Iteration 75/1000 | Loss: 0.00041162
Iteration 76/1000 | Loss: 0.00041302
Iteration 77/1000 | Loss: 0.00038202
Iteration 78/1000 | Loss: 0.00041147
Iteration 79/1000 | Loss: 0.00043032
Iteration 80/1000 | Loss: 0.00042269
Iteration 81/1000 | Loss: 0.00041279
Iteration 82/1000 | Loss: 0.00042642
Iteration 83/1000 | Loss: 0.00043104
Iteration 84/1000 | Loss: 0.00039747
Iteration 85/1000 | Loss: 0.00024867
Iteration 86/1000 | Loss: 0.00047180
Iteration 87/1000 | Loss: 0.00038166
Iteration 88/1000 | Loss: 0.00042694
Iteration 89/1000 | Loss: 0.00034283
Iteration 90/1000 | Loss: 0.00042365
Iteration 91/1000 | Loss: 0.00049220
Iteration 92/1000 | Loss: 0.00039204
Iteration 93/1000 | Loss: 0.00037961
Iteration 94/1000 | Loss: 0.00026169
Iteration 95/1000 | Loss: 0.00026359
Iteration 96/1000 | Loss: 0.00028206
Iteration 97/1000 | Loss: 0.00031161
Iteration 98/1000 | Loss: 0.00049140
Iteration 99/1000 | Loss: 0.00034197
Iteration 100/1000 | Loss: 0.00031515
Iteration 101/1000 | Loss: 0.00040004
Iteration 102/1000 | Loss: 0.00038861
Iteration 103/1000 | Loss: 0.00026285
Iteration 104/1000 | Loss: 0.00025188
Iteration 105/1000 | Loss: 0.00035510
Iteration 106/1000 | Loss: 0.00038005
Iteration 107/1000 | Loss: 0.00045180
Iteration 108/1000 | Loss: 0.00024288
Iteration 109/1000 | Loss: 0.00014102
Iteration 110/1000 | Loss: 0.00036038
Iteration 111/1000 | Loss: 0.00031031
Iteration 112/1000 | Loss: 0.00040716
Iteration 113/1000 | Loss: 0.00027660
Iteration 114/1000 | Loss: 0.00020753
Iteration 115/1000 | Loss: 0.00020203
Iteration 116/1000 | Loss: 0.00020448
Iteration 117/1000 | Loss: 0.00017298
Iteration 118/1000 | Loss: 0.00012385
Iteration 119/1000 | Loss: 0.00020372
Iteration 120/1000 | Loss: 0.00018701
Iteration 121/1000 | Loss: 0.00023635
Iteration 122/1000 | Loss: 0.00018721
Iteration 123/1000 | Loss: 0.00014912
Iteration 124/1000 | Loss: 0.00027497
Iteration 125/1000 | Loss: 0.00015674
Iteration 126/1000 | Loss: 0.00022358
Iteration 127/1000 | Loss: 0.00018749
Iteration 128/1000 | Loss: 0.00014412
Iteration 129/1000 | Loss: 0.00018141
Iteration 130/1000 | Loss: 0.00014406
Iteration 131/1000 | Loss: 0.00020466
Iteration 132/1000 | Loss: 0.00022037
Iteration 133/1000 | Loss: 0.00023716
Iteration 134/1000 | Loss: 0.00022857
Iteration 135/1000 | Loss: 0.00018414
Iteration 136/1000 | Loss: 0.00024547
Iteration 137/1000 | Loss: 0.00020852
Iteration 138/1000 | Loss: 0.00016260
Iteration 139/1000 | Loss: 0.00017780
Iteration 140/1000 | Loss: 0.00024189
Iteration 141/1000 | Loss: 0.00042885
Iteration 142/1000 | Loss: 0.00021952
Iteration 143/1000 | Loss: 0.00020617
Iteration 144/1000 | Loss: 0.00037464
Iteration 145/1000 | Loss: 0.00017625
Iteration 146/1000 | Loss: 0.00019629
Iteration 147/1000 | Loss: 0.00025031
Iteration 148/1000 | Loss: 0.00025567
Iteration 149/1000 | Loss: 0.00024583
Iteration 150/1000 | Loss: 0.00024193
Iteration 151/1000 | Loss: 0.00095113
Iteration 152/1000 | Loss: 0.00020387
Iteration 153/1000 | Loss: 0.00014483
Iteration 154/1000 | Loss: 0.00008527
Iteration 155/1000 | Loss: 0.00010425
Iteration 156/1000 | Loss: 0.00013760
Iteration 157/1000 | Loss: 0.00011055
Iteration 158/1000 | Loss: 0.00014332
Iteration 159/1000 | Loss: 0.00012221
Iteration 160/1000 | Loss: 0.00014971
Iteration 161/1000 | Loss: 0.00015384
Iteration 162/1000 | Loss: 0.00016105
Iteration 163/1000 | Loss: 0.00011957
Iteration 164/1000 | Loss: 0.00016795
Iteration 165/1000 | Loss: 0.00013785
Iteration 166/1000 | Loss: 0.00013287
Iteration 167/1000 | Loss: 0.00008026
Iteration 168/1000 | Loss: 0.00013869
Iteration 169/1000 | Loss: 0.00030534
Iteration 170/1000 | Loss: 0.00014698
Iteration 171/1000 | Loss: 0.00012019
Iteration 172/1000 | Loss: 0.00028882
Iteration 173/1000 | Loss: 0.00021537
Iteration 174/1000 | Loss: 0.00010314
Iteration 175/1000 | Loss: 0.00017635
Iteration 176/1000 | Loss: 0.00004869
Iteration 177/1000 | Loss: 0.00005321
Iteration 178/1000 | Loss: 0.00004978
Iteration 179/1000 | Loss: 0.00004925
Iteration 180/1000 | Loss: 0.00005218
Iteration 181/1000 | Loss: 0.00004601
Iteration 182/1000 | Loss: 0.00004674
Iteration 183/1000 | Loss: 0.00004622
Iteration 184/1000 | Loss: 0.00004379
Iteration 185/1000 | Loss: 0.00005224
Iteration 186/1000 | Loss: 0.00005162
Iteration 187/1000 | Loss: 0.00005254
Iteration 188/1000 | Loss: 0.00005132
Iteration 189/1000 | Loss: 0.00005268
Iteration 190/1000 | Loss: 0.00005038
Iteration 191/1000 | Loss: 0.00005169
Iteration 192/1000 | Loss: 0.00005166
Iteration 193/1000 | Loss: 0.00004173
Iteration 194/1000 | Loss: 0.00004338
Iteration 195/1000 | Loss: 0.00004611
Iteration 196/1000 | Loss: 0.00004017
Iteration 197/1000 | Loss: 0.00004795
Iteration 198/1000 | Loss: 0.00004501
Iteration 199/1000 | Loss: 0.00004718
Iteration 200/1000 | Loss: 0.00004496
Iteration 201/1000 | Loss: 0.00004459
Iteration 202/1000 | Loss: 0.00004488
Iteration 203/1000 | Loss: 0.00004858
Iteration 204/1000 | Loss: 0.00004483
Iteration 205/1000 | Loss: 0.00004862
Iteration 206/1000 | Loss: 0.00004194
Iteration 207/1000 | Loss: 0.00004897
Iteration 208/1000 | Loss: 0.00004596
Iteration 209/1000 | Loss: 0.00004756
Iteration 210/1000 | Loss: 0.00004546
Iteration 211/1000 | Loss: 0.00004873
Iteration 212/1000 | Loss: 0.00004571
Iteration 213/1000 | Loss: 0.00004880
Iteration 214/1000 | Loss: 0.00004577
Iteration 215/1000 | Loss: 0.00004668
Iteration 216/1000 | Loss: 0.00004429
Iteration 217/1000 | Loss: 0.00004517
Iteration 218/1000 | Loss: 0.00004537
Iteration 219/1000 | Loss: 0.00004625
Iteration 220/1000 | Loss: 0.00004471
Iteration 221/1000 | Loss: 0.00004594
Iteration 222/1000 | Loss: 0.00004443
Iteration 223/1000 | Loss: 0.00004692
Iteration 224/1000 | Loss: 0.00004606
Iteration 225/1000 | Loss: 0.00004301
Iteration 226/1000 | Loss: 0.00004478
Iteration 227/1000 | Loss: 0.00004659
Iteration 228/1000 | Loss: 0.00004462
Iteration 229/1000 | Loss: 0.00004512
Iteration 230/1000 | Loss: 0.00004463
Iteration 231/1000 | Loss: 0.00004582
Iteration 232/1000 | Loss: 0.00004667
Iteration 233/1000 | Loss: 0.00004547
Iteration 234/1000 | Loss: 0.00004677
Iteration 235/1000 | Loss: 0.00004563
Iteration 236/1000 | Loss: 0.00004483
Iteration 237/1000 | Loss: 0.00004524
Iteration 238/1000 | Loss: 0.00004490
Iteration 239/1000 | Loss: 0.00004466
Iteration 240/1000 | Loss: 0.00004429
Iteration 241/1000 | Loss: 0.00004454
Iteration 242/1000 | Loss: 0.00003955
Iteration 243/1000 | Loss: 0.00004468
Iteration 244/1000 | Loss: 0.00004842
Iteration 245/1000 | Loss: 0.00003823
Iteration 246/1000 | Loss: 0.00004986
Iteration 247/1000 | Loss: 0.00004484
Iteration 248/1000 | Loss: 0.00004619
Iteration 249/1000 | Loss: 0.00004558
Iteration 250/1000 | Loss: 0.00004430
Iteration 251/1000 | Loss: 0.00004562
Iteration 252/1000 | Loss: 0.00004466
Iteration 253/1000 | Loss: 0.00004533
Iteration 254/1000 | Loss: 0.00004947
Iteration 255/1000 | Loss: 0.00004241
Iteration 256/1000 | Loss: 0.00004440
Iteration 257/1000 | Loss: 0.00004637
Iteration 258/1000 | Loss: 0.00004547
Iteration 259/1000 | Loss: 0.00004714
Iteration 260/1000 | Loss: 0.00004527
Iteration 261/1000 | Loss: 0.00004662
Iteration 262/1000 | Loss: 0.00004542
Iteration 263/1000 | Loss: 0.00004619
Iteration 264/1000 | Loss: 0.00004515
Iteration 265/1000 | Loss: 0.00004474
Iteration 266/1000 | Loss: 0.00005324
Iteration 267/1000 | Loss: 0.00004834
Iteration 268/1000 | Loss: 0.00004965
Iteration 269/1000 | Loss: 0.00004568
Iteration 270/1000 | Loss: 0.00004454
Iteration 271/1000 | Loss: 0.00004564
Iteration 272/1000 | Loss: 0.00004634
Iteration 273/1000 | Loss: 0.00004529
Iteration 274/1000 | Loss: 0.00004594
Iteration 275/1000 | Loss: 0.00004448
Iteration 276/1000 | Loss: 0.00004432
Iteration 277/1000 | Loss: 0.00004532
Iteration 278/1000 | Loss: 0.00004754
Iteration 279/1000 | Loss: 0.00004312
Iteration 280/1000 | Loss: 0.00004466
Iteration 281/1000 | Loss: 0.00004514
Iteration 282/1000 | Loss: 0.00004675
Iteration 283/1000 | Loss: 0.00004456
Iteration 284/1000 | Loss: 0.00004749
Iteration 285/1000 | Loss: 0.00004292
Iteration 286/1000 | Loss: 0.00004411
Iteration 287/1000 | Loss: 0.00004598
Iteration 288/1000 | Loss: 0.00004451
Iteration 289/1000 | Loss: 0.00004524
Iteration 290/1000 | Loss: 0.00004661
Iteration 291/1000 | Loss: 0.00004356
Iteration 292/1000 | Loss: 0.00004594
Iteration 293/1000 | Loss: 0.00004203
Iteration 294/1000 | Loss: 0.00004410
Iteration 295/1000 | Loss: 0.00004681
Iteration 296/1000 | Loss: 0.00004944
Iteration 297/1000 | Loss: 0.00004287
Iteration 298/1000 | Loss: 0.00004650
Iteration 299/1000 | Loss: 0.00004553
Iteration 300/1000 | Loss: 0.00004257
Iteration 301/1000 | Loss: 0.00004419
Iteration 302/1000 | Loss: 0.00004492
Iteration 303/1000 | Loss: 0.00004613
Iteration 304/1000 | Loss: 0.00004471
Iteration 305/1000 | Loss: 0.00004609
Iteration 306/1000 | Loss: 0.00004334
Iteration 307/1000 | Loss: 0.00004428
Iteration 308/1000 | Loss: 0.00004230
Iteration 309/1000 | Loss: 0.00004382
Iteration 310/1000 | Loss: 0.00004382
Iteration 311/1000 | Loss: 0.00004691
Iteration 312/1000 | Loss: 0.00004376
Iteration 313/1000 | Loss: 0.00004498
Iteration 314/1000 | Loss: 0.00004566
Iteration 315/1000 | Loss: 0.00004375
Iteration 316/1000 | Loss: 0.00004605
Iteration 317/1000 | Loss: 0.00005194
Iteration 318/1000 | Loss: 0.00004206
Iteration 319/1000 | Loss: 0.00004459
Iteration 320/1000 | Loss: 0.00004359
Iteration 321/1000 | Loss: 0.00004189
Iteration 322/1000 | Loss: 0.00004379
Iteration 323/1000 | Loss: 0.00004518
Iteration 324/1000 | Loss: 0.00004370
Iteration 325/1000 | Loss: 0.00004442
Iteration 326/1000 | Loss: 0.00004352
Iteration 327/1000 | Loss: 0.00004560
Iteration 328/1000 | Loss: 0.00004932
Iteration 329/1000 | Loss: 0.00004550
Iteration 330/1000 | Loss: 0.00004353
Iteration 331/1000 | Loss: 0.00004575
Iteration 332/1000 | Loss: 0.00005349
Iteration 333/1000 | Loss: 0.00004993
Iteration 334/1000 | Loss: 0.00004343
Iteration 335/1000 | Loss: 0.00004519
Iteration 336/1000 | Loss: 0.00004402
Iteration 337/1000 | Loss: 0.00004247
Iteration 338/1000 | Loss: 0.00004454
Iteration 339/1000 | Loss: 0.00004223
Iteration 340/1000 | Loss: 0.00004480
Iteration 341/1000 | Loss: 0.00005799
Iteration 342/1000 | Loss: 0.00004475
Iteration 343/1000 | Loss: 0.00005836
Iteration 344/1000 | Loss: 0.00004466
Iteration 345/1000 | Loss: 0.00005520
Iteration 346/1000 | Loss: 0.00004422
Iteration 347/1000 | Loss: 0.00005048
Iteration 348/1000 | Loss: 0.00004402
Iteration 349/1000 | Loss: 0.00004473
Iteration 350/1000 | Loss: 0.00004400
Iteration 351/1000 | Loss: 0.00004795
Iteration 352/1000 | Loss: 0.00005009
Iteration 353/1000 | Loss: 0.00005263
Iteration 354/1000 | Loss: 0.00004068
Iteration 355/1000 | Loss: 0.00003794
Iteration 356/1000 | Loss: 0.00003702
Iteration 357/1000 | Loss: 0.00003663
Iteration 358/1000 | Loss: 0.00003639
Iteration 359/1000 | Loss: 0.00003625
Iteration 360/1000 | Loss: 0.00003625
Iteration 361/1000 | Loss: 0.00003624
Iteration 362/1000 | Loss: 0.00003623
Iteration 363/1000 | Loss: 0.00003622
Iteration 364/1000 | Loss: 0.00003622
Iteration 365/1000 | Loss: 0.00003621
Iteration 366/1000 | Loss: 0.00003621
Iteration 367/1000 | Loss: 0.00003621
Iteration 368/1000 | Loss: 0.00003621
Iteration 369/1000 | Loss: 0.00003620
Iteration 370/1000 | Loss: 0.00003620
Iteration 371/1000 | Loss: 0.00003620
Iteration 372/1000 | Loss: 0.00003620
Iteration 373/1000 | Loss: 0.00003620
Iteration 374/1000 | Loss: 0.00003620
Iteration 375/1000 | Loss: 0.00003620
Iteration 376/1000 | Loss: 0.00003620
Iteration 377/1000 | Loss: 0.00003620
Iteration 378/1000 | Loss: 0.00003619
Iteration 379/1000 | Loss: 0.00003619
Iteration 380/1000 | Loss: 0.00003619
Iteration 381/1000 | Loss: 0.00003619
Iteration 382/1000 | Loss: 0.00003619
Iteration 383/1000 | Loss: 0.00003619
Iteration 384/1000 | Loss: 0.00003619
Iteration 385/1000 | Loss: 0.00003619
Iteration 386/1000 | Loss: 0.00003618
Iteration 387/1000 | Loss: 0.00003618
Iteration 388/1000 | Loss: 0.00003618
Iteration 389/1000 | Loss: 0.00003618
Iteration 390/1000 | Loss: 0.00003618
Iteration 391/1000 | Loss: 0.00003618
Iteration 392/1000 | Loss: 0.00003618
Iteration 393/1000 | Loss: 0.00003618
Iteration 394/1000 | Loss: 0.00003618
Iteration 395/1000 | Loss: 0.00003618
Iteration 396/1000 | Loss: 0.00003618
Iteration 397/1000 | Loss: 0.00003618
Iteration 398/1000 | Loss: 0.00003618
Iteration 399/1000 | Loss: 0.00003618
Iteration 400/1000 | Loss: 0.00003618
Iteration 401/1000 | Loss: 0.00003618
Iteration 402/1000 | Loss: 0.00003618
Iteration 403/1000 | Loss: 0.00003617
Iteration 404/1000 | Loss: 0.00003617
Iteration 405/1000 | Loss: 0.00003617
Iteration 406/1000 | Loss: 0.00003617
Iteration 407/1000 | Loss: 0.00003617
Iteration 408/1000 | Loss: 0.00003617
Iteration 409/1000 | Loss: 0.00003617
Iteration 410/1000 | Loss: 0.00003617
Iteration 411/1000 | Loss: 0.00003617
Iteration 412/1000 | Loss: 0.00003617
Iteration 413/1000 | Loss: 0.00003617
Iteration 414/1000 | Loss: 0.00003617
Iteration 415/1000 | Loss: 0.00003617
Iteration 416/1000 | Loss: 0.00003617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 416. Stopping optimization.
Last 5 losses: [3.617219772422686e-05, 3.617219772422686e-05, 3.617219772422686e-05, 3.617219772422686e-05, 3.617219772422686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.617219772422686e-05

Optimization complete. Final v2v error: 4.8212175369262695 mm

Highest mean error: 6.170161724090576 mm for frame 55

Lowest mean error: 4.266845703125 mm for frame 248

Saving results

Total time: 650.4019289016724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876324
Iteration 2/25 | Loss: 0.00123157
Iteration 3/25 | Loss: 0.00103072
Iteration 4/25 | Loss: 0.00100599
Iteration 5/25 | Loss: 0.00100000
Iteration 6/25 | Loss: 0.00099734
Iteration 7/25 | Loss: 0.00099674
Iteration 8/25 | Loss: 0.00099673
Iteration 9/25 | Loss: 0.00099673
Iteration 10/25 | Loss: 0.00099674
Iteration 11/25 | Loss: 0.00099673
Iteration 12/25 | Loss: 0.00099673
Iteration 13/25 | Loss: 0.00099673
Iteration 14/25 | Loss: 0.00099673
Iteration 15/25 | Loss: 0.00099673
Iteration 16/25 | Loss: 0.00099673
Iteration 17/25 | Loss: 0.00099673
Iteration 18/25 | Loss: 0.00099673
Iteration 19/25 | Loss: 0.00099673
Iteration 20/25 | Loss: 0.00099673
Iteration 21/25 | Loss: 0.00099673
Iteration 22/25 | Loss: 0.00099673
Iteration 23/25 | Loss: 0.00099673
Iteration 24/25 | Loss: 0.00099673
Iteration 25/25 | Loss: 0.00099673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36549246
Iteration 2/25 | Loss: 0.00049443
Iteration 3/25 | Loss: 0.00049443
Iteration 4/25 | Loss: 0.00049443
Iteration 5/25 | Loss: 0.00049443
Iteration 6/25 | Loss: 0.00049443
Iteration 7/25 | Loss: 0.00049443
Iteration 8/25 | Loss: 0.00049443
Iteration 9/25 | Loss: 0.00049443
Iteration 10/25 | Loss: 0.00049443
Iteration 11/25 | Loss: 0.00049443
Iteration 12/25 | Loss: 0.00049443
Iteration 13/25 | Loss: 0.00049443
Iteration 14/25 | Loss: 0.00049443
Iteration 15/25 | Loss: 0.00049443
Iteration 16/25 | Loss: 0.00049443
Iteration 17/25 | Loss: 0.00049443
Iteration 18/25 | Loss: 0.00049443
Iteration 19/25 | Loss: 0.00049443
Iteration 20/25 | Loss: 0.00049443
Iteration 21/25 | Loss: 0.00049443
Iteration 22/25 | Loss: 0.00049443
Iteration 23/25 | Loss: 0.00049443
Iteration 24/25 | Loss: 0.00049443
Iteration 25/25 | Loss: 0.00049443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049443
Iteration 2/1000 | Loss: 0.00003251
Iteration 3/1000 | Loss: 0.00002454
Iteration 4/1000 | Loss: 0.00002292
Iteration 5/1000 | Loss: 0.00002137
Iteration 6/1000 | Loss: 0.00002048
Iteration 7/1000 | Loss: 0.00002000
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001917
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001908
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001905
Iteration 20/1000 | Loss: 0.00001905
Iteration 21/1000 | Loss: 0.00001904
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001902
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001901
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001901
Iteration 31/1000 | Loss: 0.00001901
Iteration 32/1000 | Loss: 0.00001901
Iteration 33/1000 | Loss: 0.00001901
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.8944923795061186e-05, 1.8944923795061186e-05, 1.8944923795061186e-05, 1.8944923795061186e-05, 1.8944923795061186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8944923795061186e-05

Optimization complete. Final v2v error: 3.8076720237731934 mm

Highest mean error: 4.2479963302612305 mm for frame 92

Lowest mean error: 3.2307536602020264 mm for frame 174

Saving results

Total time: 30.544525861740112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857821
Iteration 2/25 | Loss: 0.00178470
Iteration 3/25 | Loss: 0.00135564
Iteration 4/25 | Loss: 0.00120445
Iteration 5/25 | Loss: 0.00116506
Iteration 6/25 | Loss: 0.00115241
Iteration 7/25 | Loss: 0.00115275
Iteration 8/25 | Loss: 0.00115070
Iteration 9/25 | Loss: 0.00114457
Iteration 10/25 | Loss: 0.00113766
Iteration 11/25 | Loss: 0.00113557
Iteration 12/25 | Loss: 0.00113419
Iteration 13/25 | Loss: 0.00113348
Iteration 14/25 | Loss: 0.00113328
Iteration 15/25 | Loss: 0.00113320
Iteration 16/25 | Loss: 0.00113525
Iteration 17/25 | Loss: 0.00113503
Iteration 18/25 | Loss: 0.00113283
Iteration 19/25 | Loss: 0.00113213
Iteration 20/25 | Loss: 0.00113147
Iteration 21/25 | Loss: 0.00113132
Iteration 22/25 | Loss: 0.00113127
Iteration 23/25 | Loss: 0.00113127
Iteration 24/25 | Loss: 0.00113127
Iteration 25/25 | Loss: 0.00113127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36068141
Iteration 2/25 | Loss: 0.00079974
Iteration 3/25 | Loss: 0.00079972
Iteration 4/25 | Loss: 0.00079972
Iteration 5/25 | Loss: 0.00079972
Iteration 6/25 | Loss: 0.00079972
Iteration 7/25 | Loss: 0.00079972
Iteration 8/25 | Loss: 0.00079972
Iteration 9/25 | Loss: 0.00079972
Iteration 10/25 | Loss: 0.00079972
Iteration 11/25 | Loss: 0.00079972
Iteration 12/25 | Loss: 0.00079972
Iteration 13/25 | Loss: 0.00079972
Iteration 14/25 | Loss: 0.00079972
Iteration 15/25 | Loss: 0.00079972
Iteration 16/25 | Loss: 0.00079972
Iteration 17/25 | Loss: 0.00079972
Iteration 18/25 | Loss: 0.00079972
Iteration 19/25 | Loss: 0.00079972
Iteration 20/25 | Loss: 0.00079972
Iteration 21/25 | Loss: 0.00079972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007997184293344617, 0.0007997184293344617, 0.0007997184293344617, 0.0007997184293344617, 0.0007997184293344617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007997184293344617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079972
Iteration 2/1000 | Loss: 0.00004837
Iteration 3/1000 | Loss: 0.00003946
Iteration 4/1000 | Loss: 0.00005300
Iteration 5/1000 | Loss: 0.00003622
Iteration 6/1000 | Loss: 0.00003403
Iteration 7/1000 | Loss: 0.00003306
Iteration 8/1000 | Loss: 0.00003261
Iteration 9/1000 | Loss: 0.00003219
Iteration 10/1000 | Loss: 0.00003185
Iteration 11/1000 | Loss: 0.00003156
Iteration 12/1000 | Loss: 0.00003132
Iteration 13/1000 | Loss: 0.00003097
Iteration 14/1000 | Loss: 0.00003058
Iteration 15/1000 | Loss: 0.00003029
Iteration 16/1000 | Loss: 0.00003006
Iteration 17/1000 | Loss: 0.00002987
Iteration 18/1000 | Loss: 0.00002986
Iteration 19/1000 | Loss: 0.00002973
Iteration 20/1000 | Loss: 0.00002971
Iteration 21/1000 | Loss: 0.00002971
Iteration 22/1000 | Loss: 0.00002970
Iteration 23/1000 | Loss: 0.00002969
Iteration 24/1000 | Loss: 0.00002967
Iteration 25/1000 | Loss: 0.00002967
Iteration 26/1000 | Loss: 0.00002967
Iteration 27/1000 | Loss: 0.00002965
Iteration 28/1000 | Loss: 0.00002958
Iteration 29/1000 | Loss: 0.00002958
Iteration 30/1000 | Loss: 0.00002957
Iteration 31/1000 | Loss: 0.00002957
Iteration 32/1000 | Loss: 0.00002956
Iteration 33/1000 | Loss: 0.00002955
Iteration 34/1000 | Loss: 0.00002955
Iteration 35/1000 | Loss: 0.00002955
Iteration 36/1000 | Loss: 0.00002955
Iteration 37/1000 | Loss: 0.00002954
Iteration 38/1000 | Loss: 0.00002954
Iteration 39/1000 | Loss: 0.00002954
Iteration 40/1000 | Loss: 0.00002953
Iteration 41/1000 | Loss: 0.00002953
Iteration 42/1000 | Loss: 0.00002953
Iteration 43/1000 | Loss: 0.00002952
Iteration 44/1000 | Loss: 0.00002952
Iteration 45/1000 | Loss: 0.00002952
Iteration 46/1000 | Loss: 0.00002952
Iteration 47/1000 | Loss: 0.00002951
Iteration 48/1000 | Loss: 0.00002951
Iteration 49/1000 | Loss: 0.00002951
Iteration 50/1000 | Loss: 0.00002951
Iteration 51/1000 | Loss: 0.00002951
Iteration 52/1000 | Loss: 0.00002951
Iteration 53/1000 | Loss: 0.00002951
Iteration 54/1000 | Loss: 0.00002951
Iteration 55/1000 | Loss: 0.00002951
Iteration 56/1000 | Loss: 0.00002950
Iteration 57/1000 | Loss: 0.00002950
Iteration 58/1000 | Loss: 0.00002950
Iteration 59/1000 | Loss: 0.00002950
Iteration 60/1000 | Loss: 0.00002950
Iteration 61/1000 | Loss: 0.00002950
Iteration 62/1000 | Loss: 0.00002949
Iteration 63/1000 | Loss: 0.00002949
Iteration 64/1000 | Loss: 0.00002949
Iteration 65/1000 | Loss: 0.00002948
Iteration 66/1000 | Loss: 0.00002948
Iteration 67/1000 | Loss: 0.00002948
Iteration 68/1000 | Loss: 0.00002948
Iteration 69/1000 | Loss: 0.00002947
Iteration 70/1000 | Loss: 0.00002947
Iteration 71/1000 | Loss: 0.00002947
Iteration 72/1000 | Loss: 0.00002947
Iteration 73/1000 | Loss: 0.00002947
Iteration 74/1000 | Loss: 0.00002947
Iteration 75/1000 | Loss: 0.00002947
Iteration 76/1000 | Loss: 0.00002947
Iteration 77/1000 | Loss: 0.00002947
Iteration 78/1000 | Loss: 0.00002947
Iteration 79/1000 | Loss: 0.00002946
Iteration 80/1000 | Loss: 0.00002946
Iteration 81/1000 | Loss: 0.00002946
Iteration 82/1000 | Loss: 0.00002946
Iteration 83/1000 | Loss: 0.00002946
Iteration 84/1000 | Loss: 0.00002946
Iteration 85/1000 | Loss: 0.00002946
Iteration 86/1000 | Loss: 0.00002946
Iteration 87/1000 | Loss: 0.00002946
Iteration 88/1000 | Loss: 0.00002946
Iteration 89/1000 | Loss: 0.00002946
Iteration 90/1000 | Loss: 0.00002946
Iteration 91/1000 | Loss: 0.00002946
Iteration 92/1000 | Loss: 0.00002946
Iteration 93/1000 | Loss: 0.00002946
Iteration 94/1000 | Loss: 0.00002946
Iteration 95/1000 | Loss: 0.00002946
Iteration 96/1000 | Loss: 0.00002946
Iteration 97/1000 | Loss: 0.00002946
Iteration 98/1000 | Loss: 0.00002946
Iteration 99/1000 | Loss: 0.00002946
Iteration 100/1000 | Loss: 0.00002946
Iteration 101/1000 | Loss: 0.00002946
Iteration 102/1000 | Loss: 0.00002946
Iteration 103/1000 | Loss: 0.00002946
Iteration 104/1000 | Loss: 0.00002946
Iteration 105/1000 | Loss: 0.00002946
Iteration 106/1000 | Loss: 0.00002946
Iteration 107/1000 | Loss: 0.00002946
Iteration 108/1000 | Loss: 0.00002946
Iteration 109/1000 | Loss: 0.00002946
Iteration 110/1000 | Loss: 0.00002946
Iteration 111/1000 | Loss: 0.00002946
Iteration 112/1000 | Loss: 0.00002946
Iteration 113/1000 | Loss: 0.00002946
Iteration 114/1000 | Loss: 0.00002946
Iteration 115/1000 | Loss: 0.00002946
Iteration 116/1000 | Loss: 0.00002946
Iteration 117/1000 | Loss: 0.00002946
Iteration 118/1000 | Loss: 0.00002946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.9460314181051217e-05, 2.9460314181051217e-05, 2.9460314181051217e-05, 2.9460314181051217e-05, 2.9460314181051217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9460314181051217e-05

Optimization complete. Final v2v error: 4.766750335693359 mm

Highest mean error: 5.253260612487793 mm for frame 187

Lowest mean error: 4.086426734924316 mm for frame 0

Saving results

Total time: 81.8673186302185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01172090
Iteration 2/25 | Loss: 0.00242911
Iteration 3/25 | Loss: 0.00248247
Iteration 4/25 | Loss: 0.00174255
Iteration 5/25 | Loss: 0.00144999
Iteration 6/25 | Loss: 0.00131695
Iteration 7/25 | Loss: 0.00128623
Iteration 8/25 | Loss: 0.00130893
Iteration 9/25 | Loss: 0.00118861
Iteration 10/25 | Loss: 0.00117395
Iteration 11/25 | Loss: 0.00115681
Iteration 12/25 | Loss: 0.00121886
Iteration 13/25 | Loss: 0.00123971
Iteration 14/25 | Loss: 0.00113307
Iteration 15/25 | Loss: 0.00116478
Iteration 16/25 | Loss: 0.00120110
Iteration 17/25 | Loss: 0.00117603
Iteration 18/25 | Loss: 0.00118302
Iteration 19/25 | Loss: 0.00116160
Iteration 20/25 | Loss: 0.00115943
Iteration 21/25 | Loss: 0.00112603
Iteration 22/25 | Loss: 0.00110286
Iteration 23/25 | Loss: 0.00109125
Iteration 24/25 | Loss: 0.00106219
Iteration 25/25 | Loss: 0.00105413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37790024
Iteration 2/25 | Loss: 0.00216315
Iteration 3/25 | Loss: 0.00216313
Iteration 4/25 | Loss: 0.00216313
Iteration 5/25 | Loss: 0.00216312
Iteration 6/25 | Loss: 0.00216312
Iteration 7/25 | Loss: 0.00216312
Iteration 8/25 | Loss: 0.00216312
Iteration 9/25 | Loss: 0.00216312
Iteration 10/25 | Loss: 0.00216312
Iteration 11/25 | Loss: 0.00216312
Iteration 12/25 | Loss: 0.00216312
Iteration 13/25 | Loss: 0.00216312
Iteration 14/25 | Loss: 0.00216312
Iteration 15/25 | Loss: 0.00216312
Iteration 16/25 | Loss: 0.00216312
Iteration 17/25 | Loss: 0.00216312
Iteration 18/25 | Loss: 0.00216312
Iteration 19/25 | Loss: 0.00216312
Iteration 20/25 | Loss: 0.00216312
Iteration 21/25 | Loss: 0.00216312
Iteration 22/25 | Loss: 0.00216312
Iteration 23/25 | Loss: 0.00216312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002163123106583953, 0.002163123106583953, 0.002163123106583953, 0.002163123106583953, 0.002163123106583953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002163123106583953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216312
Iteration 2/1000 | Loss: 0.00306671
Iteration 3/1000 | Loss: 0.00810346
Iteration 4/1000 | Loss: 0.00499056
Iteration 5/1000 | Loss: 0.00674199
Iteration 6/1000 | Loss: 0.00314137
Iteration 7/1000 | Loss: 0.00166158
Iteration 8/1000 | Loss: 0.00211256
Iteration 9/1000 | Loss: 0.00134043
Iteration 10/1000 | Loss: 0.00115762
Iteration 11/1000 | Loss: 0.00186909
Iteration 12/1000 | Loss: 0.00270593
Iteration 13/1000 | Loss: 0.00314373
Iteration 14/1000 | Loss: 0.00408179
Iteration 15/1000 | Loss: 0.00268576
Iteration 16/1000 | Loss: 0.00206621
Iteration 17/1000 | Loss: 0.00148358
Iteration 18/1000 | Loss: 0.00226736
Iteration 19/1000 | Loss: 0.00210492
Iteration 20/1000 | Loss: 0.00306481
Iteration 21/1000 | Loss: 0.00243568
Iteration 22/1000 | Loss: 0.00327538
Iteration 23/1000 | Loss: 0.00209172
Iteration 24/1000 | Loss: 0.00212596
Iteration 25/1000 | Loss: 0.00422454
Iteration 26/1000 | Loss: 0.00213806
Iteration 27/1000 | Loss: 0.00207607
Iteration 28/1000 | Loss: 0.00255763
Iteration 29/1000 | Loss: 0.00323287
Iteration 30/1000 | Loss: 0.00142927
Iteration 31/1000 | Loss: 0.00285499
Iteration 32/1000 | Loss: 0.00104059
Iteration 33/1000 | Loss: 0.00201647
Iteration 34/1000 | Loss: 0.00178233
Iteration 35/1000 | Loss: 0.00163383
Iteration 36/1000 | Loss: 0.00455753
Iteration 37/1000 | Loss: 0.00221380
Iteration 38/1000 | Loss: 0.00146957
Iteration 39/1000 | Loss: 0.00069544
Iteration 40/1000 | Loss: 0.00300707
Iteration 41/1000 | Loss: 0.00485963
Iteration 42/1000 | Loss: 0.00174582
Iteration 43/1000 | Loss: 0.00244960
Iteration 44/1000 | Loss: 0.00273383
Iteration 45/1000 | Loss: 0.00076736
Iteration 46/1000 | Loss: 0.00075384
Iteration 47/1000 | Loss: 0.00143158
Iteration 48/1000 | Loss: 0.00283647
Iteration 49/1000 | Loss: 0.00334839
Iteration 50/1000 | Loss: 0.00332189
Iteration 51/1000 | Loss: 0.00285113
Iteration 52/1000 | Loss: 0.00148533
Iteration 53/1000 | Loss: 0.00243217
Iteration 54/1000 | Loss: 0.00153369
Iteration 55/1000 | Loss: 0.00294458
Iteration 56/1000 | Loss: 0.00161046
Iteration 57/1000 | Loss: 0.00204809
Iteration 58/1000 | Loss: 0.00164756
Iteration 59/1000 | Loss: 0.00271387
Iteration 60/1000 | Loss: 0.00237209
Iteration 61/1000 | Loss: 0.00196010
Iteration 62/1000 | Loss: 0.00209144
Iteration 63/1000 | Loss: 0.00121998
Iteration 64/1000 | Loss: 0.00156621
Iteration 65/1000 | Loss: 0.00180046
Iteration 66/1000 | Loss: 0.00075489
Iteration 67/1000 | Loss: 0.00085349
Iteration 68/1000 | Loss: 0.00103437
Iteration 69/1000 | Loss: 0.00131967
Iteration 70/1000 | Loss: 0.00241956
Iteration 71/1000 | Loss: 0.00168501
Iteration 72/1000 | Loss: 0.00151712
Iteration 73/1000 | Loss: 0.00005953
Iteration 74/1000 | Loss: 0.00004865
Iteration 75/1000 | Loss: 0.00004352
Iteration 76/1000 | Loss: 0.00057434
Iteration 77/1000 | Loss: 0.00189417
Iteration 78/1000 | Loss: 0.00104219
Iteration 79/1000 | Loss: 0.00068350
Iteration 80/1000 | Loss: 0.00144067
Iteration 81/1000 | Loss: 0.00363174
Iteration 82/1000 | Loss: 0.00059178
Iteration 83/1000 | Loss: 0.00011463
Iteration 84/1000 | Loss: 0.00279104
Iteration 85/1000 | Loss: 0.00155236
Iteration 86/1000 | Loss: 0.00179910
Iteration 87/1000 | Loss: 0.00075306
Iteration 88/1000 | Loss: 0.00192625
Iteration 89/1000 | Loss: 0.00073831
Iteration 90/1000 | Loss: 0.00265213
Iteration 91/1000 | Loss: 0.00198876
Iteration 92/1000 | Loss: 0.00163000
Iteration 93/1000 | Loss: 0.00281210
Iteration 94/1000 | Loss: 0.00142650
Iteration 95/1000 | Loss: 0.00266853
Iteration 96/1000 | Loss: 0.00092769
Iteration 97/1000 | Loss: 0.00244048
Iteration 98/1000 | Loss: 0.00157000
Iteration 99/1000 | Loss: 0.00195337
Iteration 100/1000 | Loss: 0.00326799
Iteration 101/1000 | Loss: 0.00013690
Iteration 102/1000 | Loss: 0.00019595
Iteration 103/1000 | Loss: 0.00009754
Iteration 104/1000 | Loss: 0.00091378
Iteration 105/1000 | Loss: 0.00148729
Iteration 106/1000 | Loss: 0.00073740
Iteration 107/1000 | Loss: 0.00028878
Iteration 108/1000 | Loss: 0.00028752
Iteration 109/1000 | Loss: 0.00018917
Iteration 110/1000 | Loss: 0.00013498
Iteration 111/1000 | Loss: 0.00052325
Iteration 112/1000 | Loss: 0.00020927
Iteration 113/1000 | Loss: 0.00058091
Iteration 114/1000 | Loss: 0.00074762
Iteration 115/1000 | Loss: 0.00136718
Iteration 116/1000 | Loss: 0.00068456
Iteration 117/1000 | Loss: 0.00095659
Iteration 118/1000 | Loss: 0.00035497
Iteration 119/1000 | Loss: 0.00102764
Iteration 120/1000 | Loss: 0.00044249
Iteration 121/1000 | Loss: 0.00012602
Iteration 122/1000 | Loss: 0.00005473
Iteration 123/1000 | Loss: 0.00089709
Iteration 124/1000 | Loss: 0.00035110
Iteration 125/1000 | Loss: 0.00086637
Iteration 126/1000 | Loss: 0.00066542
Iteration 127/1000 | Loss: 0.00071570
Iteration 128/1000 | Loss: 0.00116824
Iteration 129/1000 | Loss: 0.00007172
Iteration 130/1000 | Loss: 0.00004424
Iteration 131/1000 | Loss: 0.00003958
Iteration 132/1000 | Loss: 0.00003260
Iteration 133/1000 | Loss: 0.00075631
Iteration 134/1000 | Loss: 0.00003536
Iteration 135/1000 | Loss: 0.00002806
Iteration 136/1000 | Loss: 0.00002598
Iteration 137/1000 | Loss: 0.00002469
Iteration 138/1000 | Loss: 0.00002403
Iteration 139/1000 | Loss: 0.00002367
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002334
Iteration 142/1000 | Loss: 0.00002327
Iteration 143/1000 | Loss: 0.00002320
Iteration 144/1000 | Loss: 0.00002316
Iteration 145/1000 | Loss: 0.00002315
Iteration 146/1000 | Loss: 0.00002315
Iteration 147/1000 | Loss: 0.00002315
Iteration 148/1000 | Loss: 0.00002314
Iteration 149/1000 | Loss: 0.00002314
Iteration 150/1000 | Loss: 0.00002313
Iteration 151/1000 | Loss: 0.00002313
Iteration 152/1000 | Loss: 0.00002312
Iteration 153/1000 | Loss: 0.00002310
Iteration 154/1000 | Loss: 0.00002310
Iteration 155/1000 | Loss: 0.00002310
Iteration 156/1000 | Loss: 0.00002309
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Iteration 159/1000 | Loss: 0.00002309
Iteration 160/1000 | Loss: 0.00002309
Iteration 161/1000 | Loss: 0.00002308
Iteration 162/1000 | Loss: 0.00002308
Iteration 163/1000 | Loss: 0.00002308
Iteration 164/1000 | Loss: 0.00002307
Iteration 165/1000 | Loss: 0.00002307
Iteration 166/1000 | Loss: 0.00002307
Iteration 167/1000 | Loss: 0.00002307
Iteration 168/1000 | Loss: 0.00002306
Iteration 169/1000 | Loss: 0.00002306
Iteration 170/1000 | Loss: 0.00002306
Iteration 171/1000 | Loss: 0.00002306
Iteration 172/1000 | Loss: 0.00002306
Iteration 173/1000 | Loss: 0.00002305
Iteration 174/1000 | Loss: 0.00002305
Iteration 175/1000 | Loss: 0.00002305
Iteration 176/1000 | Loss: 0.00002304
Iteration 177/1000 | Loss: 0.00002304
Iteration 178/1000 | Loss: 0.00002304
Iteration 179/1000 | Loss: 0.00002304
Iteration 180/1000 | Loss: 0.00002304
Iteration 181/1000 | Loss: 0.00002304
Iteration 182/1000 | Loss: 0.00002304
Iteration 183/1000 | Loss: 0.00002304
Iteration 184/1000 | Loss: 0.00002303
Iteration 185/1000 | Loss: 0.00002303
Iteration 186/1000 | Loss: 0.00002303
Iteration 187/1000 | Loss: 0.00002303
Iteration 188/1000 | Loss: 0.00002303
Iteration 189/1000 | Loss: 0.00002303
Iteration 190/1000 | Loss: 0.00002303
Iteration 191/1000 | Loss: 0.00002302
Iteration 192/1000 | Loss: 0.00002302
Iteration 193/1000 | Loss: 0.00002302
Iteration 194/1000 | Loss: 0.00002302
Iteration 195/1000 | Loss: 0.00002302
Iteration 196/1000 | Loss: 0.00002301
Iteration 197/1000 | Loss: 0.00002301
Iteration 198/1000 | Loss: 0.00002301
Iteration 199/1000 | Loss: 0.00002300
Iteration 200/1000 | Loss: 0.00002300
Iteration 201/1000 | Loss: 0.00002300
Iteration 202/1000 | Loss: 0.00002300
Iteration 203/1000 | Loss: 0.00002300
Iteration 204/1000 | Loss: 0.00002300
Iteration 205/1000 | Loss: 0.00002300
Iteration 206/1000 | Loss: 0.00002300
Iteration 207/1000 | Loss: 0.00002300
Iteration 208/1000 | Loss: 0.00002300
Iteration 209/1000 | Loss: 0.00002300
Iteration 210/1000 | Loss: 0.00002300
Iteration 211/1000 | Loss: 0.00002300
Iteration 212/1000 | Loss: 0.00002300
Iteration 213/1000 | Loss: 0.00002300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.3003291062195785e-05, 2.3003291062195785e-05, 2.3003291062195785e-05, 2.3003291062195785e-05, 2.3003291062195785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3003291062195785e-05

Optimization complete. Final v2v error: 4.071567535400391 mm

Highest mean error: 10.270208358764648 mm for frame 121

Lowest mean error: 3.5700557231903076 mm for frame 148

Saving results

Total time: 263.41258668899536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00636258
Iteration 2/25 | Loss: 0.00140059
Iteration 3/25 | Loss: 0.00111399
Iteration 4/25 | Loss: 0.00108393
Iteration 5/25 | Loss: 0.00107663
Iteration 6/25 | Loss: 0.00107407
Iteration 7/25 | Loss: 0.00107350
Iteration 8/25 | Loss: 0.00107350
Iteration 9/25 | Loss: 0.00107350
Iteration 10/25 | Loss: 0.00107350
Iteration 11/25 | Loss: 0.00107350
Iteration 12/25 | Loss: 0.00107350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010734980460256338, 0.0010734980460256338, 0.0010734980460256338, 0.0010734980460256338, 0.0010734980460256338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010734980460256338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22277606
Iteration 2/25 | Loss: 0.00061184
Iteration 3/25 | Loss: 0.00061182
Iteration 4/25 | Loss: 0.00061182
Iteration 5/25 | Loss: 0.00061182
Iteration 6/25 | Loss: 0.00061182
Iteration 7/25 | Loss: 0.00061182
Iteration 8/25 | Loss: 0.00061182
Iteration 9/25 | Loss: 0.00061182
Iteration 10/25 | Loss: 0.00061182
Iteration 11/25 | Loss: 0.00061182
Iteration 12/25 | Loss: 0.00061182
Iteration 13/25 | Loss: 0.00061182
Iteration 14/25 | Loss: 0.00061182
Iteration 15/25 | Loss: 0.00061182
Iteration 16/25 | Loss: 0.00061182
Iteration 17/25 | Loss: 0.00061182
Iteration 18/25 | Loss: 0.00061182
Iteration 19/25 | Loss: 0.00061182
Iteration 20/25 | Loss: 0.00061182
Iteration 21/25 | Loss: 0.00061182
Iteration 22/25 | Loss: 0.00061182
Iteration 23/25 | Loss: 0.00061182
Iteration 24/25 | Loss: 0.00061182
Iteration 25/25 | Loss: 0.00061182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061182
Iteration 2/1000 | Loss: 0.00003674
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002337
Iteration 5/1000 | Loss: 0.00002197
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001949
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001948
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001937
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001932
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001931
Iteration 31/1000 | Loss: 0.00001930
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001927
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001926
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001924
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001922
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001921
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001919
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001918
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001917
Iteration 132/1000 | Loss: 0.00001917
Iteration 133/1000 | Loss: 0.00001917
Iteration 134/1000 | Loss: 0.00001917
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001916
Iteration 140/1000 | Loss: 0.00001916
Iteration 141/1000 | Loss: 0.00001916
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001916
Iteration 144/1000 | Loss: 0.00001916
Iteration 145/1000 | Loss: 0.00001916
Iteration 146/1000 | Loss: 0.00001916
Iteration 147/1000 | Loss: 0.00001916
Iteration 148/1000 | Loss: 0.00001916
Iteration 149/1000 | Loss: 0.00001916
Iteration 150/1000 | Loss: 0.00001916
Iteration 151/1000 | Loss: 0.00001916
Iteration 152/1000 | Loss: 0.00001916
Iteration 153/1000 | Loss: 0.00001916
Iteration 154/1000 | Loss: 0.00001916
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.916031578730326e-05, 1.916031578730326e-05, 1.916031578730326e-05, 1.916031578730326e-05, 1.916031578730326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.916031578730326e-05

Optimization complete. Final v2v error: 3.8424365520477295 mm

Highest mean error: 4.789242267608643 mm for frame 82

Lowest mean error: 3.5049502849578857 mm for frame 52

Saving results

Total time: 39.11184811592102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00634002
Iteration 2/25 | Loss: 0.00142757
Iteration 3/25 | Loss: 0.00118284
Iteration 4/25 | Loss: 0.00115453
Iteration 5/25 | Loss: 0.00115014
Iteration 6/25 | Loss: 0.00114997
Iteration 7/25 | Loss: 0.00114997
Iteration 8/25 | Loss: 0.00114997
Iteration 9/25 | Loss: 0.00114997
Iteration 10/25 | Loss: 0.00114997
Iteration 11/25 | Loss: 0.00114997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001149968826211989, 0.001149968826211989, 0.001149968826211989, 0.001149968826211989, 0.001149968826211989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001149968826211989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62278545
Iteration 2/25 | Loss: 0.00064336
Iteration 3/25 | Loss: 0.00064336
Iteration 4/25 | Loss: 0.00064335
Iteration 5/25 | Loss: 0.00064335
Iteration 6/25 | Loss: 0.00064335
Iteration 7/25 | Loss: 0.00064335
Iteration 8/25 | Loss: 0.00064335
Iteration 9/25 | Loss: 0.00064335
Iteration 10/25 | Loss: 0.00064335
Iteration 11/25 | Loss: 0.00064335
Iteration 12/25 | Loss: 0.00064335
Iteration 13/25 | Loss: 0.00064335
Iteration 14/25 | Loss: 0.00064335
Iteration 15/25 | Loss: 0.00064335
Iteration 16/25 | Loss: 0.00064335
Iteration 17/25 | Loss: 0.00064335
Iteration 18/25 | Loss: 0.00064335
Iteration 19/25 | Loss: 0.00064335
Iteration 20/25 | Loss: 0.00064335
Iteration 21/25 | Loss: 0.00064335
Iteration 22/25 | Loss: 0.00064335
Iteration 23/25 | Loss: 0.00064335
Iteration 24/25 | Loss: 0.00064335
Iteration 25/25 | Loss: 0.00064335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006433530361391604, 0.0006433530361391604, 0.0006433530361391604, 0.0006433530361391604, 0.0006433530361391604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006433530361391604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064335
Iteration 2/1000 | Loss: 0.00004788
Iteration 3/1000 | Loss: 0.00003900
Iteration 4/1000 | Loss: 0.00003528
Iteration 5/1000 | Loss: 0.00003387
Iteration 6/1000 | Loss: 0.00003304
Iteration 7/1000 | Loss: 0.00003244
Iteration 8/1000 | Loss: 0.00003197
Iteration 9/1000 | Loss: 0.00003163
Iteration 10/1000 | Loss: 0.00003156
Iteration 11/1000 | Loss: 0.00003155
Iteration 12/1000 | Loss: 0.00003148
Iteration 13/1000 | Loss: 0.00003147
Iteration 14/1000 | Loss: 0.00003147
Iteration 15/1000 | Loss: 0.00003147
Iteration 16/1000 | Loss: 0.00003147
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00003147
Iteration 19/1000 | Loss: 0.00003147
Iteration 20/1000 | Loss: 0.00003147
Iteration 21/1000 | Loss: 0.00003147
Iteration 22/1000 | Loss: 0.00003147
Iteration 23/1000 | Loss: 0.00003146
Iteration 24/1000 | Loss: 0.00003146
Iteration 25/1000 | Loss: 0.00003146
Iteration 26/1000 | Loss: 0.00003146
Iteration 27/1000 | Loss: 0.00003145
Iteration 28/1000 | Loss: 0.00003145
Iteration 29/1000 | Loss: 0.00003145
Iteration 30/1000 | Loss: 0.00003145
Iteration 31/1000 | Loss: 0.00003145
Iteration 32/1000 | Loss: 0.00003145
Iteration 33/1000 | Loss: 0.00003144
Iteration 34/1000 | Loss: 0.00003144
Iteration 35/1000 | Loss: 0.00003144
Iteration 36/1000 | Loss: 0.00003144
Iteration 37/1000 | Loss: 0.00003144
Iteration 38/1000 | Loss: 0.00003144
Iteration 39/1000 | Loss: 0.00003144
Iteration 40/1000 | Loss: 0.00003144
Iteration 41/1000 | Loss: 0.00003144
Iteration 42/1000 | Loss: 0.00003144
Iteration 43/1000 | Loss: 0.00003143
Iteration 44/1000 | Loss: 0.00003143
Iteration 45/1000 | Loss: 0.00003143
Iteration 46/1000 | Loss: 0.00003143
Iteration 47/1000 | Loss: 0.00003143
Iteration 48/1000 | Loss: 0.00003143
Iteration 49/1000 | Loss: 0.00003143
Iteration 50/1000 | Loss: 0.00003143
Iteration 51/1000 | Loss: 0.00003142
Iteration 52/1000 | Loss: 0.00003142
Iteration 53/1000 | Loss: 0.00003142
Iteration 54/1000 | Loss: 0.00003142
Iteration 55/1000 | Loss: 0.00003142
Iteration 56/1000 | Loss: 0.00003142
Iteration 57/1000 | Loss: 0.00003142
Iteration 58/1000 | Loss: 0.00003142
Iteration 59/1000 | Loss: 0.00003142
Iteration 60/1000 | Loss: 0.00003141
Iteration 61/1000 | Loss: 0.00003141
Iteration 62/1000 | Loss: 0.00003141
Iteration 63/1000 | Loss: 0.00003141
Iteration 64/1000 | Loss: 0.00003140
Iteration 65/1000 | Loss: 0.00003140
Iteration 66/1000 | Loss: 0.00003139
Iteration 67/1000 | Loss: 0.00003139
Iteration 68/1000 | Loss: 0.00003139
Iteration 69/1000 | Loss: 0.00003139
Iteration 70/1000 | Loss: 0.00003139
Iteration 71/1000 | Loss: 0.00003139
Iteration 72/1000 | Loss: 0.00003139
Iteration 73/1000 | Loss: 0.00003139
Iteration 74/1000 | Loss: 0.00003139
Iteration 75/1000 | Loss: 0.00003138
Iteration 76/1000 | Loss: 0.00003138
Iteration 77/1000 | Loss: 0.00003138
Iteration 78/1000 | Loss: 0.00003138
Iteration 79/1000 | Loss: 0.00003137
Iteration 80/1000 | Loss: 0.00003136
Iteration 81/1000 | Loss: 0.00003136
Iteration 82/1000 | Loss: 0.00003136
Iteration 83/1000 | Loss: 0.00003136
Iteration 84/1000 | Loss: 0.00003136
Iteration 85/1000 | Loss: 0.00003135
Iteration 86/1000 | Loss: 0.00003135
Iteration 87/1000 | Loss: 0.00003135
Iteration 88/1000 | Loss: 0.00003135
Iteration 89/1000 | Loss: 0.00003135
Iteration 90/1000 | Loss: 0.00003135
Iteration 91/1000 | Loss: 0.00003135
Iteration 92/1000 | Loss: 0.00003134
Iteration 93/1000 | Loss: 0.00003134
Iteration 94/1000 | Loss: 0.00003134
Iteration 95/1000 | Loss: 0.00003133
Iteration 96/1000 | Loss: 0.00003133
Iteration 97/1000 | Loss: 0.00003133
Iteration 98/1000 | Loss: 0.00003133
Iteration 99/1000 | Loss: 0.00003133
Iteration 100/1000 | Loss: 0.00003133
Iteration 101/1000 | Loss: 0.00003133
Iteration 102/1000 | Loss: 0.00003133
Iteration 103/1000 | Loss: 0.00003132
Iteration 104/1000 | Loss: 0.00003132
Iteration 105/1000 | Loss: 0.00003131
Iteration 106/1000 | Loss: 0.00003130
Iteration 107/1000 | Loss: 0.00003130
Iteration 108/1000 | Loss: 0.00003130
Iteration 109/1000 | Loss: 0.00003130
Iteration 110/1000 | Loss: 0.00003130
Iteration 111/1000 | Loss: 0.00003130
Iteration 112/1000 | Loss: 0.00003130
Iteration 113/1000 | Loss: 0.00003130
Iteration 114/1000 | Loss: 0.00003130
Iteration 115/1000 | Loss: 0.00003129
Iteration 116/1000 | Loss: 0.00003129
Iteration 117/1000 | Loss: 0.00003129
Iteration 118/1000 | Loss: 0.00003129
Iteration 119/1000 | Loss: 0.00003129
Iteration 120/1000 | Loss: 0.00003129
Iteration 121/1000 | Loss: 0.00003129
Iteration 122/1000 | Loss: 0.00003129
Iteration 123/1000 | Loss: 0.00003129
Iteration 124/1000 | Loss: 0.00003129
Iteration 125/1000 | Loss: 0.00003128
Iteration 126/1000 | Loss: 0.00003128
Iteration 127/1000 | Loss: 0.00003128
Iteration 128/1000 | Loss: 0.00003128
Iteration 129/1000 | Loss: 0.00003128
Iteration 130/1000 | Loss: 0.00003128
Iteration 131/1000 | Loss: 0.00003128
Iteration 132/1000 | Loss: 0.00003128
Iteration 133/1000 | Loss: 0.00003128
Iteration 134/1000 | Loss: 0.00003128
Iteration 135/1000 | Loss: 0.00003128
Iteration 136/1000 | Loss: 0.00003128
Iteration 137/1000 | Loss: 0.00003128
Iteration 138/1000 | Loss: 0.00003128
Iteration 139/1000 | Loss: 0.00003128
Iteration 140/1000 | Loss: 0.00003128
Iteration 141/1000 | Loss: 0.00003128
Iteration 142/1000 | Loss: 0.00003128
Iteration 143/1000 | Loss: 0.00003128
Iteration 144/1000 | Loss: 0.00003128
Iteration 145/1000 | Loss: 0.00003128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.127728632534854e-05, 3.127728632534854e-05, 3.127728632534854e-05, 3.127728632534854e-05, 3.127728632534854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.127728632534854e-05

Optimization complete. Final v2v error: 4.826364994049072 mm

Highest mean error: 5.031346321105957 mm for frame 44

Lowest mean error: 4.559979438781738 mm for frame 9

Saving results

Total time: 36.33233141899109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476968
Iteration 2/25 | Loss: 0.00124784
Iteration 3/25 | Loss: 0.00107244
Iteration 4/25 | Loss: 0.00105961
Iteration 5/25 | Loss: 0.00105665
Iteration 6/25 | Loss: 0.00105614
Iteration 7/25 | Loss: 0.00105614
Iteration 8/25 | Loss: 0.00105614
Iteration 9/25 | Loss: 0.00105611
Iteration 10/25 | Loss: 0.00105611
Iteration 11/25 | Loss: 0.00105611
Iteration 12/25 | Loss: 0.00105611
Iteration 13/25 | Loss: 0.00105611
Iteration 14/25 | Loss: 0.00105611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001056114910170436, 0.001056114910170436, 0.001056114910170436, 0.001056114910170436, 0.001056114910170436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001056114910170436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34457791
Iteration 2/25 | Loss: 0.00061711
Iteration 3/25 | Loss: 0.00061711
Iteration 4/25 | Loss: 0.00061711
Iteration 5/25 | Loss: 0.00061711
Iteration 6/25 | Loss: 0.00061711
Iteration 7/25 | Loss: 0.00061711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0006171137792989612, 0.0006171137792989612, 0.0006171137792989612, 0.0006171137792989612, 0.0006171137792989612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006171137792989612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061711
Iteration 2/1000 | Loss: 0.00004579
Iteration 3/1000 | Loss: 0.00003570
Iteration 4/1000 | Loss: 0.00003159
Iteration 5/1000 | Loss: 0.00002977
Iteration 6/1000 | Loss: 0.00002871
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00002772
Iteration 9/1000 | Loss: 0.00002737
Iteration 10/1000 | Loss: 0.00002733
Iteration 11/1000 | Loss: 0.00002732
Iteration 12/1000 | Loss: 0.00002731
Iteration 13/1000 | Loss: 0.00002728
Iteration 14/1000 | Loss: 0.00002711
Iteration 15/1000 | Loss: 0.00002705
Iteration 16/1000 | Loss: 0.00002688
Iteration 17/1000 | Loss: 0.00002688
Iteration 18/1000 | Loss: 0.00002687
Iteration 19/1000 | Loss: 0.00002687
Iteration 20/1000 | Loss: 0.00002686
Iteration 21/1000 | Loss: 0.00002686
Iteration 22/1000 | Loss: 0.00002685
Iteration 23/1000 | Loss: 0.00002684
Iteration 24/1000 | Loss: 0.00002683
Iteration 25/1000 | Loss: 0.00002683
Iteration 26/1000 | Loss: 0.00002683
Iteration 27/1000 | Loss: 0.00002676
Iteration 28/1000 | Loss: 0.00002673
Iteration 29/1000 | Loss: 0.00002672
Iteration 30/1000 | Loss: 0.00002671
Iteration 31/1000 | Loss: 0.00002671
Iteration 32/1000 | Loss: 0.00002658
Iteration 33/1000 | Loss: 0.00002656
Iteration 34/1000 | Loss: 0.00002655
Iteration 35/1000 | Loss: 0.00002655
Iteration 36/1000 | Loss: 0.00002654
Iteration 37/1000 | Loss: 0.00002654
Iteration 38/1000 | Loss: 0.00002653
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002650
Iteration 42/1000 | Loss: 0.00002649
Iteration 43/1000 | Loss: 0.00002649
Iteration 44/1000 | Loss: 0.00002648
Iteration 45/1000 | Loss: 0.00002647
Iteration 46/1000 | Loss: 0.00002647
Iteration 47/1000 | Loss: 0.00002647
Iteration 48/1000 | Loss: 0.00002646
Iteration 49/1000 | Loss: 0.00002646
Iteration 50/1000 | Loss: 0.00002646
Iteration 51/1000 | Loss: 0.00002645
Iteration 52/1000 | Loss: 0.00002643
Iteration 53/1000 | Loss: 0.00002643
Iteration 54/1000 | Loss: 0.00002643
Iteration 55/1000 | Loss: 0.00002642
Iteration 56/1000 | Loss: 0.00002642
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002641
Iteration 59/1000 | Loss: 0.00002641
Iteration 60/1000 | Loss: 0.00002641
Iteration 61/1000 | Loss: 0.00002640
Iteration 62/1000 | Loss: 0.00002640
Iteration 63/1000 | Loss: 0.00002640
Iteration 64/1000 | Loss: 0.00002640
Iteration 65/1000 | Loss: 0.00002640
Iteration 66/1000 | Loss: 0.00002640
Iteration 67/1000 | Loss: 0.00002640
Iteration 68/1000 | Loss: 0.00002640
Iteration 69/1000 | Loss: 0.00002640
Iteration 70/1000 | Loss: 0.00002640
Iteration 71/1000 | Loss: 0.00002640
Iteration 72/1000 | Loss: 0.00002639
Iteration 73/1000 | Loss: 0.00002639
Iteration 74/1000 | Loss: 0.00002639
Iteration 75/1000 | Loss: 0.00002639
Iteration 76/1000 | Loss: 0.00002639
Iteration 77/1000 | Loss: 0.00002638
Iteration 78/1000 | Loss: 0.00002638
Iteration 79/1000 | Loss: 0.00002638
Iteration 80/1000 | Loss: 0.00002638
Iteration 81/1000 | Loss: 0.00002638
Iteration 82/1000 | Loss: 0.00002638
Iteration 83/1000 | Loss: 0.00002637
Iteration 84/1000 | Loss: 0.00002637
Iteration 85/1000 | Loss: 0.00002637
Iteration 86/1000 | Loss: 0.00002637
Iteration 87/1000 | Loss: 0.00002637
Iteration 88/1000 | Loss: 0.00002637
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002636
Iteration 92/1000 | Loss: 0.00002635
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002635
Iteration 97/1000 | Loss: 0.00002635
Iteration 98/1000 | Loss: 0.00002635
Iteration 99/1000 | Loss: 0.00002635
Iteration 100/1000 | Loss: 0.00002635
Iteration 101/1000 | Loss: 0.00002635
Iteration 102/1000 | Loss: 0.00002634
Iteration 103/1000 | Loss: 0.00002634
Iteration 104/1000 | Loss: 0.00002634
Iteration 105/1000 | Loss: 0.00002634
Iteration 106/1000 | Loss: 0.00002634
Iteration 107/1000 | Loss: 0.00002634
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002632
Iteration 112/1000 | Loss: 0.00002632
Iteration 113/1000 | Loss: 0.00002631
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002631
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00002631
Iteration 119/1000 | Loss: 0.00002631
Iteration 120/1000 | Loss: 0.00002631
Iteration 121/1000 | Loss: 0.00002631
Iteration 122/1000 | Loss: 0.00002631
Iteration 123/1000 | Loss: 0.00002631
Iteration 124/1000 | Loss: 0.00002631
Iteration 125/1000 | Loss: 0.00002631
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002630
Iteration 129/1000 | Loss: 0.00002630
Iteration 130/1000 | Loss: 0.00002630
Iteration 131/1000 | Loss: 0.00002630
Iteration 132/1000 | Loss: 0.00002630
Iteration 133/1000 | Loss: 0.00002629
Iteration 134/1000 | Loss: 0.00002629
Iteration 135/1000 | Loss: 0.00002629
Iteration 136/1000 | Loss: 0.00002629
Iteration 137/1000 | Loss: 0.00002629
Iteration 138/1000 | Loss: 0.00002629
Iteration 139/1000 | Loss: 0.00002629
Iteration 140/1000 | Loss: 0.00002629
Iteration 141/1000 | Loss: 0.00002629
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Iteration 157/1000 | Loss: 0.00002628
Iteration 158/1000 | Loss: 0.00002628
Iteration 159/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.62835110333981e-05, 2.62835110333981e-05, 2.62835110333981e-05, 2.62835110333981e-05, 2.62835110333981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.62835110333981e-05

Optimization complete. Final v2v error: 4.420471668243408 mm

Highest mean error: 4.807340621948242 mm for frame 184

Lowest mean error: 3.974829912185669 mm for frame 21

Saving results

Total time: 39.51721906661987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868767
Iteration 2/25 | Loss: 0.00144686
Iteration 3/25 | Loss: 0.00117915
Iteration 4/25 | Loss: 0.00113470
Iteration 5/25 | Loss: 0.00111826
Iteration 6/25 | Loss: 0.00111559
Iteration 7/25 | Loss: 0.00111513
Iteration 8/25 | Loss: 0.00111513
Iteration 9/25 | Loss: 0.00111513
Iteration 10/25 | Loss: 0.00111513
Iteration 11/25 | Loss: 0.00111513
Iteration 12/25 | Loss: 0.00111513
Iteration 13/25 | Loss: 0.00111513
Iteration 14/25 | Loss: 0.00111513
Iteration 15/25 | Loss: 0.00111513
Iteration 16/25 | Loss: 0.00111513
Iteration 17/25 | Loss: 0.00111513
Iteration 18/25 | Loss: 0.00111513
Iteration 19/25 | Loss: 0.00111513
Iteration 20/25 | Loss: 0.00111513
Iteration 21/25 | Loss: 0.00111513
Iteration 22/25 | Loss: 0.00111513
Iteration 23/25 | Loss: 0.00111513
Iteration 24/25 | Loss: 0.00111513
Iteration 25/25 | Loss: 0.00111513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32582486
Iteration 2/25 | Loss: 0.00066086
Iteration 3/25 | Loss: 0.00066085
Iteration 4/25 | Loss: 0.00066085
Iteration 5/25 | Loss: 0.00066085
Iteration 6/25 | Loss: 0.00066085
Iteration 7/25 | Loss: 0.00066085
Iteration 8/25 | Loss: 0.00066085
Iteration 9/25 | Loss: 0.00066085
Iteration 10/25 | Loss: 0.00066085
Iteration 11/25 | Loss: 0.00066085
Iteration 12/25 | Loss: 0.00066085
Iteration 13/25 | Loss: 0.00066085
Iteration 14/25 | Loss: 0.00066085
Iteration 15/25 | Loss: 0.00066085
Iteration 16/25 | Loss: 0.00066085
Iteration 17/25 | Loss: 0.00066085
Iteration 18/25 | Loss: 0.00066085
Iteration 19/25 | Loss: 0.00066085
Iteration 20/25 | Loss: 0.00066085
Iteration 21/25 | Loss: 0.00066085
Iteration 22/25 | Loss: 0.00066085
Iteration 23/25 | Loss: 0.00066085
Iteration 24/25 | Loss: 0.00066085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006608451367355883, 0.0006608451367355883, 0.0006608451367355883, 0.0006608451367355883, 0.0006608451367355883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006608451367355883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066085
Iteration 2/1000 | Loss: 0.00004823
Iteration 3/1000 | Loss: 0.00003655
Iteration 4/1000 | Loss: 0.00003220
Iteration 5/1000 | Loss: 0.00002999
Iteration 6/1000 | Loss: 0.00002892
Iteration 7/1000 | Loss: 0.00002804
Iteration 8/1000 | Loss: 0.00002743
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002638
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002624
Iteration 14/1000 | Loss: 0.00002622
Iteration 15/1000 | Loss: 0.00002605
Iteration 16/1000 | Loss: 0.00002601
Iteration 17/1000 | Loss: 0.00002600
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002591
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002588
Iteration 22/1000 | Loss: 0.00002587
Iteration 23/1000 | Loss: 0.00002586
Iteration 24/1000 | Loss: 0.00002585
Iteration 25/1000 | Loss: 0.00002585
Iteration 26/1000 | Loss: 0.00002584
Iteration 27/1000 | Loss: 0.00002584
Iteration 28/1000 | Loss: 0.00002583
Iteration 29/1000 | Loss: 0.00002583
Iteration 30/1000 | Loss: 0.00002583
Iteration 31/1000 | Loss: 0.00002582
Iteration 32/1000 | Loss: 0.00002581
Iteration 33/1000 | Loss: 0.00002581
Iteration 34/1000 | Loss: 0.00002580
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002580
Iteration 37/1000 | Loss: 0.00002580
Iteration 38/1000 | Loss: 0.00002578
Iteration 39/1000 | Loss: 0.00002578
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002576
Iteration 44/1000 | Loss: 0.00002576
Iteration 45/1000 | Loss: 0.00002576
Iteration 46/1000 | Loss: 0.00002576
Iteration 47/1000 | Loss: 0.00002576
Iteration 48/1000 | Loss: 0.00002576
Iteration 49/1000 | Loss: 0.00002576
Iteration 50/1000 | Loss: 0.00002576
Iteration 51/1000 | Loss: 0.00002576
Iteration 52/1000 | Loss: 0.00002575
Iteration 53/1000 | Loss: 0.00002575
Iteration 54/1000 | Loss: 0.00002574
Iteration 55/1000 | Loss: 0.00002574
Iteration 56/1000 | Loss: 0.00002574
Iteration 57/1000 | Loss: 0.00002573
Iteration 58/1000 | Loss: 0.00002573
Iteration 59/1000 | Loss: 0.00002572
Iteration 60/1000 | Loss: 0.00002572
Iteration 61/1000 | Loss: 0.00002572
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002571
Iteration 64/1000 | Loss: 0.00002571
Iteration 65/1000 | Loss: 0.00002570
Iteration 66/1000 | Loss: 0.00002570
Iteration 67/1000 | Loss: 0.00002570
Iteration 68/1000 | Loss: 0.00002570
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002569
Iteration 72/1000 | Loss: 0.00002569
Iteration 73/1000 | Loss: 0.00002569
Iteration 74/1000 | Loss: 0.00002568
Iteration 75/1000 | Loss: 0.00002568
Iteration 76/1000 | Loss: 0.00002568
Iteration 77/1000 | Loss: 0.00002568
Iteration 78/1000 | Loss: 0.00002568
Iteration 79/1000 | Loss: 0.00002568
Iteration 80/1000 | Loss: 0.00002568
Iteration 81/1000 | Loss: 0.00002568
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00002567
Iteration 85/1000 | Loss: 0.00002567
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002566
Iteration 92/1000 | Loss: 0.00002566
Iteration 93/1000 | Loss: 0.00002565
Iteration 94/1000 | Loss: 0.00002565
Iteration 95/1000 | Loss: 0.00002565
Iteration 96/1000 | Loss: 0.00002565
Iteration 97/1000 | Loss: 0.00002565
Iteration 98/1000 | Loss: 0.00002565
Iteration 99/1000 | Loss: 0.00002565
Iteration 100/1000 | Loss: 0.00002565
Iteration 101/1000 | Loss: 0.00002565
Iteration 102/1000 | Loss: 0.00002565
Iteration 103/1000 | Loss: 0.00002565
Iteration 104/1000 | Loss: 0.00002565
Iteration 105/1000 | Loss: 0.00002565
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.5646384528954513e-05, 2.5646384528954513e-05, 2.5646384528954513e-05, 2.5646384528954513e-05, 2.5646384528954513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5646384528954513e-05

Optimization complete. Final v2v error: 4.4408392906188965 mm

Highest mean error: 5.230872631072998 mm for frame 164

Lowest mean error: 3.7576117515563965 mm for frame 219

Saving results

Total time: 42.771440744400024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064094
Iteration 2/25 | Loss: 0.00176580
Iteration 3/25 | Loss: 0.00124971
Iteration 4/25 | Loss: 0.00121518
Iteration 5/25 | Loss: 0.00120065
Iteration 6/25 | Loss: 0.00119731
Iteration 7/25 | Loss: 0.00119652
Iteration 8/25 | Loss: 0.00119652
Iteration 9/25 | Loss: 0.00119652
Iteration 10/25 | Loss: 0.00119652
Iteration 11/25 | Loss: 0.00119652
Iteration 12/25 | Loss: 0.00119652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011965178418904543, 0.0011965178418904543, 0.0011965178418904543, 0.0011965178418904543, 0.0011965178418904543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011965178418904543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86437941
Iteration 2/25 | Loss: 0.00043531
Iteration 3/25 | Loss: 0.00043530
Iteration 4/25 | Loss: 0.00043530
Iteration 5/25 | Loss: 0.00043530
Iteration 6/25 | Loss: 0.00043530
Iteration 7/25 | Loss: 0.00043530
Iteration 8/25 | Loss: 0.00043530
Iteration 9/25 | Loss: 0.00043530
Iteration 10/25 | Loss: 0.00043530
Iteration 11/25 | Loss: 0.00043530
Iteration 12/25 | Loss: 0.00043530
Iteration 13/25 | Loss: 0.00043530
Iteration 14/25 | Loss: 0.00043530
Iteration 15/25 | Loss: 0.00043530
Iteration 16/25 | Loss: 0.00043530
Iteration 17/25 | Loss: 0.00043530
Iteration 18/25 | Loss: 0.00043530
Iteration 19/25 | Loss: 0.00043530
Iteration 20/25 | Loss: 0.00043530
Iteration 21/25 | Loss: 0.00043530
Iteration 22/25 | Loss: 0.00043530
Iteration 23/25 | Loss: 0.00043530
Iteration 24/25 | Loss: 0.00043530
Iteration 25/25 | Loss: 0.00043530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043530
Iteration 2/1000 | Loss: 0.00007532
Iteration 3/1000 | Loss: 0.00005786
Iteration 4/1000 | Loss: 0.00005271
Iteration 5/1000 | Loss: 0.00005012
Iteration 6/1000 | Loss: 0.00004800
Iteration 7/1000 | Loss: 0.00004683
Iteration 8/1000 | Loss: 0.00004594
Iteration 9/1000 | Loss: 0.00004545
Iteration 10/1000 | Loss: 0.00004495
Iteration 11/1000 | Loss: 0.00004459
Iteration 12/1000 | Loss: 0.00004442
Iteration 13/1000 | Loss: 0.00004437
Iteration 14/1000 | Loss: 0.00004430
Iteration 15/1000 | Loss: 0.00004427
Iteration 16/1000 | Loss: 0.00004427
Iteration 17/1000 | Loss: 0.00004422
Iteration 18/1000 | Loss: 0.00004422
Iteration 19/1000 | Loss: 0.00004422
Iteration 20/1000 | Loss: 0.00004421
Iteration 21/1000 | Loss: 0.00004421
Iteration 22/1000 | Loss: 0.00004420
Iteration 23/1000 | Loss: 0.00004416
Iteration 24/1000 | Loss: 0.00004416
Iteration 25/1000 | Loss: 0.00004413
Iteration 26/1000 | Loss: 0.00004412
Iteration 27/1000 | Loss: 0.00004411
Iteration 28/1000 | Loss: 0.00004411
Iteration 29/1000 | Loss: 0.00004411
Iteration 30/1000 | Loss: 0.00004411
Iteration 31/1000 | Loss: 0.00004411
Iteration 32/1000 | Loss: 0.00004410
Iteration 33/1000 | Loss: 0.00004410
Iteration 34/1000 | Loss: 0.00004409
Iteration 35/1000 | Loss: 0.00004408
Iteration 36/1000 | Loss: 0.00004408
Iteration 37/1000 | Loss: 0.00004408
Iteration 38/1000 | Loss: 0.00004407
Iteration 39/1000 | Loss: 0.00004407
Iteration 40/1000 | Loss: 0.00004406
Iteration 41/1000 | Loss: 0.00004406
Iteration 42/1000 | Loss: 0.00004406
Iteration 43/1000 | Loss: 0.00004406
Iteration 44/1000 | Loss: 0.00004406
Iteration 45/1000 | Loss: 0.00004406
Iteration 46/1000 | Loss: 0.00004406
Iteration 47/1000 | Loss: 0.00004405
Iteration 48/1000 | Loss: 0.00004404
Iteration 49/1000 | Loss: 0.00004404
Iteration 50/1000 | Loss: 0.00004403
Iteration 51/1000 | Loss: 0.00004403
Iteration 52/1000 | Loss: 0.00004402
Iteration 53/1000 | Loss: 0.00004402
Iteration 54/1000 | Loss: 0.00004402
Iteration 55/1000 | Loss: 0.00004402
Iteration 56/1000 | Loss: 0.00004402
Iteration 57/1000 | Loss: 0.00004402
Iteration 58/1000 | Loss: 0.00004402
Iteration 59/1000 | Loss: 0.00004401
Iteration 60/1000 | Loss: 0.00004401
Iteration 61/1000 | Loss: 0.00004401
Iteration 62/1000 | Loss: 0.00004401
Iteration 63/1000 | Loss: 0.00004401
Iteration 64/1000 | Loss: 0.00004401
Iteration 65/1000 | Loss: 0.00004401
Iteration 66/1000 | Loss: 0.00004401
Iteration 67/1000 | Loss: 0.00004400
Iteration 68/1000 | Loss: 0.00004399
Iteration 69/1000 | Loss: 0.00004399
Iteration 70/1000 | Loss: 0.00004399
Iteration 71/1000 | Loss: 0.00004399
Iteration 72/1000 | Loss: 0.00004399
Iteration 73/1000 | Loss: 0.00004399
Iteration 74/1000 | Loss: 0.00004399
Iteration 75/1000 | Loss: 0.00004399
Iteration 76/1000 | Loss: 0.00004399
Iteration 77/1000 | Loss: 0.00004398
Iteration 78/1000 | Loss: 0.00004398
Iteration 79/1000 | Loss: 0.00004398
Iteration 80/1000 | Loss: 0.00004398
Iteration 81/1000 | Loss: 0.00004398
Iteration 82/1000 | Loss: 0.00004398
Iteration 83/1000 | Loss: 0.00004397
Iteration 84/1000 | Loss: 0.00004397
Iteration 85/1000 | Loss: 0.00004397
Iteration 86/1000 | Loss: 0.00004397
Iteration 87/1000 | Loss: 0.00004397
Iteration 88/1000 | Loss: 0.00004397
Iteration 89/1000 | Loss: 0.00004397
Iteration 90/1000 | Loss: 0.00004397
Iteration 91/1000 | Loss: 0.00004396
Iteration 92/1000 | Loss: 0.00004396
Iteration 93/1000 | Loss: 0.00004396
Iteration 94/1000 | Loss: 0.00004396
Iteration 95/1000 | Loss: 0.00004396
Iteration 96/1000 | Loss: 0.00004396
Iteration 97/1000 | Loss: 0.00004396
Iteration 98/1000 | Loss: 0.00004396
Iteration 99/1000 | Loss: 0.00004396
Iteration 100/1000 | Loss: 0.00004396
Iteration 101/1000 | Loss: 0.00004396
Iteration 102/1000 | Loss: 0.00004396
Iteration 103/1000 | Loss: 0.00004395
Iteration 104/1000 | Loss: 0.00004395
Iteration 105/1000 | Loss: 0.00004395
Iteration 106/1000 | Loss: 0.00004394
Iteration 107/1000 | Loss: 0.00004394
Iteration 108/1000 | Loss: 0.00004394
Iteration 109/1000 | Loss: 0.00004394
Iteration 110/1000 | Loss: 0.00004394
Iteration 111/1000 | Loss: 0.00004394
Iteration 112/1000 | Loss: 0.00004394
Iteration 113/1000 | Loss: 0.00004394
Iteration 114/1000 | Loss: 0.00004394
Iteration 115/1000 | Loss: 0.00004394
Iteration 116/1000 | Loss: 0.00004394
Iteration 117/1000 | Loss: 0.00004394
Iteration 118/1000 | Loss: 0.00004394
Iteration 119/1000 | Loss: 0.00004394
Iteration 120/1000 | Loss: 0.00004393
Iteration 121/1000 | Loss: 0.00004393
Iteration 122/1000 | Loss: 0.00004393
Iteration 123/1000 | Loss: 0.00004393
Iteration 124/1000 | Loss: 0.00004393
Iteration 125/1000 | Loss: 0.00004392
Iteration 126/1000 | Loss: 0.00004392
Iteration 127/1000 | Loss: 0.00004392
Iteration 128/1000 | Loss: 0.00004392
Iteration 129/1000 | Loss: 0.00004392
Iteration 130/1000 | Loss: 0.00004392
Iteration 131/1000 | Loss: 0.00004391
Iteration 132/1000 | Loss: 0.00004391
Iteration 133/1000 | Loss: 0.00004391
Iteration 134/1000 | Loss: 0.00004391
Iteration 135/1000 | Loss: 0.00004391
Iteration 136/1000 | Loss: 0.00004391
Iteration 137/1000 | Loss: 0.00004390
Iteration 138/1000 | Loss: 0.00004390
Iteration 139/1000 | Loss: 0.00004390
Iteration 140/1000 | Loss: 0.00004390
Iteration 141/1000 | Loss: 0.00004390
Iteration 142/1000 | Loss: 0.00004390
Iteration 143/1000 | Loss: 0.00004390
Iteration 144/1000 | Loss: 0.00004390
Iteration 145/1000 | Loss: 0.00004390
Iteration 146/1000 | Loss: 0.00004390
Iteration 147/1000 | Loss: 0.00004389
Iteration 148/1000 | Loss: 0.00004388
Iteration 149/1000 | Loss: 0.00004388
Iteration 150/1000 | Loss: 0.00004388
Iteration 151/1000 | Loss: 0.00004388
Iteration 152/1000 | Loss: 0.00004388
Iteration 153/1000 | Loss: 0.00004388
Iteration 154/1000 | Loss: 0.00004388
Iteration 155/1000 | Loss: 0.00004388
Iteration 156/1000 | Loss: 0.00004388
Iteration 157/1000 | Loss: 0.00004387
Iteration 158/1000 | Loss: 0.00004387
Iteration 159/1000 | Loss: 0.00004387
Iteration 160/1000 | Loss: 0.00004387
Iteration 161/1000 | Loss: 0.00004386
Iteration 162/1000 | Loss: 0.00004386
Iteration 163/1000 | Loss: 0.00004386
Iteration 164/1000 | Loss: 0.00004386
Iteration 165/1000 | Loss: 0.00004385
Iteration 166/1000 | Loss: 0.00004385
Iteration 167/1000 | Loss: 0.00004385
Iteration 168/1000 | Loss: 0.00004385
Iteration 169/1000 | Loss: 0.00004385
Iteration 170/1000 | Loss: 0.00004385
Iteration 171/1000 | Loss: 0.00004385
Iteration 172/1000 | Loss: 0.00004385
Iteration 173/1000 | Loss: 0.00004384
Iteration 174/1000 | Loss: 0.00004384
Iteration 175/1000 | Loss: 0.00004384
Iteration 176/1000 | Loss: 0.00004384
Iteration 177/1000 | Loss: 0.00004384
Iteration 178/1000 | Loss: 0.00004384
Iteration 179/1000 | Loss: 0.00004384
Iteration 180/1000 | Loss: 0.00004384
Iteration 181/1000 | Loss: 0.00004384
Iteration 182/1000 | Loss: 0.00004384
Iteration 183/1000 | Loss: 0.00004384
Iteration 184/1000 | Loss: 0.00004383
Iteration 185/1000 | Loss: 0.00004383
Iteration 186/1000 | Loss: 0.00004383
Iteration 187/1000 | Loss: 0.00004383
Iteration 188/1000 | Loss: 0.00004383
Iteration 189/1000 | Loss: 0.00004383
Iteration 190/1000 | Loss: 0.00004383
Iteration 191/1000 | Loss: 0.00004382
Iteration 192/1000 | Loss: 0.00004382
Iteration 193/1000 | Loss: 0.00004382
Iteration 194/1000 | Loss: 0.00004382
Iteration 195/1000 | Loss: 0.00004382
Iteration 196/1000 | Loss: 0.00004382
Iteration 197/1000 | Loss: 0.00004382
Iteration 198/1000 | Loss: 0.00004381
Iteration 199/1000 | Loss: 0.00004381
Iteration 200/1000 | Loss: 0.00004381
Iteration 201/1000 | Loss: 0.00004381
Iteration 202/1000 | Loss: 0.00004381
Iteration 203/1000 | Loss: 0.00004381
Iteration 204/1000 | Loss: 0.00004381
Iteration 205/1000 | Loss: 0.00004380
Iteration 206/1000 | Loss: 0.00004380
Iteration 207/1000 | Loss: 0.00004380
Iteration 208/1000 | Loss: 0.00004380
Iteration 209/1000 | Loss: 0.00004380
Iteration 210/1000 | Loss: 0.00004380
Iteration 211/1000 | Loss: 0.00004380
Iteration 212/1000 | Loss: 0.00004380
Iteration 213/1000 | Loss: 0.00004380
Iteration 214/1000 | Loss: 0.00004379
Iteration 215/1000 | Loss: 0.00004379
Iteration 216/1000 | Loss: 0.00004379
Iteration 217/1000 | Loss: 0.00004379
Iteration 218/1000 | Loss: 0.00004379
Iteration 219/1000 | Loss: 0.00004379
Iteration 220/1000 | Loss: 0.00004379
Iteration 221/1000 | Loss: 0.00004379
Iteration 222/1000 | Loss: 0.00004378
Iteration 223/1000 | Loss: 0.00004378
Iteration 224/1000 | Loss: 0.00004378
Iteration 225/1000 | Loss: 0.00004378
Iteration 226/1000 | Loss: 0.00004378
Iteration 227/1000 | Loss: 0.00004378
Iteration 228/1000 | Loss: 0.00004378
Iteration 229/1000 | Loss: 0.00004378
Iteration 230/1000 | Loss: 0.00004378
Iteration 231/1000 | Loss: 0.00004378
Iteration 232/1000 | Loss: 0.00004378
Iteration 233/1000 | Loss: 0.00004378
Iteration 234/1000 | Loss: 0.00004378
Iteration 235/1000 | Loss: 0.00004378
Iteration 236/1000 | Loss: 0.00004378
Iteration 237/1000 | Loss: 0.00004378
Iteration 238/1000 | Loss: 0.00004378
Iteration 239/1000 | Loss: 0.00004378
Iteration 240/1000 | Loss: 0.00004378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [4.378059020382352e-05, 4.378059020382352e-05, 4.378059020382352e-05, 4.378059020382352e-05, 4.378059020382352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.378059020382352e-05

Optimization complete. Final v2v error: 5.333991050720215 mm

Highest mean error: 6.143905162811279 mm for frame 79

Lowest mean error: 4.4084086418151855 mm for frame 29

Saving results

Total time: 49.18350553512573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141136
Iteration 2/25 | Loss: 0.00200642
Iteration 3/25 | Loss: 0.00130538
Iteration 4/25 | Loss: 0.00116794
Iteration 5/25 | Loss: 0.00114019
Iteration 6/25 | Loss: 0.00113199
Iteration 7/25 | Loss: 0.00112976
Iteration 8/25 | Loss: 0.00112931
Iteration 9/25 | Loss: 0.00112931
Iteration 10/25 | Loss: 0.00112931
Iteration 11/25 | Loss: 0.00112931
Iteration 12/25 | Loss: 0.00112931
Iteration 13/25 | Loss: 0.00112931
Iteration 14/25 | Loss: 0.00112931
Iteration 15/25 | Loss: 0.00112931
Iteration 16/25 | Loss: 0.00112931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011293076677247882, 0.0011293076677247882, 0.0011293076677247882, 0.0011293076677247882, 0.0011293076677247882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011293076677247882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74216795
Iteration 2/25 | Loss: 0.00059864
Iteration 3/25 | Loss: 0.00059854
Iteration 4/25 | Loss: 0.00059854
Iteration 5/25 | Loss: 0.00059854
Iteration 6/25 | Loss: 0.00059854
Iteration 7/25 | Loss: 0.00059854
Iteration 8/25 | Loss: 0.00059854
Iteration 9/25 | Loss: 0.00059854
Iteration 10/25 | Loss: 0.00059854
Iteration 11/25 | Loss: 0.00059854
Iteration 12/25 | Loss: 0.00059854
Iteration 13/25 | Loss: 0.00059854
Iteration 14/25 | Loss: 0.00059854
Iteration 15/25 | Loss: 0.00059854
Iteration 16/25 | Loss: 0.00059854
Iteration 17/25 | Loss: 0.00059854
Iteration 18/25 | Loss: 0.00059854
Iteration 19/25 | Loss: 0.00059854
Iteration 20/25 | Loss: 0.00059854
Iteration 21/25 | Loss: 0.00059854
Iteration 22/25 | Loss: 0.00059854
Iteration 23/25 | Loss: 0.00059854
Iteration 24/25 | Loss: 0.00059854
Iteration 25/25 | Loss: 0.00059854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059854
Iteration 2/1000 | Loss: 0.00006099
Iteration 3/1000 | Loss: 0.00004276
Iteration 4/1000 | Loss: 0.00003769
Iteration 5/1000 | Loss: 0.00003532
Iteration 6/1000 | Loss: 0.00003376
Iteration 7/1000 | Loss: 0.00003292
Iteration 8/1000 | Loss: 0.00003233
Iteration 9/1000 | Loss: 0.00003180
Iteration 10/1000 | Loss: 0.00003142
Iteration 11/1000 | Loss: 0.00003113
Iteration 12/1000 | Loss: 0.00003089
Iteration 13/1000 | Loss: 0.00003072
Iteration 14/1000 | Loss: 0.00003063
Iteration 15/1000 | Loss: 0.00003053
Iteration 16/1000 | Loss: 0.00003048
Iteration 17/1000 | Loss: 0.00003048
Iteration 18/1000 | Loss: 0.00003047
Iteration 19/1000 | Loss: 0.00003047
Iteration 20/1000 | Loss: 0.00003046
Iteration 21/1000 | Loss: 0.00003046
Iteration 22/1000 | Loss: 0.00003045
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00003044
Iteration 25/1000 | Loss: 0.00003043
Iteration 26/1000 | Loss: 0.00003041
Iteration 27/1000 | Loss: 0.00003040
Iteration 28/1000 | Loss: 0.00003040
Iteration 29/1000 | Loss: 0.00003039
Iteration 30/1000 | Loss: 0.00003039
Iteration 31/1000 | Loss: 0.00003037
Iteration 32/1000 | Loss: 0.00003035
Iteration 33/1000 | Loss: 0.00003035
Iteration 34/1000 | Loss: 0.00003034
Iteration 35/1000 | Loss: 0.00003032
Iteration 36/1000 | Loss: 0.00003031
Iteration 37/1000 | Loss: 0.00003031
Iteration 38/1000 | Loss: 0.00003030
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00003030
Iteration 41/1000 | Loss: 0.00003030
Iteration 42/1000 | Loss: 0.00003030
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003029
Iteration 45/1000 | Loss: 0.00003029
Iteration 46/1000 | Loss: 0.00003029
Iteration 47/1000 | Loss: 0.00003029
Iteration 48/1000 | Loss: 0.00003029
Iteration 49/1000 | Loss: 0.00003029
Iteration 50/1000 | Loss: 0.00003029
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00003028
Iteration 53/1000 | Loss: 0.00003028
Iteration 54/1000 | Loss: 0.00003028
Iteration 55/1000 | Loss: 0.00003028
Iteration 56/1000 | Loss: 0.00003028
Iteration 57/1000 | Loss: 0.00003028
Iteration 58/1000 | Loss: 0.00003028
Iteration 59/1000 | Loss: 0.00003027
Iteration 60/1000 | Loss: 0.00003027
Iteration 61/1000 | Loss: 0.00003027
Iteration 62/1000 | Loss: 0.00003027
Iteration 63/1000 | Loss: 0.00003027
Iteration 64/1000 | Loss: 0.00003027
Iteration 65/1000 | Loss: 0.00003026
Iteration 66/1000 | Loss: 0.00003026
Iteration 67/1000 | Loss: 0.00003026
Iteration 68/1000 | Loss: 0.00003026
Iteration 69/1000 | Loss: 0.00003026
Iteration 70/1000 | Loss: 0.00003026
Iteration 71/1000 | Loss: 0.00003026
Iteration 72/1000 | Loss: 0.00003026
Iteration 73/1000 | Loss: 0.00003025
Iteration 74/1000 | Loss: 0.00003025
Iteration 75/1000 | Loss: 0.00003025
Iteration 76/1000 | Loss: 0.00003025
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00003024
Iteration 79/1000 | Loss: 0.00003024
Iteration 80/1000 | Loss: 0.00003024
Iteration 81/1000 | Loss: 0.00003024
Iteration 82/1000 | Loss: 0.00003023
Iteration 83/1000 | Loss: 0.00003023
Iteration 84/1000 | Loss: 0.00003023
Iteration 85/1000 | Loss: 0.00003023
Iteration 86/1000 | Loss: 0.00003022
Iteration 87/1000 | Loss: 0.00003022
Iteration 88/1000 | Loss: 0.00003022
Iteration 89/1000 | Loss: 0.00003022
Iteration 90/1000 | Loss: 0.00003022
Iteration 91/1000 | Loss: 0.00003021
Iteration 92/1000 | Loss: 0.00003021
Iteration 93/1000 | Loss: 0.00003021
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003021
Iteration 96/1000 | Loss: 0.00003021
Iteration 97/1000 | Loss: 0.00003021
Iteration 98/1000 | Loss: 0.00003021
Iteration 99/1000 | Loss: 0.00003021
Iteration 100/1000 | Loss: 0.00003021
Iteration 101/1000 | Loss: 0.00003021
Iteration 102/1000 | Loss: 0.00003021
Iteration 103/1000 | Loss: 0.00003021
Iteration 104/1000 | Loss: 0.00003021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.0205010261852294e-05, 3.0205010261852294e-05, 3.0205010261852294e-05, 3.0205010261852294e-05, 3.0205010261852294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0205010261852294e-05

Optimization complete. Final v2v error: 4.672718524932861 mm

Highest mean error: 6.01364278793335 mm for frame 212

Lowest mean error: 3.9111649990081787 mm for frame 9

Saving results

Total time: 45.53268766403198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418232
Iteration 2/25 | Loss: 0.00121473
Iteration 3/25 | Loss: 0.00106933
Iteration 4/25 | Loss: 0.00104775
Iteration 5/25 | Loss: 0.00103844
Iteration 6/25 | Loss: 0.00103599
Iteration 7/25 | Loss: 0.00103554
Iteration 8/25 | Loss: 0.00103554
Iteration 9/25 | Loss: 0.00103554
Iteration 10/25 | Loss: 0.00103554
Iteration 11/25 | Loss: 0.00103554
Iteration 12/25 | Loss: 0.00103554
Iteration 13/25 | Loss: 0.00103554
Iteration 14/25 | Loss: 0.00103554
Iteration 15/25 | Loss: 0.00103554
Iteration 16/25 | Loss: 0.00103554
Iteration 17/25 | Loss: 0.00103554
Iteration 18/25 | Loss: 0.00103554
Iteration 19/25 | Loss: 0.00103554
Iteration 20/25 | Loss: 0.00103554
Iteration 21/25 | Loss: 0.00103554
Iteration 22/25 | Loss: 0.00103554
Iteration 23/25 | Loss: 0.00103554
Iteration 24/25 | Loss: 0.00103554
Iteration 25/25 | Loss: 0.00103554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010355358244851232, 0.0010355358244851232, 0.0010355358244851232, 0.0010355358244851232, 0.0010355358244851232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010355358244851232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64497757
Iteration 2/25 | Loss: 0.00052620
Iteration 3/25 | Loss: 0.00052620
Iteration 4/25 | Loss: 0.00052620
Iteration 5/25 | Loss: 0.00052620
Iteration 6/25 | Loss: 0.00052620
Iteration 7/25 | Loss: 0.00052620
Iteration 8/25 | Loss: 0.00052620
Iteration 9/25 | Loss: 0.00052620
Iteration 10/25 | Loss: 0.00052620
Iteration 11/25 | Loss: 0.00052620
Iteration 12/25 | Loss: 0.00052620
Iteration 13/25 | Loss: 0.00052620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005261980113573372, 0.0005261980113573372, 0.0005261980113573372, 0.0005261980113573372, 0.0005261980113573372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005261980113573372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052620
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00002083
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001897
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001894
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001894
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001890
Iteration 41/1000 | Loss: 0.00001890
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001890
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001886
Iteration 93/1000 | Loss: 0.00001886
Iteration 94/1000 | Loss: 0.00001886
Iteration 95/1000 | Loss: 0.00001886
Iteration 96/1000 | Loss: 0.00001886
Iteration 97/1000 | Loss: 0.00001886
Iteration 98/1000 | Loss: 0.00001886
Iteration 99/1000 | Loss: 0.00001886
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001886
Iteration 102/1000 | Loss: 0.00001886
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001886
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001886
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001886
Iteration 110/1000 | Loss: 0.00001886
Iteration 111/1000 | Loss: 0.00001886
Iteration 112/1000 | Loss: 0.00001886
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001886
Iteration 115/1000 | Loss: 0.00001886
Iteration 116/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.8856371752917767e-05, 1.8856371752917767e-05, 1.8856371752917767e-05, 1.8856371752917767e-05, 1.8856371752917767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8856371752917767e-05

Optimization complete. Final v2v error: 3.83161997795105 mm

Highest mean error: 4.09652853012085 mm for frame 160

Lowest mean error: 3.4654433727264404 mm for frame 257

Saving results

Total time: 38.29393458366394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843051
Iteration 2/25 | Loss: 0.00133418
Iteration 3/25 | Loss: 0.00110519
Iteration 4/25 | Loss: 0.00103241
Iteration 5/25 | Loss: 0.00100420
Iteration 6/25 | Loss: 0.00099992
Iteration 7/25 | Loss: 0.00099570
Iteration 8/25 | Loss: 0.00099284
Iteration 9/25 | Loss: 0.00099177
Iteration 10/25 | Loss: 0.00099099
Iteration 11/25 | Loss: 0.00099040
Iteration 12/25 | Loss: 0.00099013
Iteration 13/25 | Loss: 0.00099000
Iteration 14/25 | Loss: 0.00098991
Iteration 15/25 | Loss: 0.00098990
Iteration 16/25 | Loss: 0.00098990
Iteration 17/25 | Loss: 0.00098990
Iteration 18/25 | Loss: 0.00098990
Iteration 19/25 | Loss: 0.00098990
Iteration 20/25 | Loss: 0.00098990
Iteration 21/25 | Loss: 0.00098990
Iteration 22/25 | Loss: 0.00098990
Iteration 23/25 | Loss: 0.00098989
Iteration 24/25 | Loss: 0.00098989
Iteration 25/25 | Loss: 0.00098989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13293767
Iteration 2/25 | Loss: 0.00047860
Iteration 3/25 | Loss: 0.00047859
Iteration 4/25 | Loss: 0.00047859
Iteration 5/25 | Loss: 0.00047859
Iteration 6/25 | Loss: 0.00047859
Iteration 7/25 | Loss: 0.00047859
Iteration 8/25 | Loss: 0.00047859
Iteration 9/25 | Loss: 0.00047859
Iteration 10/25 | Loss: 0.00047859
Iteration 11/25 | Loss: 0.00047859
Iteration 12/25 | Loss: 0.00047858
Iteration 13/25 | Loss: 0.00047858
Iteration 14/25 | Loss: 0.00047858
Iteration 15/25 | Loss: 0.00047858
Iteration 16/25 | Loss: 0.00047858
Iteration 17/25 | Loss: 0.00047858
Iteration 18/25 | Loss: 0.00047858
Iteration 19/25 | Loss: 0.00047858
Iteration 20/25 | Loss: 0.00047858
Iteration 21/25 | Loss: 0.00047859
Iteration 22/25 | Loss: 0.00047859
Iteration 23/25 | Loss: 0.00047859
Iteration 24/25 | Loss: 0.00047859
Iteration 25/25 | Loss: 0.00047859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047859
Iteration 2/1000 | Loss: 0.00003842
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002358
Iteration 6/1000 | Loss: 0.00002290
Iteration 7/1000 | Loss: 0.00002242
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002183
Iteration 10/1000 | Loss: 0.00002164
Iteration 11/1000 | Loss: 0.00002157
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002156
Iteration 14/1000 | Loss: 0.00002155
Iteration 15/1000 | Loss: 0.00002154
Iteration 16/1000 | Loss: 0.00002154
Iteration 17/1000 | Loss: 0.00002152
Iteration 18/1000 | Loss: 0.00002152
Iteration 19/1000 | Loss: 0.00002151
Iteration 20/1000 | Loss: 0.00002156
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002155
Iteration 23/1000 | Loss: 0.00002155
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002147
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002145
Iteration 33/1000 | Loss: 0.00002145
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002144
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002143
Iteration 38/1000 | Loss: 0.00002143
Iteration 39/1000 | Loss: 0.00002143
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002142
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002142
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002141
Iteration 47/1000 | Loss: 0.00002140
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002140
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002140
Iteration 53/1000 | Loss: 0.00002140
Iteration 54/1000 | Loss: 0.00002140
Iteration 55/1000 | Loss: 0.00002140
Iteration 56/1000 | Loss: 0.00002140
Iteration 57/1000 | Loss: 0.00002140
Iteration 58/1000 | Loss: 0.00002140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.1401172489277087e-05, 2.1401172489277087e-05, 2.1401172489277087e-05, 2.1401172489277087e-05, 2.1401172489277087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1401172489277087e-05

Optimization complete. Final v2v error: 3.934133768081665 mm

Highest mean error: 9.895400047302246 mm for frame 3

Lowest mean error: 3.5358426570892334 mm for frame 232

Saving results

Total time: 51.485600233078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987596
Iteration 2/25 | Loss: 0.00170743
Iteration 3/25 | Loss: 0.00120761
Iteration 4/25 | Loss: 0.00115701
Iteration 5/25 | Loss: 0.00113336
Iteration 6/25 | Loss: 0.00112576
Iteration 7/25 | Loss: 0.00112512
Iteration 8/25 | Loss: 0.00112512
Iteration 9/25 | Loss: 0.00112512
Iteration 10/25 | Loss: 0.00112512
Iteration 11/25 | Loss: 0.00112512
Iteration 12/25 | Loss: 0.00112512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011251192772760987, 0.0011251192772760987, 0.0011251192772760987, 0.0011251192772760987, 0.0011251192772760987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011251192772760987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11652482
Iteration 2/25 | Loss: 0.00053519
Iteration 3/25 | Loss: 0.00053517
Iteration 4/25 | Loss: 0.00053517
Iteration 5/25 | Loss: 0.00053517
Iteration 6/25 | Loss: 0.00053517
Iteration 7/25 | Loss: 0.00053517
Iteration 8/25 | Loss: 0.00053517
Iteration 9/25 | Loss: 0.00053517
Iteration 10/25 | Loss: 0.00053517
Iteration 11/25 | Loss: 0.00053517
Iteration 12/25 | Loss: 0.00053517
Iteration 13/25 | Loss: 0.00053517
Iteration 14/25 | Loss: 0.00053517
Iteration 15/25 | Loss: 0.00053517
Iteration 16/25 | Loss: 0.00053517
Iteration 17/25 | Loss: 0.00053517
Iteration 18/25 | Loss: 0.00053517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005351715954020619, 0.0005351715954020619, 0.0005351715954020619, 0.0005351715954020619, 0.0005351715954020619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005351715954020619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053517
Iteration 2/1000 | Loss: 0.00005239
Iteration 3/1000 | Loss: 0.00004400
Iteration 4/1000 | Loss: 0.00004046
Iteration 5/1000 | Loss: 0.00003855
Iteration 6/1000 | Loss: 0.00003706
Iteration 7/1000 | Loss: 0.00003597
Iteration 8/1000 | Loss: 0.00003532
Iteration 9/1000 | Loss: 0.00003496
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00003452
Iteration 12/1000 | Loss: 0.00003449
Iteration 13/1000 | Loss: 0.00003449
Iteration 14/1000 | Loss: 0.00003449
Iteration 15/1000 | Loss: 0.00003448
Iteration 16/1000 | Loss: 0.00003447
Iteration 17/1000 | Loss: 0.00003446
Iteration 18/1000 | Loss: 0.00003445
Iteration 19/1000 | Loss: 0.00003445
Iteration 20/1000 | Loss: 0.00003444
Iteration 21/1000 | Loss: 0.00003441
Iteration 22/1000 | Loss: 0.00003440
Iteration 23/1000 | Loss: 0.00003440
Iteration 24/1000 | Loss: 0.00003440
Iteration 25/1000 | Loss: 0.00003440
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00003440
Iteration 28/1000 | Loss: 0.00003439
Iteration 29/1000 | Loss: 0.00003439
Iteration 30/1000 | Loss: 0.00003439
Iteration 31/1000 | Loss: 0.00003439
Iteration 32/1000 | Loss: 0.00003439
Iteration 33/1000 | Loss: 0.00003439
Iteration 34/1000 | Loss: 0.00003439
Iteration 35/1000 | Loss: 0.00003439
Iteration 36/1000 | Loss: 0.00003439
Iteration 37/1000 | Loss: 0.00003439
Iteration 38/1000 | Loss: 0.00003439
Iteration 39/1000 | Loss: 0.00003439
Iteration 40/1000 | Loss: 0.00003439
Iteration 41/1000 | Loss: 0.00003438
Iteration 42/1000 | Loss: 0.00003437
Iteration 43/1000 | Loss: 0.00003437
Iteration 44/1000 | Loss: 0.00003437
Iteration 45/1000 | Loss: 0.00003437
Iteration 46/1000 | Loss: 0.00003436
Iteration 47/1000 | Loss: 0.00003436
Iteration 48/1000 | Loss: 0.00003436
Iteration 49/1000 | Loss: 0.00003435
Iteration 50/1000 | Loss: 0.00003435
Iteration 51/1000 | Loss: 0.00003435
Iteration 52/1000 | Loss: 0.00003435
Iteration 53/1000 | Loss: 0.00003435
Iteration 54/1000 | Loss: 0.00003434
Iteration 55/1000 | Loss: 0.00003434
Iteration 56/1000 | Loss: 0.00003434
Iteration 57/1000 | Loss: 0.00003434
Iteration 58/1000 | Loss: 0.00003434
Iteration 59/1000 | Loss: 0.00003434
Iteration 60/1000 | Loss: 0.00003433
Iteration 61/1000 | Loss: 0.00003433
Iteration 62/1000 | Loss: 0.00003433
Iteration 63/1000 | Loss: 0.00003433
Iteration 64/1000 | Loss: 0.00003433
Iteration 65/1000 | Loss: 0.00003432
Iteration 66/1000 | Loss: 0.00003432
Iteration 67/1000 | Loss: 0.00003432
Iteration 68/1000 | Loss: 0.00003432
Iteration 69/1000 | Loss: 0.00003432
Iteration 70/1000 | Loss: 0.00003432
Iteration 71/1000 | Loss: 0.00003432
Iteration 72/1000 | Loss: 0.00003431
Iteration 73/1000 | Loss: 0.00003431
Iteration 74/1000 | Loss: 0.00003431
Iteration 75/1000 | Loss: 0.00003431
Iteration 76/1000 | Loss: 0.00003431
Iteration 77/1000 | Loss: 0.00003431
Iteration 78/1000 | Loss: 0.00003431
Iteration 79/1000 | Loss: 0.00003431
Iteration 80/1000 | Loss: 0.00003431
Iteration 81/1000 | Loss: 0.00003431
Iteration 82/1000 | Loss: 0.00003431
Iteration 83/1000 | Loss: 0.00003430
Iteration 84/1000 | Loss: 0.00003430
Iteration 85/1000 | Loss: 0.00003430
Iteration 86/1000 | Loss: 0.00003430
Iteration 87/1000 | Loss: 0.00003430
Iteration 88/1000 | Loss: 0.00003430
Iteration 89/1000 | Loss: 0.00003430
Iteration 90/1000 | Loss: 0.00003430
Iteration 91/1000 | Loss: 0.00003429
Iteration 92/1000 | Loss: 0.00003429
Iteration 93/1000 | Loss: 0.00003429
Iteration 94/1000 | Loss: 0.00003429
Iteration 95/1000 | Loss: 0.00003429
Iteration 96/1000 | Loss: 0.00003429
Iteration 97/1000 | Loss: 0.00003429
Iteration 98/1000 | Loss: 0.00003429
Iteration 99/1000 | Loss: 0.00003429
Iteration 100/1000 | Loss: 0.00003429
Iteration 101/1000 | Loss: 0.00003428
Iteration 102/1000 | Loss: 0.00003428
Iteration 103/1000 | Loss: 0.00003428
Iteration 104/1000 | Loss: 0.00003428
Iteration 105/1000 | Loss: 0.00003428
Iteration 106/1000 | Loss: 0.00003428
Iteration 107/1000 | Loss: 0.00003428
Iteration 108/1000 | Loss: 0.00003428
Iteration 109/1000 | Loss: 0.00003428
Iteration 110/1000 | Loss: 0.00003428
Iteration 111/1000 | Loss: 0.00003428
Iteration 112/1000 | Loss: 0.00003428
Iteration 113/1000 | Loss: 0.00003427
Iteration 114/1000 | Loss: 0.00003427
Iteration 115/1000 | Loss: 0.00003427
Iteration 116/1000 | Loss: 0.00003427
Iteration 117/1000 | Loss: 0.00003427
Iteration 118/1000 | Loss: 0.00003427
Iteration 119/1000 | Loss: 0.00003427
Iteration 120/1000 | Loss: 0.00003427
Iteration 121/1000 | Loss: 0.00003427
Iteration 122/1000 | Loss: 0.00003427
Iteration 123/1000 | Loss: 0.00003427
Iteration 124/1000 | Loss: 0.00003427
Iteration 125/1000 | Loss: 0.00003427
Iteration 126/1000 | Loss: 0.00003426
Iteration 127/1000 | Loss: 0.00003426
Iteration 128/1000 | Loss: 0.00003426
Iteration 129/1000 | Loss: 0.00003426
Iteration 130/1000 | Loss: 0.00003426
Iteration 131/1000 | Loss: 0.00003425
Iteration 132/1000 | Loss: 0.00003425
Iteration 133/1000 | Loss: 0.00003425
Iteration 134/1000 | Loss: 0.00003425
Iteration 135/1000 | Loss: 0.00003425
Iteration 136/1000 | Loss: 0.00003425
Iteration 137/1000 | Loss: 0.00003425
Iteration 138/1000 | Loss: 0.00003425
Iteration 139/1000 | Loss: 0.00003425
Iteration 140/1000 | Loss: 0.00003425
Iteration 141/1000 | Loss: 0.00003424
Iteration 142/1000 | Loss: 0.00003424
Iteration 143/1000 | Loss: 0.00003424
Iteration 144/1000 | Loss: 0.00003424
Iteration 145/1000 | Loss: 0.00003424
Iteration 146/1000 | Loss: 0.00003424
Iteration 147/1000 | Loss: 0.00003423
Iteration 148/1000 | Loss: 0.00003423
Iteration 149/1000 | Loss: 0.00003423
Iteration 150/1000 | Loss: 0.00003423
Iteration 151/1000 | Loss: 0.00003423
Iteration 152/1000 | Loss: 0.00003423
Iteration 153/1000 | Loss: 0.00003423
Iteration 154/1000 | Loss: 0.00003423
Iteration 155/1000 | Loss: 0.00003423
Iteration 156/1000 | Loss: 0.00003422
Iteration 157/1000 | Loss: 0.00003422
Iteration 158/1000 | Loss: 0.00003422
Iteration 159/1000 | Loss: 0.00003422
Iteration 160/1000 | Loss: 0.00003422
Iteration 161/1000 | Loss: 0.00003422
Iteration 162/1000 | Loss: 0.00003422
Iteration 163/1000 | Loss: 0.00003421
Iteration 164/1000 | Loss: 0.00003421
Iteration 165/1000 | Loss: 0.00003421
Iteration 166/1000 | Loss: 0.00003421
Iteration 167/1000 | Loss: 0.00003421
Iteration 168/1000 | Loss: 0.00003421
Iteration 169/1000 | Loss: 0.00003421
Iteration 170/1000 | Loss: 0.00003421
Iteration 171/1000 | Loss: 0.00003421
Iteration 172/1000 | Loss: 0.00003421
Iteration 173/1000 | Loss: 0.00003421
Iteration 174/1000 | Loss: 0.00003421
Iteration 175/1000 | Loss: 0.00003421
Iteration 176/1000 | Loss: 0.00003421
Iteration 177/1000 | Loss: 0.00003421
Iteration 178/1000 | Loss: 0.00003420
Iteration 179/1000 | Loss: 0.00003420
Iteration 180/1000 | Loss: 0.00003420
Iteration 181/1000 | Loss: 0.00003420
Iteration 182/1000 | Loss: 0.00003420
Iteration 183/1000 | Loss: 0.00003420
Iteration 184/1000 | Loss: 0.00003420
Iteration 185/1000 | Loss: 0.00003420
Iteration 186/1000 | Loss: 0.00003420
Iteration 187/1000 | Loss: 0.00003420
Iteration 188/1000 | Loss: 0.00003420
Iteration 189/1000 | Loss: 0.00003420
Iteration 190/1000 | Loss: 0.00003420
Iteration 191/1000 | Loss: 0.00003420
Iteration 192/1000 | Loss: 0.00003420
Iteration 193/1000 | Loss: 0.00003420
Iteration 194/1000 | Loss: 0.00003420
Iteration 195/1000 | Loss: 0.00003420
Iteration 196/1000 | Loss: 0.00003420
Iteration 197/1000 | Loss: 0.00003420
Iteration 198/1000 | Loss: 0.00003420
Iteration 199/1000 | Loss: 0.00003420
Iteration 200/1000 | Loss: 0.00003420
Iteration 201/1000 | Loss: 0.00003420
Iteration 202/1000 | Loss: 0.00003419
Iteration 203/1000 | Loss: 0.00003419
Iteration 204/1000 | Loss: 0.00003419
Iteration 205/1000 | Loss: 0.00003419
Iteration 206/1000 | Loss: 0.00003419
Iteration 207/1000 | Loss: 0.00003419
Iteration 208/1000 | Loss: 0.00003419
Iteration 209/1000 | Loss: 0.00003419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [3.419469794607721e-05, 3.419469794607721e-05, 3.419469794607721e-05, 3.419469794607721e-05, 3.419469794607721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.419469794607721e-05

Optimization complete. Final v2v error: 4.864763259887695 mm

Highest mean error: 5.85872220993042 mm for frame 102

Lowest mean error: 4.1183977127075195 mm for frame 39

Saving results

Total time: 39.745399475097656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477386
Iteration 2/25 | Loss: 0.00126868
Iteration 3/25 | Loss: 0.00115842
Iteration 4/25 | Loss: 0.00112694
Iteration 5/25 | Loss: 0.00111181
Iteration 6/25 | Loss: 0.00110905
Iteration 7/25 | Loss: 0.00110812
Iteration 8/25 | Loss: 0.00110791
Iteration 9/25 | Loss: 0.00110791
Iteration 10/25 | Loss: 0.00110791
Iteration 11/25 | Loss: 0.00110791
Iteration 12/25 | Loss: 0.00110791
Iteration 13/25 | Loss: 0.00110791
Iteration 14/25 | Loss: 0.00110791
Iteration 15/25 | Loss: 0.00110791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001107912277802825, 0.001107912277802825, 0.001107912277802825, 0.001107912277802825, 0.001107912277802825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001107912277802825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23613274
Iteration 2/25 | Loss: 0.00063144
Iteration 3/25 | Loss: 0.00063141
Iteration 4/25 | Loss: 0.00063141
Iteration 5/25 | Loss: 0.00063141
Iteration 6/25 | Loss: 0.00063141
Iteration 7/25 | Loss: 0.00063141
Iteration 8/25 | Loss: 0.00063141
Iteration 9/25 | Loss: 0.00063141
Iteration 10/25 | Loss: 0.00063141
Iteration 11/25 | Loss: 0.00063141
Iteration 12/25 | Loss: 0.00063141
Iteration 13/25 | Loss: 0.00063141
Iteration 14/25 | Loss: 0.00063141
Iteration 15/25 | Loss: 0.00063141
Iteration 16/25 | Loss: 0.00063141
Iteration 17/25 | Loss: 0.00063141
Iteration 18/25 | Loss: 0.00063141
Iteration 19/25 | Loss: 0.00063141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006314124329946935, 0.0006314124329946935, 0.0006314124329946935, 0.0006314124329946935, 0.0006314124329946935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006314124329946935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063141
Iteration 2/1000 | Loss: 0.00003858
Iteration 3/1000 | Loss: 0.00003092
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002456
Iteration 7/1000 | Loss: 0.00002386
Iteration 8/1000 | Loss: 0.00002332
Iteration 9/1000 | Loss: 0.00002300
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002253
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002241
Iteration 14/1000 | Loss: 0.00002240
Iteration 15/1000 | Loss: 0.00002230
Iteration 16/1000 | Loss: 0.00002227
Iteration 17/1000 | Loss: 0.00002226
Iteration 18/1000 | Loss: 0.00002226
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002226
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00002225
Iteration 23/1000 | Loss: 0.00002225
Iteration 24/1000 | Loss: 0.00002225
Iteration 25/1000 | Loss: 0.00002225
Iteration 26/1000 | Loss: 0.00002225
Iteration 27/1000 | Loss: 0.00002224
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002222
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002222
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00002221
Iteration 34/1000 | Loss: 0.00002221
Iteration 35/1000 | Loss: 0.00002221
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002218
Iteration 47/1000 | Loss: 0.00002218
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002217
Iteration 50/1000 | Loss: 0.00002217
Iteration 51/1000 | Loss: 0.00002217
Iteration 52/1000 | Loss: 0.00002217
Iteration 53/1000 | Loss: 0.00002216
Iteration 54/1000 | Loss: 0.00002216
Iteration 55/1000 | Loss: 0.00002216
Iteration 56/1000 | Loss: 0.00002216
Iteration 57/1000 | Loss: 0.00002215
Iteration 58/1000 | Loss: 0.00002215
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002215
Iteration 61/1000 | Loss: 0.00002215
Iteration 62/1000 | Loss: 0.00002215
Iteration 63/1000 | Loss: 0.00002215
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00002215
Iteration 66/1000 | Loss: 0.00002214
Iteration 67/1000 | Loss: 0.00002214
Iteration 68/1000 | Loss: 0.00002214
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002214
Iteration 72/1000 | Loss: 0.00002214
Iteration 73/1000 | Loss: 0.00002214
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00002214
Iteration 76/1000 | Loss: 0.00002214
Iteration 77/1000 | Loss: 0.00002213
Iteration 78/1000 | Loss: 0.00002213
Iteration 79/1000 | Loss: 0.00002213
Iteration 80/1000 | Loss: 0.00002213
Iteration 81/1000 | Loss: 0.00002213
Iteration 82/1000 | Loss: 0.00002213
Iteration 83/1000 | Loss: 0.00002213
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002213
Iteration 86/1000 | Loss: 0.00002212
Iteration 87/1000 | Loss: 0.00002212
Iteration 88/1000 | Loss: 0.00002212
Iteration 89/1000 | Loss: 0.00002212
Iteration 90/1000 | Loss: 0.00002212
Iteration 91/1000 | Loss: 0.00002211
Iteration 92/1000 | Loss: 0.00002211
Iteration 93/1000 | Loss: 0.00002211
Iteration 94/1000 | Loss: 0.00002211
Iteration 95/1000 | Loss: 0.00002211
Iteration 96/1000 | Loss: 0.00002211
Iteration 97/1000 | Loss: 0.00002211
Iteration 98/1000 | Loss: 0.00002211
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002211
Iteration 101/1000 | Loss: 0.00002211
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00002211
Iteration 104/1000 | Loss: 0.00002211
Iteration 105/1000 | Loss: 0.00002210
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002210
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002210
Iteration 111/1000 | Loss: 0.00002210
Iteration 112/1000 | Loss: 0.00002210
Iteration 113/1000 | Loss: 0.00002210
Iteration 114/1000 | Loss: 0.00002210
Iteration 115/1000 | Loss: 0.00002210
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002210
Iteration 119/1000 | Loss: 0.00002210
Iteration 120/1000 | Loss: 0.00002210
Iteration 121/1000 | Loss: 0.00002210
Iteration 122/1000 | Loss: 0.00002210
Iteration 123/1000 | Loss: 0.00002210
Iteration 124/1000 | Loss: 0.00002210
Iteration 125/1000 | Loss: 0.00002210
Iteration 126/1000 | Loss: 0.00002210
Iteration 127/1000 | Loss: 0.00002210
Iteration 128/1000 | Loss: 0.00002210
Iteration 129/1000 | Loss: 0.00002210
Iteration 130/1000 | Loss: 0.00002210
Iteration 131/1000 | Loss: 0.00002210
Iteration 132/1000 | Loss: 0.00002210
Iteration 133/1000 | Loss: 0.00002210
Iteration 134/1000 | Loss: 0.00002210
Iteration 135/1000 | Loss: 0.00002210
Iteration 136/1000 | Loss: 0.00002210
Iteration 137/1000 | Loss: 0.00002210
Iteration 138/1000 | Loss: 0.00002210
Iteration 139/1000 | Loss: 0.00002210
Iteration 140/1000 | Loss: 0.00002210
Iteration 141/1000 | Loss: 0.00002210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2104746676632203e-05, 2.2104746676632203e-05, 2.2104746676632203e-05, 2.2104746676632203e-05, 2.2104746676632203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2104746676632203e-05

Optimization complete. Final v2v error: 4.088545322418213 mm

Highest mean error: 4.420101642608643 mm for frame 94

Lowest mean error: 3.5267391204833984 mm for frame 9

Saving results

Total time: 37.7144935131073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075914
Iteration 2/25 | Loss: 0.01075914
Iteration 3/25 | Loss: 0.01075914
Iteration 4/25 | Loss: 0.01075913
Iteration 5/25 | Loss: 0.01075913
Iteration 6/25 | Loss: 0.00243218
Iteration 7/25 | Loss: 0.00180396
Iteration 8/25 | Loss: 0.00166128
Iteration 9/25 | Loss: 0.00144517
Iteration 10/25 | Loss: 0.00141908
Iteration 11/25 | Loss: 0.00138381
Iteration 12/25 | Loss: 0.00136604
Iteration 13/25 | Loss: 0.00127979
Iteration 14/25 | Loss: 0.00122045
Iteration 15/25 | Loss: 0.00122038
Iteration 16/25 | Loss: 0.00119715
Iteration 17/25 | Loss: 0.00115956
Iteration 18/25 | Loss: 0.00114292
Iteration 19/25 | Loss: 0.00110885
Iteration 20/25 | Loss: 0.00107421
Iteration 21/25 | Loss: 0.00103478
Iteration 22/25 | Loss: 0.00100982
Iteration 23/25 | Loss: 0.00100325
Iteration 24/25 | Loss: 0.00100135
Iteration 25/25 | Loss: 0.00100481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38328850
Iteration 2/25 | Loss: 0.00065841
Iteration 3/25 | Loss: 0.00061123
Iteration 4/25 | Loss: 0.00061123
Iteration 5/25 | Loss: 0.00061123
Iteration 6/25 | Loss: 0.00061123
Iteration 7/25 | Loss: 0.00061123
Iteration 8/25 | Loss: 0.00061123
Iteration 9/25 | Loss: 0.00061123
Iteration 10/25 | Loss: 0.00061123
Iteration 11/25 | Loss: 0.00061123
Iteration 12/25 | Loss: 0.00061123
Iteration 13/25 | Loss: 0.00061123
Iteration 14/25 | Loss: 0.00061123
Iteration 15/25 | Loss: 0.00061123
Iteration 16/25 | Loss: 0.00061123
Iteration 17/25 | Loss: 0.00061123
Iteration 18/25 | Loss: 0.00061123
Iteration 19/25 | Loss: 0.00061123
Iteration 20/25 | Loss: 0.00061123
Iteration 21/25 | Loss: 0.00061123
Iteration 22/25 | Loss: 0.00061123
Iteration 23/25 | Loss: 0.00061123
Iteration 24/25 | Loss: 0.00061123
Iteration 25/25 | Loss: 0.00061123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006112336996011436, 0.0006112336996011436, 0.0006112336996011436, 0.0006112336996011436, 0.0006112336996011436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006112336996011436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061123
Iteration 2/1000 | Loss: 0.00012616
Iteration 3/1000 | Loss: 0.00012525
Iteration 4/1000 | Loss: 0.00013007
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00008261
Iteration 7/1000 | Loss: 0.00006063
Iteration 8/1000 | Loss: 0.00023877
Iteration 9/1000 | Loss: 0.00012140
Iteration 10/1000 | Loss: 0.00005289
Iteration 11/1000 | Loss: 0.00009073
Iteration 12/1000 | Loss: 0.00004892
Iteration 13/1000 | Loss: 0.00004378
Iteration 14/1000 | Loss: 0.00007407
Iteration 15/1000 | Loss: 0.00009457
Iteration 16/1000 | Loss: 0.00004495
Iteration 17/1000 | Loss: 0.00003315
Iteration 18/1000 | Loss: 0.00005696
Iteration 19/1000 | Loss: 0.00006383
Iteration 20/1000 | Loss: 0.00020269
Iteration 21/1000 | Loss: 0.00014328
Iteration 22/1000 | Loss: 0.00006663
Iteration 23/1000 | Loss: 0.00003952
Iteration 24/1000 | Loss: 0.00003344
Iteration 25/1000 | Loss: 0.00005873
Iteration 26/1000 | Loss: 0.00005224
Iteration 27/1000 | Loss: 0.00012137
Iteration 28/1000 | Loss: 0.00003900
Iteration 29/1000 | Loss: 0.00003467
Iteration 30/1000 | Loss: 0.00002828
Iteration 31/1000 | Loss: 0.00003161
Iteration 32/1000 | Loss: 0.00008385
Iteration 33/1000 | Loss: 0.00002725
Iteration 34/1000 | Loss: 0.00003213
Iteration 35/1000 | Loss: 0.00005459
Iteration 36/1000 | Loss: 0.00004029
Iteration 37/1000 | Loss: 0.00003599
Iteration 38/1000 | Loss: 0.00006393
Iteration 39/1000 | Loss: 0.00005262
Iteration 40/1000 | Loss: 0.00004364
Iteration 41/1000 | Loss: 0.00003303
Iteration 42/1000 | Loss: 0.00003207
Iteration 43/1000 | Loss: 0.00003231
Iteration 44/1000 | Loss: 0.00003370
Iteration 45/1000 | Loss: 0.00003010
Iteration 46/1000 | Loss: 0.00003155
Iteration 47/1000 | Loss: 0.00006126
Iteration 48/1000 | Loss: 0.00003286
Iteration 49/1000 | Loss: 0.00005698
Iteration 50/1000 | Loss: 0.00003571
Iteration 51/1000 | Loss: 0.00003875
Iteration 52/1000 | Loss: 0.00003214
Iteration 53/1000 | Loss: 0.00009704
Iteration 54/1000 | Loss: 0.00003594
Iteration 55/1000 | Loss: 0.00003735
Iteration 56/1000 | Loss: 0.00003657
Iteration 57/1000 | Loss: 0.00003168
Iteration 58/1000 | Loss: 0.00004518
Iteration 59/1000 | Loss: 0.00003140
Iteration 60/1000 | Loss: 0.00003139
Iteration 61/1000 | Loss: 0.00009172
Iteration 62/1000 | Loss: 0.00005515
Iteration 63/1000 | Loss: 0.00004009
Iteration 64/1000 | Loss: 0.00003692
Iteration 65/1000 | Loss: 0.00003698
Iteration 66/1000 | Loss: 0.00003167
Iteration 67/1000 | Loss: 0.00002874
Iteration 68/1000 | Loss: 0.00002874
Iteration 69/1000 | Loss: 0.00003511
Iteration 70/1000 | Loss: 0.00003832
Iteration 71/1000 | Loss: 0.00002445
Iteration 72/1000 | Loss: 0.00002372
Iteration 73/1000 | Loss: 0.00004922
Iteration 74/1000 | Loss: 0.00002646
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00006732
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00002858
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002852
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00002202
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00003643
Iteration 87/1000 | Loss: 0.00001734
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001732
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001732
Iteration 97/1000 | Loss: 0.00001731
Iteration 98/1000 | Loss: 0.00001731
Iteration 99/1000 | Loss: 0.00001731
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001731
Iteration 105/1000 | Loss: 0.00001730
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001730
Iteration 108/1000 | Loss: 0.00001729
Iteration 109/1000 | Loss: 0.00001729
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001727
Iteration 118/1000 | Loss: 0.00001727
Iteration 119/1000 | Loss: 0.00001726
Iteration 120/1000 | Loss: 0.00001726
Iteration 121/1000 | Loss: 0.00001726
Iteration 122/1000 | Loss: 0.00001726
Iteration 123/1000 | Loss: 0.00001726
Iteration 124/1000 | Loss: 0.00001726
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00002615
Iteration 129/1000 | Loss: 0.00003285
Iteration 130/1000 | Loss: 0.00001919
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001724
Iteration 133/1000 | Loss: 0.00001724
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00002007
Iteration 141/1000 | Loss: 0.00003480
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001722
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001721
Iteration 147/1000 | Loss: 0.00001721
Iteration 148/1000 | Loss: 0.00001721
Iteration 149/1000 | Loss: 0.00001721
Iteration 150/1000 | Loss: 0.00001721
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001721
Iteration 153/1000 | Loss: 0.00001721
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001720
Iteration 158/1000 | Loss: 0.00001720
Iteration 159/1000 | Loss: 0.00001720
Iteration 160/1000 | Loss: 0.00001720
Iteration 161/1000 | Loss: 0.00001720
Iteration 162/1000 | Loss: 0.00001720
Iteration 163/1000 | Loss: 0.00001720
Iteration 164/1000 | Loss: 0.00001720
Iteration 165/1000 | Loss: 0.00001720
Iteration 166/1000 | Loss: 0.00001720
Iteration 167/1000 | Loss: 0.00001720
Iteration 168/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.720320688036736e-05, 1.720320688036736e-05, 1.720320688036736e-05, 1.720320688036736e-05, 1.720320688036736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720320688036736e-05

Optimization complete. Final v2v error: 3.643089532852173 mm

Highest mean error: 4.417936325073242 mm for frame 237

Lowest mean error: 3.40665864944458 mm for frame 66

Saving results

Total time: 190.88608765602112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046659
Iteration 2/25 | Loss: 0.00229227
Iteration 3/25 | Loss: 0.00166025
Iteration 4/25 | Loss: 0.00163654
Iteration 5/25 | Loss: 0.00163714
Iteration 6/25 | Loss: 0.00152816
Iteration 7/25 | Loss: 0.00139907
Iteration 8/25 | Loss: 0.00132024
Iteration 9/25 | Loss: 0.00130125
Iteration 10/25 | Loss: 0.00130500
Iteration 11/25 | Loss: 0.00129797
Iteration 12/25 | Loss: 0.00129701
Iteration 13/25 | Loss: 0.00129665
Iteration 14/25 | Loss: 0.00129646
Iteration 15/25 | Loss: 0.00130176
Iteration 16/25 | Loss: 0.00130203
Iteration 17/25 | Loss: 0.00130157
Iteration 18/25 | Loss: 0.00130111
Iteration 19/25 | Loss: 0.00129671
Iteration 20/25 | Loss: 0.00130208
Iteration 21/25 | Loss: 0.00129887
Iteration 22/25 | Loss: 0.00129697
Iteration 23/25 | Loss: 0.00129647
Iteration 24/25 | Loss: 0.00129839
Iteration 25/25 | Loss: 0.00129468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30270529
Iteration 2/25 | Loss: 0.00082451
Iteration 3/25 | Loss: 0.00082448
Iteration 4/25 | Loss: 0.00082448
Iteration 5/25 | Loss: 0.00082448
Iteration 6/25 | Loss: 0.00082448
Iteration 7/25 | Loss: 0.00082448
Iteration 8/25 | Loss: 0.00082448
Iteration 9/25 | Loss: 0.00082448
Iteration 10/25 | Loss: 0.00082448
Iteration 11/25 | Loss: 0.00082448
Iteration 12/25 | Loss: 0.00082448
Iteration 13/25 | Loss: 0.00082448
Iteration 14/25 | Loss: 0.00082448
Iteration 15/25 | Loss: 0.00082448
Iteration 16/25 | Loss: 0.00082448
Iteration 17/25 | Loss: 0.00082448
Iteration 18/25 | Loss: 0.00082448
Iteration 19/25 | Loss: 0.00082448
Iteration 20/25 | Loss: 0.00082448
Iteration 21/25 | Loss: 0.00082448
Iteration 22/25 | Loss: 0.00082448
Iteration 23/25 | Loss: 0.00082448
Iteration 24/25 | Loss: 0.00082448
Iteration 25/25 | Loss: 0.00082448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082448
Iteration 2/1000 | Loss: 0.00898779
Iteration 3/1000 | Loss: 0.00470602
Iteration 4/1000 | Loss: 0.00431431
Iteration 5/1000 | Loss: 0.00316255
Iteration 6/1000 | Loss: 0.00025324
Iteration 7/1000 | Loss: 0.00014840
Iteration 8/1000 | Loss: 0.00009030
Iteration 9/1000 | Loss: 0.00198056
Iteration 10/1000 | Loss: 0.00117452
Iteration 11/1000 | Loss: 0.00025180
Iteration 12/1000 | Loss: 0.00030371
Iteration 13/1000 | Loss: 0.00011669
Iteration 14/1000 | Loss: 0.00007805
Iteration 15/1000 | Loss: 0.00006584
Iteration 16/1000 | Loss: 0.00005957
Iteration 17/1000 | Loss: 0.00005744
Iteration 18/1000 | Loss: 0.00044426
Iteration 19/1000 | Loss: 0.00007129
Iteration 20/1000 | Loss: 0.00006542
Iteration 21/1000 | Loss: 0.00006045
Iteration 22/1000 | Loss: 0.00029446
Iteration 23/1000 | Loss: 0.00007252
Iteration 24/1000 | Loss: 0.00006448
Iteration 25/1000 | Loss: 0.00005798
Iteration 26/1000 | Loss: 0.00005446
Iteration 27/1000 | Loss: 0.00005292
Iteration 28/1000 | Loss: 0.00005162
Iteration 29/1000 | Loss: 0.00005073
Iteration 30/1000 | Loss: 0.00005004
Iteration 31/1000 | Loss: 0.00004973
Iteration 32/1000 | Loss: 0.00004948
Iteration 33/1000 | Loss: 0.00004924
Iteration 34/1000 | Loss: 0.00004907
Iteration 35/1000 | Loss: 0.00004904
Iteration 36/1000 | Loss: 0.00004903
Iteration 37/1000 | Loss: 0.00004901
Iteration 38/1000 | Loss: 0.00004901
Iteration 39/1000 | Loss: 0.00004896
Iteration 40/1000 | Loss: 0.00004892
Iteration 41/1000 | Loss: 0.00004892
Iteration 42/1000 | Loss: 0.00004892
Iteration 43/1000 | Loss: 0.00004892
Iteration 44/1000 | Loss: 0.00004892
Iteration 45/1000 | Loss: 0.00004892
Iteration 46/1000 | Loss: 0.00004892
Iteration 47/1000 | Loss: 0.00004892
Iteration 48/1000 | Loss: 0.00004892
Iteration 49/1000 | Loss: 0.00004892
Iteration 50/1000 | Loss: 0.00004892
Iteration 51/1000 | Loss: 0.00004892
Iteration 52/1000 | Loss: 0.00004891
Iteration 53/1000 | Loss: 0.00004891
Iteration 54/1000 | Loss: 0.00004891
Iteration 55/1000 | Loss: 0.00004890
Iteration 56/1000 | Loss: 0.00004890
Iteration 57/1000 | Loss: 0.00004890
Iteration 58/1000 | Loss: 0.00004890
Iteration 59/1000 | Loss: 0.00004890
Iteration 60/1000 | Loss: 0.00004890
Iteration 61/1000 | Loss: 0.00004890
Iteration 62/1000 | Loss: 0.00004890
Iteration 63/1000 | Loss: 0.00004890
Iteration 64/1000 | Loss: 0.00004890
Iteration 65/1000 | Loss: 0.00004890
Iteration 66/1000 | Loss: 0.00004890
Iteration 67/1000 | Loss: 0.00004890
Iteration 68/1000 | Loss: 0.00004890
Iteration 69/1000 | Loss: 0.00004889
Iteration 70/1000 | Loss: 0.00004889
Iteration 71/1000 | Loss: 0.00004889
Iteration 72/1000 | Loss: 0.00004889
Iteration 73/1000 | Loss: 0.00004889
Iteration 74/1000 | Loss: 0.00004889
Iteration 75/1000 | Loss: 0.00004889
Iteration 76/1000 | Loss: 0.00004889
Iteration 77/1000 | Loss: 0.00004888
Iteration 78/1000 | Loss: 0.00004888
Iteration 79/1000 | Loss: 0.00004888
Iteration 80/1000 | Loss: 0.00004888
Iteration 81/1000 | Loss: 0.00004888
Iteration 82/1000 | Loss: 0.00004888
Iteration 83/1000 | Loss: 0.00004888
Iteration 84/1000 | Loss: 0.00004888
Iteration 85/1000 | Loss: 0.00004888
Iteration 86/1000 | Loss: 0.00004888
Iteration 87/1000 | Loss: 0.00004888
Iteration 88/1000 | Loss: 0.00004888
Iteration 89/1000 | Loss: 0.00004887
Iteration 90/1000 | Loss: 0.00004887
Iteration 91/1000 | Loss: 0.00004886
Iteration 92/1000 | Loss: 0.00004886
Iteration 93/1000 | Loss: 0.00004886
Iteration 94/1000 | Loss: 0.00004886
Iteration 95/1000 | Loss: 0.00004886
Iteration 96/1000 | Loss: 0.00004886
Iteration 97/1000 | Loss: 0.00004886
Iteration 98/1000 | Loss: 0.00004886
Iteration 99/1000 | Loss: 0.00004886
Iteration 100/1000 | Loss: 0.00004886
Iteration 101/1000 | Loss: 0.00004886
Iteration 102/1000 | Loss: 0.00004886
Iteration 103/1000 | Loss: 0.00004885
Iteration 104/1000 | Loss: 0.00004885
Iteration 105/1000 | Loss: 0.00004885
Iteration 106/1000 | Loss: 0.00004885
Iteration 107/1000 | Loss: 0.00004885
Iteration 108/1000 | Loss: 0.00004885
Iteration 109/1000 | Loss: 0.00004885
Iteration 110/1000 | Loss: 0.00004885
Iteration 111/1000 | Loss: 0.00004885
Iteration 112/1000 | Loss: 0.00004885
Iteration 113/1000 | Loss: 0.00004884
Iteration 114/1000 | Loss: 0.00004884
Iteration 115/1000 | Loss: 0.00004884
Iteration 116/1000 | Loss: 0.00004884
Iteration 117/1000 | Loss: 0.00004884
Iteration 118/1000 | Loss: 0.00004884
Iteration 119/1000 | Loss: 0.00004884
Iteration 120/1000 | Loss: 0.00004884
Iteration 121/1000 | Loss: 0.00004884
Iteration 122/1000 | Loss: 0.00004883
Iteration 123/1000 | Loss: 0.00004883
Iteration 124/1000 | Loss: 0.00004883
Iteration 125/1000 | Loss: 0.00004883
Iteration 126/1000 | Loss: 0.00004883
Iteration 127/1000 | Loss: 0.00004883
Iteration 128/1000 | Loss: 0.00004883
Iteration 129/1000 | Loss: 0.00004883
Iteration 130/1000 | Loss: 0.00004883
Iteration 131/1000 | Loss: 0.00004883
Iteration 132/1000 | Loss: 0.00004883
Iteration 133/1000 | Loss: 0.00004883
Iteration 134/1000 | Loss: 0.00004883
Iteration 135/1000 | Loss: 0.00004883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [4.882907524006441e-05, 4.882907524006441e-05, 4.882907524006441e-05, 4.882907524006441e-05, 4.882907524006441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.882907524006441e-05

Optimization complete. Final v2v error: 5.39564323425293 mm

Highest mean error: 7.07561731338501 mm for frame 65

Lowest mean error: 4.72824239730835 mm for frame 125

Saving results

Total time: 99.685476064682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445824
Iteration 2/25 | Loss: 0.00124983
Iteration 3/25 | Loss: 0.00107128
Iteration 4/25 | Loss: 0.00105061
Iteration 5/25 | Loss: 0.00104288
Iteration 6/25 | Loss: 0.00104123
Iteration 7/25 | Loss: 0.00104113
Iteration 8/25 | Loss: 0.00104113
Iteration 9/25 | Loss: 0.00104113
Iteration 10/25 | Loss: 0.00104113
Iteration 11/25 | Loss: 0.00104113
Iteration 12/25 | Loss: 0.00104113
Iteration 13/25 | Loss: 0.00104113
Iteration 14/25 | Loss: 0.00104113
Iteration 15/25 | Loss: 0.00104113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010411273688077927, 0.0010411273688077927, 0.0010411273688077927, 0.0010411273688077927, 0.0010411273688077927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010411273688077927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.03660822
Iteration 2/25 | Loss: 0.00054520
Iteration 3/25 | Loss: 0.00054518
Iteration 4/25 | Loss: 0.00054518
Iteration 5/25 | Loss: 0.00054518
Iteration 6/25 | Loss: 0.00054518
Iteration 7/25 | Loss: 0.00054518
Iteration 8/25 | Loss: 0.00054518
Iteration 9/25 | Loss: 0.00054518
Iteration 10/25 | Loss: 0.00054518
Iteration 11/25 | Loss: 0.00054518
Iteration 12/25 | Loss: 0.00054517
Iteration 13/25 | Loss: 0.00054517
Iteration 14/25 | Loss: 0.00054517
Iteration 15/25 | Loss: 0.00054517
Iteration 16/25 | Loss: 0.00054517
Iteration 17/25 | Loss: 0.00054517
Iteration 18/25 | Loss: 0.00054517
Iteration 19/25 | Loss: 0.00054517
Iteration 20/25 | Loss: 0.00054517
Iteration 21/25 | Loss: 0.00054517
Iteration 22/25 | Loss: 0.00054517
Iteration 23/25 | Loss: 0.00054517
Iteration 24/25 | Loss: 0.00054517
Iteration 25/25 | Loss: 0.00054517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054517
Iteration 2/1000 | Loss: 0.00003844
Iteration 3/1000 | Loss: 0.00003152
Iteration 4/1000 | Loss: 0.00002914
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00002656
Iteration 7/1000 | Loss: 0.00002608
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002544
Iteration 14/1000 | Loss: 0.00002543
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002537
Iteration 17/1000 | Loss: 0.00002536
Iteration 18/1000 | Loss: 0.00002536
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002534
Iteration 21/1000 | Loss: 0.00002534
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00002534
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002533
Iteration 26/1000 | Loss: 0.00002533
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002532
Iteration 29/1000 | Loss: 0.00002531
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002531
Iteration 32/1000 | Loss: 0.00002531
Iteration 33/1000 | Loss: 0.00002530
Iteration 34/1000 | Loss: 0.00002530
Iteration 35/1000 | Loss: 0.00002529
Iteration 36/1000 | Loss: 0.00002529
Iteration 37/1000 | Loss: 0.00002529
Iteration 38/1000 | Loss: 0.00002528
Iteration 39/1000 | Loss: 0.00002528
Iteration 40/1000 | Loss: 0.00002528
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002528
Iteration 43/1000 | Loss: 0.00002527
Iteration 44/1000 | Loss: 0.00002526
Iteration 45/1000 | Loss: 0.00002526
Iteration 46/1000 | Loss: 0.00002526
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002525
Iteration 49/1000 | Loss: 0.00002525
Iteration 50/1000 | Loss: 0.00002525
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002524
Iteration 54/1000 | Loss: 0.00002523
Iteration 55/1000 | Loss: 0.00002523
Iteration 56/1000 | Loss: 0.00002523
Iteration 57/1000 | Loss: 0.00002523
Iteration 58/1000 | Loss: 0.00002523
Iteration 59/1000 | Loss: 0.00002523
Iteration 60/1000 | Loss: 0.00002522
Iteration 61/1000 | Loss: 0.00002522
Iteration 62/1000 | Loss: 0.00002522
Iteration 63/1000 | Loss: 0.00002522
Iteration 64/1000 | Loss: 0.00002522
Iteration 65/1000 | Loss: 0.00002522
Iteration 66/1000 | Loss: 0.00002522
Iteration 67/1000 | Loss: 0.00002522
Iteration 68/1000 | Loss: 0.00002522
Iteration 69/1000 | Loss: 0.00002522
Iteration 70/1000 | Loss: 0.00002522
Iteration 71/1000 | Loss: 0.00002522
Iteration 72/1000 | Loss: 0.00002522
Iteration 73/1000 | Loss: 0.00002522
Iteration 74/1000 | Loss: 0.00002522
Iteration 75/1000 | Loss: 0.00002522
Iteration 76/1000 | Loss: 0.00002522
Iteration 77/1000 | Loss: 0.00002522
Iteration 78/1000 | Loss: 0.00002522
Iteration 79/1000 | Loss: 0.00002522
Iteration 80/1000 | Loss: 0.00002522
Iteration 81/1000 | Loss: 0.00002522
Iteration 82/1000 | Loss: 0.00002522
Iteration 83/1000 | Loss: 0.00002522
Iteration 84/1000 | Loss: 0.00002522
Iteration 85/1000 | Loss: 0.00002522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.5223331249435432e-05, 2.5223331249435432e-05, 2.5223331249435432e-05, 2.5223331249435432e-05, 2.5223331249435432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5223331249435432e-05

Optimization complete. Final v2v error: 4.319349765777588 mm

Highest mean error: 4.622349739074707 mm for frame 104

Lowest mean error: 4.08300256729126 mm for frame 15

Saving results

Total time: 29.750385522842407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161737
Iteration 2/25 | Loss: 0.00192801
Iteration 3/25 | Loss: 0.00158900
Iteration 4/25 | Loss: 0.00151898
Iteration 5/25 | Loss: 0.00150254
Iteration 6/25 | Loss: 0.00144847
Iteration 7/25 | Loss: 0.00140079
Iteration 8/25 | Loss: 0.00138599
Iteration 9/25 | Loss: 0.00136897
Iteration 10/25 | Loss: 0.00136099
Iteration 11/25 | Loss: 0.00136519
Iteration 12/25 | Loss: 0.00135666
Iteration 13/25 | Loss: 0.00135251
Iteration 14/25 | Loss: 0.00135182
Iteration 15/25 | Loss: 0.00135137
Iteration 16/25 | Loss: 0.00135119
Iteration 17/25 | Loss: 0.00135114
Iteration 18/25 | Loss: 0.00135113
Iteration 19/25 | Loss: 0.00135112
Iteration 20/25 | Loss: 0.00135112
Iteration 21/25 | Loss: 0.00135112
Iteration 22/25 | Loss: 0.00135112
Iteration 23/25 | Loss: 0.00135112
Iteration 24/25 | Loss: 0.00135112
Iteration 25/25 | Loss: 0.00135112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29063880
Iteration 2/25 | Loss: 0.00295905
Iteration 3/25 | Loss: 0.00286975
Iteration 4/25 | Loss: 0.00286975
Iteration 5/25 | Loss: 0.00286975
Iteration 6/25 | Loss: 0.00286975
Iteration 7/25 | Loss: 0.00286975
Iteration 8/25 | Loss: 0.00286975
Iteration 9/25 | Loss: 0.00286975
Iteration 10/25 | Loss: 0.00286975
Iteration 11/25 | Loss: 0.00286975
Iteration 12/25 | Loss: 0.00286975
Iteration 13/25 | Loss: 0.00286975
Iteration 14/25 | Loss: 0.00286975
Iteration 15/25 | Loss: 0.00286975
Iteration 16/25 | Loss: 0.00286975
Iteration 17/25 | Loss: 0.00286975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028697489760816097, 0.0028697489760816097, 0.0028697489760816097, 0.0028697489760816097, 0.0028697489760816097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028697489760816097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286975
Iteration 2/1000 | Loss: 0.00070744
Iteration 3/1000 | Loss: 0.00037803
Iteration 4/1000 | Loss: 0.00115640
Iteration 5/1000 | Loss: 0.00079009
Iteration 6/1000 | Loss: 0.00175961
Iteration 7/1000 | Loss: 0.00071192
Iteration 8/1000 | Loss: 0.00044920
Iteration 9/1000 | Loss: 0.00021260
Iteration 10/1000 | Loss: 0.00029465
Iteration 11/1000 | Loss: 0.00023142
Iteration 12/1000 | Loss: 0.00020089
Iteration 13/1000 | Loss: 0.00091388
Iteration 14/1000 | Loss: 0.00149575
Iteration 15/1000 | Loss: 0.01569427
Iteration 16/1000 | Loss: 0.01291224
Iteration 17/1000 | Loss: 0.00156254
Iteration 18/1000 | Loss: 0.00043334
Iteration 19/1000 | Loss: 0.00174910
Iteration 20/1000 | Loss: 0.00081791
Iteration 21/1000 | Loss: 0.00121419
Iteration 22/1000 | Loss: 0.01642997
Iteration 23/1000 | Loss: 0.00451027
Iteration 24/1000 | Loss: 0.00078027
Iteration 25/1000 | Loss: 0.01381697
Iteration 26/1000 | Loss: 0.00586881
Iteration 27/1000 | Loss: 0.00276040
Iteration 28/1000 | Loss: 0.01436797
Iteration 29/1000 | Loss: 0.01067829
Iteration 30/1000 | Loss: 0.00955565
Iteration 31/1000 | Loss: 0.00576593
Iteration 32/1000 | Loss: 0.00306304
Iteration 33/1000 | Loss: 0.00394859
Iteration 34/1000 | Loss: 0.00606431
Iteration 35/1000 | Loss: 0.00400391
Iteration 36/1000 | Loss: 0.00378350
Iteration 37/1000 | Loss: 0.00269134
Iteration 38/1000 | Loss: 0.00154128
Iteration 39/1000 | Loss: 0.00120624
Iteration 40/1000 | Loss: 0.00104733
Iteration 41/1000 | Loss: 0.00421421
Iteration 42/1000 | Loss: 0.00324571
Iteration 43/1000 | Loss: 0.00347961
Iteration 44/1000 | Loss: 0.00277929
Iteration 45/1000 | Loss: 0.00245624
Iteration 46/1000 | Loss: 0.00191131
Iteration 47/1000 | Loss: 0.00202440
Iteration 48/1000 | Loss: 0.00169495
Iteration 49/1000 | Loss: 0.00254105
Iteration 50/1000 | Loss: 0.00165172
Iteration 51/1000 | Loss: 0.00091418
Iteration 52/1000 | Loss: 0.00221680
Iteration 53/1000 | Loss: 0.00103925
Iteration 54/1000 | Loss: 0.00060608
Iteration 55/1000 | Loss: 0.00079865
Iteration 56/1000 | Loss: 0.00072943
Iteration 57/1000 | Loss: 0.00176773
Iteration 58/1000 | Loss: 0.00165633
Iteration 59/1000 | Loss: 0.00197145
Iteration 60/1000 | Loss: 0.00080574
Iteration 61/1000 | Loss: 0.00048492
Iteration 62/1000 | Loss: 0.00134321
Iteration 63/1000 | Loss: 0.00092943
Iteration 64/1000 | Loss: 0.00011263
Iteration 65/1000 | Loss: 0.00100693
Iteration 66/1000 | Loss: 0.00063599
Iteration 67/1000 | Loss: 0.00072572
Iteration 68/1000 | Loss: 0.00220244
Iteration 69/1000 | Loss: 0.00243591
Iteration 70/1000 | Loss: 0.00190717
Iteration 71/1000 | Loss: 0.00140534
Iteration 72/1000 | Loss: 0.00155958
Iteration 73/1000 | Loss: 0.00161539
Iteration 74/1000 | Loss: 0.00067333
Iteration 75/1000 | Loss: 0.00102310
Iteration 76/1000 | Loss: 0.00085626
Iteration 77/1000 | Loss: 0.00124457
Iteration 78/1000 | Loss: 0.00120015
Iteration 79/1000 | Loss: 0.00033539
Iteration 80/1000 | Loss: 0.00016185
Iteration 81/1000 | Loss: 0.00005200
Iteration 82/1000 | Loss: 0.00010320
Iteration 83/1000 | Loss: 0.00004509
Iteration 84/1000 | Loss: 0.00004078
Iteration 85/1000 | Loss: 0.00051408
Iteration 86/1000 | Loss: 0.00004196
Iteration 87/1000 | Loss: 0.00003792
Iteration 88/1000 | Loss: 0.00003685
Iteration 89/1000 | Loss: 0.00103369
Iteration 90/1000 | Loss: 0.00039209
Iteration 91/1000 | Loss: 0.00009801
Iteration 92/1000 | Loss: 0.00036038
Iteration 93/1000 | Loss: 0.00045045
Iteration 94/1000 | Loss: 0.00003994
Iteration 95/1000 | Loss: 0.00036453
Iteration 96/1000 | Loss: 0.00003289
Iteration 97/1000 | Loss: 0.00003029
Iteration 98/1000 | Loss: 0.00002933
Iteration 99/1000 | Loss: 0.00013095
Iteration 100/1000 | Loss: 0.00003202
Iteration 101/1000 | Loss: 0.00020775
Iteration 102/1000 | Loss: 0.00021984
Iteration 103/1000 | Loss: 0.00005251
Iteration 104/1000 | Loss: 0.00018246
Iteration 105/1000 | Loss: 0.00073361
Iteration 106/1000 | Loss: 0.00062235
Iteration 107/1000 | Loss: 0.00049594
Iteration 108/1000 | Loss: 0.00038242
Iteration 109/1000 | Loss: 0.00102199
Iteration 110/1000 | Loss: 0.00034153
Iteration 111/1000 | Loss: 0.00049129
Iteration 112/1000 | Loss: 0.00020571
Iteration 113/1000 | Loss: 0.00035809
Iteration 114/1000 | Loss: 0.00026755
Iteration 115/1000 | Loss: 0.00005218
Iteration 116/1000 | Loss: 0.00031573
Iteration 117/1000 | Loss: 0.00012205
Iteration 118/1000 | Loss: 0.00003319
Iteration 119/1000 | Loss: 0.00002868
Iteration 120/1000 | Loss: 0.00002661
Iteration 121/1000 | Loss: 0.00002491
Iteration 122/1000 | Loss: 0.00002373
Iteration 123/1000 | Loss: 0.00002301
Iteration 124/1000 | Loss: 0.00002240
Iteration 125/1000 | Loss: 0.00002205
Iteration 126/1000 | Loss: 0.00002193
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002180
Iteration 129/1000 | Loss: 0.00002179
Iteration 130/1000 | Loss: 0.00002178
Iteration 131/1000 | Loss: 0.00002175
Iteration 132/1000 | Loss: 0.00002174
Iteration 133/1000 | Loss: 0.00002173
Iteration 134/1000 | Loss: 0.00002173
Iteration 135/1000 | Loss: 0.00002172
Iteration 136/1000 | Loss: 0.00002172
Iteration 137/1000 | Loss: 0.00002172
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002168
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002157
Iteration 145/1000 | Loss: 0.00002157
Iteration 146/1000 | Loss: 0.00002157
Iteration 147/1000 | Loss: 0.00002157
Iteration 148/1000 | Loss: 0.00002157
Iteration 149/1000 | Loss: 0.00002157
Iteration 150/1000 | Loss: 0.00002157
Iteration 151/1000 | Loss: 0.00002156
Iteration 152/1000 | Loss: 0.00002156
Iteration 153/1000 | Loss: 0.00002156
Iteration 154/1000 | Loss: 0.00002151
Iteration 155/1000 | Loss: 0.00002151
Iteration 156/1000 | Loss: 0.00002151
Iteration 157/1000 | Loss: 0.00002151
Iteration 158/1000 | Loss: 0.00002151
Iteration 159/1000 | Loss: 0.00002149
Iteration 160/1000 | Loss: 0.00002147
Iteration 161/1000 | Loss: 0.00002147
Iteration 162/1000 | Loss: 0.00002147
Iteration 163/1000 | Loss: 0.00002146
Iteration 164/1000 | Loss: 0.00002146
Iteration 165/1000 | Loss: 0.00002145
Iteration 166/1000 | Loss: 0.00002145
Iteration 167/1000 | Loss: 0.00002145
Iteration 168/1000 | Loss: 0.00002145
Iteration 169/1000 | Loss: 0.00002145
Iteration 170/1000 | Loss: 0.00002144
Iteration 171/1000 | Loss: 0.00002144
Iteration 172/1000 | Loss: 0.00002144
Iteration 173/1000 | Loss: 0.00002144
Iteration 174/1000 | Loss: 0.00002144
Iteration 175/1000 | Loss: 0.00002144
Iteration 176/1000 | Loss: 0.00002144
Iteration 177/1000 | Loss: 0.00002144
Iteration 178/1000 | Loss: 0.00002144
Iteration 179/1000 | Loss: 0.00002144
Iteration 180/1000 | Loss: 0.00002144
Iteration 181/1000 | Loss: 0.00002144
Iteration 182/1000 | Loss: 0.00002144
Iteration 183/1000 | Loss: 0.00002144
Iteration 184/1000 | Loss: 0.00002144
Iteration 185/1000 | Loss: 0.00002144
Iteration 186/1000 | Loss: 0.00002144
Iteration 187/1000 | Loss: 0.00002144
Iteration 188/1000 | Loss: 0.00002144
Iteration 189/1000 | Loss: 0.00002144
Iteration 190/1000 | Loss: 0.00002144
Iteration 191/1000 | Loss: 0.00002144
Iteration 192/1000 | Loss: 0.00002144
Iteration 193/1000 | Loss: 0.00002144
Iteration 194/1000 | Loss: 0.00002144
Iteration 195/1000 | Loss: 0.00002144
Iteration 196/1000 | Loss: 0.00002144
Iteration 197/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.1438670955831185e-05, 2.1438670955831185e-05, 2.1438670955831185e-05, 2.1438670955831185e-05, 2.1438670955831185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1438670955831185e-05

Optimization complete. Final v2v error: 3.959273338317871 mm

Highest mean error: 4.526687145233154 mm for frame 23

Lowest mean error: 3.782731771469116 mm for frame 42

Saving results

Total time: 215.21012544631958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882796
Iteration 2/25 | Loss: 0.00147331
Iteration 3/25 | Loss: 0.00121871
Iteration 4/25 | Loss: 0.00117758
Iteration 5/25 | Loss: 0.00116359
Iteration 6/25 | Loss: 0.00116063
Iteration 7/25 | Loss: 0.00115973
Iteration 8/25 | Loss: 0.00115973
Iteration 9/25 | Loss: 0.00115973
Iteration 10/25 | Loss: 0.00115973
Iteration 11/25 | Loss: 0.00115973
Iteration 12/25 | Loss: 0.00115973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011597288539633155, 0.0011597288539633155, 0.0011597288539633155, 0.0011597288539633155, 0.0011597288539633155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011597288539633155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14971471
Iteration 2/25 | Loss: 0.00054769
Iteration 3/25 | Loss: 0.00054769
Iteration 4/25 | Loss: 0.00054768
Iteration 5/25 | Loss: 0.00054768
Iteration 6/25 | Loss: 0.00054768
Iteration 7/25 | Loss: 0.00054768
Iteration 8/25 | Loss: 0.00054768
Iteration 9/25 | Loss: 0.00054768
Iteration 10/25 | Loss: 0.00054768
Iteration 11/25 | Loss: 0.00054768
Iteration 12/25 | Loss: 0.00054768
Iteration 13/25 | Loss: 0.00054768
Iteration 14/25 | Loss: 0.00054768
Iteration 15/25 | Loss: 0.00054768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00054768385598436, 0.00054768385598436, 0.00054768385598436, 0.00054768385598436, 0.00054768385598436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00054768385598436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054768
Iteration 2/1000 | Loss: 0.00005247
Iteration 3/1000 | Loss: 0.00004241
Iteration 4/1000 | Loss: 0.00003781
Iteration 5/1000 | Loss: 0.00003599
Iteration 6/1000 | Loss: 0.00003468
Iteration 7/1000 | Loss: 0.00003371
Iteration 8/1000 | Loss: 0.00003302
Iteration 9/1000 | Loss: 0.00003241
Iteration 10/1000 | Loss: 0.00003212
Iteration 11/1000 | Loss: 0.00003181
Iteration 12/1000 | Loss: 0.00003149
Iteration 13/1000 | Loss: 0.00003127
Iteration 14/1000 | Loss: 0.00003120
Iteration 15/1000 | Loss: 0.00003116
Iteration 16/1000 | Loss: 0.00003115
Iteration 17/1000 | Loss: 0.00003114
Iteration 18/1000 | Loss: 0.00003113
Iteration 19/1000 | Loss: 0.00003112
Iteration 20/1000 | Loss: 0.00003111
Iteration 21/1000 | Loss: 0.00003111
Iteration 22/1000 | Loss: 0.00003109
Iteration 23/1000 | Loss: 0.00003108
Iteration 24/1000 | Loss: 0.00003108
Iteration 25/1000 | Loss: 0.00003104
Iteration 26/1000 | Loss: 0.00003104
Iteration 27/1000 | Loss: 0.00003102
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003101
Iteration 30/1000 | Loss: 0.00003101
Iteration 31/1000 | Loss: 0.00003101
Iteration 32/1000 | Loss: 0.00003100
Iteration 33/1000 | Loss: 0.00003100
Iteration 34/1000 | Loss: 0.00003100
Iteration 35/1000 | Loss: 0.00003099
Iteration 36/1000 | Loss: 0.00003099
Iteration 37/1000 | Loss: 0.00003099
Iteration 38/1000 | Loss: 0.00003099
Iteration 39/1000 | Loss: 0.00003098
Iteration 40/1000 | Loss: 0.00003098
Iteration 41/1000 | Loss: 0.00003098
Iteration 42/1000 | Loss: 0.00003098
Iteration 43/1000 | Loss: 0.00003097
Iteration 44/1000 | Loss: 0.00003097
Iteration 45/1000 | Loss: 0.00003097
Iteration 46/1000 | Loss: 0.00003097
Iteration 47/1000 | Loss: 0.00003097
Iteration 48/1000 | Loss: 0.00003097
Iteration 49/1000 | Loss: 0.00003097
Iteration 50/1000 | Loss: 0.00003097
Iteration 51/1000 | Loss: 0.00003097
Iteration 52/1000 | Loss: 0.00003097
Iteration 53/1000 | Loss: 0.00003097
Iteration 54/1000 | Loss: 0.00003097
Iteration 55/1000 | Loss: 0.00003097
Iteration 56/1000 | Loss: 0.00003097
Iteration 57/1000 | Loss: 0.00003097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.097248918493278e-05, 3.097248918493278e-05, 3.097248918493278e-05, 3.097248918493278e-05, 3.097248918493278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.097248918493278e-05

Optimization complete. Final v2v error: 4.729531288146973 mm

Highest mean error: 5.782973289489746 mm for frame 114

Lowest mean error: 3.9246268272399902 mm for frame 217

Saving results

Total time: 38.03908371925354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01172054
Iteration 2/25 | Loss: 0.00215877
Iteration 3/25 | Loss: 0.00132202
Iteration 4/25 | Loss: 0.00123607
Iteration 5/25 | Loss: 0.00120525
Iteration 6/25 | Loss: 0.00120443
Iteration 7/25 | Loss: 0.00119739
Iteration 8/25 | Loss: 0.00117959
Iteration 9/25 | Loss: 0.00117279
Iteration 10/25 | Loss: 0.00116545
Iteration 11/25 | Loss: 0.00116124
Iteration 12/25 | Loss: 0.00115312
Iteration 13/25 | Loss: 0.00115159
Iteration 14/25 | Loss: 0.00115354
Iteration 15/25 | Loss: 0.00114797
Iteration 16/25 | Loss: 0.00114830
Iteration 17/25 | Loss: 0.00114879
Iteration 18/25 | Loss: 0.00114803
Iteration 19/25 | Loss: 0.00114808
Iteration 20/25 | Loss: 0.00114804
Iteration 21/25 | Loss: 0.00114803
Iteration 22/25 | Loss: 0.00114828
Iteration 23/25 | Loss: 0.00114795
Iteration 24/25 | Loss: 0.00114763
Iteration 25/25 | Loss: 0.00114776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71538538
Iteration 2/25 | Loss: 0.00061337
Iteration 3/25 | Loss: 0.00061335
Iteration 4/25 | Loss: 0.00061335
Iteration 5/25 | Loss: 0.00061335
Iteration 6/25 | Loss: 0.00061335
Iteration 7/25 | Loss: 0.00061335
Iteration 8/25 | Loss: 0.00061335
Iteration 9/25 | Loss: 0.00061335
Iteration 10/25 | Loss: 0.00061335
Iteration 11/25 | Loss: 0.00061335
Iteration 12/25 | Loss: 0.00061335
Iteration 13/25 | Loss: 0.00061335
Iteration 14/25 | Loss: 0.00061335
Iteration 15/25 | Loss: 0.00061335
Iteration 16/25 | Loss: 0.00061335
Iteration 17/25 | Loss: 0.00061335
Iteration 18/25 | Loss: 0.00061335
Iteration 19/25 | Loss: 0.00061335
Iteration 20/25 | Loss: 0.00061335
Iteration 21/25 | Loss: 0.00061335
Iteration 22/25 | Loss: 0.00061335
Iteration 23/25 | Loss: 0.00061335
Iteration 24/25 | Loss: 0.00061335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006133493734523654, 0.0006133493734523654, 0.0006133493734523654, 0.0006133493734523654, 0.0006133493734523654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006133493734523654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061335
Iteration 2/1000 | Loss: 0.00006812
Iteration 3/1000 | Loss: 0.00007168
Iteration 4/1000 | Loss: 0.00006452
Iteration 5/1000 | Loss: 0.00005287
Iteration 6/1000 | Loss: 0.00005040
Iteration 7/1000 | Loss: 0.00007435
Iteration 8/1000 | Loss: 0.00005343
Iteration 9/1000 | Loss: 0.00004762
Iteration 10/1000 | Loss: 0.00005951
Iteration 11/1000 | Loss: 0.00007110
Iteration 12/1000 | Loss: 0.00005152
Iteration 13/1000 | Loss: 0.00005601
Iteration 14/1000 | Loss: 0.00005525
Iteration 15/1000 | Loss: 0.00004581
Iteration 16/1000 | Loss: 0.00004160
Iteration 17/1000 | Loss: 0.00004009
Iteration 18/1000 | Loss: 0.00003882
Iteration 19/1000 | Loss: 0.00003807
Iteration 20/1000 | Loss: 0.00003753
Iteration 21/1000 | Loss: 0.00003714
Iteration 22/1000 | Loss: 0.00003684
Iteration 23/1000 | Loss: 0.00003656
Iteration 24/1000 | Loss: 0.00003641
Iteration 25/1000 | Loss: 0.00003626
Iteration 26/1000 | Loss: 0.00003626
Iteration 27/1000 | Loss: 0.00003626
Iteration 28/1000 | Loss: 0.00003625
Iteration 29/1000 | Loss: 0.00003623
Iteration 30/1000 | Loss: 0.00003623
Iteration 31/1000 | Loss: 0.00003618
Iteration 32/1000 | Loss: 0.00003618
Iteration 33/1000 | Loss: 0.00003616
Iteration 34/1000 | Loss: 0.00003615
Iteration 35/1000 | Loss: 0.00003615
Iteration 36/1000 | Loss: 0.00003615
Iteration 37/1000 | Loss: 0.00003615
Iteration 38/1000 | Loss: 0.00003615
Iteration 39/1000 | Loss: 0.00003615
Iteration 40/1000 | Loss: 0.00003615
Iteration 41/1000 | Loss: 0.00003614
Iteration 42/1000 | Loss: 0.00003614
Iteration 43/1000 | Loss: 0.00003614
Iteration 44/1000 | Loss: 0.00003614
Iteration 45/1000 | Loss: 0.00003613
Iteration 46/1000 | Loss: 0.00003613
Iteration 47/1000 | Loss: 0.00003613
Iteration 48/1000 | Loss: 0.00003613
Iteration 49/1000 | Loss: 0.00003613
Iteration 50/1000 | Loss: 0.00003612
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003612
Iteration 53/1000 | Loss: 0.00003612
Iteration 54/1000 | Loss: 0.00003611
Iteration 55/1000 | Loss: 0.00003611
Iteration 56/1000 | Loss: 0.00003611
Iteration 57/1000 | Loss: 0.00003611
Iteration 58/1000 | Loss: 0.00003611
Iteration 59/1000 | Loss: 0.00003611
Iteration 60/1000 | Loss: 0.00003611
Iteration 61/1000 | Loss: 0.00003610
Iteration 62/1000 | Loss: 0.00003610
Iteration 63/1000 | Loss: 0.00003610
Iteration 64/1000 | Loss: 0.00003610
Iteration 65/1000 | Loss: 0.00003610
Iteration 66/1000 | Loss: 0.00003610
Iteration 67/1000 | Loss: 0.00003610
Iteration 68/1000 | Loss: 0.00003609
Iteration 69/1000 | Loss: 0.00003609
Iteration 70/1000 | Loss: 0.00003609
Iteration 71/1000 | Loss: 0.00003609
Iteration 72/1000 | Loss: 0.00003609
Iteration 73/1000 | Loss: 0.00003609
Iteration 74/1000 | Loss: 0.00003609
Iteration 75/1000 | Loss: 0.00003609
Iteration 76/1000 | Loss: 0.00003609
Iteration 77/1000 | Loss: 0.00003608
Iteration 78/1000 | Loss: 0.00003608
Iteration 79/1000 | Loss: 0.00003608
Iteration 80/1000 | Loss: 0.00003608
Iteration 81/1000 | Loss: 0.00003608
Iteration 82/1000 | Loss: 0.00003608
Iteration 83/1000 | Loss: 0.00003608
Iteration 84/1000 | Loss: 0.00003608
Iteration 85/1000 | Loss: 0.00003608
Iteration 86/1000 | Loss: 0.00003607
Iteration 87/1000 | Loss: 0.00003607
Iteration 88/1000 | Loss: 0.00003607
Iteration 89/1000 | Loss: 0.00003607
Iteration 90/1000 | Loss: 0.00003607
Iteration 91/1000 | Loss: 0.00003607
Iteration 92/1000 | Loss: 0.00003607
Iteration 93/1000 | Loss: 0.00003607
Iteration 94/1000 | Loss: 0.00003607
Iteration 95/1000 | Loss: 0.00003607
Iteration 96/1000 | Loss: 0.00003607
Iteration 97/1000 | Loss: 0.00003606
Iteration 98/1000 | Loss: 0.00003606
Iteration 99/1000 | Loss: 0.00003606
Iteration 100/1000 | Loss: 0.00003606
Iteration 101/1000 | Loss: 0.00003606
Iteration 102/1000 | Loss: 0.00003606
Iteration 103/1000 | Loss: 0.00003606
Iteration 104/1000 | Loss: 0.00003606
Iteration 105/1000 | Loss: 0.00003606
Iteration 106/1000 | Loss: 0.00003606
Iteration 107/1000 | Loss: 0.00003606
Iteration 108/1000 | Loss: 0.00003606
Iteration 109/1000 | Loss: 0.00003606
Iteration 110/1000 | Loss: 0.00003606
Iteration 111/1000 | Loss: 0.00003606
Iteration 112/1000 | Loss: 0.00003606
Iteration 113/1000 | Loss: 0.00003606
Iteration 114/1000 | Loss: 0.00003606
Iteration 115/1000 | Loss: 0.00003606
Iteration 116/1000 | Loss: 0.00003606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [3.606090467656031e-05, 3.606090467656031e-05, 3.606090467656031e-05, 3.606090467656031e-05, 3.606090467656031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.606090467656031e-05

Optimization complete. Final v2v error: 5.108222007751465 mm

Highest mean error: 5.803962707519531 mm for frame 174

Lowest mean error: 4.325675010681152 mm for frame 87

Saving results

Total time: 101.54980731010437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846866
Iteration 2/25 | Loss: 0.00176331
Iteration 3/25 | Loss: 0.00126674
Iteration 4/25 | Loss: 0.00112973
Iteration 5/25 | Loss: 0.00108395
Iteration 6/25 | Loss: 0.00108177
Iteration 7/25 | Loss: 0.00106884
Iteration 8/25 | Loss: 0.00105269
Iteration 9/25 | Loss: 0.00104511
Iteration 10/25 | Loss: 0.00104239
Iteration 11/25 | Loss: 0.00104103
Iteration 12/25 | Loss: 0.00104050
Iteration 13/25 | Loss: 0.00104030
Iteration 14/25 | Loss: 0.00104027
Iteration 15/25 | Loss: 0.00104027
Iteration 16/25 | Loss: 0.00104026
Iteration 17/25 | Loss: 0.00104026
Iteration 18/25 | Loss: 0.00104026
Iteration 19/25 | Loss: 0.00104025
Iteration 20/25 | Loss: 0.00104025
Iteration 21/25 | Loss: 0.00104025
Iteration 22/25 | Loss: 0.00104025
Iteration 23/25 | Loss: 0.00104025
Iteration 24/25 | Loss: 0.00104025
Iteration 25/25 | Loss: 0.00104025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89542580
Iteration 2/25 | Loss: 0.00058879
Iteration 3/25 | Loss: 0.00052374
Iteration 4/25 | Loss: 0.00052374
Iteration 5/25 | Loss: 0.00052374
Iteration 6/25 | Loss: 0.00052374
Iteration 7/25 | Loss: 0.00052374
Iteration 8/25 | Loss: 0.00052374
Iteration 9/25 | Loss: 0.00052374
Iteration 10/25 | Loss: 0.00052374
Iteration 11/25 | Loss: 0.00052374
Iteration 12/25 | Loss: 0.00052374
Iteration 13/25 | Loss: 0.00052374
Iteration 14/25 | Loss: 0.00052374
Iteration 15/25 | Loss: 0.00052374
Iteration 16/25 | Loss: 0.00052374
Iteration 17/25 | Loss: 0.00052374
Iteration 18/25 | Loss: 0.00052374
Iteration 19/25 | Loss: 0.00052374
Iteration 20/25 | Loss: 0.00052374
Iteration 21/25 | Loss: 0.00052374
Iteration 22/25 | Loss: 0.00052374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000523736234754324, 0.000523736234754324, 0.000523736234754324, 0.000523736234754324, 0.000523736234754324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000523736234754324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052374
Iteration 2/1000 | Loss: 0.00007881
Iteration 3/1000 | Loss: 0.00002642
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002315
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002207
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002170
Iteration 10/1000 | Loss: 0.00002168
Iteration 11/1000 | Loss: 0.00002162
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002152
Iteration 14/1000 | Loss: 0.00002146
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002144
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002142
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002141
Iteration 25/1000 | Loss: 0.00002141
Iteration 26/1000 | Loss: 0.00002140
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002139
Iteration 30/1000 | Loss: 0.00002139
Iteration 31/1000 | Loss: 0.00002138
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002137
Iteration 34/1000 | Loss: 0.00002137
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002135
Iteration 43/1000 | Loss: 0.00002135
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002134
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002134
Iteration 49/1000 | Loss: 0.00002134
Iteration 50/1000 | Loss: 0.00002134
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002133
Iteration 53/1000 | Loss: 0.00002133
Iteration 54/1000 | Loss: 0.00002133
Iteration 55/1000 | Loss: 0.00002133
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.1330813979147933e-05, 2.1330813979147933e-05, 2.1330813979147933e-05, 2.1330813979147933e-05, 2.1330813979147933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1330813979147933e-05

Optimization complete. Final v2v error: 3.993435859680176 mm

Highest mean error: 4.242698669433594 mm for frame 77

Lowest mean error: 3.65219783782959 mm for frame 116

Saving results

Total time: 42.71120548248291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828537
Iteration 2/25 | Loss: 0.00129744
Iteration 3/25 | Loss: 0.00114964
Iteration 4/25 | Loss: 0.00112087
Iteration 5/25 | Loss: 0.00111169
Iteration 6/25 | Loss: 0.00110954
Iteration 7/25 | Loss: 0.00110943
Iteration 8/25 | Loss: 0.00110943
Iteration 9/25 | Loss: 0.00110943
Iteration 10/25 | Loss: 0.00110943
Iteration 11/25 | Loss: 0.00110943
Iteration 12/25 | Loss: 0.00110943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011094269575551152, 0.0011094269575551152, 0.0011094269575551152, 0.0011094269575551152, 0.0011094269575551152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011094269575551152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29974115
Iteration 2/25 | Loss: 0.00069054
Iteration 3/25 | Loss: 0.00069052
Iteration 4/25 | Loss: 0.00069052
Iteration 5/25 | Loss: 0.00069052
Iteration 6/25 | Loss: 0.00069052
Iteration 7/25 | Loss: 0.00069052
Iteration 8/25 | Loss: 0.00069052
Iteration 9/25 | Loss: 0.00069052
Iteration 10/25 | Loss: 0.00069052
Iteration 11/25 | Loss: 0.00069052
Iteration 12/25 | Loss: 0.00069052
Iteration 13/25 | Loss: 0.00069052
Iteration 14/25 | Loss: 0.00069052
Iteration 15/25 | Loss: 0.00069052
Iteration 16/25 | Loss: 0.00069052
Iteration 17/25 | Loss: 0.00069052
Iteration 18/25 | Loss: 0.00069052
Iteration 19/25 | Loss: 0.00069052
Iteration 20/25 | Loss: 0.00069052
Iteration 21/25 | Loss: 0.00069052
Iteration 22/25 | Loss: 0.00069052
Iteration 23/25 | Loss: 0.00069052
Iteration 24/25 | Loss: 0.00069052
Iteration 25/25 | Loss: 0.00069052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069052
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002956
Iteration 4/1000 | Loss: 0.00002716
Iteration 5/1000 | Loss: 0.00002584
Iteration 6/1000 | Loss: 0.00002493
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00002407
Iteration 9/1000 | Loss: 0.00002377
Iteration 10/1000 | Loss: 0.00002357
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002339
Iteration 13/1000 | Loss: 0.00002335
Iteration 14/1000 | Loss: 0.00002333
Iteration 15/1000 | Loss: 0.00002330
Iteration 16/1000 | Loss: 0.00002329
Iteration 17/1000 | Loss: 0.00002328
Iteration 18/1000 | Loss: 0.00002328
Iteration 19/1000 | Loss: 0.00002327
Iteration 20/1000 | Loss: 0.00002327
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002327
Iteration 23/1000 | Loss: 0.00002326
Iteration 24/1000 | Loss: 0.00002325
Iteration 25/1000 | Loss: 0.00002324
Iteration 26/1000 | Loss: 0.00002324
Iteration 27/1000 | Loss: 0.00002323
Iteration 28/1000 | Loss: 0.00002323
Iteration 29/1000 | Loss: 0.00002323
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002323
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00002323
Iteration 34/1000 | Loss: 0.00002322
Iteration 35/1000 | Loss: 0.00002322
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002321
Iteration 38/1000 | Loss: 0.00002321
Iteration 39/1000 | Loss: 0.00002321
Iteration 40/1000 | Loss: 0.00002320
Iteration 41/1000 | Loss: 0.00002320
Iteration 42/1000 | Loss: 0.00002320
Iteration 43/1000 | Loss: 0.00002320
Iteration 44/1000 | Loss: 0.00002320
Iteration 45/1000 | Loss: 0.00002319
Iteration 46/1000 | Loss: 0.00002319
Iteration 47/1000 | Loss: 0.00002319
Iteration 48/1000 | Loss: 0.00002318
Iteration 49/1000 | Loss: 0.00002318
Iteration 50/1000 | Loss: 0.00002318
Iteration 51/1000 | Loss: 0.00002318
Iteration 52/1000 | Loss: 0.00002317
Iteration 53/1000 | Loss: 0.00002317
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00002315
Iteration 56/1000 | Loss: 0.00002315
Iteration 57/1000 | Loss: 0.00002315
Iteration 58/1000 | Loss: 0.00002315
Iteration 59/1000 | Loss: 0.00002315
Iteration 60/1000 | Loss: 0.00002315
Iteration 61/1000 | Loss: 0.00002315
Iteration 62/1000 | Loss: 0.00002315
Iteration 63/1000 | Loss: 0.00002314
Iteration 64/1000 | Loss: 0.00002314
Iteration 65/1000 | Loss: 0.00002314
Iteration 66/1000 | Loss: 0.00002314
Iteration 67/1000 | Loss: 0.00002314
Iteration 68/1000 | Loss: 0.00002313
Iteration 69/1000 | Loss: 0.00002313
Iteration 70/1000 | Loss: 0.00002313
Iteration 71/1000 | Loss: 0.00002313
Iteration 72/1000 | Loss: 0.00002313
Iteration 73/1000 | Loss: 0.00002313
Iteration 74/1000 | Loss: 0.00002313
Iteration 75/1000 | Loss: 0.00002313
Iteration 76/1000 | Loss: 0.00002313
Iteration 77/1000 | Loss: 0.00002312
Iteration 78/1000 | Loss: 0.00002312
Iteration 79/1000 | Loss: 0.00002312
Iteration 80/1000 | Loss: 0.00002312
Iteration 81/1000 | Loss: 0.00002312
Iteration 82/1000 | Loss: 0.00002312
Iteration 83/1000 | Loss: 0.00002312
Iteration 84/1000 | Loss: 0.00002312
Iteration 85/1000 | Loss: 0.00002312
Iteration 86/1000 | Loss: 0.00002312
Iteration 87/1000 | Loss: 0.00002312
Iteration 88/1000 | Loss: 0.00002312
Iteration 89/1000 | Loss: 0.00002312
Iteration 90/1000 | Loss: 0.00002312
Iteration 91/1000 | Loss: 0.00002312
Iteration 92/1000 | Loss: 0.00002312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.3123002392821945e-05, 2.3123002392821945e-05, 2.3123002392821945e-05, 2.3123002392821945e-05, 2.3123002392821945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3123002392821945e-05

Optimization complete. Final v2v error: 4.205044746398926 mm

Highest mean error: 4.633522033691406 mm for frame 74

Lowest mean error: 3.812368869781494 mm for frame 100

Saving results

Total time: 36.211708545684814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135595
Iteration 2/25 | Loss: 0.00256460
Iteration 3/25 | Loss: 0.00191029
Iteration 4/25 | Loss: 0.00166837
Iteration 5/25 | Loss: 0.00140012
Iteration 6/25 | Loss: 0.00132762
Iteration 7/25 | Loss: 0.00116407
Iteration 8/25 | Loss: 0.00108621
Iteration 9/25 | Loss: 0.00107976
Iteration 10/25 | Loss: 0.00109691
Iteration 11/25 | Loss: 0.00100197
Iteration 12/25 | Loss: 0.00099480
Iteration 13/25 | Loss: 0.00097959
Iteration 14/25 | Loss: 0.00098422
Iteration 15/25 | Loss: 0.00097613
Iteration 16/25 | Loss: 0.00097637
Iteration 17/25 | Loss: 0.00097298
Iteration 18/25 | Loss: 0.00097260
Iteration 19/25 | Loss: 0.00097244
Iteration 20/25 | Loss: 0.00097243
Iteration 21/25 | Loss: 0.00097242
Iteration 22/25 | Loss: 0.00097241
Iteration 23/25 | Loss: 0.00097241
Iteration 24/25 | Loss: 0.00097241
Iteration 25/25 | Loss: 0.00097241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37071276
Iteration 2/25 | Loss: 0.00070859
Iteration 3/25 | Loss: 0.00070859
Iteration 4/25 | Loss: 0.00048761
Iteration 5/25 | Loss: 0.00048760
Iteration 6/25 | Loss: 0.00048760
Iteration 7/25 | Loss: 0.00048760
Iteration 8/25 | Loss: 0.00048760
Iteration 9/25 | Loss: 0.00048759
Iteration 10/25 | Loss: 0.00048759
Iteration 11/25 | Loss: 0.00048759
Iteration 12/25 | Loss: 0.00048759
Iteration 13/25 | Loss: 0.00048759
Iteration 14/25 | Loss: 0.00048759
Iteration 15/25 | Loss: 0.00048759
Iteration 16/25 | Loss: 0.00048759
Iteration 17/25 | Loss: 0.00048759
Iteration 18/25 | Loss: 0.00048759
Iteration 19/25 | Loss: 0.00048759
Iteration 20/25 | Loss: 0.00048759
Iteration 21/25 | Loss: 0.00048759
Iteration 22/25 | Loss: 0.00048759
Iteration 23/25 | Loss: 0.00048759
Iteration 24/25 | Loss: 0.00048759
Iteration 25/25 | Loss: 0.00048759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048759
Iteration 2/1000 | Loss: 0.00004174
Iteration 3/1000 | Loss: 0.00012137
Iteration 4/1000 | Loss: 0.00065388
Iteration 5/1000 | Loss: 0.00005760
Iteration 6/1000 | Loss: 0.00022592
Iteration 7/1000 | Loss: 0.00003205
Iteration 8/1000 | Loss: 0.00002967
Iteration 9/1000 | Loss: 0.00002891
Iteration 10/1000 | Loss: 0.00002846
Iteration 11/1000 | Loss: 0.00002814
Iteration 12/1000 | Loss: 0.00002811
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002753
Iteration 15/1000 | Loss: 0.00002748
Iteration 16/1000 | Loss: 0.00002748
Iteration 17/1000 | Loss: 0.00002748
Iteration 18/1000 | Loss: 0.00002747
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00002746
Iteration 21/1000 | Loss: 0.00002745
Iteration 22/1000 | Loss: 0.00002745
Iteration 23/1000 | Loss: 0.00002743
Iteration 24/1000 | Loss: 0.00002743
Iteration 25/1000 | Loss: 0.00002742
Iteration 26/1000 | Loss: 0.00002741
Iteration 27/1000 | Loss: 0.00002734
Iteration 28/1000 | Loss: 0.00002728
Iteration 29/1000 | Loss: 0.00002727
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00002726
Iteration 32/1000 | Loss: 0.00002726
Iteration 33/1000 | Loss: 0.00002726
Iteration 34/1000 | Loss: 0.00002726
Iteration 35/1000 | Loss: 0.00002725
Iteration 36/1000 | Loss: 0.00002725
Iteration 37/1000 | Loss: 0.00002725
Iteration 38/1000 | Loss: 0.00002725
Iteration 39/1000 | Loss: 0.00002724
Iteration 40/1000 | Loss: 0.00002724
Iteration 41/1000 | Loss: 0.00002724
Iteration 42/1000 | Loss: 0.00002723
Iteration 43/1000 | Loss: 0.00002723
Iteration 44/1000 | Loss: 0.00002723
Iteration 45/1000 | Loss: 0.00002722
Iteration 46/1000 | Loss: 0.00002722
Iteration 47/1000 | Loss: 0.00002722
Iteration 48/1000 | Loss: 0.00002722
Iteration 49/1000 | Loss: 0.00002722
Iteration 50/1000 | Loss: 0.00002721
Iteration 51/1000 | Loss: 0.00002721
Iteration 52/1000 | Loss: 0.00002721
Iteration 53/1000 | Loss: 0.00002721
Iteration 54/1000 | Loss: 0.00002721
Iteration 55/1000 | Loss: 0.00002721
Iteration 56/1000 | Loss: 0.00002721
Iteration 57/1000 | Loss: 0.00002721
Iteration 58/1000 | Loss: 0.00002721
Iteration 59/1000 | Loss: 0.00002721
Iteration 60/1000 | Loss: 0.00002721
Iteration 61/1000 | Loss: 0.00002721
Iteration 62/1000 | Loss: 0.00002721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.7208185201743618e-05, 2.7208185201743618e-05, 2.7208185201743618e-05, 2.7208185201743618e-05, 2.7208185201743618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7208185201743618e-05

Optimization complete. Final v2v error: 4.203902244567871 mm

Highest mean error: 19.861831665039062 mm for frame 71

Lowest mean error: 3.7782716751098633 mm for frame 5

Saving results

Total time: 69.6703577041626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_it_4368/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_it_4368/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994021
Iteration 2/25 | Loss: 0.00275482
Iteration 3/25 | Loss: 0.00409096
Iteration 4/25 | Loss: 0.00161674
Iteration 5/25 | Loss: 0.00148338
Iteration 6/25 | Loss: 0.00140642
Iteration 7/25 | Loss: 0.00129526
Iteration 8/25 | Loss: 0.00119150
Iteration 9/25 | Loss: 0.00118969
Iteration 10/25 | Loss: 0.00113675
Iteration 11/25 | Loss: 0.00117538
Iteration 12/25 | Loss: 0.00112751
Iteration 13/25 | Loss: 0.00111020
Iteration 14/25 | Loss: 0.00111300
Iteration 15/25 | Loss: 0.00111592
Iteration 16/25 | Loss: 0.00111190
Iteration 17/25 | Loss: 0.00110953
Iteration 18/25 | Loss: 0.00110852
Iteration 19/25 | Loss: 0.00109813
Iteration 20/25 | Loss: 0.00109645
Iteration 21/25 | Loss: 0.00109511
Iteration 22/25 | Loss: 0.00109451
Iteration 23/25 | Loss: 0.00109436
Iteration 24/25 | Loss: 0.00109435
Iteration 25/25 | Loss: 0.00109431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42526448
Iteration 2/25 | Loss: 0.00051402
Iteration 3/25 | Loss: 0.00051401
Iteration 4/25 | Loss: 0.00051401
Iteration 5/25 | Loss: 0.00051401
Iteration 6/25 | Loss: 0.00051401
Iteration 7/25 | Loss: 0.00051401
Iteration 8/25 | Loss: 0.00051401
Iteration 9/25 | Loss: 0.00051401
Iteration 10/25 | Loss: 0.00051401
Iteration 11/25 | Loss: 0.00051401
Iteration 12/25 | Loss: 0.00051401
Iteration 13/25 | Loss: 0.00051401
Iteration 14/25 | Loss: 0.00051401
Iteration 15/25 | Loss: 0.00051401
Iteration 16/25 | Loss: 0.00051401
Iteration 17/25 | Loss: 0.00051401
Iteration 18/25 | Loss: 0.00051401
Iteration 19/25 | Loss: 0.00051401
Iteration 20/25 | Loss: 0.00051401
Iteration 21/25 | Loss: 0.00051401
Iteration 22/25 | Loss: 0.00051401
Iteration 23/25 | Loss: 0.00051401
Iteration 24/25 | Loss: 0.00051401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005140055436640978, 0.0005140055436640978, 0.0005140055436640978, 0.0005140055436640978, 0.0005140055436640978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005140055436640978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051401
Iteration 2/1000 | Loss: 0.00005058
Iteration 3/1000 | Loss: 0.00003678
Iteration 4/1000 | Loss: 0.00003280
Iteration 5/1000 | Loss: 0.00003121
Iteration 6/1000 | Loss: 0.00002998
Iteration 7/1000 | Loss: 0.00002923
Iteration 8/1000 | Loss: 0.00002867
Iteration 9/1000 | Loss: 0.00002827
Iteration 10/1000 | Loss: 0.00002800
Iteration 11/1000 | Loss: 0.00002782
Iteration 12/1000 | Loss: 0.00002774
Iteration 13/1000 | Loss: 0.00002768
Iteration 14/1000 | Loss: 0.00002768
Iteration 15/1000 | Loss: 0.00002768
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002763
Iteration 19/1000 | Loss: 0.00002762
Iteration 20/1000 | Loss: 0.00002762
Iteration 21/1000 | Loss: 0.00002762
Iteration 22/1000 | Loss: 0.00002761
Iteration 23/1000 | Loss: 0.00002760
Iteration 24/1000 | Loss: 0.00002760
Iteration 25/1000 | Loss: 0.00002759
Iteration 26/1000 | Loss: 0.00002759
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00002759
Iteration 29/1000 | Loss: 0.00002759
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00002759
Iteration 32/1000 | Loss: 0.00002759
Iteration 33/1000 | Loss: 0.00002758
Iteration 34/1000 | Loss: 0.00002758
Iteration 35/1000 | Loss: 0.00002758
Iteration 36/1000 | Loss: 0.00002758
Iteration 37/1000 | Loss: 0.00002758
Iteration 38/1000 | Loss: 0.00002758
Iteration 39/1000 | Loss: 0.00002758
Iteration 40/1000 | Loss: 0.00002757
Iteration 41/1000 | Loss: 0.00002757
Iteration 42/1000 | Loss: 0.00002757
Iteration 43/1000 | Loss: 0.00002757
Iteration 44/1000 | Loss: 0.00002756
Iteration 45/1000 | Loss: 0.00002756
Iteration 46/1000 | Loss: 0.00002756
Iteration 47/1000 | Loss: 0.00002756
Iteration 48/1000 | Loss: 0.00002756
Iteration 49/1000 | Loss: 0.00002756
Iteration 50/1000 | Loss: 0.00002755
Iteration 51/1000 | Loss: 0.00002755
Iteration 52/1000 | Loss: 0.00002755
Iteration 53/1000 | Loss: 0.00002755
Iteration 54/1000 | Loss: 0.00002755
Iteration 55/1000 | Loss: 0.00002754
Iteration 56/1000 | Loss: 0.00002754
Iteration 57/1000 | Loss: 0.00002754
Iteration 58/1000 | Loss: 0.00002754
Iteration 59/1000 | Loss: 0.00002754
Iteration 60/1000 | Loss: 0.00002754
Iteration 61/1000 | Loss: 0.00002754
Iteration 62/1000 | Loss: 0.00002754
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002753
Iteration 65/1000 | Loss: 0.00002753
Iteration 66/1000 | Loss: 0.00002753
Iteration 67/1000 | Loss: 0.00002753
Iteration 68/1000 | Loss: 0.00002753
Iteration 69/1000 | Loss: 0.00002753
Iteration 70/1000 | Loss: 0.00002752
Iteration 71/1000 | Loss: 0.00002752
Iteration 72/1000 | Loss: 0.00002752
Iteration 73/1000 | Loss: 0.00002752
Iteration 74/1000 | Loss: 0.00002752
Iteration 75/1000 | Loss: 0.00002752
Iteration 76/1000 | Loss: 0.00002752
Iteration 77/1000 | Loss: 0.00002752
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00002752
Iteration 80/1000 | Loss: 0.00002751
Iteration 81/1000 | Loss: 0.00002751
Iteration 82/1000 | Loss: 0.00002751
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002751
Iteration 85/1000 | Loss: 0.00002751
Iteration 86/1000 | Loss: 0.00002750
Iteration 87/1000 | Loss: 0.00002750
Iteration 88/1000 | Loss: 0.00002750
Iteration 89/1000 | Loss: 0.00002750
Iteration 90/1000 | Loss: 0.00002750
Iteration 91/1000 | Loss: 0.00002750
Iteration 92/1000 | Loss: 0.00002750
Iteration 93/1000 | Loss: 0.00002750
Iteration 94/1000 | Loss: 0.00002750
Iteration 95/1000 | Loss: 0.00002750
Iteration 96/1000 | Loss: 0.00002750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.7497017072164454e-05, 2.7497017072164454e-05, 2.7497017072164454e-05, 2.7497017072164454e-05, 2.7497017072164454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7497017072164454e-05

Optimization complete. Final v2v error: 4.435588836669922 mm

Highest mean error: 6.058838367462158 mm for frame 76

Lowest mean error: 3.9231157302856445 mm for frame 100

Saving results

Total time: 65.44154024124146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600477
Iteration 2/25 | Loss: 0.00119118
Iteration 3/25 | Loss: 0.00113976
Iteration 4/25 | Loss: 0.00113507
Iteration 5/25 | Loss: 0.00113393
Iteration 6/25 | Loss: 0.00113393
Iteration 7/25 | Loss: 0.00113393
Iteration 8/25 | Loss: 0.00113393
Iteration 9/25 | Loss: 0.00113393
Iteration 10/25 | Loss: 0.00113393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011339327320456505, 0.0011339327320456505, 0.0011339327320456505, 0.0011339327320456505, 0.0011339327320456505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011339327320456505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.09343719
Iteration 2/25 | Loss: 0.00074286
Iteration 3/25 | Loss: 0.00074285
Iteration 4/25 | Loss: 0.00074284
Iteration 5/25 | Loss: 0.00074284
Iteration 6/25 | Loss: 0.00074284
Iteration 7/25 | Loss: 0.00074284
Iteration 8/25 | Loss: 0.00074284
Iteration 9/25 | Loss: 0.00074284
Iteration 10/25 | Loss: 0.00074284
Iteration 11/25 | Loss: 0.00074284
Iteration 12/25 | Loss: 0.00074284
Iteration 13/25 | Loss: 0.00074284
Iteration 14/25 | Loss: 0.00074284
Iteration 15/25 | Loss: 0.00074284
Iteration 16/25 | Loss: 0.00074284
Iteration 17/25 | Loss: 0.00074284
Iteration 18/25 | Loss: 0.00074284
Iteration 19/25 | Loss: 0.00074284
Iteration 20/25 | Loss: 0.00074284
Iteration 21/25 | Loss: 0.00074284
Iteration 22/25 | Loss: 0.00074284
Iteration 23/25 | Loss: 0.00074284
Iteration 24/25 | Loss: 0.00074284
Iteration 25/25 | Loss: 0.00074284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074284
Iteration 2/1000 | Loss: 0.00002699
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001290
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001132
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001092
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001070
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001069
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001067
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001056
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001055
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001053
Iteration 84/1000 | Loss: 0.00001053
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001050
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001045
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001040
Iteration 103/1000 | Loss: 0.00001039
Iteration 104/1000 | Loss: 0.00001039
Iteration 105/1000 | Loss: 0.00001039
Iteration 106/1000 | Loss: 0.00001039
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001038
Iteration 110/1000 | Loss: 0.00001038
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001038
Iteration 113/1000 | Loss: 0.00001038
Iteration 114/1000 | Loss: 0.00001038
Iteration 115/1000 | Loss: 0.00001038
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001037
Iteration 118/1000 | Loss: 0.00001037
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001035
Iteration 128/1000 | Loss: 0.00001035
Iteration 129/1000 | Loss: 0.00001035
Iteration 130/1000 | Loss: 0.00001035
Iteration 131/1000 | Loss: 0.00001034
Iteration 132/1000 | Loss: 0.00001034
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001033
Iteration 135/1000 | Loss: 0.00001033
Iteration 136/1000 | Loss: 0.00001033
Iteration 137/1000 | Loss: 0.00001033
Iteration 138/1000 | Loss: 0.00001033
Iteration 139/1000 | Loss: 0.00001033
Iteration 140/1000 | Loss: 0.00001033
Iteration 141/1000 | Loss: 0.00001033
Iteration 142/1000 | Loss: 0.00001033
Iteration 143/1000 | Loss: 0.00001033
Iteration 144/1000 | Loss: 0.00001033
Iteration 145/1000 | Loss: 0.00001033
Iteration 146/1000 | Loss: 0.00001033
Iteration 147/1000 | Loss: 0.00001032
Iteration 148/1000 | Loss: 0.00001032
Iteration 149/1000 | Loss: 0.00001032
Iteration 150/1000 | Loss: 0.00001032
Iteration 151/1000 | Loss: 0.00001032
Iteration 152/1000 | Loss: 0.00001032
Iteration 153/1000 | Loss: 0.00001032
Iteration 154/1000 | Loss: 0.00001032
Iteration 155/1000 | Loss: 0.00001032
Iteration 156/1000 | Loss: 0.00001032
Iteration 157/1000 | Loss: 0.00001032
Iteration 158/1000 | Loss: 0.00001032
Iteration 159/1000 | Loss: 0.00001032
Iteration 160/1000 | Loss: 0.00001032
Iteration 161/1000 | Loss: 0.00001032
Iteration 162/1000 | Loss: 0.00001032
Iteration 163/1000 | Loss: 0.00001032
Iteration 164/1000 | Loss: 0.00001032
Iteration 165/1000 | Loss: 0.00001032
Iteration 166/1000 | Loss: 0.00001032
Iteration 167/1000 | Loss: 0.00001032
Iteration 168/1000 | Loss: 0.00001032
Iteration 169/1000 | Loss: 0.00001032
Iteration 170/1000 | Loss: 0.00001032
Iteration 171/1000 | Loss: 0.00001032
Iteration 172/1000 | Loss: 0.00001032
Iteration 173/1000 | Loss: 0.00001032
Iteration 174/1000 | Loss: 0.00001032
Iteration 175/1000 | Loss: 0.00001032
Iteration 176/1000 | Loss: 0.00001032
Iteration 177/1000 | Loss: 0.00001032
Iteration 178/1000 | Loss: 0.00001032
Iteration 179/1000 | Loss: 0.00001032
Iteration 180/1000 | Loss: 0.00001032
Iteration 181/1000 | Loss: 0.00001032
Iteration 182/1000 | Loss: 0.00001032
Iteration 183/1000 | Loss: 0.00001032
Iteration 184/1000 | Loss: 0.00001032
Iteration 185/1000 | Loss: 0.00001032
Iteration 186/1000 | Loss: 0.00001032
Iteration 187/1000 | Loss: 0.00001032
Iteration 188/1000 | Loss: 0.00001032
Iteration 189/1000 | Loss: 0.00001032
Iteration 190/1000 | Loss: 0.00001032
Iteration 191/1000 | Loss: 0.00001032
Iteration 192/1000 | Loss: 0.00001032
Iteration 193/1000 | Loss: 0.00001032
Iteration 194/1000 | Loss: 0.00001032
Iteration 195/1000 | Loss: 0.00001032
Iteration 196/1000 | Loss: 0.00001032
Iteration 197/1000 | Loss: 0.00001032
Iteration 198/1000 | Loss: 0.00001032
Iteration 199/1000 | Loss: 0.00001032
Iteration 200/1000 | Loss: 0.00001032
Iteration 201/1000 | Loss: 0.00001032
Iteration 202/1000 | Loss: 0.00001032
Iteration 203/1000 | Loss: 0.00001032
Iteration 204/1000 | Loss: 0.00001032
Iteration 205/1000 | Loss: 0.00001032
Iteration 206/1000 | Loss: 0.00001032
Iteration 207/1000 | Loss: 0.00001032
Iteration 208/1000 | Loss: 0.00001032
Iteration 209/1000 | Loss: 0.00001032
Iteration 210/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.032496311381692e-05, 1.032496311381692e-05, 1.032496311381692e-05, 1.032496311381692e-05, 1.032496311381692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.032496311381692e-05

Optimization complete. Final v2v error: 2.7376317977905273 mm

Highest mean error: 2.9034059047698975 mm for frame 185

Lowest mean error: 2.488914966583252 mm for frame 82

Saving results

Total time: 40.06419801712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421758
Iteration 2/25 | Loss: 0.00114978
Iteration 3/25 | Loss: 0.00108805
Iteration 4/25 | Loss: 0.00107537
Iteration 5/25 | Loss: 0.00107166
Iteration 6/25 | Loss: 0.00107083
Iteration 7/25 | Loss: 0.00107082
Iteration 8/25 | Loss: 0.00107082
Iteration 9/25 | Loss: 0.00107082
Iteration 10/25 | Loss: 0.00107082
Iteration 11/25 | Loss: 0.00107082
Iteration 12/25 | Loss: 0.00107082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010708225890994072, 0.0010708225890994072, 0.0010708225890994072, 0.0010708225890994072, 0.0010708225890994072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010708225890994072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62419915
Iteration 2/25 | Loss: 0.00080184
Iteration 3/25 | Loss: 0.00080184
Iteration 4/25 | Loss: 0.00080184
Iteration 5/25 | Loss: 0.00080184
Iteration 6/25 | Loss: 0.00080184
Iteration 7/25 | Loss: 0.00080184
Iteration 8/25 | Loss: 0.00080184
Iteration 9/25 | Loss: 0.00080184
Iteration 10/25 | Loss: 0.00080184
Iteration 11/25 | Loss: 0.00080184
Iteration 12/25 | Loss: 0.00080184
Iteration 13/25 | Loss: 0.00080184
Iteration 14/25 | Loss: 0.00080184
Iteration 15/25 | Loss: 0.00080184
Iteration 16/25 | Loss: 0.00080184
Iteration 17/25 | Loss: 0.00080184
Iteration 18/25 | Loss: 0.00080184
Iteration 19/25 | Loss: 0.00080184
Iteration 20/25 | Loss: 0.00080184
Iteration 21/25 | Loss: 0.00080184
Iteration 22/25 | Loss: 0.00080184
Iteration 23/25 | Loss: 0.00080184
Iteration 24/25 | Loss: 0.00080184
Iteration 25/25 | Loss: 0.00080184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080184
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001420
Iteration 4/1000 | Loss: 0.00001323
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001131
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001129
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001115
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001105
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001090
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001090
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.089719808078371e-05, 1.089719808078371e-05, 1.089719808078371e-05, 1.089719808078371e-05, 1.089719808078371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.089719808078371e-05

Optimization complete. Final v2v error: 2.8415751457214355 mm

Highest mean error: 3.3578171730041504 mm for frame 62

Lowest mean error: 2.6548261642456055 mm for frame 85

Saving results

Total time: 34.34724164009094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812113
Iteration 2/25 | Loss: 0.00126573
Iteration 3/25 | Loss: 0.00111762
Iteration 4/25 | Loss: 0.00109882
Iteration 5/25 | Loss: 0.00109372
Iteration 6/25 | Loss: 0.00109257
Iteration 7/25 | Loss: 0.00109257
Iteration 8/25 | Loss: 0.00109257
Iteration 9/25 | Loss: 0.00109257
Iteration 10/25 | Loss: 0.00109257
Iteration 11/25 | Loss: 0.00109257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001092569320462644, 0.001092569320462644, 0.001092569320462644, 0.001092569320462644, 0.001092569320462644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001092569320462644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37493610
Iteration 2/25 | Loss: 0.00082538
Iteration 3/25 | Loss: 0.00082537
Iteration 4/25 | Loss: 0.00082537
Iteration 5/25 | Loss: 0.00082537
Iteration 6/25 | Loss: 0.00082537
Iteration 7/25 | Loss: 0.00082537
Iteration 8/25 | Loss: 0.00082537
Iteration 9/25 | Loss: 0.00082537
Iteration 10/25 | Loss: 0.00082537
Iteration 11/25 | Loss: 0.00082537
Iteration 12/25 | Loss: 0.00082537
Iteration 13/25 | Loss: 0.00082537
Iteration 14/25 | Loss: 0.00082537
Iteration 15/25 | Loss: 0.00082537
Iteration 16/25 | Loss: 0.00082537
Iteration 17/25 | Loss: 0.00082537
Iteration 18/25 | Loss: 0.00082537
Iteration 19/25 | Loss: 0.00082537
Iteration 20/25 | Loss: 0.00082537
Iteration 21/25 | Loss: 0.00082537
Iteration 22/25 | Loss: 0.00082537
Iteration 23/25 | Loss: 0.00082537
Iteration 24/25 | Loss: 0.00082537
Iteration 25/25 | Loss: 0.00082537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082537
Iteration 2/1000 | Loss: 0.00002344
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001404
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001200
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001143
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001116
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001112
Iteration 33/1000 | Loss: 0.00001112
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001107
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001103
Iteration 48/1000 | Loss: 0.00001103
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001102
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001086
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001084
Iteration 136/1000 | Loss: 0.00001084
Iteration 137/1000 | Loss: 0.00001084
Iteration 138/1000 | Loss: 0.00001084
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001083
Iteration 142/1000 | Loss: 0.00001083
Iteration 143/1000 | Loss: 0.00001083
Iteration 144/1000 | Loss: 0.00001083
Iteration 145/1000 | Loss: 0.00001083
Iteration 146/1000 | Loss: 0.00001083
Iteration 147/1000 | Loss: 0.00001083
Iteration 148/1000 | Loss: 0.00001083
Iteration 149/1000 | Loss: 0.00001082
Iteration 150/1000 | Loss: 0.00001082
Iteration 151/1000 | Loss: 0.00001082
Iteration 152/1000 | Loss: 0.00001082
Iteration 153/1000 | Loss: 0.00001082
Iteration 154/1000 | Loss: 0.00001082
Iteration 155/1000 | Loss: 0.00001082
Iteration 156/1000 | Loss: 0.00001082
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001081
Iteration 159/1000 | Loss: 0.00001081
Iteration 160/1000 | Loss: 0.00001081
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001081
Iteration 168/1000 | Loss: 0.00001081
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001080
Iteration 174/1000 | Loss: 0.00001080
Iteration 175/1000 | Loss: 0.00001080
Iteration 176/1000 | Loss: 0.00001080
Iteration 177/1000 | Loss: 0.00001080
Iteration 178/1000 | Loss: 0.00001080
Iteration 179/1000 | Loss: 0.00001080
Iteration 180/1000 | Loss: 0.00001080
Iteration 181/1000 | Loss: 0.00001080
Iteration 182/1000 | Loss: 0.00001080
Iteration 183/1000 | Loss: 0.00001080
Iteration 184/1000 | Loss: 0.00001079
Iteration 185/1000 | Loss: 0.00001079
Iteration 186/1000 | Loss: 0.00001079
Iteration 187/1000 | Loss: 0.00001079
Iteration 188/1000 | Loss: 0.00001079
Iteration 189/1000 | Loss: 0.00001079
Iteration 190/1000 | Loss: 0.00001079
Iteration 191/1000 | Loss: 0.00001079
Iteration 192/1000 | Loss: 0.00001079
Iteration 193/1000 | Loss: 0.00001079
Iteration 194/1000 | Loss: 0.00001079
Iteration 195/1000 | Loss: 0.00001079
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001079
Iteration 199/1000 | Loss: 0.00001079
Iteration 200/1000 | Loss: 0.00001079
Iteration 201/1000 | Loss: 0.00001079
Iteration 202/1000 | Loss: 0.00001079
Iteration 203/1000 | Loss: 0.00001079
Iteration 204/1000 | Loss: 0.00001079
Iteration 205/1000 | Loss: 0.00001079
Iteration 206/1000 | Loss: 0.00001079
Iteration 207/1000 | Loss: 0.00001079
Iteration 208/1000 | Loss: 0.00001079
Iteration 209/1000 | Loss: 0.00001079
Iteration 210/1000 | Loss: 0.00001079
Iteration 211/1000 | Loss: 0.00001079
Iteration 212/1000 | Loss: 0.00001079
Iteration 213/1000 | Loss: 0.00001079
Iteration 214/1000 | Loss: 0.00001079
Iteration 215/1000 | Loss: 0.00001079
Iteration 216/1000 | Loss: 0.00001079
Iteration 217/1000 | Loss: 0.00001079
Iteration 218/1000 | Loss: 0.00001079
Iteration 219/1000 | Loss: 0.00001079
Iteration 220/1000 | Loss: 0.00001079
Iteration 221/1000 | Loss: 0.00001079
Iteration 222/1000 | Loss: 0.00001079
Iteration 223/1000 | Loss: 0.00001079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.0789508451125585e-05, 1.0789508451125585e-05, 1.0789508451125585e-05, 1.0789508451125585e-05, 1.0789508451125585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0789508451125585e-05

Optimization complete. Final v2v error: 2.785446882247925 mm

Highest mean error: 3.84735107421875 mm for frame 85

Lowest mean error: 2.4927496910095215 mm for frame 119

Saving results

Total time: 41.05511283874512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430782
Iteration 2/25 | Loss: 0.00139653
Iteration 3/25 | Loss: 0.00114479
Iteration 4/25 | Loss: 0.00112259
Iteration 5/25 | Loss: 0.00112033
Iteration 6/25 | Loss: 0.00111963
Iteration 7/25 | Loss: 0.00111963
Iteration 8/25 | Loss: 0.00111963
Iteration 9/25 | Loss: 0.00111963
Iteration 10/25 | Loss: 0.00111963
Iteration 11/25 | Loss: 0.00111963
Iteration 12/25 | Loss: 0.00111963
Iteration 13/25 | Loss: 0.00111963
Iteration 14/25 | Loss: 0.00111963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011196313425898552, 0.0011196313425898552, 0.0011196313425898552, 0.0011196313425898552, 0.0011196313425898552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011196313425898552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.27962399
Iteration 2/25 | Loss: 0.00072512
Iteration 3/25 | Loss: 0.00072508
Iteration 4/25 | Loss: 0.00072508
Iteration 5/25 | Loss: 0.00072508
Iteration 6/25 | Loss: 0.00072508
Iteration 7/25 | Loss: 0.00072508
Iteration 8/25 | Loss: 0.00072508
Iteration 9/25 | Loss: 0.00072508
Iteration 10/25 | Loss: 0.00072508
Iteration 11/25 | Loss: 0.00072508
Iteration 12/25 | Loss: 0.00072508
Iteration 13/25 | Loss: 0.00072508
Iteration 14/25 | Loss: 0.00072508
Iteration 15/25 | Loss: 0.00072508
Iteration 16/25 | Loss: 0.00072508
Iteration 17/25 | Loss: 0.00072508
Iteration 18/25 | Loss: 0.00072508
Iteration 19/25 | Loss: 0.00072508
Iteration 20/25 | Loss: 0.00072508
Iteration 21/25 | Loss: 0.00072508
Iteration 22/25 | Loss: 0.00072508
Iteration 23/25 | Loss: 0.00072508
Iteration 24/25 | Loss: 0.00072508
Iteration 25/25 | Loss: 0.00072508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072508
Iteration 2/1000 | Loss: 0.00002787
Iteration 3/1000 | Loss: 0.00001806
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001280
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001226
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001221
Iteration 122/1000 | Loss: 0.00001221
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001217
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001216
Iteration 155/1000 | Loss: 0.00001216
Iteration 156/1000 | Loss: 0.00001216
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Iteration 160/1000 | Loss: 0.00001216
Iteration 161/1000 | Loss: 0.00001215
Iteration 162/1000 | Loss: 0.00001215
Iteration 163/1000 | Loss: 0.00001215
Iteration 164/1000 | Loss: 0.00001215
Iteration 165/1000 | Loss: 0.00001215
Iteration 166/1000 | Loss: 0.00001215
Iteration 167/1000 | Loss: 0.00001215
Iteration 168/1000 | Loss: 0.00001215
Iteration 169/1000 | Loss: 0.00001215
Iteration 170/1000 | Loss: 0.00001215
Iteration 171/1000 | Loss: 0.00001215
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001215
Iteration 185/1000 | Loss: 0.00001215
Iteration 186/1000 | Loss: 0.00001215
Iteration 187/1000 | Loss: 0.00001215
Iteration 188/1000 | Loss: 0.00001215
Iteration 189/1000 | Loss: 0.00001215
Iteration 190/1000 | Loss: 0.00001215
Iteration 191/1000 | Loss: 0.00001215
Iteration 192/1000 | Loss: 0.00001215
Iteration 193/1000 | Loss: 0.00001215
Iteration 194/1000 | Loss: 0.00001215
Iteration 195/1000 | Loss: 0.00001215
Iteration 196/1000 | Loss: 0.00001215
Iteration 197/1000 | Loss: 0.00001215
Iteration 198/1000 | Loss: 0.00001215
Iteration 199/1000 | Loss: 0.00001215
Iteration 200/1000 | Loss: 0.00001215
Iteration 201/1000 | Loss: 0.00001215
Iteration 202/1000 | Loss: 0.00001215
Iteration 203/1000 | Loss: 0.00001215
Iteration 204/1000 | Loss: 0.00001215
Iteration 205/1000 | Loss: 0.00001215
Iteration 206/1000 | Loss: 0.00001215
Iteration 207/1000 | Loss: 0.00001215
Iteration 208/1000 | Loss: 0.00001215
Iteration 209/1000 | Loss: 0.00001215
Iteration 210/1000 | Loss: 0.00001215
Iteration 211/1000 | Loss: 0.00001215
Iteration 212/1000 | Loss: 0.00001215
Iteration 213/1000 | Loss: 0.00001215
Iteration 214/1000 | Loss: 0.00001215
Iteration 215/1000 | Loss: 0.00001215
Iteration 216/1000 | Loss: 0.00001215
Iteration 217/1000 | Loss: 0.00001215
Iteration 218/1000 | Loss: 0.00001215
Iteration 219/1000 | Loss: 0.00001215
Iteration 220/1000 | Loss: 0.00001215
Iteration 221/1000 | Loss: 0.00001215
Iteration 222/1000 | Loss: 0.00001215
Iteration 223/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2145740583946463e-05, 1.2145740583946463e-05, 1.2145740583946463e-05, 1.2145740583946463e-05, 1.2145740583946463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2145740583946463e-05

Optimization complete. Final v2v error: 2.947678327560425 mm

Highest mean error: 3.6646153926849365 mm for frame 71

Lowest mean error: 2.544020652770996 mm for frame 100

Saving results

Total time: 40.53049087524414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936407
Iteration 2/25 | Loss: 0.00207767
Iteration 3/25 | Loss: 0.00158112
Iteration 4/25 | Loss: 0.00142077
Iteration 5/25 | Loss: 0.00138430
Iteration 6/25 | Loss: 0.00141294
Iteration 7/25 | Loss: 0.00161857
Iteration 8/25 | Loss: 0.00174546
Iteration 9/25 | Loss: 0.00145204
Iteration 10/25 | Loss: 0.00134978
Iteration 11/25 | Loss: 0.00128906
Iteration 12/25 | Loss: 0.00126527
Iteration 13/25 | Loss: 0.00126109
Iteration 14/25 | Loss: 0.00126832
Iteration 15/25 | Loss: 0.00125813
Iteration 16/25 | Loss: 0.00123821
Iteration 17/25 | Loss: 0.00123326
Iteration 18/25 | Loss: 0.00123230
Iteration 19/25 | Loss: 0.00123213
Iteration 20/25 | Loss: 0.00123211
Iteration 21/25 | Loss: 0.00123211
Iteration 22/25 | Loss: 0.00123211
Iteration 23/25 | Loss: 0.00123211
Iteration 24/25 | Loss: 0.00123210
Iteration 25/25 | Loss: 0.00123210

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27828753
Iteration 2/25 | Loss: 0.00078910
Iteration 3/25 | Loss: 0.00078907
Iteration 4/25 | Loss: 0.00078907
Iteration 5/25 | Loss: 0.00078907
Iteration 6/25 | Loss: 0.00078907
Iteration 7/25 | Loss: 0.00078906
Iteration 8/25 | Loss: 0.00078906
Iteration 9/25 | Loss: 0.00078906
Iteration 10/25 | Loss: 0.00078906
Iteration 11/25 | Loss: 0.00078906
Iteration 12/25 | Loss: 0.00078906
Iteration 13/25 | Loss: 0.00078906
Iteration 14/25 | Loss: 0.00078906
Iteration 15/25 | Loss: 0.00078906
Iteration 16/25 | Loss: 0.00078906
Iteration 17/25 | Loss: 0.00078906
Iteration 18/25 | Loss: 0.00078906
Iteration 19/25 | Loss: 0.00078906
Iteration 20/25 | Loss: 0.00078906
Iteration 21/25 | Loss: 0.00078906
Iteration 22/25 | Loss: 0.00078906
Iteration 23/25 | Loss: 0.00078906
Iteration 24/25 | Loss: 0.00078906
Iteration 25/25 | Loss: 0.00078906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078906
Iteration 2/1000 | Loss: 0.00732591
Iteration 3/1000 | Loss: 0.00386440
Iteration 4/1000 | Loss: 0.00777997
Iteration 5/1000 | Loss: 0.00407113
Iteration 6/1000 | Loss: 0.00042342
Iteration 7/1000 | Loss: 0.00033386
Iteration 8/1000 | Loss: 0.00029242
Iteration 9/1000 | Loss: 0.00022948
Iteration 10/1000 | Loss: 0.00020451
Iteration 11/1000 | Loss: 0.00019366
Iteration 12/1000 | Loss: 0.00018324
Iteration 13/1000 | Loss: 0.00017656
Iteration 14/1000 | Loss: 0.00017013
Iteration 15/1000 | Loss: 0.00016413
Iteration 16/1000 | Loss: 0.00121074
Iteration 17/1000 | Loss: 0.00136477
Iteration 18/1000 | Loss: 0.00080079
Iteration 19/1000 | Loss: 0.00046341
Iteration 20/1000 | Loss: 0.00018639
Iteration 21/1000 | Loss: 0.00015047
Iteration 22/1000 | Loss: 0.00013333
Iteration 23/1000 | Loss: 0.00011910
Iteration 24/1000 | Loss: 0.00083122
Iteration 25/1000 | Loss: 0.00042744
Iteration 26/1000 | Loss: 0.00016237
Iteration 27/1000 | Loss: 0.00011829
Iteration 28/1000 | Loss: 0.00009871
Iteration 29/1000 | Loss: 0.00008844
Iteration 30/1000 | Loss: 0.00007623
Iteration 31/1000 | Loss: 0.00006701
Iteration 32/1000 | Loss: 0.00006164
Iteration 33/1000 | Loss: 0.00005615
Iteration 34/1000 | Loss: 0.00005212
Iteration 35/1000 | Loss: 0.00004829
Iteration 36/1000 | Loss: 0.00004409
Iteration 37/1000 | Loss: 0.00004112
Iteration 38/1000 | Loss: 0.00003774
Iteration 39/1000 | Loss: 0.00003453
Iteration 40/1000 | Loss: 0.00003230
Iteration 41/1000 | Loss: 0.00003043
Iteration 42/1000 | Loss: 0.00014541
Iteration 43/1000 | Loss: 0.00002996
Iteration 44/1000 | Loss: 0.00002730
Iteration 45/1000 | Loss: 0.00002628
Iteration 46/1000 | Loss: 0.00002473
Iteration 47/1000 | Loss: 0.00002380
Iteration 48/1000 | Loss: 0.00002321
Iteration 49/1000 | Loss: 0.00002281
Iteration 50/1000 | Loss: 0.00002236
Iteration 51/1000 | Loss: 0.00002201
Iteration 52/1000 | Loss: 0.00002172
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002148
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002146
Iteration 58/1000 | Loss: 0.00002145
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002140
Iteration 63/1000 | Loss: 0.00002131
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002122
Iteration 67/1000 | Loss: 0.00002120
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002119
Iteration 70/1000 | Loss: 0.00002118
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002109
Iteration 77/1000 | Loss: 0.00002109
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002107
Iteration 81/1000 | Loss: 0.00002107
Iteration 82/1000 | Loss: 0.00002107
Iteration 83/1000 | Loss: 0.00002106
Iteration 84/1000 | Loss: 0.00002105
Iteration 85/1000 | Loss: 0.00002105
Iteration 86/1000 | Loss: 0.00002105
Iteration 87/1000 | Loss: 0.00002105
Iteration 88/1000 | Loss: 0.00002105
Iteration 89/1000 | Loss: 0.00002104
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002104
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002103
Iteration 101/1000 | Loss: 0.00002103
Iteration 102/1000 | Loss: 0.00002103
Iteration 103/1000 | Loss: 0.00002103
Iteration 104/1000 | Loss: 0.00002103
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002101
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002100
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002099
Iteration 122/1000 | Loss: 0.00002099
Iteration 123/1000 | Loss: 0.00002099
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002099
Iteration 126/1000 | Loss: 0.00002099
Iteration 127/1000 | Loss: 0.00002099
Iteration 128/1000 | Loss: 0.00002099
Iteration 129/1000 | Loss: 0.00002099
Iteration 130/1000 | Loss: 0.00002098
Iteration 131/1000 | Loss: 0.00002098
Iteration 132/1000 | Loss: 0.00002098
Iteration 133/1000 | Loss: 0.00002098
Iteration 134/1000 | Loss: 0.00002098
Iteration 135/1000 | Loss: 0.00002098
Iteration 136/1000 | Loss: 0.00002098
Iteration 137/1000 | Loss: 0.00002098
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002098
Iteration 142/1000 | Loss: 0.00002098
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002097
Iteration 148/1000 | Loss: 0.00002097
Iteration 149/1000 | Loss: 0.00002097
Iteration 150/1000 | Loss: 0.00002097
Iteration 151/1000 | Loss: 0.00002097
Iteration 152/1000 | Loss: 0.00002097
Iteration 153/1000 | Loss: 0.00002097
Iteration 154/1000 | Loss: 0.00002097
Iteration 155/1000 | Loss: 0.00002097
Iteration 156/1000 | Loss: 0.00002097
Iteration 157/1000 | Loss: 0.00002097
Iteration 158/1000 | Loss: 0.00002097
Iteration 159/1000 | Loss: 0.00002097
Iteration 160/1000 | Loss: 0.00002096
Iteration 161/1000 | Loss: 0.00002096
Iteration 162/1000 | Loss: 0.00002096
Iteration 163/1000 | Loss: 0.00002096
Iteration 164/1000 | Loss: 0.00002096
Iteration 165/1000 | Loss: 0.00002096
Iteration 166/1000 | Loss: 0.00002096
Iteration 167/1000 | Loss: 0.00002096
Iteration 168/1000 | Loss: 0.00002096
Iteration 169/1000 | Loss: 0.00002096
Iteration 170/1000 | Loss: 0.00002096
Iteration 171/1000 | Loss: 0.00002096
Iteration 172/1000 | Loss: 0.00002096
Iteration 173/1000 | Loss: 0.00002096
Iteration 174/1000 | Loss: 0.00002096
Iteration 175/1000 | Loss: 0.00002096
Iteration 176/1000 | Loss: 0.00002096
Iteration 177/1000 | Loss: 0.00002096
Iteration 178/1000 | Loss: 0.00002096
Iteration 179/1000 | Loss: 0.00002096
Iteration 180/1000 | Loss: 0.00002095
Iteration 181/1000 | Loss: 0.00002095
Iteration 182/1000 | Loss: 0.00002095
Iteration 183/1000 | Loss: 0.00002095
Iteration 184/1000 | Loss: 0.00002095
Iteration 185/1000 | Loss: 0.00002095
Iteration 186/1000 | Loss: 0.00002095
Iteration 187/1000 | Loss: 0.00002095
Iteration 188/1000 | Loss: 0.00002095
Iteration 189/1000 | Loss: 0.00002095
Iteration 190/1000 | Loss: 0.00002095
Iteration 191/1000 | Loss: 0.00002095
Iteration 192/1000 | Loss: 0.00002095
Iteration 193/1000 | Loss: 0.00002095
Iteration 194/1000 | Loss: 0.00002095
Iteration 195/1000 | Loss: 0.00002095
Iteration 196/1000 | Loss: 0.00002095
Iteration 197/1000 | Loss: 0.00002095
Iteration 198/1000 | Loss: 0.00002095
Iteration 199/1000 | Loss: 0.00002095
Iteration 200/1000 | Loss: 0.00002095
Iteration 201/1000 | Loss: 0.00002095
Iteration 202/1000 | Loss: 0.00002095
Iteration 203/1000 | Loss: 0.00002095
Iteration 204/1000 | Loss: 0.00002095
Iteration 205/1000 | Loss: 0.00002095
Iteration 206/1000 | Loss: 0.00002095
Iteration 207/1000 | Loss: 0.00002095
Iteration 208/1000 | Loss: 0.00002095
Iteration 209/1000 | Loss: 0.00002095
Iteration 210/1000 | Loss: 0.00002095
Iteration 211/1000 | Loss: 0.00002095
Iteration 212/1000 | Loss: 0.00002095
Iteration 213/1000 | Loss: 0.00002095
Iteration 214/1000 | Loss: 0.00002095
Iteration 215/1000 | Loss: 0.00002095
Iteration 216/1000 | Loss: 0.00002095
Iteration 217/1000 | Loss: 0.00002095
Iteration 218/1000 | Loss: 0.00002095
Iteration 219/1000 | Loss: 0.00002095
Iteration 220/1000 | Loss: 0.00002095
Iteration 221/1000 | Loss: 0.00002095
Iteration 222/1000 | Loss: 0.00002095
Iteration 223/1000 | Loss: 0.00002095
Iteration 224/1000 | Loss: 0.00002095
Iteration 225/1000 | Loss: 0.00002095
Iteration 226/1000 | Loss: 0.00002095
Iteration 227/1000 | Loss: 0.00002095
Iteration 228/1000 | Loss: 0.00002095
Iteration 229/1000 | Loss: 0.00002095
Iteration 230/1000 | Loss: 0.00002095
Iteration 231/1000 | Loss: 0.00002095
Iteration 232/1000 | Loss: 0.00002095
Iteration 233/1000 | Loss: 0.00002095
Iteration 234/1000 | Loss: 0.00002095
Iteration 235/1000 | Loss: 0.00002095
Iteration 236/1000 | Loss: 0.00002095
Iteration 237/1000 | Loss: 0.00002095
Iteration 238/1000 | Loss: 0.00002095
Iteration 239/1000 | Loss: 0.00002095
Iteration 240/1000 | Loss: 0.00002095
Iteration 241/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.0950654288753867e-05, 2.0950654288753867e-05, 2.0950654288753867e-05, 2.0950654288753867e-05, 2.0950654288753867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0950654288753867e-05

Optimization complete. Final v2v error: 3.7900588512420654 mm

Highest mean error: 4.435611248016357 mm for frame 4

Lowest mean error: 3.5009801387786865 mm for frame 127

Saving results

Total time: 121.49124050140381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593853
Iteration 2/25 | Loss: 0.00126741
Iteration 3/25 | Loss: 0.00118399
Iteration 4/25 | Loss: 0.00117449
Iteration 5/25 | Loss: 0.00117141
Iteration 6/25 | Loss: 0.00117099
Iteration 7/25 | Loss: 0.00117099
Iteration 8/25 | Loss: 0.00117099
Iteration 9/25 | Loss: 0.00117099
Iteration 10/25 | Loss: 0.00117099
Iteration 11/25 | Loss: 0.00117099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011709873797371984, 0.0011709873797371984, 0.0011709873797371984, 0.0011709873797371984, 0.0011709873797371984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011709873797371984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37023520
Iteration 2/25 | Loss: 0.00075499
Iteration 3/25 | Loss: 0.00075496
Iteration 4/25 | Loss: 0.00075495
Iteration 5/25 | Loss: 0.00075495
Iteration 6/25 | Loss: 0.00075495
Iteration 7/25 | Loss: 0.00075495
Iteration 8/25 | Loss: 0.00075495
Iteration 9/25 | Loss: 0.00075495
Iteration 10/25 | Loss: 0.00075495
Iteration 11/25 | Loss: 0.00075495
Iteration 12/25 | Loss: 0.00075495
Iteration 13/25 | Loss: 0.00075495
Iteration 14/25 | Loss: 0.00075495
Iteration 15/25 | Loss: 0.00075495
Iteration 16/25 | Loss: 0.00075495
Iteration 17/25 | Loss: 0.00075495
Iteration 18/25 | Loss: 0.00075495
Iteration 19/25 | Loss: 0.00075495
Iteration 20/25 | Loss: 0.00075495
Iteration 21/25 | Loss: 0.00075495
Iteration 22/25 | Loss: 0.00075495
Iteration 23/25 | Loss: 0.00075495
Iteration 24/25 | Loss: 0.00075495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007549505098722875, 0.0007549505098722875, 0.0007549505098722875, 0.0007549505098722875, 0.0007549505098722875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007549505098722875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075495
Iteration 2/1000 | Loss: 0.00003497
Iteration 3/1000 | Loss: 0.00002248
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001660
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001645
Iteration 26/1000 | Loss: 0.00001645
Iteration 27/1000 | Loss: 0.00001644
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001642
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001613
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001611
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001610
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001609
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001609
Iteration 97/1000 | Loss: 0.00001609
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001607
Iteration 105/1000 | Loss: 0.00001607
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001607
Iteration 108/1000 | Loss: 0.00001607
Iteration 109/1000 | Loss: 0.00001607
Iteration 110/1000 | Loss: 0.00001606
Iteration 111/1000 | Loss: 0.00001606
Iteration 112/1000 | Loss: 0.00001606
Iteration 113/1000 | Loss: 0.00001606
Iteration 114/1000 | Loss: 0.00001606
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001605
Iteration 117/1000 | Loss: 0.00001605
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001601
Iteration 131/1000 | Loss: 0.00001601
Iteration 132/1000 | Loss: 0.00001601
Iteration 133/1000 | Loss: 0.00001601
Iteration 134/1000 | Loss: 0.00001601
Iteration 135/1000 | Loss: 0.00001601
Iteration 136/1000 | Loss: 0.00001601
Iteration 137/1000 | Loss: 0.00001601
Iteration 138/1000 | Loss: 0.00001600
Iteration 139/1000 | Loss: 0.00001600
Iteration 140/1000 | Loss: 0.00001600
Iteration 141/1000 | Loss: 0.00001600
Iteration 142/1000 | Loss: 0.00001600
Iteration 143/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.6004803910618648e-05, 1.6004803910618648e-05, 1.6004803910618648e-05, 1.6004803910618648e-05, 1.6004803910618648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6004803910618648e-05

Optimization complete. Final v2v error: 3.3227360248565674 mm

Highest mean error: 3.732168674468994 mm for frame 160

Lowest mean error: 2.8175556659698486 mm for frame 239

Saving results

Total time: 43.55285358428955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009756
Iteration 2/25 | Loss: 0.01009756
Iteration 3/25 | Loss: 0.01009756
Iteration 4/25 | Loss: 0.01009756
Iteration 5/25 | Loss: 0.01009756
Iteration 6/25 | Loss: 0.01009756
Iteration 7/25 | Loss: 0.01009756
Iteration 8/25 | Loss: 0.01009756
Iteration 9/25 | Loss: 0.01009756
Iteration 10/25 | Loss: 0.01009756
Iteration 11/25 | Loss: 0.01009756
Iteration 12/25 | Loss: 0.01009756
Iteration 13/25 | Loss: 0.01009755
Iteration 14/25 | Loss: 0.01009755
Iteration 15/25 | Loss: 0.01009755
Iteration 16/25 | Loss: 0.01009755
Iteration 17/25 | Loss: 0.01009755
Iteration 18/25 | Loss: 0.01009755
Iteration 19/25 | Loss: 0.01009755
Iteration 20/25 | Loss: 0.01009755
Iteration 21/25 | Loss: 0.01009755
Iteration 22/25 | Loss: 0.01009755
Iteration 23/25 | Loss: 0.01009755
Iteration 24/25 | Loss: 0.01009755
Iteration 25/25 | Loss: 0.01009755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99932063
Iteration 2/25 | Loss: 0.00464624
Iteration 3/25 | Loss: 0.00464624
Iteration 4/25 | Loss: 0.00464623
Iteration 5/25 | Loss: 0.00464623
Iteration 6/25 | Loss: 0.00464623
Iteration 7/25 | Loss: 0.00464623
Iteration 8/25 | Loss: 0.00464623
Iteration 9/25 | Loss: 0.00464623
Iteration 10/25 | Loss: 0.00464623
Iteration 11/25 | Loss: 0.00464623
Iteration 12/25 | Loss: 0.00464623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.004646229092031717, 0.004646229092031717, 0.004646229092031717, 0.004646229092031717, 0.004646229092031717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004646229092031717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00464623
Iteration 2/1000 | Loss: 0.00689703
Iteration 3/1000 | Loss: 0.00124165
Iteration 4/1000 | Loss: 0.00034835
Iteration 5/1000 | Loss: 0.00017600
Iteration 6/1000 | Loss: 0.00013815
Iteration 7/1000 | Loss: 0.00075810
Iteration 8/1000 | Loss: 0.00008555
Iteration 9/1000 | Loss: 0.00084684
Iteration 10/1000 | Loss: 0.00004094
Iteration 11/1000 | Loss: 0.00020451
Iteration 12/1000 | Loss: 0.00062479
Iteration 13/1000 | Loss: 0.00026623
Iteration 14/1000 | Loss: 0.00019165
Iteration 15/1000 | Loss: 0.00128139
Iteration 16/1000 | Loss: 0.00029935
Iteration 17/1000 | Loss: 0.00004033
Iteration 18/1000 | Loss: 0.00044844
Iteration 19/1000 | Loss: 0.00031815
Iteration 20/1000 | Loss: 0.00013926
Iteration 21/1000 | Loss: 0.00004630
Iteration 22/1000 | Loss: 0.00011035
Iteration 23/1000 | Loss: 0.00044839
Iteration 24/1000 | Loss: 0.00036783
Iteration 25/1000 | Loss: 0.00040936
Iteration 26/1000 | Loss: 0.00018654
Iteration 27/1000 | Loss: 0.00015931
Iteration 28/1000 | Loss: 0.00025245
Iteration 29/1000 | Loss: 0.00009769
Iteration 30/1000 | Loss: 0.00008528
Iteration 31/1000 | Loss: 0.00016379
Iteration 32/1000 | Loss: 0.00008204
Iteration 33/1000 | Loss: 0.00020666
Iteration 34/1000 | Loss: 0.00004015
Iteration 35/1000 | Loss: 0.00011379
Iteration 36/1000 | Loss: 0.00004371
Iteration 37/1000 | Loss: 0.00015974
Iteration 38/1000 | Loss: 0.00064564
Iteration 39/1000 | Loss: 0.00012521
Iteration 40/1000 | Loss: 0.00003253
Iteration 41/1000 | Loss: 0.00006900
Iteration 42/1000 | Loss: 0.00007092
Iteration 43/1000 | Loss: 0.00014050
Iteration 44/1000 | Loss: 0.00008159
Iteration 45/1000 | Loss: 0.00016004
Iteration 46/1000 | Loss: 0.00008680
Iteration 47/1000 | Loss: 0.00009555
Iteration 48/1000 | Loss: 0.00029251
Iteration 49/1000 | Loss: 0.00093462
Iteration 50/1000 | Loss: 0.00004956
Iteration 51/1000 | Loss: 0.00060891
Iteration 52/1000 | Loss: 0.00004999
Iteration 53/1000 | Loss: 0.00011546
Iteration 54/1000 | Loss: 0.00011970
Iteration 55/1000 | Loss: 0.00008520
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00011148
Iteration 58/1000 | Loss: 0.00006102
Iteration 59/1000 | Loss: 0.00016594
Iteration 60/1000 | Loss: 0.00004379
Iteration 61/1000 | Loss: 0.00008800
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00010258
Iteration 64/1000 | Loss: 0.00004378
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00027617
Iteration 67/1000 | Loss: 0.00008828
Iteration 68/1000 | Loss: 0.00009977
Iteration 69/1000 | Loss: 0.00014215
Iteration 70/1000 | Loss: 0.00010877
Iteration 71/1000 | Loss: 0.00005414
Iteration 72/1000 | Loss: 0.00014226
Iteration 73/1000 | Loss: 0.00041113
Iteration 74/1000 | Loss: 0.00016276
Iteration 75/1000 | Loss: 0.00003492
Iteration 76/1000 | Loss: 0.00012474
Iteration 77/1000 | Loss: 0.00016411
Iteration 78/1000 | Loss: 0.00014680
Iteration 79/1000 | Loss: 0.00005621
Iteration 80/1000 | Loss: 0.00002396
Iteration 81/1000 | Loss: 0.00005202
Iteration 82/1000 | Loss: 0.00003517
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00006153
Iteration 85/1000 | Loss: 0.00024782
Iteration 86/1000 | Loss: 0.00006073
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00008286
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00023466
Iteration 91/1000 | Loss: 0.00002883
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00005947
Iteration 94/1000 | Loss: 0.00059991
Iteration 95/1000 | Loss: 0.00004654
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00006360
Iteration 98/1000 | Loss: 0.00005474
Iteration 99/1000 | Loss: 0.00002485
Iteration 100/1000 | Loss: 0.00004022
Iteration 101/1000 | Loss: 0.00017450
Iteration 102/1000 | Loss: 0.00004661
Iteration 103/1000 | Loss: 0.00002513
Iteration 104/1000 | Loss: 0.00005647
Iteration 105/1000 | Loss: 0.00017010
Iteration 106/1000 | Loss: 0.00005655
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00003365
Iteration 109/1000 | Loss: 0.00002949
Iteration 110/1000 | Loss: 0.00005835
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00008654
Iteration 114/1000 | Loss: 0.00001964
Iteration 115/1000 | Loss: 0.00013701
Iteration 116/1000 | Loss: 0.00004826
Iteration 117/1000 | Loss: 0.00001589
Iteration 118/1000 | Loss: 0.00011118
Iteration 119/1000 | Loss: 0.00002080
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00005099
Iteration 124/1000 | Loss: 0.00003558
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001418
Iteration 128/1000 | Loss: 0.00001418
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001418
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001417
Iteration 139/1000 | Loss: 0.00001417
Iteration 140/1000 | Loss: 0.00001417
Iteration 141/1000 | Loss: 0.00001417
Iteration 142/1000 | Loss: 0.00007825
Iteration 143/1000 | Loss: 0.00051077
Iteration 144/1000 | Loss: 0.00004149
Iteration 145/1000 | Loss: 0.00008271
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00014182
Iteration 148/1000 | Loss: 0.00002019
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001424
Iteration 153/1000 | Loss: 0.00007226
Iteration 154/1000 | Loss: 0.00006445
Iteration 155/1000 | Loss: 0.00002715
Iteration 156/1000 | Loss: 0.00001429
Iteration 157/1000 | Loss: 0.00005525
Iteration 158/1000 | Loss: 0.00002356
Iteration 159/1000 | Loss: 0.00005570
Iteration 160/1000 | Loss: 0.00004018
Iteration 161/1000 | Loss: 0.00003630
Iteration 162/1000 | Loss: 0.00052991
Iteration 163/1000 | Loss: 0.00012379
Iteration 164/1000 | Loss: 0.00005493
Iteration 165/1000 | Loss: 0.00005106
Iteration 166/1000 | Loss: 0.00003184
Iteration 167/1000 | Loss: 0.00011673
Iteration 168/1000 | Loss: 0.00003837
Iteration 169/1000 | Loss: 0.00006380
Iteration 170/1000 | Loss: 0.00001844
Iteration 171/1000 | Loss: 0.00002130
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00003692
Iteration 174/1000 | Loss: 0.00002539
Iteration 175/1000 | Loss: 0.00004851
Iteration 176/1000 | Loss: 0.00001772
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00002964
Iteration 179/1000 | Loss: 0.00001558
Iteration 180/1000 | Loss: 0.00001412
Iteration 181/1000 | Loss: 0.00001412
Iteration 182/1000 | Loss: 0.00001412
Iteration 183/1000 | Loss: 0.00001412
Iteration 184/1000 | Loss: 0.00001412
Iteration 185/1000 | Loss: 0.00001412
Iteration 186/1000 | Loss: 0.00001412
Iteration 187/1000 | Loss: 0.00001412
Iteration 188/1000 | Loss: 0.00001412
Iteration 189/1000 | Loss: 0.00001412
Iteration 190/1000 | Loss: 0.00001412
Iteration 191/1000 | Loss: 0.00001412
Iteration 192/1000 | Loss: 0.00001410
Iteration 193/1000 | Loss: 0.00001410
Iteration 194/1000 | Loss: 0.00001409
Iteration 195/1000 | Loss: 0.00001409
Iteration 196/1000 | Loss: 0.00001409
Iteration 197/1000 | Loss: 0.00003544
Iteration 198/1000 | Loss: 0.00015360
Iteration 199/1000 | Loss: 0.00002088
Iteration 200/1000 | Loss: 0.00003838
Iteration 201/1000 | Loss: 0.00001416
Iteration 202/1000 | Loss: 0.00001410
Iteration 203/1000 | Loss: 0.00001409
Iteration 204/1000 | Loss: 0.00001409
Iteration 205/1000 | Loss: 0.00001409
Iteration 206/1000 | Loss: 0.00001409
Iteration 207/1000 | Loss: 0.00001409
Iteration 208/1000 | Loss: 0.00001409
Iteration 209/1000 | Loss: 0.00001409
Iteration 210/1000 | Loss: 0.00001409
Iteration 211/1000 | Loss: 0.00001409
Iteration 212/1000 | Loss: 0.00001409
Iteration 213/1000 | Loss: 0.00001409
Iteration 214/1000 | Loss: 0.00001408
Iteration 215/1000 | Loss: 0.00001408
Iteration 216/1000 | Loss: 0.00001408
Iteration 217/1000 | Loss: 0.00001407
Iteration 218/1000 | Loss: 0.00001407
Iteration 219/1000 | Loss: 0.00001407
Iteration 220/1000 | Loss: 0.00001407
Iteration 221/1000 | Loss: 0.00001407
Iteration 222/1000 | Loss: 0.00001407
Iteration 223/1000 | Loss: 0.00001407
Iteration 224/1000 | Loss: 0.00001407
Iteration 225/1000 | Loss: 0.00001406
Iteration 226/1000 | Loss: 0.00001406
Iteration 227/1000 | Loss: 0.00001406
Iteration 228/1000 | Loss: 0.00004926
Iteration 229/1000 | Loss: 0.00002130
Iteration 230/1000 | Loss: 0.00001408
Iteration 231/1000 | Loss: 0.00001407
Iteration 232/1000 | Loss: 0.00001407
Iteration 233/1000 | Loss: 0.00001406
Iteration 234/1000 | Loss: 0.00001406
Iteration 235/1000 | Loss: 0.00001406
Iteration 236/1000 | Loss: 0.00001406
Iteration 237/1000 | Loss: 0.00001406
Iteration 238/1000 | Loss: 0.00001405
Iteration 239/1000 | Loss: 0.00001405
Iteration 240/1000 | Loss: 0.00001405
Iteration 241/1000 | Loss: 0.00001405
Iteration 242/1000 | Loss: 0.00001405
Iteration 243/1000 | Loss: 0.00001405
Iteration 244/1000 | Loss: 0.00001405
Iteration 245/1000 | Loss: 0.00001405
Iteration 246/1000 | Loss: 0.00001405
Iteration 247/1000 | Loss: 0.00001405
Iteration 248/1000 | Loss: 0.00001404
Iteration 249/1000 | Loss: 0.00002834
Iteration 250/1000 | Loss: 0.00001406
Iteration 251/1000 | Loss: 0.00001405
Iteration 252/1000 | Loss: 0.00001404
Iteration 253/1000 | Loss: 0.00001404
Iteration 254/1000 | Loss: 0.00001404
Iteration 255/1000 | Loss: 0.00001404
Iteration 256/1000 | Loss: 0.00001403
Iteration 257/1000 | Loss: 0.00001403
Iteration 258/1000 | Loss: 0.00001403
Iteration 259/1000 | Loss: 0.00001403
Iteration 260/1000 | Loss: 0.00001403
Iteration 261/1000 | Loss: 0.00001403
Iteration 262/1000 | Loss: 0.00001403
Iteration 263/1000 | Loss: 0.00001403
Iteration 264/1000 | Loss: 0.00001403
Iteration 265/1000 | Loss: 0.00001403
Iteration 266/1000 | Loss: 0.00001403
Iteration 267/1000 | Loss: 0.00001403
Iteration 268/1000 | Loss: 0.00001403
Iteration 269/1000 | Loss: 0.00001403
Iteration 270/1000 | Loss: 0.00001403
Iteration 271/1000 | Loss: 0.00001403
Iteration 272/1000 | Loss: 0.00001403
Iteration 273/1000 | Loss: 0.00001403
Iteration 274/1000 | Loss: 0.00001403
Iteration 275/1000 | Loss: 0.00001402
Iteration 276/1000 | Loss: 0.00001402
Iteration 277/1000 | Loss: 0.00001402
Iteration 278/1000 | Loss: 0.00001402
Iteration 279/1000 | Loss: 0.00001402
Iteration 280/1000 | Loss: 0.00001402
Iteration 281/1000 | Loss: 0.00001402
Iteration 282/1000 | Loss: 0.00001402
Iteration 283/1000 | Loss: 0.00001402
Iteration 284/1000 | Loss: 0.00001402
Iteration 285/1000 | Loss: 0.00001402
Iteration 286/1000 | Loss: 0.00001402
Iteration 287/1000 | Loss: 0.00001402
Iteration 288/1000 | Loss: 0.00001402
Iteration 289/1000 | Loss: 0.00001402
Iteration 290/1000 | Loss: 0.00001402
Iteration 291/1000 | Loss: 0.00001402
Iteration 292/1000 | Loss: 0.00001402
Iteration 293/1000 | Loss: 0.00001402
Iteration 294/1000 | Loss: 0.00001402
Iteration 295/1000 | Loss: 0.00001402
Iteration 296/1000 | Loss: 0.00001402
Iteration 297/1000 | Loss: 0.00001402
Iteration 298/1000 | Loss: 0.00001402
Iteration 299/1000 | Loss: 0.00001402
Iteration 300/1000 | Loss: 0.00001402
Iteration 301/1000 | Loss: 0.00001401
Iteration 302/1000 | Loss: 0.00001401
Iteration 303/1000 | Loss: 0.00001401
Iteration 304/1000 | Loss: 0.00001401
Iteration 305/1000 | Loss: 0.00001401
Iteration 306/1000 | Loss: 0.00001401
Iteration 307/1000 | Loss: 0.00007100
Iteration 308/1000 | Loss: 0.00015301
Iteration 309/1000 | Loss: 0.00015224
Iteration 310/1000 | Loss: 0.00003621
Iteration 311/1000 | Loss: 0.00003133
Iteration 312/1000 | Loss: 0.00011025
Iteration 313/1000 | Loss: 0.00006045
Iteration 314/1000 | Loss: 0.00001660
Iteration 315/1000 | Loss: 0.00008151
Iteration 316/1000 | Loss: 0.00001593
Iteration 317/1000 | Loss: 0.00005731
Iteration 318/1000 | Loss: 0.00006691
Iteration 319/1000 | Loss: 0.00001415
Iteration 320/1000 | Loss: 0.00002160
Iteration 321/1000 | Loss: 0.00001409
Iteration 322/1000 | Loss: 0.00001407
Iteration 323/1000 | Loss: 0.00001407
Iteration 324/1000 | Loss: 0.00002838
Iteration 325/1000 | Loss: 0.00001409
Iteration 326/1000 | Loss: 0.00001408
Iteration 327/1000 | Loss: 0.00001408
Iteration 328/1000 | Loss: 0.00001406
Iteration 329/1000 | Loss: 0.00001406
Iteration 330/1000 | Loss: 0.00001406
Iteration 331/1000 | Loss: 0.00001405
Iteration 332/1000 | Loss: 0.00001405
Iteration 333/1000 | Loss: 0.00001405
Iteration 334/1000 | Loss: 0.00001405
Iteration 335/1000 | Loss: 0.00001405
Iteration 336/1000 | Loss: 0.00001405
Iteration 337/1000 | Loss: 0.00001405
Iteration 338/1000 | Loss: 0.00001405
Iteration 339/1000 | Loss: 0.00002444
Iteration 340/1000 | Loss: 0.00001407
Iteration 341/1000 | Loss: 0.00001407
Iteration 342/1000 | Loss: 0.00004009
Iteration 343/1000 | Loss: 0.00007986
Iteration 344/1000 | Loss: 0.00004598
Iteration 345/1000 | Loss: 0.00001658
Iteration 346/1000 | Loss: 0.00003973
Iteration 347/1000 | Loss: 0.00001413
Iteration 348/1000 | Loss: 0.00003824
Iteration 349/1000 | Loss: 0.00003286
Iteration 350/1000 | Loss: 0.00003793
Iteration 351/1000 | Loss: 0.00002884
Iteration 352/1000 | Loss: 0.00001529
Iteration 353/1000 | Loss: 0.00008806
Iteration 354/1000 | Loss: 0.00006132
Iteration 355/1000 | Loss: 0.00004030
Iteration 356/1000 | Loss: 0.00001408
Iteration 357/1000 | Loss: 0.00004519
Iteration 358/1000 | Loss: 0.00004353
Iteration 359/1000 | Loss: 0.00003795
Iteration 360/1000 | Loss: 0.00004764
Iteration 361/1000 | Loss: 0.00001523
Iteration 362/1000 | Loss: 0.00001407
Iteration 363/1000 | Loss: 0.00001407
Iteration 364/1000 | Loss: 0.00001407
Iteration 365/1000 | Loss: 0.00001407
Iteration 366/1000 | Loss: 0.00001406
Iteration 367/1000 | Loss: 0.00001406
Iteration 368/1000 | Loss: 0.00001406
Iteration 369/1000 | Loss: 0.00001406
Iteration 370/1000 | Loss: 0.00001406
Iteration 371/1000 | Loss: 0.00001406
Iteration 372/1000 | Loss: 0.00001406
Iteration 373/1000 | Loss: 0.00001406
Iteration 374/1000 | Loss: 0.00001406
Iteration 375/1000 | Loss: 0.00001406
Iteration 376/1000 | Loss: 0.00001406
Iteration 377/1000 | Loss: 0.00001406
Iteration 378/1000 | Loss: 0.00008298
Iteration 379/1000 | Loss: 0.00004807
Iteration 380/1000 | Loss: 0.00001867
Iteration 381/1000 | Loss: 0.00001406
Iteration 382/1000 | Loss: 0.00001406
Iteration 383/1000 | Loss: 0.00001406
Iteration 384/1000 | Loss: 0.00001406
Iteration 385/1000 | Loss: 0.00001406
Iteration 386/1000 | Loss: 0.00001406
Iteration 387/1000 | Loss: 0.00001406
Iteration 388/1000 | Loss: 0.00001406
Iteration 389/1000 | Loss: 0.00001406
Iteration 390/1000 | Loss: 0.00001406
Iteration 391/1000 | Loss: 0.00001406
Iteration 392/1000 | Loss: 0.00001406
Iteration 393/1000 | Loss: 0.00001405
Iteration 394/1000 | Loss: 0.00001405
Iteration 395/1000 | Loss: 0.00001405
Iteration 396/1000 | Loss: 0.00003267
Iteration 397/1000 | Loss: 0.00002393
Iteration 398/1000 | Loss: 0.00001440
Iteration 399/1000 | Loss: 0.00003528
Iteration 400/1000 | Loss: 0.00001635
Iteration 401/1000 | Loss: 0.00001406
Iteration 402/1000 | Loss: 0.00001406
Iteration 403/1000 | Loss: 0.00001406
Iteration 404/1000 | Loss: 0.00001406
Iteration 405/1000 | Loss: 0.00001406
Iteration 406/1000 | Loss: 0.00001406
Iteration 407/1000 | Loss: 0.00001405
Iteration 408/1000 | Loss: 0.00001405
Iteration 409/1000 | Loss: 0.00001405
Iteration 410/1000 | Loss: 0.00001405
Iteration 411/1000 | Loss: 0.00001405
Iteration 412/1000 | Loss: 0.00001405
Iteration 413/1000 | Loss: 0.00001405
Iteration 414/1000 | Loss: 0.00001405
Iteration 415/1000 | Loss: 0.00001405
Iteration 416/1000 | Loss: 0.00001405
Iteration 417/1000 | Loss: 0.00001405
Iteration 418/1000 | Loss: 0.00001405
Iteration 419/1000 | Loss: 0.00001405
Iteration 420/1000 | Loss: 0.00001404
Iteration 421/1000 | Loss: 0.00001404
Iteration 422/1000 | Loss: 0.00001404
Iteration 423/1000 | Loss: 0.00001404
Iteration 424/1000 | Loss: 0.00001404
Iteration 425/1000 | Loss: 0.00001404
Iteration 426/1000 | Loss: 0.00001404
Iteration 427/1000 | Loss: 0.00001404
Iteration 428/1000 | Loss: 0.00001404
Iteration 429/1000 | Loss: 0.00001404
Iteration 430/1000 | Loss: 0.00001403
Iteration 431/1000 | Loss: 0.00001403
Iteration 432/1000 | Loss: 0.00001403
Iteration 433/1000 | Loss: 0.00001402
Iteration 434/1000 | Loss: 0.00001402
Iteration 435/1000 | Loss: 0.00001401
Iteration 436/1000 | Loss: 0.00001401
Iteration 437/1000 | Loss: 0.00001401
Iteration 438/1000 | Loss: 0.00001401
Iteration 439/1000 | Loss: 0.00001401
Iteration 440/1000 | Loss: 0.00001401
Iteration 441/1000 | Loss: 0.00001401
Iteration 442/1000 | Loss: 0.00001401
Iteration 443/1000 | Loss: 0.00001401
Iteration 444/1000 | Loss: 0.00001401
Iteration 445/1000 | Loss: 0.00001401
Iteration 446/1000 | Loss: 0.00001401
Iteration 447/1000 | Loss: 0.00001400
Iteration 448/1000 | Loss: 0.00001400
Iteration 449/1000 | Loss: 0.00001400
Iteration 450/1000 | Loss: 0.00001400
Iteration 451/1000 | Loss: 0.00001400
Iteration 452/1000 | Loss: 0.00001400
Iteration 453/1000 | Loss: 0.00001400
Iteration 454/1000 | Loss: 0.00001400
Iteration 455/1000 | Loss: 0.00001400
Iteration 456/1000 | Loss: 0.00001400
Iteration 457/1000 | Loss: 0.00001400
Iteration 458/1000 | Loss: 0.00001400
Iteration 459/1000 | Loss: 0.00001400
Iteration 460/1000 | Loss: 0.00001400
Iteration 461/1000 | Loss: 0.00001400
Iteration 462/1000 | Loss: 0.00001400
Iteration 463/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 463. Stopping optimization.
Last 5 losses: [1.3998221220390406e-05, 1.3998221220390406e-05, 1.3998221220390406e-05, 1.3998221220390406e-05, 1.3998221220390406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3998221220390406e-05

Optimization complete. Final v2v error: 3.1952645778656006 mm

Highest mean error: 3.5838308334350586 mm for frame 2

Lowest mean error: 2.9737911224365234 mm for frame 108

Saving results

Total time: 293.9349114894867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011324
Iteration 2/25 | Loss: 0.00157933
Iteration 3/25 | Loss: 0.00130042
Iteration 4/25 | Loss: 0.00124832
Iteration 5/25 | Loss: 0.00123179
Iteration 6/25 | Loss: 0.00122717
Iteration 7/25 | Loss: 0.00122645
Iteration 8/25 | Loss: 0.00122645
Iteration 9/25 | Loss: 0.00122645
Iteration 10/25 | Loss: 0.00122645
Iteration 11/25 | Loss: 0.00122645
Iteration 12/25 | Loss: 0.00122645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012264462420716882, 0.0012264462420716882, 0.0012264462420716882, 0.0012264462420716882, 0.0012264462420716882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012264462420716882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90939814
Iteration 2/25 | Loss: 0.00093236
Iteration 3/25 | Loss: 0.00093226
Iteration 4/25 | Loss: 0.00093225
Iteration 5/25 | Loss: 0.00093225
Iteration 6/25 | Loss: 0.00093225
Iteration 7/25 | Loss: 0.00093225
Iteration 8/25 | Loss: 0.00093225
Iteration 9/25 | Loss: 0.00093225
Iteration 10/25 | Loss: 0.00093225
Iteration 11/25 | Loss: 0.00093225
Iteration 12/25 | Loss: 0.00093225
Iteration 13/25 | Loss: 0.00093225
Iteration 14/25 | Loss: 0.00093225
Iteration 15/25 | Loss: 0.00093225
Iteration 16/25 | Loss: 0.00093225
Iteration 17/25 | Loss: 0.00093225
Iteration 18/25 | Loss: 0.00093225
Iteration 19/25 | Loss: 0.00093225
Iteration 20/25 | Loss: 0.00093225
Iteration 21/25 | Loss: 0.00093225
Iteration 22/25 | Loss: 0.00093225
Iteration 23/25 | Loss: 0.00093225
Iteration 24/25 | Loss: 0.00093225
Iteration 25/25 | Loss: 0.00093225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093225
Iteration 2/1000 | Loss: 0.00008205
Iteration 3/1000 | Loss: 0.00006160
Iteration 4/1000 | Loss: 0.00005371
Iteration 5/1000 | Loss: 0.00005027
Iteration 6/1000 | Loss: 0.00004851
Iteration 7/1000 | Loss: 0.00004716
Iteration 8/1000 | Loss: 0.00004605
Iteration 9/1000 | Loss: 0.00004521
Iteration 10/1000 | Loss: 0.00004433
Iteration 11/1000 | Loss: 0.00004364
Iteration 12/1000 | Loss: 0.00004312
Iteration 13/1000 | Loss: 0.00004269
Iteration 14/1000 | Loss: 0.00004236
Iteration 15/1000 | Loss: 0.00004202
Iteration 16/1000 | Loss: 0.00004180
Iteration 17/1000 | Loss: 0.00004163
Iteration 18/1000 | Loss: 0.00004147
Iteration 19/1000 | Loss: 0.00004130
Iteration 20/1000 | Loss: 0.00004126
Iteration 21/1000 | Loss: 0.00004119
Iteration 22/1000 | Loss: 0.00004112
Iteration 23/1000 | Loss: 0.00004103
Iteration 24/1000 | Loss: 0.00004099
Iteration 25/1000 | Loss: 0.00004099
Iteration 26/1000 | Loss: 0.00004094
Iteration 27/1000 | Loss: 0.00004092
Iteration 28/1000 | Loss: 0.00004091
Iteration 29/1000 | Loss: 0.00004091
Iteration 30/1000 | Loss: 0.00004090
Iteration 31/1000 | Loss: 0.00004090
Iteration 32/1000 | Loss: 0.00004089
Iteration 33/1000 | Loss: 0.00004088
Iteration 34/1000 | Loss: 0.00004088
Iteration 35/1000 | Loss: 0.00004088
Iteration 36/1000 | Loss: 0.00004087
Iteration 37/1000 | Loss: 0.00004087
Iteration 38/1000 | Loss: 0.00004087
Iteration 39/1000 | Loss: 0.00004087
Iteration 40/1000 | Loss: 0.00004087
Iteration 41/1000 | Loss: 0.00004087
Iteration 42/1000 | Loss: 0.00004087
Iteration 43/1000 | Loss: 0.00004086
Iteration 44/1000 | Loss: 0.00004086
Iteration 45/1000 | Loss: 0.00004085
Iteration 46/1000 | Loss: 0.00004085
Iteration 47/1000 | Loss: 0.00004085
Iteration 48/1000 | Loss: 0.00004084
Iteration 49/1000 | Loss: 0.00004084
Iteration 50/1000 | Loss: 0.00004084
Iteration 51/1000 | Loss: 0.00004084
Iteration 52/1000 | Loss: 0.00004083
Iteration 53/1000 | Loss: 0.00004083
Iteration 54/1000 | Loss: 0.00004083
Iteration 55/1000 | Loss: 0.00004082
Iteration 56/1000 | Loss: 0.00004082
Iteration 57/1000 | Loss: 0.00004081
Iteration 58/1000 | Loss: 0.00004081
Iteration 59/1000 | Loss: 0.00004081
Iteration 60/1000 | Loss: 0.00004080
Iteration 61/1000 | Loss: 0.00004080
Iteration 62/1000 | Loss: 0.00004080
Iteration 63/1000 | Loss: 0.00004079
Iteration 64/1000 | Loss: 0.00004079
Iteration 65/1000 | Loss: 0.00004079
Iteration 66/1000 | Loss: 0.00004078
Iteration 67/1000 | Loss: 0.00004078
Iteration 68/1000 | Loss: 0.00004078
Iteration 69/1000 | Loss: 0.00004077
Iteration 70/1000 | Loss: 0.00004077
Iteration 71/1000 | Loss: 0.00004076
Iteration 72/1000 | Loss: 0.00004076
Iteration 73/1000 | Loss: 0.00004076
Iteration 74/1000 | Loss: 0.00004075
Iteration 75/1000 | Loss: 0.00004075
Iteration 76/1000 | Loss: 0.00004074
Iteration 77/1000 | Loss: 0.00004074
Iteration 78/1000 | Loss: 0.00004074
Iteration 79/1000 | Loss: 0.00004074
Iteration 80/1000 | Loss: 0.00004073
Iteration 81/1000 | Loss: 0.00004073
Iteration 82/1000 | Loss: 0.00004073
Iteration 83/1000 | Loss: 0.00004072
Iteration 84/1000 | Loss: 0.00004071
Iteration 85/1000 | Loss: 0.00004071
Iteration 86/1000 | Loss: 0.00004070
Iteration 87/1000 | Loss: 0.00004070
Iteration 88/1000 | Loss: 0.00004069
Iteration 89/1000 | Loss: 0.00004069
Iteration 90/1000 | Loss: 0.00004069
Iteration 91/1000 | Loss: 0.00004068
Iteration 92/1000 | Loss: 0.00004068
Iteration 93/1000 | Loss: 0.00004068
Iteration 94/1000 | Loss: 0.00004068
Iteration 95/1000 | Loss: 0.00004068
Iteration 96/1000 | Loss: 0.00004067
Iteration 97/1000 | Loss: 0.00004067
Iteration 98/1000 | Loss: 0.00004067
Iteration 99/1000 | Loss: 0.00004067
Iteration 100/1000 | Loss: 0.00004067
Iteration 101/1000 | Loss: 0.00004067
Iteration 102/1000 | Loss: 0.00004066
Iteration 103/1000 | Loss: 0.00004066
Iteration 104/1000 | Loss: 0.00004066
Iteration 105/1000 | Loss: 0.00004066
Iteration 106/1000 | Loss: 0.00004065
Iteration 107/1000 | Loss: 0.00004065
Iteration 108/1000 | Loss: 0.00004065
Iteration 109/1000 | Loss: 0.00004065
Iteration 110/1000 | Loss: 0.00004065
Iteration 111/1000 | Loss: 0.00004065
Iteration 112/1000 | Loss: 0.00004065
Iteration 113/1000 | Loss: 0.00004065
Iteration 114/1000 | Loss: 0.00004065
Iteration 115/1000 | Loss: 0.00004064
Iteration 116/1000 | Loss: 0.00004064
Iteration 117/1000 | Loss: 0.00004064
Iteration 118/1000 | Loss: 0.00004064
Iteration 119/1000 | Loss: 0.00004064
Iteration 120/1000 | Loss: 0.00004064
Iteration 121/1000 | Loss: 0.00004063
Iteration 122/1000 | Loss: 0.00004063
Iteration 123/1000 | Loss: 0.00004063
Iteration 124/1000 | Loss: 0.00004063
Iteration 125/1000 | Loss: 0.00004063
Iteration 126/1000 | Loss: 0.00004062
Iteration 127/1000 | Loss: 0.00004062
Iteration 128/1000 | Loss: 0.00004062
Iteration 129/1000 | Loss: 0.00004062
Iteration 130/1000 | Loss: 0.00004062
Iteration 131/1000 | Loss: 0.00004062
Iteration 132/1000 | Loss: 0.00004062
Iteration 133/1000 | Loss: 0.00004062
Iteration 134/1000 | Loss: 0.00004062
Iteration 135/1000 | Loss: 0.00004062
Iteration 136/1000 | Loss: 0.00004062
Iteration 137/1000 | Loss: 0.00004061
Iteration 138/1000 | Loss: 0.00004061
Iteration 139/1000 | Loss: 0.00004061
Iteration 140/1000 | Loss: 0.00004061
Iteration 141/1000 | Loss: 0.00004061
Iteration 142/1000 | Loss: 0.00004061
Iteration 143/1000 | Loss: 0.00004061
Iteration 144/1000 | Loss: 0.00004061
Iteration 145/1000 | Loss: 0.00004061
Iteration 146/1000 | Loss: 0.00004061
Iteration 147/1000 | Loss: 0.00004061
Iteration 148/1000 | Loss: 0.00004060
Iteration 149/1000 | Loss: 0.00004060
Iteration 150/1000 | Loss: 0.00004060
Iteration 151/1000 | Loss: 0.00004060
Iteration 152/1000 | Loss: 0.00004060
Iteration 153/1000 | Loss: 0.00004060
Iteration 154/1000 | Loss: 0.00004060
Iteration 155/1000 | Loss: 0.00004059
Iteration 156/1000 | Loss: 0.00004059
Iteration 157/1000 | Loss: 0.00004059
Iteration 158/1000 | Loss: 0.00004058
Iteration 159/1000 | Loss: 0.00004058
Iteration 160/1000 | Loss: 0.00004058
Iteration 161/1000 | Loss: 0.00004058
Iteration 162/1000 | Loss: 0.00004058
Iteration 163/1000 | Loss: 0.00004058
Iteration 164/1000 | Loss: 0.00004057
Iteration 165/1000 | Loss: 0.00004057
Iteration 166/1000 | Loss: 0.00004057
Iteration 167/1000 | Loss: 0.00004057
Iteration 168/1000 | Loss: 0.00004057
Iteration 169/1000 | Loss: 0.00004056
Iteration 170/1000 | Loss: 0.00004056
Iteration 171/1000 | Loss: 0.00004056
Iteration 172/1000 | Loss: 0.00004056
Iteration 173/1000 | Loss: 0.00004056
Iteration 174/1000 | Loss: 0.00004056
Iteration 175/1000 | Loss: 0.00004056
Iteration 176/1000 | Loss: 0.00004056
Iteration 177/1000 | Loss: 0.00004056
Iteration 178/1000 | Loss: 0.00004056
Iteration 179/1000 | Loss: 0.00004056
Iteration 180/1000 | Loss: 0.00004056
Iteration 181/1000 | Loss: 0.00004056
Iteration 182/1000 | Loss: 0.00004056
Iteration 183/1000 | Loss: 0.00004056
Iteration 184/1000 | Loss: 0.00004056
Iteration 185/1000 | Loss: 0.00004056
Iteration 186/1000 | Loss: 0.00004056
Iteration 187/1000 | Loss: 0.00004056
Iteration 188/1000 | Loss: 0.00004056
Iteration 189/1000 | Loss: 0.00004056
Iteration 190/1000 | Loss: 0.00004056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [4.056029865751043e-05, 4.056029865751043e-05, 4.056029865751043e-05, 4.056029865751043e-05, 4.056029865751043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.056029865751043e-05

Optimization complete. Final v2v error: 5.202341079711914 mm

Highest mean error: 5.713601589202881 mm for frame 181

Lowest mean error: 4.693075656890869 mm for frame 150

Saving results

Total time: 61.3041832447052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786309
Iteration 2/25 | Loss: 0.00130309
Iteration 3/25 | Loss: 0.00117949
Iteration 4/25 | Loss: 0.00116103
Iteration 5/25 | Loss: 0.00115277
Iteration 6/25 | Loss: 0.00115030
Iteration 7/25 | Loss: 0.00115022
Iteration 8/25 | Loss: 0.00115022
Iteration 9/25 | Loss: 0.00115022
Iteration 10/25 | Loss: 0.00115022
Iteration 11/25 | Loss: 0.00115022
Iteration 12/25 | Loss: 0.00115022
Iteration 13/25 | Loss: 0.00115022
Iteration 14/25 | Loss: 0.00115022
Iteration 15/25 | Loss: 0.00115022
Iteration 16/25 | Loss: 0.00115022
Iteration 17/25 | Loss: 0.00115022
Iteration 18/25 | Loss: 0.00115022
Iteration 19/25 | Loss: 0.00115022
Iteration 20/25 | Loss: 0.00115022
Iteration 21/25 | Loss: 0.00115022
Iteration 22/25 | Loss: 0.00115022
Iteration 23/25 | Loss: 0.00115022
Iteration 24/25 | Loss: 0.00115022
Iteration 25/25 | Loss: 0.00115022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25127316
Iteration 2/25 | Loss: 0.00091802
Iteration 3/25 | Loss: 0.00091802
Iteration 4/25 | Loss: 0.00091802
Iteration 5/25 | Loss: 0.00091802
Iteration 6/25 | Loss: 0.00091802
Iteration 7/25 | Loss: 0.00091802
Iteration 8/25 | Loss: 0.00091802
Iteration 9/25 | Loss: 0.00091802
Iteration 10/25 | Loss: 0.00091802
Iteration 11/25 | Loss: 0.00091802
Iteration 12/25 | Loss: 0.00091802
Iteration 13/25 | Loss: 0.00091802
Iteration 14/25 | Loss: 0.00091802
Iteration 15/25 | Loss: 0.00091802
Iteration 16/25 | Loss: 0.00091802
Iteration 17/25 | Loss: 0.00091802
Iteration 18/25 | Loss: 0.00091802
Iteration 19/25 | Loss: 0.00091802
Iteration 20/25 | Loss: 0.00091802
Iteration 21/25 | Loss: 0.00091802
Iteration 22/25 | Loss: 0.00091802
Iteration 23/25 | Loss: 0.00091802
Iteration 24/25 | Loss: 0.00091802
Iteration 25/25 | Loss: 0.00091802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091802
Iteration 2/1000 | Loss: 0.00003329
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001859
Iteration 6/1000 | Loss: 0.00001811
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001712
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001631
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Iteration 115/1000 | Loss: 0.00001617
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001614
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001612
Iteration 143/1000 | Loss: 0.00001612
Iteration 144/1000 | Loss: 0.00001612
Iteration 145/1000 | Loss: 0.00001612
Iteration 146/1000 | Loss: 0.00001612
Iteration 147/1000 | Loss: 0.00001612
Iteration 148/1000 | Loss: 0.00001612
Iteration 149/1000 | Loss: 0.00001612
Iteration 150/1000 | Loss: 0.00001612
Iteration 151/1000 | Loss: 0.00001612
Iteration 152/1000 | Loss: 0.00001612
Iteration 153/1000 | Loss: 0.00001612
Iteration 154/1000 | Loss: 0.00001612
Iteration 155/1000 | Loss: 0.00001612
Iteration 156/1000 | Loss: 0.00001612
Iteration 157/1000 | Loss: 0.00001612
Iteration 158/1000 | Loss: 0.00001612
Iteration 159/1000 | Loss: 0.00001612
Iteration 160/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6119112842716277e-05, 1.6119112842716277e-05, 1.6119112842716277e-05, 1.6119112842716277e-05, 1.6119112842716277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6119112842716277e-05

Optimization complete. Final v2v error: 3.3691132068634033 mm

Highest mean error: 3.5302202701568604 mm for frame 122

Lowest mean error: 3.2217087745666504 mm for frame 40

Saving results

Total time: 36.775439500808716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827477
Iteration 2/25 | Loss: 0.00138643
Iteration 3/25 | Loss: 0.00120234
Iteration 4/25 | Loss: 0.00115476
Iteration 5/25 | Loss: 0.00114575
Iteration 6/25 | Loss: 0.00114455
Iteration 7/25 | Loss: 0.00114451
Iteration 8/25 | Loss: 0.00114451
Iteration 9/25 | Loss: 0.00114451
Iteration 10/25 | Loss: 0.00114451
Iteration 11/25 | Loss: 0.00114451
Iteration 12/25 | Loss: 0.00114451
Iteration 13/25 | Loss: 0.00114451
Iteration 14/25 | Loss: 0.00114451
Iteration 15/25 | Loss: 0.00114451
Iteration 16/25 | Loss: 0.00114451
Iteration 17/25 | Loss: 0.00114451
Iteration 18/25 | Loss: 0.00114451
Iteration 19/25 | Loss: 0.00114451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011445119744166732, 0.0011445119744166732, 0.0011445119744166732, 0.0011445119744166732, 0.0011445119744166732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011445119744166732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32004225
Iteration 2/25 | Loss: 0.00086597
Iteration 3/25 | Loss: 0.00086597
Iteration 4/25 | Loss: 0.00086596
Iteration 5/25 | Loss: 0.00086596
Iteration 6/25 | Loss: 0.00086596
Iteration 7/25 | Loss: 0.00086596
Iteration 8/25 | Loss: 0.00086596
Iteration 9/25 | Loss: 0.00086596
Iteration 10/25 | Loss: 0.00086596
Iteration 11/25 | Loss: 0.00086596
Iteration 12/25 | Loss: 0.00086596
Iteration 13/25 | Loss: 0.00086596
Iteration 14/25 | Loss: 0.00086596
Iteration 15/25 | Loss: 0.00086596
Iteration 16/25 | Loss: 0.00086596
Iteration 17/25 | Loss: 0.00086596
Iteration 18/25 | Loss: 0.00086596
Iteration 19/25 | Loss: 0.00086596
Iteration 20/25 | Loss: 0.00086596
Iteration 21/25 | Loss: 0.00086596
Iteration 22/25 | Loss: 0.00086596
Iteration 23/25 | Loss: 0.00086596
Iteration 24/25 | Loss: 0.00086596
Iteration 25/25 | Loss: 0.00086596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086596
Iteration 2/1000 | Loss: 0.00004182
Iteration 3/1000 | Loss: 0.00002515
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002068
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001838
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001713
Iteration 83/1000 | Loss: 0.00001713
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001707
Iteration 115/1000 | Loss: 0.00001707
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001703
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001702
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001702
Iteration 147/1000 | Loss: 0.00001702
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001699
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001698
Iteration 176/1000 | Loss: 0.00001698
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001698
Iteration 179/1000 | Loss: 0.00001698
Iteration 180/1000 | Loss: 0.00001698
Iteration 181/1000 | Loss: 0.00001698
Iteration 182/1000 | Loss: 0.00001698
Iteration 183/1000 | Loss: 0.00001698
Iteration 184/1000 | Loss: 0.00001698
Iteration 185/1000 | Loss: 0.00001698
Iteration 186/1000 | Loss: 0.00001698
Iteration 187/1000 | Loss: 0.00001698
Iteration 188/1000 | Loss: 0.00001698
Iteration 189/1000 | Loss: 0.00001698
Iteration 190/1000 | Loss: 0.00001698
Iteration 191/1000 | Loss: 0.00001698
Iteration 192/1000 | Loss: 0.00001698
Iteration 193/1000 | Loss: 0.00001698
Iteration 194/1000 | Loss: 0.00001698
Iteration 195/1000 | Loss: 0.00001698
Iteration 196/1000 | Loss: 0.00001698
Iteration 197/1000 | Loss: 0.00001698
Iteration 198/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.6977015548036434e-05, 1.6977015548036434e-05, 1.6977015548036434e-05, 1.6977015548036434e-05, 1.6977015548036434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6977015548036434e-05

Optimization complete. Final v2v error: 3.5268592834472656 mm

Highest mean error: 3.817321300506592 mm for frame 144

Lowest mean error: 3.370908260345459 mm for frame 116

Saving results

Total time: 41.83072304725647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967624
Iteration 2/25 | Loss: 0.00296973
Iteration 3/25 | Loss: 0.00239096
Iteration 4/25 | Loss: 0.00230985
Iteration 5/25 | Loss: 0.00215634
Iteration 6/25 | Loss: 0.00191364
Iteration 7/25 | Loss: 0.00186621
Iteration 8/25 | Loss: 0.00189071
Iteration 9/25 | Loss: 0.00184206
Iteration 10/25 | Loss: 0.00181880
Iteration 11/25 | Loss: 0.00181240
Iteration 12/25 | Loss: 0.00181238
Iteration 13/25 | Loss: 0.00180103
Iteration 14/25 | Loss: 0.00180045
Iteration 15/25 | Loss: 0.00180615
Iteration 16/25 | Loss: 0.00179832
Iteration 17/25 | Loss: 0.00179786
Iteration 18/25 | Loss: 0.00179776
Iteration 19/25 | Loss: 0.00179776
Iteration 20/25 | Loss: 0.00179776
Iteration 21/25 | Loss: 0.00179776
Iteration 22/25 | Loss: 0.00179776
Iteration 23/25 | Loss: 0.00179776
Iteration 24/25 | Loss: 0.00179776
Iteration 25/25 | Loss: 0.00179776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30054927
Iteration 2/25 | Loss: 0.00747245
Iteration 3/25 | Loss: 0.00402297
Iteration 4/25 | Loss: 0.00402297
Iteration 5/25 | Loss: 0.00402297
Iteration 6/25 | Loss: 0.00402297
Iteration 7/25 | Loss: 0.00402297
Iteration 8/25 | Loss: 0.00402297
Iteration 9/25 | Loss: 0.00402297
Iteration 10/25 | Loss: 0.00402297
Iteration 11/25 | Loss: 0.00402297
Iteration 12/25 | Loss: 0.00402297
Iteration 13/25 | Loss: 0.00402297
Iteration 14/25 | Loss: 0.00402297
Iteration 15/25 | Loss: 0.00402297
Iteration 16/25 | Loss: 0.00402297
Iteration 17/25 | Loss: 0.00402297
Iteration 18/25 | Loss: 0.00402297
Iteration 19/25 | Loss: 0.00402297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004022965207695961, 0.004022965207695961, 0.004022965207695961, 0.004022965207695961, 0.004022965207695961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004022965207695961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00402297
Iteration 2/1000 | Loss: 0.00646799
Iteration 3/1000 | Loss: 0.00221740
Iteration 4/1000 | Loss: 0.00273703
Iteration 5/1000 | Loss: 0.00284834
Iteration 6/1000 | Loss: 0.00135455
Iteration 7/1000 | Loss: 0.00285818
Iteration 8/1000 | Loss: 0.00040130
Iteration 9/1000 | Loss: 0.00049680
Iteration 10/1000 | Loss: 0.00033454
Iteration 11/1000 | Loss: 0.00045208
Iteration 12/1000 | Loss: 0.00030838
Iteration 13/1000 | Loss: 0.00059391
Iteration 14/1000 | Loss: 0.01472421
Iteration 15/1000 | Loss: 0.00340995
Iteration 16/1000 | Loss: 0.00237484
Iteration 17/1000 | Loss: 0.00171919
Iteration 18/1000 | Loss: 0.00249547
Iteration 19/1000 | Loss: 0.00033936
Iteration 20/1000 | Loss: 0.00121894
Iteration 21/1000 | Loss: 0.00016808
Iteration 22/1000 | Loss: 0.00033655
Iteration 23/1000 | Loss: 0.00176406
Iteration 24/1000 | Loss: 0.00200438
Iteration 25/1000 | Loss: 0.00222040
Iteration 26/1000 | Loss: 0.00310843
Iteration 27/1000 | Loss: 0.00220370
Iteration 28/1000 | Loss: 0.00081987
Iteration 29/1000 | Loss: 0.00037016
Iteration 30/1000 | Loss: 0.00024582
Iteration 31/1000 | Loss: 0.00466815
Iteration 32/1000 | Loss: 0.00022499
Iteration 33/1000 | Loss: 0.00007554
Iteration 34/1000 | Loss: 0.00024531
Iteration 35/1000 | Loss: 0.00017868
Iteration 36/1000 | Loss: 0.00025625
Iteration 37/1000 | Loss: 0.00096404
Iteration 38/1000 | Loss: 0.00017768
Iteration 39/1000 | Loss: 0.00014084
Iteration 40/1000 | Loss: 0.00210532
Iteration 41/1000 | Loss: 0.00471050
Iteration 42/1000 | Loss: 0.00245934
Iteration 43/1000 | Loss: 0.00275285
Iteration 44/1000 | Loss: 0.00095024
Iteration 45/1000 | Loss: 0.00007737
Iteration 46/1000 | Loss: 0.00013159
Iteration 47/1000 | Loss: 0.00011166
Iteration 48/1000 | Loss: 0.00007057
Iteration 49/1000 | Loss: 0.00027524
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00002394
Iteration 52/1000 | Loss: 0.00012364
Iteration 53/1000 | Loss: 0.00009986
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00006455
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00005487
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00005339
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00015503
Iteration 64/1000 | Loss: 0.00016781
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00008110
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00014260
Iteration 69/1000 | Loss: 0.00008446
Iteration 70/1000 | Loss: 0.00012541
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00006173
Iteration 74/1000 | Loss: 0.00006172
Iteration 75/1000 | Loss: 0.00121234
Iteration 76/1000 | Loss: 0.00025052
Iteration 77/1000 | Loss: 0.00003202
Iteration 78/1000 | Loss: 0.00002564
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001268
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001253
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001253
Iteration 108/1000 | Loss: 0.00001253
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001249
Iteration 145/1000 | Loss: 0.00006191
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001249
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Iteration 163/1000 | Loss: 0.00001248
Iteration 164/1000 | Loss: 0.00001248
Iteration 165/1000 | Loss: 0.00001248
Iteration 166/1000 | Loss: 0.00001248
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Iteration 169/1000 | Loss: 0.00001248
Iteration 170/1000 | Loss: 0.00001248
Iteration 171/1000 | Loss: 0.00001248
Iteration 172/1000 | Loss: 0.00001248
Iteration 173/1000 | Loss: 0.00001248
Iteration 174/1000 | Loss: 0.00001248
Iteration 175/1000 | Loss: 0.00001248
Iteration 176/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.2484300896176137e-05, 1.2484300896176137e-05, 1.2484300896176137e-05, 1.2484300896176137e-05, 1.2484300896176137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2484300896176137e-05

Optimization complete. Final v2v error: 3.045409679412842 mm

Highest mean error: 3.7802178859710693 mm for frame 192

Lowest mean error: 2.8310019969940186 mm for frame 23

Saving results

Total time: 171.32324147224426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408044
Iteration 2/25 | Loss: 0.00120027
Iteration 3/25 | Loss: 0.00111895
Iteration 4/25 | Loss: 0.00110740
Iteration 5/25 | Loss: 0.00110431
Iteration 6/25 | Loss: 0.00110392
Iteration 7/25 | Loss: 0.00110392
Iteration 8/25 | Loss: 0.00110392
Iteration 9/25 | Loss: 0.00110392
Iteration 10/25 | Loss: 0.00110392
Iteration 11/25 | Loss: 0.00110392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011039209784939885, 0.0011039209784939885, 0.0011039209784939885, 0.0011039209784939885, 0.0011039209784939885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011039209784939885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46910262
Iteration 2/25 | Loss: 0.00085661
Iteration 3/25 | Loss: 0.00085658
Iteration 4/25 | Loss: 0.00085658
Iteration 5/25 | Loss: 0.00085658
Iteration 6/25 | Loss: 0.00085658
Iteration 7/25 | Loss: 0.00085658
Iteration 8/25 | Loss: 0.00085658
Iteration 9/25 | Loss: 0.00085658
Iteration 10/25 | Loss: 0.00085658
Iteration 11/25 | Loss: 0.00085658
Iteration 12/25 | Loss: 0.00085658
Iteration 13/25 | Loss: 0.00085658
Iteration 14/25 | Loss: 0.00085658
Iteration 15/25 | Loss: 0.00085658
Iteration 16/25 | Loss: 0.00085658
Iteration 17/25 | Loss: 0.00085658
Iteration 18/25 | Loss: 0.00085658
Iteration 19/25 | Loss: 0.00085658
Iteration 20/25 | Loss: 0.00085658
Iteration 21/25 | Loss: 0.00085658
Iteration 22/25 | Loss: 0.00085658
Iteration 23/25 | Loss: 0.00085658
Iteration 24/25 | Loss: 0.00085658
Iteration 25/25 | Loss: 0.00085658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085658
Iteration 2/1000 | Loss: 0.00002917
Iteration 3/1000 | Loss: 0.00002117
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001670
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001562
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001544
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001535
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001533
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001531
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001530
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001519
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001514
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001513
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001513
Iteration 110/1000 | Loss: 0.00001513
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001512
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.5116986105567776e-05, 1.5116986105567776e-05, 1.5116986105567776e-05, 1.5116986105567776e-05, 1.5116986105567776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5116986105567776e-05

Optimization complete. Final v2v error: 3.333066940307617 mm

Highest mean error: 3.4760541915893555 mm for frame 106

Lowest mean error: 3.1666183471679688 mm for frame 12

Saving results

Total time: 35.698206186294556
