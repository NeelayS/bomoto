Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=23, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1288-1343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958173
Iteration 2/25 | Loss: 0.00207933
Iteration 3/25 | Loss: 0.00184925
Iteration 4/25 | Loss: 0.00182142
Iteration 5/25 | Loss: 0.00181756
Iteration 6/25 | Loss: 0.00181609
Iteration 7/25 | Loss: 0.00181737
Iteration 8/25 | Loss: 0.00181882
Iteration 9/25 | Loss: 0.00181698
Iteration 10/25 | Loss: 0.00181737
Iteration 11/25 | Loss: 0.00181907
Iteration 12/25 | Loss: 0.00181899
Iteration 13/25 | Loss: 0.00181720
Iteration 14/25 | Loss: 0.00181817
Iteration 15/25 | Loss: 0.00181881
Iteration 16/25 | Loss: 0.00181654
Iteration 17/25 | Loss: 0.00181910
Iteration 18/25 | Loss: 0.00181803
Iteration 19/25 | Loss: 0.00181751
Iteration 20/25 | Loss: 0.00181789
Iteration 21/25 | Loss: 0.00181773
Iteration 22/25 | Loss: 0.00181801
Iteration 23/25 | Loss: 0.00181823
Iteration 24/25 | Loss: 0.00181871
Iteration 25/25 | Loss: 0.00181793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49216473
Iteration 2/25 | Loss: 0.00223315
Iteration 3/25 | Loss: 0.00223315
Iteration 4/25 | Loss: 0.00223315
Iteration 5/25 | Loss: 0.00223315
Iteration 6/25 | Loss: 0.00223315
Iteration 7/25 | Loss: 0.00223315
Iteration 8/25 | Loss: 0.00223315
Iteration 9/25 | Loss: 0.00223315
Iteration 10/25 | Loss: 0.00223315
Iteration 11/25 | Loss: 0.00223315
Iteration 12/25 | Loss: 0.00223315
Iteration 13/25 | Loss: 0.00223315
Iteration 14/25 | Loss: 0.00223315
Iteration 15/25 | Loss: 0.00223315
Iteration 16/25 | Loss: 0.00223315
Iteration 17/25 | Loss: 0.00223315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002233147155493498, 0.002233147155493498, 0.002233147155493498, 0.002233147155493498, 0.002233147155493498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002233147155493498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223315
Iteration 2/1000 | Loss: 0.00010448
Iteration 3/1000 | Loss: 0.00009272
Iteration 4/1000 | Loss: 0.00007938
Iteration 5/1000 | Loss: 0.00006727
Iteration 6/1000 | Loss: 0.00006960
Iteration 7/1000 | Loss: 0.00007436
Iteration 8/1000 | Loss: 0.00007999
Iteration 9/1000 | Loss: 0.00006383
Iteration 10/1000 | Loss: 0.00006928
Iteration 11/1000 | Loss: 0.00006502
Iteration 12/1000 | Loss: 0.00008070
Iteration 13/1000 | Loss: 0.00007057
Iteration 14/1000 | Loss: 0.00008795
Iteration 15/1000 | Loss: 0.00007670
Iteration 16/1000 | Loss: 0.00007547
Iteration 17/1000 | Loss: 0.00009214
Iteration 18/1000 | Loss: 0.00005422
Iteration 19/1000 | Loss: 0.00006726
Iteration 20/1000 | Loss: 0.00006757
Iteration 21/1000 | Loss: 0.00009895
Iteration 22/1000 | Loss: 0.00006823
Iteration 23/1000 | Loss: 0.00007298
Iteration 24/1000 | Loss: 0.00008504
Iteration 25/1000 | Loss: 0.00009558
Iteration 26/1000 | Loss: 0.00008442
Iteration 27/1000 | Loss: 0.00009302
Iteration 28/1000 | Loss: 0.00008386
Iteration 29/1000 | Loss: 0.00008895
Iteration 30/1000 | Loss: 0.00007619
Iteration 31/1000 | Loss: 0.00007736
Iteration 32/1000 | Loss: 0.00009450
Iteration 33/1000 | Loss: 0.00009262
Iteration 34/1000 | Loss: 0.00005087
Iteration 35/1000 | Loss: 0.00004607
Iteration 36/1000 | Loss: 0.00004323
Iteration 37/1000 | Loss: 0.00004183
Iteration 38/1000 | Loss: 0.00004121
Iteration 39/1000 | Loss: 0.00004075
Iteration 40/1000 | Loss: 0.00004040
Iteration 41/1000 | Loss: 0.00004013
Iteration 42/1000 | Loss: 0.00003996
Iteration 43/1000 | Loss: 0.00003995
Iteration 44/1000 | Loss: 0.00003987
Iteration 45/1000 | Loss: 0.00003972
Iteration 46/1000 | Loss: 0.00003968
Iteration 47/1000 | Loss: 0.00003968
Iteration 48/1000 | Loss: 0.00003967
Iteration 49/1000 | Loss: 0.00003967
Iteration 50/1000 | Loss: 0.00003966
Iteration 51/1000 | Loss: 0.00003966
Iteration 52/1000 | Loss: 0.00003965
Iteration 53/1000 | Loss: 0.00003961
Iteration 54/1000 | Loss: 0.00003959
Iteration 55/1000 | Loss: 0.00003958
Iteration 56/1000 | Loss: 0.00003958
Iteration 57/1000 | Loss: 0.00003958
Iteration 58/1000 | Loss: 0.00003958
Iteration 59/1000 | Loss: 0.00003956
Iteration 60/1000 | Loss: 0.00003955
Iteration 61/1000 | Loss: 0.00003953
Iteration 62/1000 | Loss: 0.00003950
Iteration 63/1000 | Loss: 0.00003949
Iteration 64/1000 | Loss: 0.00003949
Iteration 65/1000 | Loss: 0.00003938
Iteration 66/1000 | Loss: 0.00003935
Iteration 67/1000 | Loss: 0.00003931
Iteration 68/1000 | Loss: 0.00003929
Iteration 69/1000 | Loss: 0.00003928
Iteration 70/1000 | Loss: 0.00003928
Iteration 71/1000 | Loss: 0.00003928
Iteration 72/1000 | Loss: 0.00003927
Iteration 73/1000 | Loss: 0.00003926
Iteration 74/1000 | Loss: 0.00003926
Iteration 75/1000 | Loss: 0.00003925
Iteration 76/1000 | Loss: 0.00003925
Iteration 77/1000 | Loss: 0.00003925
Iteration 78/1000 | Loss: 0.00003925
Iteration 79/1000 | Loss: 0.00003925
Iteration 80/1000 | Loss: 0.00003925
Iteration 81/1000 | Loss: 0.00003925
Iteration 82/1000 | Loss: 0.00003925
Iteration 83/1000 | Loss: 0.00003925
Iteration 84/1000 | Loss: 0.00003925
Iteration 85/1000 | Loss: 0.00003925
Iteration 86/1000 | Loss: 0.00003925
Iteration 87/1000 | Loss: 0.00003924
Iteration 88/1000 | Loss: 0.00003923
Iteration 89/1000 | Loss: 0.00003923
Iteration 90/1000 | Loss: 0.00003922
Iteration 91/1000 | Loss: 0.00003922
Iteration 92/1000 | Loss: 0.00003922
Iteration 93/1000 | Loss: 0.00003922
Iteration 94/1000 | Loss: 0.00003922
Iteration 95/1000 | Loss: 0.00003922
Iteration 96/1000 | Loss: 0.00003922
Iteration 97/1000 | Loss: 0.00003922
Iteration 98/1000 | Loss: 0.00003922
Iteration 99/1000 | Loss: 0.00003922
Iteration 100/1000 | Loss: 0.00003921
Iteration 101/1000 | Loss: 0.00003921
Iteration 102/1000 | Loss: 0.00003921
Iteration 103/1000 | Loss: 0.00003921
Iteration 104/1000 | Loss: 0.00003921
Iteration 105/1000 | Loss: 0.00003920
Iteration 106/1000 | Loss: 0.00003920
Iteration 107/1000 | Loss: 0.00003920
Iteration 108/1000 | Loss: 0.00003920
Iteration 109/1000 | Loss: 0.00003920
Iteration 110/1000 | Loss: 0.00003920
Iteration 111/1000 | Loss: 0.00003920
Iteration 112/1000 | Loss: 0.00003920
Iteration 113/1000 | Loss: 0.00003920
Iteration 114/1000 | Loss: 0.00003920
Iteration 115/1000 | Loss: 0.00003919
Iteration 116/1000 | Loss: 0.00003919
Iteration 117/1000 | Loss: 0.00003919
Iteration 118/1000 | Loss: 0.00003919
Iteration 119/1000 | Loss: 0.00003919
Iteration 120/1000 | Loss: 0.00003919
Iteration 121/1000 | Loss: 0.00003918
Iteration 122/1000 | Loss: 0.00003918
Iteration 123/1000 | Loss: 0.00003918
Iteration 124/1000 | Loss: 0.00003918
Iteration 125/1000 | Loss: 0.00003918
Iteration 126/1000 | Loss: 0.00003918
Iteration 127/1000 | Loss: 0.00003918
Iteration 128/1000 | Loss: 0.00003918
Iteration 129/1000 | Loss: 0.00003918
Iteration 130/1000 | Loss: 0.00003918
Iteration 131/1000 | Loss: 0.00003918
Iteration 132/1000 | Loss: 0.00003918
Iteration 133/1000 | Loss: 0.00003918
Iteration 134/1000 | Loss: 0.00003918
Iteration 135/1000 | Loss: 0.00003918
Iteration 136/1000 | Loss: 0.00003918
Iteration 137/1000 | Loss: 0.00003918
Iteration 138/1000 | Loss: 0.00003918
Iteration 139/1000 | Loss: 0.00003917
Iteration 140/1000 | Loss: 0.00003917
Iteration 141/1000 | Loss: 0.00003917
Iteration 142/1000 | Loss: 0.00003917
Iteration 143/1000 | Loss: 0.00003917
Iteration 144/1000 | Loss: 0.00003917
Iteration 145/1000 | Loss: 0.00003917
Iteration 146/1000 | Loss: 0.00003917
Iteration 147/1000 | Loss: 0.00003917
Iteration 148/1000 | Loss: 0.00003917
Iteration 149/1000 | Loss: 0.00003917
Iteration 150/1000 | Loss: 0.00003917
Iteration 151/1000 | Loss: 0.00003917
Iteration 152/1000 | Loss: 0.00003917
Iteration 153/1000 | Loss: 0.00003917
Iteration 154/1000 | Loss: 0.00003917
Iteration 155/1000 | Loss: 0.00003916
Iteration 156/1000 | Loss: 0.00003916
Iteration 157/1000 | Loss: 0.00003916
Iteration 158/1000 | Loss: 0.00003916
Iteration 159/1000 | Loss: 0.00003916
Iteration 160/1000 | Loss: 0.00003916
Iteration 161/1000 | Loss: 0.00003916
Iteration 162/1000 | Loss: 0.00003916
Iteration 163/1000 | Loss: 0.00003916
Iteration 164/1000 | Loss: 0.00003916
Iteration 165/1000 | Loss: 0.00003916
Iteration 166/1000 | Loss: 0.00003916
Iteration 167/1000 | Loss: 0.00003916
Iteration 168/1000 | Loss: 0.00003915
Iteration 169/1000 | Loss: 0.00003915
Iteration 170/1000 | Loss: 0.00003915
Iteration 171/1000 | Loss: 0.00003915
Iteration 172/1000 | Loss: 0.00003915
Iteration 173/1000 | Loss: 0.00003915
Iteration 174/1000 | Loss: 0.00003915
Iteration 175/1000 | Loss: 0.00003915
Iteration 176/1000 | Loss: 0.00003915
Iteration 177/1000 | Loss: 0.00003915
Iteration 178/1000 | Loss: 0.00003915
Iteration 179/1000 | Loss: 0.00003915
Iteration 180/1000 | Loss: 0.00003915
Iteration 181/1000 | Loss: 0.00003915
Iteration 182/1000 | Loss: 0.00003915
Iteration 183/1000 | Loss: 0.00003915
Iteration 184/1000 | Loss: 0.00003915
Iteration 185/1000 | Loss: 0.00003915
Iteration 186/1000 | Loss: 0.00003915
Iteration 187/1000 | Loss: 0.00003915
Iteration 188/1000 | Loss: 0.00003915
Iteration 189/1000 | Loss: 0.00003915
Iteration 190/1000 | Loss: 0.00003914
Iteration 191/1000 | Loss: 0.00003914
Iteration 192/1000 | Loss: 0.00003914
Iteration 193/1000 | Loss: 0.00003914
Iteration 194/1000 | Loss: 0.00003914
Iteration 195/1000 | Loss: 0.00003914
Iteration 196/1000 | Loss: 0.00003914
Iteration 197/1000 | Loss: 0.00003914
Iteration 198/1000 | Loss: 0.00003914
Iteration 199/1000 | Loss: 0.00003914
Iteration 200/1000 | Loss: 0.00003914
Iteration 201/1000 | Loss: 0.00003914
Iteration 202/1000 | Loss: 0.00003914
Iteration 203/1000 | Loss: 0.00003914
Iteration 204/1000 | Loss: 0.00003914
Iteration 205/1000 | Loss: 0.00003914
Iteration 206/1000 | Loss: 0.00003914
Iteration 207/1000 | Loss: 0.00003914
Iteration 208/1000 | Loss: 0.00003914
Iteration 209/1000 | Loss: 0.00003914
Iteration 210/1000 | Loss: 0.00003914
Iteration 211/1000 | Loss: 0.00003914
Iteration 212/1000 | Loss: 0.00003914
Iteration 213/1000 | Loss: 0.00003914
Iteration 214/1000 | Loss: 0.00003914
Iteration 215/1000 | Loss: 0.00003914
Iteration 216/1000 | Loss: 0.00003914
Iteration 217/1000 | Loss: 0.00003914
Iteration 218/1000 | Loss: 0.00003914
Iteration 219/1000 | Loss: 0.00003914
Iteration 220/1000 | Loss: 0.00003914
Iteration 221/1000 | Loss: 0.00003914
Iteration 222/1000 | Loss: 0.00003914
Iteration 223/1000 | Loss: 0.00003914
Iteration 224/1000 | Loss: 0.00003914
Iteration 225/1000 | Loss: 0.00003914
Iteration 226/1000 | Loss: 0.00003914
Iteration 227/1000 | Loss: 0.00003914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [3.914161788998172e-05, 3.914161788998172e-05, 3.914161788998172e-05, 3.914161788998172e-05, 3.914161788998172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.914161788998172e-05

Optimization complete. Final v2v error: 5.509536266326904 mm

Highest mean error: 6.132993221282959 mm for frame 91

Lowest mean error: 5.27745246887207 mm for frame 60

Saving results

Total time: 129.60059189796448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917821
Iteration 2/25 | Loss: 0.00237637
Iteration 3/25 | Loss: 0.00182080
Iteration 4/25 | Loss: 0.00174662
Iteration 5/25 | Loss: 0.00173489
Iteration 6/25 | Loss: 0.00170927
Iteration 7/25 | Loss: 0.00169585
Iteration 8/25 | Loss: 0.00169592
Iteration 9/25 | Loss: 0.00169390
Iteration 10/25 | Loss: 0.00169098
Iteration 11/25 | Loss: 0.00169184
Iteration 12/25 | Loss: 0.00169084
Iteration 13/25 | Loss: 0.00169056
Iteration 14/25 | Loss: 0.00169045
Iteration 15/25 | Loss: 0.00169045
Iteration 16/25 | Loss: 0.00169045
Iteration 17/25 | Loss: 0.00169045
Iteration 18/25 | Loss: 0.00169044
Iteration 19/25 | Loss: 0.00169044
Iteration 20/25 | Loss: 0.00169044
Iteration 21/25 | Loss: 0.00169044
Iteration 22/25 | Loss: 0.00169044
Iteration 23/25 | Loss: 0.00169044
Iteration 24/25 | Loss: 0.00169044
Iteration 25/25 | Loss: 0.00169044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07796168
Iteration 2/25 | Loss: 0.00209045
Iteration 3/25 | Loss: 0.00206137
Iteration 4/25 | Loss: 0.00206136
Iteration 5/25 | Loss: 0.00206136
Iteration 6/25 | Loss: 0.00206136
Iteration 7/25 | Loss: 0.00206136
Iteration 8/25 | Loss: 0.00206136
Iteration 9/25 | Loss: 0.00206136
Iteration 10/25 | Loss: 0.00206136
Iteration 11/25 | Loss: 0.00206136
Iteration 12/25 | Loss: 0.00206136
Iteration 13/25 | Loss: 0.00206136
Iteration 14/25 | Loss: 0.00206136
Iteration 15/25 | Loss: 0.00206136
Iteration 16/25 | Loss: 0.00206136
Iteration 17/25 | Loss: 0.00206136
Iteration 18/25 | Loss: 0.00206136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020613630767911673, 0.0020613630767911673, 0.0020613630767911673, 0.0020613630767911673, 0.0020613630767911673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020613630767911673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206136
Iteration 2/1000 | Loss: 0.00014485
Iteration 3/1000 | Loss: 0.00007602
Iteration 4/1000 | Loss: 0.00030021
Iteration 5/1000 | Loss: 0.00007200
Iteration 6/1000 | Loss: 0.00005508
Iteration 7/1000 | Loss: 0.00007316
Iteration 8/1000 | Loss: 0.00006181
Iteration 9/1000 | Loss: 0.00007009
Iteration 10/1000 | Loss: 0.00005126
Iteration 11/1000 | Loss: 0.00005089
Iteration 12/1000 | Loss: 0.00009900
Iteration 13/1000 | Loss: 0.00005021
Iteration 14/1000 | Loss: 0.00008754
Iteration 15/1000 | Loss: 0.00005032
Iteration 16/1000 | Loss: 0.00004981
Iteration 17/1000 | Loss: 0.00004981
Iteration 18/1000 | Loss: 0.00004980
Iteration 19/1000 | Loss: 0.00004976
Iteration 20/1000 | Loss: 0.00004976
Iteration 21/1000 | Loss: 0.00004975
Iteration 22/1000 | Loss: 0.00004974
Iteration 23/1000 | Loss: 0.00004974
Iteration 24/1000 | Loss: 0.00004969
Iteration 25/1000 | Loss: 0.00004966
Iteration 26/1000 | Loss: 0.00004966
Iteration 27/1000 | Loss: 0.00004966
Iteration 28/1000 | Loss: 0.00004965
Iteration 29/1000 | Loss: 0.00004965
Iteration 30/1000 | Loss: 0.00004965
Iteration 31/1000 | Loss: 0.00004965
Iteration 32/1000 | Loss: 0.00004965
Iteration 33/1000 | Loss: 0.00004964
Iteration 34/1000 | Loss: 0.00004964
Iteration 35/1000 | Loss: 0.00004964
Iteration 36/1000 | Loss: 0.00004964
Iteration 37/1000 | Loss: 0.00004964
Iteration 38/1000 | Loss: 0.00004964
Iteration 39/1000 | Loss: 0.00004964
Iteration 40/1000 | Loss: 0.00004961
Iteration 41/1000 | Loss: 0.00004961
Iteration 42/1000 | Loss: 0.00004961
Iteration 43/1000 | Loss: 0.00004961
Iteration 44/1000 | Loss: 0.00004961
Iteration 45/1000 | Loss: 0.00004961
Iteration 46/1000 | Loss: 0.00004960
Iteration 47/1000 | Loss: 0.00004960
Iteration 48/1000 | Loss: 0.00004960
Iteration 49/1000 | Loss: 0.00004956
Iteration 50/1000 | Loss: 0.00004955
Iteration 51/1000 | Loss: 0.00004950
Iteration 52/1000 | Loss: 0.00004949
Iteration 53/1000 | Loss: 0.00004948
Iteration 54/1000 | Loss: 0.00004947
Iteration 55/1000 | Loss: 0.00004945
Iteration 56/1000 | Loss: 0.00004944
Iteration 57/1000 | Loss: 0.00004944
Iteration 58/1000 | Loss: 0.00008907
Iteration 59/1000 | Loss: 0.00004936
Iteration 60/1000 | Loss: 0.00004931
Iteration 61/1000 | Loss: 0.00004928
Iteration 62/1000 | Loss: 0.00004928
Iteration 63/1000 | Loss: 0.00004927
Iteration 64/1000 | Loss: 0.00004927
Iteration 65/1000 | Loss: 0.00004927
Iteration 66/1000 | Loss: 0.00004927
Iteration 67/1000 | Loss: 0.00004927
Iteration 68/1000 | Loss: 0.00004926
Iteration 69/1000 | Loss: 0.00004926
Iteration 70/1000 | Loss: 0.00004926
Iteration 71/1000 | Loss: 0.00004926
Iteration 72/1000 | Loss: 0.00004926
Iteration 73/1000 | Loss: 0.00004926
Iteration 74/1000 | Loss: 0.00004926
Iteration 75/1000 | Loss: 0.00004926
Iteration 76/1000 | Loss: 0.00004926
Iteration 77/1000 | Loss: 0.00004926
Iteration 78/1000 | Loss: 0.00004926
Iteration 79/1000 | Loss: 0.00004925
Iteration 80/1000 | Loss: 0.00004925
Iteration 81/1000 | Loss: 0.00004925
Iteration 82/1000 | Loss: 0.00004924
Iteration 83/1000 | Loss: 0.00004924
Iteration 84/1000 | Loss: 0.00004924
Iteration 85/1000 | Loss: 0.00004924
Iteration 86/1000 | Loss: 0.00004923
Iteration 87/1000 | Loss: 0.00004923
Iteration 88/1000 | Loss: 0.00009509
Iteration 89/1000 | Loss: 0.00004920
Iteration 90/1000 | Loss: 0.00004907
Iteration 91/1000 | Loss: 0.00004906
Iteration 92/1000 | Loss: 0.00004906
Iteration 93/1000 | Loss: 0.00004905
Iteration 94/1000 | Loss: 0.00004905
Iteration 95/1000 | Loss: 0.00004905
Iteration 96/1000 | Loss: 0.00004904
Iteration 97/1000 | Loss: 0.00004904
Iteration 98/1000 | Loss: 0.00004904
Iteration 99/1000 | Loss: 0.00004904
Iteration 100/1000 | Loss: 0.00004904
Iteration 101/1000 | Loss: 0.00004904
Iteration 102/1000 | Loss: 0.00004904
Iteration 103/1000 | Loss: 0.00004904
Iteration 104/1000 | Loss: 0.00004904
Iteration 105/1000 | Loss: 0.00004904
Iteration 106/1000 | Loss: 0.00004904
Iteration 107/1000 | Loss: 0.00004904
Iteration 108/1000 | Loss: 0.00004904
Iteration 109/1000 | Loss: 0.00004904
Iteration 110/1000 | Loss: 0.00004904
Iteration 111/1000 | Loss: 0.00004904
Iteration 112/1000 | Loss: 0.00004904
Iteration 113/1000 | Loss: 0.00004904
Iteration 114/1000 | Loss: 0.00004903
Iteration 115/1000 | Loss: 0.00004903
Iteration 116/1000 | Loss: 0.00004903
Iteration 117/1000 | Loss: 0.00004902
Iteration 118/1000 | Loss: 0.00004902
Iteration 119/1000 | Loss: 0.00004902
Iteration 120/1000 | Loss: 0.00004901
Iteration 121/1000 | Loss: 0.00004901
Iteration 122/1000 | Loss: 0.00004900
Iteration 123/1000 | Loss: 0.00004900
Iteration 124/1000 | Loss: 0.00004899
Iteration 125/1000 | Loss: 0.00005321
Iteration 126/1000 | Loss: 0.00007679
Iteration 127/1000 | Loss: 0.00004993
Iteration 128/1000 | Loss: 0.00004912
Iteration 129/1000 | Loss: 0.00004890
Iteration 130/1000 | Loss: 0.00004887
Iteration 131/1000 | Loss: 0.00004887
Iteration 132/1000 | Loss: 0.00004887
Iteration 133/1000 | Loss: 0.00004886
Iteration 134/1000 | Loss: 0.00006843
Iteration 135/1000 | Loss: 0.00004888
Iteration 136/1000 | Loss: 0.00004885
Iteration 137/1000 | Loss: 0.00004885
Iteration 138/1000 | Loss: 0.00004885
Iteration 139/1000 | Loss: 0.00004885
Iteration 140/1000 | Loss: 0.00004885
Iteration 141/1000 | Loss: 0.00004885
Iteration 142/1000 | Loss: 0.00004885
Iteration 143/1000 | Loss: 0.00004885
Iteration 144/1000 | Loss: 0.00004885
Iteration 145/1000 | Loss: 0.00004884
Iteration 146/1000 | Loss: 0.00004883
Iteration 147/1000 | Loss: 0.00004883
Iteration 148/1000 | Loss: 0.00004883
Iteration 149/1000 | Loss: 0.00004883
Iteration 150/1000 | Loss: 0.00004883
Iteration 151/1000 | Loss: 0.00004883
Iteration 152/1000 | Loss: 0.00004883
Iteration 153/1000 | Loss: 0.00004883
Iteration 154/1000 | Loss: 0.00004883
Iteration 155/1000 | Loss: 0.00004883
Iteration 156/1000 | Loss: 0.00004883
Iteration 157/1000 | Loss: 0.00004883
Iteration 158/1000 | Loss: 0.00004883
Iteration 159/1000 | Loss: 0.00004883
Iteration 160/1000 | Loss: 0.00004883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.883145447820425e-05, 4.883145447820425e-05, 4.883145447820425e-05, 4.883145447820425e-05, 4.883145447820425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.883145447820425e-05

Optimization complete. Final v2v error: 6.082726955413818 mm

Highest mean error: 11.903827667236328 mm for frame 136

Lowest mean error: 5.768335819244385 mm for frame 25

Saving results

Total time: 76.03465580940247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485897
Iteration 2/25 | Loss: 0.00188870
Iteration 3/25 | Loss: 0.00183181
Iteration 4/25 | Loss: 0.00182426
Iteration 5/25 | Loss: 0.00182202
Iteration 6/25 | Loss: 0.00182202
Iteration 7/25 | Loss: 0.00182202
Iteration 8/25 | Loss: 0.00182202
Iteration 9/25 | Loss: 0.00182202
Iteration 10/25 | Loss: 0.00182202
Iteration 11/25 | Loss: 0.00182202
Iteration 12/25 | Loss: 0.00182202
Iteration 13/25 | Loss: 0.00182202
Iteration 14/25 | Loss: 0.00182202
Iteration 15/25 | Loss: 0.00182202
Iteration 16/25 | Loss: 0.00182202
Iteration 17/25 | Loss: 0.00182202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001822017366066575, 0.001822017366066575, 0.001822017366066575, 0.001822017366066575, 0.001822017366066575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001822017366066575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47604012
Iteration 2/25 | Loss: 0.00228426
Iteration 3/25 | Loss: 0.00228426
Iteration 4/25 | Loss: 0.00228426
Iteration 5/25 | Loss: 0.00228426
Iteration 6/25 | Loss: 0.00228426
Iteration 7/25 | Loss: 0.00228426
Iteration 8/25 | Loss: 0.00228426
Iteration 9/25 | Loss: 0.00228426
Iteration 10/25 | Loss: 0.00228426
Iteration 11/25 | Loss: 0.00228426
Iteration 12/25 | Loss: 0.00228426
Iteration 13/25 | Loss: 0.00228426
Iteration 14/25 | Loss: 0.00228426
Iteration 15/25 | Loss: 0.00228426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022842567414045334, 0.0022842567414045334, 0.0022842567414045334, 0.0022842567414045334, 0.0022842567414045334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022842567414045334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228426
Iteration 2/1000 | Loss: 0.00008012
Iteration 3/1000 | Loss: 0.00005406
Iteration 4/1000 | Loss: 0.00004515
Iteration 5/1000 | Loss: 0.00004240
Iteration 6/1000 | Loss: 0.00004131
Iteration 7/1000 | Loss: 0.00004068
Iteration 8/1000 | Loss: 0.00004010
Iteration 9/1000 | Loss: 0.00003971
Iteration 10/1000 | Loss: 0.00003949
Iteration 11/1000 | Loss: 0.00003948
Iteration 12/1000 | Loss: 0.00003928
Iteration 13/1000 | Loss: 0.00003927
Iteration 14/1000 | Loss: 0.00003927
Iteration 15/1000 | Loss: 0.00003926
Iteration 16/1000 | Loss: 0.00003926
Iteration 17/1000 | Loss: 0.00003920
Iteration 18/1000 | Loss: 0.00003920
Iteration 19/1000 | Loss: 0.00003919
Iteration 20/1000 | Loss: 0.00003918
Iteration 21/1000 | Loss: 0.00003918
Iteration 22/1000 | Loss: 0.00003915
Iteration 23/1000 | Loss: 0.00003914
Iteration 24/1000 | Loss: 0.00003914
Iteration 25/1000 | Loss: 0.00003914
Iteration 26/1000 | Loss: 0.00003912
Iteration 27/1000 | Loss: 0.00003910
Iteration 28/1000 | Loss: 0.00003910
Iteration 29/1000 | Loss: 0.00003910
Iteration 30/1000 | Loss: 0.00003909
Iteration 31/1000 | Loss: 0.00003909
Iteration 32/1000 | Loss: 0.00003909
Iteration 33/1000 | Loss: 0.00003909
Iteration 34/1000 | Loss: 0.00003908
Iteration 35/1000 | Loss: 0.00003908
Iteration 36/1000 | Loss: 0.00003908
Iteration 37/1000 | Loss: 0.00003908
Iteration 38/1000 | Loss: 0.00003908
Iteration 39/1000 | Loss: 0.00003907
Iteration 40/1000 | Loss: 0.00003907
Iteration 41/1000 | Loss: 0.00003907
Iteration 42/1000 | Loss: 0.00003906
Iteration 43/1000 | Loss: 0.00003906
Iteration 44/1000 | Loss: 0.00003906
Iteration 45/1000 | Loss: 0.00003906
Iteration 46/1000 | Loss: 0.00003906
Iteration 47/1000 | Loss: 0.00003905
Iteration 48/1000 | Loss: 0.00003905
Iteration 49/1000 | Loss: 0.00003905
Iteration 50/1000 | Loss: 0.00003904
Iteration 51/1000 | Loss: 0.00003903
Iteration 52/1000 | Loss: 0.00003903
Iteration 53/1000 | Loss: 0.00003903
Iteration 54/1000 | Loss: 0.00003902
Iteration 55/1000 | Loss: 0.00003902
Iteration 56/1000 | Loss: 0.00003902
Iteration 57/1000 | Loss: 0.00003902
Iteration 58/1000 | Loss: 0.00003902
Iteration 59/1000 | Loss: 0.00003901
Iteration 60/1000 | Loss: 0.00003901
Iteration 61/1000 | Loss: 0.00003900
Iteration 62/1000 | Loss: 0.00003900
Iteration 63/1000 | Loss: 0.00003900
Iteration 64/1000 | Loss: 0.00003900
Iteration 65/1000 | Loss: 0.00003900
Iteration 66/1000 | Loss: 0.00003900
Iteration 67/1000 | Loss: 0.00003900
Iteration 68/1000 | Loss: 0.00003900
Iteration 69/1000 | Loss: 0.00003900
Iteration 70/1000 | Loss: 0.00003900
Iteration 71/1000 | Loss: 0.00003899
Iteration 72/1000 | Loss: 0.00003899
Iteration 73/1000 | Loss: 0.00003899
Iteration 74/1000 | Loss: 0.00003899
Iteration 75/1000 | Loss: 0.00003899
Iteration 76/1000 | Loss: 0.00003899
Iteration 77/1000 | Loss: 0.00003899
Iteration 78/1000 | Loss: 0.00003899
Iteration 79/1000 | Loss: 0.00003899
Iteration 80/1000 | Loss: 0.00003899
Iteration 81/1000 | Loss: 0.00003899
Iteration 82/1000 | Loss: 0.00003899
Iteration 83/1000 | Loss: 0.00003898
Iteration 84/1000 | Loss: 0.00003898
Iteration 85/1000 | Loss: 0.00003898
Iteration 86/1000 | Loss: 0.00003898
Iteration 87/1000 | Loss: 0.00003898
Iteration 88/1000 | Loss: 0.00003898
Iteration 89/1000 | Loss: 0.00003898
Iteration 90/1000 | Loss: 0.00003898
Iteration 91/1000 | Loss: 0.00003898
Iteration 92/1000 | Loss: 0.00003898
Iteration 93/1000 | Loss: 0.00003898
Iteration 94/1000 | Loss: 0.00003898
Iteration 95/1000 | Loss: 0.00003898
Iteration 96/1000 | Loss: 0.00003898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.898291106452234e-05, 3.898291106452234e-05, 3.898291106452234e-05, 3.898291106452234e-05, 3.898291106452234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.898291106452234e-05

Optimization complete. Final v2v error: 5.497157573699951 mm

Highest mean error: 5.800904273986816 mm for frame 215

Lowest mean error: 5.154520034790039 mm for frame 71

Saving results

Total time: 32.54486966133118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821634
Iteration 2/25 | Loss: 0.00225146
Iteration 3/25 | Loss: 0.00194977
Iteration 4/25 | Loss: 0.00192826
Iteration 5/25 | Loss: 0.00191229
Iteration 6/25 | Loss: 0.00190971
Iteration 7/25 | Loss: 0.00190881
Iteration 8/25 | Loss: 0.00190839
Iteration 9/25 | Loss: 0.00190834
Iteration 10/25 | Loss: 0.00190834
Iteration 11/25 | Loss: 0.00190834
Iteration 12/25 | Loss: 0.00190834
Iteration 13/25 | Loss: 0.00190834
Iteration 14/25 | Loss: 0.00190834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019083368824794888, 0.0019083368824794888, 0.0019083368824794888, 0.0019083368824794888, 0.0019083368824794888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019083368824794888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69072795
Iteration 2/25 | Loss: 0.00288218
Iteration 3/25 | Loss: 0.00288213
Iteration 4/25 | Loss: 0.00288213
Iteration 5/25 | Loss: 0.00288213
Iteration 6/25 | Loss: 0.00288213
Iteration 7/25 | Loss: 0.00288213
Iteration 8/25 | Loss: 0.00288213
Iteration 9/25 | Loss: 0.00288213
Iteration 10/25 | Loss: 0.00288213
Iteration 11/25 | Loss: 0.00288213
Iteration 12/25 | Loss: 0.00288213
Iteration 13/25 | Loss: 0.00288213
Iteration 14/25 | Loss: 0.00288213
Iteration 15/25 | Loss: 0.00288213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002882126485928893, 0.002882126485928893, 0.002882126485928893, 0.002882126485928893, 0.002882126485928893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002882126485928893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288213
Iteration 2/1000 | Loss: 0.00011858
Iteration 3/1000 | Loss: 0.00008001
Iteration 4/1000 | Loss: 0.00006604
Iteration 5/1000 | Loss: 0.00005821
Iteration 6/1000 | Loss: 0.00005321
Iteration 7/1000 | Loss: 0.00005037
Iteration 8/1000 | Loss: 0.00004835
Iteration 9/1000 | Loss: 0.00004705
Iteration 10/1000 | Loss: 0.00004616
Iteration 11/1000 | Loss: 0.00004542
Iteration 12/1000 | Loss: 0.00004495
Iteration 13/1000 | Loss: 0.00004457
Iteration 14/1000 | Loss: 0.00004425
Iteration 15/1000 | Loss: 0.00004401
Iteration 16/1000 | Loss: 0.00004383
Iteration 17/1000 | Loss: 0.00004378
Iteration 18/1000 | Loss: 0.00004371
Iteration 19/1000 | Loss: 0.00004366
Iteration 20/1000 | Loss: 0.00004356
Iteration 21/1000 | Loss: 0.00004350
Iteration 22/1000 | Loss: 0.00004349
Iteration 23/1000 | Loss: 0.00004348
Iteration 24/1000 | Loss: 0.00004348
Iteration 25/1000 | Loss: 0.00004348
Iteration 26/1000 | Loss: 0.00004348
Iteration 27/1000 | Loss: 0.00004347
Iteration 28/1000 | Loss: 0.00004347
Iteration 29/1000 | Loss: 0.00004347
Iteration 30/1000 | Loss: 0.00004347
Iteration 31/1000 | Loss: 0.00004346
Iteration 32/1000 | Loss: 0.00004346
Iteration 33/1000 | Loss: 0.00004345
Iteration 34/1000 | Loss: 0.00004345
Iteration 35/1000 | Loss: 0.00004345
Iteration 36/1000 | Loss: 0.00004345
Iteration 37/1000 | Loss: 0.00004344
Iteration 38/1000 | Loss: 0.00004344
Iteration 39/1000 | Loss: 0.00004344
Iteration 40/1000 | Loss: 0.00004344
Iteration 41/1000 | Loss: 0.00004344
Iteration 42/1000 | Loss: 0.00004344
Iteration 43/1000 | Loss: 0.00004344
Iteration 44/1000 | Loss: 0.00004343
Iteration 45/1000 | Loss: 0.00004343
Iteration 46/1000 | Loss: 0.00004343
Iteration 47/1000 | Loss: 0.00004343
Iteration 48/1000 | Loss: 0.00004342
Iteration 49/1000 | Loss: 0.00004342
Iteration 50/1000 | Loss: 0.00004342
Iteration 51/1000 | Loss: 0.00004342
Iteration 52/1000 | Loss: 0.00004342
Iteration 53/1000 | Loss: 0.00004342
Iteration 54/1000 | Loss: 0.00004342
Iteration 55/1000 | Loss: 0.00004342
Iteration 56/1000 | Loss: 0.00004341
Iteration 57/1000 | Loss: 0.00004341
Iteration 58/1000 | Loss: 0.00004341
Iteration 59/1000 | Loss: 0.00004341
Iteration 60/1000 | Loss: 0.00004340
Iteration 61/1000 | Loss: 0.00004340
Iteration 62/1000 | Loss: 0.00004340
Iteration 63/1000 | Loss: 0.00004339
Iteration 64/1000 | Loss: 0.00004339
Iteration 65/1000 | Loss: 0.00004339
Iteration 66/1000 | Loss: 0.00004338
Iteration 67/1000 | Loss: 0.00004338
Iteration 68/1000 | Loss: 0.00004338
Iteration 69/1000 | Loss: 0.00004338
Iteration 70/1000 | Loss: 0.00004338
Iteration 71/1000 | Loss: 0.00004338
Iteration 72/1000 | Loss: 0.00004338
Iteration 73/1000 | Loss: 0.00004338
Iteration 74/1000 | Loss: 0.00004338
Iteration 75/1000 | Loss: 0.00004338
Iteration 76/1000 | Loss: 0.00004338
Iteration 77/1000 | Loss: 0.00004337
Iteration 78/1000 | Loss: 0.00004337
Iteration 79/1000 | Loss: 0.00004336
Iteration 80/1000 | Loss: 0.00004336
Iteration 81/1000 | Loss: 0.00004336
Iteration 82/1000 | Loss: 0.00004336
Iteration 83/1000 | Loss: 0.00004336
Iteration 84/1000 | Loss: 0.00004336
Iteration 85/1000 | Loss: 0.00004336
Iteration 86/1000 | Loss: 0.00004336
Iteration 87/1000 | Loss: 0.00004336
Iteration 88/1000 | Loss: 0.00004336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [4.335832272772677e-05, 4.335832272772677e-05, 4.335832272772677e-05, 4.335832272772677e-05, 4.335832272772677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.335832272772677e-05

Optimization complete. Final v2v error: 5.745114326477051 mm

Highest mean error: 6.878750801086426 mm for frame 111

Lowest mean error: 4.989433288574219 mm for frame 213

Saving results

Total time: 49.77434420585632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498133
Iteration 2/25 | Loss: 0.00202150
Iteration 3/25 | Loss: 0.00189479
Iteration 4/25 | Loss: 0.00187840
Iteration 5/25 | Loss: 0.00187452
Iteration 6/25 | Loss: 0.00187310
Iteration 7/25 | Loss: 0.00187300
Iteration 8/25 | Loss: 0.00187300
Iteration 9/25 | Loss: 0.00187300
Iteration 10/25 | Loss: 0.00187300
Iteration 11/25 | Loss: 0.00187300
Iteration 12/25 | Loss: 0.00187300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018730019219219685, 0.0018730019219219685, 0.0018730019219219685, 0.0018730019219219685, 0.0018730019219219685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018730019219219685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47402430
Iteration 2/25 | Loss: 0.00241500
Iteration 3/25 | Loss: 0.00241500
Iteration 4/25 | Loss: 0.00241500
Iteration 5/25 | Loss: 0.00241500
Iteration 6/25 | Loss: 0.00241500
Iteration 7/25 | Loss: 0.00241500
Iteration 8/25 | Loss: 0.00241500
Iteration 9/25 | Loss: 0.00241500
Iteration 10/25 | Loss: 0.00241500
Iteration 11/25 | Loss: 0.00241500
Iteration 12/25 | Loss: 0.00241500
Iteration 13/25 | Loss: 0.00241500
Iteration 14/25 | Loss: 0.00241500
Iteration 15/25 | Loss: 0.00241500
Iteration 16/25 | Loss: 0.00241500
Iteration 17/25 | Loss: 0.00241500
Iteration 18/25 | Loss: 0.00241500
Iteration 19/25 | Loss: 0.00241500
Iteration 20/25 | Loss: 0.00241500
Iteration 21/25 | Loss: 0.00241500
Iteration 22/25 | Loss: 0.00241500
Iteration 23/25 | Loss: 0.00241500
Iteration 24/25 | Loss: 0.00241500
Iteration 25/25 | Loss: 0.00241500

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241500
Iteration 2/1000 | Loss: 0.00007802
Iteration 3/1000 | Loss: 0.00005875
Iteration 4/1000 | Loss: 0.00004938
Iteration 5/1000 | Loss: 0.00004556
Iteration 6/1000 | Loss: 0.00004386
Iteration 7/1000 | Loss: 0.00004265
Iteration 8/1000 | Loss: 0.00004196
Iteration 9/1000 | Loss: 0.00004158
Iteration 10/1000 | Loss: 0.00004124
Iteration 11/1000 | Loss: 0.00004097
Iteration 12/1000 | Loss: 0.00004088
Iteration 13/1000 | Loss: 0.00004073
Iteration 14/1000 | Loss: 0.00004064
Iteration 15/1000 | Loss: 0.00004057
Iteration 16/1000 | Loss: 0.00004055
Iteration 17/1000 | Loss: 0.00004051
Iteration 18/1000 | Loss: 0.00004051
Iteration 19/1000 | Loss: 0.00004049
Iteration 20/1000 | Loss: 0.00004049
Iteration 21/1000 | Loss: 0.00004049
Iteration 22/1000 | Loss: 0.00004045
Iteration 23/1000 | Loss: 0.00004044
Iteration 24/1000 | Loss: 0.00004044
Iteration 25/1000 | Loss: 0.00004041
Iteration 26/1000 | Loss: 0.00004040
Iteration 27/1000 | Loss: 0.00004039
Iteration 28/1000 | Loss: 0.00004039
Iteration 29/1000 | Loss: 0.00004039
Iteration 30/1000 | Loss: 0.00004039
Iteration 31/1000 | Loss: 0.00004039
Iteration 32/1000 | Loss: 0.00004039
Iteration 33/1000 | Loss: 0.00004039
Iteration 34/1000 | Loss: 0.00004039
Iteration 35/1000 | Loss: 0.00004039
Iteration 36/1000 | Loss: 0.00004038
Iteration 37/1000 | Loss: 0.00004038
Iteration 38/1000 | Loss: 0.00004037
Iteration 39/1000 | Loss: 0.00004037
Iteration 40/1000 | Loss: 0.00004036
Iteration 41/1000 | Loss: 0.00004036
Iteration 42/1000 | Loss: 0.00004036
Iteration 43/1000 | Loss: 0.00004036
Iteration 44/1000 | Loss: 0.00004036
Iteration 45/1000 | Loss: 0.00004036
Iteration 46/1000 | Loss: 0.00004036
Iteration 47/1000 | Loss: 0.00004036
Iteration 48/1000 | Loss: 0.00004036
Iteration 49/1000 | Loss: 0.00004036
Iteration 50/1000 | Loss: 0.00004036
Iteration 51/1000 | Loss: 0.00004036
Iteration 52/1000 | Loss: 0.00004036
Iteration 53/1000 | Loss: 0.00004036
Iteration 54/1000 | Loss: 0.00004034
Iteration 55/1000 | Loss: 0.00004034
Iteration 56/1000 | Loss: 0.00004034
Iteration 57/1000 | Loss: 0.00004033
Iteration 58/1000 | Loss: 0.00004033
Iteration 59/1000 | Loss: 0.00004033
Iteration 60/1000 | Loss: 0.00004032
Iteration 61/1000 | Loss: 0.00004032
Iteration 62/1000 | Loss: 0.00004032
Iteration 63/1000 | Loss: 0.00004032
Iteration 64/1000 | Loss: 0.00004032
Iteration 65/1000 | Loss: 0.00004032
Iteration 66/1000 | Loss: 0.00004031
Iteration 67/1000 | Loss: 0.00004031
Iteration 68/1000 | Loss: 0.00004031
Iteration 69/1000 | Loss: 0.00004031
Iteration 70/1000 | Loss: 0.00004030
Iteration 71/1000 | Loss: 0.00004030
Iteration 72/1000 | Loss: 0.00004030
Iteration 73/1000 | Loss: 0.00004030
Iteration 74/1000 | Loss: 0.00004030
Iteration 75/1000 | Loss: 0.00004030
Iteration 76/1000 | Loss: 0.00004030
Iteration 77/1000 | Loss: 0.00004030
Iteration 78/1000 | Loss: 0.00004030
Iteration 79/1000 | Loss: 0.00004030
Iteration 80/1000 | Loss: 0.00004030
Iteration 81/1000 | Loss: 0.00004030
Iteration 82/1000 | Loss: 0.00004029
Iteration 83/1000 | Loss: 0.00004029
Iteration 84/1000 | Loss: 0.00004029
Iteration 85/1000 | Loss: 0.00004029
Iteration 86/1000 | Loss: 0.00004029
Iteration 87/1000 | Loss: 0.00004029
Iteration 88/1000 | Loss: 0.00004029
Iteration 89/1000 | Loss: 0.00004029
Iteration 90/1000 | Loss: 0.00004029
Iteration 91/1000 | Loss: 0.00004029
Iteration 92/1000 | Loss: 0.00004029
Iteration 93/1000 | Loss: 0.00004029
Iteration 94/1000 | Loss: 0.00004028
Iteration 95/1000 | Loss: 0.00004028
Iteration 96/1000 | Loss: 0.00004028
Iteration 97/1000 | Loss: 0.00004028
Iteration 98/1000 | Loss: 0.00004028
Iteration 99/1000 | Loss: 0.00004028
Iteration 100/1000 | Loss: 0.00004028
Iteration 101/1000 | Loss: 0.00004028
Iteration 102/1000 | Loss: 0.00004028
Iteration 103/1000 | Loss: 0.00004028
Iteration 104/1000 | Loss: 0.00004028
Iteration 105/1000 | Loss: 0.00004028
Iteration 106/1000 | Loss: 0.00004028
Iteration 107/1000 | Loss: 0.00004028
Iteration 108/1000 | Loss: 0.00004028
Iteration 109/1000 | Loss: 0.00004028
Iteration 110/1000 | Loss: 0.00004028
Iteration 111/1000 | Loss: 0.00004028
Iteration 112/1000 | Loss: 0.00004028
Iteration 113/1000 | Loss: 0.00004028
Iteration 114/1000 | Loss: 0.00004028
Iteration 115/1000 | Loss: 0.00004028
Iteration 116/1000 | Loss: 0.00004028
Iteration 117/1000 | Loss: 0.00004028
Iteration 118/1000 | Loss: 0.00004028
Iteration 119/1000 | Loss: 0.00004028
Iteration 120/1000 | Loss: 0.00004028
Iteration 121/1000 | Loss: 0.00004028
Iteration 122/1000 | Loss: 0.00004028
Iteration 123/1000 | Loss: 0.00004028
Iteration 124/1000 | Loss: 0.00004028
Iteration 125/1000 | Loss: 0.00004028
Iteration 126/1000 | Loss: 0.00004028
Iteration 127/1000 | Loss: 0.00004028
Iteration 128/1000 | Loss: 0.00004028
Iteration 129/1000 | Loss: 0.00004028
Iteration 130/1000 | Loss: 0.00004028
Iteration 131/1000 | Loss: 0.00004028
Iteration 132/1000 | Loss: 0.00004028
Iteration 133/1000 | Loss: 0.00004028
Iteration 134/1000 | Loss: 0.00004028
Iteration 135/1000 | Loss: 0.00004028
Iteration 136/1000 | Loss: 0.00004028
Iteration 137/1000 | Loss: 0.00004028
Iteration 138/1000 | Loss: 0.00004028
Iteration 139/1000 | Loss: 0.00004028
Iteration 140/1000 | Loss: 0.00004028
Iteration 141/1000 | Loss: 0.00004028
Iteration 142/1000 | Loss: 0.00004028
Iteration 143/1000 | Loss: 0.00004028
Iteration 144/1000 | Loss: 0.00004028
Iteration 145/1000 | Loss: 0.00004028
Iteration 146/1000 | Loss: 0.00004028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [4.027915929327719e-05, 4.027915929327719e-05, 4.027915929327719e-05, 4.027915929327719e-05, 4.027915929327719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.027915929327719e-05

Optimization complete. Final v2v error: 5.546341419219971 mm

Highest mean error: 5.963475704193115 mm for frame 77

Lowest mean error: 5.118276119232178 mm for frame 1

Saving results

Total time: 36.92228412628174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01215148
Iteration 2/25 | Loss: 0.00285832
Iteration 3/25 | Loss: 0.00239747
Iteration 4/25 | Loss: 0.00225311
Iteration 5/25 | Loss: 0.00225085
Iteration 6/25 | Loss: 0.00216038
Iteration 7/25 | Loss: 0.00212575
Iteration 8/25 | Loss: 0.00210077
Iteration 9/25 | Loss: 0.00210534
Iteration 10/25 | Loss: 0.00207868
Iteration 11/25 | Loss: 0.00207124
Iteration 12/25 | Loss: 0.00206773
Iteration 13/25 | Loss: 0.00206663
Iteration 14/25 | Loss: 0.00206551
Iteration 15/25 | Loss: 0.00206598
Iteration 16/25 | Loss: 0.00206569
Iteration 17/25 | Loss: 0.00206687
Iteration 18/25 | Loss: 0.00206683
Iteration 19/25 | Loss: 0.00206675
Iteration 20/25 | Loss: 0.00206663
Iteration 21/25 | Loss: 0.00206636
Iteration 22/25 | Loss: 0.00206575
Iteration 23/25 | Loss: 0.00206521
Iteration 24/25 | Loss: 0.00206502
Iteration 25/25 | Loss: 0.00206501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34960127
Iteration 2/25 | Loss: 0.00567853
Iteration 3/25 | Loss: 0.00567852
Iteration 4/25 | Loss: 0.00567852
Iteration 5/25 | Loss: 0.00567852
Iteration 6/25 | Loss: 0.00567852
Iteration 7/25 | Loss: 0.00567852
Iteration 8/25 | Loss: 0.00567852
Iteration 9/25 | Loss: 0.00567852
Iteration 10/25 | Loss: 0.00567852
Iteration 11/25 | Loss: 0.00567852
Iteration 12/25 | Loss: 0.00567852
Iteration 13/25 | Loss: 0.00567852
Iteration 14/25 | Loss: 0.00567852
Iteration 15/25 | Loss: 0.00567852
Iteration 16/25 | Loss: 0.00567852
Iteration 17/25 | Loss: 0.00567852
Iteration 18/25 | Loss: 0.00567852
Iteration 19/25 | Loss: 0.00567852
Iteration 20/25 | Loss: 0.00567852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005678524728864431, 0.005678524728864431, 0.005678524728864431, 0.005678524728864431, 0.005678524728864431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005678524728864431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00567852
Iteration 2/1000 | Loss: 0.00328743
Iteration 3/1000 | Loss: 0.00145740
Iteration 4/1000 | Loss: 0.01286194
Iteration 5/1000 | Loss: 0.00182295
Iteration 6/1000 | Loss: 0.01497544
Iteration 7/1000 | Loss: 0.00165486
Iteration 8/1000 | Loss: 0.02204800
Iteration 9/1000 | Loss: 0.00154944
Iteration 10/1000 | Loss: 0.01115245
Iteration 11/1000 | Loss: 0.00140689
Iteration 12/1000 | Loss: 0.01449517
Iteration 13/1000 | Loss: 0.00266491
Iteration 14/1000 | Loss: 0.01328472
Iteration 15/1000 | Loss: 0.00312586
Iteration 16/1000 | Loss: 0.01176919
Iteration 17/1000 | Loss: 0.00222423
Iteration 18/1000 | Loss: 0.00671137
Iteration 19/1000 | Loss: 0.00262995
Iteration 20/1000 | Loss: 0.00547334
Iteration 21/1000 | Loss: 0.00735111
Iteration 22/1000 | Loss: 0.00270858
Iteration 23/1000 | Loss: 0.00428974
Iteration 24/1000 | Loss: 0.00684799
Iteration 25/1000 | Loss: 0.00354045
Iteration 26/1000 | Loss: 0.00096856
Iteration 27/1000 | Loss: 0.00493410
Iteration 28/1000 | Loss: 0.00107735
Iteration 29/1000 | Loss: 0.00229052
Iteration 30/1000 | Loss: 0.00031507
Iteration 31/1000 | Loss: 0.00022110
Iteration 32/1000 | Loss: 0.00029533
Iteration 33/1000 | Loss: 0.00101256
Iteration 34/1000 | Loss: 0.00208841
Iteration 35/1000 | Loss: 0.00256258
Iteration 36/1000 | Loss: 0.00182867
Iteration 37/1000 | Loss: 0.00084955
Iteration 38/1000 | Loss: 0.00032653
Iteration 39/1000 | Loss: 0.00013809
Iteration 40/1000 | Loss: 0.00175896
Iteration 41/1000 | Loss: 0.00091638
Iteration 42/1000 | Loss: 0.00087105
Iteration 43/1000 | Loss: 0.00097154
Iteration 44/1000 | Loss: 0.00225253
Iteration 45/1000 | Loss: 0.00107734
Iteration 46/1000 | Loss: 0.00101858
Iteration 47/1000 | Loss: 0.00172434
Iteration 48/1000 | Loss: 0.00080417
Iteration 49/1000 | Loss: 0.00032340
Iteration 50/1000 | Loss: 0.00034794
Iteration 51/1000 | Loss: 0.00019697
Iteration 52/1000 | Loss: 0.00103939
Iteration 53/1000 | Loss: 0.00109428
Iteration 54/1000 | Loss: 0.00012863
Iteration 55/1000 | Loss: 0.00055317
Iteration 56/1000 | Loss: 0.00079478
Iteration 57/1000 | Loss: 0.00121828
Iteration 58/1000 | Loss: 0.00082355
Iteration 59/1000 | Loss: 0.00106706
Iteration 60/1000 | Loss: 0.00079253
Iteration 61/1000 | Loss: 0.00050136
Iteration 62/1000 | Loss: 0.00055242
Iteration 63/1000 | Loss: 0.00009348
Iteration 64/1000 | Loss: 0.00017413
Iteration 65/1000 | Loss: 0.00015837
Iteration 66/1000 | Loss: 0.00017137
Iteration 67/1000 | Loss: 0.00061983
Iteration 68/1000 | Loss: 0.00094997
Iteration 69/1000 | Loss: 0.00229277
Iteration 70/1000 | Loss: 0.00245685
Iteration 71/1000 | Loss: 0.00106731
Iteration 72/1000 | Loss: 0.00036689
Iteration 73/1000 | Loss: 0.00126233
Iteration 74/1000 | Loss: 0.00079746
Iteration 75/1000 | Loss: 0.00040589
Iteration 76/1000 | Loss: 0.00023535
Iteration 77/1000 | Loss: 0.00031562
Iteration 78/1000 | Loss: 0.00042925
Iteration 79/1000 | Loss: 0.00091773
Iteration 80/1000 | Loss: 0.00042865
Iteration 81/1000 | Loss: 0.00040582
Iteration 82/1000 | Loss: 0.00024510
Iteration 83/1000 | Loss: 0.00036711
Iteration 84/1000 | Loss: 0.00061146
Iteration 85/1000 | Loss: 0.00026592
Iteration 86/1000 | Loss: 0.00091817
Iteration 87/1000 | Loss: 0.00023166
Iteration 88/1000 | Loss: 0.00007192
Iteration 89/1000 | Loss: 0.00006599
Iteration 90/1000 | Loss: 0.00006292
Iteration 91/1000 | Loss: 0.00006138
Iteration 92/1000 | Loss: 0.00006029
Iteration 93/1000 | Loss: 0.00005921
Iteration 94/1000 | Loss: 0.00005829
Iteration 95/1000 | Loss: 0.00005733
Iteration 96/1000 | Loss: 0.00005652
Iteration 97/1000 | Loss: 0.00005578
Iteration 98/1000 | Loss: 0.00005512
Iteration 99/1000 | Loss: 0.00005459
Iteration 100/1000 | Loss: 0.00005413
Iteration 101/1000 | Loss: 0.00005370
Iteration 102/1000 | Loss: 0.00169182
Iteration 103/1000 | Loss: 0.00160182
Iteration 104/1000 | Loss: 0.00150677
Iteration 105/1000 | Loss: 0.00098944
Iteration 106/1000 | Loss: 0.00182416
Iteration 107/1000 | Loss: 0.00097563
Iteration 108/1000 | Loss: 0.00115180
Iteration 109/1000 | Loss: 0.00110099
Iteration 110/1000 | Loss: 0.00107007
Iteration 111/1000 | Loss: 0.00105399
Iteration 112/1000 | Loss: 0.00072695
Iteration 113/1000 | Loss: 0.00085536
Iteration 114/1000 | Loss: 0.00114224
Iteration 115/1000 | Loss: 0.00008651
Iteration 116/1000 | Loss: 0.00052498
Iteration 117/1000 | Loss: 0.00159167
Iteration 118/1000 | Loss: 0.00008685
Iteration 119/1000 | Loss: 0.00017978
Iteration 120/1000 | Loss: 0.00085530
Iteration 121/1000 | Loss: 0.00012086
Iteration 122/1000 | Loss: 0.00006300
Iteration 123/1000 | Loss: 0.00005767
Iteration 124/1000 | Loss: 0.00008650
Iteration 125/1000 | Loss: 0.00005652
Iteration 126/1000 | Loss: 0.00005408
Iteration 127/1000 | Loss: 0.00005152
Iteration 128/1000 | Loss: 0.00005044
Iteration 129/1000 | Loss: 0.00004963
Iteration 130/1000 | Loss: 0.00004912
Iteration 131/1000 | Loss: 0.00004892
Iteration 132/1000 | Loss: 0.00004877
Iteration 133/1000 | Loss: 0.00004873
Iteration 134/1000 | Loss: 0.00004869
Iteration 135/1000 | Loss: 0.00004868
Iteration 136/1000 | Loss: 0.00004866
Iteration 137/1000 | Loss: 0.00004865
Iteration 138/1000 | Loss: 0.00004864
Iteration 139/1000 | Loss: 0.00004863
Iteration 140/1000 | Loss: 0.00004862
Iteration 141/1000 | Loss: 0.00004861
Iteration 142/1000 | Loss: 0.00004861
Iteration 143/1000 | Loss: 0.00004860
Iteration 144/1000 | Loss: 0.00004860
Iteration 145/1000 | Loss: 0.00004860
Iteration 146/1000 | Loss: 0.00004859
Iteration 147/1000 | Loss: 0.00004859
Iteration 148/1000 | Loss: 0.00004858
Iteration 149/1000 | Loss: 0.00004858
Iteration 150/1000 | Loss: 0.00004858
Iteration 151/1000 | Loss: 0.00004850
Iteration 152/1000 | Loss: 0.00004847
Iteration 153/1000 | Loss: 0.00004847
Iteration 154/1000 | Loss: 0.00004847
Iteration 155/1000 | Loss: 0.00004847
Iteration 156/1000 | Loss: 0.00004846
Iteration 157/1000 | Loss: 0.00004846
Iteration 158/1000 | Loss: 0.00004845
Iteration 159/1000 | Loss: 0.00004845
Iteration 160/1000 | Loss: 0.00004845
Iteration 161/1000 | Loss: 0.00004845
Iteration 162/1000 | Loss: 0.00004845
Iteration 163/1000 | Loss: 0.00004845
Iteration 164/1000 | Loss: 0.00004845
Iteration 165/1000 | Loss: 0.00004845
Iteration 166/1000 | Loss: 0.00004845
Iteration 167/1000 | Loss: 0.00004845
Iteration 168/1000 | Loss: 0.00004845
Iteration 169/1000 | Loss: 0.00004844
Iteration 170/1000 | Loss: 0.00004844
Iteration 171/1000 | Loss: 0.00004844
Iteration 172/1000 | Loss: 0.00004844
Iteration 173/1000 | Loss: 0.00004844
Iteration 174/1000 | Loss: 0.00004844
Iteration 175/1000 | Loss: 0.00004844
Iteration 176/1000 | Loss: 0.00004844
Iteration 177/1000 | Loss: 0.00004844
Iteration 178/1000 | Loss: 0.00004844
Iteration 179/1000 | Loss: 0.00004844
Iteration 180/1000 | Loss: 0.00004844
Iteration 181/1000 | Loss: 0.00004843
Iteration 182/1000 | Loss: 0.00004843
Iteration 183/1000 | Loss: 0.00004843
Iteration 184/1000 | Loss: 0.00004843
Iteration 185/1000 | Loss: 0.00004843
Iteration 186/1000 | Loss: 0.00004843
Iteration 187/1000 | Loss: 0.00004843
Iteration 188/1000 | Loss: 0.00004843
Iteration 189/1000 | Loss: 0.00004843
Iteration 190/1000 | Loss: 0.00004842
Iteration 191/1000 | Loss: 0.00004842
Iteration 192/1000 | Loss: 0.00004842
Iteration 193/1000 | Loss: 0.00004842
Iteration 194/1000 | Loss: 0.00004842
Iteration 195/1000 | Loss: 0.00004842
Iteration 196/1000 | Loss: 0.00004842
Iteration 197/1000 | Loss: 0.00004842
Iteration 198/1000 | Loss: 0.00004842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [4.842256385018118e-05, 4.842256385018118e-05, 4.842256385018118e-05, 4.842256385018118e-05, 4.842256385018118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.842256385018118e-05

Optimization complete. Final v2v error: 5.799740314483643 mm

Highest mean error: 12.90045166015625 mm for frame 55

Lowest mean error: 4.622387409210205 mm for frame 133

Saving results

Total time: 225.8494164943695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796537
Iteration 2/25 | Loss: 0.00220225
Iteration 3/25 | Loss: 0.00198616
Iteration 4/25 | Loss: 0.00196718
Iteration 5/25 | Loss: 0.00196178
Iteration 6/25 | Loss: 0.00195333
Iteration 7/25 | Loss: 0.00193126
Iteration 8/25 | Loss: 0.00193235
Iteration 9/25 | Loss: 0.00192422
Iteration 10/25 | Loss: 0.00192291
Iteration 11/25 | Loss: 0.00192286
Iteration 12/25 | Loss: 0.00192286
Iteration 13/25 | Loss: 0.00192286
Iteration 14/25 | Loss: 0.00192286
Iteration 15/25 | Loss: 0.00192285
Iteration 16/25 | Loss: 0.00192285
Iteration 17/25 | Loss: 0.00192285
Iteration 18/25 | Loss: 0.00192285
Iteration 19/25 | Loss: 0.00192285
Iteration 20/25 | Loss: 0.00192285
Iteration 21/25 | Loss: 0.00192285
Iteration 22/25 | Loss: 0.00192285
Iteration 23/25 | Loss: 0.00192285
Iteration 24/25 | Loss: 0.00192285
Iteration 25/25 | Loss: 0.00192285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44625020
Iteration 2/25 | Loss: 0.00270412
Iteration 3/25 | Loss: 0.00270388
Iteration 4/25 | Loss: 0.00270388
Iteration 5/25 | Loss: 0.00270388
Iteration 6/25 | Loss: 0.00270387
Iteration 7/25 | Loss: 0.00270388
Iteration 8/25 | Loss: 0.00270388
Iteration 9/25 | Loss: 0.00270388
Iteration 10/25 | Loss: 0.00270388
Iteration 11/25 | Loss: 0.00270388
Iteration 12/25 | Loss: 0.00270388
Iteration 13/25 | Loss: 0.00270388
Iteration 14/25 | Loss: 0.00270388
Iteration 15/25 | Loss: 0.00270388
Iteration 16/25 | Loss: 0.00270388
Iteration 17/25 | Loss: 0.00270388
Iteration 18/25 | Loss: 0.00270388
Iteration 19/25 | Loss: 0.00270388
Iteration 20/25 | Loss: 0.00270388
Iteration 21/25 | Loss: 0.00270388
Iteration 22/25 | Loss: 0.00270388
Iteration 23/25 | Loss: 0.00270388
Iteration 24/25 | Loss: 0.00270388
Iteration 25/25 | Loss: 0.00270388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270388
Iteration 2/1000 | Loss: 0.00010394
Iteration 3/1000 | Loss: 0.00007154
Iteration 4/1000 | Loss: 0.00006414
Iteration 5/1000 | Loss: 0.00005622
Iteration 6/1000 | Loss: 0.00005180
Iteration 7/1000 | Loss: 0.00004888
Iteration 8/1000 | Loss: 0.00004709
Iteration 9/1000 | Loss: 0.00004591
Iteration 10/1000 | Loss: 0.00004511
Iteration 11/1000 | Loss: 0.00004440
Iteration 12/1000 | Loss: 0.00004377
Iteration 13/1000 | Loss: 0.00004322
Iteration 14/1000 | Loss: 0.00004285
Iteration 15/1000 | Loss: 0.00004250
Iteration 16/1000 | Loss: 0.00004227
Iteration 17/1000 | Loss: 0.00004214
Iteration 18/1000 | Loss: 0.00004194
Iteration 19/1000 | Loss: 0.00004186
Iteration 20/1000 | Loss: 0.00004178
Iteration 21/1000 | Loss: 0.00004174
Iteration 22/1000 | Loss: 0.00004173
Iteration 23/1000 | Loss: 0.00004173
Iteration 24/1000 | Loss: 0.00004173
Iteration 25/1000 | Loss: 0.00004172
Iteration 26/1000 | Loss: 0.00004171
Iteration 27/1000 | Loss: 0.00004166
Iteration 28/1000 | Loss: 0.00004166
Iteration 29/1000 | Loss: 0.00004166
Iteration 30/1000 | Loss: 0.00004166
Iteration 31/1000 | Loss: 0.00004166
Iteration 32/1000 | Loss: 0.00004166
Iteration 33/1000 | Loss: 0.00004166
Iteration 34/1000 | Loss: 0.00004166
Iteration 35/1000 | Loss: 0.00004166
Iteration 36/1000 | Loss: 0.00004166
Iteration 37/1000 | Loss: 0.00004166
Iteration 38/1000 | Loss: 0.00004165
Iteration 39/1000 | Loss: 0.00004165
Iteration 40/1000 | Loss: 0.00004165
Iteration 41/1000 | Loss: 0.00004165
Iteration 42/1000 | Loss: 0.00004165
Iteration 43/1000 | Loss: 0.00004165
Iteration 44/1000 | Loss: 0.00004165
Iteration 45/1000 | Loss: 0.00004165
Iteration 46/1000 | Loss: 0.00004164
Iteration 47/1000 | Loss: 0.00004164
Iteration 48/1000 | Loss: 0.00004162
Iteration 49/1000 | Loss: 0.00004161
Iteration 50/1000 | Loss: 0.00004161
Iteration 51/1000 | Loss: 0.00004161
Iteration 52/1000 | Loss: 0.00004161
Iteration 53/1000 | Loss: 0.00004161
Iteration 54/1000 | Loss: 0.00004161
Iteration 55/1000 | Loss: 0.00004161
Iteration 56/1000 | Loss: 0.00004160
Iteration 57/1000 | Loss: 0.00004160
Iteration 58/1000 | Loss: 0.00004159
Iteration 59/1000 | Loss: 0.00004159
Iteration 60/1000 | Loss: 0.00004158
Iteration 61/1000 | Loss: 0.00004158
Iteration 62/1000 | Loss: 0.00004158
Iteration 63/1000 | Loss: 0.00004157
Iteration 64/1000 | Loss: 0.00004157
Iteration 65/1000 | Loss: 0.00004157
Iteration 66/1000 | Loss: 0.00004157
Iteration 67/1000 | Loss: 0.00004157
Iteration 68/1000 | Loss: 0.00004156
Iteration 69/1000 | Loss: 0.00004156
Iteration 70/1000 | Loss: 0.00004156
Iteration 71/1000 | Loss: 0.00004156
Iteration 72/1000 | Loss: 0.00004156
Iteration 73/1000 | Loss: 0.00004156
Iteration 74/1000 | Loss: 0.00004156
Iteration 75/1000 | Loss: 0.00004156
Iteration 76/1000 | Loss: 0.00004156
Iteration 77/1000 | Loss: 0.00004156
Iteration 78/1000 | Loss: 0.00004156
Iteration 79/1000 | Loss: 0.00004155
Iteration 80/1000 | Loss: 0.00004155
Iteration 81/1000 | Loss: 0.00004155
Iteration 82/1000 | Loss: 0.00004155
Iteration 83/1000 | Loss: 0.00004155
Iteration 84/1000 | Loss: 0.00004154
Iteration 85/1000 | Loss: 0.00004154
Iteration 86/1000 | Loss: 0.00004154
Iteration 87/1000 | Loss: 0.00004154
Iteration 88/1000 | Loss: 0.00004154
Iteration 89/1000 | Loss: 0.00004154
Iteration 90/1000 | Loss: 0.00004154
Iteration 91/1000 | Loss: 0.00004154
Iteration 92/1000 | Loss: 0.00004154
Iteration 93/1000 | Loss: 0.00004154
Iteration 94/1000 | Loss: 0.00004154
Iteration 95/1000 | Loss: 0.00004153
Iteration 96/1000 | Loss: 0.00004153
Iteration 97/1000 | Loss: 0.00004153
Iteration 98/1000 | Loss: 0.00004153
Iteration 99/1000 | Loss: 0.00004153
Iteration 100/1000 | Loss: 0.00004153
Iteration 101/1000 | Loss: 0.00004153
Iteration 102/1000 | Loss: 0.00004153
Iteration 103/1000 | Loss: 0.00004153
Iteration 104/1000 | Loss: 0.00004153
Iteration 105/1000 | Loss: 0.00004153
Iteration 106/1000 | Loss: 0.00004153
Iteration 107/1000 | Loss: 0.00004153
Iteration 108/1000 | Loss: 0.00004153
Iteration 109/1000 | Loss: 0.00004153
Iteration 110/1000 | Loss: 0.00004153
Iteration 111/1000 | Loss: 0.00004153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [4.153292320552282e-05, 4.153292320552282e-05, 4.153292320552282e-05, 4.153292320552282e-05, 4.153292320552282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.153292320552282e-05

Optimization complete. Final v2v error: 5.552023887634277 mm

Highest mean error: 7.156187534332275 mm for frame 132

Lowest mean error: 4.77619743347168 mm for frame 171

Saving results

Total time: 59.404173612594604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384359
Iteration 2/25 | Loss: 0.00202558
Iteration 3/25 | Loss: 0.00188647
Iteration 4/25 | Loss: 0.00187470
Iteration 5/25 | Loss: 0.00187151
Iteration 6/25 | Loss: 0.00187063
Iteration 7/25 | Loss: 0.00187063
Iteration 8/25 | Loss: 0.00187063
Iteration 9/25 | Loss: 0.00187063
Iteration 10/25 | Loss: 0.00187063
Iteration 11/25 | Loss: 0.00187063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001870625652372837, 0.001870625652372837, 0.001870625652372837, 0.001870625652372837, 0.001870625652372837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001870625652372837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51450050
Iteration 2/25 | Loss: 0.00255529
Iteration 3/25 | Loss: 0.00255529
Iteration 4/25 | Loss: 0.00255529
Iteration 5/25 | Loss: 0.00255529
Iteration 6/25 | Loss: 0.00255529
Iteration 7/25 | Loss: 0.00255529
Iteration 8/25 | Loss: 0.00255529
Iteration 9/25 | Loss: 0.00255529
Iteration 10/25 | Loss: 0.00255529
Iteration 11/25 | Loss: 0.00255529
Iteration 12/25 | Loss: 0.00255529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002555286046117544, 0.002555286046117544, 0.002555286046117544, 0.002555286046117544, 0.002555286046117544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002555286046117544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255529
Iteration 2/1000 | Loss: 0.00011070
Iteration 3/1000 | Loss: 0.00006814
Iteration 4/1000 | Loss: 0.00005505
Iteration 5/1000 | Loss: 0.00004738
Iteration 6/1000 | Loss: 0.00004400
Iteration 7/1000 | Loss: 0.00004117
Iteration 8/1000 | Loss: 0.00004030
Iteration 9/1000 | Loss: 0.00003952
Iteration 10/1000 | Loss: 0.00003894
Iteration 11/1000 | Loss: 0.00003838
Iteration 12/1000 | Loss: 0.00003789
Iteration 13/1000 | Loss: 0.00003752
Iteration 14/1000 | Loss: 0.00003723
Iteration 15/1000 | Loss: 0.00003702
Iteration 16/1000 | Loss: 0.00003686
Iteration 17/1000 | Loss: 0.00003684
Iteration 18/1000 | Loss: 0.00003679
Iteration 19/1000 | Loss: 0.00003677
Iteration 20/1000 | Loss: 0.00003677
Iteration 21/1000 | Loss: 0.00003677
Iteration 22/1000 | Loss: 0.00003676
Iteration 23/1000 | Loss: 0.00003676
Iteration 24/1000 | Loss: 0.00003675
Iteration 25/1000 | Loss: 0.00003675
Iteration 26/1000 | Loss: 0.00003674
Iteration 27/1000 | Loss: 0.00003673
Iteration 28/1000 | Loss: 0.00003672
Iteration 29/1000 | Loss: 0.00003672
Iteration 30/1000 | Loss: 0.00003671
Iteration 31/1000 | Loss: 0.00003671
Iteration 32/1000 | Loss: 0.00003671
Iteration 33/1000 | Loss: 0.00003670
Iteration 34/1000 | Loss: 0.00003668
Iteration 35/1000 | Loss: 0.00003668
Iteration 36/1000 | Loss: 0.00003667
Iteration 37/1000 | Loss: 0.00003667
Iteration 38/1000 | Loss: 0.00003666
Iteration 39/1000 | Loss: 0.00003666
Iteration 40/1000 | Loss: 0.00003666
Iteration 41/1000 | Loss: 0.00003666
Iteration 42/1000 | Loss: 0.00003666
Iteration 43/1000 | Loss: 0.00003666
Iteration 44/1000 | Loss: 0.00003666
Iteration 45/1000 | Loss: 0.00003665
Iteration 46/1000 | Loss: 0.00003665
Iteration 47/1000 | Loss: 0.00003665
Iteration 48/1000 | Loss: 0.00003665
Iteration 49/1000 | Loss: 0.00003665
Iteration 50/1000 | Loss: 0.00003665
Iteration 51/1000 | Loss: 0.00003665
Iteration 52/1000 | Loss: 0.00003665
Iteration 53/1000 | Loss: 0.00003665
Iteration 54/1000 | Loss: 0.00003665
Iteration 55/1000 | Loss: 0.00003665
Iteration 56/1000 | Loss: 0.00003665
Iteration 57/1000 | Loss: 0.00003665
Iteration 58/1000 | Loss: 0.00003664
Iteration 59/1000 | Loss: 0.00003664
Iteration 60/1000 | Loss: 0.00003664
Iteration 61/1000 | Loss: 0.00003663
Iteration 62/1000 | Loss: 0.00003663
Iteration 63/1000 | Loss: 0.00003663
Iteration 64/1000 | Loss: 0.00003663
Iteration 65/1000 | Loss: 0.00003662
Iteration 66/1000 | Loss: 0.00003662
Iteration 67/1000 | Loss: 0.00003662
Iteration 68/1000 | Loss: 0.00003661
Iteration 69/1000 | Loss: 0.00003661
Iteration 70/1000 | Loss: 0.00003661
Iteration 71/1000 | Loss: 0.00003661
Iteration 72/1000 | Loss: 0.00003661
Iteration 73/1000 | Loss: 0.00003661
Iteration 74/1000 | Loss: 0.00003661
Iteration 75/1000 | Loss: 0.00003661
Iteration 76/1000 | Loss: 0.00003660
Iteration 77/1000 | Loss: 0.00003660
Iteration 78/1000 | Loss: 0.00003660
Iteration 79/1000 | Loss: 0.00003659
Iteration 80/1000 | Loss: 0.00003659
Iteration 81/1000 | Loss: 0.00003659
Iteration 82/1000 | Loss: 0.00003658
Iteration 83/1000 | Loss: 0.00003658
Iteration 84/1000 | Loss: 0.00003658
Iteration 85/1000 | Loss: 0.00003657
Iteration 86/1000 | Loss: 0.00003657
Iteration 87/1000 | Loss: 0.00003657
Iteration 88/1000 | Loss: 0.00003656
Iteration 89/1000 | Loss: 0.00003656
Iteration 90/1000 | Loss: 0.00003655
Iteration 91/1000 | Loss: 0.00003655
Iteration 92/1000 | Loss: 0.00003655
Iteration 93/1000 | Loss: 0.00003654
Iteration 94/1000 | Loss: 0.00003654
Iteration 95/1000 | Loss: 0.00003654
Iteration 96/1000 | Loss: 0.00003654
Iteration 97/1000 | Loss: 0.00003654
Iteration 98/1000 | Loss: 0.00003654
Iteration 99/1000 | Loss: 0.00003654
Iteration 100/1000 | Loss: 0.00003654
Iteration 101/1000 | Loss: 0.00003654
Iteration 102/1000 | Loss: 0.00003654
Iteration 103/1000 | Loss: 0.00003653
Iteration 104/1000 | Loss: 0.00003652
Iteration 105/1000 | Loss: 0.00003652
Iteration 106/1000 | Loss: 0.00003651
Iteration 107/1000 | Loss: 0.00003651
Iteration 108/1000 | Loss: 0.00003651
Iteration 109/1000 | Loss: 0.00003651
Iteration 110/1000 | Loss: 0.00003651
Iteration 111/1000 | Loss: 0.00003651
Iteration 112/1000 | Loss: 0.00003651
Iteration 113/1000 | Loss: 0.00003650
Iteration 114/1000 | Loss: 0.00003650
Iteration 115/1000 | Loss: 0.00003650
Iteration 116/1000 | Loss: 0.00003650
Iteration 117/1000 | Loss: 0.00003649
Iteration 118/1000 | Loss: 0.00003649
Iteration 119/1000 | Loss: 0.00003649
Iteration 120/1000 | Loss: 0.00003649
Iteration 121/1000 | Loss: 0.00003648
Iteration 122/1000 | Loss: 0.00003648
Iteration 123/1000 | Loss: 0.00003648
Iteration 124/1000 | Loss: 0.00003647
Iteration 125/1000 | Loss: 0.00003647
Iteration 126/1000 | Loss: 0.00003647
Iteration 127/1000 | Loss: 0.00003647
Iteration 128/1000 | Loss: 0.00003646
Iteration 129/1000 | Loss: 0.00003646
Iteration 130/1000 | Loss: 0.00003646
Iteration 131/1000 | Loss: 0.00003646
Iteration 132/1000 | Loss: 0.00003646
Iteration 133/1000 | Loss: 0.00003645
Iteration 134/1000 | Loss: 0.00003645
Iteration 135/1000 | Loss: 0.00003645
Iteration 136/1000 | Loss: 0.00003645
Iteration 137/1000 | Loss: 0.00003645
Iteration 138/1000 | Loss: 0.00003645
Iteration 139/1000 | Loss: 0.00003644
Iteration 140/1000 | Loss: 0.00003644
Iteration 141/1000 | Loss: 0.00003644
Iteration 142/1000 | Loss: 0.00003644
Iteration 143/1000 | Loss: 0.00003643
Iteration 144/1000 | Loss: 0.00003643
Iteration 145/1000 | Loss: 0.00003643
Iteration 146/1000 | Loss: 0.00003642
Iteration 147/1000 | Loss: 0.00003642
Iteration 148/1000 | Loss: 0.00003642
Iteration 149/1000 | Loss: 0.00003641
Iteration 150/1000 | Loss: 0.00003641
Iteration 151/1000 | Loss: 0.00003641
Iteration 152/1000 | Loss: 0.00003641
Iteration 153/1000 | Loss: 0.00003640
Iteration 154/1000 | Loss: 0.00003640
Iteration 155/1000 | Loss: 0.00003640
Iteration 156/1000 | Loss: 0.00003640
Iteration 157/1000 | Loss: 0.00003640
Iteration 158/1000 | Loss: 0.00003640
Iteration 159/1000 | Loss: 0.00003640
Iteration 160/1000 | Loss: 0.00003639
Iteration 161/1000 | Loss: 0.00003639
Iteration 162/1000 | Loss: 0.00003639
Iteration 163/1000 | Loss: 0.00003639
Iteration 164/1000 | Loss: 0.00003639
Iteration 165/1000 | Loss: 0.00003639
Iteration 166/1000 | Loss: 0.00003639
Iteration 167/1000 | Loss: 0.00003639
Iteration 168/1000 | Loss: 0.00003639
Iteration 169/1000 | Loss: 0.00003639
Iteration 170/1000 | Loss: 0.00003639
Iteration 171/1000 | Loss: 0.00003639
Iteration 172/1000 | Loss: 0.00003639
Iteration 173/1000 | Loss: 0.00003639
Iteration 174/1000 | Loss: 0.00003638
Iteration 175/1000 | Loss: 0.00003638
Iteration 176/1000 | Loss: 0.00003638
Iteration 177/1000 | Loss: 0.00003638
Iteration 178/1000 | Loss: 0.00003638
Iteration 179/1000 | Loss: 0.00003638
Iteration 180/1000 | Loss: 0.00003638
Iteration 181/1000 | Loss: 0.00003638
Iteration 182/1000 | Loss: 0.00003638
Iteration 183/1000 | Loss: 0.00003638
Iteration 184/1000 | Loss: 0.00003638
Iteration 185/1000 | Loss: 0.00003638
Iteration 186/1000 | Loss: 0.00003638
Iteration 187/1000 | Loss: 0.00003638
Iteration 188/1000 | Loss: 0.00003638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.6382447433425114e-05, 3.6382447433425114e-05, 3.6382447433425114e-05, 3.6382447433425114e-05, 3.6382447433425114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6382447433425114e-05

Optimization complete. Final v2v error: 5.300377368927002 mm

Highest mean error: 5.6708478927612305 mm for frame 192

Lowest mean error: 5.030060768127441 mm for frame 8

Saving results

Total time: 48.70751905441284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926212
Iteration 2/25 | Loss: 0.00190921
Iteration 3/25 | Loss: 0.00181761
Iteration 4/25 | Loss: 0.00180915
Iteration 5/25 | Loss: 0.00180802
Iteration 6/25 | Loss: 0.00180802
Iteration 7/25 | Loss: 0.00180802
Iteration 8/25 | Loss: 0.00180802
Iteration 9/25 | Loss: 0.00180802
Iteration 10/25 | Loss: 0.00180802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0018080229638144374, 0.0018080229638144374, 0.0018080229638144374, 0.0018080229638144374, 0.0018080229638144374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018080229638144374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49486792
Iteration 2/25 | Loss: 0.00221764
Iteration 3/25 | Loss: 0.00221764
Iteration 4/25 | Loss: 0.00221764
Iteration 5/25 | Loss: 0.00221764
Iteration 6/25 | Loss: 0.00221764
Iteration 7/25 | Loss: 0.00221764
Iteration 8/25 | Loss: 0.00221764
Iteration 9/25 | Loss: 0.00221764
Iteration 10/25 | Loss: 0.00221764
Iteration 11/25 | Loss: 0.00221764
Iteration 12/25 | Loss: 0.00221764
Iteration 13/25 | Loss: 0.00221764
Iteration 14/25 | Loss: 0.00221764
Iteration 15/25 | Loss: 0.00221764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022176355123519897, 0.0022176355123519897, 0.0022176355123519897, 0.0022176355123519897, 0.0022176355123519897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022176355123519897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221764
Iteration 2/1000 | Loss: 0.00007222
Iteration 3/1000 | Loss: 0.00004826
Iteration 4/1000 | Loss: 0.00004093
Iteration 5/1000 | Loss: 0.00003726
Iteration 6/1000 | Loss: 0.00003573
Iteration 7/1000 | Loss: 0.00003499
Iteration 8/1000 | Loss: 0.00003460
Iteration 9/1000 | Loss: 0.00003416
Iteration 10/1000 | Loss: 0.00003389
Iteration 11/1000 | Loss: 0.00003380
Iteration 12/1000 | Loss: 0.00003373
Iteration 13/1000 | Loss: 0.00003373
Iteration 14/1000 | Loss: 0.00003368
Iteration 15/1000 | Loss: 0.00003366
Iteration 16/1000 | Loss: 0.00003366
Iteration 17/1000 | Loss: 0.00003365
Iteration 18/1000 | Loss: 0.00003359
Iteration 19/1000 | Loss: 0.00003358
Iteration 20/1000 | Loss: 0.00003356
Iteration 21/1000 | Loss: 0.00003355
Iteration 22/1000 | Loss: 0.00003354
Iteration 23/1000 | Loss: 0.00003353
Iteration 24/1000 | Loss: 0.00003353
Iteration 25/1000 | Loss: 0.00003352
Iteration 26/1000 | Loss: 0.00003352
Iteration 27/1000 | Loss: 0.00003351
Iteration 28/1000 | Loss: 0.00003350
Iteration 29/1000 | Loss: 0.00003350
Iteration 30/1000 | Loss: 0.00003349
Iteration 31/1000 | Loss: 0.00003349
Iteration 32/1000 | Loss: 0.00003349
Iteration 33/1000 | Loss: 0.00003348
Iteration 34/1000 | Loss: 0.00003347
Iteration 35/1000 | Loss: 0.00003347
Iteration 36/1000 | Loss: 0.00003347
Iteration 37/1000 | Loss: 0.00003347
Iteration 38/1000 | Loss: 0.00003347
Iteration 39/1000 | Loss: 0.00003346
Iteration 40/1000 | Loss: 0.00003346
Iteration 41/1000 | Loss: 0.00003346
Iteration 42/1000 | Loss: 0.00003346
Iteration 43/1000 | Loss: 0.00003345
Iteration 44/1000 | Loss: 0.00003345
Iteration 45/1000 | Loss: 0.00003343
Iteration 46/1000 | Loss: 0.00003343
Iteration 47/1000 | Loss: 0.00003342
Iteration 48/1000 | Loss: 0.00003342
Iteration 49/1000 | Loss: 0.00003342
Iteration 50/1000 | Loss: 0.00003341
Iteration 51/1000 | Loss: 0.00003341
Iteration 52/1000 | Loss: 0.00003341
Iteration 53/1000 | Loss: 0.00003340
Iteration 54/1000 | Loss: 0.00003340
Iteration 55/1000 | Loss: 0.00003340
Iteration 56/1000 | Loss: 0.00003339
Iteration 57/1000 | Loss: 0.00003339
Iteration 58/1000 | Loss: 0.00003339
Iteration 59/1000 | Loss: 0.00003338
Iteration 60/1000 | Loss: 0.00003338
Iteration 61/1000 | Loss: 0.00003338
Iteration 62/1000 | Loss: 0.00003338
Iteration 63/1000 | Loss: 0.00003338
Iteration 64/1000 | Loss: 0.00003338
Iteration 65/1000 | Loss: 0.00003338
Iteration 66/1000 | Loss: 0.00003338
Iteration 67/1000 | Loss: 0.00003338
Iteration 68/1000 | Loss: 0.00003338
Iteration 69/1000 | Loss: 0.00003338
Iteration 70/1000 | Loss: 0.00003337
Iteration 71/1000 | Loss: 0.00003337
Iteration 72/1000 | Loss: 0.00003337
Iteration 73/1000 | Loss: 0.00003337
Iteration 74/1000 | Loss: 0.00003337
Iteration 75/1000 | Loss: 0.00003337
Iteration 76/1000 | Loss: 0.00003336
Iteration 77/1000 | Loss: 0.00003336
Iteration 78/1000 | Loss: 0.00003336
Iteration 79/1000 | Loss: 0.00003336
Iteration 80/1000 | Loss: 0.00003336
Iteration 81/1000 | Loss: 0.00003335
Iteration 82/1000 | Loss: 0.00003335
Iteration 83/1000 | Loss: 0.00003335
Iteration 84/1000 | Loss: 0.00003335
Iteration 85/1000 | Loss: 0.00003335
Iteration 86/1000 | Loss: 0.00003335
Iteration 87/1000 | Loss: 0.00003335
Iteration 88/1000 | Loss: 0.00003335
Iteration 89/1000 | Loss: 0.00003335
Iteration 90/1000 | Loss: 0.00003335
Iteration 91/1000 | Loss: 0.00003335
Iteration 92/1000 | Loss: 0.00003335
Iteration 93/1000 | Loss: 0.00003335
Iteration 94/1000 | Loss: 0.00003335
Iteration 95/1000 | Loss: 0.00003335
Iteration 96/1000 | Loss: 0.00003335
Iteration 97/1000 | Loss: 0.00003334
Iteration 98/1000 | Loss: 0.00003334
Iteration 99/1000 | Loss: 0.00003334
Iteration 100/1000 | Loss: 0.00003334
Iteration 101/1000 | Loss: 0.00003334
Iteration 102/1000 | Loss: 0.00003334
Iteration 103/1000 | Loss: 0.00003334
Iteration 104/1000 | Loss: 0.00003334
Iteration 105/1000 | Loss: 0.00003334
Iteration 106/1000 | Loss: 0.00003334
Iteration 107/1000 | Loss: 0.00003334
Iteration 108/1000 | Loss: 0.00003334
Iteration 109/1000 | Loss: 0.00003334
Iteration 110/1000 | Loss: 0.00003334
Iteration 111/1000 | Loss: 0.00003334
Iteration 112/1000 | Loss: 0.00003333
Iteration 113/1000 | Loss: 0.00003333
Iteration 114/1000 | Loss: 0.00003333
Iteration 115/1000 | Loss: 0.00003333
Iteration 116/1000 | Loss: 0.00003333
Iteration 117/1000 | Loss: 0.00003333
Iteration 118/1000 | Loss: 0.00003333
Iteration 119/1000 | Loss: 0.00003333
Iteration 120/1000 | Loss: 0.00003333
Iteration 121/1000 | Loss: 0.00003333
Iteration 122/1000 | Loss: 0.00003333
Iteration 123/1000 | Loss: 0.00003332
Iteration 124/1000 | Loss: 0.00003332
Iteration 125/1000 | Loss: 0.00003332
Iteration 126/1000 | Loss: 0.00003332
Iteration 127/1000 | Loss: 0.00003332
Iteration 128/1000 | Loss: 0.00003332
Iteration 129/1000 | Loss: 0.00003332
Iteration 130/1000 | Loss: 0.00003332
Iteration 131/1000 | Loss: 0.00003332
Iteration 132/1000 | Loss: 0.00003332
Iteration 133/1000 | Loss: 0.00003332
Iteration 134/1000 | Loss: 0.00003332
Iteration 135/1000 | Loss: 0.00003332
Iteration 136/1000 | Loss: 0.00003332
Iteration 137/1000 | Loss: 0.00003332
Iteration 138/1000 | Loss: 0.00003332
Iteration 139/1000 | Loss: 0.00003332
Iteration 140/1000 | Loss: 0.00003332
Iteration 141/1000 | Loss: 0.00003332
Iteration 142/1000 | Loss: 0.00003332
Iteration 143/1000 | Loss: 0.00003332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.331559491925873e-05, 3.331559491925873e-05, 3.331559491925873e-05, 3.331559491925873e-05, 3.331559491925873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.331559491925873e-05

Optimization complete. Final v2v error: 5.046642303466797 mm

Highest mean error: 5.3885273933410645 mm for frame 109

Lowest mean error: 4.777358531951904 mm for frame 27

Saving results

Total time: 34.000911235809326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160833
Iteration 2/25 | Loss: 0.00249395
Iteration 3/25 | Loss: 0.00220559
Iteration 4/25 | Loss: 0.00226865
Iteration 5/25 | Loss: 0.00208973
Iteration 6/25 | Loss: 0.00203364
Iteration 7/25 | Loss: 0.00197270
Iteration 8/25 | Loss: 0.00194035
Iteration 9/25 | Loss: 0.00193427
Iteration 10/25 | Loss: 0.00191927
Iteration 11/25 | Loss: 0.00191209
Iteration 12/25 | Loss: 0.00191105
Iteration 13/25 | Loss: 0.00191306
Iteration 14/25 | Loss: 0.00190304
Iteration 15/25 | Loss: 0.00190629
Iteration 16/25 | Loss: 0.00190417
Iteration 17/25 | Loss: 0.00190631
Iteration 18/25 | Loss: 0.00190451
Iteration 19/25 | Loss: 0.00190507
Iteration 20/25 | Loss: 0.00190462
Iteration 21/25 | Loss: 0.00190473
Iteration 22/25 | Loss: 0.00190429
Iteration 23/25 | Loss: 0.00190360
Iteration 24/25 | Loss: 0.00190346
Iteration 25/25 | Loss: 0.00190394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95444024
Iteration 2/25 | Loss: 0.00280969
Iteration 3/25 | Loss: 0.00280967
Iteration 4/25 | Loss: 0.00280967
Iteration 5/25 | Loss: 0.00280967
Iteration 6/25 | Loss: 0.00280967
Iteration 7/25 | Loss: 0.00280967
Iteration 8/25 | Loss: 0.00280967
Iteration 9/25 | Loss: 0.00280967
Iteration 10/25 | Loss: 0.00280967
Iteration 11/25 | Loss: 0.00280967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0028096658643335104, 0.0028096658643335104, 0.0028096658643335104, 0.0028096658643335104, 0.0028096658643335104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028096658643335104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280967
Iteration 2/1000 | Loss: 0.00014192
Iteration 3/1000 | Loss: 0.00009479
Iteration 4/1000 | Loss: 0.00009308
Iteration 5/1000 | Loss: 0.00008176
Iteration 6/1000 | Loss: 0.00008505
Iteration 7/1000 | Loss: 0.00007687
Iteration 8/1000 | Loss: 0.00029419
Iteration 9/1000 | Loss: 0.00011689
Iteration 10/1000 | Loss: 0.00009202
Iteration 11/1000 | Loss: 0.00008410
Iteration 12/1000 | Loss: 0.00008986
Iteration 13/1000 | Loss: 0.00008138
Iteration 14/1000 | Loss: 0.00008537
Iteration 15/1000 | Loss: 0.00008604
Iteration 16/1000 | Loss: 0.00008263
Iteration 17/1000 | Loss: 0.00008564
Iteration 18/1000 | Loss: 0.00008052
Iteration 19/1000 | Loss: 0.00008796
Iteration 20/1000 | Loss: 0.00010121
Iteration 21/1000 | Loss: 0.00010182
Iteration 22/1000 | Loss: 0.00009505
Iteration 23/1000 | Loss: 0.00007766
Iteration 24/1000 | Loss: 0.00007856
Iteration 25/1000 | Loss: 0.00007881
Iteration 26/1000 | Loss: 0.00008382
Iteration 27/1000 | Loss: 0.00008794
Iteration 28/1000 | Loss: 0.00009612
Iteration 29/1000 | Loss: 0.00008733
Iteration 30/1000 | Loss: 0.00007944
Iteration 31/1000 | Loss: 0.00008789
Iteration 32/1000 | Loss: 0.00007852
Iteration 33/1000 | Loss: 0.00008785
Iteration 34/1000 | Loss: 0.00008241
Iteration 35/1000 | Loss: 0.00008950
Iteration 36/1000 | Loss: 0.00008170
Iteration 37/1000 | Loss: 0.00009113
Iteration 38/1000 | Loss: 0.00009226
Iteration 39/1000 | Loss: 0.00009410
Iteration 40/1000 | Loss: 0.00008095
Iteration 41/1000 | Loss: 0.00009763
Iteration 42/1000 | Loss: 0.00006411
Iteration 43/1000 | Loss: 0.00005425
Iteration 44/1000 | Loss: 0.00005274
Iteration 45/1000 | Loss: 0.00007868
Iteration 46/1000 | Loss: 0.00006674
Iteration 47/1000 | Loss: 0.00008303
Iteration 48/1000 | Loss: 0.00008073
Iteration 49/1000 | Loss: 0.00005493
Iteration 50/1000 | Loss: 0.00008231
Iteration 51/1000 | Loss: 0.00007101
Iteration 52/1000 | Loss: 0.00007645
Iteration 53/1000 | Loss: 0.00005371
Iteration 54/1000 | Loss: 0.00007144
Iteration 55/1000 | Loss: 0.00006330
Iteration 56/1000 | Loss: 0.00005266
Iteration 57/1000 | Loss: 0.00005245
Iteration 58/1000 | Loss: 0.00005241
Iteration 59/1000 | Loss: 0.00005241
Iteration 60/1000 | Loss: 0.00005241
Iteration 61/1000 | Loss: 0.00005241
Iteration 62/1000 | Loss: 0.00005241
Iteration 63/1000 | Loss: 0.00005241
Iteration 64/1000 | Loss: 0.00005240
Iteration 65/1000 | Loss: 0.00005240
Iteration 66/1000 | Loss: 0.00005238
Iteration 67/1000 | Loss: 0.00005232
Iteration 68/1000 | Loss: 0.00005232
Iteration 69/1000 | Loss: 0.00005231
Iteration 70/1000 | Loss: 0.00005231
Iteration 71/1000 | Loss: 0.00005227
Iteration 72/1000 | Loss: 0.00005227
Iteration 73/1000 | Loss: 0.00005227
Iteration 74/1000 | Loss: 0.00005226
Iteration 75/1000 | Loss: 0.00005224
Iteration 76/1000 | Loss: 0.00005223
Iteration 77/1000 | Loss: 0.00005222
Iteration 78/1000 | Loss: 0.00005218
Iteration 79/1000 | Loss: 0.00005190
Iteration 80/1000 | Loss: 0.00005158
Iteration 81/1000 | Loss: 0.00005126
Iteration 82/1000 | Loss: 0.00005114
Iteration 83/1000 | Loss: 0.00005114
Iteration 84/1000 | Loss: 0.00005112
Iteration 85/1000 | Loss: 0.00005112
Iteration 86/1000 | Loss: 0.00005111
Iteration 87/1000 | Loss: 0.00005111
Iteration 88/1000 | Loss: 0.00005110
Iteration 89/1000 | Loss: 0.00005110
Iteration 90/1000 | Loss: 0.00005110
Iteration 91/1000 | Loss: 0.00005109
Iteration 92/1000 | Loss: 0.00005109
Iteration 93/1000 | Loss: 0.00005109
Iteration 94/1000 | Loss: 0.00005109
Iteration 95/1000 | Loss: 0.00005109
Iteration 96/1000 | Loss: 0.00005108
Iteration 97/1000 | Loss: 0.00005108
Iteration 98/1000 | Loss: 0.00005108
Iteration 99/1000 | Loss: 0.00005108
Iteration 100/1000 | Loss: 0.00005108
Iteration 101/1000 | Loss: 0.00005108
Iteration 102/1000 | Loss: 0.00005108
Iteration 103/1000 | Loss: 0.00005108
Iteration 104/1000 | Loss: 0.00005108
Iteration 105/1000 | Loss: 0.00005108
Iteration 106/1000 | Loss: 0.00005107
Iteration 107/1000 | Loss: 0.00005107
Iteration 108/1000 | Loss: 0.00005107
Iteration 109/1000 | Loss: 0.00005107
Iteration 110/1000 | Loss: 0.00005107
Iteration 111/1000 | Loss: 0.00005107
Iteration 112/1000 | Loss: 0.00005107
Iteration 113/1000 | Loss: 0.00005107
Iteration 114/1000 | Loss: 0.00005106
Iteration 115/1000 | Loss: 0.00005106
Iteration 116/1000 | Loss: 0.00005106
Iteration 117/1000 | Loss: 0.00005106
Iteration 118/1000 | Loss: 0.00005106
Iteration 119/1000 | Loss: 0.00005105
Iteration 120/1000 | Loss: 0.00005105
Iteration 121/1000 | Loss: 0.00005105
Iteration 122/1000 | Loss: 0.00005105
Iteration 123/1000 | Loss: 0.00005105
Iteration 124/1000 | Loss: 0.00005105
Iteration 125/1000 | Loss: 0.00005105
Iteration 126/1000 | Loss: 0.00005105
Iteration 127/1000 | Loss: 0.00005105
Iteration 128/1000 | Loss: 0.00005105
Iteration 129/1000 | Loss: 0.00005105
Iteration 130/1000 | Loss: 0.00005105
Iteration 131/1000 | Loss: 0.00005105
Iteration 132/1000 | Loss: 0.00005105
Iteration 133/1000 | Loss: 0.00005105
Iteration 134/1000 | Loss: 0.00005105
Iteration 135/1000 | Loss: 0.00005105
Iteration 136/1000 | Loss: 0.00005105
Iteration 137/1000 | Loss: 0.00005105
Iteration 138/1000 | Loss: 0.00005105
Iteration 139/1000 | Loss: 0.00005105
Iteration 140/1000 | Loss: 0.00005105
Iteration 141/1000 | Loss: 0.00005105
Iteration 142/1000 | Loss: 0.00005105
Iteration 143/1000 | Loss: 0.00005105
Iteration 144/1000 | Loss: 0.00005105
Iteration 145/1000 | Loss: 0.00005105
Iteration 146/1000 | Loss: 0.00005105
Iteration 147/1000 | Loss: 0.00005105
Iteration 148/1000 | Loss: 0.00005105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [5.105145464767702e-05, 5.105145464767702e-05, 5.105145464767702e-05, 5.105145464767702e-05, 5.105145464767702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.105145464767702e-05

Optimization complete. Final v2v error: 6.186399936676025 mm

Highest mean error: 11.846238136291504 mm for frame 67

Lowest mean error: 5.861748218536377 mm for frame 77

Saving results

Total time: 156.10757875442505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052889
Iteration 2/25 | Loss: 0.00223025
Iteration 3/25 | Loss: 0.00163808
Iteration 4/25 | Loss: 0.00148417
Iteration 5/25 | Loss: 0.00139899
Iteration 6/25 | Loss: 0.00137479
Iteration 7/25 | Loss: 0.00136562
Iteration 8/25 | Loss: 0.00134328
Iteration 9/25 | Loss: 0.00135592
Iteration 10/25 | Loss: 0.00142483
Iteration 11/25 | Loss: 0.00140199
Iteration 12/25 | Loss: 0.00135914
Iteration 13/25 | Loss: 0.00134527
Iteration 14/25 | Loss: 0.00135349
Iteration 15/25 | Loss: 0.00128768
Iteration 16/25 | Loss: 0.00126429
Iteration 17/25 | Loss: 0.00125930
Iteration 18/25 | Loss: 0.00125660
Iteration 19/25 | Loss: 0.00125463
Iteration 20/25 | Loss: 0.00125340
Iteration 21/25 | Loss: 0.00125282
Iteration 22/25 | Loss: 0.00125220
Iteration 23/25 | Loss: 0.00125179
Iteration 24/25 | Loss: 0.00125147
Iteration 25/25 | Loss: 0.00125125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46563303
Iteration 2/25 | Loss: 0.00128240
Iteration 3/25 | Loss: 0.00128240
Iteration 4/25 | Loss: 0.00128240
Iteration 5/25 | Loss: 0.00128240
Iteration 6/25 | Loss: 0.00128240
Iteration 7/25 | Loss: 0.00128240
Iteration 8/25 | Loss: 0.00128240
Iteration 9/25 | Loss: 0.00128240
Iteration 10/25 | Loss: 0.00128240
Iteration 11/25 | Loss: 0.00128240
Iteration 12/25 | Loss: 0.00128240
Iteration 13/25 | Loss: 0.00128240
Iteration 14/25 | Loss: 0.00128240
Iteration 15/25 | Loss: 0.00128240
Iteration 16/25 | Loss: 0.00128240
Iteration 17/25 | Loss: 0.00128240
Iteration 18/25 | Loss: 0.00128240
Iteration 19/25 | Loss: 0.00128240
Iteration 20/25 | Loss: 0.00128240
Iteration 21/25 | Loss: 0.00128240
Iteration 22/25 | Loss: 0.00128240
Iteration 23/25 | Loss: 0.00128240
Iteration 24/25 | Loss: 0.00128240
Iteration 25/25 | Loss: 0.00128240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128240
Iteration 2/1000 | Loss: 0.00021398
Iteration 3/1000 | Loss: 0.00012191
Iteration 4/1000 | Loss: 0.00009988
Iteration 5/1000 | Loss: 0.00008995
Iteration 6/1000 | Loss: 0.00008419
Iteration 7/1000 | Loss: 0.00008190
Iteration 8/1000 | Loss: 0.00008007
Iteration 9/1000 | Loss: 0.00022355
Iteration 10/1000 | Loss: 0.00008624
Iteration 11/1000 | Loss: 0.00008013
Iteration 12/1000 | Loss: 0.00007771
Iteration 13/1000 | Loss: 0.00007654
Iteration 14/1000 | Loss: 0.00007568
Iteration 15/1000 | Loss: 0.00007526
Iteration 16/1000 | Loss: 0.00007497
Iteration 17/1000 | Loss: 0.00007467
Iteration 18/1000 | Loss: 0.00007456
Iteration 19/1000 | Loss: 0.00007455
Iteration 20/1000 | Loss: 0.00007454
Iteration 21/1000 | Loss: 0.00007453
Iteration 22/1000 | Loss: 0.00007452
Iteration 23/1000 | Loss: 0.00007449
Iteration 24/1000 | Loss: 0.00007437
Iteration 25/1000 | Loss: 0.00007427
Iteration 26/1000 | Loss: 0.00007415
Iteration 27/1000 | Loss: 0.00007414
Iteration 28/1000 | Loss: 0.00007407
Iteration 29/1000 | Loss: 0.00007407
Iteration 30/1000 | Loss: 0.00007405
Iteration 31/1000 | Loss: 0.00007405
Iteration 32/1000 | Loss: 0.00007403
Iteration 33/1000 | Loss: 0.00007402
Iteration 34/1000 | Loss: 0.00007402
Iteration 35/1000 | Loss: 0.00007402
Iteration 36/1000 | Loss: 0.00007401
Iteration 37/1000 | Loss: 0.00007401
Iteration 38/1000 | Loss: 0.00007400
Iteration 39/1000 | Loss: 0.00007400
Iteration 40/1000 | Loss: 0.00007400
Iteration 41/1000 | Loss: 0.00007399
Iteration 42/1000 | Loss: 0.00007399
Iteration 43/1000 | Loss: 0.00007399
Iteration 44/1000 | Loss: 0.00007398
Iteration 45/1000 | Loss: 0.00007398
Iteration 46/1000 | Loss: 0.00007398
Iteration 47/1000 | Loss: 0.00007397
Iteration 48/1000 | Loss: 0.00007397
Iteration 49/1000 | Loss: 0.00007396
Iteration 50/1000 | Loss: 0.00007396
Iteration 51/1000 | Loss: 0.00007396
Iteration 52/1000 | Loss: 0.00007395
Iteration 53/1000 | Loss: 0.00007395
Iteration 54/1000 | Loss: 0.00007395
Iteration 55/1000 | Loss: 0.00007395
Iteration 56/1000 | Loss: 0.00007395
Iteration 57/1000 | Loss: 0.00007395
Iteration 58/1000 | Loss: 0.00007395
Iteration 59/1000 | Loss: 0.00007395
Iteration 60/1000 | Loss: 0.00007395
Iteration 61/1000 | Loss: 0.00007395
Iteration 62/1000 | Loss: 0.00007394
Iteration 63/1000 | Loss: 0.00007394
Iteration 64/1000 | Loss: 0.00007394
Iteration 65/1000 | Loss: 0.00007394
Iteration 66/1000 | Loss: 0.00007394
Iteration 67/1000 | Loss: 0.00007394
Iteration 68/1000 | Loss: 0.00007394
Iteration 69/1000 | Loss: 0.00007393
Iteration 70/1000 | Loss: 0.00007393
Iteration 71/1000 | Loss: 0.00007393
Iteration 72/1000 | Loss: 0.00007393
Iteration 73/1000 | Loss: 0.00007393
Iteration 74/1000 | Loss: 0.00007393
Iteration 75/1000 | Loss: 0.00007393
Iteration 76/1000 | Loss: 0.00007393
Iteration 77/1000 | Loss: 0.00007393
Iteration 78/1000 | Loss: 0.00007393
Iteration 79/1000 | Loss: 0.00007393
Iteration 80/1000 | Loss: 0.00007392
Iteration 81/1000 | Loss: 0.00007392
Iteration 82/1000 | Loss: 0.00007392
Iteration 83/1000 | Loss: 0.00007392
Iteration 84/1000 | Loss: 0.00007392
Iteration 85/1000 | Loss: 0.00007392
Iteration 86/1000 | Loss: 0.00007391
Iteration 87/1000 | Loss: 0.00007391
Iteration 88/1000 | Loss: 0.00007391
Iteration 89/1000 | Loss: 0.00007391
Iteration 90/1000 | Loss: 0.00007391
Iteration 91/1000 | Loss: 0.00007391
Iteration 92/1000 | Loss: 0.00007390
Iteration 93/1000 | Loss: 0.00007390
Iteration 94/1000 | Loss: 0.00007390
Iteration 95/1000 | Loss: 0.00007390
Iteration 96/1000 | Loss: 0.00007390
Iteration 97/1000 | Loss: 0.00007390
Iteration 98/1000 | Loss: 0.00007390
Iteration 99/1000 | Loss: 0.00007389
Iteration 100/1000 | Loss: 0.00007389
Iteration 101/1000 | Loss: 0.00007389
Iteration 102/1000 | Loss: 0.00007389
Iteration 103/1000 | Loss: 0.00007389
Iteration 104/1000 | Loss: 0.00007389
Iteration 105/1000 | Loss: 0.00007389
Iteration 106/1000 | Loss: 0.00007389
Iteration 107/1000 | Loss: 0.00007388
Iteration 108/1000 | Loss: 0.00007388
Iteration 109/1000 | Loss: 0.00007388
Iteration 110/1000 | Loss: 0.00007388
Iteration 111/1000 | Loss: 0.00007388
Iteration 112/1000 | Loss: 0.00007388
Iteration 113/1000 | Loss: 0.00007388
Iteration 114/1000 | Loss: 0.00007388
Iteration 115/1000 | Loss: 0.00007388
Iteration 116/1000 | Loss: 0.00007388
Iteration 117/1000 | Loss: 0.00007388
Iteration 118/1000 | Loss: 0.00007388
Iteration 119/1000 | Loss: 0.00007388
Iteration 120/1000 | Loss: 0.00007388
Iteration 121/1000 | Loss: 0.00007388
Iteration 122/1000 | Loss: 0.00007388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [7.388262747554109e-05, 7.388262747554109e-05, 7.388262747554109e-05, 7.388262747554109e-05, 7.388262747554109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.388262747554109e-05

Optimization complete. Final v2v error: 6.585694789886475 mm

Highest mean error: 17.463787078857422 mm for frame 204

Lowest mean error: 5.596759796142578 mm for frame 209

Saving results

Total time: 91.81779336929321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755197
Iteration 2/25 | Loss: 0.00205005
Iteration 3/25 | Loss: 0.00188009
Iteration 4/25 | Loss: 0.00186248
Iteration 5/25 | Loss: 0.00185800
Iteration 6/25 | Loss: 0.00185650
Iteration 7/25 | Loss: 0.00185639
Iteration 8/25 | Loss: 0.00185639
Iteration 9/25 | Loss: 0.00185639
Iteration 10/25 | Loss: 0.00185639
Iteration 11/25 | Loss: 0.00185639
Iteration 12/25 | Loss: 0.00185639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001856394112110138, 0.001856394112110138, 0.001856394112110138, 0.001856394112110138, 0.001856394112110138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001856394112110138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.97406197
Iteration 2/25 | Loss: 0.00241475
Iteration 3/25 | Loss: 0.00241462
Iteration 4/25 | Loss: 0.00241462
Iteration 5/25 | Loss: 0.00241462
Iteration 6/25 | Loss: 0.00241462
Iteration 7/25 | Loss: 0.00241462
Iteration 8/25 | Loss: 0.00241461
Iteration 9/25 | Loss: 0.00241461
Iteration 10/25 | Loss: 0.00241461
Iteration 11/25 | Loss: 0.00241461
Iteration 12/25 | Loss: 0.00241461
Iteration 13/25 | Loss: 0.00241461
Iteration 14/25 | Loss: 0.00241461
Iteration 15/25 | Loss: 0.00241461
Iteration 16/25 | Loss: 0.00241461
Iteration 17/25 | Loss: 0.00241461
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024146141950041056, 0.0024146141950041056, 0.0024146141950041056, 0.0024146141950041056, 0.0024146141950041056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024146141950041056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241461
Iteration 2/1000 | Loss: 0.00009579
Iteration 3/1000 | Loss: 0.00006370
Iteration 4/1000 | Loss: 0.00005566
Iteration 5/1000 | Loss: 0.00004974
Iteration 6/1000 | Loss: 0.00004675
Iteration 7/1000 | Loss: 0.00004476
Iteration 8/1000 | Loss: 0.00004378
Iteration 9/1000 | Loss: 0.00004320
Iteration 10/1000 | Loss: 0.00004286
Iteration 11/1000 | Loss: 0.00004247
Iteration 12/1000 | Loss: 0.00004215
Iteration 13/1000 | Loss: 0.00004200
Iteration 14/1000 | Loss: 0.00004194
Iteration 15/1000 | Loss: 0.00004190
Iteration 16/1000 | Loss: 0.00004184
Iteration 17/1000 | Loss: 0.00004174
Iteration 18/1000 | Loss: 0.00004169
Iteration 19/1000 | Loss: 0.00004169
Iteration 20/1000 | Loss: 0.00004169
Iteration 21/1000 | Loss: 0.00004167
Iteration 22/1000 | Loss: 0.00004167
Iteration 23/1000 | Loss: 0.00004165
Iteration 24/1000 | Loss: 0.00004164
Iteration 25/1000 | Loss: 0.00004162
Iteration 26/1000 | Loss: 0.00004161
Iteration 27/1000 | Loss: 0.00004161
Iteration 28/1000 | Loss: 0.00004160
Iteration 29/1000 | Loss: 0.00004159
Iteration 30/1000 | Loss: 0.00004159
Iteration 31/1000 | Loss: 0.00004159
Iteration 32/1000 | Loss: 0.00004158
Iteration 33/1000 | Loss: 0.00004158
Iteration 34/1000 | Loss: 0.00004158
Iteration 35/1000 | Loss: 0.00004157
Iteration 36/1000 | Loss: 0.00004157
Iteration 37/1000 | Loss: 0.00004157
Iteration 38/1000 | Loss: 0.00004156
Iteration 39/1000 | Loss: 0.00004156
Iteration 40/1000 | Loss: 0.00004156
Iteration 41/1000 | Loss: 0.00004155
Iteration 42/1000 | Loss: 0.00004155
Iteration 43/1000 | Loss: 0.00004154
Iteration 44/1000 | Loss: 0.00004154
Iteration 45/1000 | Loss: 0.00004154
Iteration 46/1000 | Loss: 0.00004153
Iteration 47/1000 | Loss: 0.00004153
Iteration 48/1000 | Loss: 0.00004153
Iteration 49/1000 | Loss: 0.00004153
Iteration 50/1000 | Loss: 0.00004152
Iteration 51/1000 | Loss: 0.00004152
Iteration 52/1000 | Loss: 0.00004152
Iteration 53/1000 | Loss: 0.00004152
Iteration 54/1000 | Loss: 0.00004152
Iteration 55/1000 | Loss: 0.00004152
Iteration 56/1000 | Loss: 0.00004151
Iteration 57/1000 | Loss: 0.00004151
Iteration 58/1000 | Loss: 0.00004151
Iteration 59/1000 | Loss: 0.00004151
Iteration 60/1000 | Loss: 0.00004150
Iteration 61/1000 | Loss: 0.00004150
Iteration 62/1000 | Loss: 0.00004150
Iteration 63/1000 | Loss: 0.00004150
Iteration 64/1000 | Loss: 0.00004150
Iteration 65/1000 | Loss: 0.00004150
Iteration 66/1000 | Loss: 0.00004150
Iteration 67/1000 | Loss: 0.00004149
Iteration 68/1000 | Loss: 0.00004149
Iteration 69/1000 | Loss: 0.00004149
Iteration 70/1000 | Loss: 0.00004149
Iteration 71/1000 | Loss: 0.00004149
Iteration 72/1000 | Loss: 0.00004149
Iteration 73/1000 | Loss: 0.00004149
Iteration 74/1000 | Loss: 0.00004149
Iteration 75/1000 | Loss: 0.00004149
Iteration 76/1000 | Loss: 0.00004149
Iteration 77/1000 | Loss: 0.00004149
Iteration 78/1000 | Loss: 0.00004149
Iteration 79/1000 | Loss: 0.00004149
Iteration 80/1000 | Loss: 0.00004149
Iteration 81/1000 | Loss: 0.00004148
Iteration 82/1000 | Loss: 0.00004148
Iteration 83/1000 | Loss: 0.00004148
Iteration 84/1000 | Loss: 0.00004148
Iteration 85/1000 | Loss: 0.00004148
Iteration 86/1000 | Loss: 0.00004148
Iteration 87/1000 | Loss: 0.00004148
Iteration 88/1000 | Loss: 0.00004148
Iteration 89/1000 | Loss: 0.00004148
Iteration 90/1000 | Loss: 0.00004148
Iteration 91/1000 | Loss: 0.00004148
Iteration 92/1000 | Loss: 0.00004148
Iteration 93/1000 | Loss: 0.00004148
Iteration 94/1000 | Loss: 0.00004148
Iteration 95/1000 | Loss: 0.00004148
Iteration 96/1000 | Loss: 0.00004147
Iteration 97/1000 | Loss: 0.00004147
Iteration 98/1000 | Loss: 0.00004147
Iteration 99/1000 | Loss: 0.00004147
Iteration 100/1000 | Loss: 0.00004147
Iteration 101/1000 | Loss: 0.00004147
Iteration 102/1000 | Loss: 0.00004147
Iteration 103/1000 | Loss: 0.00004147
Iteration 104/1000 | Loss: 0.00004147
Iteration 105/1000 | Loss: 0.00004147
Iteration 106/1000 | Loss: 0.00004147
Iteration 107/1000 | Loss: 0.00004146
Iteration 108/1000 | Loss: 0.00004146
Iteration 109/1000 | Loss: 0.00004146
Iteration 110/1000 | Loss: 0.00004146
Iteration 111/1000 | Loss: 0.00004146
Iteration 112/1000 | Loss: 0.00004146
Iteration 113/1000 | Loss: 0.00004146
Iteration 114/1000 | Loss: 0.00004146
Iteration 115/1000 | Loss: 0.00004146
Iteration 116/1000 | Loss: 0.00004146
Iteration 117/1000 | Loss: 0.00004146
Iteration 118/1000 | Loss: 0.00004146
Iteration 119/1000 | Loss: 0.00004146
Iteration 120/1000 | Loss: 0.00004146
Iteration 121/1000 | Loss: 0.00004146
Iteration 122/1000 | Loss: 0.00004146
Iteration 123/1000 | Loss: 0.00004146
Iteration 124/1000 | Loss: 0.00004146
Iteration 125/1000 | Loss: 0.00004146
Iteration 126/1000 | Loss: 0.00004146
Iteration 127/1000 | Loss: 0.00004146
Iteration 128/1000 | Loss: 0.00004146
Iteration 129/1000 | Loss: 0.00004146
Iteration 130/1000 | Loss: 0.00004146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [4.145988350501284e-05, 4.145988350501284e-05, 4.145988350501284e-05, 4.145988350501284e-05, 4.145988350501284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.145988350501284e-05

Optimization complete. Final v2v error: 5.6316986083984375 mm

Highest mean error: 6.3311991691589355 mm for frame 128

Lowest mean error: 5.121342658996582 mm for frame 24

Saving results

Total time: 37.84691143035889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880233
Iteration 2/25 | Loss: 0.00124149
Iteration 3/25 | Loss: 0.00113589
Iteration 4/25 | Loss: 0.00112829
Iteration 5/25 | Loss: 0.00112574
Iteration 6/25 | Loss: 0.00112505
Iteration 7/25 | Loss: 0.00112505
Iteration 8/25 | Loss: 0.00112505
Iteration 9/25 | Loss: 0.00112505
Iteration 10/25 | Loss: 0.00112505
Iteration 11/25 | Loss: 0.00112505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011250515235587955, 0.0011250515235587955, 0.0011250515235587955, 0.0011250515235587955, 0.0011250515235587955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011250515235587955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31518245
Iteration 2/25 | Loss: 0.00210845
Iteration 3/25 | Loss: 0.00210845
Iteration 4/25 | Loss: 0.00210845
Iteration 5/25 | Loss: 0.00210845
Iteration 6/25 | Loss: 0.00210845
Iteration 7/25 | Loss: 0.00210844
Iteration 8/25 | Loss: 0.00210844
Iteration 9/25 | Loss: 0.00210844
Iteration 10/25 | Loss: 0.00210844
Iteration 11/25 | Loss: 0.00210844
Iteration 12/25 | Loss: 0.00210844
Iteration 13/25 | Loss: 0.00210844
Iteration 14/25 | Loss: 0.00210844
Iteration 15/25 | Loss: 0.00210844
Iteration 16/25 | Loss: 0.00210844
Iteration 17/25 | Loss: 0.00210844
Iteration 18/25 | Loss: 0.00210844
Iteration 19/25 | Loss: 0.00210844
Iteration 20/25 | Loss: 0.00210844
Iteration 21/25 | Loss: 0.00210844
Iteration 22/25 | Loss: 0.00210844
Iteration 23/25 | Loss: 0.00210844
Iteration 24/25 | Loss: 0.00210844
Iteration 25/25 | Loss: 0.00210844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210844
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001710
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001200
Iteration 42/1000 | Loss: 0.00001200
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001198
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001198
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001197
Iteration 82/1000 | Loss: 0.00001197
Iteration 83/1000 | Loss: 0.00001197
Iteration 84/1000 | Loss: 0.00001197
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001196
Iteration 87/1000 | Loss: 0.00001196
Iteration 88/1000 | Loss: 0.00001196
Iteration 89/1000 | Loss: 0.00001196
Iteration 90/1000 | Loss: 0.00001196
Iteration 91/1000 | Loss: 0.00001196
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001196
Iteration 94/1000 | Loss: 0.00001196
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.193862954096403e-05, 1.193862954096403e-05, 1.193862954096403e-05, 1.193862954096403e-05, 1.193862954096403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.193862954096403e-05

Optimization complete. Final v2v error: 2.955944538116455 mm

Highest mean error: 3.143819808959961 mm for frame 92

Lowest mean error: 2.685432195663452 mm for frame 126

Saving results

Total time: 30.273361921310425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607917
Iteration 2/25 | Loss: 0.00127120
Iteration 3/25 | Loss: 0.00117564
Iteration 4/25 | Loss: 0.00116893
Iteration 5/25 | Loss: 0.00116839
Iteration 6/25 | Loss: 0.00116839
Iteration 7/25 | Loss: 0.00116839
Iteration 8/25 | Loss: 0.00116839
Iteration 9/25 | Loss: 0.00116839
Iteration 10/25 | Loss: 0.00116839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011683859629556537, 0.0011683859629556537, 0.0011683859629556537, 0.0011683859629556537, 0.0011683859629556537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011683859629556537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.60290861
Iteration 2/25 | Loss: 0.00215972
Iteration 3/25 | Loss: 0.00215968
Iteration 4/25 | Loss: 0.00215968
Iteration 5/25 | Loss: 0.00215968
Iteration 6/25 | Loss: 0.00215968
Iteration 7/25 | Loss: 0.00215968
Iteration 8/25 | Loss: 0.00215968
Iteration 9/25 | Loss: 0.00215968
Iteration 10/25 | Loss: 0.00215968
Iteration 11/25 | Loss: 0.00215968
Iteration 12/25 | Loss: 0.00215968
Iteration 13/25 | Loss: 0.00215968
Iteration 14/25 | Loss: 0.00215968
Iteration 15/25 | Loss: 0.00215968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00215967558324337, 0.00215967558324337, 0.00215967558324337, 0.00215967558324337, 0.00215967558324337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00215967558324337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215968
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001664
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001397
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001387
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001384
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001383
Iteration 51/1000 | Loss: 0.00001383
Iteration 52/1000 | Loss: 0.00001383
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001380
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001379
Iteration 84/1000 | Loss: 0.00001379
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001379
Iteration 94/1000 | Loss: 0.00001379
Iteration 95/1000 | Loss: 0.00001379
Iteration 96/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.3794854567095172e-05, 1.3794854567095172e-05, 1.3794854567095172e-05, 1.3794854567095172e-05, 1.3794854567095172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3794854567095172e-05

Optimization complete. Final v2v error: 3.2259809970855713 mm

Highest mean error: 3.7032735347747803 mm for frame 65

Lowest mean error: 2.8878226280212402 mm for frame 202

Saving results

Total time: 29.920652866363525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402045
Iteration 2/25 | Loss: 0.00120630
Iteration 3/25 | Loss: 0.00113373
Iteration 4/25 | Loss: 0.00111894
Iteration 5/25 | Loss: 0.00111473
Iteration 6/25 | Loss: 0.00111427
Iteration 7/25 | Loss: 0.00111427
Iteration 8/25 | Loss: 0.00111427
Iteration 9/25 | Loss: 0.00111427
Iteration 10/25 | Loss: 0.00111427
Iteration 11/25 | Loss: 0.00111427
Iteration 12/25 | Loss: 0.00111427
Iteration 13/25 | Loss: 0.00111427
Iteration 14/25 | Loss: 0.00111427
Iteration 15/25 | Loss: 0.00111427
Iteration 16/25 | Loss: 0.00111427
Iteration 17/25 | Loss: 0.00111427
Iteration 18/25 | Loss: 0.00111427
Iteration 19/25 | Loss: 0.00111427
Iteration 20/25 | Loss: 0.00111427
Iteration 21/25 | Loss: 0.00111427
Iteration 22/25 | Loss: 0.00111427
Iteration 23/25 | Loss: 0.00111427
Iteration 24/25 | Loss: 0.00111427
Iteration 25/25 | Loss: 0.00111427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21197391
Iteration 2/25 | Loss: 0.00219124
Iteration 3/25 | Loss: 0.00219124
Iteration 4/25 | Loss: 0.00219124
Iteration 5/25 | Loss: 0.00219124
Iteration 6/25 | Loss: 0.00219124
Iteration 7/25 | Loss: 0.00219124
Iteration 8/25 | Loss: 0.00219124
Iteration 9/25 | Loss: 0.00219123
Iteration 10/25 | Loss: 0.00219123
Iteration 11/25 | Loss: 0.00219123
Iteration 12/25 | Loss: 0.00219123
Iteration 13/25 | Loss: 0.00219123
Iteration 14/25 | Loss: 0.00219123
Iteration 15/25 | Loss: 0.00219123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002191234612837434, 0.002191234612837434, 0.002191234612837434, 0.002191234612837434, 0.002191234612837434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002191234612837434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219123
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001880
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001586
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001420
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001355
Iteration 15/1000 | Loss: 0.00001353
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001348
Iteration 25/1000 | Loss: 0.00001348
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001332
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001331
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001329
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Iteration 150/1000 | Loss: 0.00001319
Iteration 151/1000 | Loss: 0.00001319
Iteration 152/1000 | Loss: 0.00001319
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001319
Iteration 162/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.3188696357246954e-05, 1.3188696357246954e-05, 1.3188696357246954e-05, 1.3188696357246954e-05, 1.3188696357246954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3188696357246954e-05

Optimization complete. Final v2v error: 3.131892681121826 mm

Highest mean error: 3.4094743728637695 mm for frame 147

Lowest mean error: 2.796478509902954 mm for frame 0

Saving results

Total time: 33.153494358062744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849374
Iteration 2/25 | Loss: 0.00135416
Iteration 3/25 | Loss: 0.00124620
Iteration 4/25 | Loss: 0.00123261
Iteration 5/25 | Loss: 0.00122805
Iteration 6/25 | Loss: 0.00122695
Iteration 7/25 | Loss: 0.00122695
Iteration 8/25 | Loss: 0.00122695
Iteration 9/25 | Loss: 0.00122695
Iteration 10/25 | Loss: 0.00122695
Iteration 11/25 | Loss: 0.00122695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012269543949514627, 0.0012269543949514627, 0.0012269543949514627, 0.0012269543949514627, 0.0012269543949514627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012269543949514627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39335024
Iteration 2/25 | Loss: 0.00263873
Iteration 3/25 | Loss: 0.00263867
Iteration 4/25 | Loss: 0.00263867
Iteration 5/25 | Loss: 0.00263867
Iteration 6/25 | Loss: 0.00263867
Iteration 7/25 | Loss: 0.00263867
Iteration 8/25 | Loss: 0.00263867
Iteration 9/25 | Loss: 0.00263867
Iteration 10/25 | Loss: 0.00263867
Iteration 11/25 | Loss: 0.00263867
Iteration 12/25 | Loss: 0.00263867
Iteration 13/25 | Loss: 0.00263867
Iteration 14/25 | Loss: 0.00263867
Iteration 15/25 | Loss: 0.00263867
Iteration 16/25 | Loss: 0.00263867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026386657264083624, 0.0026386657264083624, 0.0026386657264083624, 0.0026386657264083624, 0.0026386657264083624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026386657264083624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263867
Iteration 2/1000 | Loss: 0.00004062
Iteration 3/1000 | Loss: 0.00003037
Iteration 4/1000 | Loss: 0.00002415
Iteration 5/1000 | Loss: 0.00002171
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001883
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001773
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001678
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001657
Iteration 22/1000 | Loss: 0.00001656
Iteration 23/1000 | Loss: 0.00001656
Iteration 24/1000 | Loss: 0.00001655
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001643
Iteration 96/1000 | Loss: 0.00001643
Iteration 97/1000 | Loss: 0.00001643
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001643
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001642
Iteration 104/1000 | Loss: 0.00001642
Iteration 105/1000 | Loss: 0.00001642
Iteration 106/1000 | Loss: 0.00001642
Iteration 107/1000 | Loss: 0.00001642
Iteration 108/1000 | Loss: 0.00001642
Iteration 109/1000 | Loss: 0.00001642
Iteration 110/1000 | Loss: 0.00001642
Iteration 111/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6417416190961376e-05, 1.6417416190961376e-05, 1.6417416190961376e-05, 1.6417416190961376e-05, 1.6417416190961376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6417416190961376e-05

Optimization complete. Final v2v error: 3.405442714691162 mm

Highest mean error: 3.709125518798828 mm for frame 0

Lowest mean error: 3.1206743717193604 mm for frame 46

Saving results

Total time: 36.31216216087341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804035
Iteration 2/25 | Loss: 0.00217424
Iteration 3/25 | Loss: 0.00136581
Iteration 4/25 | Loss: 0.00124797
Iteration 5/25 | Loss: 0.00120863
Iteration 6/25 | Loss: 0.00120600
Iteration 7/25 | Loss: 0.00121905
Iteration 8/25 | Loss: 0.00122016
Iteration 9/25 | Loss: 0.00121227
Iteration 10/25 | Loss: 0.00119870
Iteration 11/25 | Loss: 0.00119142
Iteration 12/25 | Loss: 0.00118661
Iteration 13/25 | Loss: 0.00118442
Iteration 14/25 | Loss: 0.00118294
Iteration 15/25 | Loss: 0.00118254
Iteration 16/25 | Loss: 0.00118829
Iteration 17/25 | Loss: 0.00118836
Iteration 18/25 | Loss: 0.00118283
Iteration 19/25 | Loss: 0.00118191
Iteration 20/25 | Loss: 0.00118175
Iteration 21/25 | Loss: 0.00118161
Iteration 22/25 | Loss: 0.00118125
Iteration 23/25 | Loss: 0.00118804
Iteration 24/25 | Loss: 0.00118216
Iteration 25/25 | Loss: 0.00117975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21454406
Iteration 2/25 | Loss: 0.00127720
Iteration 3/25 | Loss: 0.00127720
Iteration 4/25 | Loss: 0.00127720
Iteration 5/25 | Loss: 0.00127720
Iteration 6/25 | Loss: 0.00127719
Iteration 7/25 | Loss: 0.00127719
Iteration 8/25 | Loss: 0.00127719
Iteration 9/25 | Loss: 0.00127719
Iteration 10/25 | Loss: 0.00127719
Iteration 11/25 | Loss: 0.00127719
Iteration 12/25 | Loss: 0.00127719
Iteration 13/25 | Loss: 0.00127719
Iteration 14/25 | Loss: 0.00127719
Iteration 15/25 | Loss: 0.00127719
Iteration 16/25 | Loss: 0.00127719
Iteration 17/25 | Loss: 0.00127719
Iteration 18/25 | Loss: 0.00127719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012771938927471638, 0.0012771938927471638, 0.0012771938927471638, 0.0012771938927471638, 0.0012771938927471638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012771938927471638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127719
Iteration 2/1000 | Loss: 0.00004109
Iteration 3/1000 | Loss: 0.00016356
Iteration 4/1000 | Loss: 0.00003638
Iteration 5/1000 | Loss: 0.00013970
Iteration 6/1000 | Loss: 0.00003954
Iteration 7/1000 | Loss: 0.00003257
Iteration 8/1000 | Loss: 0.00003109
Iteration 9/1000 | Loss: 0.00011127
Iteration 10/1000 | Loss: 0.00025577
Iteration 11/1000 | Loss: 0.00006303
Iteration 12/1000 | Loss: 0.00003480
Iteration 13/1000 | Loss: 0.00008261
Iteration 14/1000 | Loss: 0.00004888
Iteration 15/1000 | Loss: 0.00005050
Iteration 16/1000 | Loss: 0.00005929
Iteration 17/1000 | Loss: 0.00002609
Iteration 18/1000 | Loss: 0.00006658
Iteration 19/1000 | Loss: 0.00008225
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002429
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002381
Iteration 25/1000 | Loss: 0.00002380
Iteration 26/1000 | Loss: 0.00002367
Iteration 27/1000 | Loss: 0.00002358
Iteration 28/1000 | Loss: 0.00002356
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002354
Iteration 31/1000 | Loss: 0.00002353
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002352
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002348
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002343
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002342
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00002341
Iteration 50/1000 | Loss: 0.00002341
Iteration 51/1000 | Loss: 0.00002340
Iteration 52/1000 | Loss: 0.00002340
Iteration 53/1000 | Loss: 0.00002340
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00002339
Iteration 56/1000 | Loss: 0.00002338
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00002338
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002337
Iteration 61/1000 | Loss: 0.00002337
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00002336
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002335
Iteration 66/1000 | Loss: 0.00002335
Iteration 67/1000 | Loss: 0.00002335
Iteration 68/1000 | Loss: 0.00002334
Iteration 69/1000 | Loss: 0.00002334
Iteration 70/1000 | Loss: 0.00002334
Iteration 71/1000 | Loss: 0.00002333
Iteration 72/1000 | Loss: 0.00002333
Iteration 73/1000 | Loss: 0.00002333
Iteration 74/1000 | Loss: 0.00002332
Iteration 75/1000 | Loss: 0.00002332
Iteration 76/1000 | Loss: 0.00002332
Iteration 77/1000 | Loss: 0.00002332
Iteration 78/1000 | Loss: 0.00002332
Iteration 79/1000 | Loss: 0.00002332
Iteration 80/1000 | Loss: 0.00002332
Iteration 81/1000 | Loss: 0.00002332
Iteration 82/1000 | Loss: 0.00002332
Iteration 83/1000 | Loss: 0.00002332
Iteration 84/1000 | Loss: 0.00002332
Iteration 85/1000 | Loss: 0.00002332
Iteration 86/1000 | Loss: 0.00002332
Iteration 87/1000 | Loss: 0.00002331
Iteration 88/1000 | Loss: 0.00002331
Iteration 89/1000 | Loss: 0.00002331
Iteration 90/1000 | Loss: 0.00002331
Iteration 91/1000 | Loss: 0.00002331
Iteration 92/1000 | Loss: 0.00002331
Iteration 93/1000 | Loss: 0.00002331
Iteration 94/1000 | Loss: 0.00002330
Iteration 95/1000 | Loss: 0.00002330
Iteration 96/1000 | Loss: 0.00002330
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002329
Iteration 100/1000 | Loss: 0.00002329
Iteration 101/1000 | Loss: 0.00002329
Iteration 102/1000 | Loss: 0.00002329
Iteration 103/1000 | Loss: 0.00002329
Iteration 104/1000 | Loss: 0.00002329
Iteration 105/1000 | Loss: 0.00002329
Iteration 106/1000 | Loss: 0.00002329
Iteration 107/1000 | Loss: 0.00002329
Iteration 108/1000 | Loss: 0.00002329
Iteration 109/1000 | Loss: 0.00002328
Iteration 110/1000 | Loss: 0.00002328
Iteration 111/1000 | Loss: 0.00002328
Iteration 112/1000 | Loss: 0.00002328
Iteration 113/1000 | Loss: 0.00002328
Iteration 114/1000 | Loss: 0.00002328
Iteration 115/1000 | Loss: 0.00002328
Iteration 116/1000 | Loss: 0.00002328
Iteration 117/1000 | Loss: 0.00002328
Iteration 118/1000 | Loss: 0.00002328
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002327
Iteration 121/1000 | Loss: 0.00002327
Iteration 122/1000 | Loss: 0.00002327
Iteration 123/1000 | Loss: 0.00002326
Iteration 124/1000 | Loss: 0.00002326
Iteration 125/1000 | Loss: 0.00002326
Iteration 126/1000 | Loss: 0.00002326
Iteration 127/1000 | Loss: 0.00002326
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002326
Iteration 131/1000 | Loss: 0.00002326
Iteration 132/1000 | Loss: 0.00002326
Iteration 133/1000 | Loss: 0.00002326
Iteration 134/1000 | Loss: 0.00002326
Iteration 135/1000 | Loss: 0.00002326
Iteration 136/1000 | Loss: 0.00002326
Iteration 137/1000 | Loss: 0.00002325
Iteration 138/1000 | Loss: 0.00002325
Iteration 139/1000 | Loss: 0.00002325
Iteration 140/1000 | Loss: 0.00002325
Iteration 141/1000 | Loss: 0.00002325
Iteration 142/1000 | Loss: 0.00002325
Iteration 143/1000 | Loss: 0.00002325
Iteration 144/1000 | Loss: 0.00002325
Iteration 145/1000 | Loss: 0.00002325
Iteration 146/1000 | Loss: 0.00002325
Iteration 147/1000 | Loss: 0.00002325
Iteration 148/1000 | Loss: 0.00002325
Iteration 149/1000 | Loss: 0.00002325
Iteration 150/1000 | Loss: 0.00002324
Iteration 151/1000 | Loss: 0.00002324
Iteration 152/1000 | Loss: 0.00002324
Iteration 153/1000 | Loss: 0.00002324
Iteration 154/1000 | Loss: 0.00002324
Iteration 155/1000 | Loss: 0.00002324
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002324
Iteration 158/1000 | Loss: 0.00002324
Iteration 159/1000 | Loss: 0.00002324
Iteration 160/1000 | Loss: 0.00002324
Iteration 161/1000 | Loss: 0.00002324
Iteration 162/1000 | Loss: 0.00002324
Iteration 163/1000 | Loss: 0.00002324
Iteration 164/1000 | Loss: 0.00002324
Iteration 165/1000 | Loss: 0.00002324
Iteration 166/1000 | Loss: 0.00002324
Iteration 167/1000 | Loss: 0.00002324
Iteration 168/1000 | Loss: 0.00002324
Iteration 169/1000 | Loss: 0.00002324
Iteration 170/1000 | Loss: 0.00002324
Iteration 171/1000 | Loss: 0.00002324
Iteration 172/1000 | Loss: 0.00002324
Iteration 173/1000 | Loss: 0.00002324
Iteration 174/1000 | Loss: 0.00002324
Iteration 175/1000 | Loss: 0.00002324
Iteration 176/1000 | Loss: 0.00002324
Iteration 177/1000 | Loss: 0.00002324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.323750413779635e-05, 2.323750413779635e-05, 2.323750413779635e-05, 2.323750413779635e-05, 2.323750413779635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.323750413779635e-05

Optimization complete. Final v2v error: 3.9136195182800293 mm

Highest mean error: 9.08533763885498 mm for frame 95

Lowest mean error: 3.3962528705596924 mm for frame 136

Saving results

Total time: 101.33863115310669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466869
Iteration 2/25 | Loss: 0.00136858
Iteration 3/25 | Loss: 0.00115852
Iteration 4/25 | Loss: 0.00114750
Iteration 5/25 | Loss: 0.00114471
Iteration 6/25 | Loss: 0.00114351
Iteration 7/25 | Loss: 0.00114343
Iteration 8/25 | Loss: 0.00114343
Iteration 9/25 | Loss: 0.00114343
Iteration 10/25 | Loss: 0.00114343
Iteration 11/25 | Loss: 0.00114343
Iteration 12/25 | Loss: 0.00114343
Iteration 13/25 | Loss: 0.00114343
Iteration 14/25 | Loss: 0.00114343
Iteration 15/25 | Loss: 0.00114343
Iteration 16/25 | Loss: 0.00114343
Iteration 17/25 | Loss: 0.00114343
Iteration 18/25 | Loss: 0.00114343
Iteration 19/25 | Loss: 0.00114343
Iteration 20/25 | Loss: 0.00114343
Iteration 21/25 | Loss: 0.00114343
Iteration 22/25 | Loss: 0.00114343
Iteration 23/25 | Loss: 0.00114343
Iteration 24/25 | Loss: 0.00114343
Iteration 25/25 | Loss: 0.00114343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30815232
Iteration 2/25 | Loss: 0.00204193
Iteration 3/25 | Loss: 0.00204193
Iteration 4/25 | Loss: 0.00204193
Iteration 5/25 | Loss: 0.00204192
Iteration 6/25 | Loss: 0.00204192
Iteration 7/25 | Loss: 0.00204192
Iteration 8/25 | Loss: 0.00204192
Iteration 9/25 | Loss: 0.00204192
Iteration 10/25 | Loss: 0.00204192
Iteration 11/25 | Loss: 0.00204192
Iteration 12/25 | Loss: 0.00204192
Iteration 13/25 | Loss: 0.00204192
Iteration 14/25 | Loss: 0.00204192
Iteration 15/25 | Loss: 0.00204192
Iteration 16/25 | Loss: 0.00204192
Iteration 17/25 | Loss: 0.00204192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020419235806912184, 0.0020419235806912184, 0.0020419235806912184, 0.0020419235806912184, 0.0020419235806912184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020419235806912184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204192
Iteration 2/1000 | Loss: 0.00003571
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00002287
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001970
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001910
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001850
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001835
Iteration 16/1000 | Loss: 0.00001835
Iteration 17/1000 | Loss: 0.00001835
Iteration 18/1000 | Loss: 0.00001835
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001832
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001829
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001827
Iteration 27/1000 | Loss: 0.00001826
Iteration 28/1000 | Loss: 0.00001820
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001806
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001776
Iteration 70/1000 | Loss: 0.00001776
Iteration 71/1000 | Loss: 0.00001776
Iteration 72/1000 | Loss: 0.00001776
Iteration 73/1000 | Loss: 0.00001775
Iteration 74/1000 | Loss: 0.00001775
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001772
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001765
Iteration 122/1000 | Loss: 0.00001765
Iteration 123/1000 | Loss: 0.00001765
Iteration 124/1000 | Loss: 0.00001765
Iteration 125/1000 | Loss: 0.00001765
Iteration 126/1000 | Loss: 0.00001765
Iteration 127/1000 | Loss: 0.00001765
Iteration 128/1000 | Loss: 0.00001765
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001764
Iteration 134/1000 | Loss: 0.00001764
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001763
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001761
Iteration 152/1000 | Loss: 0.00001761
Iteration 153/1000 | Loss: 0.00001761
Iteration 154/1000 | Loss: 0.00001761
Iteration 155/1000 | Loss: 0.00001761
Iteration 156/1000 | Loss: 0.00001761
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001760
Iteration 169/1000 | Loss: 0.00001760
Iteration 170/1000 | Loss: 0.00001760
Iteration 171/1000 | Loss: 0.00001759
Iteration 172/1000 | Loss: 0.00001759
Iteration 173/1000 | Loss: 0.00001759
Iteration 174/1000 | Loss: 0.00001759
Iteration 175/1000 | Loss: 0.00001759
Iteration 176/1000 | Loss: 0.00001758
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001758
Iteration 179/1000 | Loss: 0.00001758
Iteration 180/1000 | Loss: 0.00001758
Iteration 181/1000 | Loss: 0.00001758
Iteration 182/1000 | Loss: 0.00001758
Iteration 183/1000 | Loss: 0.00001758
Iteration 184/1000 | Loss: 0.00001757
Iteration 185/1000 | Loss: 0.00001757
Iteration 186/1000 | Loss: 0.00001757
Iteration 187/1000 | Loss: 0.00001757
Iteration 188/1000 | Loss: 0.00001757
Iteration 189/1000 | Loss: 0.00001757
Iteration 190/1000 | Loss: 0.00001757
Iteration 191/1000 | Loss: 0.00001757
Iteration 192/1000 | Loss: 0.00001757
Iteration 193/1000 | Loss: 0.00001757
Iteration 194/1000 | Loss: 0.00001757
Iteration 195/1000 | Loss: 0.00001757
Iteration 196/1000 | Loss: 0.00001757
Iteration 197/1000 | Loss: 0.00001757
Iteration 198/1000 | Loss: 0.00001757
Iteration 199/1000 | Loss: 0.00001756
Iteration 200/1000 | Loss: 0.00001756
Iteration 201/1000 | Loss: 0.00001756
Iteration 202/1000 | Loss: 0.00001756
Iteration 203/1000 | Loss: 0.00001756
Iteration 204/1000 | Loss: 0.00001756
Iteration 205/1000 | Loss: 0.00001756
Iteration 206/1000 | Loss: 0.00001756
Iteration 207/1000 | Loss: 0.00001756
Iteration 208/1000 | Loss: 0.00001756
Iteration 209/1000 | Loss: 0.00001756
Iteration 210/1000 | Loss: 0.00001756
Iteration 211/1000 | Loss: 0.00001756
Iteration 212/1000 | Loss: 0.00001756
Iteration 213/1000 | Loss: 0.00001756
Iteration 214/1000 | Loss: 0.00001756
Iteration 215/1000 | Loss: 0.00001756
Iteration 216/1000 | Loss: 0.00001756
Iteration 217/1000 | Loss: 0.00001756
Iteration 218/1000 | Loss: 0.00001756
Iteration 219/1000 | Loss: 0.00001756
Iteration 220/1000 | Loss: 0.00001756
Iteration 221/1000 | Loss: 0.00001756
Iteration 222/1000 | Loss: 0.00001755
Iteration 223/1000 | Loss: 0.00001755
Iteration 224/1000 | Loss: 0.00001755
Iteration 225/1000 | Loss: 0.00001755
Iteration 226/1000 | Loss: 0.00001755
Iteration 227/1000 | Loss: 0.00001755
Iteration 228/1000 | Loss: 0.00001755
Iteration 229/1000 | Loss: 0.00001755
Iteration 230/1000 | Loss: 0.00001755
Iteration 231/1000 | Loss: 0.00001755
Iteration 232/1000 | Loss: 0.00001755
Iteration 233/1000 | Loss: 0.00001755
Iteration 234/1000 | Loss: 0.00001755
Iteration 235/1000 | Loss: 0.00001755
Iteration 236/1000 | Loss: 0.00001755
Iteration 237/1000 | Loss: 0.00001755
Iteration 238/1000 | Loss: 0.00001755
Iteration 239/1000 | Loss: 0.00001755
Iteration 240/1000 | Loss: 0.00001755
Iteration 241/1000 | Loss: 0.00001755
Iteration 242/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.755054654495325e-05, 1.755054654495325e-05, 1.755054654495325e-05, 1.755054654495325e-05, 1.755054654495325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.755054654495325e-05

Optimization complete. Final v2v error: 3.2926862239837646 mm

Highest mean error: 3.9054770469665527 mm for frame 15

Lowest mean error: 2.56585431098938 mm for frame 112

Saving results

Total time: 46.682372093200684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889955
Iteration 2/25 | Loss: 0.00124080
Iteration 3/25 | Loss: 0.00114279
Iteration 4/25 | Loss: 0.00112723
Iteration 5/25 | Loss: 0.00112133
Iteration 6/25 | Loss: 0.00111972
Iteration 7/25 | Loss: 0.00111960
Iteration 8/25 | Loss: 0.00111960
Iteration 9/25 | Loss: 0.00111960
Iteration 10/25 | Loss: 0.00111960
Iteration 11/25 | Loss: 0.00111960
Iteration 12/25 | Loss: 0.00111960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011195953702554107, 0.0011195953702554107, 0.0011195953702554107, 0.0011195953702554107, 0.0011195953702554107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011195953702554107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70788014
Iteration 2/25 | Loss: 0.00212443
Iteration 3/25 | Loss: 0.00212443
Iteration 4/25 | Loss: 0.00212442
Iteration 5/25 | Loss: 0.00212442
Iteration 6/25 | Loss: 0.00212442
Iteration 7/25 | Loss: 0.00212442
Iteration 8/25 | Loss: 0.00212442
Iteration 9/25 | Loss: 0.00212442
Iteration 10/25 | Loss: 0.00212442
Iteration 11/25 | Loss: 0.00212442
Iteration 12/25 | Loss: 0.00212442
Iteration 13/25 | Loss: 0.00212442
Iteration 14/25 | Loss: 0.00212442
Iteration 15/25 | Loss: 0.00212442
Iteration 16/25 | Loss: 0.00212442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021244226954877377, 0.0021244226954877377, 0.0021244226954877377, 0.0021244226954877377, 0.0021244226954877377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021244226954877377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212442
Iteration 2/1000 | Loss: 0.00002084
Iteration 3/1000 | Loss: 0.00001521
Iteration 4/1000 | Loss: 0.00001345
Iteration 5/1000 | Loss: 0.00001234
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001174
Iteration 8/1000 | Loss: 0.00001151
Iteration 9/1000 | Loss: 0.00001151
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001121
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001112
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001106
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001104
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001101
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001100
Iteration 100/1000 | Loss: 0.00001100
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001100
Iteration 107/1000 | Loss: 0.00001100
Iteration 108/1000 | Loss: 0.00001100
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001100
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001099
Iteration 118/1000 | Loss: 0.00001099
Iteration 119/1000 | Loss: 0.00001099
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001099
Iteration 133/1000 | Loss: 0.00001099
Iteration 134/1000 | Loss: 0.00001099
Iteration 135/1000 | Loss: 0.00001099
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001098
Iteration 138/1000 | Loss: 0.00001098
Iteration 139/1000 | Loss: 0.00001098
Iteration 140/1000 | Loss: 0.00001098
Iteration 141/1000 | Loss: 0.00001098
Iteration 142/1000 | Loss: 0.00001098
Iteration 143/1000 | Loss: 0.00001098
Iteration 144/1000 | Loss: 0.00001098
Iteration 145/1000 | Loss: 0.00001098
Iteration 146/1000 | Loss: 0.00001098
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001097
Iteration 153/1000 | Loss: 0.00001097
Iteration 154/1000 | Loss: 0.00001097
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Iteration 161/1000 | Loss: 0.00001097
Iteration 162/1000 | Loss: 0.00001097
Iteration 163/1000 | Loss: 0.00001097
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001097
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001097
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.0967853995680343e-05, 1.0967853995680343e-05, 1.0967853995680343e-05, 1.0967853995680343e-05, 1.0967853995680343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0967853995680343e-05

Optimization complete. Final v2v error: 2.8616769313812256 mm

Highest mean error: 3.375533103942871 mm for frame 74

Lowest mean error: 2.5867273807525635 mm for frame 0

Saving results

Total time: 33.1918671131134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927861
Iteration 2/25 | Loss: 0.00129708
Iteration 3/25 | Loss: 0.00118901
Iteration 4/25 | Loss: 0.00118349
Iteration 5/25 | Loss: 0.00118203
Iteration 6/25 | Loss: 0.00118175
Iteration 7/25 | Loss: 0.00118175
Iteration 8/25 | Loss: 0.00118175
Iteration 9/25 | Loss: 0.00118175
Iteration 10/25 | Loss: 0.00118175
Iteration 11/25 | Loss: 0.00118175
Iteration 12/25 | Loss: 0.00118175
Iteration 13/25 | Loss: 0.00118173
Iteration 14/25 | Loss: 0.00118173
Iteration 15/25 | Loss: 0.00118173
Iteration 16/25 | Loss: 0.00118173
Iteration 17/25 | Loss: 0.00118173
Iteration 18/25 | Loss: 0.00118173
Iteration 19/25 | Loss: 0.00118173
Iteration 20/25 | Loss: 0.00118173
Iteration 21/25 | Loss: 0.00118173
Iteration 22/25 | Loss: 0.00118173
Iteration 23/25 | Loss: 0.00118173
Iteration 24/25 | Loss: 0.00118173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011817346094176173, 0.0011817346094176173, 0.0011817346094176173, 0.0011817346094176173, 0.0011817346094176173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011817346094176173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79587245
Iteration 2/25 | Loss: 0.00206848
Iteration 3/25 | Loss: 0.00206848
Iteration 4/25 | Loss: 0.00206848
Iteration 5/25 | Loss: 0.00206848
Iteration 6/25 | Loss: 0.00206848
Iteration 7/25 | Loss: 0.00206848
Iteration 8/25 | Loss: 0.00206848
Iteration 9/25 | Loss: 0.00206848
Iteration 10/25 | Loss: 0.00206848
Iteration 11/25 | Loss: 0.00206848
Iteration 12/25 | Loss: 0.00206848
Iteration 13/25 | Loss: 0.00206848
Iteration 14/25 | Loss: 0.00206848
Iteration 15/25 | Loss: 0.00206848
Iteration 16/25 | Loss: 0.00206848
Iteration 17/25 | Loss: 0.00206848
Iteration 18/25 | Loss: 0.00206848
Iteration 19/25 | Loss: 0.00206848
Iteration 20/25 | Loss: 0.00206848
Iteration 21/25 | Loss: 0.00206848
Iteration 22/25 | Loss: 0.00206848
Iteration 23/25 | Loss: 0.00206848
Iteration 24/25 | Loss: 0.00206848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020684783812612295, 0.0020684783812612295, 0.0020684783812612295, 0.0020684783812612295, 0.0020684783812612295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020684783812612295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206848
Iteration 2/1000 | Loss: 0.00003818
Iteration 3/1000 | Loss: 0.00002462
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001703
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001327
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001326
Iteration 43/1000 | Loss: 0.00001325
Iteration 44/1000 | Loss: 0.00001325
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001325
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001321
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001308
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001306
Iteration 141/1000 | Loss: 0.00001306
Iteration 142/1000 | Loss: 0.00001306
Iteration 143/1000 | Loss: 0.00001305
Iteration 144/1000 | Loss: 0.00001305
Iteration 145/1000 | Loss: 0.00001305
Iteration 146/1000 | Loss: 0.00001305
Iteration 147/1000 | Loss: 0.00001305
Iteration 148/1000 | Loss: 0.00001305
Iteration 149/1000 | Loss: 0.00001305
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001305
Iteration 152/1000 | Loss: 0.00001305
Iteration 153/1000 | Loss: 0.00001305
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001305
Iteration 156/1000 | Loss: 0.00001305
Iteration 157/1000 | Loss: 0.00001305
Iteration 158/1000 | Loss: 0.00001305
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.304742181673646e-05, 1.304742181673646e-05, 1.304742181673646e-05, 1.304742181673646e-05, 1.304742181673646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.304742181673646e-05

Optimization complete. Final v2v error: 3.1884844303131104 mm

Highest mean error: 3.5645864009857178 mm for frame 116

Lowest mean error: 3.005722999572754 mm for frame 83

Saving results

Total time: 36.954243421554565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872414
Iteration 2/25 | Loss: 0.00142187
Iteration 3/25 | Loss: 0.00121151
Iteration 4/25 | Loss: 0.00119363
Iteration 5/25 | Loss: 0.00119233
Iteration 6/25 | Loss: 0.00119233
Iteration 7/25 | Loss: 0.00119233
Iteration 8/25 | Loss: 0.00119233
Iteration 9/25 | Loss: 0.00119233
Iteration 10/25 | Loss: 0.00119233
Iteration 11/25 | Loss: 0.00119233
Iteration 12/25 | Loss: 0.00119233
Iteration 13/25 | Loss: 0.00119233
Iteration 14/25 | Loss: 0.00119233
Iteration 15/25 | Loss: 0.00119233
Iteration 16/25 | Loss: 0.00119233
Iteration 17/25 | Loss: 0.00119233
Iteration 18/25 | Loss: 0.00119233
Iteration 19/25 | Loss: 0.00119233
Iteration 20/25 | Loss: 0.00119233
Iteration 21/25 | Loss: 0.00119233
Iteration 22/25 | Loss: 0.00119233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011923278216272593, 0.0011923278216272593, 0.0011923278216272593, 0.0011923278216272593, 0.0011923278216272593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011923278216272593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16327274
Iteration 2/25 | Loss: 0.00189727
Iteration 3/25 | Loss: 0.00189726
Iteration 4/25 | Loss: 0.00189725
Iteration 5/25 | Loss: 0.00189725
Iteration 6/25 | Loss: 0.00189725
Iteration 7/25 | Loss: 0.00189725
Iteration 8/25 | Loss: 0.00189725
Iteration 9/25 | Loss: 0.00189725
Iteration 10/25 | Loss: 0.00189725
Iteration 11/25 | Loss: 0.00189725
Iteration 12/25 | Loss: 0.00189725
Iteration 13/25 | Loss: 0.00189725
Iteration 14/25 | Loss: 0.00189725
Iteration 15/25 | Loss: 0.00189725
Iteration 16/25 | Loss: 0.00189725
Iteration 17/25 | Loss: 0.00189725
Iteration 18/25 | Loss: 0.00189725
Iteration 19/25 | Loss: 0.00189725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00189725193195045, 0.00189725193195045, 0.00189725193195045, 0.00189725193195045, 0.00189725193195045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00189725193195045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189725
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00002192
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001698
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001488
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001366
Iteration 39/1000 | Loss: 0.00001366
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001359
Iteration 67/1000 | Loss: 0.00001359
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.358893950964557e-05, 1.358893950964557e-05, 1.358893950964557e-05, 1.358893950964557e-05, 1.358893950964557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.358893950964557e-05

Optimization complete. Final v2v error: 3.1785151958465576 mm

Highest mean error: 3.6391196250915527 mm for frame 4

Lowest mean error: 2.7303669452667236 mm for frame 175

Saving results

Total time: 29.551642894744873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365297
Iteration 2/25 | Loss: 0.00122128
Iteration 3/25 | Loss: 0.00113456
Iteration 4/25 | Loss: 0.00112423
Iteration 5/25 | Loss: 0.00112056
Iteration 6/25 | Loss: 0.00112028
Iteration 7/25 | Loss: 0.00112028
Iteration 8/25 | Loss: 0.00112028
Iteration 9/25 | Loss: 0.00112028
Iteration 10/25 | Loss: 0.00112028
Iteration 11/25 | Loss: 0.00112028
Iteration 12/25 | Loss: 0.00112028
Iteration 13/25 | Loss: 0.00112028
Iteration 14/25 | Loss: 0.00112028
Iteration 15/25 | Loss: 0.00112028
Iteration 16/25 | Loss: 0.00112028
Iteration 17/25 | Loss: 0.00112028
Iteration 18/25 | Loss: 0.00112028
Iteration 19/25 | Loss: 0.00112028
Iteration 20/25 | Loss: 0.00112028
Iteration 21/25 | Loss: 0.00112028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011202840832993388, 0.0011202840832993388, 0.0011202840832993388, 0.0011202840832993388, 0.0011202840832993388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011202840832993388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47612917
Iteration 2/25 | Loss: 0.00219192
Iteration 3/25 | Loss: 0.00219192
Iteration 4/25 | Loss: 0.00219192
Iteration 5/25 | Loss: 0.00219192
Iteration 6/25 | Loss: 0.00219192
Iteration 7/25 | Loss: 0.00219192
Iteration 8/25 | Loss: 0.00219192
Iteration 9/25 | Loss: 0.00219192
Iteration 10/25 | Loss: 0.00219192
Iteration 11/25 | Loss: 0.00219192
Iteration 12/25 | Loss: 0.00219192
Iteration 13/25 | Loss: 0.00219192
Iteration 14/25 | Loss: 0.00219192
Iteration 15/25 | Loss: 0.00219192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021919177379459143, 0.0021919177379459143, 0.0021919177379459143, 0.0021919177379459143, 0.0021919177379459143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021919177379459143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219192
Iteration 2/1000 | Loss: 0.00002160
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001495
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001266
Iteration 36/1000 | Loss: 0.00001266
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001263
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001263
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001262
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001262
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001261
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001257
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001256
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001253
Iteration 92/1000 | Loss: 0.00001253
Iteration 93/1000 | Loss: 0.00001253
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001252
Iteration 96/1000 | Loss: 0.00001252
Iteration 97/1000 | Loss: 0.00001252
Iteration 98/1000 | Loss: 0.00001252
Iteration 99/1000 | Loss: 0.00001252
Iteration 100/1000 | Loss: 0.00001252
Iteration 101/1000 | Loss: 0.00001252
Iteration 102/1000 | Loss: 0.00001252
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001251
Iteration 107/1000 | Loss: 0.00001251
Iteration 108/1000 | Loss: 0.00001251
Iteration 109/1000 | Loss: 0.00001251
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001250
Iteration 125/1000 | Loss: 0.00001250
Iteration 126/1000 | Loss: 0.00001250
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001249
Iteration 131/1000 | Loss: 0.00001249
Iteration 132/1000 | Loss: 0.00001249
Iteration 133/1000 | Loss: 0.00001249
Iteration 134/1000 | Loss: 0.00001249
Iteration 135/1000 | Loss: 0.00001249
Iteration 136/1000 | Loss: 0.00001249
Iteration 137/1000 | Loss: 0.00001249
Iteration 138/1000 | Loss: 0.00001249
Iteration 139/1000 | Loss: 0.00001249
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001247
Iteration 159/1000 | Loss: 0.00001247
Iteration 160/1000 | Loss: 0.00001247
Iteration 161/1000 | Loss: 0.00001247
Iteration 162/1000 | Loss: 0.00001247
Iteration 163/1000 | Loss: 0.00001247
Iteration 164/1000 | Loss: 0.00001247
Iteration 165/1000 | Loss: 0.00001247
Iteration 166/1000 | Loss: 0.00001247
Iteration 167/1000 | Loss: 0.00001247
Iteration 168/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2472894923121203e-05, 1.2472894923121203e-05, 1.2472894923121203e-05, 1.2472894923121203e-05, 1.2472894923121203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2472894923121203e-05

Optimization complete. Final v2v error: 3.0275628566741943 mm

Highest mean error: 3.4647417068481445 mm for frame 132

Lowest mean error: 2.841892957687378 mm for frame 147

Saving results

Total time: 37.49667525291443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472021
Iteration 2/25 | Loss: 0.00130535
Iteration 3/25 | Loss: 0.00118363
Iteration 4/25 | Loss: 0.00115981
Iteration 5/25 | Loss: 0.00115254
Iteration 6/25 | Loss: 0.00115068
Iteration 7/25 | Loss: 0.00115056
Iteration 8/25 | Loss: 0.00115056
Iteration 9/25 | Loss: 0.00115056
Iteration 10/25 | Loss: 0.00115056
Iteration 11/25 | Loss: 0.00115056
Iteration 12/25 | Loss: 0.00115056
Iteration 13/25 | Loss: 0.00115056
Iteration 14/25 | Loss: 0.00115056
Iteration 15/25 | Loss: 0.00115056
Iteration 16/25 | Loss: 0.00115056
Iteration 17/25 | Loss: 0.00115056
Iteration 18/25 | Loss: 0.00115056
Iteration 19/25 | Loss: 0.00115056
Iteration 20/25 | Loss: 0.00115056
Iteration 21/25 | Loss: 0.00115056
Iteration 22/25 | Loss: 0.00115056
Iteration 23/25 | Loss: 0.00115056
Iteration 24/25 | Loss: 0.00115056
Iteration 25/25 | Loss: 0.00115056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27607751
Iteration 2/25 | Loss: 0.00209161
Iteration 3/25 | Loss: 0.00209160
Iteration 4/25 | Loss: 0.00209160
Iteration 5/25 | Loss: 0.00209160
Iteration 6/25 | Loss: 0.00209160
Iteration 7/25 | Loss: 0.00209160
Iteration 8/25 | Loss: 0.00209160
Iteration 9/25 | Loss: 0.00209160
Iteration 10/25 | Loss: 0.00209160
Iteration 11/25 | Loss: 0.00209160
Iteration 12/25 | Loss: 0.00209160
Iteration 13/25 | Loss: 0.00209160
Iteration 14/25 | Loss: 0.00209160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002091602887958288, 0.002091602887958288, 0.002091602887958288, 0.002091602887958288, 0.002091602887958288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002091602887958288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209160
Iteration 2/1000 | Loss: 0.00003380
Iteration 3/1000 | Loss: 0.00002471
Iteration 4/1000 | Loss: 0.00002243
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001874
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001849
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001847
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001844
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001843
Iteration 35/1000 | Loss: 0.00001843
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001841
Iteration 42/1000 | Loss: 0.00001841
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001840
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001840
Iteration 53/1000 | Loss: 0.00001840
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001839
Iteration 56/1000 | Loss: 0.00001839
Iteration 57/1000 | Loss: 0.00001839
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001833
Iteration 86/1000 | Loss: 0.00001833
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001833
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001832
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001832
Iteration 93/1000 | Loss: 0.00001832
Iteration 94/1000 | Loss: 0.00001832
Iteration 95/1000 | Loss: 0.00001832
Iteration 96/1000 | Loss: 0.00001832
Iteration 97/1000 | Loss: 0.00001832
Iteration 98/1000 | Loss: 0.00001832
Iteration 99/1000 | Loss: 0.00001832
Iteration 100/1000 | Loss: 0.00001832
Iteration 101/1000 | Loss: 0.00001832
Iteration 102/1000 | Loss: 0.00001832
Iteration 103/1000 | Loss: 0.00001832
Iteration 104/1000 | Loss: 0.00001832
Iteration 105/1000 | Loss: 0.00001832
Iteration 106/1000 | Loss: 0.00001832
Iteration 107/1000 | Loss: 0.00001832
Iteration 108/1000 | Loss: 0.00001832
Iteration 109/1000 | Loss: 0.00001832
Iteration 110/1000 | Loss: 0.00001832
Iteration 111/1000 | Loss: 0.00001832
Iteration 112/1000 | Loss: 0.00001832
Iteration 113/1000 | Loss: 0.00001832
Iteration 114/1000 | Loss: 0.00001832
Iteration 115/1000 | Loss: 0.00001832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.831971349020023e-05, 1.831971349020023e-05, 1.831971349020023e-05, 1.831971349020023e-05, 1.831971349020023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.831971349020023e-05

Optimization complete. Final v2v error: 3.6018903255462646 mm

Highest mean error: 4.141577243804932 mm for frame 42

Lowest mean error: 3.2384636402130127 mm for frame 89

Saving results

Total time: 32.09501242637634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840578
Iteration 2/25 | Loss: 0.00132653
Iteration 3/25 | Loss: 0.00122955
Iteration 4/25 | Loss: 0.00121776
Iteration 5/25 | Loss: 0.00121476
Iteration 6/25 | Loss: 0.00121377
Iteration 7/25 | Loss: 0.00121377
Iteration 8/25 | Loss: 0.00121377
Iteration 9/25 | Loss: 0.00121377
Iteration 10/25 | Loss: 0.00121377
Iteration 11/25 | Loss: 0.00121377
Iteration 12/25 | Loss: 0.00121377
Iteration 13/25 | Loss: 0.00121377
Iteration 14/25 | Loss: 0.00121377
Iteration 15/25 | Loss: 0.00121377
Iteration 16/25 | Loss: 0.00121377
Iteration 17/25 | Loss: 0.00121377
Iteration 18/25 | Loss: 0.00121377
Iteration 19/25 | Loss: 0.00121377
Iteration 20/25 | Loss: 0.00121377
Iteration 21/25 | Loss: 0.00121377
Iteration 22/25 | Loss: 0.00121377
Iteration 23/25 | Loss: 0.00121377
Iteration 24/25 | Loss: 0.00121377
Iteration 25/25 | Loss: 0.00121377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33574975
Iteration 2/25 | Loss: 0.00255591
Iteration 3/25 | Loss: 0.00255591
Iteration 4/25 | Loss: 0.00255591
Iteration 5/25 | Loss: 0.00255591
Iteration 6/25 | Loss: 0.00255591
Iteration 7/25 | Loss: 0.00255591
Iteration 8/25 | Loss: 0.00255590
Iteration 9/25 | Loss: 0.00255590
Iteration 10/25 | Loss: 0.00255590
Iteration 11/25 | Loss: 0.00255590
Iteration 12/25 | Loss: 0.00255590
Iteration 13/25 | Loss: 0.00255590
Iteration 14/25 | Loss: 0.00255590
Iteration 15/25 | Loss: 0.00255590
Iteration 16/25 | Loss: 0.00255590
Iteration 17/25 | Loss: 0.00255590
Iteration 18/25 | Loss: 0.00255590
Iteration 19/25 | Loss: 0.00255590
Iteration 20/25 | Loss: 0.00255590
Iteration 21/25 | Loss: 0.00255590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002555904444307089, 0.002555904444307089, 0.002555904444307089, 0.002555904444307089, 0.002555904444307089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002555904444307089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255590
Iteration 2/1000 | Loss: 0.00005874
Iteration 3/1000 | Loss: 0.00003320
Iteration 4/1000 | Loss: 0.00002483
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001628
Iteration 26/1000 | Loss: 0.00001627
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001625
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001624
Iteration 39/1000 | Loss: 0.00001624
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.6194551790249534e-05, 1.6194551790249534e-05, 1.6194551790249534e-05, 1.6194551790249534e-05, 1.6194551790249534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6194551790249534e-05

Optimization complete. Final v2v error: 3.4244425296783447 mm

Highest mean error: 4.216835021972656 mm for frame 122

Lowest mean error: 2.908815622329712 mm for frame 2

Saving results

Total time: 32.38028168678284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821895
Iteration 2/25 | Loss: 0.00131650
Iteration 3/25 | Loss: 0.00115780
Iteration 4/25 | Loss: 0.00113918
Iteration 5/25 | Loss: 0.00113531
Iteration 6/25 | Loss: 0.00113444
Iteration 7/25 | Loss: 0.00113444
Iteration 8/25 | Loss: 0.00113444
Iteration 9/25 | Loss: 0.00113444
Iteration 10/25 | Loss: 0.00113444
Iteration 11/25 | Loss: 0.00113444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011344426311552525, 0.0011344426311552525, 0.0011344426311552525, 0.0011344426311552525, 0.0011344426311552525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011344426311552525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22151732
Iteration 2/25 | Loss: 0.00208178
Iteration 3/25 | Loss: 0.00208178
Iteration 4/25 | Loss: 0.00208178
Iteration 5/25 | Loss: 0.00208178
Iteration 6/25 | Loss: 0.00208178
Iteration 7/25 | Loss: 0.00208178
Iteration 8/25 | Loss: 0.00208178
Iteration 9/25 | Loss: 0.00208178
Iteration 10/25 | Loss: 0.00208178
Iteration 11/25 | Loss: 0.00208178
Iteration 12/25 | Loss: 0.00208178
Iteration 13/25 | Loss: 0.00208178
Iteration 14/25 | Loss: 0.00208178
Iteration 15/25 | Loss: 0.00208178
Iteration 16/25 | Loss: 0.00208178
Iteration 17/25 | Loss: 0.00208178
Iteration 18/25 | Loss: 0.00208178
Iteration 19/25 | Loss: 0.00208178
Iteration 20/25 | Loss: 0.00208178
Iteration 21/25 | Loss: 0.00208178
Iteration 22/25 | Loss: 0.00208178
Iteration 23/25 | Loss: 0.00208178
Iteration 24/25 | Loss: 0.00208178
Iteration 25/25 | Loss: 0.00208178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208178
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00001902
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001192
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001180
Iteration 154/1000 | Loss: 0.00001180
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001180
Iteration 159/1000 | Loss: 0.00001180
Iteration 160/1000 | Loss: 0.00001180
Iteration 161/1000 | Loss: 0.00001180
Iteration 162/1000 | Loss: 0.00001180
Iteration 163/1000 | Loss: 0.00001180
Iteration 164/1000 | Loss: 0.00001180
Iteration 165/1000 | Loss: 0.00001180
Iteration 166/1000 | Loss: 0.00001180
Iteration 167/1000 | Loss: 0.00001180
Iteration 168/1000 | Loss: 0.00001180
Iteration 169/1000 | Loss: 0.00001180
Iteration 170/1000 | Loss: 0.00001180
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001180
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.1796225408033933e-05, 1.1796225408033933e-05, 1.1796225408033933e-05, 1.1796225408033933e-05, 1.1796225408033933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1796225408033933e-05

Optimization complete. Final v2v error: 2.9588944911956787 mm

Highest mean error: 3.1711339950561523 mm for frame 85

Lowest mean error: 2.748368501663208 mm for frame 60

Saving results

Total time: 35.86104655265808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909588
Iteration 2/25 | Loss: 0.00458303
Iteration 3/25 | Loss: 0.00401509
Iteration 4/25 | Loss: 0.00385875
Iteration 5/25 | Loss: 0.00380996
Iteration 6/25 | Loss: 0.00376808
Iteration 7/25 | Loss: 0.00369093
Iteration 8/25 | Loss: 0.00363544
Iteration 9/25 | Loss: 0.00350283
Iteration 10/25 | Loss: 0.00334064
Iteration 11/25 | Loss: 0.00329844
Iteration 12/25 | Loss: 0.00328190
Iteration 13/25 | Loss: 0.00326466
Iteration 14/25 | Loss: 0.00323879
Iteration 15/25 | Loss: 0.00322644
Iteration 16/25 | Loss: 0.00314479
Iteration 17/25 | Loss: 0.00311659
Iteration 18/25 | Loss: 0.00311122
Iteration 19/25 | Loss: 0.00311285
Iteration 20/25 | Loss: 0.00310354
Iteration 21/25 | Loss: 0.00310867
Iteration 22/25 | Loss: 0.00309783
Iteration 23/25 | Loss: 0.00309255
Iteration 24/25 | Loss: 0.00309394
Iteration 25/25 | Loss: 0.00309288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99430352
Iteration 2/25 | Loss: 0.00759305
Iteration 3/25 | Loss: 0.00759305
Iteration 4/25 | Loss: 0.00759304
Iteration 5/25 | Loss: 0.00759304
Iteration 6/25 | Loss: 0.00759304
Iteration 7/25 | Loss: 0.00759304
Iteration 8/25 | Loss: 0.00759304
Iteration 9/25 | Loss: 0.00759304
Iteration 10/25 | Loss: 0.00759304
Iteration 11/25 | Loss: 0.00759304
Iteration 12/25 | Loss: 0.00759304
Iteration 13/25 | Loss: 0.00759304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.007593041751533747, 0.007593041751533747, 0.007593041751533747, 0.007593041751533747, 0.007593041751533747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007593041751533747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00759304
Iteration 2/1000 | Loss: 0.00038120
Iteration 3/1000 | Loss: 0.00056217
Iteration 4/1000 | Loss: 0.00066114
Iteration 5/1000 | Loss: 0.00142088
Iteration 6/1000 | Loss: 0.00202255
Iteration 7/1000 | Loss: 0.00157019
Iteration 8/1000 | Loss: 0.00189731
Iteration 9/1000 | Loss: 0.00233031
Iteration 10/1000 | Loss: 0.00207345
Iteration 11/1000 | Loss: 0.00130803
Iteration 12/1000 | Loss: 0.00168311
Iteration 13/1000 | Loss: 0.00059624
Iteration 14/1000 | Loss: 0.00029288
Iteration 15/1000 | Loss: 0.00059108
Iteration 16/1000 | Loss: 0.00030718
Iteration 17/1000 | Loss: 0.00032100
Iteration 18/1000 | Loss: 0.00021584
Iteration 19/1000 | Loss: 0.00060934
Iteration 20/1000 | Loss: 0.00030114
Iteration 21/1000 | Loss: 0.00018281
Iteration 22/1000 | Loss: 0.00017699
Iteration 23/1000 | Loss: 0.00040702
Iteration 24/1000 | Loss: 0.00016529
Iteration 25/1000 | Loss: 0.00036252
Iteration 26/1000 | Loss: 0.00016020
Iteration 27/1000 | Loss: 0.00015553
Iteration 28/1000 | Loss: 0.00040173
Iteration 29/1000 | Loss: 0.00042028
Iteration 30/1000 | Loss: 0.00086701
Iteration 31/1000 | Loss: 0.00081046
Iteration 32/1000 | Loss: 0.00068722
Iteration 33/1000 | Loss: 0.00037390
Iteration 34/1000 | Loss: 0.00037393
Iteration 35/1000 | Loss: 0.00055510
Iteration 36/1000 | Loss: 0.00050492
Iteration 37/1000 | Loss: 0.00029942
Iteration 38/1000 | Loss: 0.00027469
Iteration 39/1000 | Loss: 0.00016300
Iteration 40/1000 | Loss: 0.00014869
Iteration 41/1000 | Loss: 0.00014303
Iteration 42/1000 | Loss: 0.00014015
Iteration 43/1000 | Loss: 0.00013783
Iteration 44/1000 | Loss: 0.00013577
Iteration 45/1000 | Loss: 0.00013331
Iteration 46/1000 | Loss: 0.00021218
Iteration 47/1000 | Loss: 0.00074495
Iteration 48/1000 | Loss: 0.00096930
Iteration 49/1000 | Loss: 0.00031670
Iteration 50/1000 | Loss: 0.00042933
Iteration 51/1000 | Loss: 0.00042512
Iteration 52/1000 | Loss: 0.00023263
Iteration 53/1000 | Loss: 0.00046100
Iteration 54/1000 | Loss: 0.00026268
Iteration 55/1000 | Loss: 0.00018257
Iteration 56/1000 | Loss: 0.00017305
Iteration 57/1000 | Loss: 0.00015953
Iteration 58/1000 | Loss: 0.00013464
Iteration 59/1000 | Loss: 0.00013197
Iteration 60/1000 | Loss: 0.00019603
Iteration 61/1000 | Loss: 0.00013539
Iteration 62/1000 | Loss: 0.00013036
Iteration 63/1000 | Loss: 0.00012860
Iteration 64/1000 | Loss: 0.00012760
Iteration 65/1000 | Loss: 0.00012669
Iteration 66/1000 | Loss: 0.00047619
Iteration 67/1000 | Loss: 0.00056228
Iteration 68/1000 | Loss: 0.00019919
Iteration 69/1000 | Loss: 0.00013487
Iteration 70/1000 | Loss: 0.00018741
Iteration 71/1000 | Loss: 0.00026856
Iteration 72/1000 | Loss: 0.00017108
Iteration 73/1000 | Loss: 0.00013291
Iteration 74/1000 | Loss: 0.00015750
Iteration 75/1000 | Loss: 0.00022076
Iteration 76/1000 | Loss: 0.00055330
Iteration 77/1000 | Loss: 0.00035055
Iteration 78/1000 | Loss: 0.00020187
Iteration 79/1000 | Loss: 0.00013381
Iteration 80/1000 | Loss: 0.00012997
Iteration 81/1000 | Loss: 0.00054389
Iteration 82/1000 | Loss: 0.00021311
Iteration 83/1000 | Loss: 0.00018866
Iteration 84/1000 | Loss: 0.00025191
Iteration 85/1000 | Loss: 0.00013168
Iteration 86/1000 | Loss: 0.00012809
Iteration 87/1000 | Loss: 0.00012624
Iteration 88/1000 | Loss: 0.00012482
Iteration 89/1000 | Loss: 0.00012392
Iteration 90/1000 | Loss: 0.00012324
Iteration 91/1000 | Loss: 0.00033070
Iteration 92/1000 | Loss: 0.00033067
Iteration 93/1000 | Loss: 0.00057350
Iteration 94/1000 | Loss: 0.00022087
Iteration 95/1000 | Loss: 0.00020374
Iteration 96/1000 | Loss: 0.00014315
Iteration 97/1000 | Loss: 0.00013431
Iteration 98/1000 | Loss: 0.00012988
Iteration 99/1000 | Loss: 0.00012765
Iteration 100/1000 | Loss: 0.00012589
Iteration 101/1000 | Loss: 0.00012457
Iteration 102/1000 | Loss: 0.00012310
Iteration 103/1000 | Loss: 0.00012174
Iteration 104/1000 | Loss: 0.00012093
Iteration 105/1000 | Loss: 0.00012000
Iteration 106/1000 | Loss: 0.00011932
Iteration 107/1000 | Loss: 0.00011869
Iteration 108/1000 | Loss: 0.00011818
Iteration 109/1000 | Loss: 0.00011760
Iteration 110/1000 | Loss: 0.00011731
Iteration 111/1000 | Loss: 0.00020773
Iteration 112/1000 | Loss: 0.00012282
Iteration 113/1000 | Loss: 0.00017869
Iteration 114/1000 | Loss: 0.00022901
Iteration 115/1000 | Loss: 0.00017906
Iteration 116/1000 | Loss: 0.00021126
Iteration 117/1000 | Loss: 0.00033408
Iteration 118/1000 | Loss: 0.00026536
Iteration 119/1000 | Loss: 0.00019580
Iteration 120/1000 | Loss: 0.00013088
Iteration 121/1000 | Loss: 0.00018008
Iteration 122/1000 | Loss: 0.00021487
Iteration 123/1000 | Loss: 0.00013168
Iteration 124/1000 | Loss: 0.00012393
Iteration 125/1000 | Loss: 0.00012121
Iteration 126/1000 | Loss: 0.00011978
Iteration 127/1000 | Loss: 0.00011904
Iteration 128/1000 | Loss: 0.00011872
Iteration 129/1000 | Loss: 0.00011848
Iteration 130/1000 | Loss: 0.00011824
Iteration 131/1000 | Loss: 0.00011802
Iteration 132/1000 | Loss: 0.00011770
Iteration 133/1000 | Loss: 0.00011743
Iteration 134/1000 | Loss: 0.00011718
Iteration 135/1000 | Loss: 0.00011693
Iteration 136/1000 | Loss: 0.00020934
Iteration 137/1000 | Loss: 0.00020932
Iteration 138/1000 | Loss: 0.00019833
Iteration 139/1000 | Loss: 0.00011644
Iteration 140/1000 | Loss: 0.00011624
Iteration 141/1000 | Loss: 0.00020355
Iteration 142/1000 | Loss: 0.00012767
Iteration 143/1000 | Loss: 0.00012195
Iteration 144/1000 | Loss: 0.00011920
Iteration 145/1000 | Loss: 0.00011777
Iteration 146/1000 | Loss: 0.00011712
Iteration 147/1000 | Loss: 0.00022526
Iteration 148/1000 | Loss: 0.00022525
Iteration 149/1000 | Loss: 0.00019234
Iteration 150/1000 | Loss: 0.00011692
Iteration 151/1000 | Loss: 0.00011658
Iteration 152/1000 | Loss: 0.00011651
Iteration 153/1000 | Loss: 0.00011650
Iteration 154/1000 | Loss: 0.00011649
Iteration 155/1000 | Loss: 0.00011648
Iteration 156/1000 | Loss: 0.00011648
Iteration 157/1000 | Loss: 0.00011648
Iteration 158/1000 | Loss: 0.00011648
Iteration 159/1000 | Loss: 0.00011648
Iteration 160/1000 | Loss: 0.00011648
Iteration 161/1000 | Loss: 0.00011648
Iteration 162/1000 | Loss: 0.00011648
Iteration 163/1000 | Loss: 0.00011648
Iteration 164/1000 | Loss: 0.00011648
Iteration 165/1000 | Loss: 0.00011647
Iteration 166/1000 | Loss: 0.00011647
Iteration 167/1000 | Loss: 0.00011647
Iteration 168/1000 | Loss: 0.00011647
Iteration 169/1000 | Loss: 0.00011647
Iteration 170/1000 | Loss: 0.00011646
Iteration 171/1000 | Loss: 0.00011643
Iteration 172/1000 | Loss: 0.00011643
Iteration 173/1000 | Loss: 0.00011642
Iteration 174/1000 | Loss: 0.00011642
Iteration 175/1000 | Loss: 0.00023007
Iteration 176/1000 | Loss: 0.00017801
Iteration 177/1000 | Loss: 0.00022279
Iteration 178/1000 | Loss: 0.00012266
Iteration 179/1000 | Loss: 0.00011952
Iteration 180/1000 | Loss: 0.00011833
Iteration 181/1000 | Loss: 0.00011751
Iteration 182/1000 | Loss: 0.00011699
Iteration 183/1000 | Loss: 0.00011684
Iteration 184/1000 | Loss: 0.00011683
Iteration 185/1000 | Loss: 0.00011678
Iteration 186/1000 | Loss: 0.00011663
Iteration 187/1000 | Loss: 0.00011655
Iteration 188/1000 | Loss: 0.00024031
Iteration 189/1000 | Loss: 0.00012171
Iteration 190/1000 | Loss: 0.00020384
Iteration 191/1000 | Loss: 0.00011661
Iteration 192/1000 | Loss: 0.00025866
Iteration 193/1000 | Loss: 0.00018914
Iteration 194/1000 | Loss: 0.00023449
Iteration 195/1000 | Loss: 0.00018563
Iteration 196/1000 | Loss: 0.00021677
Iteration 197/1000 | Loss: 0.00017297
Iteration 198/1000 | Loss: 0.00012128
Iteration 199/1000 | Loss: 0.00011968
Iteration 200/1000 | Loss: 0.00011863
Iteration 201/1000 | Loss: 0.00028104
Iteration 202/1000 | Loss: 0.00020994
Iteration 203/1000 | Loss: 0.00022004
Iteration 204/1000 | Loss: 0.00027510
Iteration 205/1000 | Loss: 0.00019705
Iteration 206/1000 | Loss: 0.00020541
Iteration 207/1000 | Loss: 0.00022276
Iteration 208/1000 | Loss: 0.00012877
Iteration 209/1000 | Loss: 0.00012305
Iteration 210/1000 | Loss: 0.00012067
Iteration 211/1000 | Loss: 0.00011942
Iteration 212/1000 | Loss: 0.00011837
Iteration 213/1000 | Loss: 0.00011771
Iteration 214/1000 | Loss: 0.00011716
Iteration 215/1000 | Loss: 0.00011693
Iteration 216/1000 | Loss: 0.00011671
Iteration 217/1000 | Loss: 0.00011668
Iteration 218/1000 | Loss: 0.00011648
Iteration 219/1000 | Loss: 0.00011627
Iteration 220/1000 | Loss: 0.00011614
Iteration 221/1000 | Loss: 0.00011609
Iteration 222/1000 | Loss: 0.00011598
Iteration 223/1000 | Loss: 0.00011579
Iteration 224/1000 | Loss: 0.00011562
Iteration 225/1000 | Loss: 0.00020919
Iteration 226/1000 | Loss: 0.00016971
Iteration 227/1000 | Loss: 0.00020856
Iteration 228/1000 | Loss: 0.00016322
Iteration 229/1000 | Loss: 0.00019633
Iteration 230/1000 | Loss: 0.00017552
Iteration 231/1000 | Loss: 0.00019351
Iteration 232/1000 | Loss: 0.00017275
Iteration 233/1000 | Loss: 0.00018680
Iteration 234/1000 | Loss: 0.00017733
Iteration 235/1000 | Loss: 0.00012258
Iteration 236/1000 | Loss: 0.00014057
Iteration 237/1000 | Loss: 0.00013013
Iteration 238/1000 | Loss: 0.00013561
Iteration 239/1000 | Loss: 0.00012745
Iteration 240/1000 | Loss: 0.00011590
Iteration 241/1000 | Loss: 0.00011530
Iteration 242/1000 | Loss: 0.00012933
Iteration 243/1000 | Loss: 0.00012671
Iteration 244/1000 | Loss: 0.00013038
Iteration 245/1000 | Loss: 0.00019915
Iteration 246/1000 | Loss: 0.00012006
Iteration 247/1000 | Loss: 0.00011703
Iteration 248/1000 | Loss: 0.00013354
Iteration 249/1000 | Loss: 0.00011953
Iteration 250/1000 | Loss: 0.00011797
Iteration 251/1000 | Loss: 0.00011718
Iteration 252/1000 | Loss: 0.00015188
Iteration 253/1000 | Loss: 0.00012045
Iteration 254/1000 | Loss: 0.00011797
Iteration 255/1000 | Loss: 0.00011676
Iteration 256/1000 | Loss: 0.00011627
Iteration 257/1000 | Loss: 0.00011586
Iteration 258/1000 | Loss: 0.00011549
Iteration 259/1000 | Loss: 0.00011538
Iteration 260/1000 | Loss: 0.00011535
Iteration 261/1000 | Loss: 0.00011532
Iteration 262/1000 | Loss: 0.00011528
Iteration 263/1000 | Loss: 0.00011525
Iteration 264/1000 | Loss: 0.00011507
Iteration 265/1000 | Loss: 0.00011487
Iteration 266/1000 | Loss: 0.00011476
Iteration 267/1000 | Loss: 0.00011470
Iteration 268/1000 | Loss: 0.00011468
Iteration 269/1000 | Loss: 0.00011468
Iteration 270/1000 | Loss: 0.00011466
Iteration 271/1000 | Loss: 0.00011466
Iteration 272/1000 | Loss: 0.00011466
Iteration 273/1000 | Loss: 0.00011466
Iteration 274/1000 | Loss: 0.00011466
Iteration 275/1000 | Loss: 0.00011465
Iteration 276/1000 | Loss: 0.00011465
Iteration 277/1000 | Loss: 0.00011465
Iteration 278/1000 | Loss: 0.00011464
Iteration 279/1000 | Loss: 0.00011464
Iteration 280/1000 | Loss: 0.00011464
Iteration 281/1000 | Loss: 0.00011464
Iteration 282/1000 | Loss: 0.00011464
Iteration 283/1000 | Loss: 0.00011464
Iteration 284/1000 | Loss: 0.00011464
Iteration 285/1000 | Loss: 0.00011464
Iteration 286/1000 | Loss: 0.00011464
Iteration 287/1000 | Loss: 0.00011463
Iteration 288/1000 | Loss: 0.00011462
Iteration 289/1000 | Loss: 0.00011462
Iteration 290/1000 | Loss: 0.00011462
Iteration 291/1000 | Loss: 0.00011462
Iteration 292/1000 | Loss: 0.00011462
Iteration 293/1000 | Loss: 0.00011461
Iteration 294/1000 | Loss: 0.00011461
Iteration 295/1000 | Loss: 0.00011461
Iteration 296/1000 | Loss: 0.00011461
Iteration 297/1000 | Loss: 0.00011461
Iteration 298/1000 | Loss: 0.00011460
Iteration 299/1000 | Loss: 0.00011460
Iteration 300/1000 | Loss: 0.00011460
Iteration 301/1000 | Loss: 0.00011460
Iteration 302/1000 | Loss: 0.00011460
Iteration 303/1000 | Loss: 0.00011460
Iteration 304/1000 | Loss: 0.00011460
Iteration 305/1000 | Loss: 0.00011460
Iteration 306/1000 | Loss: 0.00011460
Iteration 307/1000 | Loss: 0.00011460
Iteration 308/1000 | Loss: 0.00011460
Iteration 309/1000 | Loss: 0.00011460
Iteration 310/1000 | Loss: 0.00011460
Iteration 311/1000 | Loss: 0.00011459
Iteration 312/1000 | Loss: 0.00011459
Iteration 313/1000 | Loss: 0.00011459
Iteration 314/1000 | Loss: 0.00011459
Iteration 315/1000 | Loss: 0.00011459
Iteration 316/1000 | Loss: 0.00011458
Iteration 317/1000 | Loss: 0.00011458
Iteration 318/1000 | Loss: 0.00011458
Iteration 319/1000 | Loss: 0.00011458
Iteration 320/1000 | Loss: 0.00011458
Iteration 321/1000 | Loss: 0.00011458
Iteration 322/1000 | Loss: 0.00011458
Iteration 323/1000 | Loss: 0.00011458
Iteration 324/1000 | Loss: 0.00011458
Iteration 325/1000 | Loss: 0.00011458
Iteration 326/1000 | Loss: 0.00011458
Iteration 327/1000 | Loss: 0.00011458
Iteration 328/1000 | Loss: 0.00011457
Iteration 329/1000 | Loss: 0.00011457
Iteration 330/1000 | Loss: 0.00011457
Iteration 331/1000 | Loss: 0.00011457
Iteration 332/1000 | Loss: 0.00011457
Iteration 333/1000 | Loss: 0.00011456
Iteration 334/1000 | Loss: 0.00011456
Iteration 335/1000 | Loss: 0.00011455
Iteration 336/1000 | Loss: 0.00011455
Iteration 337/1000 | Loss: 0.00011455
Iteration 338/1000 | Loss: 0.00011455
Iteration 339/1000 | Loss: 0.00011455
Iteration 340/1000 | Loss: 0.00011455
Iteration 341/1000 | Loss: 0.00011454
Iteration 342/1000 | Loss: 0.00011454
Iteration 343/1000 | Loss: 0.00011454
Iteration 344/1000 | Loss: 0.00011454
Iteration 345/1000 | Loss: 0.00011454
Iteration 346/1000 | Loss: 0.00011454
Iteration 347/1000 | Loss: 0.00011453
Iteration 348/1000 | Loss: 0.00011453
Iteration 349/1000 | Loss: 0.00011452
Iteration 350/1000 | Loss: 0.00011452
Iteration 351/1000 | Loss: 0.00011452
Iteration 352/1000 | Loss: 0.00011452
Iteration 353/1000 | Loss: 0.00011452
Iteration 354/1000 | Loss: 0.00011452
Iteration 355/1000 | Loss: 0.00011452
Iteration 356/1000 | Loss: 0.00011452
Iteration 357/1000 | Loss: 0.00011452
Iteration 358/1000 | Loss: 0.00011452
Iteration 359/1000 | Loss: 0.00011451
Iteration 360/1000 | Loss: 0.00011451
Iteration 361/1000 | Loss: 0.00011451
Iteration 362/1000 | Loss: 0.00011451
Iteration 363/1000 | Loss: 0.00011451
Iteration 364/1000 | Loss: 0.00011451
Iteration 365/1000 | Loss: 0.00011451
Iteration 366/1000 | Loss: 0.00011451
Iteration 367/1000 | Loss: 0.00011451
Iteration 368/1000 | Loss: 0.00011451
Iteration 369/1000 | Loss: 0.00011451
Iteration 370/1000 | Loss: 0.00011451
Iteration 371/1000 | Loss: 0.00011451
Iteration 372/1000 | Loss: 0.00011451
Iteration 373/1000 | Loss: 0.00011451
Iteration 374/1000 | Loss: 0.00011451
Iteration 375/1000 | Loss: 0.00011451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 375. Stopping optimization.
Last 5 losses: [0.00011450637975940481, 0.00011450637975940481, 0.00011450637975940481, 0.00011450637975940481, 0.00011450637975940481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011450637975940481

Optimization complete. Final v2v error: 8.600772857666016 mm

Highest mean error: 11.669078826904297 mm for frame 86

Lowest mean error: 7.275240421295166 mm for frame 47

Saving results

Total time: 384.2553482055664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421643
Iteration 2/25 | Loss: 0.00121293
Iteration 3/25 | Loss: 0.00114666
Iteration 4/25 | Loss: 0.00113126
Iteration 5/25 | Loss: 0.00112781
Iteration 6/25 | Loss: 0.00112750
Iteration 7/25 | Loss: 0.00112750
Iteration 8/25 | Loss: 0.00112750
Iteration 9/25 | Loss: 0.00112750
Iteration 10/25 | Loss: 0.00112750
Iteration 11/25 | Loss: 0.00112750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011274995049461722, 0.0011274995049461722, 0.0011274995049461722, 0.0011274995049461722, 0.0011274995049461722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011274995049461722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23385382
Iteration 2/25 | Loss: 0.00219242
Iteration 3/25 | Loss: 0.00219242
Iteration 4/25 | Loss: 0.00219242
Iteration 5/25 | Loss: 0.00219242
Iteration 6/25 | Loss: 0.00219242
Iteration 7/25 | Loss: 0.00219242
Iteration 8/25 | Loss: 0.00219242
Iteration 9/25 | Loss: 0.00219242
Iteration 10/25 | Loss: 0.00219242
Iteration 11/25 | Loss: 0.00219242
Iteration 12/25 | Loss: 0.00219242
Iteration 13/25 | Loss: 0.00219242
Iteration 14/25 | Loss: 0.00219242
Iteration 15/25 | Loss: 0.00219242
Iteration 16/25 | Loss: 0.00219242
Iteration 17/25 | Loss: 0.00219242
Iteration 18/25 | Loss: 0.00219242
Iteration 19/25 | Loss: 0.00219242
Iteration 20/25 | Loss: 0.00219242
Iteration 21/25 | Loss: 0.00219242
Iteration 22/25 | Loss: 0.00219242
Iteration 23/25 | Loss: 0.00219242
Iteration 24/25 | Loss: 0.00219242
Iteration 25/25 | Loss: 0.00219242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219242
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00001977
Iteration 4/1000 | Loss: 0.00001773
Iteration 5/1000 | Loss: 0.00001633
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001389
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001389
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001381
Iteration 64/1000 | Loss: 0.00001381
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001380
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001379
Iteration 84/1000 | Loss: 0.00001379
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001372
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001371
Iteration 141/1000 | Loss: 0.00001371
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001371
Iteration 147/1000 | Loss: 0.00001371
Iteration 148/1000 | Loss: 0.00001371
Iteration 149/1000 | Loss: 0.00001371
Iteration 150/1000 | Loss: 0.00001371
Iteration 151/1000 | Loss: 0.00001371
Iteration 152/1000 | Loss: 0.00001371
Iteration 153/1000 | Loss: 0.00001371
Iteration 154/1000 | Loss: 0.00001371
Iteration 155/1000 | Loss: 0.00001371
Iteration 156/1000 | Loss: 0.00001371
Iteration 157/1000 | Loss: 0.00001371
Iteration 158/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.3705122910323553e-05, 1.3705122910323553e-05, 1.3705122910323553e-05, 1.3705122910323553e-05, 1.3705122910323553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3705122910323553e-05

Optimization complete. Final v2v error: 3.1759939193725586 mm

Highest mean error: 3.488386869430542 mm for frame 118

Lowest mean error: 2.679128885269165 mm for frame 145

Saving results

Total time: 33.39059281349182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437533
Iteration 2/25 | Loss: 0.00129134
Iteration 3/25 | Loss: 0.00116058
Iteration 4/25 | Loss: 0.00114770
Iteration 5/25 | Loss: 0.00114474
Iteration 6/25 | Loss: 0.00114416
Iteration 7/25 | Loss: 0.00114416
Iteration 8/25 | Loss: 0.00114416
Iteration 9/25 | Loss: 0.00114416
Iteration 10/25 | Loss: 0.00114416
Iteration 11/25 | Loss: 0.00114416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00114415620919317, 0.00114415620919317, 0.00114415620919317, 0.00114415620919317, 0.00114415620919317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00114415620919317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87427837
Iteration 2/25 | Loss: 0.00186402
Iteration 3/25 | Loss: 0.00186402
Iteration 4/25 | Loss: 0.00186402
Iteration 5/25 | Loss: 0.00186402
Iteration 6/25 | Loss: 0.00186402
Iteration 7/25 | Loss: 0.00186402
Iteration 8/25 | Loss: 0.00186402
Iteration 9/25 | Loss: 0.00186402
Iteration 10/25 | Loss: 0.00186402
Iteration 11/25 | Loss: 0.00186402
Iteration 12/25 | Loss: 0.00186402
Iteration 13/25 | Loss: 0.00186402
Iteration 14/25 | Loss: 0.00186402
Iteration 15/25 | Loss: 0.00186402
Iteration 16/25 | Loss: 0.00186402
Iteration 17/25 | Loss: 0.00186402
Iteration 18/25 | Loss: 0.00186402
Iteration 19/25 | Loss: 0.00186402
Iteration 20/25 | Loss: 0.00186402
Iteration 21/25 | Loss: 0.00186402
Iteration 22/25 | Loss: 0.00186402
Iteration 23/25 | Loss: 0.00186402
Iteration 24/25 | Loss: 0.00186402
Iteration 25/25 | Loss: 0.00186402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001864020829088986, 0.001864020829088986, 0.001864020829088986, 0.001864020829088986, 0.001864020829088986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001864020829088986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186402
Iteration 2/1000 | Loss: 0.00004254
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001328
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001295
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001284
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001284
Iteration 121/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2838089787692297e-05, 1.2838089787692297e-05, 1.2838089787692297e-05, 1.2838089787692297e-05, 1.2838089787692297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2838089787692297e-05

Optimization complete. Final v2v error: 3.010392665863037 mm

Highest mean error: 3.1743147373199463 mm for frame 79

Lowest mean error: 2.906423330307007 mm for frame 111

Saving results

Total time: 30.27238440513611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938772
Iteration 2/25 | Loss: 0.00141480
Iteration 3/25 | Loss: 0.00128374
Iteration 4/25 | Loss: 0.00126776
Iteration 5/25 | Loss: 0.00126267
Iteration 6/25 | Loss: 0.00126267
Iteration 7/25 | Loss: 0.00126267
Iteration 8/25 | Loss: 0.00126267
Iteration 9/25 | Loss: 0.00126267
Iteration 10/25 | Loss: 0.00126267
Iteration 11/25 | Loss: 0.00126267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001262665493413806, 0.001262665493413806, 0.001262665493413806, 0.001262665493413806, 0.001262665493413806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001262665493413806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18886197
Iteration 2/25 | Loss: 0.00207554
Iteration 3/25 | Loss: 0.00207546
Iteration 4/25 | Loss: 0.00207546
Iteration 5/25 | Loss: 0.00207546
Iteration 6/25 | Loss: 0.00207546
Iteration 7/25 | Loss: 0.00207546
Iteration 8/25 | Loss: 0.00207546
Iteration 9/25 | Loss: 0.00207546
Iteration 10/25 | Loss: 0.00207546
Iteration 11/25 | Loss: 0.00207546
Iteration 12/25 | Loss: 0.00207546
Iteration 13/25 | Loss: 0.00207546
Iteration 14/25 | Loss: 0.00207546
Iteration 15/25 | Loss: 0.00207546
Iteration 16/25 | Loss: 0.00207546
Iteration 17/25 | Loss: 0.00207546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020754581782966852, 0.0020754581782966852, 0.0020754581782966852, 0.0020754581782966852, 0.0020754581782966852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020754581782966852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207546
Iteration 2/1000 | Loss: 0.00004620
Iteration 3/1000 | Loss: 0.00003365
Iteration 4/1000 | Loss: 0.00002947
Iteration 5/1000 | Loss: 0.00002781
Iteration 6/1000 | Loss: 0.00002661
Iteration 7/1000 | Loss: 0.00002607
Iteration 8/1000 | Loss: 0.00002561
Iteration 9/1000 | Loss: 0.00002528
Iteration 10/1000 | Loss: 0.00002509
Iteration 11/1000 | Loss: 0.00002483
Iteration 12/1000 | Loss: 0.00002460
Iteration 13/1000 | Loss: 0.00002457
Iteration 14/1000 | Loss: 0.00002452
Iteration 15/1000 | Loss: 0.00002441
Iteration 16/1000 | Loss: 0.00002439
Iteration 17/1000 | Loss: 0.00002438
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002437
Iteration 20/1000 | Loss: 0.00002432
Iteration 21/1000 | Loss: 0.00002432
Iteration 22/1000 | Loss: 0.00002431
Iteration 23/1000 | Loss: 0.00002431
Iteration 24/1000 | Loss: 0.00002430
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002427
Iteration 27/1000 | Loss: 0.00002426
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002426
Iteration 31/1000 | Loss: 0.00002426
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002426
Iteration 34/1000 | Loss: 0.00002426
Iteration 35/1000 | Loss: 0.00002426
Iteration 36/1000 | Loss: 0.00002426
Iteration 37/1000 | Loss: 0.00002426
Iteration 38/1000 | Loss: 0.00002425
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002424
Iteration 41/1000 | Loss: 0.00002424
Iteration 42/1000 | Loss: 0.00002424
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002422
Iteration 57/1000 | Loss: 0.00002422
Iteration 58/1000 | Loss: 0.00002422
Iteration 59/1000 | Loss: 0.00002422
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002421
Iteration 65/1000 | Loss: 0.00002420
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002420
Iteration 70/1000 | Loss: 0.00002420
Iteration 71/1000 | Loss: 0.00002420
Iteration 72/1000 | Loss: 0.00002420
Iteration 73/1000 | Loss: 0.00002420
Iteration 74/1000 | Loss: 0.00002420
Iteration 75/1000 | Loss: 0.00002420
Iteration 76/1000 | Loss: 0.00002420
Iteration 77/1000 | Loss: 0.00002420
Iteration 78/1000 | Loss: 0.00002420
Iteration 79/1000 | Loss: 0.00002420
Iteration 80/1000 | Loss: 0.00002420
Iteration 81/1000 | Loss: 0.00002420
Iteration 82/1000 | Loss: 0.00002420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.4199034669436514e-05, 2.4199034669436514e-05, 2.4199034669436514e-05, 2.4199034669436514e-05, 2.4199034669436514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4199034669436514e-05

Optimization complete. Final v2v error: 4.146965026855469 mm

Highest mean error: 4.362542629241943 mm for frame 186

Lowest mean error: 3.8013546466827393 mm for frame 1

Saving results

Total time: 34.484869718551636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709427
Iteration 2/25 | Loss: 0.00128388
Iteration 3/25 | Loss: 0.00118981
Iteration 4/25 | Loss: 0.00118095
Iteration 5/25 | Loss: 0.00117884
Iteration 6/25 | Loss: 0.00117807
Iteration 7/25 | Loss: 0.00117807
Iteration 8/25 | Loss: 0.00117807
Iteration 9/25 | Loss: 0.00117807
Iteration 10/25 | Loss: 0.00117807
Iteration 11/25 | Loss: 0.00117807
Iteration 12/25 | Loss: 0.00117807
Iteration 13/25 | Loss: 0.00117807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011780732311308384, 0.0011780732311308384, 0.0011780732311308384, 0.0011780732311308384, 0.0011780732311308384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011780732311308384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30698144
Iteration 2/25 | Loss: 0.00211232
Iteration 3/25 | Loss: 0.00211232
Iteration 4/25 | Loss: 0.00211231
Iteration 5/25 | Loss: 0.00211231
Iteration 6/25 | Loss: 0.00211231
Iteration 7/25 | Loss: 0.00211231
Iteration 8/25 | Loss: 0.00211231
Iteration 9/25 | Loss: 0.00211231
Iteration 10/25 | Loss: 0.00211231
Iteration 11/25 | Loss: 0.00211231
Iteration 12/25 | Loss: 0.00211231
Iteration 13/25 | Loss: 0.00211231
Iteration 14/25 | Loss: 0.00211231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0021123122423887253, 0.0021123122423887253, 0.0021123122423887253, 0.0021123122423887253, 0.0021123122423887253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021123122423887253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211231
Iteration 2/1000 | Loss: 0.00003895
Iteration 3/1000 | Loss: 0.00002321
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001702
Iteration 7/1000 | Loss: 0.00001640
Iteration 8/1000 | Loss: 0.00001597
Iteration 9/1000 | Loss: 0.00001569
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001534
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001527
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001508
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00001501
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00001501
Iteration 60/1000 | Loss: 0.00001501
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001501
Iteration 65/1000 | Loss: 0.00001501
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001501
Iteration 68/1000 | Loss: 0.00001501
Iteration 69/1000 | Loss: 0.00001501
Iteration 70/1000 | Loss: 0.00001501
Iteration 71/1000 | Loss: 0.00001501
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.5007620277174283e-05, 1.5007620277174283e-05, 1.5007620277174283e-05, 1.5007620277174283e-05, 1.5007620277174283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5007620277174283e-05

Optimization complete. Final v2v error: 3.378373384475708 mm

Highest mean error: 4.126601219177246 mm for frame 73

Lowest mean error: 2.942509412765503 mm for frame 10

Saving results

Total time: 29.156322717666626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934439
Iteration 2/25 | Loss: 0.00171628
Iteration 3/25 | Loss: 0.00132185
Iteration 4/25 | Loss: 0.00125928
Iteration 5/25 | Loss: 0.00123864
Iteration 6/25 | Loss: 0.00123197
Iteration 7/25 | Loss: 0.00122833
Iteration 8/25 | Loss: 0.00122774
Iteration 9/25 | Loss: 0.00123008
Iteration 10/25 | Loss: 0.00122458
Iteration 11/25 | Loss: 0.00122324
Iteration 12/25 | Loss: 0.00122128
Iteration 13/25 | Loss: 0.00121991
Iteration 14/25 | Loss: 0.00121885
Iteration 15/25 | Loss: 0.00122281
Iteration 16/25 | Loss: 0.00121744
Iteration 17/25 | Loss: 0.00121609
Iteration 18/25 | Loss: 0.00121583
Iteration 19/25 | Loss: 0.00121577
Iteration 20/25 | Loss: 0.00121577
Iteration 21/25 | Loss: 0.00121576
Iteration 22/25 | Loss: 0.00121576
Iteration 23/25 | Loss: 0.00121576
Iteration 24/25 | Loss: 0.00121576
Iteration 25/25 | Loss: 0.00121576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.60434723
Iteration 2/25 | Loss: 0.00202109
Iteration 3/25 | Loss: 0.00193072
Iteration 4/25 | Loss: 0.00193072
Iteration 5/25 | Loss: 0.00193072
Iteration 6/25 | Loss: 0.00193072
Iteration 7/25 | Loss: 0.00193072
Iteration 8/25 | Loss: 0.00193072
Iteration 9/25 | Loss: 0.00193072
Iteration 10/25 | Loss: 0.00193072
Iteration 11/25 | Loss: 0.00193072
Iteration 12/25 | Loss: 0.00193072
Iteration 13/25 | Loss: 0.00193072
Iteration 14/25 | Loss: 0.00193072
Iteration 15/25 | Loss: 0.00193072
Iteration 16/25 | Loss: 0.00193072
Iteration 17/25 | Loss: 0.00193072
Iteration 18/25 | Loss: 0.00193072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019307159818708897, 0.0019307159818708897, 0.0019307159818708897, 0.0019307159818708897, 0.0019307159818708897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019307159818708897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193072
Iteration 2/1000 | Loss: 0.00007378
Iteration 3/1000 | Loss: 0.00012624
Iteration 4/1000 | Loss: 0.00002768
Iteration 5/1000 | Loss: 0.00005812
Iteration 6/1000 | Loss: 0.00008994
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00002087
Iteration 9/1000 | Loss: 0.00002025
Iteration 10/1000 | Loss: 0.00008625
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00007244
Iteration 13/1000 | Loss: 0.00001927
Iteration 14/1000 | Loss: 0.00006365
Iteration 15/1000 | Loss: 0.00008092
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001880
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00006310
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001856
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001855
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001855
Iteration 27/1000 | Loss: 0.00001854
Iteration 28/1000 | Loss: 0.00001853
Iteration 29/1000 | Loss: 0.00001853
Iteration 30/1000 | Loss: 0.00001853
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001843
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001842
Iteration 49/1000 | Loss: 0.00001842
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001842
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001840
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001837
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00001836
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001835
Iteration 75/1000 | Loss: 0.00001835
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001835
Iteration 80/1000 | Loss: 0.00001835
Iteration 81/1000 | Loss: 0.00001835
Iteration 82/1000 | Loss: 0.00001835
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001835
Iteration 87/1000 | Loss: 0.00001835
Iteration 88/1000 | Loss: 0.00001835
Iteration 89/1000 | Loss: 0.00001835
Iteration 90/1000 | Loss: 0.00001835
Iteration 91/1000 | Loss: 0.00001835
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001832
Iteration 103/1000 | Loss: 0.00001832
Iteration 104/1000 | Loss: 0.00001832
Iteration 105/1000 | Loss: 0.00001832
Iteration 106/1000 | Loss: 0.00001832
Iteration 107/1000 | Loss: 0.00001832
Iteration 108/1000 | Loss: 0.00001832
Iteration 109/1000 | Loss: 0.00001831
Iteration 110/1000 | Loss: 0.00001831
Iteration 111/1000 | Loss: 0.00001831
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001829
Iteration 116/1000 | Loss: 0.00001829
Iteration 117/1000 | Loss: 0.00001829
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00001828
Iteration 120/1000 | Loss: 0.00001828
Iteration 121/1000 | Loss: 0.00001828
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Iteration 127/1000 | Loss: 0.00001827
Iteration 128/1000 | Loss: 0.00001826
Iteration 129/1000 | Loss: 0.00001826
Iteration 130/1000 | Loss: 0.00001826
Iteration 131/1000 | Loss: 0.00001826
Iteration 132/1000 | Loss: 0.00001826
Iteration 133/1000 | Loss: 0.00001826
Iteration 134/1000 | Loss: 0.00001825
Iteration 135/1000 | Loss: 0.00001825
Iteration 136/1000 | Loss: 0.00001825
Iteration 137/1000 | Loss: 0.00001825
Iteration 138/1000 | Loss: 0.00001825
Iteration 139/1000 | Loss: 0.00001825
Iteration 140/1000 | Loss: 0.00001825
Iteration 141/1000 | Loss: 0.00001825
Iteration 142/1000 | Loss: 0.00001825
Iteration 143/1000 | Loss: 0.00001825
Iteration 144/1000 | Loss: 0.00001825
Iteration 145/1000 | Loss: 0.00001825
Iteration 146/1000 | Loss: 0.00001825
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001825
Iteration 149/1000 | Loss: 0.00001825
Iteration 150/1000 | Loss: 0.00001825
Iteration 151/1000 | Loss: 0.00001824
Iteration 152/1000 | Loss: 0.00001824
Iteration 153/1000 | Loss: 0.00001824
Iteration 154/1000 | Loss: 0.00001824
Iteration 155/1000 | Loss: 0.00001824
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001824
Iteration 158/1000 | Loss: 0.00001824
Iteration 159/1000 | Loss: 0.00001824
Iteration 160/1000 | Loss: 0.00001824
Iteration 161/1000 | Loss: 0.00001824
Iteration 162/1000 | Loss: 0.00001824
Iteration 163/1000 | Loss: 0.00001824
Iteration 164/1000 | Loss: 0.00001824
Iteration 165/1000 | Loss: 0.00001824
Iteration 166/1000 | Loss: 0.00001823
Iteration 167/1000 | Loss: 0.00001823
Iteration 168/1000 | Loss: 0.00001823
Iteration 169/1000 | Loss: 0.00001823
Iteration 170/1000 | Loss: 0.00001823
Iteration 171/1000 | Loss: 0.00001823
Iteration 172/1000 | Loss: 0.00001823
Iteration 173/1000 | Loss: 0.00001823
Iteration 174/1000 | Loss: 0.00001823
Iteration 175/1000 | Loss: 0.00001823
Iteration 176/1000 | Loss: 0.00001823
Iteration 177/1000 | Loss: 0.00001823
Iteration 178/1000 | Loss: 0.00001823
Iteration 179/1000 | Loss: 0.00001823
Iteration 180/1000 | Loss: 0.00001823
Iteration 181/1000 | Loss: 0.00001823
Iteration 182/1000 | Loss: 0.00001823
Iteration 183/1000 | Loss: 0.00001823
Iteration 184/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.8231436115456745e-05, 1.8231436115456745e-05, 1.8231436115456745e-05, 1.8231436115456745e-05, 1.8231436115456745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8231436115456745e-05

Optimization complete. Final v2v error: 3.6845555305480957 mm

Highest mean error: 4.541561126708984 mm for frame 70

Lowest mean error: 2.955612897872925 mm for frame 0

Saving results

Total time: 71.16803431510925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048682
Iteration 2/25 | Loss: 0.00293701
Iteration 3/25 | Loss: 0.00245839
Iteration 4/25 | Loss: 0.00234333
Iteration 5/25 | Loss: 0.00208156
Iteration 6/25 | Loss: 0.00202241
Iteration 7/25 | Loss: 0.00200558
Iteration 8/25 | Loss: 0.00199475
Iteration 9/25 | Loss: 0.00198983
Iteration 10/25 | Loss: 0.00198824
Iteration 11/25 | Loss: 0.00198782
Iteration 12/25 | Loss: 0.00198758
Iteration 13/25 | Loss: 0.00198748
Iteration 14/25 | Loss: 0.00198747
Iteration 15/25 | Loss: 0.00198747
Iteration 16/25 | Loss: 0.00198747
Iteration 17/25 | Loss: 0.00198747
Iteration 18/25 | Loss: 0.00198747
Iteration 19/25 | Loss: 0.00198746
Iteration 20/25 | Loss: 0.00198746
Iteration 21/25 | Loss: 0.00198746
Iteration 22/25 | Loss: 0.00198746
Iteration 23/25 | Loss: 0.00198746
Iteration 24/25 | Loss: 0.00198746
Iteration 25/25 | Loss: 0.00198746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18797922
Iteration 2/25 | Loss: 0.00705509
Iteration 3/25 | Loss: 0.00705509
Iteration 4/25 | Loss: 0.00705509
Iteration 5/25 | Loss: 0.00705509
Iteration 6/25 | Loss: 0.00705509
Iteration 7/25 | Loss: 0.00705509
Iteration 8/25 | Loss: 0.00705509
Iteration 9/25 | Loss: 0.00705509
Iteration 10/25 | Loss: 0.00705509
Iteration 11/25 | Loss: 0.00705509
Iteration 12/25 | Loss: 0.00705509
Iteration 13/25 | Loss: 0.00705509
Iteration 14/25 | Loss: 0.00705509
Iteration 15/25 | Loss: 0.00705509
Iteration 16/25 | Loss: 0.00705509
Iteration 17/25 | Loss: 0.00705509
Iteration 18/25 | Loss: 0.00705509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0070550888776779175, 0.0070550888776779175, 0.0070550888776779175, 0.0070550888776779175, 0.0070550888776779175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0070550888776779175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00705509
Iteration 2/1000 | Loss: 0.00085335
Iteration 3/1000 | Loss: 0.00066200
Iteration 4/1000 | Loss: 0.00056926
Iteration 5/1000 | Loss: 0.00051073
Iteration 6/1000 | Loss: 0.00046630
Iteration 7/1000 | Loss: 0.00043905
Iteration 8/1000 | Loss: 0.00040904
Iteration 9/1000 | Loss: 0.00038863
Iteration 10/1000 | Loss: 0.00037107
Iteration 11/1000 | Loss: 0.00035873
Iteration 12/1000 | Loss: 0.00035248
Iteration 13/1000 | Loss: 0.00034891
Iteration 14/1000 | Loss: 0.00034397
Iteration 15/1000 | Loss: 0.00034031
Iteration 16/1000 | Loss: 0.00033853
Iteration 17/1000 | Loss: 0.00033720
Iteration 18/1000 | Loss: 0.00033574
Iteration 19/1000 | Loss: 0.00033392
Iteration 20/1000 | Loss: 0.00033283
Iteration 21/1000 | Loss: 0.00211586
Iteration 22/1000 | Loss: 0.02404622
Iteration 23/1000 | Loss: 0.01005273
Iteration 24/1000 | Loss: 0.02100142
Iteration 25/1000 | Loss: 0.01141834
Iteration 26/1000 | Loss: 0.01811571
Iteration 27/1000 | Loss: 0.00846495
Iteration 28/1000 | Loss: 0.01218976
Iteration 29/1000 | Loss: 0.00760983
Iteration 30/1000 | Loss: 0.01747781
Iteration 31/1000 | Loss: 0.01332931
Iteration 32/1000 | Loss: 0.00810083
Iteration 33/1000 | Loss: 0.00148236
Iteration 34/1000 | Loss: 0.00074518
Iteration 35/1000 | Loss: 0.00041292
Iteration 36/1000 | Loss: 0.00030200
Iteration 37/1000 | Loss: 0.00021056
Iteration 38/1000 | Loss: 0.00015531
Iteration 39/1000 | Loss: 0.00012143
Iteration 40/1000 | Loss: 0.00010117
Iteration 41/1000 | Loss: 0.00008647
Iteration 42/1000 | Loss: 0.00007183
Iteration 43/1000 | Loss: 0.00005853
Iteration 44/1000 | Loss: 0.00004904
Iteration 45/1000 | Loss: 0.00004165
Iteration 46/1000 | Loss: 0.00003484
Iteration 47/1000 | Loss: 0.00002870
Iteration 48/1000 | Loss: 0.00002450
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00001947
Iteration 52/1000 | Loss: 0.00001892
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001829
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001768
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001752
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001751
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001751
Iteration 139/1000 | Loss: 0.00001751
Iteration 140/1000 | Loss: 0.00001751
Iteration 141/1000 | Loss: 0.00001751
Iteration 142/1000 | Loss: 0.00001751
Iteration 143/1000 | Loss: 0.00001751
Iteration 144/1000 | Loss: 0.00001751
Iteration 145/1000 | Loss: 0.00001751
Iteration 146/1000 | Loss: 0.00001751
Iteration 147/1000 | Loss: 0.00001751
Iteration 148/1000 | Loss: 0.00001751
Iteration 149/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.7508278688183054e-05, 1.7508278688183054e-05, 1.7508278688183054e-05, 1.7508278688183054e-05, 1.7508278688183054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7508278688183054e-05

Optimization complete. Final v2v error: 3.569917678833008 mm

Highest mean error: 3.7437095642089844 mm for frame 75

Lowest mean error: 3.4670703411102295 mm for frame 118

Saving results

Total time: 105.7086889743805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511189
Iteration 2/25 | Loss: 0.00131461
Iteration 3/25 | Loss: 0.00121311
Iteration 4/25 | Loss: 0.00118852
Iteration 5/25 | Loss: 0.00117945
Iteration 6/25 | Loss: 0.00117631
Iteration 7/25 | Loss: 0.00117555
Iteration 8/25 | Loss: 0.00117555
Iteration 9/25 | Loss: 0.00117555
Iteration 10/25 | Loss: 0.00117555
Iteration 11/25 | Loss: 0.00117555
Iteration 12/25 | Loss: 0.00117555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011755505111068487, 0.0011755505111068487, 0.0011755505111068487, 0.0011755505111068487, 0.0011755505111068487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011755505111068487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.56139326
Iteration 2/25 | Loss: 0.00241354
Iteration 3/25 | Loss: 0.00241354
Iteration 4/25 | Loss: 0.00241354
Iteration 5/25 | Loss: 0.00241354
Iteration 6/25 | Loss: 0.00241354
Iteration 7/25 | Loss: 0.00241354
Iteration 8/25 | Loss: 0.00241354
Iteration 9/25 | Loss: 0.00241354
Iteration 10/25 | Loss: 0.00241354
Iteration 11/25 | Loss: 0.00241354
Iteration 12/25 | Loss: 0.00241354
Iteration 13/25 | Loss: 0.00241354
Iteration 14/25 | Loss: 0.00241354
Iteration 15/25 | Loss: 0.00241354
Iteration 16/25 | Loss: 0.00241354
Iteration 17/25 | Loss: 0.00241354
Iteration 18/25 | Loss: 0.00241354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002413535723462701, 0.002413535723462701, 0.002413535723462701, 0.002413535723462701, 0.002413535723462701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002413535723462701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241354
Iteration 2/1000 | Loss: 0.00004468
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00002368
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00002015
Iteration 7/1000 | Loss: 0.00001942
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001764
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001759
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001759
Iteration 39/1000 | Loss: 0.00001759
Iteration 40/1000 | Loss: 0.00001759
Iteration 41/1000 | Loss: 0.00001759
Iteration 42/1000 | Loss: 0.00001759
Iteration 43/1000 | Loss: 0.00001759
Iteration 44/1000 | Loss: 0.00001759
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.7590235074749216e-05, 1.7590235074749216e-05, 1.7590235074749216e-05, 1.7590235074749216e-05, 1.7590235074749216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7590235074749216e-05

Optimization complete. Final v2v error: 3.6329214572906494 mm

Highest mean error: 3.9539997577667236 mm for frame 93

Lowest mean error: 3.2446088790893555 mm for frame 138

Saving results

Total time: 30.689939975738525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447940
Iteration 2/25 | Loss: 0.00125103
Iteration 3/25 | Loss: 0.00115690
Iteration 4/25 | Loss: 0.00114045
Iteration 5/25 | Loss: 0.00113416
Iteration 6/25 | Loss: 0.00113260
Iteration 7/25 | Loss: 0.00113240
Iteration 8/25 | Loss: 0.00113240
Iteration 9/25 | Loss: 0.00113240
Iteration 10/25 | Loss: 0.00113240
Iteration 11/25 | Loss: 0.00113240
Iteration 12/25 | Loss: 0.00113240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011324031511321664, 0.0011324031511321664, 0.0011324031511321664, 0.0011324031511321664, 0.0011324031511321664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011324031511321664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09618282
Iteration 2/25 | Loss: 0.00241022
Iteration 3/25 | Loss: 0.00241019
Iteration 4/25 | Loss: 0.00241019
Iteration 5/25 | Loss: 0.00241019
Iteration 6/25 | Loss: 0.00241019
Iteration 7/25 | Loss: 0.00241019
Iteration 8/25 | Loss: 0.00241019
Iteration 9/25 | Loss: 0.00241019
Iteration 10/25 | Loss: 0.00241019
Iteration 11/25 | Loss: 0.00241019
Iteration 12/25 | Loss: 0.00241019
Iteration 13/25 | Loss: 0.00241019
Iteration 14/25 | Loss: 0.00241019
Iteration 15/25 | Loss: 0.00241019
Iteration 16/25 | Loss: 0.00241019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024101922754198313, 0.0024101922754198313, 0.0024101922754198313, 0.0024101922754198313, 0.0024101922754198313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024101922754198313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241019
Iteration 2/1000 | Loss: 0.00005090
Iteration 3/1000 | Loss: 0.00002656
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001335
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001330
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001314
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001312
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001310
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001305
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001303
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.303343287872849e-05, 1.303343287872849e-05, 1.303343287872849e-05, 1.303343287872849e-05, 1.303343287872849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.303343287872849e-05

Optimization complete. Final v2v error: 3.082732677459717 mm

Highest mean error: 3.3754560947418213 mm for frame 58

Lowest mean error: 2.7603137493133545 mm for frame 135

Saving results

Total time: 30.470974683761597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902446
Iteration 2/25 | Loss: 0.00146736
Iteration 3/25 | Loss: 0.00125344
Iteration 4/25 | Loss: 0.00123026
Iteration 5/25 | Loss: 0.00122477
Iteration 6/25 | Loss: 0.00123645
Iteration 7/25 | Loss: 0.00123638
Iteration 8/25 | Loss: 0.00122988
Iteration 9/25 | Loss: 0.00121878
Iteration 10/25 | Loss: 0.00120115
Iteration 11/25 | Loss: 0.00118925
Iteration 12/25 | Loss: 0.00118740
Iteration 13/25 | Loss: 0.00118693
Iteration 14/25 | Loss: 0.00118679
Iteration 15/25 | Loss: 0.00119152
Iteration 16/25 | Loss: 0.00118717
Iteration 17/25 | Loss: 0.00118495
Iteration 18/25 | Loss: 0.00118432
Iteration 19/25 | Loss: 0.00118413
Iteration 20/25 | Loss: 0.00118410
Iteration 21/25 | Loss: 0.00118410
Iteration 22/25 | Loss: 0.00118410
Iteration 23/25 | Loss: 0.00118410
Iteration 24/25 | Loss: 0.00118410
Iteration 25/25 | Loss: 0.00118410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05321264
Iteration 2/25 | Loss: 0.00191390
Iteration 3/25 | Loss: 0.00191387
Iteration 4/25 | Loss: 0.00191386
Iteration 5/25 | Loss: 0.00191386
Iteration 6/25 | Loss: 0.00191386
Iteration 7/25 | Loss: 0.00191386
Iteration 8/25 | Loss: 0.00191386
Iteration 9/25 | Loss: 0.00191386
Iteration 10/25 | Loss: 0.00191386
Iteration 11/25 | Loss: 0.00191386
Iteration 12/25 | Loss: 0.00191386
Iteration 13/25 | Loss: 0.00191386
Iteration 14/25 | Loss: 0.00191386
Iteration 15/25 | Loss: 0.00191386
Iteration 16/25 | Loss: 0.00191386
Iteration 17/25 | Loss: 0.00191386
Iteration 18/25 | Loss: 0.00191386
Iteration 19/25 | Loss: 0.00191386
Iteration 20/25 | Loss: 0.00191386
Iteration 21/25 | Loss: 0.00191386
Iteration 22/25 | Loss: 0.00191386
Iteration 23/25 | Loss: 0.00191386
Iteration 24/25 | Loss: 0.00191386
Iteration 25/25 | Loss: 0.00191386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191386
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002237
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001630
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001324
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001321
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001315
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001314
Iteration 52/1000 | Loss: 0.00001314
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001307
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001306
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.3009495887672529e-05, 1.3009495887672529e-05, 1.3009495887672529e-05, 1.3009495887672529e-05, 1.3009495887672529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3009495887672529e-05

Optimization complete. Final v2v error: 3.1366050243377686 mm

Highest mean error: 3.4658405780792236 mm for frame 151

Lowest mean error: 2.904430627822876 mm for frame 99

Saving results

Total time: 58.983335733413696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117131
Iteration 2/25 | Loss: 0.01117131
Iteration 3/25 | Loss: 0.01117131
Iteration 4/25 | Loss: 0.01117131
Iteration 5/25 | Loss: 0.01117130
Iteration 6/25 | Loss: 0.01117130
Iteration 7/25 | Loss: 0.01117130
Iteration 8/25 | Loss: 0.01117130
Iteration 9/25 | Loss: 0.01117130
Iteration 10/25 | Loss: 0.01117130
Iteration 11/25 | Loss: 0.01117130
Iteration 12/25 | Loss: 0.01117130
Iteration 13/25 | Loss: 0.01117130
Iteration 14/25 | Loss: 0.01117130
Iteration 15/25 | Loss: 0.01117130
Iteration 16/25 | Loss: 0.01117129
Iteration 17/25 | Loss: 0.01117129
Iteration 18/25 | Loss: 0.01117129
Iteration 19/25 | Loss: 0.01117129
Iteration 20/25 | Loss: 0.01117129
Iteration 21/25 | Loss: 0.01117129
Iteration 22/25 | Loss: 0.01117129
Iteration 23/25 | Loss: 0.01117129
Iteration 24/25 | Loss: 0.01117129
Iteration 25/25 | Loss: 0.01117129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47657156
Iteration 2/25 | Loss: 0.08064294
Iteration 3/25 | Loss: 0.08064293
Iteration 4/25 | Loss: 0.08064293
Iteration 5/25 | Loss: 0.08064292
Iteration 6/25 | Loss: 0.08064292
Iteration 7/25 | Loss: 0.08064292
Iteration 8/25 | Loss: 0.08064292
Iteration 9/25 | Loss: 0.08064292
Iteration 10/25 | Loss: 0.08064292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08064291626214981, 0.08064291626214981, 0.08064291626214981, 0.08064291626214981, 0.08064291626214981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08064291626214981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08064292
Iteration 2/1000 | Loss: 0.00048104
Iteration 3/1000 | Loss: 0.00011857
Iteration 4/1000 | Loss: 0.00005623
Iteration 5/1000 | Loss: 0.00003529
Iteration 6/1000 | Loss: 0.00002865
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001345
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001282
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001271
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001253
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001241
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.2319990673859138e-05, 1.2319990673859138e-05, 1.2319990673859138e-05, 1.2319990673859138e-05, 1.2319990673859138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2319990673859138e-05

Optimization complete. Final v2v error: 3.072911262512207 mm

Highest mean error: 3.483668565750122 mm for frame 25

Lowest mean error: 2.7127292156219482 mm for frame 171

Saving results

Total time: 49.21913480758667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1530/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1530/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381411
Iteration 2/25 | Loss: 0.00118958
Iteration 3/25 | Loss: 0.00112049
Iteration 4/25 | Loss: 0.00111114
Iteration 5/25 | Loss: 0.00110732
Iteration 6/25 | Loss: 0.00110583
Iteration 7/25 | Loss: 0.00110583
Iteration 8/25 | Loss: 0.00110583
Iteration 9/25 | Loss: 0.00110583
Iteration 10/25 | Loss: 0.00110583
Iteration 11/25 | Loss: 0.00110583
Iteration 12/25 | Loss: 0.00110583
Iteration 13/25 | Loss: 0.00110583
Iteration 14/25 | Loss: 0.00110583
Iteration 15/25 | Loss: 0.00110583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011058268137276173, 0.0011058268137276173, 0.0011058268137276173, 0.0011058268137276173, 0.0011058268137276173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011058268137276173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75129533
Iteration 2/25 | Loss: 0.00221275
Iteration 3/25 | Loss: 0.00221275
Iteration 4/25 | Loss: 0.00221275
Iteration 5/25 | Loss: 0.00221275
Iteration 6/25 | Loss: 0.00221274
Iteration 7/25 | Loss: 0.00221274
Iteration 8/25 | Loss: 0.00221274
Iteration 9/25 | Loss: 0.00221274
Iteration 10/25 | Loss: 0.00221274
Iteration 11/25 | Loss: 0.00221274
Iteration 12/25 | Loss: 0.00221274
Iteration 13/25 | Loss: 0.00221274
Iteration 14/25 | Loss: 0.00221274
Iteration 15/25 | Loss: 0.00221274
Iteration 16/25 | Loss: 0.00221274
Iteration 17/25 | Loss: 0.00221274
Iteration 18/25 | Loss: 0.00221274
Iteration 19/25 | Loss: 0.00221274
Iteration 20/25 | Loss: 0.00221274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022127442061901093, 0.0022127442061901093, 0.0022127442061901093, 0.0022127442061901093, 0.0022127442061901093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022127442061901093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221274
Iteration 2/1000 | Loss: 0.00002362
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001371
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001311
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.231814985658275e-05, 1.231814985658275e-05, 1.231814985658275e-05, 1.231814985658275e-05, 1.231814985658275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.231814985658275e-05

Optimization complete. Final v2v error: 2.9803833961486816 mm

Highest mean error: 3.4741671085357666 mm for frame 26

Lowest mean error: 2.684701919555664 mm for frame 188

Saving results

Total time: 29.82967233657837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098552
Iteration 2/25 | Loss: 0.00177349
Iteration 3/25 | Loss: 0.00099807
Iteration 4/25 | Loss: 0.00090605
Iteration 5/25 | Loss: 0.00086084
Iteration 6/25 | Loss: 0.00084663
Iteration 7/25 | Loss: 0.00084852
Iteration 8/25 | Loss: 0.00083027
Iteration 9/25 | Loss: 0.00080502
Iteration 10/25 | Loss: 0.00078495
Iteration 11/25 | Loss: 0.00077195
Iteration 12/25 | Loss: 0.00076344
Iteration 13/25 | Loss: 0.00076110
Iteration 14/25 | Loss: 0.00076841
Iteration 15/25 | Loss: 0.00076531
Iteration 16/25 | Loss: 0.00076101
Iteration 17/25 | Loss: 0.00076521
Iteration 18/25 | Loss: 0.00076338
Iteration 19/25 | Loss: 0.00075946
Iteration 20/25 | Loss: 0.00075977
Iteration 21/25 | Loss: 0.00075868
Iteration 22/25 | Loss: 0.00075717
Iteration 23/25 | Loss: 0.00075715
Iteration 24/25 | Loss: 0.00075715
Iteration 25/25 | Loss: 0.00075715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.17937279
Iteration 2/25 | Loss: 0.00046351
Iteration 3/25 | Loss: 0.00038567
Iteration 4/25 | Loss: 0.00038566
Iteration 5/25 | Loss: 0.00038566
Iteration 6/25 | Loss: 0.00038566
Iteration 7/25 | Loss: 0.00038566
Iteration 8/25 | Loss: 0.00038566
Iteration 9/25 | Loss: 0.00038566
Iteration 10/25 | Loss: 0.00038566
Iteration 11/25 | Loss: 0.00038566
Iteration 12/25 | Loss: 0.00038566
Iteration 13/25 | Loss: 0.00038566
Iteration 14/25 | Loss: 0.00038566
Iteration 15/25 | Loss: 0.00038566
Iteration 16/25 | Loss: 0.00038566
Iteration 17/25 | Loss: 0.00038566
Iteration 18/25 | Loss: 0.00038566
Iteration 19/25 | Loss: 0.00038566
Iteration 20/25 | Loss: 0.00038566
Iteration 21/25 | Loss: 0.00038566
Iteration 22/25 | Loss: 0.00038566
Iteration 23/25 | Loss: 0.00038566
Iteration 24/25 | Loss: 0.00038566
Iteration 25/25 | Loss: 0.00038566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038566
Iteration 2/1000 | Loss: 0.00015352
Iteration 3/1000 | Loss: 0.00008238
Iteration 4/1000 | Loss: 0.00005055
Iteration 5/1000 | Loss: 0.00021093
Iteration 6/1000 | Loss: 0.00007755
Iteration 7/1000 | Loss: 0.00005305
Iteration 8/1000 | Loss: 0.00004481
Iteration 9/1000 | Loss: 0.00003956
Iteration 10/1000 | Loss: 0.00021484
Iteration 11/1000 | Loss: 0.00019638
Iteration 12/1000 | Loss: 0.00015249
Iteration 13/1000 | Loss: 0.00019930
Iteration 14/1000 | Loss: 0.00011665
Iteration 15/1000 | Loss: 0.00034590
Iteration 16/1000 | Loss: 0.00008510
Iteration 17/1000 | Loss: 0.00016656
Iteration 18/1000 | Loss: 0.00028561
Iteration 19/1000 | Loss: 0.00019561
Iteration 20/1000 | Loss: 0.00019656
Iteration 21/1000 | Loss: 0.00032605
Iteration 22/1000 | Loss: 0.00026473
Iteration 23/1000 | Loss: 0.00030854
Iteration 24/1000 | Loss: 0.00028194
Iteration 25/1000 | Loss: 0.00018503
Iteration 26/1000 | Loss: 0.00021122
Iteration 27/1000 | Loss: 0.00014142
Iteration 28/1000 | Loss: 0.00017056
Iteration 29/1000 | Loss: 0.00009292
Iteration 30/1000 | Loss: 0.00018051
Iteration 31/1000 | Loss: 0.00009988
Iteration 32/1000 | Loss: 0.00016719
Iteration 33/1000 | Loss: 0.00013229
Iteration 34/1000 | Loss: 0.00023157
Iteration 35/1000 | Loss: 0.00014296
Iteration 36/1000 | Loss: 0.00024427
Iteration 37/1000 | Loss: 0.00008004
Iteration 38/1000 | Loss: 0.00014708
Iteration 39/1000 | Loss: 0.00018739
Iteration 40/1000 | Loss: 0.00021156
Iteration 41/1000 | Loss: 0.00017648
Iteration 42/1000 | Loss: 0.00021848
Iteration 43/1000 | Loss: 0.00014713
Iteration 44/1000 | Loss: 0.00023475
Iteration 45/1000 | Loss: 0.00011191
Iteration 46/1000 | Loss: 0.00027742
Iteration 47/1000 | Loss: 0.00031841
Iteration 48/1000 | Loss: 0.00031510
Iteration 49/1000 | Loss: 0.00007166
Iteration 50/1000 | Loss: 0.00020005
Iteration 51/1000 | Loss: 0.00048090
Iteration 52/1000 | Loss: 0.00041993
Iteration 53/1000 | Loss: 0.00004792
Iteration 54/1000 | Loss: 0.00063486
Iteration 55/1000 | Loss: 0.00075215
Iteration 56/1000 | Loss: 0.00008385
Iteration 57/1000 | Loss: 0.00018862
Iteration 58/1000 | Loss: 0.00005982
Iteration 59/1000 | Loss: 0.00004026
Iteration 60/1000 | Loss: 0.00003733
Iteration 61/1000 | Loss: 0.00003472
Iteration 62/1000 | Loss: 0.00003365
Iteration 63/1000 | Loss: 0.00008023
Iteration 64/1000 | Loss: 0.00003258
Iteration 65/1000 | Loss: 0.00003193
Iteration 66/1000 | Loss: 0.00007730
Iteration 67/1000 | Loss: 0.00003098
Iteration 68/1000 | Loss: 0.00006517
Iteration 69/1000 | Loss: 0.00003121
Iteration 70/1000 | Loss: 0.00003004
Iteration 71/1000 | Loss: 0.00002959
Iteration 72/1000 | Loss: 0.00002920
Iteration 73/1000 | Loss: 0.00002898
Iteration 74/1000 | Loss: 0.00002879
Iteration 75/1000 | Loss: 0.00002864
Iteration 76/1000 | Loss: 0.00002854
Iteration 77/1000 | Loss: 0.00002851
Iteration 78/1000 | Loss: 0.00002851
Iteration 79/1000 | Loss: 0.00002850
Iteration 80/1000 | Loss: 0.00002849
Iteration 81/1000 | Loss: 0.00002849
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002848
Iteration 86/1000 | Loss: 0.00002848
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002847
Iteration 89/1000 | Loss: 0.00002846
Iteration 90/1000 | Loss: 0.00002846
Iteration 91/1000 | Loss: 0.00002846
Iteration 92/1000 | Loss: 0.00002845
Iteration 93/1000 | Loss: 0.00002845
Iteration 94/1000 | Loss: 0.00002844
Iteration 95/1000 | Loss: 0.00002844
Iteration 96/1000 | Loss: 0.00002843
Iteration 97/1000 | Loss: 0.00002843
Iteration 98/1000 | Loss: 0.00002843
Iteration 99/1000 | Loss: 0.00002843
Iteration 100/1000 | Loss: 0.00002842
Iteration 101/1000 | Loss: 0.00002842
Iteration 102/1000 | Loss: 0.00002842
Iteration 103/1000 | Loss: 0.00011134
Iteration 104/1000 | Loss: 0.00011134
Iteration 105/1000 | Loss: 0.00011134
Iteration 106/1000 | Loss: 0.00035211
Iteration 107/1000 | Loss: 0.00005771
Iteration 108/1000 | Loss: 0.00076713
Iteration 109/1000 | Loss: 0.00005360
Iteration 110/1000 | Loss: 0.00005697
Iteration 111/1000 | Loss: 0.00003641
Iteration 112/1000 | Loss: 0.00002850
Iteration 113/1000 | Loss: 0.00002840
Iteration 114/1000 | Loss: 0.00002838
Iteration 115/1000 | Loss: 0.00002837
Iteration 116/1000 | Loss: 0.00002837
Iteration 117/1000 | Loss: 0.00002837
Iteration 118/1000 | Loss: 0.00002837
Iteration 119/1000 | Loss: 0.00002837
Iteration 120/1000 | Loss: 0.00002837
Iteration 121/1000 | Loss: 0.00002837
Iteration 122/1000 | Loss: 0.00002837
Iteration 123/1000 | Loss: 0.00002837
Iteration 124/1000 | Loss: 0.00002837
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002835
Iteration 130/1000 | Loss: 0.00002835
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Iteration 134/1000 | Loss: 0.00002834
Iteration 135/1000 | Loss: 0.00002834
Iteration 136/1000 | Loss: 0.00002834
Iteration 137/1000 | Loss: 0.00002834
Iteration 138/1000 | Loss: 0.00002834
Iteration 139/1000 | Loss: 0.00002834
Iteration 140/1000 | Loss: 0.00008790
Iteration 141/1000 | Loss: 0.00003191
Iteration 142/1000 | Loss: 0.00005736
Iteration 143/1000 | Loss: 0.00002841
Iteration 144/1000 | Loss: 0.00002836
Iteration 145/1000 | Loss: 0.00002835
Iteration 146/1000 | Loss: 0.00002833
Iteration 147/1000 | Loss: 0.00002832
Iteration 148/1000 | Loss: 0.00002832
Iteration 149/1000 | Loss: 0.00002832
Iteration 150/1000 | Loss: 0.00002831
Iteration 151/1000 | Loss: 0.00002831
Iteration 152/1000 | Loss: 0.00002831
Iteration 153/1000 | Loss: 0.00002831
Iteration 154/1000 | Loss: 0.00002831
Iteration 155/1000 | Loss: 0.00002831
Iteration 156/1000 | Loss: 0.00002831
Iteration 157/1000 | Loss: 0.00002831
Iteration 158/1000 | Loss: 0.00002831
Iteration 159/1000 | Loss: 0.00002831
Iteration 160/1000 | Loss: 0.00002831
Iteration 161/1000 | Loss: 0.00002831
Iteration 162/1000 | Loss: 0.00002831
Iteration 163/1000 | Loss: 0.00002831
Iteration 164/1000 | Loss: 0.00002831
Iteration 165/1000 | Loss: 0.00002830
Iteration 166/1000 | Loss: 0.00002830
Iteration 167/1000 | Loss: 0.00002830
Iteration 168/1000 | Loss: 0.00002830
Iteration 169/1000 | Loss: 0.00002830
Iteration 170/1000 | Loss: 0.00002830
Iteration 171/1000 | Loss: 0.00002830
Iteration 172/1000 | Loss: 0.00002830
Iteration 173/1000 | Loss: 0.00002829
Iteration 174/1000 | Loss: 0.00002829
Iteration 175/1000 | Loss: 0.00002829
Iteration 176/1000 | Loss: 0.00002829
Iteration 177/1000 | Loss: 0.00002829
Iteration 178/1000 | Loss: 0.00002829
Iteration 179/1000 | Loss: 0.00002829
Iteration 180/1000 | Loss: 0.00002829
Iteration 181/1000 | Loss: 0.00002829
Iteration 182/1000 | Loss: 0.00002829
Iteration 183/1000 | Loss: 0.00002829
Iteration 184/1000 | Loss: 0.00002829
Iteration 185/1000 | Loss: 0.00002829
Iteration 186/1000 | Loss: 0.00002829
Iteration 187/1000 | Loss: 0.00002829
Iteration 188/1000 | Loss: 0.00002829
Iteration 189/1000 | Loss: 0.00002829
Iteration 190/1000 | Loss: 0.00002829
Iteration 191/1000 | Loss: 0.00002829
Iteration 192/1000 | Loss: 0.00002829
Iteration 193/1000 | Loss: 0.00002829
Iteration 194/1000 | Loss: 0.00002829
Iteration 195/1000 | Loss: 0.00002829
Iteration 196/1000 | Loss: 0.00002829
Iteration 197/1000 | Loss: 0.00002829
Iteration 198/1000 | Loss: 0.00002829
Iteration 199/1000 | Loss: 0.00002829
Iteration 200/1000 | Loss: 0.00002829
Iteration 201/1000 | Loss: 0.00002829
Iteration 202/1000 | Loss: 0.00002829
Iteration 203/1000 | Loss: 0.00002829
Iteration 204/1000 | Loss: 0.00002829
Iteration 205/1000 | Loss: 0.00002829
Iteration 206/1000 | Loss: 0.00002829
Iteration 207/1000 | Loss: 0.00002829
Iteration 208/1000 | Loss: 0.00002829
Iteration 209/1000 | Loss: 0.00002829
Iteration 210/1000 | Loss: 0.00002829
Iteration 211/1000 | Loss: 0.00002829
Iteration 212/1000 | Loss: 0.00002829
Iteration 213/1000 | Loss: 0.00002829
Iteration 214/1000 | Loss: 0.00002829
Iteration 215/1000 | Loss: 0.00002829
Iteration 216/1000 | Loss: 0.00002829
Iteration 217/1000 | Loss: 0.00002829
Iteration 218/1000 | Loss: 0.00002829
Iteration 219/1000 | Loss: 0.00002829
Iteration 220/1000 | Loss: 0.00002829
Iteration 221/1000 | Loss: 0.00002829
Iteration 222/1000 | Loss: 0.00002829
Iteration 223/1000 | Loss: 0.00002829
Iteration 224/1000 | Loss: 0.00002829
Iteration 225/1000 | Loss: 0.00002829
Iteration 226/1000 | Loss: 0.00002829
Iteration 227/1000 | Loss: 0.00002829
Iteration 228/1000 | Loss: 0.00002829
Iteration 229/1000 | Loss: 0.00002829
Iteration 230/1000 | Loss: 0.00002829
Iteration 231/1000 | Loss: 0.00002829
Iteration 232/1000 | Loss: 0.00002829
Iteration 233/1000 | Loss: 0.00002829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.8285110602155328e-05, 2.8285110602155328e-05, 2.8285110602155328e-05, 2.8285110602155328e-05, 2.8285110602155328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8285110602155328e-05

Optimization complete. Final v2v error: 4.457993507385254 mm

Highest mean error: 5.329070568084717 mm for frame 135

Lowest mean error: 3.3755688667297363 mm for frame 7

Saving results

Total time: 168.8195285797119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408583
Iteration 2/25 | Loss: 0.00076662
Iteration 3/25 | Loss: 0.00066963
Iteration 4/25 | Loss: 0.00064290
Iteration 5/25 | Loss: 0.00063917
Iteration 6/25 | Loss: 0.00063861
Iteration 7/25 | Loss: 0.00063852
Iteration 8/25 | Loss: 0.00063852
Iteration 9/25 | Loss: 0.00063852
Iteration 10/25 | Loss: 0.00063852
Iteration 11/25 | Loss: 0.00063852
Iteration 12/25 | Loss: 0.00063852
Iteration 13/25 | Loss: 0.00063852
Iteration 14/25 | Loss: 0.00063852
Iteration 15/25 | Loss: 0.00063852
Iteration 16/25 | Loss: 0.00063852
Iteration 17/25 | Loss: 0.00063852
Iteration 18/25 | Loss: 0.00063852
Iteration 19/25 | Loss: 0.00063852
Iteration 20/25 | Loss: 0.00063852
Iteration 21/25 | Loss: 0.00063852
Iteration 22/25 | Loss: 0.00063852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006385207525454462, 0.0006385207525454462, 0.0006385207525454462, 0.0006385207525454462, 0.0006385207525454462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006385207525454462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33795857
Iteration 2/25 | Loss: 0.00037990
Iteration 3/25 | Loss: 0.00037990
Iteration 4/25 | Loss: 0.00037990
Iteration 5/25 | Loss: 0.00037990
Iteration 6/25 | Loss: 0.00037990
Iteration 7/25 | Loss: 0.00037990
Iteration 8/25 | Loss: 0.00037990
Iteration 9/25 | Loss: 0.00037990
Iteration 10/25 | Loss: 0.00037990
Iteration 11/25 | Loss: 0.00037990
Iteration 12/25 | Loss: 0.00037990
Iteration 13/25 | Loss: 0.00037990
Iteration 14/25 | Loss: 0.00037990
Iteration 15/25 | Loss: 0.00037990
Iteration 16/25 | Loss: 0.00037990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003798971010837704, 0.0003798971010837704, 0.0003798971010837704, 0.0003798971010837704, 0.0003798971010837704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003798971010837704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037990
Iteration 2/1000 | Loss: 0.00004629
Iteration 3/1000 | Loss: 0.00002871
Iteration 4/1000 | Loss: 0.00002587
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002226
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002157
Iteration 10/1000 | Loss: 0.00002136
Iteration 11/1000 | Loss: 0.00002131
Iteration 12/1000 | Loss: 0.00002120
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002118
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00002099
Iteration 21/1000 | Loss: 0.00002099
Iteration 22/1000 | Loss: 0.00002098
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002093
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002089
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002076
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002073
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002072
Iteration 45/1000 | Loss: 0.00002071
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002069
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002068
Iteration 50/1000 | Loss: 0.00002068
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002067
Iteration 53/1000 | Loss: 0.00002067
Iteration 54/1000 | Loss: 0.00002067
Iteration 55/1000 | Loss: 0.00002066
Iteration 56/1000 | Loss: 0.00002065
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002065
Iteration 60/1000 | Loss: 0.00002065
Iteration 61/1000 | Loss: 0.00002065
Iteration 62/1000 | Loss: 0.00002065
Iteration 63/1000 | Loss: 0.00002065
Iteration 64/1000 | Loss: 0.00002065
Iteration 65/1000 | Loss: 0.00002065
Iteration 66/1000 | Loss: 0.00002064
Iteration 67/1000 | Loss: 0.00002064
Iteration 68/1000 | Loss: 0.00002064
Iteration 69/1000 | Loss: 0.00002064
Iteration 70/1000 | Loss: 0.00002064
Iteration 71/1000 | Loss: 0.00002064
Iteration 72/1000 | Loss: 0.00002064
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00002063
Iteration 75/1000 | Loss: 0.00002063
Iteration 76/1000 | Loss: 0.00002063
Iteration 77/1000 | Loss: 0.00002063
Iteration 78/1000 | Loss: 0.00002062
Iteration 79/1000 | Loss: 0.00002062
Iteration 80/1000 | Loss: 0.00002062
Iteration 81/1000 | Loss: 0.00002062
Iteration 82/1000 | Loss: 0.00002062
Iteration 83/1000 | Loss: 0.00002061
Iteration 84/1000 | Loss: 0.00002061
Iteration 85/1000 | Loss: 0.00002061
Iteration 86/1000 | Loss: 0.00002061
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002060
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002059
Iteration 95/1000 | Loss: 0.00002059
Iteration 96/1000 | Loss: 0.00002059
Iteration 97/1000 | Loss: 0.00002059
Iteration 98/1000 | Loss: 0.00002059
Iteration 99/1000 | Loss: 0.00002059
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002059
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002058
Iteration 115/1000 | Loss: 0.00002058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.0582521756296046e-05, 2.0582521756296046e-05, 2.0582521756296046e-05, 2.0582521756296046e-05, 2.0582521756296046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0582521756296046e-05

Optimization complete. Final v2v error: 3.8807122707366943 mm

Highest mean error: 4.246407508850098 mm for frame 80

Lowest mean error: 3.625485420227051 mm for frame 104

Saving results

Total time: 35.88103270530701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051213
Iteration 2/25 | Loss: 0.00222508
Iteration 3/25 | Loss: 0.00171764
Iteration 4/25 | Loss: 0.00125126
Iteration 5/25 | Loss: 0.00149176
Iteration 6/25 | Loss: 0.00117309
Iteration 7/25 | Loss: 0.00109483
Iteration 8/25 | Loss: 0.00107704
Iteration 9/25 | Loss: 0.00096905
Iteration 10/25 | Loss: 0.00090957
Iteration 11/25 | Loss: 0.00089552
Iteration 12/25 | Loss: 0.00088389
Iteration 13/25 | Loss: 0.00081379
Iteration 14/25 | Loss: 0.00080477
Iteration 15/25 | Loss: 0.00079781
Iteration 16/25 | Loss: 0.00080317
Iteration 17/25 | Loss: 0.00079756
Iteration 18/25 | Loss: 0.00080180
Iteration 19/25 | Loss: 0.00079821
Iteration 20/25 | Loss: 0.00079502
Iteration 21/25 | Loss: 0.00079410
Iteration 22/25 | Loss: 0.00079390
Iteration 23/25 | Loss: 0.00079375
Iteration 24/25 | Loss: 0.00079373
Iteration 25/25 | Loss: 0.00079373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27592254
Iteration 2/25 | Loss: 0.00058644
Iteration 3/25 | Loss: 0.00051011
Iteration 4/25 | Loss: 0.00051011
Iteration 5/25 | Loss: 0.00051011
Iteration 6/25 | Loss: 0.00051011
Iteration 7/25 | Loss: 0.00051011
Iteration 8/25 | Loss: 0.00051011
Iteration 9/25 | Loss: 0.00051011
Iteration 10/25 | Loss: 0.00051011
Iteration 11/25 | Loss: 0.00051011
Iteration 12/25 | Loss: 0.00051011
Iteration 13/25 | Loss: 0.00051011
Iteration 14/25 | Loss: 0.00051011
Iteration 15/25 | Loss: 0.00051011
Iteration 16/25 | Loss: 0.00051011
Iteration 17/25 | Loss: 0.00051011
Iteration 18/25 | Loss: 0.00051011
Iteration 19/25 | Loss: 0.00051011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005101101705804467, 0.0005101101705804467, 0.0005101101705804467, 0.0005101101705804467, 0.0005101101705804467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005101101705804467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051011
Iteration 2/1000 | Loss: 0.00013642
Iteration 3/1000 | Loss: 0.00012734
Iteration 4/1000 | Loss: 0.00007505
Iteration 5/1000 | Loss: 0.00004800
Iteration 6/1000 | Loss: 0.00011633
Iteration 7/1000 | Loss: 0.00004027
Iteration 8/1000 | Loss: 0.00011333
Iteration 9/1000 | Loss: 0.00004482
Iteration 10/1000 | Loss: 0.00003878
Iteration 11/1000 | Loss: 0.00003695
Iteration 12/1000 | Loss: 0.00003604
Iteration 13/1000 | Loss: 0.00029019
Iteration 14/1000 | Loss: 0.00021257
Iteration 15/1000 | Loss: 0.00022096
Iteration 16/1000 | Loss: 0.00013239
Iteration 17/1000 | Loss: 0.00005484
Iteration 18/1000 | Loss: 0.00004088
Iteration 19/1000 | Loss: 0.00010835
Iteration 20/1000 | Loss: 0.00003513
Iteration 21/1000 | Loss: 0.00033906
Iteration 22/1000 | Loss: 0.00004766
Iteration 23/1000 | Loss: 0.00003965
Iteration 24/1000 | Loss: 0.00003442
Iteration 25/1000 | Loss: 0.00003261
Iteration 26/1000 | Loss: 0.00003180
Iteration 27/1000 | Loss: 0.00003126
Iteration 28/1000 | Loss: 0.00003097
Iteration 29/1000 | Loss: 0.00003091
Iteration 30/1000 | Loss: 0.00003072
Iteration 31/1000 | Loss: 0.00003040
Iteration 32/1000 | Loss: 0.00010053
Iteration 33/1000 | Loss: 0.00004683
Iteration 34/1000 | Loss: 0.00015876
Iteration 35/1000 | Loss: 0.00006944
Iteration 36/1000 | Loss: 0.00012773
Iteration 37/1000 | Loss: 0.00003111
Iteration 38/1000 | Loss: 0.00003005
Iteration 39/1000 | Loss: 0.00002801
Iteration 40/1000 | Loss: 0.00002641
Iteration 41/1000 | Loss: 0.00002591
Iteration 42/1000 | Loss: 0.00002569
Iteration 43/1000 | Loss: 0.00002544
Iteration 44/1000 | Loss: 0.00002529
Iteration 45/1000 | Loss: 0.00002511
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002494
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002475
Iteration 53/1000 | Loss: 0.00002475
Iteration 54/1000 | Loss: 0.00002475
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002474
Iteration 58/1000 | Loss: 0.00002474
Iteration 59/1000 | Loss: 0.00002474
Iteration 60/1000 | Loss: 0.00002474
Iteration 61/1000 | Loss: 0.00002474
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002472
Iteration 68/1000 | Loss: 0.00002472
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002472
Iteration 74/1000 | Loss: 0.00002472
Iteration 75/1000 | Loss: 0.00002472
Iteration 76/1000 | Loss: 0.00002472
Iteration 77/1000 | Loss: 0.00002472
Iteration 78/1000 | Loss: 0.00002471
Iteration 79/1000 | Loss: 0.00002471
Iteration 80/1000 | Loss: 0.00002471
Iteration 81/1000 | Loss: 0.00002471
Iteration 82/1000 | Loss: 0.00002471
Iteration 83/1000 | Loss: 0.00002470
Iteration 84/1000 | Loss: 0.00002470
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002468
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002468
Iteration 93/1000 | Loss: 0.00002468
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002467
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002467
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002467
Iteration 103/1000 | Loss: 0.00002467
Iteration 104/1000 | Loss: 0.00002467
Iteration 105/1000 | Loss: 0.00002467
Iteration 106/1000 | Loss: 0.00002467
Iteration 107/1000 | Loss: 0.00002467
Iteration 108/1000 | Loss: 0.00002467
Iteration 109/1000 | Loss: 0.00002467
Iteration 110/1000 | Loss: 0.00002467
Iteration 111/1000 | Loss: 0.00002467
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002466
Iteration 115/1000 | Loss: 0.00002466
Iteration 116/1000 | Loss: 0.00002466
Iteration 117/1000 | Loss: 0.00002466
Iteration 118/1000 | Loss: 0.00002466
Iteration 119/1000 | Loss: 0.00002466
Iteration 120/1000 | Loss: 0.00002466
Iteration 121/1000 | Loss: 0.00002466
Iteration 122/1000 | Loss: 0.00002466
Iteration 123/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.466210935381241e-05, 2.466210935381241e-05, 2.466210935381241e-05, 2.466210935381241e-05, 2.466210935381241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.466210935381241e-05

Optimization complete. Final v2v error: 4.180245876312256 mm

Highest mean error: 5.248793125152588 mm for frame 3

Lowest mean error: 3.985038995742798 mm for frame 41

Saving results

Total time: 107.86479187011719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430648
Iteration 2/25 | Loss: 0.00082145
Iteration 3/25 | Loss: 0.00070852
Iteration 4/25 | Loss: 0.00067750
Iteration 5/25 | Loss: 0.00067186
Iteration 6/25 | Loss: 0.00067097
Iteration 7/25 | Loss: 0.00067097
Iteration 8/25 | Loss: 0.00067097
Iteration 9/25 | Loss: 0.00067097
Iteration 10/25 | Loss: 0.00067097
Iteration 11/25 | Loss: 0.00067097
Iteration 12/25 | Loss: 0.00067097
Iteration 13/25 | Loss: 0.00067097
Iteration 14/25 | Loss: 0.00067097
Iteration 15/25 | Loss: 0.00067097
Iteration 16/25 | Loss: 0.00067097
Iteration 17/25 | Loss: 0.00067097
Iteration 18/25 | Loss: 0.00067097
Iteration 19/25 | Loss: 0.00067097
Iteration 20/25 | Loss: 0.00067097
Iteration 21/25 | Loss: 0.00067097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006709732115268707, 0.0006709732115268707, 0.0006709732115268707, 0.0006709732115268707, 0.0006709732115268707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006709732115268707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30479121
Iteration 2/25 | Loss: 0.00046269
Iteration 3/25 | Loss: 0.00046268
Iteration 4/25 | Loss: 0.00046268
Iteration 5/25 | Loss: 0.00046268
Iteration 6/25 | Loss: 0.00046268
Iteration 7/25 | Loss: 0.00046268
Iteration 8/25 | Loss: 0.00046268
Iteration 9/25 | Loss: 0.00046268
Iteration 10/25 | Loss: 0.00046268
Iteration 11/25 | Loss: 0.00046268
Iteration 12/25 | Loss: 0.00046268
Iteration 13/25 | Loss: 0.00046268
Iteration 14/25 | Loss: 0.00046268
Iteration 15/25 | Loss: 0.00046268
Iteration 16/25 | Loss: 0.00046268
Iteration 17/25 | Loss: 0.00046268
Iteration 18/25 | Loss: 0.00046268
Iteration 19/25 | Loss: 0.00046268
Iteration 20/25 | Loss: 0.00046268
Iteration 21/25 | Loss: 0.00046268
Iteration 22/25 | Loss: 0.00046268
Iteration 23/25 | Loss: 0.00046268
Iteration 24/25 | Loss: 0.00046268
Iteration 25/25 | Loss: 0.00046268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046268
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00003885
Iteration 4/1000 | Loss: 0.00003511
Iteration 5/1000 | Loss: 0.00003344
Iteration 6/1000 | Loss: 0.00003191
Iteration 7/1000 | Loss: 0.00003089
Iteration 8/1000 | Loss: 0.00003006
Iteration 9/1000 | Loss: 0.00002958
Iteration 10/1000 | Loss: 0.00002930
Iteration 11/1000 | Loss: 0.00002907
Iteration 12/1000 | Loss: 0.00002890
Iteration 13/1000 | Loss: 0.00002890
Iteration 14/1000 | Loss: 0.00002889
Iteration 15/1000 | Loss: 0.00002888
Iteration 16/1000 | Loss: 0.00002885
Iteration 17/1000 | Loss: 0.00002884
Iteration 18/1000 | Loss: 0.00002883
Iteration 19/1000 | Loss: 0.00002880
Iteration 20/1000 | Loss: 0.00002873
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00002868
Iteration 25/1000 | Loss: 0.00002868
Iteration 26/1000 | Loss: 0.00002868
Iteration 27/1000 | Loss: 0.00002868
Iteration 28/1000 | Loss: 0.00002867
Iteration 29/1000 | Loss: 0.00002867
Iteration 30/1000 | Loss: 0.00002867
Iteration 31/1000 | Loss: 0.00002866
Iteration 32/1000 | Loss: 0.00002866
Iteration 33/1000 | Loss: 0.00002865
Iteration 34/1000 | Loss: 0.00002865
Iteration 35/1000 | Loss: 0.00002865
Iteration 36/1000 | Loss: 0.00002865
Iteration 37/1000 | Loss: 0.00002864
Iteration 38/1000 | Loss: 0.00002864
Iteration 39/1000 | Loss: 0.00002864
Iteration 40/1000 | Loss: 0.00002864
Iteration 41/1000 | Loss: 0.00002864
Iteration 42/1000 | Loss: 0.00002864
Iteration 43/1000 | Loss: 0.00002863
Iteration 44/1000 | Loss: 0.00002863
Iteration 45/1000 | Loss: 0.00002863
Iteration 46/1000 | Loss: 0.00002863
Iteration 47/1000 | Loss: 0.00002863
Iteration 48/1000 | Loss: 0.00002863
Iteration 49/1000 | Loss: 0.00002863
Iteration 50/1000 | Loss: 0.00002863
Iteration 51/1000 | Loss: 0.00002863
Iteration 52/1000 | Loss: 0.00002862
Iteration 53/1000 | Loss: 0.00002862
Iteration 54/1000 | Loss: 0.00002861
Iteration 55/1000 | Loss: 0.00002861
Iteration 56/1000 | Loss: 0.00002860
Iteration 57/1000 | Loss: 0.00002860
Iteration 58/1000 | Loss: 0.00002860
Iteration 59/1000 | Loss: 0.00002859
Iteration 60/1000 | Loss: 0.00002859
Iteration 61/1000 | Loss: 0.00002859
Iteration 62/1000 | Loss: 0.00002859
Iteration 63/1000 | Loss: 0.00002858
Iteration 64/1000 | Loss: 0.00002858
Iteration 65/1000 | Loss: 0.00002858
Iteration 66/1000 | Loss: 0.00002857
Iteration 67/1000 | Loss: 0.00002857
Iteration 68/1000 | Loss: 0.00002857
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002856
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002855
Iteration 73/1000 | Loss: 0.00002855
Iteration 74/1000 | Loss: 0.00002855
Iteration 75/1000 | Loss: 0.00002854
Iteration 76/1000 | Loss: 0.00002854
Iteration 77/1000 | Loss: 0.00002854
Iteration 78/1000 | Loss: 0.00002853
Iteration 79/1000 | Loss: 0.00002853
Iteration 80/1000 | Loss: 0.00002852
Iteration 81/1000 | Loss: 0.00002852
Iteration 82/1000 | Loss: 0.00002852
Iteration 83/1000 | Loss: 0.00002851
Iteration 84/1000 | Loss: 0.00002851
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002850
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002850
Iteration 91/1000 | Loss: 0.00002850
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002849
Iteration 94/1000 | Loss: 0.00002849
Iteration 95/1000 | Loss: 0.00002848
Iteration 96/1000 | Loss: 0.00002848
Iteration 97/1000 | Loss: 0.00002848
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002848
Iteration 100/1000 | Loss: 0.00002848
Iteration 101/1000 | Loss: 0.00002848
Iteration 102/1000 | Loss: 0.00002848
Iteration 103/1000 | Loss: 0.00002847
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00002847
Iteration 106/1000 | Loss: 0.00002847
Iteration 107/1000 | Loss: 0.00002847
Iteration 108/1000 | Loss: 0.00002847
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002847
Iteration 112/1000 | Loss: 0.00002847
Iteration 113/1000 | Loss: 0.00002847
Iteration 114/1000 | Loss: 0.00002847
Iteration 115/1000 | Loss: 0.00002847
Iteration 116/1000 | Loss: 0.00002847
Iteration 117/1000 | Loss: 0.00002846
Iteration 118/1000 | Loss: 0.00002846
Iteration 119/1000 | Loss: 0.00002846
Iteration 120/1000 | Loss: 0.00002846
Iteration 121/1000 | Loss: 0.00002846
Iteration 122/1000 | Loss: 0.00002845
Iteration 123/1000 | Loss: 0.00002845
Iteration 124/1000 | Loss: 0.00002845
Iteration 125/1000 | Loss: 0.00002845
Iteration 126/1000 | Loss: 0.00002845
Iteration 127/1000 | Loss: 0.00002845
Iteration 128/1000 | Loss: 0.00002845
Iteration 129/1000 | Loss: 0.00002845
Iteration 130/1000 | Loss: 0.00002845
Iteration 131/1000 | Loss: 0.00002845
Iteration 132/1000 | Loss: 0.00002844
Iteration 133/1000 | Loss: 0.00002844
Iteration 134/1000 | Loss: 0.00002844
Iteration 135/1000 | Loss: 0.00002844
Iteration 136/1000 | Loss: 0.00002844
Iteration 137/1000 | Loss: 0.00002844
Iteration 138/1000 | Loss: 0.00002844
Iteration 139/1000 | Loss: 0.00002843
Iteration 140/1000 | Loss: 0.00002843
Iteration 141/1000 | Loss: 0.00002843
Iteration 142/1000 | Loss: 0.00002843
Iteration 143/1000 | Loss: 0.00002843
Iteration 144/1000 | Loss: 0.00002843
Iteration 145/1000 | Loss: 0.00002842
Iteration 146/1000 | Loss: 0.00002842
Iteration 147/1000 | Loss: 0.00002842
Iteration 148/1000 | Loss: 0.00002842
Iteration 149/1000 | Loss: 0.00002842
Iteration 150/1000 | Loss: 0.00002842
Iteration 151/1000 | Loss: 0.00002842
Iteration 152/1000 | Loss: 0.00002842
Iteration 153/1000 | Loss: 0.00002842
Iteration 154/1000 | Loss: 0.00002842
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002841
Iteration 157/1000 | Loss: 0.00002841
Iteration 158/1000 | Loss: 0.00002841
Iteration 159/1000 | Loss: 0.00002841
Iteration 160/1000 | Loss: 0.00002841
Iteration 161/1000 | Loss: 0.00002840
Iteration 162/1000 | Loss: 0.00002840
Iteration 163/1000 | Loss: 0.00002840
Iteration 164/1000 | Loss: 0.00002840
Iteration 165/1000 | Loss: 0.00002840
Iteration 166/1000 | Loss: 0.00002840
Iteration 167/1000 | Loss: 0.00002840
Iteration 168/1000 | Loss: 0.00002839
Iteration 169/1000 | Loss: 0.00002839
Iteration 170/1000 | Loss: 0.00002839
Iteration 171/1000 | Loss: 0.00002839
Iteration 172/1000 | Loss: 0.00002839
Iteration 173/1000 | Loss: 0.00002839
Iteration 174/1000 | Loss: 0.00002839
Iteration 175/1000 | Loss: 0.00002839
Iteration 176/1000 | Loss: 0.00002839
Iteration 177/1000 | Loss: 0.00002839
Iteration 178/1000 | Loss: 0.00002839
Iteration 179/1000 | Loss: 0.00002839
Iteration 180/1000 | Loss: 0.00002839
Iteration 181/1000 | Loss: 0.00002839
Iteration 182/1000 | Loss: 0.00002839
Iteration 183/1000 | Loss: 0.00002839
Iteration 184/1000 | Loss: 0.00002839
Iteration 185/1000 | Loss: 0.00002839
Iteration 186/1000 | Loss: 0.00002839
Iteration 187/1000 | Loss: 0.00002839
Iteration 188/1000 | Loss: 0.00002839
Iteration 189/1000 | Loss: 0.00002839
Iteration 190/1000 | Loss: 0.00002839
Iteration 191/1000 | Loss: 0.00002839
Iteration 192/1000 | Loss: 0.00002839
Iteration 193/1000 | Loss: 0.00002839
Iteration 194/1000 | Loss: 0.00002839
Iteration 195/1000 | Loss: 0.00002839
Iteration 196/1000 | Loss: 0.00002839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.839347507688217e-05, 2.839347507688217e-05, 2.839347507688217e-05, 2.839347507688217e-05, 2.839347507688217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.839347507688217e-05

Optimization complete. Final v2v error: 4.503376483917236 mm

Highest mean error: 4.940777778625488 mm for frame 210

Lowest mean error: 3.7751073837280273 mm for frame 25

Saving results

Total time: 45.13217902183533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008438
Iteration 2/25 | Loss: 0.00277043
Iteration 3/25 | Loss: 0.00229880
Iteration 4/25 | Loss: 0.00188469
Iteration 5/25 | Loss: 0.00184679
Iteration 6/25 | Loss: 0.00200745
Iteration 7/25 | Loss: 0.00177063
Iteration 8/25 | Loss: 0.00166723
Iteration 9/25 | Loss: 0.00163595
Iteration 10/25 | Loss: 0.00159726
Iteration 11/25 | Loss: 0.00159915
Iteration 12/25 | Loss: 0.00158556
Iteration 13/25 | Loss: 0.00158577
Iteration 14/25 | Loss: 0.00159292
Iteration 15/25 | Loss: 0.00158095
Iteration 16/25 | Loss: 0.00157974
Iteration 17/25 | Loss: 0.00157964
Iteration 18/25 | Loss: 0.00157964
Iteration 19/25 | Loss: 0.00157963
Iteration 20/25 | Loss: 0.00157963
Iteration 21/25 | Loss: 0.00157963
Iteration 22/25 | Loss: 0.00157963
Iteration 23/25 | Loss: 0.00157963
Iteration 24/25 | Loss: 0.00157963
Iteration 25/25 | Loss: 0.00157962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23437476
Iteration 2/25 | Loss: 0.01017213
Iteration 3/25 | Loss: 0.00689517
Iteration 4/25 | Loss: 0.00674364
Iteration 5/25 | Loss: 0.00674322
Iteration 6/25 | Loss: 0.00674270
Iteration 7/25 | Loss: 0.00670316
Iteration 8/25 | Loss: 0.00670316
Iteration 9/25 | Loss: 0.00670316
Iteration 10/25 | Loss: 0.00670316
Iteration 11/25 | Loss: 0.00670316
Iteration 12/25 | Loss: 0.00670316
Iteration 13/25 | Loss: 0.00670316
Iteration 14/25 | Loss: 0.00670316
Iteration 15/25 | Loss: 0.00670316
Iteration 16/25 | Loss: 0.00670316
Iteration 17/25 | Loss: 0.00670316
Iteration 18/25 | Loss: 0.00670316
Iteration 19/25 | Loss: 0.00670316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006703156512230635, 0.006703156512230635, 0.006703156512230635, 0.006703156512230635, 0.006703156512230635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006703156512230635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00670316
Iteration 2/1000 | Loss: 0.00928426
Iteration 3/1000 | Loss: 0.00247396
Iteration 4/1000 | Loss: 0.00093361
Iteration 5/1000 | Loss: 0.01000153
Iteration 6/1000 | Loss: 0.00070274
Iteration 7/1000 | Loss: 0.00082442
Iteration 8/1000 | Loss: 0.00048826
Iteration 9/1000 | Loss: 0.00044891
Iteration 10/1000 | Loss: 0.00098012
Iteration 11/1000 | Loss: 0.01422185
Iteration 12/1000 | Loss: 0.01404158
Iteration 13/1000 | Loss: 0.00564913
Iteration 14/1000 | Loss: 0.00198625
Iteration 15/1000 | Loss: 0.00118366
Iteration 16/1000 | Loss: 0.00246704
Iteration 17/1000 | Loss: 0.00164239
Iteration 18/1000 | Loss: 0.00170887
Iteration 19/1000 | Loss: 0.00385306
Iteration 20/1000 | Loss: 0.00092650
Iteration 21/1000 | Loss: 0.00087049
Iteration 22/1000 | Loss: 0.00038582
Iteration 23/1000 | Loss: 0.00050011
Iteration 24/1000 | Loss: 0.00039461
Iteration 25/1000 | Loss: 0.00015364
Iteration 26/1000 | Loss: 0.00032262
Iteration 27/1000 | Loss: 0.00018606
Iteration 28/1000 | Loss: 0.00007752
Iteration 29/1000 | Loss: 0.00024475
Iteration 30/1000 | Loss: 0.00012891
Iteration 31/1000 | Loss: 0.00055977
Iteration 32/1000 | Loss: 0.00006665
Iteration 33/1000 | Loss: 0.00009386
Iteration 34/1000 | Loss: 0.00004596
Iteration 35/1000 | Loss: 0.00019370
Iteration 36/1000 | Loss: 0.00069322
Iteration 37/1000 | Loss: 0.00066870
Iteration 38/1000 | Loss: 0.00038754
Iteration 39/1000 | Loss: 0.00080125
Iteration 40/1000 | Loss: 0.00050508
Iteration 41/1000 | Loss: 0.00007705
Iteration 42/1000 | Loss: 0.00004747
Iteration 43/1000 | Loss: 0.00004110
Iteration 44/1000 | Loss: 0.00039946
Iteration 45/1000 | Loss: 0.00044053
Iteration 46/1000 | Loss: 0.00007410
Iteration 47/1000 | Loss: 0.00008453
Iteration 48/1000 | Loss: 0.00003471
Iteration 49/1000 | Loss: 0.00006243
Iteration 50/1000 | Loss: 0.00006280
Iteration 51/1000 | Loss: 0.00005160
Iteration 52/1000 | Loss: 0.00006207
Iteration 53/1000 | Loss: 0.00005241
Iteration 54/1000 | Loss: 0.00021039
Iteration 55/1000 | Loss: 0.00006596
Iteration 56/1000 | Loss: 0.00009504
Iteration 57/1000 | Loss: 0.00005178
Iteration 58/1000 | Loss: 0.00016912
Iteration 59/1000 | Loss: 0.00004817
Iteration 60/1000 | Loss: 0.00015697
Iteration 61/1000 | Loss: 0.00004207
Iteration 62/1000 | Loss: 0.00003577
Iteration 63/1000 | Loss: 0.00003154
Iteration 64/1000 | Loss: 0.00008481
Iteration 65/1000 | Loss: 0.00003531
Iteration 66/1000 | Loss: 0.00003022
Iteration 67/1000 | Loss: 0.00002984
Iteration 68/1000 | Loss: 0.00002950
Iteration 69/1000 | Loss: 0.00016126
Iteration 70/1000 | Loss: 0.00170053
Iteration 71/1000 | Loss: 0.00524497
Iteration 72/1000 | Loss: 0.01078111
Iteration 73/1000 | Loss: 0.00443075
Iteration 74/1000 | Loss: 0.00749626
Iteration 75/1000 | Loss: 0.00319123
Iteration 76/1000 | Loss: 0.00377540
Iteration 77/1000 | Loss: 0.00072848
Iteration 78/1000 | Loss: 0.00025689
Iteration 79/1000 | Loss: 0.00088011
Iteration 80/1000 | Loss: 0.00112523
Iteration 81/1000 | Loss: 0.00032135
Iteration 82/1000 | Loss: 0.00018896
Iteration 83/1000 | Loss: 0.00098170
Iteration 84/1000 | Loss: 0.00005180
Iteration 85/1000 | Loss: 0.00022525
Iteration 86/1000 | Loss: 0.00006754
Iteration 87/1000 | Loss: 0.00003987
Iteration 88/1000 | Loss: 0.00003686
Iteration 89/1000 | Loss: 0.00012129
Iteration 90/1000 | Loss: 0.00003441
Iteration 91/1000 | Loss: 0.00003369
Iteration 92/1000 | Loss: 0.00013360
Iteration 93/1000 | Loss: 0.00003403
Iteration 94/1000 | Loss: 0.00020990
Iteration 95/1000 | Loss: 0.00007191
Iteration 96/1000 | Loss: 0.00003000
Iteration 97/1000 | Loss: 0.00002936
Iteration 98/1000 | Loss: 0.00002890
Iteration 99/1000 | Loss: 0.00023625
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002855
Iteration 102/1000 | Loss: 0.00002846
Iteration 103/1000 | Loss: 0.00002845
Iteration 104/1000 | Loss: 0.00002845
Iteration 105/1000 | Loss: 0.00002843
Iteration 106/1000 | Loss: 0.00008981
Iteration 107/1000 | Loss: 0.00012131
Iteration 108/1000 | Loss: 0.00026926
Iteration 109/1000 | Loss: 0.00003083
Iteration 110/1000 | Loss: 0.00004489
Iteration 111/1000 | Loss: 0.00008456
Iteration 112/1000 | Loss: 0.00008004
Iteration 113/1000 | Loss: 0.00004659
Iteration 114/1000 | Loss: 0.00012042
Iteration 115/1000 | Loss: 0.00002868
Iteration 116/1000 | Loss: 0.00005801
Iteration 117/1000 | Loss: 0.00002870
Iteration 118/1000 | Loss: 0.00002846
Iteration 119/1000 | Loss: 0.00002842
Iteration 120/1000 | Loss: 0.00002837
Iteration 121/1000 | Loss: 0.00002836
Iteration 122/1000 | Loss: 0.00002836
Iteration 123/1000 | Loss: 0.00006463
Iteration 124/1000 | Loss: 0.00002954
Iteration 125/1000 | Loss: 0.00002844
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002835
Iteration 128/1000 | Loss: 0.00002832
Iteration 129/1000 | Loss: 0.00002832
Iteration 130/1000 | Loss: 0.00002830
Iteration 131/1000 | Loss: 0.00002830
Iteration 132/1000 | Loss: 0.00002829
Iteration 133/1000 | Loss: 0.00002829
Iteration 134/1000 | Loss: 0.00002829
Iteration 135/1000 | Loss: 0.00002828
Iteration 136/1000 | Loss: 0.00002828
Iteration 137/1000 | Loss: 0.00002828
Iteration 138/1000 | Loss: 0.00002828
Iteration 139/1000 | Loss: 0.00002828
Iteration 140/1000 | Loss: 0.00002828
Iteration 141/1000 | Loss: 0.00002828
Iteration 142/1000 | Loss: 0.00002828
Iteration 143/1000 | Loss: 0.00002828
Iteration 144/1000 | Loss: 0.00002828
Iteration 145/1000 | Loss: 0.00002827
Iteration 146/1000 | Loss: 0.00002827
Iteration 147/1000 | Loss: 0.00002827
Iteration 148/1000 | Loss: 0.00002827
Iteration 149/1000 | Loss: 0.00002827
Iteration 150/1000 | Loss: 0.00002827
Iteration 151/1000 | Loss: 0.00002827
Iteration 152/1000 | Loss: 0.00002827
Iteration 153/1000 | Loss: 0.00002827
Iteration 154/1000 | Loss: 0.00002827
Iteration 155/1000 | Loss: 0.00002827
Iteration 156/1000 | Loss: 0.00002827
Iteration 157/1000 | Loss: 0.00002827
Iteration 158/1000 | Loss: 0.00002827
Iteration 159/1000 | Loss: 0.00002827
Iteration 160/1000 | Loss: 0.00002827
Iteration 161/1000 | Loss: 0.00002827
Iteration 162/1000 | Loss: 0.00002827
Iteration 163/1000 | Loss: 0.00002827
Iteration 164/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.827144089678768e-05, 2.827144089678768e-05, 2.827144089678768e-05, 2.827144089678768e-05, 2.827144089678768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.827144089678768e-05

Optimization complete. Final v2v error: 4.559910774230957 mm

Highest mean error: 5.40129280090332 mm for frame 1

Lowest mean error: 4.217757225036621 mm for frame 0

Saving results

Total time: 206.4887547492981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829219
Iteration 2/25 | Loss: 0.00095350
Iteration 3/25 | Loss: 0.00071299
Iteration 4/25 | Loss: 0.00066015
Iteration 5/25 | Loss: 0.00065123
Iteration 6/25 | Loss: 0.00065018
Iteration 7/25 | Loss: 0.00065009
Iteration 8/25 | Loss: 0.00065009
Iteration 9/25 | Loss: 0.00065009
Iteration 10/25 | Loss: 0.00065009
Iteration 11/25 | Loss: 0.00065009
Iteration 12/25 | Loss: 0.00065009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000650088710244745, 0.000650088710244745, 0.000650088710244745, 0.000650088710244745, 0.000650088710244745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000650088710244745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26036501
Iteration 2/25 | Loss: 0.00022231
Iteration 3/25 | Loss: 0.00022227
Iteration 4/25 | Loss: 0.00022227
Iteration 5/25 | Loss: 0.00022226
Iteration 6/25 | Loss: 0.00022226
Iteration 7/25 | Loss: 0.00022226
Iteration 8/25 | Loss: 0.00022226
Iteration 9/25 | Loss: 0.00022226
Iteration 10/25 | Loss: 0.00022226
Iteration 11/25 | Loss: 0.00022226
Iteration 12/25 | Loss: 0.00022226
Iteration 13/25 | Loss: 0.00022226
Iteration 14/25 | Loss: 0.00022226
Iteration 15/25 | Loss: 0.00022226
Iteration 16/25 | Loss: 0.00022226
Iteration 17/25 | Loss: 0.00022226
Iteration 18/25 | Loss: 0.00022226
Iteration 19/25 | Loss: 0.00022226
Iteration 20/25 | Loss: 0.00022226
Iteration 21/25 | Loss: 0.00022226
Iteration 22/25 | Loss: 0.00022226
Iteration 23/25 | Loss: 0.00022226
Iteration 24/25 | Loss: 0.00022226
Iteration 25/25 | Loss: 0.00022226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022226
Iteration 2/1000 | Loss: 0.00003709
Iteration 3/1000 | Loss: 0.00002632
Iteration 4/1000 | Loss: 0.00002380
Iteration 5/1000 | Loss: 0.00002213
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001963
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001956
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001915
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001913
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001905
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001894
Iteration 92/1000 | Loss: 0.00001894
Iteration 93/1000 | Loss: 0.00001894
Iteration 94/1000 | Loss: 0.00001893
Iteration 95/1000 | Loss: 0.00001893
Iteration 96/1000 | Loss: 0.00001893
Iteration 97/1000 | Loss: 0.00001892
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001892
Iteration 100/1000 | Loss: 0.00001892
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001891
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001890
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001888
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001888
Iteration 128/1000 | Loss: 0.00001888
Iteration 129/1000 | Loss: 0.00001888
Iteration 130/1000 | Loss: 0.00001888
Iteration 131/1000 | Loss: 0.00001888
Iteration 132/1000 | Loss: 0.00001888
Iteration 133/1000 | Loss: 0.00001888
Iteration 134/1000 | Loss: 0.00001888
Iteration 135/1000 | Loss: 0.00001888
Iteration 136/1000 | Loss: 0.00001888
Iteration 137/1000 | Loss: 0.00001888
Iteration 138/1000 | Loss: 0.00001888
Iteration 139/1000 | Loss: 0.00001888
Iteration 140/1000 | Loss: 0.00001888
Iteration 141/1000 | Loss: 0.00001888
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001887
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001887
Iteration 148/1000 | Loss: 0.00001887
Iteration 149/1000 | Loss: 0.00001887
Iteration 150/1000 | Loss: 0.00001887
Iteration 151/1000 | Loss: 0.00001887
Iteration 152/1000 | Loss: 0.00001887
Iteration 153/1000 | Loss: 0.00001887
Iteration 154/1000 | Loss: 0.00001887
Iteration 155/1000 | Loss: 0.00001887
Iteration 156/1000 | Loss: 0.00001887
Iteration 157/1000 | Loss: 0.00001887
Iteration 158/1000 | Loss: 0.00001887
Iteration 159/1000 | Loss: 0.00001887
Iteration 160/1000 | Loss: 0.00001887
Iteration 161/1000 | Loss: 0.00001887
Iteration 162/1000 | Loss: 0.00001887
Iteration 163/1000 | Loss: 0.00001887
Iteration 164/1000 | Loss: 0.00001887
Iteration 165/1000 | Loss: 0.00001887
Iteration 166/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.886868994915858e-05, 1.886868994915858e-05, 1.886868994915858e-05, 1.886868994915858e-05, 1.886868994915858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.886868994915858e-05

Optimization complete. Final v2v error: 3.719430923461914 mm

Highest mean error: 4.669883728027344 mm for frame 158

Lowest mean error: 2.994117021560669 mm for frame 223

Saving results

Total time: 46.828861713409424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861043
Iteration 2/25 | Loss: 0.00082662
Iteration 3/25 | Loss: 0.00066454
Iteration 4/25 | Loss: 0.00064152
Iteration 5/25 | Loss: 0.00063310
Iteration 6/25 | Loss: 0.00063091
Iteration 7/25 | Loss: 0.00063058
Iteration 8/25 | Loss: 0.00063058
Iteration 9/25 | Loss: 0.00063058
Iteration 10/25 | Loss: 0.00063058
Iteration 11/25 | Loss: 0.00063058
Iteration 12/25 | Loss: 0.00063058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006305832066573203, 0.0006305832066573203, 0.0006305832066573203, 0.0006305832066573203, 0.0006305832066573203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006305832066573203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28052056
Iteration 2/25 | Loss: 0.00033267
Iteration 3/25 | Loss: 0.00033267
Iteration 4/25 | Loss: 0.00033266
Iteration 5/25 | Loss: 0.00033266
Iteration 6/25 | Loss: 0.00033266
Iteration 7/25 | Loss: 0.00033266
Iteration 8/25 | Loss: 0.00033266
Iteration 9/25 | Loss: 0.00033266
Iteration 10/25 | Loss: 0.00033266
Iteration 11/25 | Loss: 0.00033266
Iteration 12/25 | Loss: 0.00033266
Iteration 13/25 | Loss: 0.00033266
Iteration 14/25 | Loss: 0.00033266
Iteration 15/25 | Loss: 0.00033266
Iteration 16/25 | Loss: 0.00033266
Iteration 17/25 | Loss: 0.00033266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00033266295213252306, 0.00033266295213252306, 0.00033266295213252306, 0.00033266295213252306, 0.00033266295213252306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033266295213252306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033266
Iteration 2/1000 | Loss: 0.00003542
Iteration 3/1000 | Loss: 0.00002674
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001943
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001908
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001905
Iteration 20/1000 | Loss: 0.00001903
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001890
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001884
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001880
Iteration 45/1000 | Loss: 0.00001880
Iteration 46/1000 | Loss: 0.00001880
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001879
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001870
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001865
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001860
Iteration 76/1000 | Loss: 0.00001860
Iteration 77/1000 | Loss: 0.00001860
Iteration 78/1000 | Loss: 0.00001860
Iteration 79/1000 | Loss: 0.00001860
Iteration 80/1000 | Loss: 0.00001860
Iteration 81/1000 | Loss: 0.00001860
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001859
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001858
Iteration 89/1000 | Loss: 0.00001858
Iteration 90/1000 | Loss: 0.00001858
Iteration 91/1000 | Loss: 0.00001858
Iteration 92/1000 | Loss: 0.00001858
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001857
Iteration 98/1000 | Loss: 0.00001857
Iteration 99/1000 | Loss: 0.00001857
Iteration 100/1000 | Loss: 0.00001857
Iteration 101/1000 | Loss: 0.00001857
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001850
Iteration 164/1000 | Loss: 0.00001850
Iteration 165/1000 | Loss: 0.00001850
Iteration 166/1000 | Loss: 0.00001850
Iteration 167/1000 | Loss: 0.00001850
Iteration 168/1000 | Loss: 0.00001850
Iteration 169/1000 | Loss: 0.00001850
Iteration 170/1000 | Loss: 0.00001850
Iteration 171/1000 | Loss: 0.00001850
Iteration 172/1000 | Loss: 0.00001850
Iteration 173/1000 | Loss: 0.00001850
Iteration 174/1000 | Loss: 0.00001850
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001849
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001849
Iteration 179/1000 | Loss: 0.00001849
Iteration 180/1000 | Loss: 0.00001849
Iteration 181/1000 | Loss: 0.00001849
Iteration 182/1000 | Loss: 0.00001849
Iteration 183/1000 | Loss: 0.00001849
Iteration 184/1000 | Loss: 0.00001849
Iteration 185/1000 | Loss: 0.00001849
Iteration 186/1000 | Loss: 0.00001849
Iteration 187/1000 | Loss: 0.00001849
Iteration 188/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.849203727033455e-05, 1.849203727033455e-05, 1.849203727033455e-05, 1.849203727033455e-05, 1.849203727033455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.849203727033455e-05

Optimization complete. Final v2v error: 3.650104522705078 mm

Highest mean error: 4.078026294708252 mm for frame 95

Lowest mean error: 2.8862996101379395 mm for frame 8

Saving results

Total time: 42.00402879714966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452561
Iteration 2/25 | Loss: 0.00072613
Iteration 3/25 | Loss: 0.00060867
Iteration 4/25 | Loss: 0.00059003
Iteration 5/25 | Loss: 0.00058342
Iteration 6/25 | Loss: 0.00058122
Iteration 7/25 | Loss: 0.00058062
Iteration 8/25 | Loss: 0.00058048
Iteration 9/25 | Loss: 0.00058048
Iteration 10/25 | Loss: 0.00058048
Iteration 11/25 | Loss: 0.00058048
Iteration 12/25 | Loss: 0.00058048
Iteration 13/25 | Loss: 0.00058048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005804832908324897, 0.0005804832908324897, 0.0005804832908324897, 0.0005804832908324897, 0.0005804832908324897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005804832908324897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29410112
Iteration 2/25 | Loss: 0.00036496
Iteration 3/25 | Loss: 0.00036495
Iteration 4/25 | Loss: 0.00036495
Iteration 5/25 | Loss: 0.00036495
Iteration 6/25 | Loss: 0.00036495
Iteration 7/25 | Loss: 0.00036495
Iteration 8/25 | Loss: 0.00036495
Iteration 9/25 | Loss: 0.00036495
Iteration 10/25 | Loss: 0.00036495
Iteration 11/25 | Loss: 0.00036495
Iteration 12/25 | Loss: 0.00036495
Iteration 13/25 | Loss: 0.00036495
Iteration 14/25 | Loss: 0.00036495
Iteration 15/25 | Loss: 0.00036495
Iteration 16/25 | Loss: 0.00036495
Iteration 17/25 | Loss: 0.00036495
Iteration 18/25 | Loss: 0.00036495
Iteration 19/25 | Loss: 0.00036495
Iteration 20/25 | Loss: 0.00036495
Iteration 21/25 | Loss: 0.00036495
Iteration 22/25 | Loss: 0.00036495
Iteration 23/25 | Loss: 0.00036495
Iteration 24/25 | Loss: 0.00036495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003649501013569534, 0.0003649501013569534, 0.0003649501013569534, 0.0003649501013569534, 0.0003649501013569534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003649501013569534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036495
Iteration 2/1000 | Loss: 0.00002989
Iteration 3/1000 | Loss: 0.00002034
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001377
Iteration 13/1000 | Loss: 0.00001362
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001325
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001320
Iteration 40/1000 | Loss: 0.00001320
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001319
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001295
Iteration 108/1000 | Loss: 0.00001295
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001293
Iteration 133/1000 | Loss: 0.00001293
Iteration 134/1000 | Loss: 0.00001293
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001292
Iteration 140/1000 | Loss: 0.00001292
Iteration 141/1000 | Loss: 0.00001292
Iteration 142/1000 | Loss: 0.00001292
Iteration 143/1000 | Loss: 0.00001292
Iteration 144/1000 | Loss: 0.00001292
Iteration 145/1000 | Loss: 0.00001292
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Iteration 149/1000 | Loss: 0.00001292
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001291
Iteration 154/1000 | Loss: 0.00001291
Iteration 155/1000 | Loss: 0.00001291
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001290
Iteration 160/1000 | Loss: 0.00001290
Iteration 161/1000 | Loss: 0.00001290
Iteration 162/1000 | Loss: 0.00001290
Iteration 163/1000 | Loss: 0.00001290
Iteration 164/1000 | Loss: 0.00001290
Iteration 165/1000 | Loss: 0.00001290
Iteration 166/1000 | Loss: 0.00001290
Iteration 167/1000 | Loss: 0.00001290
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001288
Iteration 174/1000 | Loss: 0.00001288
Iteration 175/1000 | Loss: 0.00001288
Iteration 176/1000 | Loss: 0.00001288
Iteration 177/1000 | Loss: 0.00001288
Iteration 178/1000 | Loss: 0.00001287
Iteration 179/1000 | Loss: 0.00001287
Iteration 180/1000 | Loss: 0.00001287
Iteration 181/1000 | Loss: 0.00001287
Iteration 182/1000 | Loss: 0.00001287
Iteration 183/1000 | Loss: 0.00001287
Iteration 184/1000 | Loss: 0.00001287
Iteration 185/1000 | Loss: 0.00001287
Iteration 186/1000 | Loss: 0.00001287
Iteration 187/1000 | Loss: 0.00001287
Iteration 188/1000 | Loss: 0.00001286
Iteration 189/1000 | Loss: 0.00001286
Iteration 190/1000 | Loss: 0.00001286
Iteration 191/1000 | Loss: 0.00001286
Iteration 192/1000 | Loss: 0.00001286
Iteration 193/1000 | Loss: 0.00001286
Iteration 194/1000 | Loss: 0.00001286
Iteration 195/1000 | Loss: 0.00001286
Iteration 196/1000 | Loss: 0.00001286
Iteration 197/1000 | Loss: 0.00001286
Iteration 198/1000 | Loss: 0.00001286
Iteration 199/1000 | Loss: 0.00001286
Iteration 200/1000 | Loss: 0.00001285
Iteration 201/1000 | Loss: 0.00001285
Iteration 202/1000 | Loss: 0.00001285
Iteration 203/1000 | Loss: 0.00001285
Iteration 204/1000 | Loss: 0.00001285
Iteration 205/1000 | Loss: 0.00001285
Iteration 206/1000 | Loss: 0.00001285
Iteration 207/1000 | Loss: 0.00001285
Iteration 208/1000 | Loss: 0.00001285
Iteration 209/1000 | Loss: 0.00001285
Iteration 210/1000 | Loss: 0.00001285
Iteration 211/1000 | Loss: 0.00001285
Iteration 212/1000 | Loss: 0.00001285
Iteration 213/1000 | Loss: 0.00001285
Iteration 214/1000 | Loss: 0.00001285
Iteration 215/1000 | Loss: 0.00001284
Iteration 216/1000 | Loss: 0.00001284
Iteration 217/1000 | Loss: 0.00001284
Iteration 218/1000 | Loss: 0.00001284
Iteration 219/1000 | Loss: 0.00001284
Iteration 220/1000 | Loss: 0.00001284
Iteration 221/1000 | Loss: 0.00001284
Iteration 222/1000 | Loss: 0.00001284
Iteration 223/1000 | Loss: 0.00001284
Iteration 224/1000 | Loss: 0.00001284
Iteration 225/1000 | Loss: 0.00001284
Iteration 226/1000 | Loss: 0.00001284
Iteration 227/1000 | Loss: 0.00001284
Iteration 228/1000 | Loss: 0.00001284
Iteration 229/1000 | Loss: 0.00001284
Iteration 230/1000 | Loss: 0.00001284
Iteration 231/1000 | Loss: 0.00001284
Iteration 232/1000 | Loss: 0.00001284
Iteration 233/1000 | Loss: 0.00001284
Iteration 234/1000 | Loss: 0.00001284
Iteration 235/1000 | Loss: 0.00001284
Iteration 236/1000 | Loss: 0.00001284
Iteration 237/1000 | Loss: 0.00001283
Iteration 238/1000 | Loss: 0.00001283
Iteration 239/1000 | Loss: 0.00001283
Iteration 240/1000 | Loss: 0.00001283
Iteration 241/1000 | Loss: 0.00001283
Iteration 242/1000 | Loss: 0.00001283
Iteration 243/1000 | Loss: 0.00001283
Iteration 244/1000 | Loss: 0.00001283
Iteration 245/1000 | Loss: 0.00001283
Iteration 246/1000 | Loss: 0.00001283
Iteration 247/1000 | Loss: 0.00001283
Iteration 248/1000 | Loss: 0.00001283
Iteration 249/1000 | Loss: 0.00001283
Iteration 250/1000 | Loss: 0.00001283
Iteration 251/1000 | Loss: 0.00001283
Iteration 252/1000 | Loss: 0.00001283
Iteration 253/1000 | Loss: 0.00001283
Iteration 254/1000 | Loss: 0.00001283
Iteration 255/1000 | Loss: 0.00001283
Iteration 256/1000 | Loss: 0.00001283
Iteration 257/1000 | Loss: 0.00001283
Iteration 258/1000 | Loss: 0.00001283
Iteration 259/1000 | Loss: 0.00001283
Iteration 260/1000 | Loss: 0.00001283
Iteration 261/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.2826314559788443e-05, 1.2826314559788443e-05, 1.2826314559788443e-05, 1.2826314559788443e-05, 1.2826314559788443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2826314559788443e-05

Optimization complete. Final v2v error: 2.9638407230377197 mm

Highest mean error: 3.4199142456054688 mm for frame 60

Lowest mean error: 2.5724070072174072 mm for frame 133

Saving results

Total time: 44.14408206939697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491244
Iteration 2/25 | Loss: 0.00122137
Iteration 3/25 | Loss: 0.00083454
Iteration 4/25 | Loss: 0.00074927
Iteration 5/25 | Loss: 0.00073173
Iteration 6/25 | Loss: 0.00072690
Iteration 7/25 | Loss: 0.00072582
Iteration 8/25 | Loss: 0.00072572
Iteration 9/25 | Loss: 0.00072572
Iteration 10/25 | Loss: 0.00072572
Iteration 11/25 | Loss: 0.00072572
Iteration 12/25 | Loss: 0.00072572
Iteration 13/25 | Loss: 0.00072572
Iteration 14/25 | Loss: 0.00072572
Iteration 15/25 | Loss: 0.00072572
Iteration 16/25 | Loss: 0.00072572
Iteration 17/25 | Loss: 0.00072572
Iteration 18/25 | Loss: 0.00072572
Iteration 19/25 | Loss: 0.00072572
Iteration 20/25 | Loss: 0.00072572
Iteration 21/25 | Loss: 0.00072572
Iteration 22/25 | Loss: 0.00072572
Iteration 23/25 | Loss: 0.00072572
Iteration 24/25 | Loss: 0.00072572
Iteration 25/25 | Loss: 0.00072572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37736130
Iteration 2/25 | Loss: 0.00034582
Iteration 3/25 | Loss: 0.00034582
Iteration 4/25 | Loss: 0.00034582
Iteration 5/25 | Loss: 0.00034582
Iteration 6/25 | Loss: 0.00034582
Iteration 7/25 | Loss: 0.00034582
Iteration 8/25 | Loss: 0.00034582
Iteration 9/25 | Loss: 0.00034582
Iteration 10/25 | Loss: 0.00034582
Iteration 11/25 | Loss: 0.00034582
Iteration 12/25 | Loss: 0.00034582
Iteration 13/25 | Loss: 0.00034582
Iteration 14/25 | Loss: 0.00034582
Iteration 15/25 | Loss: 0.00034582
Iteration 16/25 | Loss: 0.00034582
Iteration 17/25 | Loss: 0.00034582
Iteration 18/25 | Loss: 0.00034582
Iteration 19/25 | Loss: 0.00034582
Iteration 20/25 | Loss: 0.00034582
Iteration 21/25 | Loss: 0.00034582
Iteration 22/25 | Loss: 0.00034582
Iteration 23/25 | Loss: 0.00034582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00034581965883262455, 0.00034581965883262455, 0.00034581965883262455, 0.00034581965883262455, 0.00034581965883262455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034581965883262455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034582
Iteration 2/1000 | Loss: 0.00004495
Iteration 3/1000 | Loss: 0.00003410
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002787
Iteration 6/1000 | Loss: 0.00002704
Iteration 7/1000 | Loss: 0.00002645
Iteration 8/1000 | Loss: 0.00002578
Iteration 9/1000 | Loss: 0.00002539
Iteration 10/1000 | Loss: 0.00002512
Iteration 11/1000 | Loss: 0.00002510
Iteration 12/1000 | Loss: 0.00002490
Iteration 13/1000 | Loss: 0.00002472
Iteration 14/1000 | Loss: 0.00002457
Iteration 15/1000 | Loss: 0.00002450
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002448
Iteration 18/1000 | Loss: 0.00002446
Iteration 19/1000 | Loss: 0.00002445
Iteration 20/1000 | Loss: 0.00002445
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002443
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002439
Iteration 25/1000 | Loss: 0.00002439
Iteration 26/1000 | Loss: 0.00002439
Iteration 27/1000 | Loss: 0.00002438
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002434
Iteration 36/1000 | Loss: 0.00002434
Iteration 37/1000 | Loss: 0.00002434
Iteration 38/1000 | Loss: 0.00002434
Iteration 39/1000 | Loss: 0.00002434
Iteration 40/1000 | Loss: 0.00002434
Iteration 41/1000 | Loss: 0.00002434
Iteration 42/1000 | Loss: 0.00002434
Iteration 43/1000 | Loss: 0.00002433
Iteration 44/1000 | Loss: 0.00002433
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002432
Iteration 47/1000 | Loss: 0.00002432
Iteration 48/1000 | Loss: 0.00002431
Iteration 49/1000 | Loss: 0.00002431
Iteration 50/1000 | Loss: 0.00002431
Iteration 51/1000 | Loss: 0.00002431
Iteration 52/1000 | Loss: 0.00002430
Iteration 53/1000 | Loss: 0.00002430
Iteration 54/1000 | Loss: 0.00002429
Iteration 55/1000 | Loss: 0.00002429
Iteration 56/1000 | Loss: 0.00002429
Iteration 57/1000 | Loss: 0.00002428
Iteration 58/1000 | Loss: 0.00002428
Iteration 59/1000 | Loss: 0.00002428
Iteration 60/1000 | Loss: 0.00002428
Iteration 61/1000 | Loss: 0.00002428
Iteration 62/1000 | Loss: 0.00002428
Iteration 63/1000 | Loss: 0.00002428
Iteration 64/1000 | Loss: 0.00002428
Iteration 65/1000 | Loss: 0.00002428
Iteration 66/1000 | Loss: 0.00002428
Iteration 67/1000 | Loss: 0.00002428
Iteration 68/1000 | Loss: 0.00002428
Iteration 69/1000 | Loss: 0.00002428
Iteration 70/1000 | Loss: 0.00002428
Iteration 71/1000 | Loss: 0.00002427
Iteration 72/1000 | Loss: 0.00002427
Iteration 73/1000 | Loss: 0.00002427
Iteration 74/1000 | Loss: 0.00002427
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002427
Iteration 77/1000 | Loss: 0.00002427
Iteration 78/1000 | Loss: 0.00002427
Iteration 79/1000 | Loss: 0.00002427
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002425
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002424
Iteration 88/1000 | Loss: 0.00002424
Iteration 89/1000 | Loss: 0.00002424
Iteration 90/1000 | Loss: 0.00002423
Iteration 91/1000 | Loss: 0.00002423
Iteration 92/1000 | Loss: 0.00002423
Iteration 93/1000 | Loss: 0.00002422
Iteration 94/1000 | Loss: 0.00002422
Iteration 95/1000 | Loss: 0.00002422
Iteration 96/1000 | Loss: 0.00002421
Iteration 97/1000 | Loss: 0.00002421
Iteration 98/1000 | Loss: 0.00002421
Iteration 99/1000 | Loss: 0.00002420
Iteration 100/1000 | Loss: 0.00002420
Iteration 101/1000 | Loss: 0.00002420
Iteration 102/1000 | Loss: 0.00002420
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002419
Iteration 110/1000 | Loss: 0.00002419
Iteration 111/1000 | Loss: 0.00002419
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00002419
Iteration 114/1000 | Loss: 0.00002419
Iteration 115/1000 | Loss: 0.00002419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.4189241230487823e-05, 2.4189241230487823e-05, 2.4189241230487823e-05, 2.4189241230487823e-05, 2.4189241230487823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4189241230487823e-05

Optimization complete. Final v2v error: 4.215250015258789 mm

Highest mean error: 4.669395446777344 mm for frame 144

Lowest mean error: 3.6999013423919678 mm for frame 213

Saving results

Total time: 42.260225772857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696047
Iteration 2/25 | Loss: 0.00081484
Iteration 3/25 | Loss: 0.00063694
Iteration 4/25 | Loss: 0.00060907
Iteration 5/25 | Loss: 0.00059872
Iteration 6/25 | Loss: 0.00059646
Iteration 7/25 | Loss: 0.00059601
Iteration 8/25 | Loss: 0.00059594
Iteration 9/25 | Loss: 0.00059594
Iteration 10/25 | Loss: 0.00059594
Iteration 11/25 | Loss: 0.00059594
Iteration 12/25 | Loss: 0.00059594
Iteration 13/25 | Loss: 0.00059594
Iteration 14/25 | Loss: 0.00059594
Iteration 15/25 | Loss: 0.00059594
Iteration 16/25 | Loss: 0.00059594
Iteration 17/25 | Loss: 0.00059594
Iteration 18/25 | Loss: 0.00059594
Iteration 19/25 | Loss: 0.00059594
Iteration 20/25 | Loss: 0.00059594
Iteration 21/25 | Loss: 0.00059594
Iteration 22/25 | Loss: 0.00059594
Iteration 23/25 | Loss: 0.00059594
Iteration 24/25 | Loss: 0.00059594
Iteration 25/25 | Loss: 0.00059594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37785721
Iteration 2/25 | Loss: 0.00029204
Iteration 3/25 | Loss: 0.00029204
Iteration 4/25 | Loss: 0.00029204
Iteration 5/25 | Loss: 0.00029204
Iteration 6/25 | Loss: 0.00029204
Iteration 7/25 | Loss: 0.00029204
Iteration 8/25 | Loss: 0.00029204
Iteration 9/25 | Loss: 0.00029204
Iteration 10/25 | Loss: 0.00029204
Iteration 11/25 | Loss: 0.00029204
Iteration 12/25 | Loss: 0.00029204
Iteration 13/25 | Loss: 0.00029204
Iteration 14/25 | Loss: 0.00029204
Iteration 15/25 | Loss: 0.00029204
Iteration 16/25 | Loss: 0.00029204
Iteration 17/25 | Loss: 0.00029204
Iteration 18/25 | Loss: 0.00029204
Iteration 19/25 | Loss: 0.00029204
Iteration 20/25 | Loss: 0.00029204
Iteration 21/25 | Loss: 0.00029204
Iteration 22/25 | Loss: 0.00029204
Iteration 23/25 | Loss: 0.00029204
Iteration 24/25 | Loss: 0.00029204
Iteration 25/25 | Loss: 0.00029204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029204
Iteration 2/1000 | Loss: 0.00002855
Iteration 3/1000 | Loss: 0.00002081
Iteration 4/1000 | Loss: 0.00001878
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001611
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001571
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001564
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001530
Iteration 72/1000 | Loss: 0.00001530
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001528
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001527
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001526
Iteration 118/1000 | Loss: 0.00001526
Iteration 119/1000 | Loss: 0.00001526
Iteration 120/1000 | Loss: 0.00001526
Iteration 121/1000 | Loss: 0.00001526
Iteration 122/1000 | Loss: 0.00001526
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001526
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001525
Iteration 138/1000 | Loss: 0.00001525
Iteration 139/1000 | Loss: 0.00001525
Iteration 140/1000 | Loss: 0.00001525
Iteration 141/1000 | Loss: 0.00001525
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001524
Iteration 151/1000 | Loss: 0.00001524
Iteration 152/1000 | Loss: 0.00001524
Iteration 153/1000 | Loss: 0.00001524
Iteration 154/1000 | Loss: 0.00001524
Iteration 155/1000 | Loss: 0.00001524
Iteration 156/1000 | Loss: 0.00001524
Iteration 157/1000 | Loss: 0.00001524
Iteration 158/1000 | Loss: 0.00001524
Iteration 159/1000 | Loss: 0.00001524
Iteration 160/1000 | Loss: 0.00001524
Iteration 161/1000 | Loss: 0.00001524
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001523
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001523
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001523
Iteration 174/1000 | Loss: 0.00001523
Iteration 175/1000 | Loss: 0.00001523
Iteration 176/1000 | Loss: 0.00001523
Iteration 177/1000 | Loss: 0.00001523
Iteration 178/1000 | Loss: 0.00001523
Iteration 179/1000 | Loss: 0.00001523
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Iteration 182/1000 | Loss: 0.00001523
Iteration 183/1000 | Loss: 0.00001523
Iteration 184/1000 | Loss: 0.00001523
Iteration 185/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.523064656794304e-05, 1.523064656794304e-05, 1.523064656794304e-05, 1.523064656794304e-05, 1.523064656794304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.523064656794304e-05

Optimization complete. Final v2v error: 3.365187883377075 mm

Highest mean error: 3.868464946746826 mm for frame 98

Lowest mean error: 2.9974138736724854 mm for frame 118

Saving results

Total time: 39.10888338088989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055341
Iteration 2/25 | Loss: 0.01055341
Iteration 3/25 | Loss: 0.01055341
Iteration 4/25 | Loss: 0.01055341
Iteration 5/25 | Loss: 0.01055340
Iteration 6/25 | Loss: 0.01055340
Iteration 7/25 | Loss: 0.01055340
Iteration 8/25 | Loss: 0.01055340
Iteration 9/25 | Loss: 0.01055340
Iteration 10/25 | Loss: 0.01055340
Iteration 11/25 | Loss: 0.01055340
Iteration 12/25 | Loss: 0.01055340
Iteration 13/25 | Loss: 0.01055340
Iteration 14/25 | Loss: 0.01055340
Iteration 15/25 | Loss: 0.01055340
Iteration 16/25 | Loss: 0.01055339
Iteration 17/25 | Loss: 0.01055339
Iteration 18/25 | Loss: 0.01055339
Iteration 19/25 | Loss: 0.01055339
Iteration 20/25 | Loss: 0.01055339
Iteration 21/25 | Loss: 0.01055339
Iteration 22/25 | Loss: 0.01055339
Iteration 23/25 | Loss: 0.01055339
Iteration 24/25 | Loss: 0.01055339
Iteration 25/25 | Loss: 0.01055339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58845234
Iteration 2/25 | Loss: 0.14471255
Iteration 3/25 | Loss: 0.11591394
Iteration 4/25 | Loss: 0.10814367
Iteration 5/25 | Loss: 0.10795117
Iteration 6/25 | Loss: 0.10795116
Iteration 7/25 | Loss: 0.10795116
Iteration 8/25 | Loss: 0.10795116
Iteration 9/25 | Loss: 0.10795117
Iteration 10/25 | Loss: 0.10795116
Iteration 11/25 | Loss: 0.10798807
Iteration 12/25 | Loss: 0.10775577
Iteration 13/25 | Loss: 0.10798765
Iteration 14/25 | Loss: 0.10798761
Iteration 15/25 | Loss: 0.10798762
Iteration 16/25 | Loss: 0.10798762
Iteration 17/25 | Loss: 0.10775541
Iteration 18/25 | Loss: 0.10797998
Iteration 19/25 | Loss: 0.10775052
Iteration 20/25 | Loss: 0.10775051
Iteration 21/25 | Loss: 0.10775051
Iteration 22/25 | Loss: 0.10775049
Iteration 23/25 | Loss: 0.10775049
Iteration 24/25 | Loss: 0.10775049
Iteration 25/25 | Loss: 0.10775048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10775048
Iteration 2/1000 | Loss: 0.00365657
Iteration 3/1000 | Loss: 0.00603267
Iteration 4/1000 | Loss: 0.00216339
Iteration 5/1000 | Loss: 0.00133861
Iteration 6/1000 | Loss: 0.00229235
Iteration 7/1000 | Loss: 0.00073002
Iteration 8/1000 | Loss: 0.00036181
Iteration 9/1000 | Loss: 0.00054065
Iteration 10/1000 | Loss: 0.00033207
Iteration 11/1000 | Loss: 0.00035173
Iteration 12/1000 | Loss: 0.00038077
Iteration 13/1000 | Loss: 0.00049337
Iteration 14/1000 | Loss: 0.00047145
Iteration 15/1000 | Loss: 0.00026395
Iteration 16/1000 | Loss: 0.00021147
Iteration 17/1000 | Loss: 0.00014036
Iteration 18/1000 | Loss: 0.00097887
Iteration 19/1000 | Loss: 0.00005977
Iteration 20/1000 | Loss: 0.00015022
Iteration 21/1000 | Loss: 0.00004459
Iteration 22/1000 | Loss: 0.00004489
Iteration 23/1000 | Loss: 0.00014767
Iteration 24/1000 | Loss: 0.00015064
Iteration 25/1000 | Loss: 0.00003860
Iteration 26/1000 | Loss: 0.00007365
Iteration 27/1000 | Loss: 0.00008565
Iteration 28/1000 | Loss: 0.00003333
Iteration 29/1000 | Loss: 0.00006325
Iteration 30/1000 | Loss: 0.00003234
Iteration 31/1000 | Loss: 0.00005391
Iteration 32/1000 | Loss: 0.00006569
Iteration 33/1000 | Loss: 0.00003318
Iteration 34/1000 | Loss: 0.00004827
Iteration 35/1000 | Loss: 0.00009577
Iteration 36/1000 | Loss: 0.00007880
Iteration 37/1000 | Loss: 0.00037405
Iteration 38/1000 | Loss: 0.00003672
Iteration 39/1000 | Loss: 0.00003775
Iteration 40/1000 | Loss: 0.00017464
Iteration 41/1000 | Loss: 0.00003570
Iteration 42/1000 | Loss: 0.00003870
Iteration 43/1000 | Loss: 0.00004050
Iteration 44/1000 | Loss: 0.00003063
Iteration 45/1000 | Loss: 0.00003441
Iteration 46/1000 | Loss: 0.00003555
Iteration 47/1000 | Loss: 0.00004590
Iteration 48/1000 | Loss: 0.00003596
Iteration 49/1000 | Loss: 0.00004401
Iteration 50/1000 | Loss: 0.00003046
Iteration 51/1000 | Loss: 0.00003046
Iteration 52/1000 | Loss: 0.00003046
Iteration 53/1000 | Loss: 0.00003046
Iteration 54/1000 | Loss: 0.00003046
Iteration 55/1000 | Loss: 0.00003045
Iteration 56/1000 | Loss: 0.00003045
Iteration 57/1000 | Loss: 0.00003045
Iteration 58/1000 | Loss: 0.00003045
Iteration 59/1000 | Loss: 0.00003045
Iteration 60/1000 | Loss: 0.00003045
Iteration 61/1000 | Loss: 0.00003045
Iteration 62/1000 | Loss: 0.00003045
Iteration 63/1000 | Loss: 0.00003045
Iteration 64/1000 | Loss: 0.00003045
Iteration 65/1000 | Loss: 0.00003045
Iteration 66/1000 | Loss: 0.00003045
Iteration 67/1000 | Loss: 0.00003045
Iteration 68/1000 | Loss: 0.00003045
Iteration 69/1000 | Loss: 0.00003045
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00003045
Iteration 72/1000 | Loss: 0.00003045
Iteration 73/1000 | Loss: 0.00003045
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003045
Iteration 79/1000 | Loss: 0.00003045
Iteration 80/1000 | Loss: 0.00003045
Iteration 81/1000 | Loss: 0.00003045
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003045
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00003045
Iteration 95/1000 | Loss: 0.00003045
Iteration 96/1000 | Loss: 0.00003045
Iteration 97/1000 | Loss: 0.00003045
Iteration 98/1000 | Loss: 0.00003045
Iteration 99/1000 | Loss: 0.00003045
Iteration 100/1000 | Loss: 0.00003045
Iteration 101/1000 | Loss: 0.00003045
Iteration 102/1000 | Loss: 0.00003045
Iteration 103/1000 | Loss: 0.00003045
Iteration 104/1000 | Loss: 0.00003045
Iteration 105/1000 | Loss: 0.00003045
Iteration 106/1000 | Loss: 0.00003045
Iteration 107/1000 | Loss: 0.00003045
Iteration 108/1000 | Loss: 0.00003045
Iteration 109/1000 | Loss: 0.00003045
Iteration 110/1000 | Loss: 0.00003045
Iteration 111/1000 | Loss: 0.00003045
Iteration 112/1000 | Loss: 0.00003045
Iteration 113/1000 | Loss: 0.00003045
Iteration 114/1000 | Loss: 0.00003045
Iteration 115/1000 | Loss: 0.00003045
Iteration 116/1000 | Loss: 0.00003045
Iteration 117/1000 | Loss: 0.00003045
Iteration 118/1000 | Loss: 0.00003045
Iteration 119/1000 | Loss: 0.00003045
Iteration 120/1000 | Loss: 0.00003045
Iteration 121/1000 | Loss: 0.00003045
Iteration 122/1000 | Loss: 0.00003045
Iteration 123/1000 | Loss: 0.00003045
Iteration 124/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [3.0451605198322795e-05, 3.0451605198322795e-05, 3.0451605198322795e-05, 3.0451605198322795e-05, 3.0451605198322795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0451605198322795e-05

Optimization complete. Final v2v error: 4.661865234375 mm

Highest mean error: 5.048618316650391 mm for frame 150

Lowest mean error: 4.299978733062744 mm for frame 223

Saving results

Total time: 92.7033200263977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604843
Iteration 2/25 | Loss: 0.00097027
Iteration 3/25 | Loss: 0.00068897
Iteration 4/25 | Loss: 0.00064847
Iteration 5/25 | Loss: 0.00063932
Iteration 6/25 | Loss: 0.00063689
Iteration 7/25 | Loss: 0.00063652
Iteration 8/25 | Loss: 0.00063652
Iteration 9/25 | Loss: 0.00063652
Iteration 10/25 | Loss: 0.00063652
Iteration 11/25 | Loss: 0.00063652
Iteration 12/25 | Loss: 0.00063652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006365224835462868, 0.0006365224835462868, 0.0006365224835462868, 0.0006365224835462868, 0.0006365224835462868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006365224835462868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34434676
Iteration 2/25 | Loss: 0.00028945
Iteration 3/25 | Loss: 0.00028945
Iteration 4/25 | Loss: 0.00028945
Iteration 5/25 | Loss: 0.00028945
Iteration 6/25 | Loss: 0.00028945
Iteration 7/25 | Loss: 0.00028945
Iteration 8/25 | Loss: 0.00028945
Iteration 9/25 | Loss: 0.00028945
Iteration 10/25 | Loss: 0.00028945
Iteration 11/25 | Loss: 0.00028945
Iteration 12/25 | Loss: 0.00028945
Iteration 13/25 | Loss: 0.00028945
Iteration 14/25 | Loss: 0.00028945
Iteration 15/25 | Loss: 0.00028945
Iteration 16/25 | Loss: 0.00028945
Iteration 17/25 | Loss: 0.00028945
Iteration 18/25 | Loss: 0.00028945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002894482749979943, 0.0002894482749979943, 0.0002894482749979943, 0.0002894482749979943, 0.0002894482749979943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002894482749979943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028945
Iteration 2/1000 | Loss: 0.00003170
Iteration 3/1000 | Loss: 0.00002404
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002068
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001851
Iteration 11/1000 | Loss: 0.00001845
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001828
Iteration 21/1000 | Loss: 0.00001828
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001825
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001823
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001817
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001815
Iteration 37/1000 | Loss: 0.00001814
Iteration 38/1000 | Loss: 0.00001814
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001813
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001812
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001805
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001804
Iteration 57/1000 | Loss: 0.00001804
Iteration 58/1000 | Loss: 0.00001804
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001803
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001796
Iteration 73/1000 | Loss: 0.00001796
Iteration 74/1000 | Loss: 0.00001796
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001793
Iteration 80/1000 | Loss: 0.00001793
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001791
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001790
Iteration 90/1000 | Loss: 0.00001790
Iteration 91/1000 | Loss: 0.00001790
Iteration 92/1000 | Loss: 0.00001790
Iteration 93/1000 | Loss: 0.00001790
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001789
Iteration 99/1000 | Loss: 0.00001789
Iteration 100/1000 | Loss: 0.00001789
Iteration 101/1000 | Loss: 0.00001789
Iteration 102/1000 | Loss: 0.00001789
Iteration 103/1000 | Loss: 0.00001789
Iteration 104/1000 | Loss: 0.00001789
Iteration 105/1000 | Loss: 0.00001789
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001789
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001787
Iteration 113/1000 | Loss: 0.00001787
Iteration 114/1000 | Loss: 0.00001787
Iteration 115/1000 | Loss: 0.00001787
Iteration 116/1000 | Loss: 0.00001787
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001784
Iteration 125/1000 | Loss: 0.00001784
Iteration 126/1000 | Loss: 0.00001784
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001783
Iteration 130/1000 | Loss: 0.00001783
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001782
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001779
Iteration 161/1000 | Loss: 0.00001779
Iteration 162/1000 | Loss: 0.00001779
Iteration 163/1000 | Loss: 0.00001779
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001779
Iteration 171/1000 | Loss: 0.00001779
Iteration 172/1000 | Loss: 0.00001779
Iteration 173/1000 | Loss: 0.00001779
Iteration 174/1000 | Loss: 0.00001779
Iteration 175/1000 | Loss: 0.00001779
Iteration 176/1000 | Loss: 0.00001779
Iteration 177/1000 | Loss: 0.00001779
Iteration 178/1000 | Loss: 0.00001779
Iteration 179/1000 | Loss: 0.00001779
Iteration 180/1000 | Loss: 0.00001779
Iteration 181/1000 | Loss: 0.00001779
Iteration 182/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.7786882381187752e-05, 1.7786882381187752e-05, 1.7786882381187752e-05, 1.7786882381187752e-05, 1.7786882381187752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7786882381187752e-05

Optimization complete. Final v2v error: 3.6325578689575195 mm

Highest mean error: 4.348705768585205 mm for frame 80

Lowest mean error: 3.279118537902832 mm for frame 171

Saving results

Total time: 44.07858228683472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559131
Iteration 2/25 | Loss: 0.00094037
Iteration 3/25 | Loss: 0.00073963
Iteration 4/25 | Loss: 0.00070287
Iteration 5/25 | Loss: 0.00069012
Iteration 6/25 | Loss: 0.00068694
Iteration 7/25 | Loss: 0.00068549
Iteration 8/25 | Loss: 0.00068546
Iteration 9/25 | Loss: 0.00068546
Iteration 10/25 | Loss: 0.00068546
Iteration 11/25 | Loss: 0.00068546
Iteration 12/25 | Loss: 0.00068546
Iteration 13/25 | Loss: 0.00068546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006854583625681698, 0.0006854583625681698, 0.0006854583625681698, 0.0006854583625681698, 0.0006854583625681698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006854583625681698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29200363
Iteration 2/25 | Loss: 0.00022725
Iteration 3/25 | Loss: 0.00022725
Iteration 4/25 | Loss: 0.00022725
Iteration 5/25 | Loss: 0.00022725
Iteration 6/25 | Loss: 0.00022725
Iteration 7/25 | Loss: 0.00022725
Iteration 8/25 | Loss: 0.00022725
Iteration 9/25 | Loss: 0.00022725
Iteration 10/25 | Loss: 0.00022725
Iteration 11/25 | Loss: 0.00022725
Iteration 12/25 | Loss: 0.00022725
Iteration 13/25 | Loss: 0.00022725
Iteration 14/25 | Loss: 0.00022725
Iteration 15/25 | Loss: 0.00022725
Iteration 16/25 | Loss: 0.00022725
Iteration 17/25 | Loss: 0.00022725
Iteration 18/25 | Loss: 0.00022725
Iteration 19/25 | Loss: 0.00022725
Iteration 20/25 | Loss: 0.00022725
Iteration 21/25 | Loss: 0.00022725
Iteration 22/25 | Loss: 0.00022725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00022724943119101226, 0.00022724943119101226, 0.00022724943119101226, 0.00022724943119101226, 0.00022724943119101226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022724943119101226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022725
Iteration 2/1000 | Loss: 0.00003811
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002265
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002124
Iteration 7/1000 | Loss: 0.00002067
Iteration 8/1000 | Loss: 0.00002021
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001941
Iteration 22/1000 | Loss: 0.00001941
Iteration 23/1000 | Loss: 0.00001939
Iteration 24/1000 | Loss: 0.00001939
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001930
Iteration 30/1000 | Loss: 0.00001929
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001904
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001902
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001901
Iteration 67/1000 | Loss: 0.00001901
Iteration 68/1000 | Loss: 0.00001901
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001900
Iteration 72/1000 | Loss: 0.00001900
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001888
Iteration 112/1000 | Loss: 0.00001888
Iteration 113/1000 | Loss: 0.00001888
Iteration 114/1000 | Loss: 0.00001887
Iteration 115/1000 | Loss: 0.00001887
Iteration 116/1000 | Loss: 0.00001887
Iteration 117/1000 | Loss: 0.00001887
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001885
Iteration 131/1000 | Loss: 0.00001885
Iteration 132/1000 | Loss: 0.00001885
Iteration 133/1000 | Loss: 0.00001884
Iteration 134/1000 | Loss: 0.00001884
Iteration 135/1000 | Loss: 0.00001884
Iteration 136/1000 | Loss: 0.00001884
Iteration 137/1000 | Loss: 0.00001884
Iteration 138/1000 | Loss: 0.00001884
Iteration 139/1000 | Loss: 0.00001884
Iteration 140/1000 | Loss: 0.00001884
Iteration 141/1000 | Loss: 0.00001884
Iteration 142/1000 | Loss: 0.00001884
Iteration 143/1000 | Loss: 0.00001884
Iteration 144/1000 | Loss: 0.00001884
Iteration 145/1000 | Loss: 0.00001884
Iteration 146/1000 | Loss: 0.00001884
Iteration 147/1000 | Loss: 0.00001884
Iteration 148/1000 | Loss: 0.00001884
Iteration 149/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.8837276002159342e-05, 1.8837276002159342e-05, 1.8837276002159342e-05, 1.8837276002159342e-05, 1.8837276002159342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8837276002159342e-05

Optimization complete. Final v2v error: 3.7676291465759277 mm

Highest mean error: 4.461325645446777 mm for frame 94

Lowest mean error: 3.227710008621216 mm for frame 10

Saving results

Total time: 44.99343943595886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873937
Iteration 2/25 | Loss: 0.00116719
Iteration 3/25 | Loss: 0.00074069
Iteration 4/25 | Loss: 0.00065273
Iteration 5/25 | Loss: 0.00063231
Iteration 6/25 | Loss: 0.00062648
Iteration 7/25 | Loss: 0.00062434
Iteration 8/25 | Loss: 0.00062401
Iteration 9/25 | Loss: 0.00062401
Iteration 10/25 | Loss: 0.00062401
Iteration 11/25 | Loss: 0.00062401
Iteration 12/25 | Loss: 0.00062401
Iteration 13/25 | Loss: 0.00062401
Iteration 14/25 | Loss: 0.00062401
Iteration 15/25 | Loss: 0.00062401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006240098737180233, 0.0006240098737180233, 0.0006240098737180233, 0.0006240098737180233, 0.0006240098737180233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006240098737180233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29911780
Iteration 2/25 | Loss: 0.00026562
Iteration 3/25 | Loss: 0.00026562
Iteration 4/25 | Loss: 0.00026562
Iteration 5/25 | Loss: 0.00026562
Iteration 6/25 | Loss: 0.00026562
Iteration 7/25 | Loss: 0.00026562
Iteration 8/25 | Loss: 0.00026562
Iteration 9/25 | Loss: 0.00026562
Iteration 10/25 | Loss: 0.00026562
Iteration 11/25 | Loss: 0.00026562
Iteration 12/25 | Loss: 0.00026562
Iteration 13/25 | Loss: 0.00026562
Iteration 14/25 | Loss: 0.00026562
Iteration 15/25 | Loss: 0.00026562
Iteration 16/25 | Loss: 0.00026562
Iteration 17/25 | Loss: 0.00026562
Iteration 18/25 | Loss: 0.00026562
Iteration 19/25 | Loss: 0.00026562
Iteration 20/25 | Loss: 0.00026562
Iteration 21/25 | Loss: 0.00026562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00026561509002931416, 0.00026561509002931416, 0.00026561509002931416, 0.00026561509002931416, 0.00026561509002931416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026561509002931416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026562
Iteration 2/1000 | Loss: 0.00002642
Iteration 3/1000 | Loss: 0.00001939
Iteration 4/1000 | Loss: 0.00001644
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001446
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001352
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001350
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00001348
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001344
Iteration 85/1000 | Loss: 0.00001344
Iteration 86/1000 | Loss: 0.00001344
Iteration 87/1000 | Loss: 0.00001344
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001343
Iteration 94/1000 | Loss: 0.00001343
Iteration 95/1000 | Loss: 0.00001343
Iteration 96/1000 | Loss: 0.00001343
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001338
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001337
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001335
Iteration 157/1000 | Loss: 0.00001335
Iteration 158/1000 | Loss: 0.00001335
Iteration 159/1000 | Loss: 0.00001335
Iteration 160/1000 | Loss: 0.00001335
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001334
Iteration 163/1000 | Loss: 0.00001334
Iteration 164/1000 | Loss: 0.00001334
Iteration 165/1000 | Loss: 0.00001334
Iteration 166/1000 | Loss: 0.00001334
Iteration 167/1000 | Loss: 0.00001334
Iteration 168/1000 | Loss: 0.00001334
Iteration 169/1000 | Loss: 0.00001333
Iteration 170/1000 | Loss: 0.00001333
Iteration 171/1000 | Loss: 0.00001333
Iteration 172/1000 | Loss: 0.00001333
Iteration 173/1000 | Loss: 0.00001333
Iteration 174/1000 | Loss: 0.00001333
Iteration 175/1000 | Loss: 0.00001333
Iteration 176/1000 | Loss: 0.00001333
Iteration 177/1000 | Loss: 0.00001333
Iteration 178/1000 | Loss: 0.00001333
Iteration 179/1000 | Loss: 0.00001333
Iteration 180/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3331253285286948e-05, 1.3331253285286948e-05, 1.3331253285286948e-05, 1.3331253285286948e-05, 1.3331253285286948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3331253285286948e-05

Optimization complete. Final v2v error: 3.1233668327331543 mm

Highest mean error: 3.4790141582489014 mm for frame 55

Lowest mean error: 2.824645519256592 mm for frame 10

Saving results

Total time: 44.07804274559021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464521
Iteration 2/25 | Loss: 0.00124007
Iteration 3/25 | Loss: 0.00089945
Iteration 4/25 | Loss: 0.00078890
Iteration 5/25 | Loss: 0.00074386
Iteration 6/25 | Loss: 0.00072616
Iteration 7/25 | Loss: 0.00071449
Iteration 8/25 | Loss: 0.00071055
Iteration 9/25 | Loss: 0.00071132
Iteration 10/25 | Loss: 0.00071387
Iteration 11/25 | Loss: 0.00071099
Iteration 12/25 | Loss: 0.00071174
Iteration 13/25 | Loss: 0.00071199
Iteration 14/25 | Loss: 0.00071200
Iteration 15/25 | Loss: 0.00070886
Iteration 16/25 | Loss: 0.00070836
Iteration 17/25 | Loss: 0.00071411
Iteration 18/25 | Loss: 0.00070987
Iteration 19/25 | Loss: 0.00071393
Iteration 20/25 | Loss: 0.00071334
Iteration 21/25 | Loss: 0.00070832
Iteration 22/25 | Loss: 0.00070601
Iteration 23/25 | Loss: 0.00070509
Iteration 24/25 | Loss: 0.00070954
Iteration 25/25 | Loss: 0.00070627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29940867
Iteration 2/25 | Loss: 0.00079822
Iteration 3/25 | Loss: 0.00079822
Iteration 4/25 | Loss: 0.00079822
Iteration 5/25 | Loss: 0.00079822
Iteration 6/25 | Loss: 0.00079822
Iteration 7/25 | Loss: 0.00079822
Iteration 8/25 | Loss: 0.00079822
Iteration 9/25 | Loss: 0.00079822
Iteration 10/25 | Loss: 0.00079822
Iteration 11/25 | Loss: 0.00079822
Iteration 12/25 | Loss: 0.00079822
Iteration 13/25 | Loss: 0.00079822
Iteration 14/25 | Loss: 0.00079822
Iteration 15/25 | Loss: 0.00079822
Iteration 16/25 | Loss: 0.00079822
Iteration 17/25 | Loss: 0.00079822
Iteration 18/25 | Loss: 0.00079822
Iteration 19/25 | Loss: 0.00079822
Iteration 20/25 | Loss: 0.00079822
Iteration 21/25 | Loss: 0.00079822
Iteration 22/25 | Loss: 0.00079822
Iteration 23/25 | Loss: 0.00079822
Iteration 24/25 | Loss: 0.00079822
Iteration 25/25 | Loss: 0.00079822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079822
Iteration 2/1000 | Loss: 0.00015466
Iteration 3/1000 | Loss: 0.00015425
Iteration 4/1000 | Loss: 0.00007823
Iteration 5/1000 | Loss: 0.00006145
Iteration 6/1000 | Loss: 0.00004684
Iteration 7/1000 | Loss: 0.00004283
Iteration 8/1000 | Loss: 0.00004024
Iteration 9/1000 | Loss: 0.00003726
Iteration 10/1000 | Loss: 0.00004387
Iteration 11/1000 | Loss: 0.00004992
Iteration 12/1000 | Loss: 0.00004216
Iteration 13/1000 | Loss: 0.00003394
Iteration 14/1000 | Loss: 0.00004750
Iteration 15/1000 | Loss: 0.00006339
Iteration 16/1000 | Loss: 0.00006007
Iteration 17/1000 | Loss: 0.00005404
Iteration 18/1000 | Loss: 0.00003574
Iteration 19/1000 | Loss: 0.00005792
Iteration 20/1000 | Loss: 0.00007714
Iteration 21/1000 | Loss: 0.00004865
Iteration 22/1000 | Loss: 0.00005131
Iteration 23/1000 | Loss: 0.00005934
Iteration 24/1000 | Loss: 0.00006018
Iteration 25/1000 | Loss: 0.00004958
Iteration 26/1000 | Loss: 0.00005787
Iteration 27/1000 | Loss: 0.00005510
Iteration 28/1000 | Loss: 0.00006685
Iteration 29/1000 | Loss: 0.00006223
Iteration 30/1000 | Loss: 0.00006429
Iteration 31/1000 | Loss: 0.00005822
Iteration 32/1000 | Loss: 0.00005513
Iteration 33/1000 | Loss: 0.00005648
Iteration 34/1000 | Loss: 0.00005526
Iteration 35/1000 | Loss: 0.00005764
Iteration 36/1000 | Loss: 0.00004773
Iteration 37/1000 | Loss: 0.00006303
Iteration 38/1000 | Loss: 0.00006137
Iteration 39/1000 | Loss: 0.00006305
Iteration 40/1000 | Loss: 0.00006973
Iteration 41/1000 | Loss: 0.00006143
Iteration 42/1000 | Loss: 0.00006666
Iteration 43/1000 | Loss: 0.00006015
Iteration 44/1000 | Loss: 0.00006468
Iteration 45/1000 | Loss: 0.00005718
Iteration 46/1000 | Loss: 0.00006833
Iteration 47/1000 | Loss: 0.00005693
Iteration 48/1000 | Loss: 0.00006849
Iteration 49/1000 | Loss: 0.00005287
Iteration 50/1000 | Loss: 0.00007494
Iteration 51/1000 | Loss: 0.00005327
Iteration 52/1000 | Loss: 0.00007110
Iteration 53/1000 | Loss: 0.00005145
Iteration 54/1000 | Loss: 0.00005460
Iteration 55/1000 | Loss: 0.00005609
Iteration 56/1000 | Loss: 0.00005661
Iteration 57/1000 | Loss: 0.00005712
Iteration 58/1000 | Loss: 0.00006388
Iteration 59/1000 | Loss: 0.00006148
Iteration 60/1000 | Loss: 0.00005747
Iteration 61/1000 | Loss: 0.00007648
Iteration 62/1000 | Loss: 0.00004531
Iteration 63/1000 | Loss: 0.00003835
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003054
Iteration 66/1000 | Loss: 0.00003022
Iteration 67/1000 | Loss: 0.00003045
Iteration 68/1000 | Loss: 0.00003002
Iteration 69/1000 | Loss: 0.00003178
Iteration 70/1000 | Loss: 0.00003092
Iteration 71/1000 | Loss: 0.00003038
Iteration 72/1000 | Loss: 0.00003041
Iteration 73/1000 | Loss: 0.00002890
Iteration 74/1000 | Loss: 0.00003216
Iteration 75/1000 | Loss: 0.00003011
Iteration 76/1000 | Loss: 0.00005032
Iteration 77/1000 | Loss: 0.00004552
Iteration 78/1000 | Loss: 0.00003794
Iteration 79/1000 | Loss: 0.00002979
Iteration 80/1000 | Loss: 0.00003124
Iteration 81/1000 | Loss: 0.00003439
Iteration 82/1000 | Loss: 0.00003015
Iteration 83/1000 | Loss: 0.00002921
Iteration 84/1000 | Loss: 0.00003028
Iteration 85/1000 | Loss: 0.00002950
Iteration 86/1000 | Loss: 0.00002926
Iteration 87/1000 | Loss: 0.00003991
Iteration 88/1000 | Loss: 0.00003410
Iteration 89/1000 | Loss: 0.00002983
Iteration 90/1000 | Loss: 0.00002909
Iteration 91/1000 | Loss: 0.00003025
Iteration 92/1000 | Loss: 0.00002993
Iteration 93/1000 | Loss: 0.00002935
Iteration 94/1000 | Loss: 0.00003051
Iteration 95/1000 | Loss: 0.00003042
Iteration 96/1000 | Loss: 0.00002977
Iteration 97/1000 | Loss: 0.00002856
Iteration 98/1000 | Loss: 0.00002841
Iteration 99/1000 | Loss: 0.00002934
Iteration 100/1000 | Loss: 0.00003023
Iteration 101/1000 | Loss: 0.00003047
Iteration 102/1000 | Loss: 0.00003084
Iteration 103/1000 | Loss: 0.00003447
Iteration 104/1000 | Loss: 0.00003269
Iteration 105/1000 | Loss: 0.00003362
Iteration 106/1000 | Loss: 0.00003498
Iteration 107/1000 | Loss: 0.00003625
Iteration 108/1000 | Loss: 0.00003598
Iteration 109/1000 | Loss: 0.00003767
Iteration 110/1000 | Loss: 0.00003809
Iteration 111/1000 | Loss: 0.00004223
Iteration 112/1000 | Loss: 0.00004356
Iteration 113/1000 | Loss: 0.00004758
Iteration 114/1000 | Loss: 0.00004529
Iteration 115/1000 | Loss: 0.00004954
Iteration 116/1000 | Loss: 0.00004744
Iteration 117/1000 | Loss: 0.00005177
Iteration 118/1000 | Loss: 0.00004318
Iteration 119/1000 | Loss: 0.00005040
Iteration 120/1000 | Loss: 0.00004180
Iteration 121/1000 | Loss: 0.00005682
Iteration 122/1000 | Loss: 0.00004061
Iteration 123/1000 | Loss: 0.00004802
Iteration 124/1000 | Loss: 0.00004112
Iteration 125/1000 | Loss: 0.00004697
Iteration 126/1000 | Loss: 0.00004116
Iteration 127/1000 | Loss: 0.00004377
Iteration 128/1000 | Loss: 0.00004110
Iteration 129/1000 | Loss: 0.00005095
Iteration 130/1000 | Loss: 0.00004474
Iteration 131/1000 | Loss: 0.00003586
Iteration 132/1000 | Loss: 0.00003374
Iteration 133/1000 | Loss: 0.00003643
Iteration 134/1000 | Loss: 0.00004369
Iteration 135/1000 | Loss: 0.00004261
Iteration 136/1000 | Loss: 0.00004681
Iteration 137/1000 | Loss: 0.00005284
Iteration 138/1000 | Loss: 0.00005054
Iteration 139/1000 | Loss: 0.00005520
Iteration 140/1000 | Loss: 0.00006109
Iteration 141/1000 | Loss: 0.00005402
Iteration 142/1000 | Loss: 0.00005007
Iteration 143/1000 | Loss: 0.00004495
Iteration 144/1000 | Loss: 0.00005289
Iteration 145/1000 | Loss: 0.00005717
Iteration 146/1000 | Loss: 0.00005937
Iteration 147/1000 | Loss: 0.00006295
Iteration 148/1000 | Loss: 0.00005873
Iteration 149/1000 | Loss: 0.00005954
Iteration 150/1000 | Loss: 0.00005971
Iteration 151/1000 | Loss: 0.00003959
Iteration 152/1000 | Loss: 0.00003041
Iteration 153/1000 | Loss: 0.00003381
Iteration 154/1000 | Loss: 0.00003038
Iteration 155/1000 | Loss: 0.00005542
Iteration 156/1000 | Loss: 0.00004393
Iteration 157/1000 | Loss: 0.00006443
Iteration 158/1000 | Loss: 0.00004529
Iteration 159/1000 | Loss: 0.00005413
Iteration 160/1000 | Loss: 0.00005061
Iteration 161/1000 | Loss: 0.00004759
Iteration 162/1000 | Loss: 0.00003774
Iteration 163/1000 | Loss: 0.00003671
Iteration 164/1000 | Loss: 0.00002942
Iteration 165/1000 | Loss: 0.00005649
Iteration 166/1000 | Loss: 0.00004985
Iteration 167/1000 | Loss: 0.00006263
Iteration 168/1000 | Loss: 0.00005301
Iteration 169/1000 | Loss: 0.00006653
Iteration 170/1000 | Loss: 0.00005672
Iteration 171/1000 | Loss: 0.00006646
Iteration 172/1000 | Loss: 0.00005522
Iteration 173/1000 | Loss: 0.00006926
Iteration 174/1000 | Loss: 0.00004552
Iteration 175/1000 | Loss: 0.00003384
Iteration 176/1000 | Loss: 0.00003301
Iteration 177/1000 | Loss: 0.00002788
Iteration 178/1000 | Loss: 0.00003301
Iteration 179/1000 | Loss: 0.00006333
Iteration 180/1000 | Loss: 0.00004422
Iteration 181/1000 | Loss: 0.00005512
Iteration 182/1000 | Loss: 0.00003402
Iteration 183/1000 | Loss: 0.00002994
Iteration 184/1000 | Loss: 0.00002924
Iteration 185/1000 | Loss: 0.00002748
Iteration 186/1000 | Loss: 0.00005070
Iteration 187/1000 | Loss: 0.00004748
Iteration 188/1000 | Loss: 0.00006271
Iteration 189/1000 | Loss: 0.00003993
Iteration 190/1000 | Loss: 0.00006193
Iteration 191/1000 | Loss: 0.00005094
Iteration 192/1000 | Loss: 0.00007359
Iteration 193/1000 | Loss: 0.00004901
Iteration 194/1000 | Loss: 0.00004613
Iteration 195/1000 | Loss: 0.00005179
Iteration 196/1000 | Loss: 0.00003344
Iteration 197/1000 | Loss: 0.00003037
Iteration 198/1000 | Loss: 0.00005219
Iteration 199/1000 | Loss: 0.00003450
Iteration 200/1000 | Loss: 0.00003182
Iteration 201/1000 | Loss: 0.00003911
Iteration 202/1000 | Loss: 0.00003252
Iteration 203/1000 | Loss: 0.00002947
Iteration 204/1000 | Loss: 0.00004835
Iteration 205/1000 | Loss: 0.00003439
Iteration 206/1000 | Loss: 0.00002956
Iteration 207/1000 | Loss: 0.00003281
Iteration 208/1000 | Loss: 0.00005433
Iteration 209/1000 | Loss: 0.00004205
Iteration 210/1000 | Loss: 0.00003317
Iteration 211/1000 | Loss: 0.00003476
Iteration 212/1000 | Loss: 0.00005213
Iteration 213/1000 | Loss: 0.00004394
Iteration 214/1000 | Loss: 0.00005968
Iteration 215/1000 | Loss: 0.00004589
Iteration 216/1000 | Loss: 0.00005400
Iteration 217/1000 | Loss: 0.00003728
Iteration 218/1000 | Loss: 0.00005476
Iteration 219/1000 | Loss: 0.00003271
Iteration 220/1000 | Loss: 0.00002635
Iteration 221/1000 | Loss: 0.00003112
Iteration 222/1000 | Loss: 0.00004884
Iteration 223/1000 | Loss: 0.00002940
Iteration 224/1000 | Loss: 0.00003133
Iteration 225/1000 | Loss: 0.00003153
Iteration 226/1000 | Loss: 0.00003022
Iteration 227/1000 | Loss: 0.00002766
Iteration 228/1000 | Loss: 0.00002951
Iteration 229/1000 | Loss: 0.00002614
Iteration 230/1000 | Loss: 0.00002905
Iteration 231/1000 | Loss: 0.00004020
Iteration 232/1000 | Loss: 0.00003586
Iteration 233/1000 | Loss: 0.00003504
Iteration 234/1000 | Loss: 0.00003249
Iteration 235/1000 | Loss: 0.00003255
Iteration 236/1000 | Loss: 0.00002864
Iteration 237/1000 | Loss: 0.00002939
Iteration 238/1000 | Loss: 0.00002919
Iteration 239/1000 | Loss: 0.00002896
Iteration 240/1000 | Loss: 0.00003351
Iteration 241/1000 | Loss: 0.00004303
Iteration 242/1000 | Loss: 0.00003686
Iteration 243/1000 | Loss: 0.00004341
Iteration 244/1000 | Loss: 0.00003945
Iteration 245/1000 | Loss: 0.00003520
Iteration 246/1000 | Loss: 0.00003515
Iteration 247/1000 | Loss: 0.00003700
Iteration 248/1000 | Loss: 0.00004352
Iteration 249/1000 | Loss: 0.00004397
Iteration 250/1000 | Loss: 0.00003627
Iteration 251/1000 | Loss: 0.00003473
Iteration 252/1000 | Loss: 0.00003574
Iteration 253/1000 | Loss: 0.00003316
Iteration 254/1000 | Loss: 0.00003808
Iteration 255/1000 | Loss: 0.00003502
Iteration 256/1000 | Loss: 0.00003132
Iteration 257/1000 | Loss: 0.00003511
Iteration 258/1000 | Loss: 0.00003641
Iteration 259/1000 | Loss: 0.00004038
Iteration 260/1000 | Loss: 0.00004713
Iteration 261/1000 | Loss: 0.00004774
Iteration 262/1000 | Loss: 0.00003633
Iteration 263/1000 | Loss: 0.00003154
Iteration 264/1000 | Loss: 0.00003751
Iteration 265/1000 | Loss: 0.00004681
Iteration 266/1000 | Loss: 0.00004641
Iteration 267/1000 | Loss: 0.00003998
Iteration 268/1000 | Loss: 0.00003133
Iteration 269/1000 | Loss: 0.00004525
Iteration 270/1000 | Loss: 0.00003230
Iteration 271/1000 | Loss: 0.00002671
Iteration 272/1000 | Loss: 0.00003243
Iteration 273/1000 | Loss: 0.00004790
Iteration 274/1000 | Loss: 0.00004112
Iteration 275/1000 | Loss: 0.00003472
Iteration 276/1000 | Loss: 0.00003223
Iteration 277/1000 | Loss: 0.00004668
Iteration 278/1000 | Loss: 0.00004403
Iteration 279/1000 | Loss: 0.00005085
Iteration 280/1000 | Loss: 0.00004890
Iteration 281/1000 | Loss: 0.00005356
Iteration 282/1000 | Loss: 0.00005296
Iteration 283/1000 | Loss: 0.00005821
Iteration 284/1000 | Loss: 0.00004044
Iteration 285/1000 | Loss: 0.00004743
Iteration 286/1000 | Loss: 0.00003306
Iteration 287/1000 | Loss: 0.00002842
Iteration 288/1000 | Loss: 0.00003822
Iteration 289/1000 | Loss: 0.00004178
Iteration 290/1000 | Loss: 0.00003954
Iteration 291/1000 | Loss: 0.00003711
Iteration 292/1000 | Loss: 0.00002762
Iteration 293/1000 | Loss: 0.00002704
Iteration 294/1000 | Loss: 0.00003639
Iteration 295/1000 | Loss: 0.00003407
Iteration 296/1000 | Loss: 0.00003225
Iteration 297/1000 | Loss: 0.00003437
Iteration 298/1000 | Loss: 0.00004391
Iteration 299/1000 | Loss: 0.00004469
Iteration 300/1000 | Loss: 0.00004575
Iteration 301/1000 | Loss: 0.00005047
Iteration 302/1000 | Loss: 0.00003406
Iteration 303/1000 | Loss: 0.00002929
Iteration 304/1000 | Loss: 0.00004041
Iteration 305/1000 | Loss: 0.00004612
Iteration 306/1000 | Loss: 0.00004013
Iteration 307/1000 | Loss: 0.00004657
Iteration 308/1000 | Loss: 0.00003365
Iteration 309/1000 | Loss: 0.00004174
Iteration 310/1000 | Loss: 0.00004067
Iteration 311/1000 | Loss: 0.00003926
Iteration 312/1000 | Loss: 0.00004109
Iteration 313/1000 | Loss: 0.00003929
Iteration 314/1000 | Loss: 0.00003339
Iteration 315/1000 | Loss: 0.00002975
Iteration 316/1000 | Loss: 0.00002647
Iteration 317/1000 | Loss: 0.00002573
Iteration 318/1000 | Loss: 0.00002885
Iteration 319/1000 | Loss: 0.00003512
Iteration 320/1000 | Loss: 0.00003795
Iteration 321/1000 | Loss: 0.00003620
Iteration 322/1000 | Loss: 0.00003849
Iteration 323/1000 | Loss: 0.00003847
Iteration 324/1000 | Loss: 0.00004273
Iteration 325/1000 | Loss: 0.00004559
Iteration 326/1000 | Loss: 0.00003161
Iteration 327/1000 | Loss: 0.00003055
Iteration 328/1000 | Loss: 0.00003616
Iteration 329/1000 | Loss: 0.00004410
Iteration 330/1000 | Loss: 0.00004744
Iteration 331/1000 | Loss: 0.00004129
Iteration 332/1000 | Loss: 0.00004053
Iteration 333/1000 | Loss: 0.00004710
Iteration 334/1000 | Loss: 0.00004846
Iteration 335/1000 | Loss: 0.00005221
Iteration 336/1000 | Loss: 0.00004602
Iteration 337/1000 | Loss: 0.00005097
Iteration 338/1000 | Loss: 0.00004951
Iteration 339/1000 | Loss: 0.00005582
Iteration 340/1000 | Loss: 0.00005121
Iteration 341/1000 | Loss: 0.00004519
Iteration 342/1000 | Loss: 0.00004543
Iteration 343/1000 | Loss: 0.00005297
Iteration 344/1000 | Loss: 0.00005542
Iteration 345/1000 | Loss: 0.00005400
Iteration 346/1000 | Loss: 0.00004534
Iteration 347/1000 | Loss: 0.00005442
Iteration 348/1000 | Loss: 0.00004832
Iteration 349/1000 | Loss: 0.00005147
Iteration 350/1000 | Loss: 0.00004963
Iteration 351/1000 | Loss: 0.00005266
Iteration 352/1000 | Loss: 0.00005359
Iteration 353/1000 | Loss: 0.00004539
Iteration 354/1000 | Loss: 0.00004957
Iteration 355/1000 | Loss: 0.00004665
Iteration 356/1000 | Loss: 0.00005043
Iteration 357/1000 | Loss: 0.00003338
Iteration 358/1000 | Loss: 0.00004910
Iteration 359/1000 | Loss: 0.00004635
Iteration 360/1000 | Loss: 0.00004422
Iteration 361/1000 | Loss: 0.00004285
Iteration 362/1000 | Loss: 0.00004703
Iteration 363/1000 | Loss: 0.00003119
Iteration 364/1000 | Loss: 0.00002791
Iteration 365/1000 | Loss: 0.00002932
Iteration 366/1000 | Loss: 0.00003584
Iteration 367/1000 | Loss: 0.00003757
Iteration 368/1000 | Loss: 0.00004435
Iteration 369/1000 | Loss: 0.00003872
Iteration 370/1000 | Loss: 0.00005308
Iteration 371/1000 | Loss: 0.00004000
Iteration 372/1000 | Loss: 0.00004779
Iteration 373/1000 | Loss: 0.00004331
Iteration 374/1000 | Loss: 0.00003471
Iteration 375/1000 | Loss: 0.00003791
Iteration 376/1000 | Loss: 0.00004265
Iteration 377/1000 | Loss: 0.00004209
Iteration 378/1000 | Loss: 0.00004574
Iteration 379/1000 | Loss: 0.00003488
Iteration 380/1000 | Loss: 0.00003760
Iteration 381/1000 | Loss: 0.00003504
Iteration 382/1000 | Loss: 0.00003084
Iteration 383/1000 | Loss: 0.00003054
Iteration 384/1000 | Loss: 0.00003109
Iteration 385/1000 | Loss: 0.00003545
Iteration 386/1000 | Loss: 0.00002903
Iteration 387/1000 | Loss: 0.00003606
Iteration 388/1000 | Loss: 0.00003303
Iteration 389/1000 | Loss: 0.00003774
Iteration 390/1000 | Loss: 0.00003495
Iteration 391/1000 | Loss: 0.00003583
Iteration 392/1000 | Loss: 0.00003683
Iteration 393/1000 | Loss: 0.00004588
Iteration 394/1000 | Loss: 0.00004081
Iteration 395/1000 | Loss: 0.00005056
Iteration 396/1000 | Loss: 0.00004347
Iteration 397/1000 | Loss: 0.00005897
Iteration 398/1000 | Loss: 0.00003678
Iteration 399/1000 | Loss: 0.00004454
Iteration 400/1000 | Loss: 0.00003790
Iteration 401/1000 | Loss: 0.00005269
Iteration 402/1000 | Loss: 0.00003268
Iteration 403/1000 | Loss: 0.00002800
Iteration 404/1000 | Loss: 0.00002647
Iteration 405/1000 | Loss: 0.00002760
Iteration 406/1000 | Loss: 0.00002646
Iteration 407/1000 | Loss: 0.00002738
Iteration 408/1000 | Loss: 0.00002652
Iteration 409/1000 | Loss: 0.00003502
Iteration 410/1000 | Loss: 0.00003079
Iteration 411/1000 | Loss: 0.00003848
Iteration 412/1000 | Loss: 0.00003254
Iteration 413/1000 | Loss: 0.00004108
Iteration 414/1000 | Loss: 0.00003404
Iteration 415/1000 | Loss: 0.00004479
Iteration 416/1000 | Loss: 0.00004141
Iteration 417/1000 | Loss: 0.00004153
Iteration 418/1000 | Loss: 0.00002725
Iteration 419/1000 | Loss: 0.00003602
Iteration 420/1000 | Loss: 0.00004303
Iteration 421/1000 | Loss: 0.00004334
Iteration 422/1000 | Loss: 0.00003798
Iteration 423/1000 | Loss: 0.00002893
Iteration 424/1000 | Loss: 0.00003096
Iteration 425/1000 | Loss: 0.00004089
Iteration 426/1000 | Loss: 0.00003265
Iteration 427/1000 | Loss: 0.00004358
Iteration 428/1000 | Loss: 0.00003531
Iteration 429/1000 | Loss: 0.00004724
Iteration 430/1000 | Loss: 0.00004560
Iteration 431/1000 | Loss: 0.00003630
Iteration 432/1000 | Loss: 0.00002820
Iteration 433/1000 | Loss: 0.00003643
Iteration 434/1000 | Loss: 0.00003620
Iteration 435/1000 | Loss: 0.00004409
Iteration 436/1000 | Loss: 0.00003692
Iteration 437/1000 | Loss: 0.00004420
Iteration 438/1000 | Loss: 0.00005406
Iteration 439/1000 | Loss: 0.00004852
Iteration 440/1000 | Loss: 0.00004042
Iteration 441/1000 | Loss: 0.00005253
Iteration 442/1000 | Loss: 0.00005164
Iteration 443/1000 | Loss: 0.00004947
Iteration 444/1000 | Loss: 0.00004286
Iteration 445/1000 | Loss: 0.00004557
Iteration 446/1000 | Loss: 0.00004342
Iteration 447/1000 | Loss: 0.00005387
Iteration 448/1000 | Loss: 0.00004469
Iteration 449/1000 | Loss: 0.00003374
Iteration 450/1000 | Loss: 0.00003083
Iteration 451/1000 | Loss: 0.00003523
Iteration 452/1000 | Loss: 0.00003716
Iteration 453/1000 | Loss: 0.00003638
Iteration 454/1000 | Loss: 0.00004176
Iteration 455/1000 | Loss: 0.00004025
Iteration 456/1000 | Loss: 0.00004435
Iteration 457/1000 | Loss: 0.00004120
Iteration 458/1000 | Loss: 0.00004156
Iteration 459/1000 | Loss: 0.00004019
Iteration 460/1000 | Loss: 0.00004478
Iteration 461/1000 | Loss: 0.00003528
Iteration 462/1000 | Loss: 0.00004201
Iteration 463/1000 | Loss: 0.00002704
Iteration 464/1000 | Loss: 0.00003497
Iteration 465/1000 | Loss: 0.00004680
Iteration 466/1000 | Loss: 0.00004132
Iteration 467/1000 | Loss: 0.00004815
Iteration 468/1000 | Loss: 0.00003060
Iteration 469/1000 | Loss: 0.00003898
Iteration 470/1000 | Loss: 0.00003755
Iteration 471/1000 | Loss: 0.00005078
Iteration 472/1000 | Loss: 0.00003606
Iteration 473/1000 | Loss: 0.00004239
Iteration 474/1000 | Loss: 0.00004627
Iteration 475/1000 | Loss: 0.00005293
Iteration 476/1000 | Loss: 0.00005139
Iteration 477/1000 | Loss: 0.00005582
Iteration 478/1000 | Loss: 0.00003550
Iteration 479/1000 | Loss: 0.00004059
Iteration 480/1000 | Loss: 0.00005114
Iteration 481/1000 | Loss: 0.00003864
Iteration 482/1000 | Loss: 0.00003779
Iteration 483/1000 | Loss: 0.00004253
Iteration 484/1000 | Loss: 0.00004752
Iteration 485/1000 | Loss: 0.00005143
Iteration 486/1000 | Loss: 0.00004594
Iteration 487/1000 | Loss: 0.00005084
Iteration 488/1000 | Loss: 0.00005097
Iteration 489/1000 | Loss: 0.00004267
Iteration 490/1000 | Loss: 0.00002866
Iteration 491/1000 | Loss: 0.00003818
Iteration 492/1000 | Loss: 0.00005140
Iteration 493/1000 | Loss: 0.00004556
Iteration 494/1000 | Loss: 0.00005291
Iteration 495/1000 | Loss: 0.00004248
Iteration 496/1000 | Loss: 0.00005394
Iteration 497/1000 | Loss: 0.00004925
Iteration 498/1000 | Loss: 0.00003368
Iteration 499/1000 | Loss: 0.00005077
Iteration 500/1000 | Loss: 0.00006440
Iteration 501/1000 | Loss: 0.00004630
Iteration 502/1000 | Loss: 0.00004249
Iteration 503/1000 | Loss: 0.00005673
Iteration 504/1000 | Loss: 0.00004122
Iteration 505/1000 | Loss: 0.00004009
Iteration 506/1000 | Loss: 0.00003111
Iteration 507/1000 | Loss: 0.00004217
Iteration 508/1000 | Loss: 0.00004052
Iteration 509/1000 | Loss: 0.00004232
Iteration 510/1000 | Loss: 0.00004947
Iteration 511/1000 | Loss: 0.00004408
Iteration 512/1000 | Loss: 0.00006073
Iteration 513/1000 | Loss: 0.00004262
Iteration 514/1000 | Loss: 0.00004954
Iteration 515/1000 | Loss: 0.00004104
Iteration 516/1000 | Loss: 0.00004802
Iteration 517/1000 | Loss: 0.00004514
Iteration 518/1000 | Loss: 0.00004537
Iteration 519/1000 | Loss: 0.00003624
Iteration 520/1000 | Loss: 0.00004142
Iteration 521/1000 | Loss: 0.00003553
Iteration 522/1000 | Loss: 0.00004573
Iteration 523/1000 | Loss: 0.00003326
Iteration 524/1000 | Loss: 0.00003545
Iteration 525/1000 | Loss: 0.00003846
Iteration 526/1000 | Loss: 0.00005270
Iteration 527/1000 | Loss: 0.00003790
Iteration 528/1000 | Loss: 0.00004709
Iteration 529/1000 | Loss: 0.00003509
Iteration 530/1000 | Loss: 0.00003071
Iteration 531/1000 | Loss: 0.00002681
Iteration 532/1000 | Loss: 0.00004266
Iteration 533/1000 | Loss: 0.00003368
Iteration 534/1000 | Loss: 0.00003395
Iteration 535/1000 | Loss: 0.00003158
Iteration 536/1000 | Loss: 0.00004489
Iteration 537/1000 | Loss: 0.00003627
Iteration 538/1000 | Loss: 0.00004979
Iteration 539/1000 | Loss: 0.00003721
Iteration 540/1000 | Loss: 0.00005243
Iteration 541/1000 | Loss: 0.00003679
Iteration 542/1000 | Loss: 0.00004129
Iteration 543/1000 | Loss: 0.00003197
Iteration 544/1000 | Loss: 0.00003101
Iteration 545/1000 | Loss: 0.00003178
Iteration 546/1000 | Loss: 0.00004593
Iteration 547/1000 | Loss: 0.00003483
Iteration 548/1000 | Loss: 0.00004916
Iteration 549/1000 | Loss: 0.00003604
Iteration 550/1000 | Loss: 0.00005945
Iteration 551/1000 | Loss: 0.00004116
Iteration 552/1000 | Loss: 0.00005022
Iteration 553/1000 | Loss: 0.00003274
Iteration 554/1000 | Loss: 0.00004569
Iteration 555/1000 | Loss: 0.00003990
Iteration 556/1000 | Loss: 0.00005347
Iteration 557/1000 | Loss: 0.00004295
Iteration 558/1000 | Loss: 0.00005891
Iteration 559/1000 | Loss: 0.00004143
Iteration 560/1000 | Loss: 0.00006024
Iteration 561/1000 | Loss: 0.00004405
Iteration 562/1000 | Loss: 0.00006374
Iteration 563/1000 | Loss: 0.00003912
Iteration 564/1000 | Loss: 0.00004995
Iteration 565/1000 | Loss: 0.00002983
Iteration 566/1000 | Loss: 0.00002688
Iteration 567/1000 | Loss: 0.00003846
Iteration 568/1000 | Loss: 0.00003806
Iteration 569/1000 | Loss: 0.00003608
Iteration 570/1000 | Loss: 0.00003866
Iteration 571/1000 | Loss: 0.00003974
Iteration 572/1000 | Loss: 0.00003676
Iteration 573/1000 | Loss: 0.00004272
Iteration 574/1000 | Loss: 0.00004084
Iteration 575/1000 | Loss: 0.00003832
Iteration 576/1000 | Loss: 0.00002993
Iteration 577/1000 | Loss: 0.00002681
Iteration 578/1000 | Loss: 0.00003399
Iteration 579/1000 | Loss: 0.00004052
Iteration 580/1000 | Loss: 0.00004256
Iteration 581/1000 | Loss: 0.00004574
Iteration 582/1000 | Loss: 0.00003952
Iteration 583/1000 | Loss: 0.00003349
Iteration 584/1000 | Loss: 0.00003066
Iteration 585/1000 | Loss: 0.00002828
Iteration 586/1000 | Loss: 0.00003103
Iteration 587/1000 | Loss: 0.00004139
Iteration 588/1000 | Loss: 0.00004168
Iteration 589/1000 | Loss: 0.00004071
Iteration 590/1000 | Loss: 0.00004012
Iteration 591/1000 | Loss: 0.00003672
Iteration 592/1000 | Loss: 0.00003121
Iteration 593/1000 | Loss: 0.00003214
Iteration 594/1000 | Loss: 0.00003667
Iteration 595/1000 | Loss: 0.00003527
Iteration 596/1000 | Loss: 0.00003779
Iteration 597/1000 | Loss: 0.00003697
Iteration 598/1000 | Loss: 0.00004295
Iteration 599/1000 | Loss: 0.00004581
Iteration 600/1000 | Loss: 0.00005382
Iteration 601/1000 | Loss: 0.00004181
Iteration 602/1000 | Loss: 0.00004769
Iteration 603/1000 | Loss: 0.00003888
Iteration 604/1000 | Loss: 0.00005356
Iteration 605/1000 | Loss: 0.00003681
Iteration 606/1000 | Loss: 0.00004106
Iteration 607/1000 | Loss: 0.00003814
Iteration 608/1000 | Loss: 0.00002943
Iteration 609/1000 | Loss: 0.00002699
Iteration 610/1000 | Loss: 0.00004028
Iteration 611/1000 | Loss: 0.00004284
Iteration 612/1000 | Loss: 0.00004563
Iteration 613/1000 | Loss: 0.00004202
Iteration 614/1000 | Loss: 0.00004926
Iteration 615/1000 | Loss: 0.00004526
Iteration 616/1000 | Loss: 0.00004562
Iteration 617/1000 | Loss: 0.00004306
Iteration 618/1000 | Loss: 0.00005607
Iteration 619/1000 | Loss: 0.00004625
Iteration 620/1000 | Loss: 0.00005636
Iteration 621/1000 | Loss: 0.00004618
Iteration 622/1000 | Loss: 0.00005956
Iteration 623/1000 | Loss: 0.00004559
Iteration 624/1000 | Loss: 0.00005783
Iteration 625/1000 | Loss: 0.00004065
Iteration 626/1000 | Loss: 0.00004903
Iteration 627/1000 | Loss: 0.00004949
Iteration 628/1000 | Loss: 0.00005706
Iteration 629/1000 | Loss: 0.00006414
Iteration 630/1000 | Loss: 0.00003583
Iteration 631/1000 | Loss: 0.00004498
Iteration 632/1000 | Loss: 0.00005349
Iteration 633/1000 | Loss: 0.00003854
Iteration 634/1000 | Loss: 0.00004951
Iteration 635/1000 | Loss: 0.00003745
Iteration 636/1000 | Loss: 0.00005010
Iteration 637/1000 | Loss: 0.00003698
Iteration 638/1000 | Loss: 0.00004715
Iteration 639/1000 | Loss: 0.00003157
Iteration 640/1000 | Loss: 0.00003512
Iteration 641/1000 | Loss: 0.00002826
Iteration 642/1000 | Loss: 0.00002743
Iteration 643/1000 | Loss: 0.00002728
Iteration 644/1000 | Loss: 0.00003557
Iteration 645/1000 | Loss: 0.00003483
Iteration 646/1000 | Loss: 0.00004400
Iteration 647/1000 | Loss: 0.00003986
Iteration 648/1000 | Loss: 0.00004207
Iteration 649/1000 | Loss: 0.00003219
Iteration 650/1000 | Loss: 0.00004370
Iteration 651/1000 | Loss: 0.00003328
Iteration 652/1000 | Loss: 0.00004289
Iteration 653/1000 | Loss: 0.00003147
Iteration 654/1000 | Loss: 0.00004208
Iteration 655/1000 | Loss: 0.00004358
Iteration 656/1000 | Loss: 0.00004982
Iteration 657/1000 | Loss: 0.00004354
Iteration 658/1000 | Loss: 0.00004002
Iteration 659/1000 | Loss: 0.00003448
Iteration 660/1000 | Loss: 0.00003817
Iteration 661/1000 | Loss: 0.00004005
Iteration 662/1000 | Loss: 0.00003737
Iteration 663/1000 | Loss: 0.00004971
Iteration 664/1000 | Loss: 0.00004251
Iteration 665/1000 | Loss: 0.00005594
Iteration 666/1000 | Loss: 0.00004329
Iteration 667/1000 | Loss: 0.00004625
Iteration 668/1000 | Loss: 0.00005076
Iteration 669/1000 | Loss: 0.00004476
Iteration 670/1000 | Loss: 0.00003951
Iteration 671/1000 | Loss: 0.00005155
Iteration 672/1000 | Loss: 0.00005352
Iteration 673/1000 | Loss: 0.00005040
Iteration 674/1000 | Loss: 0.00004351
Iteration 675/1000 | Loss: 0.00004869
Iteration 676/1000 | Loss: 0.00003672
Iteration 677/1000 | Loss: 0.00003353
Iteration 678/1000 | Loss: 0.00002919
Iteration 679/1000 | Loss: 0.00002989
Iteration 680/1000 | Loss: 0.00003905
Iteration 681/1000 | Loss: 0.00003254
Iteration 682/1000 | Loss: 0.00005187
Iteration 683/1000 | Loss: 0.00003369
Iteration 684/1000 | Loss: 0.00004022
Iteration 685/1000 | Loss: 0.00003611
Iteration 686/1000 | Loss: 0.00004247
Iteration 687/1000 | Loss: 0.00003329
Iteration 688/1000 | Loss: 0.00004821
Iteration 689/1000 | Loss: 0.00004000
Iteration 690/1000 | Loss: 0.00005210
Iteration 691/1000 | Loss: 0.00003959
Iteration 692/1000 | Loss: 0.00005525
Iteration 693/1000 | Loss: 0.00004005
Iteration 694/1000 | Loss: 0.00005473
Iteration 695/1000 | Loss: 0.00003596
Iteration 696/1000 | Loss: 0.00006088
Iteration 697/1000 | Loss: 0.00004014
Iteration 698/1000 | Loss: 0.00004894
Iteration 699/1000 | Loss: 0.00003757
Iteration 700/1000 | Loss: 0.00003349
Iteration 701/1000 | Loss: 0.00004001
Iteration 702/1000 | Loss: 0.00003887
Iteration 703/1000 | Loss: 0.00005638
Iteration 704/1000 | Loss: 0.00003487
Iteration 705/1000 | Loss: 0.00004702
Iteration 706/1000 | Loss: 0.00003489
Iteration 707/1000 | Loss: 0.00005271
Iteration 708/1000 | Loss: 0.00003298
Iteration 709/1000 | Loss: 0.00005644
Iteration 710/1000 | Loss: 0.00004348
Iteration 711/1000 | Loss: 0.00006193
Iteration 712/1000 | Loss: 0.00003877
Iteration 713/1000 | Loss: 0.00005992
Iteration 714/1000 | Loss: 0.00003906
Iteration 715/1000 | Loss: 0.00005183
Iteration 716/1000 | Loss: 0.00003373
Iteration 717/1000 | Loss: 0.00005932
Iteration 718/1000 | Loss: 0.00003727
Iteration 719/1000 | Loss: 0.00004875
Iteration 720/1000 | Loss: 0.00003339
Iteration 721/1000 | Loss: 0.00005011
Iteration 722/1000 | Loss: 0.00003185
Iteration 723/1000 | Loss: 0.00004918
Iteration 724/1000 | Loss: 0.00003070
Iteration 725/1000 | Loss: 0.00003890
Iteration 726/1000 | Loss: 0.00002964
Iteration 727/1000 | Loss: 0.00004979
Iteration 728/1000 | Loss: 0.00003101
Iteration 729/1000 | Loss: 0.00004215
Iteration 730/1000 | Loss: 0.00003354
Iteration 731/1000 | Loss: 0.00004933
Iteration 732/1000 | Loss: 0.00003622
Iteration 733/1000 | Loss: 0.00004976
Iteration 734/1000 | Loss: 0.00003595
Iteration 735/1000 | Loss: 0.00004890
Iteration 736/1000 | Loss: 0.00003653
Iteration 737/1000 | Loss: 0.00005299
Iteration 738/1000 | Loss: 0.00004200
Iteration 739/1000 | Loss: 0.00005320
Iteration 740/1000 | Loss: 0.00003867
Iteration 741/1000 | Loss: 0.00004248
Iteration 742/1000 | Loss: 0.00003738
Iteration 743/1000 | Loss: 0.00004607
Iteration 744/1000 | Loss: 0.00003527
Iteration 745/1000 | Loss: 0.00004513
Iteration 746/1000 | Loss: 0.00003372
Iteration 747/1000 | Loss: 0.00004759
Iteration 748/1000 | Loss: 0.00004567
Iteration 749/1000 | Loss: 0.00005405
Iteration 750/1000 | Loss: 0.00005178
Iteration 751/1000 | Loss: 0.00005502
Iteration 752/1000 | Loss: 0.00005510
Iteration 753/1000 | Loss: 0.00005357
Iteration 754/1000 | Loss: 0.00005958
Iteration 755/1000 | Loss: 0.00004931
Iteration 756/1000 | Loss: 0.00004428
Iteration 757/1000 | Loss: 0.00003581
Iteration 758/1000 | Loss: 0.00003391
Iteration 759/1000 | Loss: 0.00004265
Iteration 760/1000 | Loss: 0.00003694
Iteration 761/1000 | Loss: 0.00004383
Iteration 762/1000 | Loss: 0.00003913
Iteration 763/1000 | Loss: 0.00004802
Iteration 764/1000 | Loss: 0.00004697
Iteration 765/1000 | Loss: 0.00005598
Iteration 766/1000 | Loss: 0.00005000
Iteration 767/1000 | Loss: 0.00006123
Iteration 768/1000 | Loss: 0.00004945
Iteration 769/1000 | Loss: 0.00004959
Iteration 770/1000 | Loss: 0.00004399
Iteration 771/1000 | Loss: 0.00005953
Iteration 772/1000 | Loss: 0.00005709
Iteration 773/1000 | Loss: 0.00005996
Iteration 774/1000 | Loss: 0.00004842
Iteration 775/1000 | Loss: 0.00003891
Iteration 776/1000 | Loss: 0.00004007
Iteration 777/1000 | Loss: 0.00004146
Iteration 778/1000 | Loss: 0.00004425
Iteration 779/1000 | Loss: 0.00004986
Iteration 780/1000 | Loss: 0.00004708
Iteration 781/1000 | Loss: 0.00005123
Iteration 782/1000 | Loss: 0.00004717
Iteration 783/1000 | Loss: 0.00004611
Iteration 784/1000 | Loss: 0.00004817
Iteration 785/1000 | Loss: 0.00004173
Iteration 786/1000 | Loss: 0.00004104
Iteration 787/1000 | Loss: 0.00004914
Iteration 788/1000 | Loss: 0.00004171
Iteration 789/1000 | Loss: 0.00004767
Iteration 790/1000 | Loss: 0.00005129
Iteration 791/1000 | Loss: 0.00005231
Iteration 792/1000 | Loss: 0.00004879
Iteration 793/1000 | Loss: 0.00004737
Iteration 794/1000 | Loss: 0.00005803
Iteration 795/1000 | Loss: 0.00005134
Iteration 796/1000 | Loss: 0.00005139
Iteration 797/1000 | Loss: 0.00004571
Iteration 798/1000 | Loss: 0.00003258
Iteration 799/1000 | Loss: 0.00003224
Iteration 800/1000 | Loss: 0.00002773
Iteration 801/1000 | Loss: 0.00003455
Iteration 802/1000 | Loss: 0.00003225
Iteration 803/1000 | Loss: 0.00004430
Iteration 804/1000 | Loss: 0.00003648
Iteration 805/1000 | Loss: 0.00003708
Iteration 806/1000 | Loss: 0.00003666
Iteration 807/1000 | Loss: 0.00003289
Iteration 808/1000 | Loss: 0.00003398
Iteration 809/1000 | Loss: 0.00003484
Iteration 810/1000 | Loss: 0.00003272
Iteration 811/1000 | Loss: 0.00003762
Iteration 812/1000 | Loss: 0.00003268
Iteration 813/1000 | Loss: 0.00004716
Iteration 814/1000 | Loss: 0.00003337
Iteration 815/1000 | Loss: 0.00004771
Iteration 816/1000 | Loss: 0.00004161
Iteration 817/1000 | Loss: 0.00004434
Iteration 818/1000 | Loss: 0.00004110
Iteration 819/1000 | Loss: 0.00004401
Iteration 820/1000 | Loss: 0.00004401
Iteration 821/1000 | Loss: 0.00003992
Iteration 822/1000 | Loss: 0.00003653
Iteration 823/1000 | Loss: 0.00003430
Iteration 824/1000 | Loss: 0.00004211
Iteration 825/1000 | Loss: 0.00003304
Iteration 826/1000 | Loss: 0.00004175
Iteration 827/1000 | Loss: 0.00003279
Iteration 828/1000 | Loss: 0.00003163
Iteration 829/1000 | Loss: 0.00003205
Iteration 830/1000 | Loss: 0.00003864
Iteration 831/1000 | Loss: 0.00003634
Iteration 832/1000 | Loss: 0.00003644
Iteration 833/1000 | Loss: 0.00003344
Iteration 834/1000 | Loss: 0.00005195
Iteration 835/1000 | Loss: 0.00003261
Iteration 836/1000 | Loss: 0.00003889
Iteration 837/1000 | Loss: 0.00003624
Iteration 838/1000 | Loss: 0.00004409
Iteration 839/1000 | Loss: 0.00004098
Iteration 840/1000 | Loss: 0.00004025
Iteration 841/1000 | Loss: 0.00003538
Iteration 842/1000 | Loss: 0.00004969
Iteration 843/1000 | Loss: 0.00004099
Iteration 844/1000 | Loss: 0.00005043
Iteration 845/1000 | Loss: 0.00004069
Iteration 846/1000 | Loss: 0.00004223
Iteration 847/1000 | Loss: 0.00003737
Iteration 848/1000 | Loss: 0.00003716
Iteration 849/1000 | Loss: 0.00003997
Iteration 850/1000 | Loss: 0.00004627
Iteration 851/1000 | Loss: 0.00003866
Iteration 852/1000 | Loss: 0.00005168
Iteration 853/1000 | Loss: 0.00004577
Iteration 854/1000 | Loss: 0.00005131
Iteration 855/1000 | Loss: 0.00004410
Iteration 856/1000 | Loss: 0.00005144
Iteration 857/1000 | Loss: 0.00005220
Iteration 858/1000 | Loss: 0.00004936
Iteration 859/1000 | Loss: 0.00004491
Iteration 860/1000 | Loss: 0.00003456
Iteration 861/1000 | Loss: 0.00003121
Iteration 862/1000 | Loss: 0.00003816
Iteration 863/1000 | Loss: 0.00003730
Iteration 864/1000 | Loss: 0.00005154
Iteration 865/1000 | Loss: 0.00004362
Iteration 866/1000 | Loss: 0.00004497
Iteration 867/1000 | Loss: 0.00003744
Iteration 868/1000 | Loss: 0.00005188
Iteration 869/1000 | Loss: 0.00004916
Iteration 870/1000 | Loss: 0.00003917
Iteration 871/1000 | Loss: 0.00004758
Iteration 872/1000 | Loss: 0.00004005
Iteration 873/1000 | Loss: 0.00004633
Iteration 874/1000 | Loss: 0.00003314
Iteration 875/1000 | Loss: 0.00004777
Iteration 876/1000 | Loss: 0.00003834
Iteration 877/1000 | Loss: 0.00004432
Iteration 878/1000 | Loss: 0.00004205
Iteration 879/1000 | Loss: 0.00004854
Iteration 880/1000 | Loss: 0.00004823
Iteration 881/1000 | Loss: 0.00004980
Iteration 882/1000 | Loss: 0.00005276
Iteration 883/1000 | Loss: 0.00004747
Iteration 884/1000 | Loss: 0.00003746
Iteration 885/1000 | Loss: 0.00003875
Iteration 886/1000 | Loss: 0.00004752
Iteration 887/1000 | Loss: 0.00004788
Iteration 888/1000 | Loss: 0.00004196
Iteration 889/1000 | Loss: 0.00004530
Iteration 890/1000 | Loss: 0.00005225
Iteration 891/1000 | Loss: 0.00004521
Iteration 892/1000 | Loss: 0.00004807
Iteration 893/1000 | Loss: 0.00004502
Iteration 894/1000 | Loss: 0.00004762
Iteration 895/1000 | Loss: 0.00005033
Iteration 896/1000 | Loss: 0.00004479
Iteration 897/1000 | Loss: 0.00004921
Iteration 898/1000 | Loss: 0.00004282
Iteration 899/1000 | Loss: 0.00004908
Iteration 900/1000 | Loss: 0.00003445
Iteration 901/1000 | Loss: 0.00003333
Iteration 902/1000 | Loss: 0.00003808
Iteration 903/1000 | Loss: 0.00003989
Iteration 904/1000 | Loss: 0.00003256
Iteration 905/1000 | Loss: 0.00003933
Iteration 906/1000 | Loss: 0.00004698
Iteration 907/1000 | Loss: 0.00004761
Iteration 908/1000 | Loss: 0.00004679
Iteration 909/1000 | Loss: 0.00003442
Iteration 910/1000 | Loss: 0.00003505
Iteration 911/1000 | Loss: 0.00003485
Iteration 912/1000 | Loss: 0.00003329
Iteration 913/1000 | Loss: 0.00004268
Iteration 914/1000 | Loss: 0.00003332
Iteration 915/1000 | Loss: 0.00002820
Iteration 916/1000 | Loss: 0.00002681
Iteration 917/1000 | Loss: 0.00003425
Iteration 918/1000 | Loss: 0.00004338
Iteration 919/1000 | Loss: 0.00004158
Iteration 920/1000 | Loss: 0.00003805
Iteration 921/1000 | Loss: 0.00003614
Iteration 922/1000 | Loss: 0.00004487
Iteration 923/1000 | Loss: 0.00003564
Iteration 924/1000 | Loss: 0.00002987
Iteration 925/1000 | Loss: 0.00003562
Iteration 926/1000 | Loss: 0.00003923
Iteration 927/1000 | Loss: 0.00004510
Iteration 928/1000 | Loss: 0.00004990
Iteration 929/1000 | Loss: 0.00003782
Iteration 930/1000 | Loss: 0.00003744
Iteration 931/1000 | Loss: 0.00004524
Iteration 932/1000 | Loss: 0.00005417
Iteration 933/1000 | Loss: 0.00004537
Iteration 934/1000 | Loss: 0.00005083
Iteration 935/1000 | Loss: 0.00004092
Iteration 936/1000 | Loss: 0.00003966
Iteration 937/1000 | Loss: 0.00004686
Iteration 938/1000 | Loss: 0.00005454
Iteration 939/1000 | Loss: 0.00004099
Iteration 940/1000 | Loss: 0.00005094
Iteration 941/1000 | Loss: 0.00003895
Iteration 942/1000 | Loss: 0.00003539
Iteration 943/1000 | Loss: 0.00004158
Iteration 944/1000 | Loss: 0.00003762
Iteration 945/1000 | Loss: 0.00004295
Iteration 946/1000 | Loss: 0.00003623
Iteration 947/1000 | Loss: 0.00004050
Iteration 948/1000 | Loss: 0.00003441
Iteration 949/1000 | Loss: 0.00004741
Iteration 950/1000 | Loss: 0.00004315
Iteration 951/1000 | Loss: 0.00004350
Iteration 952/1000 | Loss: 0.00004384
Iteration 953/1000 | Loss: 0.00003558
Iteration 954/1000 | Loss: 0.00003486
Iteration 955/1000 | Loss: 0.00003776
Iteration 956/1000 | Loss: 0.00003427
Iteration 957/1000 | Loss: 0.00004213
Iteration 958/1000 | Loss: 0.00004309
Iteration 959/1000 | Loss: 0.00004040
Iteration 960/1000 | Loss: 0.00004019
Iteration 961/1000 | Loss: 0.00003920
Iteration 962/1000 | Loss: 0.00004009
Iteration 963/1000 | Loss: 0.00004095
Iteration 964/1000 | Loss: 0.00004560
Iteration 965/1000 | Loss: 0.00004028
Iteration 966/1000 | Loss: 0.00004551
Iteration 967/1000 | Loss: 0.00004349
Iteration 968/1000 | Loss: 0.00004399
Iteration 969/1000 | Loss: 0.00004025
Iteration 970/1000 | Loss: 0.00004311
Iteration 971/1000 | Loss: 0.00003529
Iteration 972/1000 | Loss: 0.00003220
Iteration 973/1000 | Loss: 0.00004429
Iteration 974/1000 | Loss: 0.00003541
Iteration 975/1000 | Loss: 0.00005321
Iteration 976/1000 | Loss: 0.00003630
Iteration 977/1000 | Loss: 0.00003333
Iteration 978/1000 | Loss: 0.00003760
Iteration 979/1000 | Loss: 0.00004528
Iteration 980/1000 | Loss: 0.00003710
Iteration 981/1000 | Loss: 0.00003368
Iteration 982/1000 | Loss: 0.00003430
Iteration 983/1000 | Loss: 0.00003040
Iteration 984/1000 | Loss: 0.00004610
Iteration 985/1000 | Loss: 0.00004085
Iteration 986/1000 | Loss: 0.00004571
Iteration 987/1000 | Loss: 0.00003865
Iteration 988/1000 | Loss: 0.00004450
Iteration 989/1000 | Loss: 0.00004112
Iteration 990/1000 | Loss: 0.00005255
Iteration 991/1000 | Loss: 0.00004222
Iteration 992/1000 | Loss: 0.00004818
Iteration 993/1000 | Loss: 0.00004299
Iteration 994/1000 | Loss: 0.00003644
Iteration 995/1000 | Loss: 0.00004345
Iteration 996/1000 | Loss: 0.00003985
Iteration 997/1000 | Loss: 0.00004083
Iteration 998/1000 | Loss: 0.00003500
Iteration 999/1000 | Loss: 0.00003011
Iteration 1000/1000 | Loss: 0.00002739

Optimization complete. Final v2v error: 4.289297580718994 mm

Highest mean error: 6.778011322021484 mm for frame 131

Lowest mean error: 3.071593761444092 mm for frame 44

Saving results

Total time: 1410.1398832798004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004375
Iteration 2/25 | Loss: 0.00142042
Iteration 3/25 | Loss: 0.00086570
Iteration 4/25 | Loss: 0.00079127
Iteration 5/25 | Loss: 0.00077675
Iteration 6/25 | Loss: 0.00077384
Iteration 7/25 | Loss: 0.00077331
Iteration 8/25 | Loss: 0.00077331
Iteration 9/25 | Loss: 0.00077331
Iteration 10/25 | Loss: 0.00077331
Iteration 11/25 | Loss: 0.00077331
Iteration 12/25 | Loss: 0.00077331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007733120582997799, 0.0007733120582997799, 0.0007733120582997799, 0.0007733120582997799, 0.0007733120582997799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007733120582997799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.82227612
Iteration 2/25 | Loss: 0.00018753
Iteration 3/25 | Loss: 0.00018752
Iteration 4/25 | Loss: 0.00018752
Iteration 5/25 | Loss: 0.00018752
Iteration 6/25 | Loss: 0.00018752
Iteration 7/25 | Loss: 0.00018752
Iteration 8/25 | Loss: 0.00018752
Iteration 9/25 | Loss: 0.00018752
Iteration 10/25 | Loss: 0.00018752
Iteration 11/25 | Loss: 0.00018752
Iteration 12/25 | Loss: 0.00018752
Iteration 13/25 | Loss: 0.00018752
Iteration 14/25 | Loss: 0.00018752
Iteration 15/25 | Loss: 0.00018752
Iteration 16/25 | Loss: 0.00018752
Iteration 17/25 | Loss: 0.00018752
Iteration 18/25 | Loss: 0.00018752
Iteration 19/25 | Loss: 0.00018752
Iteration 20/25 | Loss: 0.00018752
Iteration 21/25 | Loss: 0.00018752
Iteration 22/25 | Loss: 0.00018752
Iteration 23/25 | Loss: 0.00018752
Iteration 24/25 | Loss: 0.00018752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00018751998140942305, 0.00018751998140942305, 0.00018751998140942305, 0.00018751998140942305, 0.00018751998140942305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018751998140942305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018752
Iteration 2/1000 | Loss: 0.00004025
Iteration 3/1000 | Loss: 0.00003335
Iteration 4/1000 | Loss: 0.00002988
Iteration 5/1000 | Loss: 0.00002808
Iteration 6/1000 | Loss: 0.00002732
Iteration 7/1000 | Loss: 0.00002693
Iteration 8/1000 | Loss: 0.00002664
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002605
Iteration 11/1000 | Loss: 0.00002590
Iteration 12/1000 | Loss: 0.00002583
Iteration 13/1000 | Loss: 0.00002580
Iteration 14/1000 | Loss: 0.00002579
Iteration 15/1000 | Loss: 0.00002578
Iteration 16/1000 | Loss: 0.00002573
Iteration 17/1000 | Loss: 0.00002572
Iteration 18/1000 | Loss: 0.00002567
Iteration 19/1000 | Loss: 0.00002566
Iteration 20/1000 | Loss: 0.00002566
Iteration 21/1000 | Loss: 0.00002564
Iteration 22/1000 | Loss: 0.00002563
Iteration 23/1000 | Loss: 0.00002562
Iteration 24/1000 | Loss: 0.00002562
Iteration 25/1000 | Loss: 0.00002562
Iteration 26/1000 | Loss: 0.00002562
Iteration 27/1000 | Loss: 0.00002562
Iteration 28/1000 | Loss: 0.00002562
Iteration 29/1000 | Loss: 0.00002561
Iteration 30/1000 | Loss: 0.00002561
Iteration 31/1000 | Loss: 0.00002561
Iteration 32/1000 | Loss: 0.00002561
Iteration 33/1000 | Loss: 0.00002561
Iteration 34/1000 | Loss: 0.00002559
Iteration 35/1000 | Loss: 0.00002559
Iteration 36/1000 | Loss: 0.00002558
Iteration 37/1000 | Loss: 0.00002558
Iteration 38/1000 | Loss: 0.00002557
Iteration 39/1000 | Loss: 0.00002556
Iteration 40/1000 | Loss: 0.00002554
Iteration 41/1000 | Loss: 0.00002554
Iteration 42/1000 | Loss: 0.00002554
Iteration 43/1000 | Loss: 0.00002554
Iteration 44/1000 | Loss: 0.00002554
Iteration 45/1000 | Loss: 0.00002554
Iteration 46/1000 | Loss: 0.00002554
Iteration 47/1000 | Loss: 0.00002554
Iteration 48/1000 | Loss: 0.00002554
Iteration 49/1000 | Loss: 0.00002554
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002553
Iteration 52/1000 | Loss: 0.00002552
Iteration 53/1000 | Loss: 0.00002552
Iteration 54/1000 | Loss: 0.00002551
Iteration 55/1000 | Loss: 0.00002551
Iteration 56/1000 | Loss: 0.00002551
Iteration 57/1000 | Loss: 0.00002550
Iteration 58/1000 | Loss: 0.00002549
Iteration 59/1000 | Loss: 0.00002549
Iteration 60/1000 | Loss: 0.00002548
Iteration 61/1000 | Loss: 0.00002548
Iteration 62/1000 | Loss: 0.00002548
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002547
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00002546
Iteration 70/1000 | Loss: 0.00002544
Iteration 71/1000 | Loss: 0.00002544
Iteration 72/1000 | Loss: 0.00002544
Iteration 73/1000 | Loss: 0.00002544
Iteration 74/1000 | Loss: 0.00002544
Iteration 75/1000 | Loss: 0.00002543
Iteration 76/1000 | Loss: 0.00002543
Iteration 77/1000 | Loss: 0.00002543
Iteration 78/1000 | Loss: 0.00002542
Iteration 79/1000 | Loss: 0.00002541
Iteration 80/1000 | Loss: 0.00002541
Iteration 81/1000 | Loss: 0.00002541
Iteration 82/1000 | Loss: 0.00002541
Iteration 83/1000 | Loss: 0.00002541
Iteration 84/1000 | Loss: 0.00002540
Iteration 85/1000 | Loss: 0.00002540
Iteration 86/1000 | Loss: 0.00002540
Iteration 87/1000 | Loss: 0.00002540
Iteration 88/1000 | Loss: 0.00002540
Iteration 89/1000 | Loss: 0.00002540
Iteration 90/1000 | Loss: 0.00002539
Iteration 91/1000 | Loss: 0.00002539
Iteration 92/1000 | Loss: 0.00002539
Iteration 93/1000 | Loss: 0.00002539
Iteration 94/1000 | Loss: 0.00002539
Iteration 95/1000 | Loss: 0.00002539
Iteration 96/1000 | Loss: 0.00002539
Iteration 97/1000 | Loss: 0.00002539
Iteration 98/1000 | Loss: 0.00002539
Iteration 99/1000 | Loss: 0.00002538
Iteration 100/1000 | Loss: 0.00002538
Iteration 101/1000 | Loss: 0.00002538
Iteration 102/1000 | Loss: 0.00002538
Iteration 103/1000 | Loss: 0.00002538
Iteration 104/1000 | Loss: 0.00002538
Iteration 105/1000 | Loss: 0.00002537
Iteration 106/1000 | Loss: 0.00002537
Iteration 107/1000 | Loss: 0.00002537
Iteration 108/1000 | Loss: 0.00002537
Iteration 109/1000 | Loss: 0.00002537
Iteration 110/1000 | Loss: 0.00002537
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002536
Iteration 113/1000 | Loss: 0.00002536
Iteration 114/1000 | Loss: 0.00002536
Iteration 115/1000 | Loss: 0.00002536
Iteration 116/1000 | Loss: 0.00002535
Iteration 117/1000 | Loss: 0.00002535
Iteration 118/1000 | Loss: 0.00002535
Iteration 119/1000 | Loss: 0.00002535
Iteration 120/1000 | Loss: 0.00002535
Iteration 121/1000 | Loss: 0.00002535
Iteration 122/1000 | Loss: 0.00002535
Iteration 123/1000 | Loss: 0.00002535
Iteration 124/1000 | Loss: 0.00002534
Iteration 125/1000 | Loss: 0.00002534
Iteration 126/1000 | Loss: 0.00002534
Iteration 127/1000 | Loss: 0.00002533
Iteration 128/1000 | Loss: 0.00002533
Iteration 129/1000 | Loss: 0.00002533
Iteration 130/1000 | Loss: 0.00002533
Iteration 131/1000 | Loss: 0.00002533
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002533
Iteration 134/1000 | Loss: 0.00002533
Iteration 135/1000 | Loss: 0.00002533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.5331346478196792e-05, 2.5331346478196792e-05, 2.5331346478196792e-05, 2.5331346478196792e-05, 2.5331346478196792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5331346478196792e-05

Optimization complete. Final v2v error: 4.157803535461426 mm

Highest mean error: 4.987149238586426 mm for frame 138

Lowest mean error: 3.4168593883514404 mm for frame 43

Saving results

Total time: 42.46294093132019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817836
Iteration 2/25 | Loss: 0.00079518
Iteration 3/25 | Loss: 0.00062663
Iteration 4/25 | Loss: 0.00060463
Iteration 5/25 | Loss: 0.00059729
Iteration 6/25 | Loss: 0.00059597
Iteration 7/25 | Loss: 0.00059576
Iteration 8/25 | Loss: 0.00059576
Iteration 9/25 | Loss: 0.00059576
Iteration 10/25 | Loss: 0.00059576
Iteration 11/25 | Loss: 0.00059576
Iteration 12/25 | Loss: 0.00059576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005957636167295277, 0.0005957636167295277, 0.0005957636167295277, 0.0005957636167295277, 0.0005957636167295277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005957636167295277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29099059
Iteration 2/25 | Loss: 0.00028564
Iteration 3/25 | Loss: 0.00028564
Iteration 4/25 | Loss: 0.00028564
Iteration 5/25 | Loss: 0.00028564
Iteration 6/25 | Loss: 0.00028564
Iteration 7/25 | Loss: 0.00028564
Iteration 8/25 | Loss: 0.00028564
Iteration 9/25 | Loss: 0.00028564
Iteration 10/25 | Loss: 0.00028564
Iteration 11/25 | Loss: 0.00028564
Iteration 12/25 | Loss: 0.00028564
Iteration 13/25 | Loss: 0.00028564
Iteration 14/25 | Loss: 0.00028564
Iteration 15/25 | Loss: 0.00028564
Iteration 16/25 | Loss: 0.00028564
Iteration 17/25 | Loss: 0.00028564
Iteration 18/25 | Loss: 0.00028564
Iteration 19/25 | Loss: 0.00028564
Iteration 20/25 | Loss: 0.00028564
Iteration 21/25 | Loss: 0.00028564
Iteration 22/25 | Loss: 0.00028564
Iteration 23/25 | Loss: 0.00028564
Iteration 24/25 | Loss: 0.00028564
Iteration 25/25 | Loss: 0.00028564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028564
Iteration 2/1000 | Loss: 0.00002579
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001715
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001494
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001386
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001385
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001360
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001355
Iteration 107/1000 | Loss: 0.00001355
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001354
Iteration 116/1000 | Loss: 0.00001354
Iteration 117/1000 | Loss: 0.00001354
Iteration 118/1000 | Loss: 0.00001354
Iteration 119/1000 | Loss: 0.00001354
Iteration 120/1000 | Loss: 0.00001354
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001353
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001353
Iteration 146/1000 | Loss: 0.00001353
Iteration 147/1000 | Loss: 0.00001353
Iteration 148/1000 | Loss: 0.00001353
Iteration 149/1000 | Loss: 0.00001353
Iteration 150/1000 | Loss: 0.00001353
Iteration 151/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3528441741073038e-05, 1.3528441741073038e-05, 1.3528441741073038e-05, 1.3528441741073038e-05, 1.3528441741073038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3528441741073038e-05

Optimization complete. Final v2v error: 3.1718223094940186 mm

Highest mean error: 3.322967052459717 mm for frame 72

Lowest mean error: 3.003141403198242 mm for frame 10

Saving results

Total time: 37.418076515197754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967354
Iteration 2/25 | Loss: 0.00134032
Iteration 3/25 | Loss: 0.00103452
Iteration 4/25 | Loss: 0.00098984
Iteration 5/25 | Loss: 0.00096106
Iteration 6/25 | Loss: 0.00092452
Iteration 7/25 | Loss: 0.00092191
Iteration 8/25 | Loss: 0.00091134
Iteration 9/25 | Loss: 0.00091990
Iteration 10/25 | Loss: 0.00090126
Iteration 11/25 | Loss: 0.00088853
Iteration 12/25 | Loss: 0.00088828
Iteration 13/25 | Loss: 0.00089025
Iteration 14/25 | Loss: 0.00088769
Iteration 15/25 | Loss: 0.00088501
Iteration 16/25 | Loss: 0.00088552
Iteration 17/25 | Loss: 0.00088764
Iteration 18/25 | Loss: 0.00088074
Iteration 19/25 | Loss: 0.00087694
Iteration 20/25 | Loss: 0.00087577
Iteration 21/25 | Loss: 0.00088025
Iteration 22/25 | Loss: 0.00087388
Iteration 23/25 | Loss: 0.00087072
Iteration 24/25 | Loss: 0.00087319
Iteration 25/25 | Loss: 0.00087429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29865599
Iteration 2/25 | Loss: 0.00124422
Iteration 3/25 | Loss: 0.00124422
Iteration 4/25 | Loss: 0.00124422
Iteration 5/25 | Loss: 0.00124422
Iteration 6/25 | Loss: 0.00124422
Iteration 7/25 | Loss: 0.00124422
Iteration 8/25 | Loss: 0.00124422
Iteration 9/25 | Loss: 0.00124422
Iteration 10/25 | Loss: 0.00124422
Iteration 11/25 | Loss: 0.00124422
Iteration 12/25 | Loss: 0.00124422
Iteration 13/25 | Loss: 0.00124422
Iteration 14/25 | Loss: 0.00124422
Iteration 15/25 | Loss: 0.00124422
Iteration 16/25 | Loss: 0.00124422
Iteration 17/25 | Loss: 0.00124422
Iteration 18/25 | Loss: 0.00124422
Iteration 19/25 | Loss: 0.00124422
Iteration 20/25 | Loss: 0.00124422
Iteration 21/25 | Loss: 0.00124422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012442184379324317, 0.0012442184379324317, 0.0012442184379324317, 0.0012442184379324317, 0.0012442184379324317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012442184379324317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124422
Iteration 2/1000 | Loss: 0.00023404
Iteration 3/1000 | Loss: 0.00048926
Iteration 4/1000 | Loss: 0.00034066
Iteration 5/1000 | Loss: 0.00017911
Iteration 6/1000 | Loss: 0.00079667
Iteration 7/1000 | Loss: 0.00066184
Iteration 8/1000 | Loss: 0.00084715
Iteration 9/1000 | Loss: 0.00065286
Iteration 10/1000 | Loss: 0.00073795
Iteration 11/1000 | Loss: 0.00100360
Iteration 12/1000 | Loss: 0.00087102
Iteration 13/1000 | Loss: 0.00082234
Iteration 14/1000 | Loss: 0.00040809
Iteration 15/1000 | Loss: 0.00032805
Iteration 16/1000 | Loss: 0.00009240
Iteration 17/1000 | Loss: 0.00037943
Iteration 18/1000 | Loss: 0.00014664
Iteration 19/1000 | Loss: 0.00046868
Iteration 20/1000 | Loss: 0.00022622
Iteration 21/1000 | Loss: 0.00026027
Iteration 22/1000 | Loss: 0.00008303
Iteration 23/1000 | Loss: 0.00017164
Iteration 24/1000 | Loss: 0.00061574
Iteration 25/1000 | Loss: 0.00013483
Iteration 26/1000 | Loss: 0.00007794
Iteration 27/1000 | Loss: 0.00007314
Iteration 28/1000 | Loss: 0.00055895
Iteration 29/1000 | Loss: 0.00063469
Iteration 30/1000 | Loss: 0.00066246
Iteration 31/1000 | Loss: 0.00180636
Iteration 32/1000 | Loss: 0.00307407
Iteration 33/1000 | Loss: 0.00013717
Iteration 34/1000 | Loss: 0.00008977
Iteration 35/1000 | Loss: 0.00007185
Iteration 36/1000 | Loss: 0.00005331
Iteration 37/1000 | Loss: 0.00004515
Iteration 38/1000 | Loss: 0.00004175
Iteration 39/1000 | Loss: 0.00003937
Iteration 40/1000 | Loss: 0.00003756
Iteration 41/1000 | Loss: 0.00003603
Iteration 42/1000 | Loss: 0.00003521
Iteration 43/1000 | Loss: 0.00003456
Iteration 44/1000 | Loss: 0.00003409
Iteration 45/1000 | Loss: 0.00003377
Iteration 46/1000 | Loss: 0.00003350
Iteration 47/1000 | Loss: 0.00003318
Iteration 48/1000 | Loss: 0.00003306
Iteration 49/1000 | Loss: 0.00003299
Iteration 50/1000 | Loss: 0.00003299
Iteration 51/1000 | Loss: 0.00003299
Iteration 52/1000 | Loss: 0.00003286
Iteration 53/1000 | Loss: 0.00003284
Iteration 54/1000 | Loss: 0.00003281
Iteration 55/1000 | Loss: 0.00003279
Iteration 56/1000 | Loss: 0.00003279
Iteration 57/1000 | Loss: 0.00003278
Iteration 58/1000 | Loss: 0.00003278
Iteration 59/1000 | Loss: 0.00003275
Iteration 60/1000 | Loss: 0.00003275
Iteration 61/1000 | Loss: 0.00003274
Iteration 62/1000 | Loss: 0.00003274
Iteration 63/1000 | Loss: 0.00003273
Iteration 64/1000 | Loss: 0.00003273
Iteration 65/1000 | Loss: 0.00003272
Iteration 66/1000 | Loss: 0.00003272
Iteration 67/1000 | Loss: 0.00003272
Iteration 68/1000 | Loss: 0.00003271
Iteration 69/1000 | Loss: 0.00003271
Iteration 70/1000 | Loss: 0.00003271
Iteration 71/1000 | Loss: 0.00003271
Iteration 72/1000 | Loss: 0.00003271
Iteration 73/1000 | Loss: 0.00003271
Iteration 74/1000 | Loss: 0.00003271
Iteration 75/1000 | Loss: 0.00003270
Iteration 76/1000 | Loss: 0.00003270
Iteration 77/1000 | Loss: 0.00003270
Iteration 78/1000 | Loss: 0.00003269
Iteration 79/1000 | Loss: 0.00003269
Iteration 80/1000 | Loss: 0.00003269
Iteration 81/1000 | Loss: 0.00003269
Iteration 82/1000 | Loss: 0.00003268
Iteration 83/1000 | Loss: 0.00003268
Iteration 84/1000 | Loss: 0.00003268
Iteration 85/1000 | Loss: 0.00003267
Iteration 86/1000 | Loss: 0.00003267
Iteration 87/1000 | Loss: 0.00003267
Iteration 88/1000 | Loss: 0.00003267
Iteration 89/1000 | Loss: 0.00003267
Iteration 90/1000 | Loss: 0.00003266
Iteration 91/1000 | Loss: 0.00003266
Iteration 92/1000 | Loss: 0.00003265
Iteration 93/1000 | Loss: 0.00003265
Iteration 94/1000 | Loss: 0.00003265
Iteration 95/1000 | Loss: 0.00003265
Iteration 96/1000 | Loss: 0.00003265
Iteration 97/1000 | Loss: 0.00003264
Iteration 98/1000 | Loss: 0.00003264
Iteration 99/1000 | Loss: 0.00003264
Iteration 100/1000 | Loss: 0.00003263
Iteration 101/1000 | Loss: 0.00003263
Iteration 102/1000 | Loss: 0.00003263
Iteration 103/1000 | Loss: 0.00003263
Iteration 104/1000 | Loss: 0.00003263
Iteration 105/1000 | Loss: 0.00003262
Iteration 106/1000 | Loss: 0.00003262
Iteration 107/1000 | Loss: 0.00003262
Iteration 108/1000 | Loss: 0.00003262
Iteration 109/1000 | Loss: 0.00003262
Iteration 110/1000 | Loss: 0.00003262
Iteration 111/1000 | Loss: 0.00003261
Iteration 112/1000 | Loss: 0.00003261
Iteration 113/1000 | Loss: 0.00003261
Iteration 114/1000 | Loss: 0.00003261
Iteration 115/1000 | Loss: 0.00003260
Iteration 116/1000 | Loss: 0.00003260
Iteration 117/1000 | Loss: 0.00003260
Iteration 118/1000 | Loss: 0.00003260
Iteration 119/1000 | Loss: 0.00003260
Iteration 120/1000 | Loss: 0.00003260
Iteration 121/1000 | Loss: 0.00003260
Iteration 122/1000 | Loss: 0.00003259
Iteration 123/1000 | Loss: 0.00003259
Iteration 124/1000 | Loss: 0.00003259
Iteration 125/1000 | Loss: 0.00003259
Iteration 126/1000 | Loss: 0.00003259
Iteration 127/1000 | Loss: 0.00003259
Iteration 128/1000 | Loss: 0.00003258
Iteration 129/1000 | Loss: 0.00003258
Iteration 130/1000 | Loss: 0.00003258
Iteration 131/1000 | Loss: 0.00003258
Iteration 132/1000 | Loss: 0.00003258
Iteration 133/1000 | Loss: 0.00003258
Iteration 134/1000 | Loss: 0.00003258
Iteration 135/1000 | Loss: 0.00003258
Iteration 136/1000 | Loss: 0.00003258
Iteration 137/1000 | Loss: 0.00003258
Iteration 138/1000 | Loss: 0.00003258
Iteration 139/1000 | Loss: 0.00003257
Iteration 140/1000 | Loss: 0.00003257
Iteration 141/1000 | Loss: 0.00003257
Iteration 142/1000 | Loss: 0.00003257
Iteration 143/1000 | Loss: 0.00003257
Iteration 144/1000 | Loss: 0.00003257
Iteration 145/1000 | Loss: 0.00003257
Iteration 146/1000 | Loss: 0.00003257
Iteration 147/1000 | Loss: 0.00003257
Iteration 148/1000 | Loss: 0.00003257
Iteration 149/1000 | Loss: 0.00003257
Iteration 150/1000 | Loss: 0.00003257
Iteration 151/1000 | Loss: 0.00003257
Iteration 152/1000 | Loss: 0.00003257
Iteration 153/1000 | Loss: 0.00003257
Iteration 154/1000 | Loss: 0.00003257
Iteration 155/1000 | Loss: 0.00003257
Iteration 156/1000 | Loss: 0.00003257
Iteration 157/1000 | Loss: 0.00003257
Iteration 158/1000 | Loss: 0.00003256
Iteration 159/1000 | Loss: 0.00003256
Iteration 160/1000 | Loss: 0.00003256
Iteration 161/1000 | Loss: 0.00003256
Iteration 162/1000 | Loss: 0.00003256
Iteration 163/1000 | Loss: 0.00003255
Iteration 164/1000 | Loss: 0.00003255
Iteration 165/1000 | Loss: 0.00003255
Iteration 166/1000 | Loss: 0.00003255
Iteration 167/1000 | Loss: 0.00003255
Iteration 168/1000 | Loss: 0.00003255
Iteration 169/1000 | Loss: 0.00003255
Iteration 170/1000 | Loss: 0.00003255
Iteration 171/1000 | Loss: 0.00003255
Iteration 172/1000 | Loss: 0.00003254
Iteration 173/1000 | Loss: 0.00003254
Iteration 174/1000 | Loss: 0.00003254
Iteration 175/1000 | Loss: 0.00003254
Iteration 176/1000 | Loss: 0.00003254
Iteration 177/1000 | Loss: 0.00003254
Iteration 178/1000 | Loss: 0.00003254
Iteration 179/1000 | Loss: 0.00003254
Iteration 180/1000 | Loss: 0.00003254
Iteration 181/1000 | Loss: 0.00003254
Iteration 182/1000 | Loss: 0.00003254
Iteration 183/1000 | Loss: 0.00003254
Iteration 184/1000 | Loss: 0.00003254
Iteration 185/1000 | Loss: 0.00003253
Iteration 186/1000 | Loss: 0.00003253
Iteration 187/1000 | Loss: 0.00003253
Iteration 188/1000 | Loss: 0.00003253
Iteration 189/1000 | Loss: 0.00003253
Iteration 190/1000 | Loss: 0.00003253
Iteration 191/1000 | Loss: 0.00003253
Iteration 192/1000 | Loss: 0.00003252
Iteration 193/1000 | Loss: 0.00003252
Iteration 194/1000 | Loss: 0.00003252
Iteration 195/1000 | Loss: 0.00003252
Iteration 196/1000 | Loss: 0.00003252
Iteration 197/1000 | Loss: 0.00003252
Iteration 198/1000 | Loss: 0.00003252
Iteration 199/1000 | Loss: 0.00003252
Iteration 200/1000 | Loss: 0.00003252
Iteration 201/1000 | Loss: 0.00003252
Iteration 202/1000 | Loss: 0.00003252
Iteration 203/1000 | Loss: 0.00003252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [3.2522402761969715e-05, 3.2522402761969715e-05, 3.2522402761969715e-05, 3.2522402761969715e-05, 3.2522402761969715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2522402761969715e-05

Optimization complete. Final v2v error: 4.77385950088501 mm

Highest mean error: 5.694271087646484 mm for frame 31

Lowest mean error: 3.6495285034179688 mm for frame 214

Saving results

Total time: 143.21681785583496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849604
Iteration 2/25 | Loss: 0.00082616
Iteration 3/25 | Loss: 0.00064760
Iteration 4/25 | Loss: 0.00061274
Iteration 5/25 | Loss: 0.00060678
Iteration 6/25 | Loss: 0.00060530
Iteration 7/25 | Loss: 0.00060521
Iteration 8/25 | Loss: 0.00060521
Iteration 9/25 | Loss: 0.00060521
Iteration 10/25 | Loss: 0.00060521
Iteration 11/25 | Loss: 0.00060521
Iteration 12/25 | Loss: 0.00060521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006052068783901632, 0.0006052068783901632, 0.0006052068783901632, 0.0006052068783901632, 0.0006052068783901632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006052068783901632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28415024
Iteration 2/25 | Loss: 0.00032283
Iteration 3/25 | Loss: 0.00032283
Iteration 4/25 | Loss: 0.00032283
Iteration 5/25 | Loss: 0.00032283
Iteration 6/25 | Loss: 0.00032283
Iteration 7/25 | Loss: 0.00032283
Iteration 8/25 | Loss: 0.00032283
Iteration 9/25 | Loss: 0.00032283
Iteration 10/25 | Loss: 0.00032283
Iteration 11/25 | Loss: 0.00032283
Iteration 12/25 | Loss: 0.00032283
Iteration 13/25 | Loss: 0.00032283
Iteration 14/25 | Loss: 0.00032283
Iteration 15/25 | Loss: 0.00032283
Iteration 16/25 | Loss: 0.00032283
Iteration 17/25 | Loss: 0.00032283
Iteration 18/25 | Loss: 0.00032283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003228295536246151, 0.0003228295536246151, 0.0003228295536246151, 0.0003228295536246151, 0.0003228295536246151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003228295536246151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032283
Iteration 2/1000 | Loss: 0.00002949
Iteration 3/1000 | Loss: 0.00002214
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001798
Iteration 8/1000 | Loss: 0.00001758
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001721
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001690
Iteration 19/1000 | Loss: 0.00001690
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001687
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001668
Iteration 37/1000 | Loss: 0.00001668
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001663
Iteration 43/1000 | Loss: 0.00001663
Iteration 44/1000 | Loss: 0.00001663
Iteration 45/1000 | Loss: 0.00001661
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001644
Iteration 64/1000 | Loss: 0.00001644
Iteration 65/1000 | Loss: 0.00001644
Iteration 66/1000 | Loss: 0.00001644
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001643
Iteration 70/1000 | Loss: 0.00001642
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001641
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001634
Iteration 111/1000 | Loss: 0.00001634
Iteration 112/1000 | Loss: 0.00001634
Iteration 113/1000 | Loss: 0.00001634
Iteration 114/1000 | Loss: 0.00001634
Iteration 115/1000 | Loss: 0.00001634
Iteration 116/1000 | Loss: 0.00001634
Iteration 117/1000 | Loss: 0.00001634
Iteration 118/1000 | Loss: 0.00001634
Iteration 119/1000 | Loss: 0.00001634
Iteration 120/1000 | Loss: 0.00001634
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001633
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001633
Iteration 128/1000 | Loss: 0.00001633
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001633
Iteration 134/1000 | Loss: 0.00001633
Iteration 135/1000 | Loss: 0.00001633
Iteration 136/1000 | Loss: 0.00001633
Iteration 137/1000 | Loss: 0.00001633
Iteration 138/1000 | Loss: 0.00001633
Iteration 139/1000 | Loss: 0.00001633
Iteration 140/1000 | Loss: 0.00001633
Iteration 141/1000 | Loss: 0.00001633
Iteration 142/1000 | Loss: 0.00001633
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001633
Iteration 145/1000 | Loss: 0.00001633
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001633
Iteration 148/1000 | Loss: 0.00001633
Iteration 149/1000 | Loss: 0.00001633
Iteration 150/1000 | Loss: 0.00001633
Iteration 151/1000 | Loss: 0.00001633
Iteration 152/1000 | Loss: 0.00001633
Iteration 153/1000 | Loss: 0.00001633
Iteration 154/1000 | Loss: 0.00001633
Iteration 155/1000 | Loss: 0.00001633
Iteration 156/1000 | Loss: 0.00001633
Iteration 157/1000 | Loss: 0.00001633
Iteration 158/1000 | Loss: 0.00001633
Iteration 159/1000 | Loss: 0.00001633
Iteration 160/1000 | Loss: 0.00001633
Iteration 161/1000 | Loss: 0.00001633
Iteration 162/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.6334783140337095e-05, 1.6334783140337095e-05, 1.6334783140337095e-05, 1.6334783140337095e-05, 1.6334783140337095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6334783140337095e-05

Optimization complete. Final v2v error: 3.458937406539917 mm

Highest mean error: 3.8671867847442627 mm for frame 59

Lowest mean error: 3.1914799213409424 mm for frame 210

Saving results

Total time: 42.360875606536865
