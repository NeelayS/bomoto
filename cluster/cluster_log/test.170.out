Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=170, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9520-9575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041303
Iteration 2/25 | Loss: 0.01041303
Iteration 3/25 | Loss: 0.01041303
Iteration 4/25 | Loss: 0.00242519
Iteration 5/25 | Loss: 0.00199417
Iteration 6/25 | Loss: 0.00139324
Iteration 7/25 | Loss: 0.00135464
Iteration 8/25 | Loss: 0.00121054
Iteration 9/25 | Loss: 0.00116239
Iteration 10/25 | Loss: 0.00114060
Iteration 11/25 | Loss: 0.00114401
Iteration 12/25 | Loss: 0.00112539
Iteration 13/25 | Loss: 0.00111273
Iteration 14/25 | Loss: 0.00111170
Iteration 15/25 | Loss: 0.00109832
Iteration 16/25 | Loss: 0.00108942
Iteration 17/25 | Loss: 0.00108123
Iteration 18/25 | Loss: 0.00108003
Iteration 19/25 | Loss: 0.00107951
Iteration 20/25 | Loss: 0.00108054
Iteration 21/25 | Loss: 0.00108295
Iteration 22/25 | Loss: 0.00107734
Iteration 23/25 | Loss: 0.00107638
Iteration 24/25 | Loss: 0.00107443
Iteration 25/25 | Loss: 0.00107808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40209115
Iteration 2/25 | Loss: 0.00061232
Iteration 3/25 | Loss: 0.00061232
Iteration 4/25 | Loss: 0.00061232
Iteration 5/25 | Loss: 0.00061232
Iteration 6/25 | Loss: 0.00061232
Iteration 7/25 | Loss: 0.00061232
Iteration 8/25 | Loss: 0.00061232
Iteration 9/25 | Loss: 0.00061232
Iteration 10/25 | Loss: 0.00061232
Iteration 11/25 | Loss: 0.00061232
Iteration 12/25 | Loss: 0.00061232
Iteration 13/25 | Loss: 0.00061232
Iteration 14/25 | Loss: 0.00061232
Iteration 15/25 | Loss: 0.00061232
Iteration 16/25 | Loss: 0.00061232
Iteration 17/25 | Loss: 0.00061232
Iteration 18/25 | Loss: 0.00061232
Iteration 19/25 | Loss: 0.00061232
Iteration 20/25 | Loss: 0.00061232
Iteration 21/25 | Loss: 0.00061232
Iteration 22/25 | Loss: 0.00061232
Iteration 23/25 | Loss: 0.00061232
Iteration 24/25 | Loss: 0.00061232
Iteration 25/25 | Loss: 0.00061232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061232
Iteration 2/1000 | Loss: 0.00031374
Iteration 3/1000 | Loss: 0.00113684
Iteration 4/1000 | Loss: 0.00012987
Iteration 5/1000 | Loss: 0.00013055
Iteration 6/1000 | Loss: 0.00002356
Iteration 7/1000 | Loss: 0.00002226
Iteration 8/1000 | Loss: 0.00006280
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002059
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002018
Iteration 15/1000 | Loss: 0.00002011
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00002002
Iteration 18/1000 | Loss: 0.00002001
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001999
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001997
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001973
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001965
Iteration 37/1000 | Loss: 0.00001962
Iteration 38/1000 | Loss: 0.00001961
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001959
Iteration 42/1000 | Loss: 0.00001958
Iteration 43/1000 | Loss: 0.00001958
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001956
Iteration 47/1000 | Loss: 0.00006751
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001945
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001943
Iteration 54/1000 | Loss: 0.00001942
Iteration 55/1000 | Loss: 0.00001942
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00001942
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001938
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001937
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001934
Iteration 68/1000 | Loss: 0.00001934
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001933
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001930
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001924
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001924
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001923
Iteration 136/1000 | Loss: 0.00001923
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001922
Iteration 143/1000 | Loss: 0.00001922
Iteration 144/1000 | Loss: 0.00001922
Iteration 145/1000 | Loss: 0.00001922
Iteration 146/1000 | Loss: 0.00001922
Iteration 147/1000 | Loss: 0.00001922
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001922
Iteration 151/1000 | Loss: 0.00001922
Iteration 152/1000 | Loss: 0.00001922
Iteration 153/1000 | Loss: 0.00001922
Iteration 154/1000 | Loss: 0.00001922
Iteration 155/1000 | Loss: 0.00001922
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001921
Iteration 163/1000 | Loss: 0.00001921
Iteration 164/1000 | Loss: 0.00001921
Iteration 165/1000 | Loss: 0.00001921
Iteration 166/1000 | Loss: 0.00001921
Iteration 167/1000 | Loss: 0.00001921
Iteration 168/1000 | Loss: 0.00001920
Iteration 169/1000 | Loss: 0.00001920
Iteration 170/1000 | Loss: 0.00001920
Iteration 171/1000 | Loss: 0.00001920
Iteration 172/1000 | Loss: 0.00001920
Iteration 173/1000 | Loss: 0.00001920
Iteration 174/1000 | Loss: 0.00001920
Iteration 175/1000 | Loss: 0.00001920
Iteration 176/1000 | Loss: 0.00001920
Iteration 177/1000 | Loss: 0.00001920
Iteration 178/1000 | Loss: 0.00001920
Iteration 179/1000 | Loss: 0.00001920
Iteration 180/1000 | Loss: 0.00001920
Iteration 181/1000 | Loss: 0.00001920
Iteration 182/1000 | Loss: 0.00001920
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001920
Iteration 185/1000 | Loss: 0.00001920
Iteration 186/1000 | Loss: 0.00001920
Iteration 187/1000 | Loss: 0.00001920
Iteration 188/1000 | Loss: 0.00001920
Iteration 189/1000 | Loss: 0.00001920
Iteration 190/1000 | Loss: 0.00001920
Iteration 191/1000 | Loss: 0.00001920
Iteration 192/1000 | Loss: 0.00001920
Iteration 193/1000 | Loss: 0.00001920
Iteration 194/1000 | Loss: 0.00001920
Iteration 195/1000 | Loss: 0.00001920
Iteration 196/1000 | Loss: 0.00001920
Iteration 197/1000 | Loss: 0.00001920
Iteration 198/1000 | Loss: 0.00001920
Iteration 199/1000 | Loss: 0.00001920
Iteration 200/1000 | Loss: 0.00001920
Iteration 201/1000 | Loss: 0.00001920
Iteration 202/1000 | Loss: 0.00001920
Iteration 203/1000 | Loss: 0.00001920
Iteration 204/1000 | Loss: 0.00001920
Iteration 205/1000 | Loss: 0.00001920
Iteration 206/1000 | Loss: 0.00001920
Iteration 207/1000 | Loss: 0.00001920
Iteration 208/1000 | Loss: 0.00001920
Iteration 209/1000 | Loss: 0.00001920
Iteration 210/1000 | Loss: 0.00001920
Iteration 211/1000 | Loss: 0.00001920
Iteration 212/1000 | Loss: 0.00001920
Iteration 213/1000 | Loss: 0.00001920
Iteration 214/1000 | Loss: 0.00001920
Iteration 215/1000 | Loss: 0.00001920
Iteration 216/1000 | Loss: 0.00001920
Iteration 217/1000 | Loss: 0.00001920
Iteration 218/1000 | Loss: 0.00001920
Iteration 219/1000 | Loss: 0.00001920
Iteration 220/1000 | Loss: 0.00001920
Iteration 221/1000 | Loss: 0.00001920
Iteration 222/1000 | Loss: 0.00001920
Iteration 223/1000 | Loss: 0.00001920
Iteration 224/1000 | Loss: 0.00001920
Iteration 225/1000 | Loss: 0.00001920
Iteration 226/1000 | Loss: 0.00001920
Iteration 227/1000 | Loss: 0.00001920
Iteration 228/1000 | Loss: 0.00001920
Iteration 229/1000 | Loss: 0.00001920
Iteration 230/1000 | Loss: 0.00001920
Iteration 231/1000 | Loss: 0.00001920
Iteration 232/1000 | Loss: 0.00001920
Iteration 233/1000 | Loss: 0.00001920
Iteration 234/1000 | Loss: 0.00001920
Iteration 235/1000 | Loss: 0.00001920
Iteration 236/1000 | Loss: 0.00001920
Iteration 237/1000 | Loss: 0.00001920
Iteration 238/1000 | Loss: 0.00001920
Iteration 239/1000 | Loss: 0.00001920
Iteration 240/1000 | Loss: 0.00001920
Iteration 241/1000 | Loss: 0.00001920
Iteration 242/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.919615169754252e-05, 1.919615169754252e-05, 1.919615169754252e-05, 1.919615169754252e-05, 1.919615169754252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.919615169754252e-05

Optimization complete. Final v2v error: 3.394568920135498 mm

Highest mean error: 11.476173400878906 mm for frame 5

Lowest mean error: 2.7802734375 mm for frame 84

Saving results

Total time: 97.78978371620178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486803
Iteration 2/25 | Loss: 0.00112831
Iteration 3/25 | Loss: 0.00104635
Iteration 4/25 | Loss: 0.00103524
Iteration 5/25 | Loss: 0.00103129
Iteration 6/25 | Loss: 0.00103070
Iteration 7/25 | Loss: 0.00103070
Iteration 8/25 | Loss: 0.00103070
Iteration 9/25 | Loss: 0.00103070
Iteration 10/25 | Loss: 0.00103070
Iteration 11/25 | Loss: 0.00103070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010306950425729156, 0.0010306950425729156, 0.0010306950425729156, 0.0010306950425729156, 0.0010306950425729156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010306950425729156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36884582
Iteration 2/25 | Loss: 0.00076959
Iteration 3/25 | Loss: 0.00076958
Iteration 4/25 | Loss: 0.00076958
Iteration 5/25 | Loss: 0.00076958
Iteration 6/25 | Loss: 0.00076958
Iteration 7/25 | Loss: 0.00076958
Iteration 8/25 | Loss: 0.00076958
Iteration 9/25 | Loss: 0.00076958
Iteration 10/25 | Loss: 0.00076958
Iteration 11/25 | Loss: 0.00076958
Iteration 12/25 | Loss: 0.00076958
Iteration 13/25 | Loss: 0.00076958
Iteration 14/25 | Loss: 0.00076958
Iteration 15/25 | Loss: 0.00076958
Iteration 16/25 | Loss: 0.00076958
Iteration 17/25 | Loss: 0.00076958
Iteration 18/25 | Loss: 0.00076958
Iteration 19/25 | Loss: 0.00076958
Iteration 20/25 | Loss: 0.00076958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007695791427977383, 0.0007695791427977383, 0.0007695791427977383, 0.0007695791427977383, 0.0007695791427977383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007695791427977383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076958
Iteration 2/1000 | Loss: 0.00004254
Iteration 3/1000 | Loss: 0.00002889
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002258
Iteration 7/1000 | Loss: 0.00002208
Iteration 8/1000 | Loss: 0.00002160
Iteration 9/1000 | Loss: 0.00002131
Iteration 10/1000 | Loss: 0.00002105
Iteration 11/1000 | Loss: 0.00002086
Iteration 12/1000 | Loss: 0.00002086
Iteration 13/1000 | Loss: 0.00002076
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002073
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002073
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002062
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002054
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00002051
Iteration 30/1000 | Loss: 0.00002051
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00002050
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002049
Iteration 35/1000 | Loss: 0.00002049
Iteration 36/1000 | Loss: 0.00002049
Iteration 37/1000 | Loss: 0.00002048
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002046
Iteration 44/1000 | Loss: 0.00002045
Iteration 45/1000 | Loss: 0.00002045
Iteration 46/1000 | Loss: 0.00002045
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002042
Iteration 52/1000 | Loss: 0.00002041
Iteration 53/1000 | Loss: 0.00002041
Iteration 54/1000 | Loss: 0.00002041
Iteration 55/1000 | Loss: 0.00002040
Iteration 56/1000 | Loss: 0.00002040
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002038
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002032
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002027
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002026
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002025
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002024
Iteration 102/1000 | Loss: 0.00002024
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002023
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002021
Iteration 111/1000 | Loss: 0.00002021
Iteration 112/1000 | Loss: 0.00002021
Iteration 113/1000 | Loss: 0.00002020
Iteration 114/1000 | Loss: 0.00002020
Iteration 115/1000 | Loss: 0.00002020
Iteration 116/1000 | Loss: 0.00002020
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002019
Iteration 119/1000 | Loss: 0.00002019
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002018
Iteration 124/1000 | Loss: 0.00002018
Iteration 125/1000 | Loss: 0.00002017
Iteration 126/1000 | Loss: 0.00002017
Iteration 127/1000 | Loss: 0.00002017
Iteration 128/1000 | Loss: 0.00002017
Iteration 129/1000 | Loss: 0.00002017
Iteration 130/1000 | Loss: 0.00002017
Iteration 131/1000 | Loss: 0.00002017
Iteration 132/1000 | Loss: 0.00002017
Iteration 133/1000 | Loss: 0.00002017
Iteration 134/1000 | Loss: 0.00002017
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002017
Iteration 138/1000 | Loss: 0.00002017
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002016
Iteration 146/1000 | Loss: 0.00002016
Iteration 147/1000 | Loss: 0.00002016
Iteration 148/1000 | Loss: 0.00002016
Iteration 149/1000 | Loss: 0.00002016
Iteration 150/1000 | Loss: 0.00002016
Iteration 151/1000 | Loss: 0.00002015
Iteration 152/1000 | Loss: 0.00002015
Iteration 153/1000 | Loss: 0.00002015
Iteration 154/1000 | Loss: 0.00002015
Iteration 155/1000 | Loss: 0.00002015
Iteration 156/1000 | Loss: 0.00002015
Iteration 157/1000 | Loss: 0.00002015
Iteration 158/1000 | Loss: 0.00002015
Iteration 159/1000 | Loss: 0.00002015
Iteration 160/1000 | Loss: 0.00002015
Iteration 161/1000 | Loss: 0.00002015
Iteration 162/1000 | Loss: 0.00002014
Iteration 163/1000 | Loss: 0.00002014
Iteration 164/1000 | Loss: 0.00002014
Iteration 165/1000 | Loss: 0.00002014
Iteration 166/1000 | Loss: 0.00002014
Iteration 167/1000 | Loss: 0.00002014
Iteration 168/1000 | Loss: 0.00002014
Iteration 169/1000 | Loss: 0.00002014
Iteration 170/1000 | Loss: 0.00002014
Iteration 171/1000 | Loss: 0.00002014
Iteration 172/1000 | Loss: 0.00002014
Iteration 173/1000 | Loss: 0.00002014
Iteration 174/1000 | Loss: 0.00002014
Iteration 175/1000 | Loss: 0.00002013
Iteration 176/1000 | Loss: 0.00002013
Iteration 177/1000 | Loss: 0.00002013
Iteration 178/1000 | Loss: 0.00002013
Iteration 179/1000 | Loss: 0.00002012
Iteration 180/1000 | Loss: 0.00002012
Iteration 181/1000 | Loss: 0.00002012
Iteration 182/1000 | Loss: 0.00002012
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002011
Iteration 185/1000 | Loss: 0.00002011
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002011
Iteration 188/1000 | Loss: 0.00002011
Iteration 189/1000 | Loss: 0.00002011
Iteration 190/1000 | Loss: 0.00002011
Iteration 191/1000 | Loss: 0.00002011
Iteration 192/1000 | Loss: 0.00002011
Iteration 193/1000 | Loss: 0.00002011
Iteration 194/1000 | Loss: 0.00002011
Iteration 195/1000 | Loss: 0.00002011
Iteration 196/1000 | Loss: 0.00002010
Iteration 197/1000 | Loss: 0.00002010
Iteration 198/1000 | Loss: 0.00002010
Iteration 199/1000 | Loss: 0.00002010
Iteration 200/1000 | Loss: 0.00002010
Iteration 201/1000 | Loss: 0.00002010
Iteration 202/1000 | Loss: 0.00002010
Iteration 203/1000 | Loss: 0.00002010
Iteration 204/1000 | Loss: 0.00002010
Iteration 205/1000 | Loss: 0.00002010
Iteration 206/1000 | Loss: 0.00002010
Iteration 207/1000 | Loss: 0.00002009
Iteration 208/1000 | Loss: 0.00002009
Iteration 209/1000 | Loss: 0.00002009
Iteration 210/1000 | Loss: 0.00002009
Iteration 211/1000 | Loss: 0.00002009
Iteration 212/1000 | Loss: 0.00002009
Iteration 213/1000 | Loss: 0.00002009
Iteration 214/1000 | Loss: 0.00002009
Iteration 215/1000 | Loss: 0.00002009
Iteration 216/1000 | Loss: 0.00002009
Iteration 217/1000 | Loss: 0.00002009
Iteration 218/1000 | Loss: 0.00002009
Iteration 219/1000 | Loss: 0.00002009
Iteration 220/1000 | Loss: 0.00002008
Iteration 221/1000 | Loss: 0.00002008
Iteration 222/1000 | Loss: 0.00002008
Iteration 223/1000 | Loss: 0.00002008
Iteration 224/1000 | Loss: 0.00002008
Iteration 225/1000 | Loss: 0.00002008
Iteration 226/1000 | Loss: 0.00002008
Iteration 227/1000 | Loss: 0.00002008
Iteration 228/1000 | Loss: 0.00002008
Iteration 229/1000 | Loss: 0.00002008
Iteration 230/1000 | Loss: 0.00002008
Iteration 231/1000 | Loss: 0.00002008
Iteration 232/1000 | Loss: 0.00002008
Iteration 233/1000 | Loss: 0.00002008
Iteration 234/1000 | Loss: 0.00002007
Iteration 235/1000 | Loss: 0.00002007
Iteration 236/1000 | Loss: 0.00002007
Iteration 237/1000 | Loss: 0.00002007
Iteration 238/1000 | Loss: 0.00002007
Iteration 239/1000 | Loss: 0.00002007
Iteration 240/1000 | Loss: 0.00002007
Iteration 241/1000 | Loss: 0.00002007
Iteration 242/1000 | Loss: 0.00002007
Iteration 243/1000 | Loss: 0.00002007
Iteration 244/1000 | Loss: 0.00002006
Iteration 245/1000 | Loss: 0.00002006
Iteration 246/1000 | Loss: 0.00002006
Iteration 247/1000 | Loss: 0.00002006
Iteration 248/1000 | Loss: 0.00002006
Iteration 249/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.0064806449227035e-05, 2.0064806449227035e-05, 2.0064806449227035e-05, 2.0064806449227035e-05, 2.0064806449227035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0064806449227035e-05

Optimization complete. Final v2v error: 3.3930439949035645 mm

Highest mean error: 4.446129322052002 mm for frame 95

Lowest mean error: 2.8711140155792236 mm for frame 134

Saving results

Total time: 47.85404014587402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856565
Iteration 2/25 | Loss: 0.00236309
Iteration 3/25 | Loss: 0.00163341
Iteration 4/25 | Loss: 0.00147905
Iteration 5/25 | Loss: 0.00140300
Iteration 6/25 | Loss: 0.00128721
Iteration 7/25 | Loss: 0.00122902
Iteration 8/25 | Loss: 0.00119369
Iteration 9/25 | Loss: 0.00117092
Iteration 10/25 | Loss: 0.00114914
Iteration 11/25 | Loss: 0.00113901
Iteration 12/25 | Loss: 0.00113141
Iteration 13/25 | Loss: 0.00112511
Iteration 14/25 | Loss: 0.00112098
Iteration 15/25 | Loss: 0.00111740
Iteration 16/25 | Loss: 0.00111625
Iteration 17/25 | Loss: 0.00111546
Iteration 18/25 | Loss: 0.00111524
Iteration 19/25 | Loss: 0.00111516
Iteration 20/25 | Loss: 0.00111515
Iteration 21/25 | Loss: 0.00111515
Iteration 22/25 | Loss: 0.00111515
Iteration 23/25 | Loss: 0.00111514
Iteration 24/25 | Loss: 0.00111514
Iteration 25/25 | Loss: 0.00111514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04112089
Iteration 2/25 | Loss: 0.00057044
Iteration 3/25 | Loss: 0.00057044
Iteration 4/25 | Loss: 0.00057044
Iteration 5/25 | Loss: 0.00057044
Iteration 6/25 | Loss: 0.00057044
Iteration 7/25 | Loss: 0.00057044
Iteration 8/25 | Loss: 0.00057044
Iteration 9/25 | Loss: 0.00057044
Iteration 10/25 | Loss: 0.00057044
Iteration 11/25 | Loss: 0.00057044
Iteration 12/25 | Loss: 0.00057044
Iteration 13/25 | Loss: 0.00057044
Iteration 14/25 | Loss: 0.00057044
Iteration 15/25 | Loss: 0.00057044
Iteration 16/25 | Loss: 0.00057044
Iteration 17/25 | Loss: 0.00057044
Iteration 18/25 | Loss: 0.00057044
Iteration 19/25 | Loss: 0.00057044
Iteration 20/25 | Loss: 0.00057044
Iteration 21/25 | Loss: 0.00057044
Iteration 22/25 | Loss: 0.00057044
Iteration 23/25 | Loss: 0.00057044
Iteration 24/25 | Loss: 0.00057044
Iteration 25/25 | Loss: 0.00057044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057044
Iteration 2/1000 | Loss: 0.00004302
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002169
Iteration 7/1000 | Loss: 0.00002092
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001944
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001856
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001845
Iteration 23/1000 | Loss: 0.00001844
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001836
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001833
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001831
Iteration 40/1000 | Loss: 0.00001829
Iteration 41/1000 | Loss: 0.00001829
Iteration 42/1000 | Loss: 0.00001829
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001827
Iteration 58/1000 | Loss: 0.00001827
Iteration 59/1000 | Loss: 0.00001826
Iteration 60/1000 | Loss: 0.00001826
Iteration 61/1000 | Loss: 0.00001826
Iteration 62/1000 | Loss: 0.00001826
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001826
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001825
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001824
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.8244485545437783e-05, 1.8244485545437783e-05, 1.8244485545437783e-05, 1.8244485545437783e-05, 1.8244485545437783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8244485545437783e-05

Optimization complete. Final v2v error: 3.537548065185547 mm

Highest mean error: 4.432300090789795 mm for frame 99

Lowest mean error: 3.0770206451416016 mm for frame 210

Saving results

Total time: 71.38406896591187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409345
Iteration 2/25 | Loss: 0.00113554
Iteration 3/25 | Loss: 0.00103306
Iteration 4/25 | Loss: 0.00101581
Iteration 5/25 | Loss: 0.00101075
Iteration 6/25 | Loss: 0.00100989
Iteration 7/25 | Loss: 0.00100989
Iteration 8/25 | Loss: 0.00100989
Iteration 9/25 | Loss: 0.00100989
Iteration 10/25 | Loss: 0.00100989
Iteration 11/25 | Loss: 0.00100989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010098936036229134, 0.0010098936036229134, 0.0010098936036229134, 0.0010098936036229134, 0.0010098936036229134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010098936036229134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41084099
Iteration 2/25 | Loss: 0.00071699
Iteration 3/25 | Loss: 0.00071699
Iteration 4/25 | Loss: 0.00071699
Iteration 5/25 | Loss: 0.00071699
Iteration 6/25 | Loss: 0.00071699
Iteration 7/25 | Loss: 0.00071699
Iteration 8/25 | Loss: 0.00071699
Iteration 9/25 | Loss: 0.00071699
Iteration 10/25 | Loss: 0.00071699
Iteration 11/25 | Loss: 0.00071699
Iteration 12/25 | Loss: 0.00071699
Iteration 13/25 | Loss: 0.00071699
Iteration 14/25 | Loss: 0.00071699
Iteration 15/25 | Loss: 0.00071699
Iteration 16/25 | Loss: 0.00071699
Iteration 17/25 | Loss: 0.00071699
Iteration 18/25 | Loss: 0.00071699
Iteration 19/25 | Loss: 0.00071699
Iteration 20/25 | Loss: 0.00071699
Iteration 21/25 | Loss: 0.00071699
Iteration 22/25 | Loss: 0.00071699
Iteration 23/25 | Loss: 0.00071699
Iteration 24/25 | Loss: 0.00071699
Iteration 25/25 | Loss: 0.00071699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071699
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001689
Iteration 7/1000 | Loss: 0.00001633
Iteration 8/1000 | Loss: 0.00001591
Iteration 9/1000 | Loss: 0.00001579
Iteration 10/1000 | Loss: 0.00001554
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001507
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001505
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001490
Iteration 42/1000 | Loss: 0.00001490
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001477
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001476
Iteration 73/1000 | Loss: 0.00001476
Iteration 74/1000 | Loss: 0.00001476
Iteration 75/1000 | Loss: 0.00001476
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00001471
Iteration 103/1000 | Loss: 0.00001471
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001471
Iteration 106/1000 | Loss: 0.00001471
Iteration 107/1000 | Loss: 0.00001471
Iteration 108/1000 | Loss: 0.00001471
Iteration 109/1000 | Loss: 0.00001471
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.4712250049342401e-05, 1.4712250049342401e-05, 1.4712250049342401e-05, 1.4712250049342401e-05, 1.4712250049342401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4712250049342401e-05

Optimization complete. Final v2v error: 3.2408955097198486 mm

Highest mean error: 3.7184245586395264 mm for frame 13

Lowest mean error: 2.9960758686065674 mm for frame 33

Saving results

Total time: 35.583133935928345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996801
Iteration 2/25 | Loss: 0.00208689
Iteration 3/25 | Loss: 0.00153346
Iteration 4/25 | Loss: 0.00138231
Iteration 5/25 | Loss: 0.00152275
Iteration 6/25 | Loss: 0.00145306
Iteration 7/25 | Loss: 0.00128775
Iteration 8/25 | Loss: 0.00116849
Iteration 9/25 | Loss: 0.00111805
Iteration 10/25 | Loss: 0.00108619
Iteration 11/25 | Loss: 0.00107123
Iteration 12/25 | Loss: 0.00105366
Iteration 13/25 | Loss: 0.00104348
Iteration 14/25 | Loss: 0.00106780
Iteration 15/25 | Loss: 0.00104154
Iteration 16/25 | Loss: 0.00103173
Iteration 17/25 | Loss: 0.00102896
Iteration 18/25 | Loss: 0.00102444
Iteration 19/25 | Loss: 0.00101958
Iteration 20/25 | Loss: 0.00101961
Iteration 21/25 | Loss: 0.00102096
Iteration 22/25 | Loss: 0.00101693
Iteration 23/25 | Loss: 0.00101652
Iteration 24/25 | Loss: 0.00101642
Iteration 25/25 | Loss: 0.00101640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48645985
Iteration 2/25 | Loss: 0.00088162
Iteration 3/25 | Loss: 0.00088162
Iteration 4/25 | Loss: 0.00088161
Iteration 5/25 | Loss: 0.00088161
Iteration 6/25 | Loss: 0.00088161
Iteration 7/25 | Loss: 0.00088161
Iteration 8/25 | Loss: 0.00088161
Iteration 9/25 | Loss: 0.00088161
Iteration 10/25 | Loss: 0.00088161
Iteration 11/25 | Loss: 0.00088161
Iteration 12/25 | Loss: 0.00088161
Iteration 13/25 | Loss: 0.00088161
Iteration 14/25 | Loss: 0.00088161
Iteration 15/25 | Loss: 0.00088161
Iteration 16/25 | Loss: 0.00088161
Iteration 17/25 | Loss: 0.00088161
Iteration 18/25 | Loss: 0.00088161
Iteration 19/25 | Loss: 0.00088161
Iteration 20/25 | Loss: 0.00088161
Iteration 21/25 | Loss: 0.00088161
Iteration 22/25 | Loss: 0.00088161
Iteration 23/25 | Loss: 0.00088161
Iteration 24/25 | Loss: 0.00088161
Iteration 25/25 | Loss: 0.00088161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088161
Iteration 2/1000 | Loss: 0.00156049
Iteration 3/1000 | Loss: 0.00009741
Iteration 4/1000 | Loss: 0.00024619
Iteration 5/1000 | Loss: 0.00005950
Iteration 6/1000 | Loss: 0.00085036
Iteration 7/1000 | Loss: 0.00028043
Iteration 8/1000 | Loss: 0.00048097
Iteration 9/1000 | Loss: 0.00037880
Iteration 10/1000 | Loss: 0.00017902
Iteration 11/1000 | Loss: 0.00012235
Iteration 12/1000 | Loss: 0.00017343
Iteration 13/1000 | Loss: 0.00012515
Iteration 14/1000 | Loss: 0.00005048
Iteration 15/1000 | Loss: 0.00004047
Iteration 16/1000 | Loss: 0.00049595
Iteration 17/1000 | Loss: 0.00006282
Iteration 18/1000 | Loss: 0.00025641
Iteration 19/1000 | Loss: 0.00021318
Iteration 20/1000 | Loss: 0.00023981
Iteration 21/1000 | Loss: 0.00019089
Iteration 22/1000 | Loss: 0.00049483
Iteration 23/1000 | Loss: 0.00027779
Iteration 24/1000 | Loss: 0.00031898
Iteration 25/1000 | Loss: 0.00013802
Iteration 26/1000 | Loss: 0.00023909
Iteration 27/1000 | Loss: 0.00020230
Iteration 28/1000 | Loss: 0.00024016
Iteration 29/1000 | Loss: 0.00019611
Iteration 30/1000 | Loss: 0.00038779
Iteration 31/1000 | Loss: 0.00029501
Iteration 32/1000 | Loss: 0.00066536
Iteration 33/1000 | Loss: 0.00051105
Iteration 34/1000 | Loss: 0.00003561
Iteration 35/1000 | Loss: 0.00023152
Iteration 36/1000 | Loss: 0.00019855
Iteration 37/1000 | Loss: 0.00010369
Iteration 38/1000 | Loss: 0.00004751
Iteration 39/1000 | Loss: 0.00020495
Iteration 40/1000 | Loss: 0.00010124
Iteration 41/1000 | Loss: 0.00123350
Iteration 42/1000 | Loss: 0.00095578
Iteration 43/1000 | Loss: 0.00109063
Iteration 44/1000 | Loss: 0.00053739
Iteration 45/1000 | Loss: 0.00024794
Iteration 46/1000 | Loss: 0.00047515
Iteration 47/1000 | Loss: 0.00008141
Iteration 48/1000 | Loss: 0.00018329
Iteration 49/1000 | Loss: 0.00037040
Iteration 50/1000 | Loss: 0.00121441
Iteration 51/1000 | Loss: 0.00035754
Iteration 52/1000 | Loss: 0.00019252
Iteration 53/1000 | Loss: 0.00014572
Iteration 54/1000 | Loss: 0.00100961
Iteration 55/1000 | Loss: 0.00022431
Iteration 56/1000 | Loss: 0.00086247
Iteration 57/1000 | Loss: 0.00028800
Iteration 58/1000 | Loss: 0.00064378
Iteration 59/1000 | Loss: 0.00005764
Iteration 60/1000 | Loss: 0.00057522
Iteration 61/1000 | Loss: 0.00002824
Iteration 62/1000 | Loss: 0.00017943
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002143
Iteration 65/1000 | Loss: 0.00005863
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00040021
Iteration 74/1000 | Loss: 0.00003273
Iteration 75/1000 | Loss: 0.00002549
Iteration 76/1000 | Loss: 0.00006995
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00004211
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00005486
Iteration 85/1000 | Loss: 0.00023801
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00002625
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001307
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001306
Iteration 113/1000 | Loss: 0.00001306
Iteration 114/1000 | Loss: 0.00001306
Iteration 115/1000 | Loss: 0.00001306
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001297
Iteration 132/1000 | Loss: 0.00001297
Iteration 133/1000 | Loss: 0.00001297
Iteration 134/1000 | Loss: 0.00001296
Iteration 135/1000 | Loss: 0.00001295
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001285
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001285
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001282
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001282
Iteration 165/1000 | Loss: 0.00001282
Iteration 166/1000 | Loss: 0.00001282
Iteration 167/1000 | Loss: 0.00001281
Iteration 168/1000 | Loss: 0.00001281
Iteration 169/1000 | Loss: 0.00001281
Iteration 170/1000 | Loss: 0.00001281
Iteration 171/1000 | Loss: 0.00001281
Iteration 172/1000 | Loss: 0.00001281
Iteration 173/1000 | Loss: 0.00001281
Iteration 174/1000 | Loss: 0.00001281
Iteration 175/1000 | Loss: 0.00001281
Iteration 176/1000 | Loss: 0.00001281
Iteration 177/1000 | Loss: 0.00001280
Iteration 178/1000 | Loss: 0.00001280
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001280
Iteration 186/1000 | Loss: 0.00001279
Iteration 187/1000 | Loss: 0.00001279
Iteration 188/1000 | Loss: 0.00001279
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001278
Iteration 191/1000 | Loss: 0.00001277
Iteration 192/1000 | Loss: 0.00001277
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001276
Iteration 195/1000 | Loss: 0.00001276
Iteration 196/1000 | Loss: 0.00001276
Iteration 197/1000 | Loss: 0.00001275
Iteration 198/1000 | Loss: 0.00001274
Iteration 199/1000 | Loss: 0.00001274
Iteration 200/1000 | Loss: 0.00001273
Iteration 201/1000 | Loss: 0.00001273
Iteration 202/1000 | Loss: 0.00001272
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001270
Iteration 205/1000 | Loss: 0.00001270
Iteration 206/1000 | Loss: 0.00001269
Iteration 207/1000 | Loss: 0.00001269
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001268
Iteration 211/1000 | Loss: 0.00001268
Iteration 212/1000 | Loss: 0.00001268
Iteration 213/1000 | Loss: 0.00001267
Iteration 214/1000 | Loss: 0.00027743
Iteration 215/1000 | Loss: 0.00099213
Iteration 216/1000 | Loss: 0.00031887
Iteration 217/1000 | Loss: 0.00027263
Iteration 218/1000 | Loss: 0.00022291
Iteration 219/1000 | Loss: 0.00004273
Iteration 220/1000 | Loss: 0.00003425
Iteration 221/1000 | Loss: 0.00033615
Iteration 222/1000 | Loss: 0.00007216
Iteration 223/1000 | Loss: 0.00053601
Iteration 224/1000 | Loss: 0.00013061
Iteration 225/1000 | Loss: 0.00025926
Iteration 226/1000 | Loss: 0.00002972
Iteration 227/1000 | Loss: 0.00002398
Iteration 228/1000 | Loss: 0.00001958
Iteration 229/1000 | Loss: 0.00001652
Iteration 230/1000 | Loss: 0.00001540
Iteration 231/1000 | Loss: 0.00002275
Iteration 232/1000 | Loss: 0.00001768
Iteration 233/1000 | Loss: 0.00001553
Iteration 234/1000 | Loss: 0.00001455
Iteration 235/1000 | Loss: 0.00001425
Iteration 236/1000 | Loss: 0.00001405
Iteration 237/1000 | Loss: 0.00001393
Iteration 238/1000 | Loss: 0.00001393
Iteration 239/1000 | Loss: 0.00001391
Iteration 240/1000 | Loss: 0.00001391
Iteration 241/1000 | Loss: 0.00001389
Iteration 242/1000 | Loss: 0.00001388
Iteration 243/1000 | Loss: 0.00001387
Iteration 244/1000 | Loss: 0.00001387
Iteration 245/1000 | Loss: 0.00001387
Iteration 246/1000 | Loss: 0.00001386
Iteration 247/1000 | Loss: 0.00001385
Iteration 248/1000 | Loss: 0.00001385
Iteration 249/1000 | Loss: 0.00001384
Iteration 250/1000 | Loss: 0.00001383
Iteration 251/1000 | Loss: 0.00001383
Iteration 252/1000 | Loss: 0.00001382
Iteration 253/1000 | Loss: 0.00001382
Iteration 254/1000 | Loss: 0.00001381
Iteration 255/1000 | Loss: 0.00001380
Iteration 256/1000 | Loss: 0.00001380
Iteration 257/1000 | Loss: 0.00001379
Iteration 258/1000 | Loss: 0.00001379
Iteration 259/1000 | Loss: 0.00001379
Iteration 260/1000 | Loss: 0.00001379
Iteration 261/1000 | Loss: 0.00001379
Iteration 262/1000 | Loss: 0.00001379
Iteration 263/1000 | Loss: 0.00001379
Iteration 264/1000 | Loss: 0.00001379
Iteration 265/1000 | Loss: 0.00001379
Iteration 266/1000 | Loss: 0.00001379
Iteration 267/1000 | Loss: 0.00001378
Iteration 268/1000 | Loss: 0.00001378
Iteration 269/1000 | Loss: 0.00001378
Iteration 270/1000 | Loss: 0.00001378
Iteration 271/1000 | Loss: 0.00001378
Iteration 272/1000 | Loss: 0.00001376
Iteration 273/1000 | Loss: 0.00001376
Iteration 274/1000 | Loss: 0.00001375
Iteration 275/1000 | Loss: 0.00001375
Iteration 276/1000 | Loss: 0.00001374
Iteration 277/1000 | Loss: 0.00001373
Iteration 278/1000 | Loss: 0.00001373
Iteration 279/1000 | Loss: 0.00001373
Iteration 280/1000 | Loss: 0.00001372
Iteration 281/1000 | Loss: 0.00001372
Iteration 282/1000 | Loss: 0.00001371
Iteration 283/1000 | Loss: 0.00001371
Iteration 284/1000 | Loss: 0.00001370
Iteration 285/1000 | Loss: 0.00001370
Iteration 286/1000 | Loss: 0.00001370
Iteration 287/1000 | Loss: 0.00001369
Iteration 288/1000 | Loss: 0.00001369
Iteration 289/1000 | Loss: 0.00001368
Iteration 290/1000 | Loss: 0.00001368
Iteration 291/1000 | Loss: 0.00001367
Iteration 292/1000 | Loss: 0.00001367
Iteration 293/1000 | Loss: 0.00001366
Iteration 294/1000 | Loss: 0.00001365
Iteration 295/1000 | Loss: 0.00001365
Iteration 296/1000 | Loss: 0.00001365
Iteration 297/1000 | Loss: 0.00001364
Iteration 298/1000 | Loss: 0.00001364
Iteration 299/1000 | Loss: 0.00001364
Iteration 300/1000 | Loss: 0.00001363
Iteration 301/1000 | Loss: 0.00001363
Iteration 302/1000 | Loss: 0.00001363
Iteration 303/1000 | Loss: 0.00001363
Iteration 304/1000 | Loss: 0.00001363
Iteration 305/1000 | Loss: 0.00001363
Iteration 306/1000 | Loss: 0.00001362
Iteration 307/1000 | Loss: 0.00001361
Iteration 308/1000 | Loss: 0.00001361
Iteration 309/1000 | Loss: 0.00001361
Iteration 310/1000 | Loss: 0.00001360
Iteration 311/1000 | Loss: 0.00001360
Iteration 312/1000 | Loss: 0.00001359
Iteration 313/1000 | Loss: 0.00001359
Iteration 314/1000 | Loss: 0.00001359
Iteration 315/1000 | Loss: 0.00001358
Iteration 316/1000 | Loss: 0.00001358
Iteration 317/1000 | Loss: 0.00001358
Iteration 318/1000 | Loss: 0.00001358
Iteration 319/1000 | Loss: 0.00001357
Iteration 320/1000 | Loss: 0.00001357
Iteration 321/1000 | Loss: 0.00001357
Iteration 322/1000 | Loss: 0.00001357
Iteration 323/1000 | Loss: 0.00001357
Iteration 324/1000 | Loss: 0.00001356
Iteration 325/1000 | Loss: 0.00001356
Iteration 326/1000 | Loss: 0.00001356
Iteration 327/1000 | Loss: 0.00001356
Iteration 328/1000 | Loss: 0.00001356
Iteration 329/1000 | Loss: 0.00001356
Iteration 330/1000 | Loss: 0.00001355
Iteration 331/1000 | Loss: 0.00001355
Iteration 332/1000 | Loss: 0.00001355
Iteration 333/1000 | Loss: 0.00001355
Iteration 334/1000 | Loss: 0.00001355
Iteration 335/1000 | Loss: 0.00001355
Iteration 336/1000 | Loss: 0.00001355
Iteration 337/1000 | Loss: 0.00001355
Iteration 338/1000 | Loss: 0.00001355
Iteration 339/1000 | Loss: 0.00001355
Iteration 340/1000 | Loss: 0.00001355
Iteration 341/1000 | Loss: 0.00001355
Iteration 342/1000 | Loss: 0.00001355
Iteration 343/1000 | Loss: 0.00001355
Iteration 344/1000 | Loss: 0.00001355
Iteration 345/1000 | Loss: 0.00001355
Iteration 346/1000 | Loss: 0.00001355
Iteration 347/1000 | Loss: 0.00001355
Iteration 348/1000 | Loss: 0.00001355
Iteration 349/1000 | Loss: 0.00001355
Iteration 350/1000 | Loss: 0.00001355
Iteration 351/1000 | Loss: 0.00001355
Iteration 352/1000 | Loss: 0.00001355
Iteration 353/1000 | Loss: 0.00001355
Iteration 354/1000 | Loss: 0.00001355
Iteration 355/1000 | Loss: 0.00001355
Iteration 356/1000 | Loss: 0.00001355
Iteration 357/1000 | Loss: 0.00001355
Iteration 358/1000 | Loss: 0.00001355
Iteration 359/1000 | Loss: 0.00001355
Iteration 360/1000 | Loss: 0.00001355
Iteration 361/1000 | Loss: 0.00001355
Iteration 362/1000 | Loss: 0.00001355
Iteration 363/1000 | Loss: 0.00001355
Iteration 364/1000 | Loss: 0.00001355
Iteration 365/1000 | Loss: 0.00001355
Iteration 366/1000 | Loss: 0.00001355
Iteration 367/1000 | Loss: 0.00001355
Iteration 368/1000 | Loss: 0.00001355
Iteration 369/1000 | Loss: 0.00001355
Iteration 370/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 370. Stopping optimization.
Last 5 losses: [1.3548774404625874e-05, 1.3548774404625874e-05, 1.3548774404625874e-05, 1.3548774404625874e-05, 1.3548774404625874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3548774404625874e-05

Optimization complete. Final v2v error: 2.781376361846924 mm

Highest mean error: 9.468338966369629 mm for frame 110

Lowest mean error: 2.3077642917633057 mm for frame 51

Saving results

Total time: 233.9218831062317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743643
Iteration 2/25 | Loss: 0.00139562
Iteration 3/25 | Loss: 0.00115236
Iteration 4/25 | Loss: 0.00111518
Iteration 5/25 | Loss: 0.00110333
Iteration 6/25 | Loss: 0.00109798
Iteration 7/25 | Loss: 0.00109358
Iteration 8/25 | Loss: 0.00109348
Iteration 9/25 | Loss: 0.00109344
Iteration 10/25 | Loss: 0.00109344
Iteration 11/25 | Loss: 0.00109344
Iteration 12/25 | Loss: 0.00109344
Iteration 13/25 | Loss: 0.00109343
Iteration 14/25 | Loss: 0.00109343
Iteration 15/25 | Loss: 0.00109343
Iteration 16/25 | Loss: 0.00109343
Iteration 17/25 | Loss: 0.00109343
Iteration 18/25 | Loss: 0.00109343
Iteration 19/25 | Loss: 0.00109343
Iteration 20/25 | Loss: 0.00109343
Iteration 21/25 | Loss: 0.00109343
Iteration 22/25 | Loss: 0.00109342
Iteration 23/25 | Loss: 0.00109342
Iteration 24/25 | Loss: 0.00109342
Iteration 25/25 | Loss: 0.00109342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.31990075
Iteration 2/25 | Loss: 0.00096841
Iteration 3/25 | Loss: 0.00094917
Iteration 4/25 | Loss: 0.00094916
Iteration 5/25 | Loss: 0.00094916
Iteration 6/25 | Loss: 0.00094916
Iteration 7/25 | Loss: 0.00094916
Iteration 8/25 | Loss: 0.00094916
Iteration 9/25 | Loss: 0.00094916
Iteration 10/25 | Loss: 0.00094916
Iteration 11/25 | Loss: 0.00094916
Iteration 12/25 | Loss: 0.00094916
Iteration 13/25 | Loss: 0.00094916
Iteration 14/25 | Loss: 0.00094916
Iteration 15/25 | Loss: 0.00094916
Iteration 16/25 | Loss: 0.00094916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009491631062701344, 0.0009491631062701344, 0.0009491631062701344, 0.0009491631062701344, 0.0009491631062701344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009491631062701344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094916
Iteration 2/1000 | Loss: 0.00002735
Iteration 3/1000 | Loss: 0.00002112
Iteration 4/1000 | Loss: 0.00001943
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001770
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001759
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001747
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001736
Iteration 38/1000 | Loss: 0.00001736
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001731
Iteration 56/1000 | Loss: 0.00001731
Iteration 57/1000 | Loss: 0.00001731
Iteration 58/1000 | Loss: 0.00001731
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001725
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001721
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001717
Iteration 114/1000 | Loss: 0.00001717
Iteration 115/1000 | Loss: 0.00001717
Iteration 116/1000 | Loss: 0.00001717
Iteration 117/1000 | Loss: 0.00001717
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001716
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001716
Iteration 125/1000 | Loss: 0.00001716
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001715
Iteration 133/1000 | Loss: 0.00001715
Iteration 134/1000 | Loss: 0.00001715
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001714
Iteration 137/1000 | Loss: 0.00001714
Iteration 138/1000 | Loss: 0.00001714
Iteration 139/1000 | Loss: 0.00001714
Iteration 140/1000 | Loss: 0.00001714
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001712
Iteration 153/1000 | Loss: 0.00001712
Iteration 154/1000 | Loss: 0.00001712
Iteration 155/1000 | Loss: 0.00001712
Iteration 156/1000 | Loss: 0.00001712
Iteration 157/1000 | Loss: 0.00001712
Iteration 158/1000 | Loss: 0.00001712
Iteration 159/1000 | Loss: 0.00001712
Iteration 160/1000 | Loss: 0.00001712
Iteration 161/1000 | Loss: 0.00001712
Iteration 162/1000 | Loss: 0.00001712
Iteration 163/1000 | Loss: 0.00001712
Iteration 164/1000 | Loss: 0.00001712
Iteration 165/1000 | Loss: 0.00001712
Iteration 166/1000 | Loss: 0.00001712
Iteration 167/1000 | Loss: 0.00001712
Iteration 168/1000 | Loss: 0.00001712
Iteration 169/1000 | Loss: 0.00001712
Iteration 170/1000 | Loss: 0.00001712
Iteration 171/1000 | Loss: 0.00001712
Iteration 172/1000 | Loss: 0.00001712
Iteration 173/1000 | Loss: 0.00001712
Iteration 174/1000 | Loss: 0.00001712
Iteration 175/1000 | Loss: 0.00001712
Iteration 176/1000 | Loss: 0.00001712
Iteration 177/1000 | Loss: 0.00001712
Iteration 178/1000 | Loss: 0.00001712
Iteration 179/1000 | Loss: 0.00001712
Iteration 180/1000 | Loss: 0.00001712
Iteration 181/1000 | Loss: 0.00001712
Iteration 182/1000 | Loss: 0.00001712
Iteration 183/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.7118965843110345e-05, 1.7118965843110345e-05, 1.7118965843110345e-05, 1.7118965843110345e-05, 1.7118965843110345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7118965843110345e-05

Optimization complete. Final v2v error: 3.4325947761535645 mm

Highest mean error: 3.8452813625335693 mm for frame 212

Lowest mean error: 2.7932698726654053 mm for frame 144

Saving results

Total time: 49.85610818862915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007205
Iteration 2/25 | Loss: 0.00178321
Iteration 3/25 | Loss: 0.00136399
Iteration 4/25 | Loss: 0.00126343
Iteration 5/25 | Loss: 0.00127751
Iteration 6/25 | Loss: 0.00125674
Iteration 7/25 | Loss: 0.00121535
Iteration 8/25 | Loss: 0.00115974
Iteration 9/25 | Loss: 0.00115126
Iteration 10/25 | Loss: 0.00113996
Iteration 11/25 | Loss: 0.00114501
Iteration 12/25 | Loss: 0.00113668
Iteration 13/25 | Loss: 0.00113384
Iteration 14/25 | Loss: 0.00112983
Iteration 15/25 | Loss: 0.00113289
Iteration 16/25 | Loss: 0.00113684
Iteration 17/25 | Loss: 0.00113745
Iteration 18/25 | Loss: 0.00113350
Iteration 19/25 | Loss: 0.00113417
Iteration 20/25 | Loss: 0.00112972
Iteration 21/25 | Loss: 0.00112352
Iteration 22/25 | Loss: 0.00112621
Iteration 23/25 | Loss: 0.00112686
Iteration 24/25 | Loss: 0.00112520
Iteration 25/25 | Loss: 0.00112182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92331934
Iteration 2/25 | Loss: 0.00092790
Iteration 3/25 | Loss: 0.00092789
Iteration 4/25 | Loss: 0.00092789
Iteration 5/25 | Loss: 0.00092789
Iteration 6/25 | Loss: 0.00092789
Iteration 7/25 | Loss: 0.00092789
Iteration 8/25 | Loss: 0.00092789
Iteration 9/25 | Loss: 0.00092789
Iteration 10/25 | Loss: 0.00092789
Iteration 11/25 | Loss: 0.00092789
Iteration 12/25 | Loss: 0.00092789
Iteration 13/25 | Loss: 0.00092789
Iteration 14/25 | Loss: 0.00092789
Iteration 15/25 | Loss: 0.00092789
Iteration 16/25 | Loss: 0.00092789
Iteration 17/25 | Loss: 0.00092789
Iteration 18/25 | Loss: 0.00092789
Iteration 19/25 | Loss: 0.00092789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000927888962905854, 0.000927888962905854, 0.000927888962905854, 0.000927888962905854, 0.000927888962905854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000927888962905854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092789
Iteration 2/1000 | Loss: 0.00006566
Iteration 3/1000 | Loss: 0.00016476
Iteration 4/1000 | Loss: 0.00012452
Iteration 5/1000 | Loss: 0.00013466
Iteration 6/1000 | Loss: 0.00005814
Iteration 7/1000 | Loss: 0.00031826
Iteration 8/1000 | Loss: 0.00012135
Iteration 9/1000 | Loss: 0.00024113
Iteration 10/1000 | Loss: 0.00018058
Iteration 11/1000 | Loss: 0.00013255
Iteration 12/1000 | Loss: 0.00021561
Iteration 13/1000 | Loss: 0.00024071
Iteration 14/1000 | Loss: 0.00014273
Iteration 15/1000 | Loss: 0.00028500
Iteration 16/1000 | Loss: 0.00023367
Iteration 17/1000 | Loss: 0.00015848
Iteration 18/1000 | Loss: 0.00018112
Iteration 19/1000 | Loss: 0.00028935
Iteration 20/1000 | Loss: 0.00018214
Iteration 21/1000 | Loss: 0.00020166
Iteration 22/1000 | Loss: 0.00024163
Iteration 23/1000 | Loss: 0.00027350
Iteration 24/1000 | Loss: 0.00021992
Iteration 25/1000 | Loss: 0.00028776
Iteration 26/1000 | Loss: 0.00015432
Iteration 27/1000 | Loss: 0.00020734
Iteration 28/1000 | Loss: 0.00022955
Iteration 29/1000 | Loss: 0.00018695
Iteration 30/1000 | Loss: 0.00032629
Iteration 31/1000 | Loss: 0.00023518
Iteration 32/1000 | Loss: 0.00014340
Iteration 33/1000 | Loss: 0.00015763
Iteration 34/1000 | Loss: 0.00018378
Iteration 35/1000 | Loss: 0.00028359
Iteration 36/1000 | Loss: 0.00011973
Iteration 37/1000 | Loss: 0.00031082
Iteration 38/1000 | Loss: 0.00019609
Iteration 39/1000 | Loss: 0.00025686
Iteration 40/1000 | Loss: 0.00023726
Iteration 41/1000 | Loss: 0.00023925
Iteration 42/1000 | Loss: 0.00023002
Iteration 43/1000 | Loss: 0.00024761
Iteration 44/1000 | Loss: 0.00032556
Iteration 45/1000 | Loss: 0.00021506
Iteration 46/1000 | Loss: 0.00016482
Iteration 47/1000 | Loss: 0.00013290
Iteration 48/1000 | Loss: 0.00025167
Iteration 49/1000 | Loss: 0.00025837
Iteration 50/1000 | Loss: 0.00033293
Iteration 51/1000 | Loss: 0.00024614
Iteration 52/1000 | Loss: 0.00014924
Iteration 53/1000 | Loss: 0.00014554
Iteration 54/1000 | Loss: 0.00015699
Iteration 55/1000 | Loss: 0.00008375
Iteration 56/1000 | Loss: 0.00014753
Iteration 57/1000 | Loss: 0.00043922
Iteration 58/1000 | Loss: 0.00005535
Iteration 59/1000 | Loss: 0.00020931
Iteration 60/1000 | Loss: 0.00009764
Iteration 61/1000 | Loss: 0.00020320
Iteration 62/1000 | Loss: 0.00009845
Iteration 63/1000 | Loss: 0.00019835
Iteration 64/1000 | Loss: 0.00019894
Iteration 65/1000 | Loss: 0.00018213
Iteration 66/1000 | Loss: 0.00010491
Iteration 67/1000 | Loss: 0.00019400
Iteration 68/1000 | Loss: 0.00026029
Iteration 69/1000 | Loss: 0.00023307
Iteration 70/1000 | Loss: 0.00043658
Iteration 71/1000 | Loss: 0.00023348
Iteration 72/1000 | Loss: 0.00021556
Iteration 73/1000 | Loss: 0.00011793
Iteration 74/1000 | Loss: 0.00021681
Iteration 75/1000 | Loss: 0.00021903
Iteration 76/1000 | Loss: 0.00017835
Iteration 77/1000 | Loss: 0.00017716
Iteration 78/1000 | Loss: 0.00011067
Iteration 79/1000 | Loss: 0.00030952
Iteration 80/1000 | Loss: 0.00057767
Iteration 81/1000 | Loss: 0.00032098
Iteration 82/1000 | Loss: 0.00015933
Iteration 83/1000 | Loss: 0.00016041
Iteration 84/1000 | Loss: 0.00013860
Iteration 85/1000 | Loss: 0.00030974
Iteration 86/1000 | Loss: 0.00021881
Iteration 87/1000 | Loss: 0.00023623
Iteration 88/1000 | Loss: 0.00069267
Iteration 89/1000 | Loss: 0.00018071
Iteration 90/1000 | Loss: 0.00016612
Iteration 91/1000 | Loss: 0.00017426
Iteration 92/1000 | Loss: 0.00016823
Iteration 93/1000 | Loss: 0.00010043
Iteration 94/1000 | Loss: 0.00038474
Iteration 95/1000 | Loss: 0.00019443
Iteration 96/1000 | Loss: 0.00009035
Iteration 97/1000 | Loss: 0.00011532
Iteration 98/1000 | Loss: 0.00015856
Iteration 99/1000 | Loss: 0.00007110
Iteration 100/1000 | Loss: 0.00021119
Iteration 101/1000 | Loss: 0.00017585
Iteration 102/1000 | Loss: 0.00019121
Iteration 103/1000 | Loss: 0.00013656
Iteration 104/1000 | Loss: 0.00009292
Iteration 105/1000 | Loss: 0.00016713
Iteration 106/1000 | Loss: 0.00016023
Iteration 107/1000 | Loss: 0.00009020
Iteration 108/1000 | Loss: 0.00010845
Iteration 109/1000 | Loss: 0.00016217
Iteration 110/1000 | Loss: 0.00014360
Iteration 111/1000 | Loss: 0.00016520
Iteration 112/1000 | Loss: 0.00016715
Iteration 113/1000 | Loss: 0.00017225
Iteration 114/1000 | Loss: 0.00017265
Iteration 115/1000 | Loss: 0.00011708
Iteration 116/1000 | Loss: 0.00006825
Iteration 117/1000 | Loss: 0.00009667
Iteration 118/1000 | Loss: 0.00007375
Iteration 119/1000 | Loss: 0.00006334
Iteration 120/1000 | Loss: 0.00007038
Iteration 121/1000 | Loss: 0.00014477
Iteration 122/1000 | Loss: 0.00011759
Iteration 123/1000 | Loss: 0.00010846
Iteration 124/1000 | Loss: 0.00011586
Iteration 125/1000 | Loss: 0.00011559
Iteration 126/1000 | Loss: 0.00017737
Iteration 127/1000 | Loss: 0.00013819
Iteration 128/1000 | Loss: 0.00013224
Iteration 129/1000 | Loss: 0.00016749
Iteration 130/1000 | Loss: 0.00020940
Iteration 131/1000 | Loss: 0.00025068
Iteration 132/1000 | Loss: 0.00018647
Iteration 133/1000 | Loss: 0.00025497
Iteration 134/1000 | Loss: 0.00023681
Iteration 135/1000 | Loss: 0.00018943
Iteration 136/1000 | Loss: 0.00016059
Iteration 137/1000 | Loss: 0.00023986
Iteration 138/1000 | Loss: 0.00022996
Iteration 139/1000 | Loss: 0.00030112
Iteration 140/1000 | Loss: 0.00020015
Iteration 141/1000 | Loss: 0.00006321
Iteration 142/1000 | Loss: 0.00016158
Iteration 143/1000 | Loss: 0.00016708
Iteration 144/1000 | Loss: 0.00020603
Iteration 145/1000 | Loss: 0.00023780
Iteration 146/1000 | Loss: 0.00015087
Iteration 147/1000 | Loss: 0.00016537
Iteration 148/1000 | Loss: 0.00017059
Iteration 149/1000 | Loss: 0.00028966
Iteration 150/1000 | Loss: 0.00020746
Iteration 151/1000 | Loss: 0.00005788
Iteration 152/1000 | Loss: 0.00026423
Iteration 153/1000 | Loss: 0.00021200
Iteration 154/1000 | Loss: 0.00012019
Iteration 155/1000 | Loss: 0.00022629
Iteration 156/1000 | Loss: 0.00016030
Iteration 157/1000 | Loss: 0.00009669
Iteration 158/1000 | Loss: 0.00010646
Iteration 159/1000 | Loss: 0.00015314
Iteration 160/1000 | Loss: 0.00006893
Iteration 161/1000 | Loss: 0.00017832
Iteration 162/1000 | Loss: 0.00011740
Iteration 163/1000 | Loss: 0.00012094
Iteration 164/1000 | Loss: 0.00027218
Iteration 165/1000 | Loss: 0.00035047
Iteration 166/1000 | Loss: 0.00021208
Iteration 167/1000 | Loss: 0.00016013
Iteration 168/1000 | Loss: 0.00013668
Iteration 169/1000 | Loss: 0.00022223
Iteration 170/1000 | Loss: 0.00021661
Iteration 171/1000 | Loss: 0.00024866
Iteration 172/1000 | Loss: 0.00015445
Iteration 173/1000 | Loss: 0.00029827
Iteration 174/1000 | Loss: 0.00040616
Iteration 175/1000 | Loss: 0.00023445
Iteration 176/1000 | Loss: 0.00022433
Iteration 177/1000 | Loss: 0.00016440
Iteration 178/1000 | Loss: 0.00013505
Iteration 179/1000 | Loss: 0.00010240
Iteration 180/1000 | Loss: 0.00009437
Iteration 181/1000 | Loss: 0.00016923
Iteration 182/1000 | Loss: 0.00008513
Iteration 183/1000 | Loss: 0.00009499
Iteration 184/1000 | Loss: 0.00012413
Iteration 185/1000 | Loss: 0.00006548
Iteration 186/1000 | Loss: 0.00030820
Iteration 187/1000 | Loss: 0.00023952
Iteration 188/1000 | Loss: 0.00006682
Iteration 189/1000 | Loss: 0.00018788
Iteration 190/1000 | Loss: 0.00011891
Iteration 191/1000 | Loss: 0.00008207
Iteration 192/1000 | Loss: 0.00019755
Iteration 193/1000 | Loss: 0.00026731
Iteration 194/1000 | Loss: 0.00025648
Iteration 195/1000 | Loss: 0.00025898
Iteration 196/1000 | Loss: 0.00024746
Iteration 197/1000 | Loss: 0.00014536
Iteration 198/1000 | Loss: 0.00020999
Iteration 199/1000 | Loss: 0.00025426
Iteration 200/1000 | Loss: 0.00023356
Iteration 201/1000 | Loss: 0.00017395
Iteration 202/1000 | Loss: 0.00023476
Iteration 203/1000 | Loss: 0.00012953
Iteration 204/1000 | Loss: 0.00004797
Iteration 205/1000 | Loss: 0.00009708
Iteration 206/1000 | Loss: 0.00009481
Iteration 207/1000 | Loss: 0.00004243
Iteration 208/1000 | Loss: 0.00003222
Iteration 209/1000 | Loss: 0.00009231
Iteration 210/1000 | Loss: 0.00015045
Iteration 211/1000 | Loss: 0.00010218
Iteration 212/1000 | Loss: 0.00005024
Iteration 213/1000 | Loss: 0.00013948
Iteration 214/1000 | Loss: 0.00019950
Iteration 215/1000 | Loss: 0.00004413
Iteration 216/1000 | Loss: 0.00003484
Iteration 217/1000 | Loss: 0.00003819
Iteration 218/1000 | Loss: 0.00018103
Iteration 219/1000 | Loss: 0.00004605
Iteration 220/1000 | Loss: 0.00003924
Iteration 221/1000 | Loss: 0.00002792
Iteration 222/1000 | Loss: 0.00005085
Iteration 223/1000 | Loss: 0.00004186
Iteration 224/1000 | Loss: 0.00004002
Iteration 225/1000 | Loss: 0.00003662
Iteration 226/1000 | Loss: 0.00003925
Iteration 227/1000 | Loss: 0.00002960
Iteration 228/1000 | Loss: 0.00003750
Iteration 229/1000 | Loss: 0.00003760
Iteration 230/1000 | Loss: 0.00003524
Iteration 231/1000 | Loss: 0.00003160
Iteration 232/1000 | Loss: 0.00003333
Iteration 233/1000 | Loss: 0.00003200
Iteration 234/1000 | Loss: 0.00003475
Iteration 235/1000 | Loss: 0.00003228
Iteration 236/1000 | Loss: 0.00003499
Iteration 237/1000 | Loss: 0.00003937
Iteration 238/1000 | Loss: 0.00003491
Iteration 239/1000 | Loss: 0.00002635
Iteration 240/1000 | Loss: 0.00003654
Iteration 241/1000 | Loss: 0.00003471
Iteration 242/1000 | Loss: 0.00003671
Iteration 243/1000 | Loss: 0.00003542
Iteration 244/1000 | Loss: 0.00003661
Iteration 245/1000 | Loss: 0.00003650
Iteration 246/1000 | Loss: 0.00003544
Iteration 247/1000 | Loss: 0.00003549
Iteration 248/1000 | Loss: 0.00003642
Iteration 249/1000 | Loss: 0.00003660
Iteration 250/1000 | Loss: 0.00003609
Iteration 251/1000 | Loss: 0.00003574
Iteration 252/1000 | Loss: 0.00003515
Iteration 253/1000 | Loss: 0.00004350
Iteration 254/1000 | Loss: 0.00003483
Iteration 255/1000 | Loss: 0.00004056
Iteration 256/1000 | Loss: 0.00003400
Iteration 257/1000 | Loss: 0.00005868
Iteration 258/1000 | Loss: 0.00003975
Iteration 259/1000 | Loss: 0.00003614
Iteration 260/1000 | Loss: 0.00004403
Iteration 261/1000 | Loss: 0.00003554
Iteration 262/1000 | Loss: 0.00004446
Iteration 263/1000 | Loss: 0.00002679
Iteration 264/1000 | Loss: 0.00004872
Iteration 265/1000 | Loss: 0.00004093
Iteration 266/1000 | Loss: 0.00003648
Iteration 267/1000 | Loss: 0.00003563
Iteration 268/1000 | Loss: 0.00003446
Iteration 269/1000 | Loss: 0.00003296
Iteration 270/1000 | Loss: 0.00004810
Iteration 271/1000 | Loss: 0.00003982
Iteration 272/1000 | Loss: 0.00002326
Iteration 273/1000 | Loss: 0.00002189
Iteration 274/1000 | Loss: 0.00002143
Iteration 275/1000 | Loss: 0.00002102
Iteration 276/1000 | Loss: 0.00002079
Iteration 277/1000 | Loss: 0.00002068
Iteration 278/1000 | Loss: 0.00002065
Iteration 279/1000 | Loss: 0.00002064
Iteration 280/1000 | Loss: 0.00002062
Iteration 281/1000 | Loss: 0.00002061
Iteration 282/1000 | Loss: 0.00002061
Iteration 283/1000 | Loss: 0.00002060
Iteration 284/1000 | Loss: 0.00002060
Iteration 285/1000 | Loss: 0.00002060
Iteration 286/1000 | Loss: 0.00002059
Iteration 287/1000 | Loss: 0.00002059
Iteration 288/1000 | Loss: 0.00002058
Iteration 289/1000 | Loss: 0.00002054
Iteration 290/1000 | Loss: 0.00002053
Iteration 291/1000 | Loss: 0.00002051
Iteration 292/1000 | Loss: 0.00002050
Iteration 293/1000 | Loss: 0.00002050
Iteration 294/1000 | Loss: 0.00002049
Iteration 295/1000 | Loss: 0.00002042
Iteration 296/1000 | Loss: 0.00002042
Iteration 297/1000 | Loss: 0.00002041
Iteration 298/1000 | Loss: 0.00002040
Iteration 299/1000 | Loss: 0.00002040
Iteration 300/1000 | Loss: 0.00002040
Iteration 301/1000 | Loss: 0.00002039
Iteration 302/1000 | Loss: 0.00002039
Iteration 303/1000 | Loss: 0.00002035
Iteration 304/1000 | Loss: 0.00002033
Iteration 305/1000 | Loss: 0.00002033
Iteration 306/1000 | Loss: 0.00002033
Iteration 307/1000 | Loss: 0.00002032
Iteration 308/1000 | Loss: 0.00002032
Iteration 309/1000 | Loss: 0.00002031
Iteration 310/1000 | Loss: 0.00002030
Iteration 311/1000 | Loss: 0.00002030
Iteration 312/1000 | Loss: 0.00002029
Iteration 313/1000 | Loss: 0.00002028
Iteration 314/1000 | Loss: 0.00002027
Iteration 315/1000 | Loss: 0.00002026
Iteration 316/1000 | Loss: 0.00002024
Iteration 317/1000 | Loss: 0.00002024
Iteration 318/1000 | Loss: 0.00002023
Iteration 319/1000 | Loss: 0.00002022
Iteration 320/1000 | Loss: 0.00002021
Iteration 321/1000 | Loss: 0.00002020
Iteration 322/1000 | Loss: 0.00002019
Iteration 323/1000 | Loss: 0.00002019
Iteration 324/1000 | Loss: 0.00002018
Iteration 325/1000 | Loss: 0.00002018
Iteration 326/1000 | Loss: 0.00002018
Iteration 327/1000 | Loss: 0.00002018
Iteration 328/1000 | Loss: 0.00002018
Iteration 329/1000 | Loss: 0.00002018
Iteration 330/1000 | Loss: 0.00002018
Iteration 331/1000 | Loss: 0.00002018
Iteration 332/1000 | Loss: 0.00002018
Iteration 333/1000 | Loss: 0.00002018
Iteration 334/1000 | Loss: 0.00002018
Iteration 335/1000 | Loss: 0.00002017
Iteration 336/1000 | Loss: 0.00002017
Iteration 337/1000 | Loss: 0.00002017
Iteration 338/1000 | Loss: 0.00002017
Iteration 339/1000 | Loss: 0.00002017
Iteration 340/1000 | Loss: 0.00002016
Iteration 341/1000 | Loss: 0.00002016
Iteration 342/1000 | Loss: 0.00002016
Iteration 343/1000 | Loss: 0.00002016
Iteration 344/1000 | Loss: 0.00002016
Iteration 345/1000 | Loss: 0.00002016
Iteration 346/1000 | Loss: 0.00002016
Iteration 347/1000 | Loss: 0.00002016
Iteration 348/1000 | Loss: 0.00002015
Iteration 349/1000 | Loss: 0.00002015
Iteration 350/1000 | Loss: 0.00002014
Iteration 351/1000 | Loss: 0.00002014
Iteration 352/1000 | Loss: 0.00002014
Iteration 353/1000 | Loss: 0.00002014
Iteration 354/1000 | Loss: 0.00002014
Iteration 355/1000 | Loss: 0.00002014
Iteration 356/1000 | Loss: 0.00002013
Iteration 357/1000 | Loss: 0.00002013
Iteration 358/1000 | Loss: 0.00002013
Iteration 359/1000 | Loss: 0.00002013
Iteration 360/1000 | Loss: 0.00002013
Iteration 361/1000 | Loss: 0.00002013
Iteration 362/1000 | Loss: 0.00002013
Iteration 363/1000 | Loss: 0.00002013
Iteration 364/1000 | Loss: 0.00002013
Iteration 365/1000 | Loss: 0.00002013
Iteration 366/1000 | Loss: 0.00002013
Iteration 367/1000 | Loss: 0.00002013
Iteration 368/1000 | Loss: 0.00002013
Iteration 369/1000 | Loss: 0.00002013
Iteration 370/1000 | Loss: 0.00002013
Iteration 371/1000 | Loss: 0.00002013
Iteration 372/1000 | Loss: 0.00002012
Iteration 373/1000 | Loss: 0.00002012
Iteration 374/1000 | Loss: 0.00002012
Iteration 375/1000 | Loss: 0.00002012
Iteration 376/1000 | Loss: 0.00002012
Iteration 377/1000 | Loss: 0.00002012
Iteration 378/1000 | Loss: 0.00002012
Iteration 379/1000 | Loss: 0.00002012
Iteration 380/1000 | Loss: 0.00002012
Iteration 381/1000 | Loss: 0.00002012
Iteration 382/1000 | Loss: 0.00002012
Iteration 383/1000 | Loss: 0.00002012
Iteration 384/1000 | Loss: 0.00002012
Iteration 385/1000 | Loss: 0.00002012
Iteration 386/1000 | Loss: 0.00002012
Iteration 387/1000 | Loss: 0.00002012
Iteration 388/1000 | Loss: 0.00002012
Iteration 389/1000 | Loss: 0.00002012
Iteration 390/1000 | Loss: 0.00002012
Iteration 391/1000 | Loss: 0.00002012
Iteration 392/1000 | Loss: 0.00002012
Iteration 393/1000 | Loss: 0.00002012
Iteration 394/1000 | Loss: 0.00002012
Iteration 395/1000 | Loss: 0.00002012
Iteration 396/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [2.012258119066246e-05, 2.012258119066246e-05, 2.012258119066246e-05, 2.012258119066246e-05, 2.012258119066246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.012258119066246e-05

Optimization complete. Final v2v error: 3.5528409481048584 mm

Highest mean error: 5.353142738342285 mm for frame 115

Lowest mean error: 3.1132547855377197 mm for frame 161

Saving results

Total time: 475.01694893836975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795754
Iteration 2/25 | Loss: 0.00146727
Iteration 3/25 | Loss: 0.00108742
Iteration 4/25 | Loss: 0.00101125
Iteration 5/25 | Loss: 0.00100500
Iteration 6/25 | Loss: 0.00100454
Iteration 7/25 | Loss: 0.00100454
Iteration 8/25 | Loss: 0.00100454
Iteration 9/25 | Loss: 0.00100454
Iteration 10/25 | Loss: 0.00100454
Iteration 11/25 | Loss: 0.00100454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010045357048511505, 0.0010045357048511505, 0.0010045357048511505, 0.0010045357048511505, 0.0010045357048511505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010045357048511505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36091673
Iteration 2/25 | Loss: 0.00054170
Iteration 3/25 | Loss: 0.00054170
Iteration 4/25 | Loss: 0.00054170
Iteration 5/25 | Loss: 0.00054170
Iteration 6/25 | Loss: 0.00054170
Iteration 7/25 | Loss: 0.00054170
Iteration 8/25 | Loss: 0.00054170
Iteration 9/25 | Loss: 0.00054170
Iteration 10/25 | Loss: 0.00054170
Iteration 11/25 | Loss: 0.00054170
Iteration 12/25 | Loss: 0.00054170
Iteration 13/25 | Loss: 0.00054170
Iteration 14/25 | Loss: 0.00054170
Iteration 15/25 | Loss: 0.00054170
Iteration 16/25 | Loss: 0.00054170
Iteration 17/25 | Loss: 0.00054170
Iteration 18/25 | Loss: 0.00054170
Iteration 19/25 | Loss: 0.00054170
Iteration 20/25 | Loss: 0.00054170
Iteration 21/25 | Loss: 0.00054170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005417007487267256, 0.0005417007487267256, 0.0005417007487267256, 0.0005417007487267256, 0.0005417007487267256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005417007487267256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054170
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001477
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001132
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001110
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001105
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001105
Iteration 49/1000 | Loss: 0.00001105
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001086
Iteration 142/1000 | Loss: 0.00001086
Iteration 143/1000 | Loss: 0.00001086
Iteration 144/1000 | Loss: 0.00001085
Iteration 145/1000 | Loss: 0.00001085
Iteration 146/1000 | Loss: 0.00001085
Iteration 147/1000 | Loss: 0.00001085
Iteration 148/1000 | Loss: 0.00001085
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001085
Iteration 151/1000 | Loss: 0.00001085
Iteration 152/1000 | Loss: 0.00001085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.0852873856492806e-05, 1.0852873856492806e-05, 1.0852873856492806e-05, 1.0852873856492806e-05, 1.0852873856492806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0852873856492806e-05

Optimization complete. Final v2v error: 2.8315510749816895 mm

Highest mean error: 3.19329571723938 mm for frame 32

Lowest mean error: 2.372340679168701 mm for frame 3

Saving results

Total time: 41.382479667663574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085790
Iteration 2/25 | Loss: 0.01085790
Iteration 3/25 | Loss: 0.01085790
Iteration 4/25 | Loss: 0.01085790
Iteration 5/25 | Loss: 0.00202934
Iteration 6/25 | Loss: 0.00134029
Iteration 7/25 | Loss: 0.00125025
Iteration 8/25 | Loss: 0.00124340
Iteration 9/25 | Loss: 0.00124405
Iteration 10/25 | Loss: 0.00123716
Iteration 11/25 | Loss: 0.00121318
Iteration 12/25 | Loss: 0.00116620
Iteration 13/25 | Loss: 0.00115775
Iteration 14/25 | Loss: 0.00115321
Iteration 15/25 | Loss: 0.00115425
Iteration 16/25 | Loss: 0.00114980
Iteration 17/25 | Loss: 0.00114806
Iteration 18/25 | Loss: 0.00114346
Iteration 19/25 | Loss: 0.00114123
Iteration 20/25 | Loss: 0.00113906
Iteration 21/25 | Loss: 0.00114192
Iteration 22/25 | Loss: 0.00114061
Iteration 23/25 | Loss: 0.00114157
Iteration 24/25 | Loss: 0.00114248
Iteration 25/25 | Loss: 0.00114054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72781193
Iteration 2/25 | Loss: 0.00105453
Iteration 3/25 | Loss: 0.00105453
Iteration 4/25 | Loss: 0.00105452
Iteration 5/25 | Loss: 0.00105452
Iteration 6/25 | Loss: 0.00105452
Iteration 7/25 | Loss: 0.00105452
Iteration 8/25 | Loss: 0.00105452
Iteration 9/25 | Loss: 0.00105452
Iteration 10/25 | Loss: 0.00105452
Iteration 11/25 | Loss: 0.00105452
Iteration 12/25 | Loss: 0.00105452
Iteration 13/25 | Loss: 0.00105452
Iteration 14/25 | Loss: 0.00105452
Iteration 15/25 | Loss: 0.00105452
Iteration 16/25 | Loss: 0.00105452
Iteration 17/25 | Loss: 0.00105452
Iteration 18/25 | Loss: 0.00105452
Iteration 19/25 | Loss: 0.00105452
Iteration 20/25 | Loss: 0.00105452
Iteration 21/25 | Loss: 0.00105452
Iteration 22/25 | Loss: 0.00105452
Iteration 23/25 | Loss: 0.00105452
Iteration 24/25 | Loss: 0.00105452
Iteration 25/25 | Loss: 0.00105452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105452
Iteration 2/1000 | Loss: 0.00004970
Iteration 3/1000 | Loss: 0.00003665
Iteration 4/1000 | Loss: 0.00003025
Iteration 5/1000 | Loss: 0.00003261
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00003357
Iteration 8/1000 | Loss: 0.00002346
Iteration 9/1000 | Loss: 0.00003001
Iteration 10/1000 | Loss: 0.00003026
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002939
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00003229
Iteration 16/1000 | Loss: 0.00003423
Iteration 17/1000 | Loss: 0.00004124
Iteration 18/1000 | Loss: 0.00003816
Iteration 19/1000 | Loss: 0.00003064
Iteration 20/1000 | Loss: 0.00003144
Iteration 21/1000 | Loss: 0.00003291
Iteration 22/1000 | Loss: 0.00003099
Iteration 23/1000 | Loss: 0.00003184
Iteration 24/1000 | Loss: 0.00002916
Iteration 25/1000 | Loss: 0.00002826
Iteration 26/1000 | Loss: 0.00002769
Iteration 27/1000 | Loss: 0.00002808
Iteration 28/1000 | Loss: 0.00002671
Iteration 29/1000 | Loss: 0.00002768
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003452
Iteration 32/1000 | Loss: 0.00002159
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00004395
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001861
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001853
Iteration 46/1000 | Loss: 0.00001852
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00003899
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00002961
Iteration 79/1000 | Loss: 0.00002846
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001824
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001824
Iteration 94/1000 | Loss: 0.00001824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.8239083146909252e-05, 1.8239083146909252e-05, 1.8239083146909252e-05, 1.8239083146909252e-05, 1.8239083146909252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8239083146909252e-05

Optimization complete. Final v2v error: 3.450453758239746 mm

Highest mean error: 4.753227710723877 mm for frame 79

Lowest mean error: 2.906911611557007 mm for frame 208

Saving results

Total time: 121.25620746612549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457203
Iteration 2/25 | Loss: 0.00101182
Iteration 3/25 | Loss: 0.00095943
Iteration 4/25 | Loss: 0.00095074
Iteration 5/25 | Loss: 0.00094923
Iteration 6/25 | Loss: 0.00094902
Iteration 7/25 | Loss: 0.00094902
Iteration 8/25 | Loss: 0.00094902
Iteration 9/25 | Loss: 0.00094902
Iteration 10/25 | Loss: 0.00094902
Iteration 11/25 | Loss: 0.00094902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009490184020251036, 0.0009490184020251036, 0.0009490184020251036, 0.0009490184020251036, 0.0009490184020251036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009490184020251036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11519444
Iteration 2/25 | Loss: 0.00041996
Iteration 3/25 | Loss: 0.00041996
Iteration 4/25 | Loss: 0.00041996
Iteration 5/25 | Loss: 0.00041996
Iteration 6/25 | Loss: 0.00041996
Iteration 7/25 | Loss: 0.00041996
Iteration 8/25 | Loss: 0.00041996
Iteration 9/25 | Loss: 0.00041996
Iteration 10/25 | Loss: 0.00041996
Iteration 11/25 | Loss: 0.00041996
Iteration 12/25 | Loss: 0.00041996
Iteration 13/25 | Loss: 0.00041996
Iteration 14/25 | Loss: 0.00041996
Iteration 15/25 | Loss: 0.00041996
Iteration 16/25 | Loss: 0.00041996
Iteration 17/25 | Loss: 0.00041996
Iteration 18/25 | Loss: 0.00041996
Iteration 19/25 | Loss: 0.00041996
Iteration 20/25 | Loss: 0.00041996
Iteration 21/25 | Loss: 0.00041996
Iteration 22/25 | Loss: 0.00041996
Iteration 23/25 | Loss: 0.00041996
Iteration 24/25 | Loss: 0.00041996
Iteration 25/25 | Loss: 0.00041996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041996
Iteration 2/1000 | Loss: 0.00004381
Iteration 3/1000 | Loss: 0.00002684
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001960
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001661
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001657
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001653
Iteration 54/1000 | Loss: 0.00001653
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001653
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001652
Iteration 60/1000 | Loss: 0.00001652
Iteration 61/1000 | Loss: 0.00001652
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001652
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001650
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001648
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001648
Iteration 90/1000 | Loss: 0.00001648
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001646
Iteration 98/1000 | Loss: 0.00001646
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001646
Iteration 103/1000 | Loss: 0.00001646
Iteration 104/1000 | Loss: 0.00001646
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001645
Iteration 112/1000 | Loss: 0.00001645
Iteration 113/1000 | Loss: 0.00001645
Iteration 114/1000 | Loss: 0.00001645
Iteration 115/1000 | Loss: 0.00001645
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001645
Iteration 119/1000 | Loss: 0.00001645
Iteration 120/1000 | Loss: 0.00001645
Iteration 121/1000 | Loss: 0.00001644
Iteration 122/1000 | Loss: 0.00001644
Iteration 123/1000 | Loss: 0.00001644
Iteration 124/1000 | Loss: 0.00001644
Iteration 125/1000 | Loss: 0.00001644
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001643
Iteration 128/1000 | Loss: 0.00001643
Iteration 129/1000 | Loss: 0.00001643
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001639
Iteration 157/1000 | Loss: 0.00001639
Iteration 158/1000 | Loss: 0.00001639
Iteration 159/1000 | Loss: 0.00001639
Iteration 160/1000 | Loss: 0.00001639
Iteration 161/1000 | Loss: 0.00001639
Iteration 162/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.6386435163440183e-05, 1.6386435163440183e-05, 1.6386435163440183e-05, 1.6386435163440183e-05, 1.6386435163440183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6386435163440183e-05

Optimization complete. Final v2v error: 3.393228769302368 mm

Highest mean error: 3.4213924407958984 mm for frame 21

Lowest mean error: 3.3703908920288086 mm for frame 91

Saving results

Total time: 35.55483865737915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962243
Iteration 2/25 | Loss: 0.00322292
Iteration 3/25 | Loss: 0.00310974
Iteration 4/25 | Loss: 0.00170443
Iteration 5/25 | Loss: 0.00165734
Iteration 6/25 | Loss: 0.00148309
Iteration 7/25 | Loss: 0.00143201
Iteration 8/25 | Loss: 0.00161683
Iteration 9/25 | Loss: 0.00169789
Iteration 10/25 | Loss: 0.00149995
Iteration 11/25 | Loss: 0.00137904
Iteration 12/25 | Loss: 0.00135961
Iteration 13/25 | Loss: 0.00138524
Iteration 14/25 | Loss: 0.00141793
Iteration 15/25 | Loss: 0.00138376
Iteration 16/25 | Loss: 0.00137030
Iteration 17/25 | Loss: 0.00138800
Iteration 18/25 | Loss: 0.00138423
Iteration 19/25 | Loss: 0.00137556
Iteration 20/25 | Loss: 0.00135543
Iteration 21/25 | Loss: 0.00132157
Iteration 22/25 | Loss: 0.00129695
Iteration 23/25 | Loss: 0.00128328
Iteration 24/25 | Loss: 0.00125552
Iteration 25/25 | Loss: 0.00124081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35671341
Iteration 2/25 | Loss: 0.00202471
Iteration 3/25 | Loss: 0.00202471
Iteration 4/25 | Loss: 0.00202471
Iteration 5/25 | Loss: 0.00202471
Iteration 6/25 | Loss: 0.00202471
Iteration 7/25 | Loss: 0.00202471
Iteration 8/25 | Loss: 0.00202471
Iteration 9/25 | Loss: 0.00202471
Iteration 10/25 | Loss: 0.00202471
Iteration 11/25 | Loss: 0.00202471
Iteration 12/25 | Loss: 0.00202471
Iteration 13/25 | Loss: 0.00202471
Iteration 14/25 | Loss: 0.00202471
Iteration 15/25 | Loss: 0.00202471
Iteration 16/25 | Loss: 0.00202471
Iteration 17/25 | Loss: 0.00202471
Iteration 18/25 | Loss: 0.00202471
Iteration 19/25 | Loss: 0.00202471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020247073844075203, 0.0020247073844075203, 0.0020247073844075203, 0.0020247073844075203, 0.0020247073844075203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020247073844075203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202471
Iteration 2/1000 | Loss: 0.00068689
Iteration 3/1000 | Loss: 0.00037918
Iteration 4/1000 | Loss: 0.00021975
Iteration 5/1000 | Loss: 0.00041290
Iteration 6/1000 | Loss: 0.00033713
Iteration 7/1000 | Loss: 0.00039310
Iteration 8/1000 | Loss: 0.00040454
Iteration 9/1000 | Loss: 0.00015373
Iteration 10/1000 | Loss: 0.00014574
Iteration 11/1000 | Loss: 0.00007289
Iteration 12/1000 | Loss: 0.00007283
Iteration 13/1000 | Loss: 0.00083831
Iteration 14/1000 | Loss: 0.00077402
Iteration 15/1000 | Loss: 0.00040046
Iteration 16/1000 | Loss: 0.00088759
Iteration 17/1000 | Loss: 0.00041302
Iteration 18/1000 | Loss: 0.00063044
Iteration 19/1000 | Loss: 0.00055555
Iteration 20/1000 | Loss: 0.00055206
Iteration 21/1000 | Loss: 0.00025771
Iteration 22/1000 | Loss: 0.00034065
Iteration 23/1000 | Loss: 0.00014236
Iteration 24/1000 | Loss: 0.00010768
Iteration 25/1000 | Loss: 0.00009270
Iteration 26/1000 | Loss: 0.00006272
Iteration 27/1000 | Loss: 0.00008719
Iteration 28/1000 | Loss: 0.00021592
Iteration 29/1000 | Loss: 0.00065225
Iteration 30/1000 | Loss: 0.00042339
Iteration 31/1000 | Loss: 0.00021902
Iteration 32/1000 | Loss: 0.00005514
Iteration 33/1000 | Loss: 0.00034917
Iteration 34/1000 | Loss: 0.00029191
Iteration 35/1000 | Loss: 0.00013297
Iteration 36/1000 | Loss: 0.00005452
Iteration 37/1000 | Loss: 0.00004809
Iteration 38/1000 | Loss: 0.00045292
Iteration 39/1000 | Loss: 0.00038092
Iteration 40/1000 | Loss: 0.00047894
Iteration 41/1000 | Loss: 0.00011580
Iteration 42/1000 | Loss: 0.00026647
Iteration 43/1000 | Loss: 0.00034570
Iteration 44/1000 | Loss: 0.00027182
Iteration 45/1000 | Loss: 0.00034395
Iteration 46/1000 | Loss: 0.00027525
Iteration 47/1000 | Loss: 0.00033848
Iteration 48/1000 | Loss: 0.00017139
Iteration 49/1000 | Loss: 0.00004507
Iteration 50/1000 | Loss: 0.00004136
Iteration 51/1000 | Loss: 0.00003902
Iteration 52/1000 | Loss: 0.00003727
Iteration 53/1000 | Loss: 0.00003588
Iteration 54/1000 | Loss: 0.00003476
Iteration 55/1000 | Loss: 0.00003369
Iteration 56/1000 | Loss: 0.00022189
Iteration 57/1000 | Loss: 0.00013248
Iteration 58/1000 | Loss: 0.00027133
Iteration 59/1000 | Loss: 0.00004213
Iteration 60/1000 | Loss: 0.00003487
Iteration 61/1000 | Loss: 0.00003184
Iteration 62/1000 | Loss: 0.00003093
Iteration 63/1000 | Loss: 0.00002996
Iteration 64/1000 | Loss: 0.00002881
Iteration 65/1000 | Loss: 0.00030391
Iteration 66/1000 | Loss: 0.00007323
Iteration 67/1000 | Loss: 0.00028867
Iteration 68/1000 | Loss: 0.00008180
Iteration 69/1000 | Loss: 0.00002893
Iteration 70/1000 | Loss: 0.00029380
Iteration 71/1000 | Loss: 0.00008867
Iteration 72/1000 | Loss: 0.00028798
Iteration 73/1000 | Loss: 0.00033286
Iteration 74/1000 | Loss: 0.00021947
Iteration 75/1000 | Loss: 0.00032509
Iteration 76/1000 | Loss: 0.00004206
Iteration 77/1000 | Loss: 0.00028093
Iteration 78/1000 | Loss: 0.00026010
Iteration 79/1000 | Loss: 0.00022117
Iteration 80/1000 | Loss: 0.00018183
Iteration 81/1000 | Loss: 0.00017928
Iteration 82/1000 | Loss: 0.00010102
Iteration 83/1000 | Loss: 0.00003874
Iteration 84/1000 | Loss: 0.00003319
Iteration 85/1000 | Loss: 0.00002965
Iteration 86/1000 | Loss: 0.00002708
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002535
Iteration 90/1000 | Loss: 0.00029162
Iteration 91/1000 | Loss: 0.00008821
Iteration 92/1000 | Loss: 0.00002585
Iteration 93/1000 | Loss: 0.00002514
Iteration 94/1000 | Loss: 0.00029857
Iteration 95/1000 | Loss: 0.00004159
Iteration 96/1000 | Loss: 0.00015574
Iteration 97/1000 | Loss: 0.00198970
Iteration 98/1000 | Loss: 0.00026331
Iteration 99/1000 | Loss: 0.00020888
Iteration 100/1000 | Loss: 0.00011334
Iteration 101/1000 | Loss: 0.00004711
Iteration 102/1000 | Loss: 0.00031843
Iteration 103/1000 | Loss: 0.00004806
Iteration 104/1000 | Loss: 0.00003992
Iteration 105/1000 | Loss: 0.00003549
Iteration 106/1000 | Loss: 0.00003314
Iteration 107/1000 | Loss: 0.00003144
Iteration 108/1000 | Loss: 0.00002980
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002757
Iteration 111/1000 | Loss: 0.00002717
Iteration 112/1000 | Loss: 0.00026954
Iteration 113/1000 | Loss: 0.00003808
Iteration 114/1000 | Loss: 0.00003043
Iteration 115/1000 | Loss: 0.00002868
Iteration 116/1000 | Loss: 0.00002798
Iteration 117/1000 | Loss: 0.00002726
Iteration 118/1000 | Loss: 0.00002682
Iteration 119/1000 | Loss: 0.00002613
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002487
Iteration 122/1000 | Loss: 0.00002457
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002424
Iteration 125/1000 | Loss: 0.00002423
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002416
Iteration 128/1000 | Loss: 0.00002406
Iteration 129/1000 | Loss: 0.00002395
Iteration 130/1000 | Loss: 0.00002393
Iteration 131/1000 | Loss: 0.00002392
Iteration 132/1000 | Loss: 0.00002391
Iteration 133/1000 | Loss: 0.00002391
Iteration 134/1000 | Loss: 0.00002388
Iteration 135/1000 | Loss: 0.00026315
Iteration 136/1000 | Loss: 0.00025135
Iteration 137/1000 | Loss: 0.00151569
Iteration 138/1000 | Loss: 0.00064503
Iteration 139/1000 | Loss: 0.00009788
Iteration 140/1000 | Loss: 0.00030194
Iteration 141/1000 | Loss: 0.00017967
Iteration 142/1000 | Loss: 0.00030641
Iteration 143/1000 | Loss: 0.00016399
Iteration 144/1000 | Loss: 0.00005118
Iteration 145/1000 | Loss: 0.00003141
Iteration 146/1000 | Loss: 0.00002714
Iteration 147/1000 | Loss: 0.00002505
Iteration 148/1000 | Loss: 0.00002384
Iteration 149/1000 | Loss: 0.00002328
Iteration 150/1000 | Loss: 0.00002270
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002192
Iteration 153/1000 | Loss: 0.00002169
Iteration 154/1000 | Loss: 0.00002152
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002134
Iteration 157/1000 | Loss: 0.00002128
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00002125
Iteration 160/1000 | Loss: 0.00002123
Iteration 161/1000 | Loss: 0.00002122
Iteration 162/1000 | Loss: 0.00002122
Iteration 163/1000 | Loss: 0.00002121
Iteration 164/1000 | Loss: 0.00002121
Iteration 165/1000 | Loss: 0.00002121
Iteration 166/1000 | Loss: 0.00002116
Iteration 167/1000 | Loss: 0.00002115
Iteration 168/1000 | Loss: 0.00002115
Iteration 169/1000 | Loss: 0.00002114
Iteration 170/1000 | Loss: 0.00002112
Iteration 171/1000 | Loss: 0.00002112
Iteration 172/1000 | Loss: 0.00002108
Iteration 173/1000 | Loss: 0.00002108
Iteration 174/1000 | Loss: 0.00002101
Iteration 175/1000 | Loss: 0.00002101
Iteration 176/1000 | Loss: 0.00002101
Iteration 177/1000 | Loss: 0.00002101
Iteration 178/1000 | Loss: 0.00002101
Iteration 179/1000 | Loss: 0.00002100
Iteration 180/1000 | Loss: 0.00002100
Iteration 181/1000 | Loss: 0.00002100
Iteration 182/1000 | Loss: 0.00002100
Iteration 183/1000 | Loss: 0.00002100
Iteration 184/1000 | Loss: 0.00002100
Iteration 185/1000 | Loss: 0.00002100
Iteration 186/1000 | Loss: 0.00002099
Iteration 187/1000 | Loss: 0.00002099
Iteration 188/1000 | Loss: 0.00002099
Iteration 189/1000 | Loss: 0.00002099
Iteration 190/1000 | Loss: 0.00002098
Iteration 191/1000 | Loss: 0.00002098
Iteration 192/1000 | Loss: 0.00002097
Iteration 193/1000 | Loss: 0.00002096
Iteration 194/1000 | Loss: 0.00002096
Iteration 195/1000 | Loss: 0.00002096
Iteration 196/1000 | Loss: 0.00002096
Iteration 197/1000 | Loss: 0.00002096
Iteration 198/1000 | Loss: 0.00002096
Iteration 199/1000 | Loss: 0.00002096
Iteration 200/1000 | Loss: 0.00002096
Iteration 201/1000 | Loss: 0.00002095
Iteration 202/1000 | Loss: 0.00002095
Iteration 203/1000 | Loss: 0.00002094
Iteration 204/1000 | Loss: 0.00002093
Iteration 205/1000 | Loss: 0.00002092
Iteration 206/1000 | Loss: 0.00002092
Iteration 207/1000 | Loss: 0.00002092
Iteration 208/1000 | Loss: 0.00002092
Iteration 209/1000 | Loss: 0.00002091
Iteration 210/1000 | Loss: 0.00002091
Iteration 211/1000 | Loss: 0.00002091
Iteration 212/1000 | Loss: 0.00002090
Iteration 213/1000 | Loss: 0.00002090
Iteration 214/1000 | Loss: 0.00002090
Iteration 215/1000 | Loss: 0.00002089
Iteration 216/1000 | Loss: 0.00002088
Iteration 217/1000 | Loss: 0.00002088
Iteration 218/1000 | Loss: 0.00002088
Iteration 219/1000 | Loss: 0.00002087
Iteration 220/1000 | Loss: 0.00002087
Iteration 221/1000 | Loss: 0.00002086
Iteration 222/1000 | Loss: 0.00002086
Iteration 223/1000 | Loss: 0.00002086
Iteration 224/1000 | Loss: 0.00002086
Iteration 225/1000 | Loss: 0.00002085
Iteration 226/1000 | Loss: 0.00002085
Iteration 227/1000 | Loss: 0.00002085
Iteration 228/1000 | Loss: 0.00002085
Iteration 229/1000 | Loss: 0.00002085
Iteration 230/1000 | Loss: 0.00002084
Iteration 231/1000 | Loss: 0.00002084
Iteration 232/1000 | Loss: 0.00002084
Iteration 233/1000 | Loss: 0.00002083
Iteration 234/1000 | Loss: 0.00002083
Iteration 235/1000 | Loss: 0.00002083
Iteration 236/1000 | Loss: 0.00002083
Iteration 237/1000 | Loss: 0.00002082
Iteration 238/1000 | Loss: 0.00002082
Iteration 239/1000 | Loss: 0.00002082
Iteration 240/1000 | Loss: 0.00002082
Iteration 241/1000 | Loss: 0.00002082
Iteration 242/1000 | Loss: 0.00002082
Iteration 243/1000 | Loss: 0.00002081
Iteration 244/1000 | Loss: 0.00002081
Iteration 245/1000 | Loss: 0.00002081
Iteration 246/1000 | Loss: 0.00002081
Iteration 247/1000 | Loss: 0.00002081
Iteration 248/1000 | Loss: 0.00002081
Iteration 249/1000 | Loss: 0.00002081
Iteration 250/1000 | Loss: 0.00002081
Iteration 251/1000 | Loss: 0.00002079
Iteration 252/1000 | Loss: 0.00002079
Iteration 253/1000 | Loss: 0.00002079
Iteration 254/1000 | Loss: 0.00002079
Iteration 255/1000 | Loss: 0.00002079
Iteration 256/1000 | Loss: 0.00002079
Iteration 257/1000 | Loss: 0.00002079
Iteration 258/1000 | Loss: 0.00002079
Iteration 259/1000 | Loss: 0.00002078
Iteration 260/1000 | Loss: 0.00002078
Iteration 261/1000 | Loss: 0.00002078
Iteration 262/1000 | Loss: 0.00002078
Iteration 263/1000 | Loss: 0.00002078
Iteration 264/1000 | Loss: 0.00002078
Iteration 265/1000 | Loss: 0.00002078
Iteration 266/1000 | Loss: 0.00002078
Iteration 267/1000 | Loss: 0.00002078
Iteration 268/1000 | Loss: 0.00002078
Iteration 269/1000 | Loss: 0.00002078
Iteration 270/1000 | Loss: 0.00002078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [2.078394390991889e-05, 2.078394390991889e-05, 2.078394390991889e-05, 2.078394390991889e-05, 2.078394390991889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.078394390991889e-05

Optimization complete. Final v2v error: 3.5491576194763184 mm

Highest mean error: 5.90981388092041 mm for frame 57

Lowest mean error: 3.262603998184204 mm for frame 154

Saving results

Total time: 265.4019629955292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808351
Iteration 2/25 | Loss: 0.00143049
Iteration 3/25 | Loss: 0.00120993
Iteration 4/25 | Loss: 0.00116847
Iteration 5/25 | Loss: 0.00124435
Iteration 6/25 | Loss: 0.00121936
Iteration 7/25 | Loss: 0.00109291
Iteration 8/25 | Loss: 0.00105190
Iteration 9/25 | Loss: 0.00103900
Iteration 10/25 | Loss: 0.00103491
Iteration 11/25 | Loss: 0.00103364
Iteration 12/25 | Loss: 0.00103211
Iteration 13/25 | Loss: 0.00103116
Iteration 14/25 | Loss: 0.00103070
Iteration 15/25 | Loss: 0.00103028
Iteration 16/25 | Loss: 0.00106846
Iteration 17/25 | Loss: 0.00103807
Iteration 18/25 | Loss: 0.00106003
Iteration 19/25 | Loss: 0.00102411
Iteration 20/25 | Loss: 0.00101585
Iteration 21/25 | Loss: 0.00101435
Iteration 22/25 | Loss: 0.00101409
Iteration 23/25 | Loss: 0.00101390
Iteration 24/25 | Loss: 0.00101366
Iteration 25/25 | Loss: 0.00101892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30256271
Iteration 2/25 | Loss: 0.00093409
Iteration 3/25 | Loss: 0.00093408
Iteration 4/25 | Loss: 0.00093408
Iteration 5/25 | Loss: 0.00093408
Iteration 6/25 | Loss: 0.00093408
Iteration 7/25 | Loss: 0.00093408
Iteration 8/25 | Loss: 0.00093408
Iteration 9/25 | Loss: 0.00093408
Iteration 10/25 | Loss: 0.00093408
Iteration 11/25 | Loss: 0.00093408
Iteration 12/25 | Loss: 0.00093408
Iteration 13/25 | Loss: 0.00093408
Iteration 14/25 | Loss: 0.00093408
Iteration 15/25 | Loss: 0.00093408
Iteration 16/25 | Loss: 0.00093408
Iteration 17/25 | Loss: 0.00093408
Iteration 18/25 | Loss: 0.00093408
Iteration 19/25 | Loss: 0.00093408
Iteration 20/25 | Loss: 0.00093408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000934075505938381, 0.000934075505938381, 0.000934075505938381, 0.000934075505938381, 0.000934075505938381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000934075505938381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093408
Iteration 2/1000 | Loss: 0.00064072
Iteration 3/1000 | Loss: 0.00003666
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002230
Iteration 6/1000 | Loss: 0.00002077
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001789
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001521
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001510
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001502
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001501
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001498
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00001497
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001497
Iteration 51/1000 | Loss: 0.00001497
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001486
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001486
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001485
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001485
Iteration 109/1000 | Loss: 0.00001485
Iteration 110/1000 | Loss: 0.00001485
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001485
Iteration 114/1000 | Loss: 0.00001485
Iteration 115/1000 | Loss: 0.00001485
Iteration 116/1000 | Loss: 0.00001485
Iteration 117/1000 | Loss: 0.00001485
Iteration 118/1000 | Loss: 0.00001484
Iteration 119/1000 | Loss: 0.00001484
Iteration 120/1000 | Loss: 0.00001484
Iteration 121/1000 | Loss: 0.00001484
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001484
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001484
Iteration 132/1000 | Loss: 0.00001484
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001483
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001482
Iteration 142/1000 | Loss: 0.00001482
Iteration 143/1000 | Loss: 0.00001482
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001482
Iteration 146/1000 | Loss: 0.00001482
Iteration 147/1000 | Loss: 0.00001482
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001481
Iteration 157/1000 | Loss: 0.00001481
Iteration 158/1000 | Loss: 0.00001481
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Iteration 164/1000 | Loss: 0.00001481
Iteration 165/1000 | Loss: 0.00001481
Iteration 166/1000 | Loss: 0.00001481
Iteration 167/1000 | Loss: 0.00001481
Iteration 168/1000 | Loss: 0.00001481
Iteration 169/1000 | Loss: 0.00001480
Iteration 170/1000 | Loss: 0.00001480
Iteration 171/1000 | Loss: 0.00001480
Iteration 172/1000 | Loss: 0.00001480
Iteration 173/1000 | Loss: 0.00001480
Iteration 174/1000 | Loss: 0.00001480
Iteration 175/1000 | Loss: 0.00001479
Iteration 176/1000 | Loss: 0.00001479
Iteration 177/1000 | Loss: 0.00001479
Iteration 178/1000 | Loss: 0.00001479
Iteration 179/1000 | Loss: 0.00001479
Iteration 180/1000 | Loss: 0.00001479
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001479
Iteration 183/1000 | Loss: 0.00001479
Iteration 184/1000 | Loss: 0.00001479
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001478
Iteration 187/1000 | Loss: 0.00001478
Iteration 188/1000 | Loss: 0.00001478
Iteration 189/1000 | Loss: 0.00001478
Iteration 190/1000 | Loss: 0.00001478
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001478
Iteration 194/1000 | Loss: 0.00001478
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001478
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Iteration 200/1000 | Loss: 0.00001478
Iteration 201/1000 | Loss: 0.00001478
Iteration 202/1000 | Loss: 0.00001477
Iteration 203/1000 | Loss: 0.00001477
Iteration 204/1000 | Loss: 0.00001477
Iteration 205/1000 | Loss: 0.00001477
Iteration 206/1000 | Loss: 0.00001477
Iteration 207/1000 | Loss: 0.00001477
Iteration 208/1000 | Loss: 0.00001477
Iteration 209/1000 | Loss: 0.00001477
Iteration 210/1000 | Loss: 0.00001477
Iteration 211/1000 | Loss: 0.00001477
Iteration 212/1000 | Loss: 0.00001477
Iteration 213/1000 | Loss: 0.00001477
Iteration 214/1000 | Loss: 0.00001477
Iteration 215/1000 | Loss: 0.00001477
Iteration 216/1000 | Loss: 0.00001477
Iteration 217/1000 | Loss: 0.00001477
Iteration 218/1000 | Loss: 0.00001477
Iteration 219/1000 | Loss: 0.00001477
Iteration 220/1000 | Loss: 0.00001477
Iteration 221/1000 | Loss: 0.00001476
Iteration 222/1000 | Loss: 0.00001476
Iteration 223/1000 | Loss: 0.00001476
Iteration 224/1000 | Loss: 0.00001476
Iteration 225/1000 | Loss: 0.00001476
Iteration 226/1000 | Loss: 0.00001476
Iteration 227/1000 | Loss: 0.00001476
Iteration 228/1000 | Loss: 0.00001476
Iteration 229/1000 | Loss: 0.00001476
Iteration 230/1000 | Loss: 0.00001476
Iteration 231/1000 | Loss: 0.00001476
Iteration 232/1000 | Loss: 0.00001476
Iteration 233/1000 | Loss: 0.00001476
Iteration 234/1000 | Loss: 0.00001476
Iteration 235/1000 | Loss: 0.00001476
Iteration 236/1000 | Loss: 0.00001476
Iteration 237/1000 | Loss: 0.00001476
Iteration 238/1000 | Loss: 0.00001476
Iteration 239/1000 | Loss: 0.00001476
Iteration 240/1000 | Loss: 0.00001476
Iteration 241/1000 | Loss: 0.00001476
Iteration 242/1000 | Loss: 0.00001476
Iteration 243/1000 | Loss: 0.00001476
Iteration 244/1000 | Loss: 0.00001476
Iteration 245/1000 | Loss: 0.00001476
Iteration 246/1000 | Loss: 0.00001476
Iteration 247/1000 | Loss: 0.00001476
Iteration 248/1000 | Loss: 0.00001476
Iteration 249/1000 | Loss: 0.00001476
Iteration 250/1000 | Loss: 0.00001476
Iteration 251/1000 | Loss: 0.00001476
Iteration 252/1000 | Loss: 0.00001476
Iteration 253/1000 | Loss: 0.00001476
Iteration 254/1000 | Loss: 0.00001476
Iteration 255/1000 | Loss: 0.00001476
Iteration 256/1000 | Loss: 0.00001476
Iteration 257/1000 | Loss: 0.00001476
Iteration 258/1000 | Loss: 0.00001476
Iteration 259/1000 | Loss: 0.00001476
Iteration 260/1000 | Loss: 0.00001476
Iteration 261/1000 | Loss: 0.00001476
Iteration 262/1000 | Loss: 0.00001476
Iteration 263/1000 | Loss: 0.00001476
Iteration 264/1000 | Loss: 0.00001476
Iteration 265/1000 | Loss: 0.00001476
Iteration 266/1000 | Loss: 0.00001476
Iteration 267/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.4756867130927276e-05, 1.4756867130927276e-05, 1.4756867130927276e-05, 1.4756867130927276e-05, 1.4756867130927276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4756867130927276e-05

Optimization complete. Final v2v error: 3.225583791732788 mm

Highest mean error: 3.83988618850708 mm for frame 36

Lowest mean error: 2.7677927017211914 mm for frame 154

Saving results

Total time: 83.80977892875671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842898
Iteration 2/25 | Loss: 0.00154782
Iteration 3/25 | Loss: 0.00115366
Iteration 4/25 | Loss: 0.00113412
Iteration 5/25 | Loss: 0.00112993
Iteration 6/25 | Loss: 0.00112858
Iteration 7/25 | Loss: 0.00112858
Iteration 8/25 | Loss: 0.00112858
Iteration 9/25 | Loss: 0.00112858
Iteration 10/25 | Loss: 0.00112858
Iteration 11/25 | Loss: 0.00112858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011285808868706226, 0.0011285808868706226, 0.0011285808868706226, 0.0011285808868706226, 0.0011285808868706226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011285808868706226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51937675
Iteration 2/25 | Loss: 0.00076657
Iteration 3/25 | Loss: 0.00076656
Iteration 4/25 | Loss: 0.00076656
Iteration 5/25 | Loss: 0.00076656
Iteration 6/25 | Loss: 0.00076656
Iteration 7/25 | Loss: 0.00076656
Iteration 8/25 | Loss: 0.00076656
Iteration 9/25 | Loss: 0.00076656
Iteration 10/25 | Loss: 0.00076656
Iteration 11/25 | Loss: 0.00076656
Iteration 12/25 | Loss: 0.00076656
Iteration 13/25 | Loss: 0.00076656
Iteration 14/25 | Loss: 0.00076656
Iteration 15/25 | Loss: 0.00076656
Iteration 16/25 | Loss: 0.00076656
Iteration 17/25 | Loss: 0.00076656
Iteration 18/25 | Loss: 0.00076656
Iteration 19/25 | Loss: 0.00076656
Iteration 20/25 | Loss: 0.00076656
Iteration 21/25 | Loss: 0.00076656
Iteration 22/25 | Loss: 0.00076656
Iteration 23/25 | Loss: 0.00076656
Iteration 24/25 | Loss: 0.00076656
Iteration 25/25 | Loss: 0.00076656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076656
Iteration 2/1000 | Loss: 0.00005797
Iteration 3/1000 | Loss: 0.00004364
Iteration 4/1000 | Loss: 0.00003609
Iteration 5/1000 | Loss: 0.00003408
Iteration 6/1000 | Loss: 0.00003300
Iteration 7/1000 | Loss: 0.00003227
Iteration 8/1000 | Loss: 0.00003166
Iteration 9/1000 | Loss: 0.00003104
Iteration 10/1000 | Loss: 0.00003056
Iteration 11/1000 | Loss: 0.00003011
Iteration 12/1000 | Loss: 0.00002985
Iteration 13/1000 | Loss: 0.00002971
Iteration 14/1000 | Loss: 0.00002952
Iteration 15/1000 | Loss: 0.00002939
Iteration 16/1000 | Loss: 0.00002929
Iteration 17/1000 | Loss: 0.00002913
Iteration 18/1000 | Loss: 0.00002898
Iteration 19/1000 | Loss: 0.00002883
Iteration 20/1000 | Loss: 0.00002869
Iteration 21/1000 | Loss: 0.00002860
Iteration 22/1000 | Loss: 0.00002856
Iteration 23/1000 | Loss: 0.00002853
Iteration 24/1000 | Loss: 0.00002853
Iteration 25/1000 | Loss: 0.00002853
Iteration 26/1000 | Loss: 0.00002853
Iteration 27/1000 | Loss: 0.00002852
Iteration 28/1000 | Loss: 0.00002852
Iteration 29/1000 | Loss: 0.00002852
Iteration 30/1000 | Loss: 0.00002852
Iteration 31/1000 | Loss: 0.00002851
Iteration 32/1000 | Loss: 0.00002851
Iteration 33/1000 | Loss: 0.00002850
Iteration 34/1000 | Loss: 0.00002849
Iteration 35/1000 | Loss: 0.00002849
Iteration 36/1000 | Loss: 0.00002848
Iteration 37/1000 | Loss: 0.00002845
Iteration 38/1000 | Loss: 0.00002844
Iteration 39/1000 | Loss: 0.00002844
Iteration 40/1000 | Loss: 0.00002843
Iteration 41/1000 | Loss: 0.00002841
Iteration 42/1000 | Loss: 0.00002841
Iteration 43/1000 | Loss: 0.00002840
Iteration 44/1000 | Loss: 0.00002840
Iteration 45/1000 | Loss: 0.00002838
Iteration 46/1000 | Loss: 0.00002838
Iteration 47/1000 | Loss: 0.00002838
Iteration 48/1000 | Loss: 0.00002838
Iteration 49/1000 | Loss: 0.00002838
Iteration 50/1000 | Loss: 0.00002838
Iteration 51/1000 | Loss: 0.00002838
Iteration 52/1000 | Loss: 0.00002838
Iteration 53/1000 | Loss: 0.00002837
Iteration 54/1000 | Loss: 0.00002837
Iteration 55/1000 | Loss: 0.00002837
Iteration 56/1000 | Loss: 0.00002837
Iteration 57/1000 | Loss: 0.00002837
Iteration 58/1000 | Loss: 0.00002837
Iteration 59/1000 | Loss: 0.00002837
Iteration 60/1000 | Loss: 0.00002837
Iteration 61/1000 | Loss: 0.00002836
Iteration 62/1000 | Loss: 0.00002835
Iteration 63/1000 | Loss: 0.00002835
Iteration 64/1000 | Loss: 0.00002835
Iteration 65/1000 | Loss: 0.00002835
Iteration 66/1000 | Loss: 0.00002835
Iteration 67/1000 | Loss: 0.00002834
Iteration 68/1000 | Loss: 0.00002834
Iteration 69/1000 | Loss: 0.00002834
Iteration 70/1000 | Loss: 0.00002833
Iteration 71/1000 | Loss: 0.00002833
Iteration 72/1000 | Loss: 0.00002832
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002831
Iteration 77/1000 | Loss: 0.00002831
Iteration 78/1000 | Loss: 0.00002831
Iteration 79/1000 | Loss: 0.00002830
Iteration 80/1000 | Loss: 0.00002830
Iteration 81/1000 | Loss: 0.00002829
Iteration 82/1000 | Loss: 0.00002829
Iteration 83/1000 | Loss: 0.00002828
Iteration 84/1000 | Loss: 0.00002828
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00002828
Iteration 87/1000 | Loss: 0.00002828
Iteration 88/1000 | Loss: 0.00002828
Iteration 89/1000 | Loss: 0.00002828
Iteration 90/1000 | Loss: 0.00002828
Iteration 91/1000 | Loss: 0.00002828
Iteration 92/1000 | Loss: 0.00002828
Iteration 93/1000 | Loss: 0.00002828
Iteration 94/1000 | Loss: 0.00002828
Iteration 95/1000 | Loss: 0.00002828
Iteration 96/1000 | Loss: 0.00002828
Iteration 97/1000 | Loss: 0.00002828
Iteration 98/1000 | Loss: 0.00002827
Iteration 99/1000 | Loss: 0.00002827
Iteration 100/1000 | Loss: 0.00002827
Iteration 101/1000 | Loss: 0.00002827
Iteration 102/1000 | Loss: 0.00002827
Iteration 103/1000 | Loss: 0.00002827
Iteration 104/1000 | Loss: 0.00002827
Iteration 105/1000 | Loss: 0.00002827
Iteration 106/1000 | Loss: 0.00002827
Iteration 107/1000 | Loss: 0.00002827
Iteration 108/1000 | Loss: 0.00002827
Iteration 109/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.8272144845686853e-05, 2.8272144845686853e-05, 2.8272144845686853e-05, 2.8272144845686853e-05, 2.8272144845686853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8272144845686853e-05

Optimization complete. Final v2v error: 4.340016841888428 mm

Highest mean error: 5.356061935424805 mm for frame 0

Lowest mean error: 3.542940378189087 mm for frame 11

Saving results

Total time: 46.879465103149414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519934
Iteration 2/25 | Loss: 0.00116681
Iteration 3/25 | Loss: 0.00104278
Iteration 4/25 | Loss: 0.00102591
Iteration 5/25 | Loss: 0.00101985
Iteration 6/25 | Loss: 0.00101803
Iteration 7/25 | Loss: 0.00101803
Iteration 8/25 | Loss: 0.00101803
Iteration 9/25 | Loss: 0.00101803
Iteration 10/25 | Loss: 0.00101803
Iteration 11/25 | Loss: 0.00101803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001018032431602478, 0.001018032431602478, 0.001018032431602478, 0.001018032431602478, 0.001018032431602478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018032431602478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19550419
Iteration 2/25 | Loss: 0.00099387
Iteration 3/25 | Loss: 0.00099387
Iteration 4/25 | Loss: 0.00099387
Iteration 5/25 | Loss: 0.00099387
Iteration 6/25 | Loss: 0.00099386
Iteration 7/25 | Loss: 0.00099386
Iteration 8/25 | Loss: 0.00099386
Iteration 9/25 | Loss: 0.00099386
Iteration 10/25 | Loss: 0.00099386
Iteration 11/25 | Loss: 0.00099386
Iteration 12/25 | Loss: 0.00099386
Iteration 13/25 | Loss: 0.00099386
Iteration 14/25 | Loss: 0.00099386
Iteration 15/25 | Loss: 0.00099386
Iteration 16/25 | Loss: 0.00099386
Iteration 17/25 | Loss: 0.00099386
Iteration 18/25 | Loss: 0.00099386
Iteration 19/25 | Loss: 0.00099386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009938633302226663, 0.0009938633302226663, 0.0009938633302226663, 0.0009938633302226663, 0.0009938633302226663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009938633302226663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099386
Iteration 2/1000 | Loss: 0.00002875
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00001287
Iteration 5/1000 | Loss: 0.00001144
Iteration 6/1000 | Loss: 0.00001088
Iteration 7/1000 | Loss: 0.00001066
Iteration 8/1000 | Loss: 0.00001037
Iteration 9/1000 | Loss: 0.00001030
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001010
Iteration 12/1000 | Loss: 0.00000997
Iteration 13/1000 | Loss: 0.00000989
Iteration 14/1000 | Loss: 0.00000978
Iteration 15/1000 | Loss: 0.00000978
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000976
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000972
Iteration 20/1000 | Loss: 0.00000972
Iteration 21/1000 | Loss: 0.00000972
Iteration 22/1000 | Loss: 0.00000972
Iteration 23/1000 | Loss: 0.00000972
Iteration 24/1000 | Loss: 0.00000971
Iteration 25/1000 | Loss: 0.00000971
Iteration 26/1000 | Loss: 0.00000971
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000971
Iteration 32/1000 | Loss: 0.00000971
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000962
Iteration 48/1000 | Loss: 0.00000962
Iteration 49/1000 | Loss: 0.00000962
Iteration 50/1000 | Loss: 0.00000962
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000961
Iteration 54/1000 | Loss: 0.00000961
Iteration 55/1000 | Loss: 0.00000961
Iteration 56/1000 | Loss: 0.00000960
Iteration 57/1000 | Loss: 0.00000960
Iteration 58/1000 | Loss: 0.00000960
Iteration 59/1000 | Loss: 0.00000959
Iteration 60/1000 | Loss: 0.00000959
Iteration 61/1000 | Loss: 0.00000959
Iteration 62/1000 | Loss: 0.00000959
Iteration 63/1000 | Loss: 0.00000959
Iteration 64/1000 | Loss: 0.00000958
Iteration 65/1000 | Loss: 0.00000958
Iteration 66/1000 | Loss: 0.00000958
Iteration 67/1000 | Loss: 0.00000958
Iteration 68/1000 | Loss: 0.00000958
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000957
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000957
Iteration 74/1000 | Loss: 0.00000957
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000956
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000954
Iteration 86/1000 | Loss: 0.00000954
Iteration 87/1000 | Loss: 0.00000954
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000953
Iteration 90/1000 | Loss: 0.00000953
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000952
Iteration 95/1000 | Loss: 0.00000952
Iteration 96/1000 | Loss: 0.00000952
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000952
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000952
Iteration 104/1000 | Loss: 0.00000952
Iteration 105/1000 | Loss: 0.00000952
Iteration 106/1000 | Loss: 0.00000952
Iteration 107/1000 | Loss: 0.00000952
Iteration 108/1000 | Loss: 0.00000952
Iteration 109/1000 | Loss: 0.00000952
Iteration 110/1000 | Loss: 0.00000952
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000952
Iteration 113/1000 | Loss: 0.00000952
Iteration 114/1000 | Loss: 0.00000952
Iteration 115/1000 | Loss: 0.00000952
Iteration 116/1000 | Loss: 0.00000952
Iteration 117/1000 | Loss: 0.00000952
Iteration 118/1000 | Loss: 0.00000952
Iteration 119/1000 | Loss: 0.00000952
Iteration 120/1000 | Loss: 0.00000952
Iteration 121/1000 | Loss: 0.00000952
Iteration 122/1000 | Loss: 0.00000952
Iteration 123/1000 | Loss: 0.00000952
Iteration 124/1000 | Loss: 0.00000952
Iteration 125/1000 | Loss: 0.00000952
Iteration 126/1000 | Loss: 0.00000952
Iteration 127/1000 | Loss: 0.00000952
Iteration 128/1000 | Loss: 0.00000952
Iteration 129/1000 | Loss: 0.00000952
Iteration 130/1000 | Loss: 0.00000952
Iteration 131/1000 | Loss: 0.00000952
Iteration 132/1000 | Loss: 0.00000952
Iteration 133/1000 | Loss: 0.00000952
Iteration 134/1000 | Loss: 0.00000952
Iteration 135/1000 | Loss: 0.00000952
Iteration 136/1000 | Loss: 0.00000952
Iteration 137/1000 | Loss: 0.00000952
Iteration 138/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [9.5164286904037e-06, 9.5164286904037e-06, 9.5164286904037e-06, 9.5164286904037e-06, 9.5164286904037e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.5164286904037e-06

Optimization complete. Final v2v error: 2.6474952697753906 mm

Highest mean error: 3.010403633117676 mm for frame 139

Lowest mean error: 2.52803111076355 mm for frame 42

Saving results

Total time: 32.505934715270996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392022
Iteration 2/25 | Loss: 0.00110426
Iteration 3/25 | Loss: 0.00098707
Iteration 4/25 | Loss: 0.00097437
Iteration 5/25 | Loss: 0.00097156
Iteration 6/25 | Loss: 0.00097075
Iteration 7/25 | Loss: 0.00097052
Iteration 8/25 | Loss: 0.00097052
Iteration 9/25 | Loss: 0.00097052
Iteration 10/25 | Loss: 0.00097052
Iteration 11/25 | Loss: 0.00097052
Iteration 12/25 | Loss: 0.00097052
Iteration 13/25 | Loss: 0.00097052
Iteration 14/25 | Loss: 0.00097052
Iteration 15/25 | Loss: 0.00097052
Iteration 16/25 | Loss: 0.00097052
Iteration 17/25 | Loss: 0.00097052
Iteration 18/25 | Loss: 0.00097052
Iteration 19/25 | Loss: 0.00097052
Iteration 20/25 | Loss: 0.00097052
Iteration 21/25 | Loss: 0.00097052
Iteration 22/25 | Loss: 0.00097052
Iteration 23/25 | Loss: 0.00097052
Iteration 24/25 | Loss: 0.00097052
Iteration 25/25 | Loss: 0.00097052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48095977
Iteration 2/25 | Loss: 0.00068828
Iteration 3/25 | Loss: 0.00068828
Iteration 4/25 | Loss: 0.00068828
Iteration 5/25 | Loss: 0.00068828
Iteration 6/25 | Loss: 0.00068828
Iteration 7/25 | Loss: 0.00068827
Iteration 8/25 | Loss: 0.00068827
Iteration 9/25 | Loss: 0.00068827
Iteration 10/25 | Loss: 0.00068827
Iteration 11/25 | Loss: 0.00068827
Iteration 12/25 | Loss: 0.00068827
Iteration 13/25 | Loss: 0.00068827
Iteration 14/25 | Loss: 0.00068827
Iteration 15/25 | Loss: 0.00068827
Iteration 16/25 | Loss: 0.00068827
Iteration 17/25 | Loss: 0.00068827
Iteration 18/25 | Loss: 0.00068827
Iteration 19/25 | Loss: 0.00068827
Iteration 20/25 | Loss: 0.00068827
Iteration 21/25 | Loss: 0.00068827
Iteration 22/25 | Loss: 0.00068827
Iteration 23/25 | Loss: 0.00068827
Iteration 24/25 | Loss: 0.00068827
Iteration 25/25 | Loss: 0.00068827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068827
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001242
Iteration 5/1000 | Loss: 0.00001122
Iteration 6/1000 | Loss: 0.00001059
Iteration 7/1000 | Loss: 0.00001020
Iteration 8/1000 | Loss: 0.00000992
Iteration 9/1000 | Loss: 0.00000982
Iteration 10/1000 | Loss: 0.00000975
Iteration 11/1000 | Loss: 0.00000969
Iteration 12/1000 | Loss: 0.00000966
Iteration 13/1000 | Loss: 0.00000966
Iteration 14/1000 | Loss: 0.00000965
Iteration 15/1000 | Loss: 0.00000962
Iteration 16/1000 | Loss: 0.00000960
Iteration 17/1000 | Loss: 0.00000959
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000950
Iteration 23/1000 | Loss: 0.00000950
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000947
Iteration 34/1000 | Loss: 0.00000946
Iteration 35/1000 | Loss: 0.00000946
Iteration 36/1000 | Loss: 0.00000946
Iteration 37/1000 | Loss: 0.00000945
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000943
Iteration 40/1000 | Loss: 0.00000943
Iteration 41/1000 | Loss: 0.00000943
Iteration 42/1000 | Loss: 0.00000943
Iteration 43/1000 | Loss: 0.00000943
Iteration 44/1000 | Loss: 0.00000942
Iteration 45/1000 | Loss: 0.00000942
Iteration 46/1000 | Loss: 0.00000942
Iteration 47/1000 | Loss: 0.00000941
Iteration 48/1000 | Loss: 0.00000941
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000939
Iteration 53/1000 | Loss: 0.00000939
Iteration 54/1000 | Loss: 0.00000938
Iteration 55/1000 | Loss: 0.00000937
Iteration 56/1000 | Loss: 0.00000937
Iteration 57/1000 | Loss: 0.00000937
Iteration 58/1000 | Loss: 0.00000937
Iteration 59/1000 | Loss: 0.00000937
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000936
Iteration 63/1000 | Loss: 0.00000936
Iteration 64/1000 | Loss: 0.00000936
Iteration 65/1000 | Loss: 0.00000936
Iteration 66/1000 | Loss: 0.00000936
Iteration 67/1000 | Loss: 0.00000936
Iteration 68/1000 | Loss: 0.00000936
Iteration 69/1000 | Loss: 0.00000936
Iteration 70/1000 | Loss: 0.00000936
Iteration 71/1000 | Loss: 0.00000935
Iteration 72/1000 | Loss: 0.00000935
Iteration 73/1000 | Loss: 0.00000935
Iteration 74/1000 | Loss: 0.00000935
Iteration 75/1000 | Loss: 0.00000934
Iteration 76/1000 | Loss: 0.00000934
Iteration 77/1000 | Loss: 0.00000934
Iteration 78/1000 | Loss: 0.00000933
Iteration 79/1000 | Loss: 0.00000933
Iteration 80/1000 | Loss: 0.00000932
Iteration 81/1000 | Loss: 0.00000932
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000932
Iteration 84/1000 | Loss: 0.00000932
Iteration 85/1000 | Loss: 0.00000932
Iteration 86/1000 | Loss: 0.00000932
Iteration 87/1000 | Loss: 0.00000932
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000931
Iteration 91/1000 | Loss: 0.00000931
Iteration 92/1000 | Loss: 0.00000931
Iteration 93/1000 | Loss: 0.00000930
Iteration 94/1000 | Loss: 0.00000930
Iteration 95/1000 | Loss: 0.00000930
Iteration 96/1000 | Loss: 0.00000930
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000930
Iteration 101/1000 | Loss: 0.00000930
Iteration 102/1000 | Loss: 0.00000930
Iteration 103/1000 | Loss: 0.00000930
Iteration 104/1000 | Loss: 0.00000930
Iteration 105/1000 | Loss: 0.00000930
Iteration 106/1000 | Loss: 0.00000929
Iteration 107/1000 | Loss: 0.00000929
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000928
Iteration 112/1000 | Loss: 0.00000928
Iteration 113/1000 | Loss: 0.00000928
Iteration 114/1000 | Loss: 0.00000928
Iteration 115/1000 | Loss: 0.00000928
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000927
Iteration 121/1000 | Loss: 0.00000927
Iteration 122/1000 | Loss: 0.00000927
Iteration 123/1000 | Loss: 0.00000927
Iteration 124/1000 | Loss: 0.00000927
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000927
Iteration 134/1000 | Loss: 0.00000927
Iteration 135/1000 | Loss: 0.00000927
Iteration 136/1000 | Loss: 0.00000927
Iteration 137/1000 | Loss: 0.00000927
Iteration 138/1000 | Loss: 0.00000926
Iteration 139/1000 | Loss: 0.00000926
Iteration 140/1000 | Loss: 0.00000926
Iteration 141/1000 | Loss: 0.00000926
Iteration 142/1000 | Loss: 0.00000926
Iteration 143/1000 | Loss: 0.00000926
Iteration 144/1000 | Loss: 0.00000926
Iteration 145/1000 | Loss: 0.00000926
Iteration 146/1000 | Loss: 0.00000926
Iteration 147/1000 | Loss: 0.00000926
Iteration 148/1000 | Loss: 0.00000925
Iteration 149/1000 | Loss: 0.00000925
Iteration 150/1000 | Loss: 0.00000925
Iteration 151/1000 | Loss: 0.00000925
Iteration 152/1000 | Loss: 0.00000925
Iteration 153/1000 | Loss: 0.00000925
Iteration 154/1000 | Loss: 0.00000924
Iteration 155/1000 | Loss: 0.00000924
Iteration 156/1000 | Loss: 0.00000924
Iteration 157/1000 | Loss: 0.00000924
Iteration 158/1000 | Loss: 0.00000924
Iteration 159/1000 | Loss: 0.00000924
Iteration 160/1000 | Loss: 0.00000924
Iteration 161/1000 | Loss: 0.00000924
Iteration 162/1000 | Loss: 0.00000924
Iteration 163/1000 | Loss: 0.00000924
Iteration 164/1000 | Loss: 0.00000924
Iteration 165/1000 | Loss: 0.00000924
Iteration 166/1000 | Loss: 0.00000923
Iteration 167/1000 | Loss: 0.00000923
Iteration 168/1000 | Loss: 0.00000923
Iteration 169/1000 | Loss: 0.00000923
Iteration 170/1000 | Loss: 0.00000923
Iteration 171/1000 | Loss: 0.00000923
Iteration 172/1000 | Loss: 0.00000923
Iteration 173/1000 | Loss: 0.00000923
Iteration 174/1000 | Loss: 0.00000923
Iteration 175/1000 | Loss: 0.00000923
Iteration 176/1000 | Loss: 0.00000923
Iteration 177/1000 | Loss: 0.00000923
Iteration 178/1000 | Loss: 0.00000923
Iteration 179/1000 | Loss: 0.00000923
Iteration 180/1000 | Loss: 0.00000923
Iteration 181/1000 | Loss: 0.00000923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [9.234876415575854e-06, 9.234876415575854e-06, 9.234876415575854e-06, 9.234876415575854e-06, 9.234876415575854e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.234876415575854e-06

Optimization complete. Final v2v error: 2.547192096710205 mm

Highest mean error: 3.742363452911377 mm for frame 55

Lowest mean error: 2.2796831130981445 mm for frame 103

Saving results

Total time: 36.615355014801025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00361648
Iteration 2/25 | Loss: 0.00110905
Iteration 3/25 | Loss: 0.00103092
Iteration 4/25 | Loss: 0.00101548
Iteration 5/25 | Loss: 0.00100856
Iteration 6/25 | Loss: 0.00100710
Iteration 7/25 | Loss: 0.00100710
Iteration 8/25 | Loss: 0.00100710
Iteration 9/25 | Loss: 0.00100710
Iteration 10/25 | Loss: 0.00100710
Iteration 11/25 | Loss: 0.00100710
Iteration 12/25 | Loss: 0.00100710
Iteration 13/25 | Loss: 0.00100710
Iteration 14/25 | Loss: 0.00100710
Iteration 15/25 | Loss: 0.00100710
Iteration 16/25 | Loss: 0.00100710
Iteration 17/25 | Loss: 0.00100710
Iteration 18/25 | Loss: 0.00100710
Iteration 19/25 | Loss: 0.00100710
Iteration 20/25 | Loss: 0.00100710
Iteration 21/25 | Loss: 0.00100710
Iteration 22/25 | Loss: 0.00100710
Iteration 23/25 | Loss: 0.00100710
Iteration 24/25 | Loss: 0.00100710
Iteration 25/25 | Loss: 0.00100710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86978620
Iteration 2/25 | Loss: 0.00091476
Iteration 3/25 | Loss: 0.00091476
Iteration 4/25 | Loss: 0.00091476
Iteration 5/25 | Loss: 0.00091476
Iteration 6/25 | Loss: 0.00091476
Iteration 7/25 | Loss: 0.00091476
Iteration 8/25 | Loss: 0.00091476
Iteration 9/25 | Loss: 0.00091476
Iteration 10/25 | Loss: 0.00091476
Iteration 11/25 | Loss: 0.00091476
Iteration 12/25 | Loss: 0.00091476
Iteration 13/25 | Loss: 0.00091476
Iteration 14/25 | Loss: 0.00091476
Iteration 15/25 | Loss: 0.00091476
Iteration 16/25 | Loss: 0.00091476
Iteration 17/25 | Loss: 0.00091476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009147582459263504, 0.0009147582459263504, 0.0009147582459263504, 0.0009147582459263504, 0.0009147582459263504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009147582459263504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091476
Iteration 2/1000 | Loss: 0.00003225
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001886
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001469
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001449
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001437
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001410
Iteration 78/1000 | Loss: 0.00001410
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001398
Iteration 99/1000 | Loss: 0.00001398
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001398
Iteration 116/1000 | Loss: 0.00001398
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3983025382913183e-05, 1.3983025382913183e-05, 1.3983025382913183e-05, 1.3983025382913183e-05, 1.3983025382913183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3983025382913183e-05

Optimization complete. Final v2v error: 3.1481831073760986 mm

Highest mean error: 3.2435905933380127 mm for frame 215

Lowest mean error: 3.0858402252197266 mm for frame 38

Saving results

Total time: 47.586010456085205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458554
Iteration 2/25 | Loss: 0.00113228
Iteration 3/25 | Loss: 0.00103549
Iteration 4/25 | Loss: 0.00102645
Iteration 5/25 | Loss: 0.00102340
Iteration 6/25 | Loss: 0.00102340
Iteration 7/25 | Loss: 0.00102340
Iteration 8/25 | Loss: 0.00102340
Iteration 9/25 | Loss: 0.00102340
Iteration 10/25 | Loss: 0.00102340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010234022047370672, 0.0010234022047370672, 0.0010234022047370672, 0.0010234022047370672, 0.0010234022047370672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010234022047370672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40346575
Iteration 2/25 | Loss: 0.00065585
Iteration 3/25 | Loss: 0.00065585
Iteration 4/25 | Loss: 0.00065585
Iteration 5/25 | Loss: 0.00065585
Iteration 6/25 | Loss: 0.00065585
Iteration 7/25 | Loss: 0.00065585
Iteration 8/25 | Loss: 0.00065585
Iteration 9/25 | Loss: 0.00065585
Iteration 10/25 | Loss: 0.00065585
Iteration 11/25 | Loss: 0.00065585
Iteration 12/25 | Loss: 0.00065585
Iteration 13/25 | Loss: 0.00065585
Iteration 14/25 | Loss: 0.00065585
Iteration 15/25 | Loss: 0.00065585
Iteration 16/25 | Loss: 0.00065585
Iteration 17/25 | Loss: 0.00065585
Iteration 18/25 | Loss: 0.00065585
Iteration 19/25 | Loss: 0.00065585
Iteration 20/25 | Loss: 0.00065585
Iteration 21/25 | Loss: 0.00065585
Iteration 22/25 | Loss: 0.00065585
Iteration 23/25 | Loss: 0.00065585
Iteration 24/25 | Loss: 0.00065585
Iteration 25/25 | Loss: 0.00065585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065585
Iteration 2/1000 | Loss: 0.00003111
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001854
Iteration 5/1000 | Loss: 0.00001773
Iteration 6/1000 | Loss: 0.00001689
Iteration 7/1000 | Loss: 0.00001632
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001534
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001516
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001506
Iteration 45/1000 | Loss: 0.00001506
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001506
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001503
Iteration 55/1000 | Loss: 0.00001503
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001502
Iteration 62/1000 | Loss: 0.00001502
Iteration 63/1000 | Loss: 0.00001502
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001501
Iteration 68/1000 | Loss: 0.00001501
Iteration 69/1000 | Loss: 0.00001501
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001500
Iteration 73/1000 | Loss: 0.00001499
Iteration 74/1000 | Loss: 0.00001499
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001496
Iteration 80/1000 | Loss: 0.00001496
Iteration 81/1000 | Loss: 0.00001496
Iteration 82/1000 | Loss: 0.00001496
Iteration 83/1000 | Loss: 0.00001496
Iteration 84/1000 | Loss: 0.00001496
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001495
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001492
Iteration 93/1000 | Loss: 0.00001492
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001486
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001485
Iteration 114/1000 | Loss: 0.00001485
Iteration 115/1000 | Loss: 0.00001485
Iteration 116/1000 | Loss: 0.00001485
Iteration 117/1000 | Loss: 0.00001485
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001483
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001482
Iteration 128/1000 | Loss: 0.00001482
Iteration 129/1000 | Loss: 0.00001481
Iteration 130/1000 | Loss: 0.00001481
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001481
Iteration 133/1000 | Loss: 0.00001481
Iteration 134/1000 | Loss: 0.00001481
Iteration 135/1000 | Loss: 0.00001481
Iteration 136/1000 | Loss: 0.00001481
Iteration 137/1000 | Loss: 0.00001481
Iteration 138/1000 | Loss: 0.00001481
Iteration 139/1000 | Loss: 0.00001481
Iteration 140/1000 | Loss: 0.00001481
Iteration 141/1000 | Loss: 0.00001481
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001479
Iteration 147/1000 | Loss: 0.00001479
Iteration 148/1000 | Loss: 0.00001479
Iteration 149/1000 | Loss: 0.00001478
Iteration 150/1000 | Loss: 0.00001478
Iteration 151/1000 | Loss: 0.00001478
Iteration 152/1000 | Loss: 0.00001478
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001478
Iteration 155/1000 | Loss: 0.00001478
Iteration 156/1000 | Loss: 0.00001478
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001477
Iteration 159/1000 | Loss: 0.00001477
Iteration 160/1000 | Loss: 0.00001477
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001477
Iteration 163/1000 | Loss: 0.00001477
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001477
Iteration 166/1000 | Loss: 0.00001476
Iteration 167/1000 | Loss: 0.00001476
Iteration 168/1000 | Loss: 0.00001476
Iteration 169/1000 | Loss: 0.00001476
Iteration 170/1000 | Loss: 0.00001476
Iteration 171/1000 | Loss: 0.00001476
Iteration 172/1000 | Loss: 0.00001476
Iteration 173/1000 | Loss: 0.00001476
Iteration 174/1000 | Loss: 0.00001476
Iteration 175/1000 | Loss: 0.00001476
Iteration 176/1000 | Loss: 0.00001476
Iteration 177/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4764114894205704e-05, 1.4764114894205704e-05, 1.4764114894205704e-05, 1.4764114894205704e-05, 1.4764114894205704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4764114894205704e-05

Optimization complete. Final v2v error: 3.2274796962738037 mm

Highest mean error: 3.6730663776397705 mm for frame 15

Lowest mean error: 2.663867712020874 mm for frame 191

Saving results

Total time: 43.188998222351074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897499
Iteration 2/25 | Loss: 0.00162122
Iteration 3/25 | Loss: 0.00122595
Iteration 4/25 | Loss: 0.00113035
Iteration 5/25 | Loss: 0.00111662
Iteration 6/25 | Loss: 0.00114973
Iteration 7/25 | Loss: 0.00117193
Iteration 8/25 | Loss: 0.00106736
Iteration 9/25 | Loss: 0.00106827
Iteration 10/25 | Loss: 0.00101713
Iteration 11/25 | Loss: 0.00100848
Iteration 12/25 | Loss: 0.00100057
Iteration 13/25 | Loss: 0.00100225
Iteration 14/25 | Loss: 0.00099832
Iteration 15/25 | Loss: 0.00100905
Iteration 16/25 | Loss: 0.00099954
Iteration 17/25 | Loss: 0.00099523
Iteration 18/25 | Loss: 0.00099640
Iteration 19/25 | Loss: 0.00099310
Iteration 20/25 | Loss: 0.00099214
Iteration 21/25 | Loss: 0.00099737
Iteration 22/25 | Loss: 0.00099163
Iteration 23/25 | Loss: 0.00099154
Iteration 24/25 | Loss: 0.00099154
Iteration 25/25 | Loss: 0.00099154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42019868
Iteration 2/25 | Loss: 0.00066667
Iteration 3/25 | Loss: 0.00066666
Iteration 4/25 | Loss: 0.00066666
Iteration 5/25 | Loss: 0.00066666
Iteration 6/25 | Loss: 0.00066666
Iteration 7/25 | Loss: 0.00066666
Iteration 8/25 | Loss: 0.00066666
Iteration 9/25 | Loss: 0.00066666
Iteration 10/25 | Loss: 0.00066666
Iteration 11/25 | Loss: 0.00066666
Iteration 12/25 | Loss: 0.00066666
Iteration 13/25 | Loss: 0.00066666
Iteration 14/25 | Loss: 0.00066666
Iteration 15/25 | Loss: 0.00066666
Iteration 16/25 | Loss: 0.00066666
Iteration 17/25 | Loss: 0.00066666
Iteration 18/25 | Loss: 0.00066666
Iteration 19/25 | Loss: 0.00066666
Iteration 20/25 | Loss: 0.00066666
Iteration 21/25 | Loss: 0.00066666
Iteration 22/25 | Loss: 0.00066666
Iteration 23/25 | Loss: 0.00066666
Iteration 24/25 | Loss: 0.00066666
Iteration 25/25 | Loss: 0.00066666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066666
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00008259
Iteration 4/1000 | Loss: 0.00001647
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00007506
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001222
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00010316
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001162
Iteration 156/1000 | Loss: 0.00001162
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Iteration 173/1000 | Loss: 0.00001159
Iteration 174/1000 | Loss: 0.00001159
Iteration 175/1000 | Loss: 0.00001159
Iteration 176/1000 | Loss: 0.00001159
Iteration 177/1000 | Loss: 0.00001159
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001159
Iteration 193/1000 | Loss: 0.00001159
Iteration 194/1000 | Loss: 0.00001159
Iteration 195/1000 | Loss: 0.00001159
Iteration 196/1000 | Loss: 0.00001159
Iteration 197/1000 | Loss: 0.00001159
Iteration 198/1000 | Loss: 0.00001159
Iteration 199/1000 | Loss: 0.00001159
Iteration 200/1000 | Loss: 0.00001159
Iteration 201/1000 | Loss: 0.00001159
Iteration 202/1000 | Loss: 0.00001159
Iteration 203/1000 | Loss: 0.00001159
Iteration 204/1000 | Loss: 0.00001159
Iteration 205/1000 | Loss: 0.00001159
Iteration 206/1000 | Loss: 0.00001159
Iteration 207/1000 | Loss: 0.00001159
Iteration 208/1000 | Loss: 0.00001159
Iteration 209/1000 | Loss: 0.00001159
Iteration 210/1000 | Loss: 0.00001159
Iteration 211/1000 | Loss: 0.00001159
Iteration 212/1000 | Loss: 0.00001159
Iteration 213/1000 | Loss: 0.00001159
Iteration 214/1000 | Loss: 0.00001159
Iteration 215/1000 | Loss: 0.00001159
Iteration 216/1000 | Loss: 0.00001159
Iteration 217/1000 | Loss: 0.00001159
Iteration 218/1000 | Loss: 0.00001159
Iteration 219/1000 | Loss: 0.00001159
Iteration 220/1000 | Loss: 0.00001159
Iteration 221/1000 | Loss: 0.00001159
Iteration 222/1000 | Loss: 0.00001159
Iteration 223/1000 | Loss: 0.00001159
Iteration 224/1000 | Loss: 0.00001159
Iteration 225/1000 | Loss: 0.00001159
Iteration 226/1000 | Loss: 0.00001159
Iteration 227/1000 | Loss: 0.00001159
Iteration 228/1000 | Loss: 0.00001159
Iteration 229/1000 | Loss: 0.00001159
Iteration 230/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.1585204447328579e-05, 1.1585204447328579e-05, 1.1585204447328579e-05, 1.1585204447328579e-05, 1.1585204447328579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1585204447328579e-05

Optimization complete. Final v2v error: 2.9129750728607178 mm

Highest mean error: 4.105992317199707 mm for frame 75

Lowest mean error: 2.63167142868042 mm for frame 105

Saving results

Total time: 72.45072722434998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00211266
Iteration 2/25 | Loss: 0.00105841
Iteration 3/25 | Loss: 0.00096666
Iteration 4/25 | Loss: 0.00094212
Iteration 5/25 | Loss: 0.00093219
Iteration 6/25 | Loss: 0.00092960
Iteration 7/25 | Loss: 0.00092888
Iteration 8/25 | Loss: 0.00092881
Iteration 9/25 | Loss: 0.00092881
Iteration 10/25 | Loss: 0.00092881
Iteration 11/25 | Loss: 0.00092881
Iteration 12/25 | Loss: 0.00092881
Iteration 13/25 | Loss: 0.00092881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009288123692385852, 0.0009288123692385852, 0.0009288123692385852, 0.0009288123692385852, 0.0009288123692385852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009288123692385852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34047318
Iteration 2/25 | Loss: 0.00112021
Iteration 3/25 | Loss: 0.00112020
Iteration 4/25 | Loss: 0.00112020
Iteration 5/25 | Loss: 0.00112020
Iteration 6/25 | Loss: 0.00112020
Iteration 7/25 | Loss: 0.00112020
Iteration 8/25 | Loss: 0.00112020
Iteration 9/25 | Loss: 0.00112020
Iteration 10/25 | Loss: 0.00112020
Iteration 11/25 | Loss: 0.00112020
Iteration 12/25 | Loss: 0.00112020
Iteration 13/25 | Loss: 0.00112020
Iteration 14/25 | Loss: 0.00112020
Iteration 15/25 | Loss: 0.00112020
Iteration 16/25 | Loss: 0.00112020
Iteration 17/25 | Loss: 0.00112020
Iteration 18/25 | Loss: 0.00112020
Iteration 19/25 | Loss: 0.00112020
Iteration 20/25 | Loss: 0.00112020
Iteration 21/25 | Loss: 0.00112020
Iteration 22/25 | Loss: 0.00112020
Iteration 23/25 | Loss: 0.00112020
Iteration 24/25 | Loss: 0.00112020
Iteration 25/25 | Loss: 0.00112020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112020
Iteration 2/1000 | Loss: 0.00004038
Iteration 3/1000 | Loss: 0.00002261
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001253
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001151
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001108
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001090
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001076
Iteration 24/1000 | Loss: 0.00001076
Iteration 25/1000 | Loss: 0.00001076
Iteration 26/1000 | Loss: 0.00001075
Iteration 27/1000 | Loss: 0.00001075
Iteration 28/1000 | Loss: 0.00001075
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001075
Iteration 32/1000 | Loss: 0.00001074
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001073
Iteration 35/1000 | Loss: 0.00001073
Iteration 36/1000 | Loss: 0.00001072
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001070
Iteration 40/1000 | Loss: 0.00001070
Iteration 41/1000 | Loss: 0.00001070
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001069
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001061
Iteration 58/1000 | Loss: 0.00001061
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001059
Iteration 66/1000 | Loss: 0.00001059
Iteration 67/1000 | Loss: 0.00001058
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001057
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001055
Iteration 77/1000 | Loss: 0.00001055
Iteration 78/1000 | Loss: 0.00001055
Iteration 79/1000 | Loss: 0.00001055
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001054
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001053
Iteration 85/1000 | Loss: 0.00001053
Iteration 86/1000 | Loss: 0.00001053
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001052
Iteration 89/1000 | Loss: 0.00001052
Iteration 90/1000 | Loss: 0.00001052
Iteration 91/1000 | Loss: 0.00001051
Iteration 92/1000 | Loss: 0.00001051
Iteration 93/1000 | Loss: 0.00001051
Iteration 94/1000 | Loss: 0.00001051
Iteration 95/1000 | Loss: 0.00001051
Iteration 96/1000 | Loss: 0.00001050
Iteration 97/1000 | Loss: 0.00001050
Iteration 98/1000 | Loss: 0.00001050
Iteration 99/1000 | Loss: 0.00001050
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001048
Iteration 108/1000 | Loss: 0.00001048
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001046
Iteration 119/1000 | Loss: 0.00001046
Iteration 120/1000 | Loss: 0.00001046
Iteration 121/1000 | Loss: 0.00001046
Iteration 122/1000 | Loss: 0.00001045
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001045
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001045
Iteration 129/1000 | Loss: 0.00001045
Iteration 130/1000 | Loss: 0.00001045
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001044
Iteration 135/1000 | Loss: 0.00001044
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001044
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001044
Iteration 142/1000 | Loss: 0.00001044
Iteration 143/1000 | Loss: 0.00001044
Iteration 144/1000 | Loss: 0.00001044
Iteration 145/1000 | Loss: 0.00001044
Iteration 146/1000 | Loss: 0.00001044
Iteration 147/1000 | Loss: 0.00001044
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Iteration 164/1000 | Loss: 0.00001042
Iteration 165/1000 | Loss: 0.00001042
Iteration 166/1000 | Loss: 0.00001042
Iteration 167/1000 | Loss: 0.00001042
Iteration 168/1000 | Loss: 0.00001042
Iteration 169/1000 | Loss: 0.00001042
Iteration 170/1000 | Loss: 0.00001041
Iteration 171/1000 | Loss: 0.00001041
Iteration 172/1000 | Loss: 0.00001041
Iteration 173/1000 | Loss: 0.00001041
Iteration 174/1000 | Loss: 0.00001041
Iteration 175/1000 | Loss: 0.00001041
Iteration 176/1000 | Loss: 0.00001041
Iteration 177/1000 | Loss: 0.00001041
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001040
Iteration 181/1000 | Loss: 0.00001040
Iteration 182/1000 | Loss: 0.00001040
Iteration 183/1000 | Loss: 0.00001040
Iteration 184/1000 | Loss: 0.00001040
Iteration 185/1000 | Loss: 0.00001040
Iteration 186/1000 | Loss: 0.00001040
Iteration 187/1000 | Loss: 0.00001040
Iteration 188/1000 | Loss: 0.00001040
Iteration 189/1000 | Loss: 0.00001040
Iteration 190/1000 | Loss: 0.00001040
Iteration 191/1000 | Loss: 0.00001040
Iteration 192/1000 | Loss: 0.00001040
Iteration 193/1000 | Loss: 0.00001040
Iteration 194/1000 | Loss: 0.00001040
Iteration 195/1000 | Loss: 0.00001040
Iteration 196/1000 | Loss: 0.00001040
Iteration 197/1000 | Loss: 0.00001040
Iteration 198/1000 | Loss: 0.00001040
Iteration 199/1000 | Loss: 0.00001040
Iteration 200/1000 | Loss: 0.00001040
Iteration 201/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.040197821566835e-05, 1.040197821566835e-05, 1.040197821566835e-05, 1.040197821566835e-05, 1.040197821566835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.040197821566835e-05

Optimization complete. Final v2v error: 2.7468912601470947 mm

Highest mean error: 3.0997121334075928 mm for frame 48

Lowest mean error: 2.433445692062378 mm for frame 120

Saving results

Total time: 40.16093873977661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655852
Iteration 2/25 | Loss: 0.00108787
Iteration 3/25 | Loss: 0.00101527
Iteration 4/25 | Loss: 0.00101011
Iteration 5/25 | Loss: 0.00100855
Iteration 6/25 | Loss: 0.00100855
Iteration 7/25 | Loss: 0.00100855
Iteration 8/25 | Loss: 0.00100855
Iteration 9/25 | Loss: 0.00100855
Iteration 10/25 | Loss: 0.00100855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010085544781759381, 0.0010085544781759381, 0.0010085544781759381, 0.0010085544781759381, 0.0010085544781759381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010085544781759381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09282112
Iteration 2/25 | Loss: 0.00069140
Iteration 3/25 | Loss: 0.00069140
Iteration 4/25 | Loss: 0.00069140
Iteration 5/25 | Loss: 0.00069140
Iteration 6/25 | Loss: 0.00069140
Iteration 7/25 | Loss: 0.00069140
Iteration 8/25 | Loss: 0.00069140
Iteration 9/25 | Loss: 0.00069140
Iteration 10/25 | Loss: 0.00069140
Iteration 11/25 | Loss: 0.00069140
Iteration 12/25 | Loss: 0.00069140
Iteration 13/25 | Loss: 0.00069140
Iteration 14/25 | Loss: 0.00069140
Iteration 15/25 | Loss: 0.00069140
Iteration 16/25 | Loss: 0.00069140
Iteration 17/25 | Loss: 0.00069140
Iteration 18/25 | Loss: 0.00069140
Iteration 19/25 | Loss: 0.00069140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006913951365277171, 0.0006913951365277171, 0.0006913951365277171, 0.0006913951365277171, 0.0006913951365277171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006913951365277171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069140
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001627
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001542
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001414
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001404
Iteration 25/1000 | Loss: 0.00001403
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001402
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001395
Iteration 48/1000 | Loss: 0.00001395
Iteration 49/1000 | Loss: 0.00001395
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001388
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001380
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001380
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001379
Iteration 127/1000 | Loss: 0.00001379
Iteration 128/1000 | Loss: 0.00001379
Iteration 129/1000 | Loss: 0.00001379
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001378
Iteration 132/1000 | Loss: 0.00001378
Iteration 133/1000 | Loss: 0.00001378
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001376
Iteration 138/1000 | Loss: 0.00001376
Iteration 139/1000 | Loss: 0.00001376
Iteration 140/1000 | Loss: 0.00001376
Iteration 141/1000 | Loss: 0.00001376
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001376
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001376
Iteration 154/1000 | Loss: 0.00001376
Iteration 155/1000 | Loss: 0.00001376
Iteration 156/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.376206364511745e-05, 1.376206364511745e-05, 1.376206364511745e-05, 1.376206364511745e-05, 1.376206364511745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.376206364511745e-05

Optimization complete. Final v2v error: 3.184929609298706 mm

Highest mean error: 3.5731940269470215 mm for frame 52

Lowest mean error: 2.9590911865234375 mm for frame 153

Saving results

Total time: 36.09822368621826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066667
Iteration 2/25 | Loss: 0.01066667
Iteration 3/25 | Loss: 0.01066667
Iteration 4/25 | Loss: 0.01066667
Iteration 5/25 | Loss: 0.01066667
Iteration 6/25 | Loss: 0.01066667
Iteration 7/25 | Loss: 0.01066667
Iteration 8/25 | Loss: 0.01066667
Iteration 9/25 | Loss: 0.01066666
Iteration 10/25 | Loss: 0.01066666
Iteration 11/25 | Loss: 0.01066666
Iteration 12/25 | Loss: 0.01066666
Iteration 13/25 | Loss: 0.01066666
Iteration 14/25 | Loss: 0.01066666
Iteration 15/25 | Loss: 0.01066666
Iteration 16/25 | Loss: 0.01066666
Iteration 17/25 | Loss: 0.01066666
Iteration 18/25 | Loss: 0.01066666
Iteration 19/25 | Loss: 0.01066666
Iteration 20/25 | Loss: 0.01066666
Iteration 21/25 | Loss: 0.01066665
Iteration 22/25 | Loss: 0.01066665
Iteration 23/25 | Loss: 0.01066665
Iteration 24/25 | Loss: 0.01066665
Iteration 25/25 | Loss: 0.01066665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63252223
Iteration 2/25 | Loss: 0.08104652
Iteration 3/25 | Loss: 0.08103773
Iteration 4/25 | Loss: 0.08103772
Iteration 5/25 | Loss: 0.08103771
Iteration 6/25 | Loss: 0.08103771
Iteration 7/25 | Loss: 0.08103771
Iteration 8/25 | Loss: 0.08103771
Iteration 9/25 | Loss: 0.08103771
Iteration 10/25 | Loss: 0.08103771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08103770762681961, 0.08103770762681961, 0.08103770762681961, 0.08103770762681961, 0.08103770762681961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08103770762681961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08103770
Iteration 2/1000 | Loss: 0.00044683
Iteration 3/1000 | Loss: 0.00014341
Iteration 4/1000 | Loss: 0.00005931
Iteration 5/1000 | Loss: 0.00003091
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001164
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001043
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000947
Iteration 22/1000 | Loss: 0.00000919
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000887
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000872
Iteration 27/1000 | Loss: 0.00000861
Iteration 28/1000 | Loss: 0.00000859
Iteration 29/1000 | Loss: 0.00000859
Iteration 30/1000 | Loss: 0.00000857
Iteration 31/1000 | Loss: 0.00000856
Iteration 32/1000 | Loss: 0.00000854
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000849
Iteration 35/1000 | Loss: 0.00000849
Iteration 36/1000 | Loss: 0.00000848
Iteration 37/1000 | Loss: 0.00000848
Iteration 38/1000 | Loss: 0.00000847
Iteration 39/1000 | Loss: 0.00000843
Iteration 40/1000 | Loss: 0.00000842
Iteration 41/1000 | Loss: 0.00000840
Iteration 42/1000 | Loss: 0.00000838
Iteration 43/1000 | Loss: 0.00000838
Iteration 44/1000 | Loss: 0.00000837
Iteration 45/1000 | Loss: 0.00000837
Iteration 46/1000 | Loss: 0.00000836
Iteration 47/1000 | Loss: 0.00000835
Iteration 48/1000 | Loss: 0.00000834
Iteration 49/1000 | Loss: 0.00000833
Iteration 50/1000 | Loss: 0.00000832
Iteration 51/1000 | Loss: 0.00000832
Iteration 52/1000 | Loss: 0.00000831
Iteration 53/1000 | Loss: 0.00000830
Iteration 54/1000 | Loss: 0.00000830
Iteration 55/1000 | Loss: 0.00000830
Iteration 56/1000 | Loss: 0.00000829
Iteration 57/1000 | Loss: 0.00000829
Iteration 58/1000 | Loss: 0.00000829
Iteration 59/1000 | Loss: 0.00000828
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000828
Iteration 62/1000 | Loss: 0.00000828
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000827
Iteration 65/1000 | Loss: 0.00000827
Iteration 66/1000 | Loss: 0.00000827
Iteration 67/1000 | Loss: 0.00000827
Iteration 68/1000 | Loss: 0.00000827
Iteration 69/1000 | Loss: 0.00000826
Iteration 70/1000 | Loss: 0.00000826
Iteration 71/1000 | Loss: 0.00000826
Iteration 72/1000 | Loss: 0.00000826
Iteration 73/1000 | Loss: 0.00000825
Iteration 74/1000 | Loss: 0.00000825
Iteration 75/1000 | Loss: 0.00000825
Iteration 76/1000 | Loss: 0.00000825
Iteration 77/1000 | Loss: 0.00000825
Iteration 78/1000 | Loss: 0.00000825
Iteration 79/1000 | Loss: 0.00000825
Iteration 80/1000 | Loss: 0.00000824
Iteration 81/1000 | Loss: 0.00000824
Iteration 82/1000 | Loss: 0.00000824
Iteration 83/1000 | Loss: 0.00000823
Iteration 84/1000 | Loss: 0.00000823
Iteration 85/1000 | Loss: 0.00000823
Iteration 86/1000 | Loss: 0.00000822
Iteration 87/1000 | Loss: 0.00000822
Iteration 88/1000 | Loss: 0.00000822
Iteration 89/1000 | Loss: 0.00000821
Iteration 90/1000 | Loss: 0.00000821
Iteration 91/1000 | Loss: 0.00000821
Iteration 92/1000 | Loss: 0.00000821
Iteration 93/1000 | Loss: 0.00000821
Iteration 94/1000 | Loss: 0.00000821
Iteration 95/1000 | Loss: 0.00000821
Iteration 96/1000 | Loss: 0.00000821
Iteration 97/1000 | Loss: 0.00000821
Iteration 98/1000 | Loss: 0.00000820
Iteration 99/1000 | Loss: 0.00000820
Iteration 100/1000 | Loss: 0.00000820
Iteration 101/1000 | Loss: 0.00000820
Iteration 102/1000 | Loss: 0.00000820
Iteration 103/1000 | Loss: 0.00000820
Iteration 104/1000 | Loss: 0.00000819
Iteration 105/1000 | Loss: 0.00000819
Iteration 106/1000 | Loss: 0.00000819
Iteration 107/1000 | Loss: 0.00000819
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000818
Iteration 110/1000 | Loss: 0.00000818
Iteration 111/1000 | Loss: 0.00000818
Iteration 112/1000 | Loss: 0.00000818
Iteration 113/1000 | Loss: 0.00000818
Iteration 114/1000 | Loss: 0.00000817
Iteration 115/1000 | Loss: 0.00000817
Iteration 116/1000 | Loss: 0.00000817
Iteration 117/1000 | Loss: 0.00000817
Iteration 118/1000 | Loss: 0.00000817
Iteration 119/1000 | Loss: 0.00000817
Iteration 120/1000 | Loss: 0.00000817
Iteration 121/1000 | Loss: 0.00000817
Iteration 122/1000 | Loss: 0.00000817
Iteration 123/1000 | Loss: 0.00000816
Iteration 124/1000 | Loss: 0.00000816
Iteration 125/1000 | Loss: 0.00000816
Iteration 126/1000 | Loss: 0.00000816
Iteration 127/1000 | Loss: 0.00000815
Iteration 128/1000 | Loss: 0.00000815
Iteration 129/1000 | Loss: 0.00000815
Iteration 130/1000 | Loss: 0.00000815
Iteration 131/1000 | Loss: 0.00000815
Iteration 132/1000 | Loss: 0.00000815
Iteration 133/1000 | Loss: 0.00000815
Iteration 134/1000 | Loss: 0.00000815
Iteration 135/1000 | Loss: 0.00000814
Iteration 136/1000 | Loss: 0.00000814
Iteration 137/1000 | Loss: 0.00000814
Iteration 138/1000 | Loss: 0.00000814
Iteration 139/1000 | Loss: 0.00000814
Iteration 140/1000 | Loss: 0.00000814
Iteration 141/1000 | Loss: 0.00000814
Iteration 142/1000 | Loss: 0.00000813
Iteration 143/1000 | Loss: 0.00000813
Iteration 144/1000 | Loss: 0.00000813
Iteration 145/1000 | Loss: 0.00000813
Iteration 146/1000 | Loss: 0.00000813
Iteration 147/1000 | Loss: 0.00000813
Iteration 148/1000 | Loss: 0.00000813
Iteration 149/1000 | Loss: 0.00000813
Iteration 150/1000 | Loss: 0.00000813
Iteration 151/1000 | Loss: 0.00000813
Iteration 152/1000 | Loss: 0.00000813
Iteration 153/1000 | Loss: 0.00000813
Iteration 154/1000 | Loss: 0.00000813
Iteration 155/1000 | Loss: 0.00000813
Iteration 156/1000 | Loss: 0.00000813
Iteration 157/1000 | Loss: 0.00000813
Iteration 158/1000 | Loss: 0.00000812
Iteration 159/1000 | Loss: 0.00000812
Iteration 160/1000 | Loss: 0.00000812
Iteration 161/1000 | Loss: 0.00000812
Iteration 162/1000 | Loss: 0.00000812
Iteration 163/1000 | Loss: 0.00000812
Iteration 164/1000 | Loss: 0.00000812
Iteration 165/1000 | Loss: 0.00000812
Iteration 166/1000 | Loss: 0.00000812
Iteration 167/1000 | Loss: 0.00000812
Iteration 168/1000 | Loss: 0.00000812
Iteration 169/1000 | Loss: 0.00000812
Iteration 170/1000 | Loss: 0.00000812
Iteration 171/1000 | Loss: 0.00000812
Iteration 172/1000 | Loss: 0.00000812
Iteration 173/1000 | Loss: 0.00000812
Iteration 174/1000 | Loss: 0.00000812
Iteration 175/1000 | Loss: 0.00000812
Iteration 176/1000 | Loss: 0.00000812
Iteration 177/1000 | Loss: 0.00000812
Iteration 178/1000 | Loss: 0.00000811
Iteration 179/1000 | Loss: 0.00000811
Iteration 180/1000 | Loss: 0.00000811
Iteration 181/1000 | Loss: 0.00000811
Iteration 182/1000 | Loss: 0.00000811
Iteration 183/1000 | Loss: 0.00000811
Iteration 184/1000 | Loss: 0.00000811
Iteration 185/1000 | Loss: 0.00000811
Iteration 186/1000 | Loss: 0.00000811
Iteration 187/1000 | Loss: 0.00000811
Iteration 188/1000 | Loss: 0.00000811
Iteration 189/1000 | Loss: 0.00000810
Iteration 190/1000 | Loss: 0.00000810
Iteration 191/1000 | Loss: 0.00000810
Iteration 192/1000 | Loss: 0.00000810
Iteration 193/1000 | Loss: 0.00000810
Iteration 194/1000 | Loss: 0.00000810
Iteration 195/1000 | Loss: 0.00000810
Iteration 196/1000 | Loss: 0.00000810
Iteration 197/1000 | Loss: 0.00000810
Iteration 198/1000 | Loss: 0.00000810
Iteration 199/1000 | Loss: 0.00000810
Iteration 200/1000 | Loss: 0.00000810
Iteration 201/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [8.102633728412911e-06, 8.102633728412911e-06, 8.102633728412911e-06, 8.102633728412911e-06, 8.102633728412911e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.102633728412911e-06

Optimization complete. Final v2v error: 2.459455728530884 mm

Highest mean error: 2.6690621376037598 mm for frame 158

Lowest mean error: 2.274458646774292 mm for frame 190

Saving results

Total time: 56.3598792552948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996779
Iteration 2/25 | Loss: 0.00211382
Iteration 3/25 | Loss: 0.00152138
Iteration 4/25 | Loss: 0.00136430
Iteration 5/25 | Loss: 0.00149076
Iteration 6/25 | Loss: 0.00136253
Iteration 7/25 | Loss: 0.00123374
Iteration 8/25 | Loss: 0.00115516
Iteration 9/25 | Loss: 0.00111169
Iteration 10/25 | Loss: 0.00107810
Iteration 11/25 | Loss: 0.00104983
Iteration 12/25 | Loss: 0.00104377
Iteration 13/25 | Loss: 0.00103871
Iteration 14/25 | Loss: 0.00103781
Iteration 15/25 | Loss: 0.00102404
Iteration 16/25 | Loss: 0.00102138
Iteration 17/25 | Loss: 0.00101447
Iteration 18/25 | Loss: 0.00100809
Iteration 19/25 | Loss: 0.00101029
Iteration 20/25 | Loss: 0.00100811
Iteration 21/25 | Loss: 0.00100817
Iteration 22/25 | Loss: 0.00100341
Iteration 23/25 | Loss: 0.00100642
Iteration 24/25 | Loss: 0.00100884
Iteration 25/25 | Loss: 0.00100350

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48754537
Iteration 2/25 | Loss: 0.00126036
Iteration 3/25 | Loss: 0.00066982
Iteration 4/25 | Loss: 0.00066982
Iteration 5/25 | Loss: 0.00066982
Iteration 6/25 | Loss: 0.00066982
Iteration 7/25 | Loss: 0.00066982
Iteration 8/25 | Loss: 0.00066982
Iteration 9/25 | Loss: 0.00066982
Iteration 10/25 | Loss: 0.00066982
Iteration 11/25 | Loss: 0.00066982
Iteration 12/25 | Loss: 0.00066982
Iteration 13/25 | Loss: 0.00066982
Iteration 14/25 | Loss: 0.00066982
Iteration 15/25 | Loss: 0.00066982
Iteration 16/25 | Loss: 0.00066982
Iteration 17/25 | Loss: 0.00066982
Iteration 18/25 | Loss: 0.00066982
Iteration 19/25 | Loss: 0.00066982
Iteration 20/25 | Loss: 0.00066982
Iteration 21/25 | Loss: 0.00066982
Iteration 22/25 | Loss: 0.00066982
Iteration 23/25 | Loss: 0.00066982
Iteration 24/25 | Loss: 0.00066982
Iteration 25/25 | Loss: 0.00066982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066982
Iteration 2/1000 | Loss: 0.00005738
Iteration 3/1000 | Loss: 0.00231286
Iteration 4/1000 | Loss: 0.00011089
Iteration 5/1000 | Loss: 0.00032710
Iteration 6/1000 | Loss: 0.00003729
Iteration 7/1000 | Loss: 0.00003629
Iteration 8/1000 | Loss: 0.00002357
Iteration 9/1000 | Loss: 0.00001967
Iteration 10/1000 | Loss: 0.00002267
Iteration 11/1000 | Loss: 0.00002373
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00054870
Iteration 14/1000 | Loss: 0.00002762
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002447
Iteration 18/1000 | Loss: 0.00002456
Iteration 19/1000 | Loss: 0.00001882
Iteration 20/1000 | Loss: 0.00002005
Iteration 21/1000 | Loss: 0.00001958
Iteration 22/1000 | Loss: 0.00002706
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00002414
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00002563
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002566
Iteration 29/1000 | Loss: 0.00002746
Iteration 30/1000 | Loss: 0.00003232
Iteration 31/1000 | Loss: 0.00002077
Iteration 32/1000 | Loss: 0.00002492
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00002565
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00003001
Iteration 37/1000 | Loss: 0.00002666
Iteration 38/1000 | Loss: 0.00002633
Iteration 39/1000 | Loss: 0.00036353
Iteration 40/1000 | Loss: 0.00002605
Iteration 41/1000 | Loss: 0.00002160
Iteration 42/1000 | Loss: 0.00002574
Iteration 43/1000 | Loss: 0.00002079
Iteration 44/1000 | Loss: 0.00002750
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00002741
Iteration 48/1000 | Loss: 0.00002681
Iteration 49/1000 | Loss: 0.00024317
Iteration 50/1000 | Loss: 0.00005302
Iteration 51/1000 | Loss: 0.00002305
Iteration 52/1000 | Loss: 0.00002456
Iteration 53/1000 | Loss: 0.00002335
Iteration 54/1000 | Loss: 0.00021869
Iteration 55/1000 | Loss: 0.00006250
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00022622
Iteration 58/1000 | Loss: 0.00004987
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002592
Iteration 63/1000 | Loss: 0.00002153
Iteration 64/1000 | Loss: 0.00014715
Iteration 65/1000 | Loss: 0.00003291
Iteration 66/1000 | Loss: 0.00011335
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00015119
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002303
Iteration 73/1000 | Loss: 0.00002253
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00002695
Iteration 76/1000 | Loss: 0.00002424
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00003404
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00001833
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002458
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00002127
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00001999
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002214
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002236
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00002609
Iteration 99/1000 | Loss: 0.00002526
Iteration 100/1000 | Loss: 0.00002120
Iteration 101/1000 | Loss: 0.00002586
Iteration 102/1000 | Loss: 0.00002052
Iteration 103/1000 | Loss: 0.00002494
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00002094
Iteration 109/1000 | Loss: 0.00002260
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00002162
Iteration 112/1000 | Loss: 0.00002216
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00002556
Iteration 117/1000 | Loss: 0.00002082
Iteration 118/1000 | Loss: 0.00002957
Iteration 119/1000 | Loss: 0.00002216
Iteration 120/1000 | Loss: 0.00002589
Iteration 121/1000 | Loss: 0.00002060
Iteration 122/1000 | Loss: 0.00002573
Iteration 123/1000 | Loss: 0.00002816
Iteration 124/1000 | Loss: 0.00002409
Iteration 125/1000 | Loss: 0.00002435
Iteration 126/1000 | Loss: 0.00026276
Iteration 127/1000 | Loss: 0.00006824
Iteration 128/1000 | Loss: 0.00002388
Iteration 129/1000 | Loss: 0.00002832
Iteration 130/1000 | Loss: 0.00002419
Iteration 131/1000 | Loss: 0.00001691
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002312
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00003013
Iteration 137/1000 | Loss: 0.00043313
Iteration 138/1000 | Loss: 0.00004965
Iteration 139/1000 | Loss: 0.00004495
Iteration 140/1000 | Loss: 0.00002980
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00002130
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00002178
Iteration 146/1000 | Loss: 0.00002577
Iteration 147/1000 | Loss: 0.00004044
Iteration 148/1000 | Loss: 0.00002465
Iteration 149/1000 | Loss: 0.00001573
Iteration 150/1000 | Loss: 0.00002220
Iteration 151/1000 | Loss: 0.00002321
Iteration 152/1000 | Loss: 0.00002789
Iteration 153/1000 | Loss: 0.00002402
Iteration 154/1000 | Loss: 0.00002227
Iteration 155/1000 | Loss: 0.00002930
Iteration 156/1000 | Loss: 0.00005127
Iteration 157/1000 | Loss: 0.00002845
Iteration 158/1000 | Loss: 0.00002845
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00002252
Iteration 161/1000 | Loss: 0.00002407
Iteration 162/1000 | Loss: 0.00001900
Iteration 163/1000 | Loss: 0.00003173
Iteration 164/1000 | Loss: 0.00003668
Iteration 165/1000 | Loss: 0.00001980
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001242
Iteration 169/1000 | Loss: 0.00001179
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001110
Iteration 172/1000 | Loss: 0.00001098
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001095
Iteration 177/1000 | Loss: 0.00001095
Iteration 178/1000 | Loss: 0.00001093
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001090
Iteration 181/1000 | Loss: 0.00001090
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001088
Iteration 185/1000 | Loss: 0.00001088
Iteration 186/1000 | Loss: 0.00001088
Iteration 187/1000 | Loss: 0.00001088
Iteration 188/1000 | Loss: 0.00001088
Iteration 189/1000 | Loss: 0.00001087
Iteration 190/1000 | Loss: 0.00001087
Iteration 191/1000 | Loss: 0.00001087
Iteration 192/1000 | Loss: 0.00001086
Iteration 193/1000 | Loss: 0.00001086
Iteration 194/1000 | Loss: 0.00001086
Iteration 195/1000 | Loss: 0.00001086
Iteration 196/1000 | Loss: 0.00001086
Iteration 197/1000 | Loss: 0.00001086
Iteration 198/1000 | Loss: 0.00001085
Iteration 199/1000 | Loss: 0.00001085
Iteration 200/1000 | Loss: 0.00001085
Iteration 201/1000 | Loss: 0.00001085
Iteration 202/1000 | Loss: 0.00001085
Iteration 203/1000 | Loss: 0.00001085
Iteration 204/1000 | Loss: 0.00001085
Iteration 205/1000 | Loss: 0.00001084
Iteration 206/1000 | Loss: 0.00001084
Iteration 207/1000 | Loss: 0.00001084
Iteration 208/1000 | Loss: 0.00001084
Iteration 209/1000 | Loss: 0.00001084
Iteration 210/1000 | Loss: 0.00001084
Iteration 211/1000 | Loss: 0.00001083
Iteration 212/1000 | Loss: 0.00001083
Iteration 213/1000 | Loss: 0.00001083
Iteration 214/1000 | Loss: 0.00001083
Iteration 215/1000 | Loss: 0.00001083
Iteration 216/1000 | Loss: 0.00001082
Iteration 217/1000 | Loss: 0.00001082
Iteration 218/1000 | Loss: 0.00001082
Iteration 219/1000 | Loss: 0.00001082
Iteration 220/1000 | Loss: 0.00001081
Iteration 221/1000 | Loss: 0.00001081
Iteration 222/1000 | Loss: 0.00001081
Iteration 223/1000 | Loss: 0.00001081
Iteration 224/1000 | Loss: 0.00001081
Iteration 225/1000 | Loss: 0.00001081
Iteration 226/1000 | Loss: 0.00001081
Iteration 227/1000 | Loss: 0.00001080
Iteration 228/1000 | Loss: 0.00001080
Iteration 229/1000 | Loss: 0.00001080
Iteration 230/1000 | Loss: 0.00001080
Iteration 231/1000 | Loss: 0.00001080
Iteration 232/1000 | Loss: 0.00001080
Iteration 233/1000 | Loss: 0.00001080
Iteration 234/1000 | Loss: 0.00001079
Iteration 235/1000 | Loss: 0.00001079
Iteration 236/1000 | Loss: 0.00001079
Iteration 237/1000 | Loss: 0.00001079
Iteration 238/1000 | Loss: 0.00001079
Iteration 239/1000 | Loss: 0.00001079
Iteration 240/1000 | Loss: 0.00001079
Iteration 241/1000 | Loss: 0.00001079
Iteration 242/1000 | Loss: 0.00001079
Iteration 243/1000 | Loss: 0.00001079
Iteration 244/1000 | Loss: 0.00001079
Iteration 245/1000 | Loss: 0.00001079
Iteration 246/1000 | Loss: 0.00001078
Iteration 247/1000 | Loss: 0.00001078
Iteration 248/1000 | Loss: 0.00001078
Iteration 249/1000 | Loss: 0.00001078
Iteration 250/1000 | Loss: 0.00001078
Iteration 251/1000 | Loss: 0.00001078
Iteration 252/1000 | Loss: 0.00001078
Iteration 253/1000 | Loss: 0.00001077
Iteration 254/1000 | Loss: 0.00001077
Iteration 255/1000 | Loss: 0.00001077
Iteration 256/1000 | Loss: 0.00001077
Iteration 257/1000 | Loss: 0.00001077
Iteration 258/1000 | Loss: 0.00001077
Iteration 259/1000 | Loss: 0.00001077
Iteration 260/1000 | Loss: 0.00001077
Iteration 261/1000 | Loss: 0.00001077
Iteration 262/1000 | Loss: 0.00001077
Iteration 263/1000 | Loss: 0.00001077
Iteration 264/1000 | Loss: 0.00001077
Iteration 265/1000 | Loss: 0.00001077
Iteration 266/1000 | Loss: 0.00001077
Iteration 267/1000 | Loss: 0.00001077
Iteration 268/1000 | Loss: 0.00001076
Iteration 269/1000 | Loss: 0.00001076
Iteration 270/1000 | Loss: 0.00001076
Iteration 271/1000 | Loss: 0.00001076
Iteration 272/1000 | Loss: 0.00001076
Iteration 273/1000 | Loss: 0.00001076
Iteration 274/1000 | Loss: 0.00001076
Iteration 275/1000 | Loss: 0.00001076
Iteration 276/1000 | Loss: 0.00001076
Iteration 277/1000 | Loss: 0.00001076
Iteration 278/1000 | Loss: 0.00001076
Iteration 279/1000 | Loss: 0.00001076
Iteration 280/1000 | Loss: 0.00001076
Iteration 281/1000 | Loss: 0.00001076
Iteration 282/1000 | Loss: 0.00001076
Iteration 283/1000 | Loss: 0.00001076
Iteration 284/1000 | Loss: 0.00001076
Iteration 285/1000 | Loss: 0.00001076
Iteration 286/1000 | Loss: 0.00001076
Iteration 287/1000 | Loss: 0.00001076
Iteration 288/1000 | Loss: 0.00001076
Iteration 289/1000 | Loss: 0.00001076
Iteration 290/1000 | Loss: 0.00001076
Iteration 291/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.076347689377144e-05, 1.076347689377144e-05, 1.076347689377144e-05, 1.076347689377144e-05, 1.076347689377144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.076347689377144e-05

Optimization complete. Final v2v error: 2.748674154281616 mm

Highest mean error: 4.490078449249268 mm for frame 80

Lowest mean error: 2.3402719497680664 mm for frame 123

Saving results

Total time: 292.32629895210266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842137
Iteration 2/25 | Loss: 0.00221178
Iteration 3/25 | Loss: 0.00172369
Iteration 4/25 | Loss: 0.00159879
Iteration 5/25 | Loss: 0.00146647
Iteration 6/25 | Loss: 0.00126793
Iteration 7/25 | Loss: 0.00122679
Iteration 8/25 | Loss: 0.00120192
Iteration 9/25 | Loss: 0.00120155
Iteration 10/25 | Loss: 0.00119135
Iteration 11/25 | Loss: 0.00118482
Iteration 12/25 | Loss: 0.00118097
Iteration 13/25 | Loss: 0.00117945
Iteration 14/25 | Loss: 0.00118194
Iteration 15/25 | Loss: 0.00118099
Iteration 16/25 | Loss: 0.00118171
Iteration 17/25 | Loss: 0.00118174
Iteration 18/25 | Loss: 0.00118030
Iteration 19/25 | Loss: 0.00117751
Iteration 20/25 | Loss: 0.00117875
Iteration 21/25 | Loss: 0.00117776
Iteration 22/25 | Loss: 0.00117718
Iteration 23/25 | Loss: 0.00117658
Iteration 24/25 | Loss: 0.00117628
Iteration 25/25 | Loss: 0.00117718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37276065
Iteration 2/25 | Loss: 0.00048683
Iteration 3/25 | Loss: 0.00048681
Iteration 4/25 | Loss: 0.00048681
Iteration 5/25 | Loss: 0.00048681
Iteration 6/25 | Loss: 0.00048681
Iteration 7/25 | Loss: 0.00048681
Iteration 8/25 | Loss: 0.00048681
Iteration 9/25 | Loss: 0.00048681
Iteration 10/25 | Loss: 0.00048681
Iteration 11/25 | Loss: 0.00048681
Iteration 12/25 | Loss: 0.00048681
Iteration 13/25 | Loss: 0.00048681
Iteration 14/25 | Loss: 0.00048681
Iteration 15/25 | Loss: 0.00048681
Iteration 16/25 | Loss: 0.00048681
Iteration 17/25 | Loss: 0.00048681
Iteration 18/25 | Loss: 0.00048681
Iteration 19/25 | Loss: 0.00048681
Iteration 20/25 | Loss: 0.00048681
Iteration 21/25 | Loss: 0.00048681
Iteration 22/25 | Loss: 0.00048681
Iteration 23/25 | Loss: 0.00048681
Iteration 24/25 | Loss: 0.00048681
Iteration 25/25 | Loss: 0.00048681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048681
Iteration 2/1000 | Loss: 0.00005811
Iteration 3/1000 | Loss: 0.00005803
Iteration 4/1000 | Loss: 0.00003668
Iteration 5/1000 | Loss: 0.00005167
Iteration 6/1000 | Loss: 0.00005009
Iteration 7/1000 | Loss: 0.00004709
Iteration 8/1000 | Loss: 0.00003462
Iteration 9/1000 | Loss: 0.00002762
Iteration 10/1000 | Loss: 0.00003423
Iteration 11/1000 | Loss: 0.00003779
Iteration 12/1000 | Loss: 0.00004249
Iteration 13/1000 | Loss: 0.00004746
Iteration 14/1000 | Loss: 0.00005322
Iteration 15/1000 | Loss: 0.00004183
Iteration 16/1000 | Loss: 0.00003837
Iteration 17/1000 | Loss: 0.00005243
Iteration 18/1000 | Loss: 0.00005120
Iteration 19/1000 | Loss: 0.00004932
Iteration 20/1000 | Loss: 0.00004820
Iteration 21/1000 | Loss: 0.00004894
Iteration 22/1000 | Loss: 0.00004835
Iteration 23/1000 | Loss: 0.00005508
Iteration 24/1000 | Loss: 0.00005026
Iteration 25/1000 | Loss: 0.00005728
Iteration 26/1000 | Loss: 0.00005160
Iteration 27/1000 | Loss: 0.00004480
Iteration 28/1000 | Loss: 0.00005984
Iteration 29/1000 | Loss: 0.00004297
Iteration 30/1000 | Loss: 0.00004295
Iteration 31/1000 | Loss: 0.00005115
Iteration 32/1000 | Loss: 0.00005733
Iteration 33/1000 | Loss: 0.00005600
Iteration 34/1000 | Loss: 0.00005743
Iteration 35/1000 | Loss: 0.00005434
Iteration 36/1000 | Loss: 0.00006366
Iteration 37/1000 | Loss: 0.00005490
Iteration 38/1000 | Loss: 0.00003698
Iteration 39/1000 | Loss: 0.00002776
Iteration 40/1000 | Loss: 0.00002980
Iteration 41/1000 | Loss: 0.00004973
Iteration 42/1000 | Loss: 0.00003688
Iteration 43/1000 | Loss: 0.00002669
Iteration 44/1000 | Loss: 0.00002882
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002507
Iteration 48/1000 | Loss: 0.00002055
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00002623
Iteration 51/1000 | Loss: 0.00002988
Iteration 52/1000 | Loss: 0.00002928
Iteration 53/1000 | Loss: 0.00002984
Iteration 54/1000 | Loss: 0.00003593
Iteration 55/1000 | Loss: 0.00002976
Iteration 56/1000 | Loss: 0.00002642
Iteration 57/1000 | Loss: 0.00002949
Iteration 58/1000 | Loss: 0.00002992
Iteration 59/1000 | Loss: 0.00002944
Iteration 60/1000 | Loss: 0.00003783
Iteration 61/1000 | Loss: 0.00003520
Iteration 62/1000 | Loss: 0.00002871
Iteration 63/1000 | Loss: 0.00004031
Iteration 64/1000 | Loss: 0.00002686
Iteration 65/1000 | Loss: 0.00003651
Iteration 66/1000 | Loss: 0.00002098
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00002024
Iteration 72/1000 | Loss: 0.00002536
Iteration 73/1000 | Loss: 0.00002215
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00002566
Iteration 76/1000 | Loss: 0.00002156
Iteration 77/1000 | Loss: 0.00002253
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002189
Iteration 80/1000 | Loss: 0.00002311
Iteration 81/1000 | Loss: 0.00003572
Iteration 82/1000 | Loss: 0.00002306
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002546
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002707
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002697
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002432
Iteration 91/1000 | Loss: 0.00002212
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00003316
Iteration 94/1000 | Loss: 0.00002765
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002341
Iteration 98/1000 | Loss: 0.00003138
Iteration 99/1000 | Loss: 0.00002472
Iteration 100/1000 | Loss: 0.00002659
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002408
Iteration 103/1000 | Loss: 0.00002407
Iteration 104/1000 | Loss: 0.00003180
Iteration 105/1000 | Loss: 0.00002533
Iteration 106/1000 | Loss: 0.00003422
Iteration 107/1000 | Loss: 0.00003197
Iteration 108/1000 | Loss: 0.00002851
Iteration 109/1000 | Loss: 0.00003042
Iteration 110/1000 | Loss: 0.00002758
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00003272
Iteration 113/1000 | Loss: 0.00002822
Iteration 114/1000 | Loss: 0.00003008
Iteration 115/1000 | Loss: 0.00003223
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001763
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001714
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001712
Iteration 130/1000 | Loss: 0.00001710
Iteration 131/1000 | Loss: 0.00001710
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001703
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001702
Iteration 156/1000 | Loss: 0.00001702
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001698
Iteration 164/1000 | Loss: 0.00001698
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001697
Iteration 170/1000 | Loss: 0.00001697
Iteration 171/1000 | Loss: 0.00001697
Iteration 172/1000 | Loss: 0.00001697
Iteration 173/1000 | Loss: 0.00001697
Iteration 174/1000 | Loss: 0.00001697
Iteration 175/1000 | Loss: 0.00001696
Iteration 176/1000 | Loss: 0.00001696
Iteration 177/1000 | Loss: 0.00001696
Iteration 178/1000 | Loss: 0.00001696
Iteration 179/1000 | Loss: 0.00001696
Iteration 180/1000 | Loss: 0.00001696
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001696
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6956308172666468e-05, 1.6956308172666468e-05, 1.6956308172666468e-05, 1.6956308172666468e-05, 1.6956308172666468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6956308172666468e-05

Optimization complete. Final v2v error: 3.5075793266296387 mm

Highest mean error: 4.828298091888428 mm for frame 198

Lowest mean error: 3.267486333847046 mm for frame 43

Saving results

Total time: 246.44359040260315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394623
Iteration 2/25 | Loss: 0.00104851
Iteration 3/25 | Loss: 0.00097210
Iteration 4/25 | Loss: 0.00096150
Iteration 5/25 | Loss: 0.00095782
Iteration 6/25 | Loss: 0.00095711
Iteration 7/25 | Loss: 0.00095711
Iteration 8/25 | Loss: 0.00095711
Iteration 9/25 | Loss: 0.00095711
Iteration 10/25 | Loss: 0.00095711
Iteration 11/25 | Loss: 0.00095711
Iteration 12/25 | Loss: 0.00095711
Iteration 13/25 | Loss: 0.00095711
Iteration 14/25 | Loss: 0.00095711
Iteration 15/25 | Loss: 0.00095711
Iteration 16/25 | Loss: 0.00095711
Iteration 17/25 | Loss: 0.00095711
Iteration 18/25 | Loss: 0.00095711
Iteration 19/25 | Loss: 0.00095711
Iteration 20/25 | Loss: 0.00095711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009571067639626563, 0.0009571067639626563, 0.0009571067639626563, 0.0009571067639626563, 0.0009571067639626563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009571067639626563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38841057
Iteration 2/25 | Loss: 0.00071786
Iteration 3/25 | Loss: 0.00071785
Iteration 4/25 | Loss: 0.00071785
Iteration 5/25 | Loss: 0.00071785
Iteration 6/25 | Loss: 0.00071785
Iteration 7/25 | Loss: 0.00071785
Iteration 8/25 | Loss: 0.00071785
Iteration 9/25 | Loss: 0.00071785
Iteration 10/25 | Loss: 0.00071785
Iteration 11/25 | Loss: 0.00071785
Iteration 12/25 | Loss: 0.00071785
Iteration 13/25 | Loss: 0.00071785
Iteration 14/25 | Loss: 0.00071785
Iteration 15/25 | Loss: 0.00071785
Iteration 16/25 | Loss: 0.00071785
Iteration 17/25 | Loss: 0.00071785
Iteration 18/25 | Loss: 0.00071785
Iteration 19/25 | Loss: 0.00071785
Iteration 20/25 | Loss: 0.00071785
Iteration 21/25 | Loss: 0.00071785
Iteration 22/25 | Loss: 0.00071785
Iteration 23/25 | Loss: 0.00071785
Iteration 24/25 | Loss: 0.00071785
Iteration 25/25 | Loss: 0.00071785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071785
Iteration 2/1000 | Loss: 0.00001895
Iteration 3/1000 | Loss: 0.00001173
Iteration 4/1000 | Loss: 0.00001041
Iteration 5/1000 | Loss: 0.00000988
Iteration 6/1000 | Loss: 0.00000954
Iteration 7/1000 | Loss: 0.00000921
Iteration 8/1000 | Loss: 0.00000906
Iteration 9/1000 | Loss: 0.00000899
Iteration 10/1000 | Loss: 0.00000899
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000889
Iteration 13/1000 | Loss: 0.00000887
Iteration 14/1000 | Loss: 0.00000884
Iteration 15/1000 | Loss: 0.00000883
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000882
Iteration 18/1000 | Loss: 0.00000881
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000877
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000876
Iteration 23/1000 | Loss: 0.00000875
Iteration 24/1000 | Loss: 0.00000875
Iteration 25/1000 | Loss: 0.00000875
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000873
Iteration 29/1000 | Loss: 0.00000872
Iteration 30/1000 | Loss: 0.00000872
Iteration 31/1000 | Loss: 0.00000872
Iteration 32/1000 | Loss: 0.00000872
Iteration 33/1000 | Loss: 0.00000872
Iteration 34/1000 | Loss: 0.00000872
Iteration 35/1000 | Loss: 0.00000872
Iteration 36/1000 | Loss: 0.00000872
Iteration 37/1000 | Loss: 0.00000872
Iteration 38/1000 | Loss: 0.00000872
Iteration 39/1000 | Loss: 0.00000871
Iteration 40/1000 | Loss: 0.00000871
Iteration 41/1000 | Loss: 0.00000870
Iteration 42/1000 | Loss: 0.00000870
Iteration 43/1000 | Loss: 0.00000870
Iteration 44/1000 | Loss: 0.00000869
Iteration 45/1000 | Loss: 0.00000869
Iteration 46/1000 | Loss: 0.00000869
Iteration 47/1000 | Loss: 0.00000868
Iteration 48/1000 | Loss: 0.00000867
Iteration 49/1000 | Loss: 0.00000867
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000866
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000866
Iteration 54/1000 | Loss: 0.00000866
Iteration 55/1000 | Loss: 0.00000866
Iteration 56/1000 | Loss: 0.00000865
Iteration 57/1000 | Loss: 0.00000865
Iteration 58/1000 | Loss: 0.00000864
Iteration 59/1000 | Loss: 0.00000864
Iteration 60/1000 | Loss: 0.00000864
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000863
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000862
Iteration 65/1000 | Loss: 0.00000862
Iteration 66/1000 | Loss: 0.00000861
Iteration 67/1000 | Loss: 0.00000861
Iteration 68/1000 | Loss: 0.00000861
Iteration 69/1000 | Loss: 0.00000861
Iteration 70/1000 | Loss: 0.00000861
Iteration 71/1000 | Loss: 0.00000860
Iteration 72/1000 | Loss: 0.00000860
Iteration 73/1000 | Loss: 0.00000860
Iteration 74/1000 | Loss: 0.00000860
Iteration 75/1000 | Loss: 0.00000860
Iteration 76/1000 | Loss: 0.00000860
Iteration 77/1000 | Loss: 0.00000860
Iteration 78/1000 | Loss: 0.00000859
Iteration 79/1000 | Loss: 0.00000859
Iteration 80/1000 | Loss: 0.00000859
Iteration 81/1000 | Loss: 0.00000859
Iteration 82/1000 | Loss: 0.00000858
Iteration 83/1000 | Loss: 0.00000858
Iteration 84/1000 | Loss: 0.00000858
Iteration 85/1000 | Loss: 0.00000857
Iteration 86/1000 | Loss: 0.00000857
Iteration 87/1000 | Loss: 0.00000857
Iteration 88/1000 | Loss: 0.00000856
Iteration 89/1000 | Loss: 0.00000856
Iteration 90/1000 | Loss: 0.00000856
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000855
Iteration 95/1000 | Loss: 0.00000855
Iteration 96/1000 | Loss: 0.00000855
Iteration 97/1000 | Loss: 0.00000855
Iteration 98/1000 | Loss: 0.00000855
Iteration 99/1000 | Loss: 0.00000855
Iteration 100/1000 | Loss: 0.00000855
Iteration 101/1000 | Loss: 0.00000855
Iteration 102/1000 | Loss: 0.00000855
Iteration 103/1000 | Loss: 0.00000855
Iteration 104/1000 | Loss: 0.00000855
Iteration 105/1000 | Loss: 0.00000855
Iteration 106/1000 | Loss: 0.00000855
Iteration 107/1000 | Loss: 0.00000855
Iteration 108/1000 | Loss: 0.00000855
Iteration 109/1000 | Loss: 0.00000855
Iteration 110/1000 | Loss: 0.00000855
Iteration 111/1000 | Loss: 0.00000855
Iteration 112/1000 | Loss: 0.00000855
Iteration 113/1000 | Loss: 0.00000855
Iteration 114/1000 | Loss: 0.00000855
Iteration 115/1000 | Loss: 0.00000855
Iteration 116/1000 | Loss: 0.00000855
Iteration 117/1000 | Loss: 0.00000855
Iteration 118/1000 | Loss: 0.00000855
Iteration 119/1000 | Loss: 0.00000855
Iteration 120/1000 | Loss: 0.00000855
Iteration 121/1000 | Loss: 0.00000855
Iteration 122/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [8.545252967451233e-06, 8.545252967451233e-06, 8.545252967451233e-06, 8.545252967451233e-06, 8.545252967451233e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.545252967451233e-06

Optimization complete. Final v2v error: 2.5171682834625244 mm

Highest mean error: 2.757615089416504 mm for frame 104

Lowest mean error: 2.347438335418701 mm for frame 88

Saving results

Total time: 29.490347623825073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418244
Iteration 2/25 | Loss: 0.00112093
Iteration 3/25 | Loss: 0.00099613
Iteration 4/25 | Loss: 0.00098022
Iteration 5/25 | Loss: 0.00097635
Iteration 6/25 | Loss: 0.00097517
Iteration 7/25 | Loss: 0.00097517
Iteration 8/25 | Loss: 0.00097517
Iteration 9/25 | Loss: 0.00097517
Iteration 10/25 | Loss: 0.00097517
Iteration 11/25 | Loss: 0.00097517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009751654579304159, 0.0009751654579304159, 0.0009751654579304159, 0.0009751654579304159, 0.0009751654579304159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009751654579304159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36181319
Iteration 2/25 | Loss: 0.00074389
Iteration 3/25 | Loss: 0.00074389
Iteration 4/25 | Loss: 0.00074389
Iteration 5/25 | Loss: 0.00074389
Iteration 6/25 | Loss: 0.00074389
Iteration 7/25 | Loss: 0.00074389
Iteration 8/25 | Loss: 0.00074389
Iteration 9/25 | Loss: 0.00074389
Iteration 10/25 | Loss: 0.00074389
Iteration 11/25 | Loss: 0.00074389
Iteration 12/25 | Loss: 0.00074389
Iteration 13/25 | Loss: 0.00074389
Iteration 14/25 | Loss: 0.00074389
Iteration 15/25 | Loss: 0.00074389
Iteration 16/25 | Loss: 0.00074389
Iteration 17/25 | Loss: 0.00074389
Iteration 18/25 | Loss: 0.00074389
Iteration 19/25 | Loss: 0.00074389
Iteration 20/25 | Loss: 0.00074389
Iteration 21/25 | Loss: 0.00074389
Iteration 22/25 | Loss: 0.00074389
Iteration 23/25 | Loss: 0.00074389
Iteration 24/25 | Loss: 0.00074389
Iteration 25/25 | Loss: 0.00074389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074389
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001221
Iteration 5/1000 | Loss: 0.00001139
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00000997
Iteration 10/1000 | Loss: 0.00000976
Iteration 11/1000 | Loss: 0.00000964
Iteration 12/1000 | Loss: 0.00000961
Iteration 13/1000 | Loss: 0.00000960
Iteration 14/1000 | Loss: 0.00000956
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000952
Iteration 17/1000 | Loss: 0.00000951
Iteration 18/1000 | Loss: 0.00000948
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000944
Iteration 22/1000 | Loss: 0.00000944
Iteration 23/1000 | Loss: 0.00000943
Iteration 24/1000 | Loss: 0.00000943
Iteration 25/1000 | Loss: 0.00000943
Iteration 26/1000 | Loss: 0.00000942
Iteration 27/1000 | Loss: 0.00000942
Iteration 28/1000 | Loss: 0.00000942
Iteration 29/1000 | Loss: 0.00000941
Iteration 30/1000 | Loss: 0.00000941
Iteration 31/1000 | Loss: 0.00000939
Iteration 32/1000 | Loss: 0.00000939
Iteration 33/1000 | Loss: 0.00000939
Iteration 34/1000 | Loss: 0.00000939
Iteration 35/1000 | Loss: 0.00000939
Iteration 36/1000 | Loss: 0.00000939
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000936
Iteration 50/1000 | Loss: 0.00000936
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000935
Iteration 53/1000 | Loss: 0.00000935
Iteration 54/1000 | Loss: 0.00000935
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000935
Iteration 57/1000 | Loss: 0.00000935
Iteration 58/1000 | Loss: 0.00000935
Iteration 59/1000 | Loss: 0.00000934
Iteration 60/1000 | Loss: 0.00000934
Iteration 61/1000 | Loss: 0.00000934
Iteration 62/1000 | Loss: 0.00000934
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000933
Iteration 65/1000 | Loss: 0.00000933
Iteration 66/1000 | Loss: 0.00000933
Iteration 67/1000 | Loss: 0.00000933
Iteration 68/1000 | Loss: 0.00000933
Iteration 69/1000 | Loss: 0.00000933
Iteration 70/1000 | Loss: 0.00000933
Iteration 71/1000 | Loss: 0.00000932
Iteration 72/1000 | Loss: 0.00000932
Iteration 73/1000 | Loss: 0.00000932
Iteration 74/1000 | Loss: 0.00000932
Iteration 75/1000 | Loss: 0.00000932
Iteration 76/1000 | Loss: 0.00000932
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000931
Iteration 81/1000 | Loss: 0.00000931
Iteration 82/1000 | Loss: 0.00000931
Iteration 83/1000 | Loss: 0.00000931
Iteration 84/1000 | Loss: 0.00000930
Iteration 85/1000 | Loss: 0.00000930
Iteration 86/1000 | Loss: 0.00000930
Iteration 87/1000 | Loss: 0.00000930
Iteration 88/1000 | Loss: 0.00000929
Iteration 89/1000 | Loss: 0.00000929
Iteration 90/1000 | Loss: 0.00000929
Iteration 91/1000 | Loss: 0.00000929
Iteration 92/1000 | Loss: 0.00000929
Iteration 93/1000 | Loss: 0.00000928
Iteration 94/1000 | Loss: 0.00000928
Iteration 95/1000 | Loss: 0.00000928
Iteration 96/1000 | Loss: 0.00000927
Iteration 97/1000 | Loss: 0.00000927
Iteration 98/1000 | Loss: 0.00000927
Iteration 99/1000 | Loss: 0.00000927
Iteration 100/1000 | Loss: 0.00000927
Iteration 101/1000 | Loss: 0.00000927
Iteration 102/1000 | Loss: 0.00000927
Iteration 103/1000 | Loss: 0.00000927
Iteration 104/1000 | Loss: 0.00000927
Iteration 105/1000 | Loss: 0.00000927
Iteration 106/1000 | Loss: 0.00000927
Iteration 107/1000 | Loss: 0.00000927
Iteration 108/1000 | Loss: 0.00000927
Iteration 109/1000 | Loss: 0.00000926
Iteration 110/1000 | Loss: 0.00000926
Iteration 111/1000 | Loss: 0.00000926
Iteration 112/1000 | Loss: 0.00000925
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000924
Iteration 117/1000 | Loss: 0.00000924
Iteration 118/1000 | Loss: 0.00000924
Iteration 119/1000 | Loss: 0.00000924
Iteration 120/1000 | Loss: 0.00000924
Iteration 121/1000 | Loss: 0.00000924
Iteration 122/1000 | Loss: 0.00000924
Iteration 123/1000 | Loss: 0.00000924
Iteration 124/1000 | Loss: 0.00000924
Iteration 125/1000 | Loss: 0.00000923
Iteration 126/1000 | Loss: 0.00000923
Iteration 127/1000 | Loss: 0.00000923
Iteration 128/1000 | Loss: 0.00000923
Iteration 129/1000 | Loss: 0.00000923
Iteration 130/1000 | Loss: 0.00000923
Iteration 131/1000 | Loss: 0.00000923
Iteration 132/1000 | Loss: 0.00000923
Iteration 133/1000 | Loss: 0.00000923
Iteration 134/1000 | Loss: 0.00000923
Iteration 135/1000 | Loss: 0.00000923
Iteration 136/1000 | Loss: 0.00000923
Iteration 137/1000 | Loss: 0.00000923
Iteration 138/1000 | Loss: 0.00000922
Iteration 139/1000 | Loss: 0.00000922
Iteration 140/1000 | Loss: 0.00000922
Iteration 141/1000 | Loss: 0.00000922
Iteration 142/1000 | Loss: 0.00000922
Iteration 143/1000 | Loss: 0.00000922
Iteration 144/1000 | Loss: 0.00000922
Iteration 145/1000 | Loss: 0.00000922
Iteration 146/1000 | Loss: 0.00000922
Iteration 147/1000 | Loss: 0.00000921
Iteration 148/1000 | Loss: 0.00000921
Iteration 149/1000 | Loss: 0.00000921
Iteration 150/1000 | Loss: 0.00000921
Iteration 151/1000 | Loss: 0.00000921
Iteration 152/1000 | Loss: 0.00000921
Iteration 153/1000 | Loss: 0.00000921
Iteration 154/1000 | Loss: 0.00000921
Iteration 155/1000 | Loss: 0.00000920
Iteration 156/1000 | Loss: 0.00000920
Iteration 157/1000 | Loss: 0.00000920
Iteration 158/1000 | Loss: 0.00000920
Iteration 159/1000 | Loss: 0.00000920
Iteration 160/1000 | Loss: 0.00000920
Iteration 161/1000 | Loss: 0.00000920
Iteration 162/1000 | Loss: 0.00000920
Iteration 163/1000 | Loss: 0.00000920
Iteration 164/1000 | Loss: 0.00000920
Iteration 165/1000 | Loss: 0.00000920
Iteration 166/1000 | Loss: 0.00000920
Iteration 167/1000 | Loss: 0.00000920
Iteration 168/1000 | Loss: 0.00000920
Iteration 169/1000 | Loss: 0.00000920
Iteration 170/1000 | Loss: 0.00000920
Iteration 171/1000 | Loss: 0.00000920
Iteration 172/1000 | Loss: 0.00000920
Iteration 173/1000 | Loss: 0.00000920
Iteration 174/1000 | Loss: 0.00000920
Iteration 175/1000 | Loss: 0.00000920
Iteration 176/1000 | Loss: 0.00000920
Iteration 177/1000 | Loss: 0.00000920
Iteration 178/1000 | Loss: 0.00000920
Iteration 179/1000 | Loss: 0.00000920
Iteration 180/1000 | Loss: 0.00000920
Iteration 181/1000 | Loss: 0.00000920
Iteration 182/1000 | Loss: 0.00000920
Iteration 183/1000 | Loss: 0.00000920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.198211955663282e-06, 9.198211955663282e-06, 9.198211955663282e-06, 9.198211955663282e-06, 9.198211955663282e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.198211955663282e-06

Optimization complete. Final v2v error: 2.583875894546509 mm

Highest mean error: 3.631880283355713 mm for frame 56

Lowest mean error: 2.261570930480957 mm for frame 174

Saving results

Total time: 42.30235767364502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409833
Iteration 2/25 | Loss: 0.00102368
Iteration 3/25 | Loss: 0.00096892
Iteration 4/25 | Loss: 0.00096358
Iteration 5/25 | Loss: 0.00096206
Iteration 6/25 | Loss: 0.00096206
Iteration 7/25 | Loss: 0.00096206
Iteration 8/25 | Loss: 0.00096206
Iteration 9/25 | Loss: 0.00096206
Iteration 10/25 | Loss: 0.00096206
Iteration 11/25 | Loss: 0.00096206
Iteration 12/25 | Loss: 0.00096206
Iteration 13/25 | Loss: 0.00096206
Iteration 14/25 | Loss: 0.00096206
Iteration 15/25 | Loss: 0.00096206
Iteration 16/25 | Loss: 0.00096206
Iteration 17/25 | Loss: 0.00096206
Iteration 18/25 | Loss: 0.00096206
Iteration 19/25 | Loss: 0.00096206
Iteration 20/25 | Loss: 0.00096206
Iteration 21/25 | Loss: 0.00096206
Iteration 22/25 | Loss: 0.00096206
Iteration 23/25 | Loss: 0.00096206
Iteration 24/25 | Loss: 0.00096206
Iteration 25/25 | Loss: 0.00096206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47170568
Iteration 2/25 | Loss: 0.00069488
Iteration 3/25 | Loss: 0.00069486
Iteration 4/25 | Loss: 0.00069486
Iteration 5/25 | Loss: 0.00069486
Iteration 6/25 | Loss: 0.00069486
Iteration 7/25 | Loss: 0.00069486
Iteration 8/25 | Loss: 0.00069486
Iteration 9/25 | Loss: 0.00069486
Iteration 10/25 | Loss: 0.00069486
Iteration 11/25 | Loss: 0.00069486
Iteration 12/25 | Loss: 0.00069486
Iteration 13/25 | Loss: 0.00069486
Iteration 14/25 | Loss: 0.00069486
Iteration 15/25 | Loss: 0.00069486
Iteration 16/25 | Loss: 0.00069486
Iteration 17/25 | Loss: 0.00069486
Iteration 18/25 | Loss: 0.00069486
Iteration 19/25 | Loss: 0.00069486
Iteration 20/25 | Loss: 0.00069486
Iteration 21/25 | Loss: 0.00069486
Iteration 22/25 | Loss: 0.00069486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006948577938601375, 0.0006948577938601375, 0.0006948577938601375, 0.0006948577938601375, 0.0006948577938601375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006948577938601375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069486
Iteration 2/1000 | Loss: 0.00001646
Iteration 3/1000 | Loss: 0.00001229
Iteration 4/1000 | Loss: 0.00001133
Iteration 5/1000 | Loss: 0.00001068
Iteration 6/1000 | Loss: 0.00001028
Iteration 7/1000 | Loss: 0.00001002
Iteration 8/1000 | Loss: 0.00000996
Iteration 9/1000 | Loss: 0.00000972
Iteration 10/1000 | Loss: 0.00000957
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000953
Iteration 13/1000 | Loss: 0.00000952
Iteration 14/1000 | Loss: 0.00000951
Iteration 15/1000 | Loss: 0.00000950
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000944
Iteration 18/1000 | Loss: 0.00000943
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000941
Iteration 22/1000 | Loss: 0.00000938
Iteration 23/1000 | Loss: 0.00000937
Iteration 24/1000 | Loss: 0.00000936
Iteration 25/1000 | Loss: 0.00000936
Iteration 26/1000 | Loss: 0.00000935
Iteration 27/1000 | Loss: 0.00000934
Iteration 28/1000 | Loss: 0.00000932
Iteration 29/1000 | Loss: 0.00000932
Iteration 30/1000 | Loss: 0.00000932
Iteration 31/1000 | Loss: 0.00000931
Iteration 32/1000 | Loss: 0.00000931
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000931
Iteration 39/1000 | Loss: 0.00000931
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000930
Iteration 43/1000 | Loss: 0.00000930
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000930
Iteration 47/1000 | Loss: 0.00000930
Iteration 48/1000 | Loss: 0.00000928
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000926
Iteration 55/1000 | Loss: 0.00000926
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000923
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000922
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000921
Iteration 71/1000 | Loss: 0.00000920
Iteration 72/1000 | Loss: 0.00000920
Iteration 73/1000 | Loss: 0.00000919
Iteration 74/1000 | Loss: 0.00000919
Iteration 75/1000 | Loss: 0.00000919
Iteration 76/1000 | Loss: 0.00000918
Iteration 77/1000 | Loss: 0.00000918
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000916
Iteration 81/1000 | Loss: 0.00000916
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000913
Iteration 90/1000 | Loss: 0.00000912
Iteration 91/1000 | Loss: 0.00000912
Iteration 92/1000 | Loss: 0.00000912
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000911
Iteration 95/1000 | Loss: 0.00000911
Iteration 96/1000 | Loss: 0.00000911
Iteration 97/1000 | Loss: 0.00000911
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000910
Iteration 100/1000 | Loss: 0.00000910
Iteration 101/1000 | Loss: 0.00000910
Iteration 102/1000 | Loss: 0.00000910
Iteration 103/1000 | Loss: 0.00000909
Iteration 104/1000 | Loss: 0.00000909
Iteration 105/1000 | Loss: 0.00000909
Iteration 106/1000 | Loss: 0.00000909
Iteration 107/1000 | Loss: 0.00000909
Iteration 108/1000 | Loss: 0.00000908
Iteration 109/1000 | Loss: 0.00000908
Iteration 110/1000 | Loss: 0.00000908
Iteration 111/1000 | Loss: 0.00000908
Iteration 112/1000 | Loss: 0.00000908
Iteration 113/1000 | Loss: 0.00000908
Iteration 114/1000 | Loss: 0.00000908
Iteration 115/1000 | Loss: 0.00000908
Iteration 116/1000 | Loss: 0.00000908
Iteration 117/1000 | Loss: 0.00000908
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000907
Iteration 122/1000 | Loss: 0.00000907
Iteration 123/1000 | Loss: 0.00000907
Iteration 124/1000 | Loss: 0.00000907
Iteration 125/1000 | Loss: 0.00000906
Iteration 126/1000 | Loss: 0.00000906
Iteration 127/1000 | Loss: 0.00000906
Iteration 128/1000 | Loss: 0.00000906
Iteration 129/1000 | Loss: 0.00000906
Iteration 130/1000 | Loss: 0.00000906
Iteration 131/1000 | Loss: 0.00000906
Iteration 132/1000 | Loss: 0.00000905
Iteration 133/1000 | Loss: 0.00000905
Iteration 134/1000 | Loss: 0.00000905
Iteration 135/1000 | Loss: 0.00000905
Iteration 136/1000 | Loss: 0.00000905
Iteration 137/1000 | Loss: 0.00000905
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000904
Iteration 140/1000 | Loss: 0.00000904
Iteration 141/1000 | Loss: 0.00000904
Iteration 142/1000 | Loss: 0.00000903
Iteration 143/1000 | Loss: 0.00000903
Iteration 144/1000 | Loss: 0.00000903
Iteration 145/1000 | Loss: 0.00000903
Iteration 146/1000 | Loss: 0.00000903
Iteration 147/1000 | Loss: 0.00000903
Iteration 148/1000 | Loss: 0.00000903
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000902
Iteration 151/1000 | Loss: 0.00000902
Iteration 152/1000 | Loss: 0.00000902
Iteration 153/1000 | Loss: 0.00000902
Iteration 154/1000 | Loss: 0.00000901
Iteration 155/1000 | Loss: 0.00000901
Iteration 156/1000 | Loss: 0.00000900
Iteration 157/1000 | Loss: 0.00000900
Iteration 158/1000 | Loss: 0.00000900
Iteration 159/1000 | Loss: 0.00000900
Iteration 160/1000 | Loss: 0.00000900
Iteration 161/1000 | Loss: 0.00000899
Iteration 162/1000 | Loss: 0.00000899
Iteration 163/1000 | Loss: 0.00000899
Iteration 164/1000 | Loss: 0.00000899
Iteration 165/1000 | Loss: 0.00000899
Iteration 166/1000 | Loss: 0.00000899
Iteration 167/1000 | Loss: 0.00000899
Iteration 168/1000 | Loss: 0.00000898
Iteration 169/1000 | Loss: 0.00000898
Iteration 170/1000 | Loss: 0.00000898
Iteration 171/1000 | Loss: 0.00000898
Iteration 172/1000 | Loss: 0.00000898
Iteration 173/1000 | Loss: 0.00000898
Iteration 174/1000 | Loss: 0.00000898
Iteration 175/1000 | Loss: 0.00000898
Iteration 176/1000 | Loss: 0.00000898
Iteration 177/1000 | Loss: 0.00000898
Iteration 178/1000 | Loss: 0.00000897
Iteration 179/1000 | Loss: 0.00000897
Iteration 180/1000 | Loss: 0.00000897
Iteration 181/1000 | Loss: 0.00000897
Iteration 182/1000 | Loss: 0.00000897
Iteration 183/1000 | Loss: 0.00000897
Iteration 184/1000 | Loss: 0.00000897
Iteration 185/1000 | Loss: 0.00000897
Iteration 186/1000 | Loss: 0.00000897
Iteration 187/1000 | Loss: 0.00000897
Iteration 188/1000 | Loss: 0.00000897
Iteration 189/1000 | Loss: 0.00000896
Iteration 190/1000 | Loss: 0.00000896
Iteration 191/1000 | Loss: 0.00000896
Iteration 192/1000 | Loss: 0.00000896
Iteration 193/1000 | Loss: 0.00000896
Iteration 194/1000 | Loss: 0.00000896
Iteration 195/1000 | Loss: 0.00000895
Iteration 196/1000 | Loss: 0.00000895
Iteration 197/1000 | Loss: 0.00000895
Iteration 198/1000 | Loss: 0.00000895
Iteration 199/1000 | Loss: 0.00000895
Iteration 200/1000 | Loss: 0.00000895
Iteration 201/1000 | Loss: 0.00000895
Iteration 202/1000 | Loss: 0.00000894
Iteration 203/1000 | Loss: 0.00000894
Iteration 204/1000 | Loss: 0.00000894
Iteration 205/1000 | Loss: 0.00000894
Iteration 206/1000 | Loss: 0.00000894
Iteration 207/1000 | Loss: 0.00000893
Iteration 208/1000 | Loss: 0.00000893
Iteration 209/1000 | Loss: 0.00000893
Iteration 210/1000 | Loss: 0.00000893
Iteration 211/1000 | Loss: 0.00000893
Iteration 212/1000 | Loss: 0.00000893
Iteration 213/1000 | Loss: 0.00000892
Iteration 214/1000 | Loss: 0.00000892
Iteration 215/1000 | Loss: 0.00000892
Iteration 216/1000 | Loss: 0.00000892
Iteration 217/1000 | Loss: 0.00000892
Iteration 218/1000 | Loss: 0.00000892
Iteration 219/1000 | Loss: 0.00000892
Iteration 220/1000 | Loss: 0.00000892
Iteration 221/1000 | Loss: 0.00000892
Iteration 222/1000 | Loss: 0.00000892
Iteration 223/1000 | Loss: 0.00000892
Iteration 224/1000 | Loss: 0.00000892
Iteration 225/1000 | Loss: 0.00000892
Iteration 226/1000 | Loss: 0.00000892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [8.915658327168785e-06, 8.915658327168785e-06, 8.915658327168785e-06, 8.915658327168785e-06, 8.915658327168785e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.915658327168785e-06

Optimization complete. Final v2v error: 2.5282740592956543 mm

Highest mean error: 2.861717700958252 mm for frame 156

Lowest mean error: 2.3593108654022217 mm for frame 133

Saving results

Total time: 44.351728200912476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911720
Iteration 2/25 | Loss: 0.00150616
Iteration 3/25 | Loss: 0.00113411
Iteration 4/25 | Loss: 0.00110331
Iteration 5/25 | Loss: 0.00109514
Iteration 6/25 | Loss: 0.00109262
Iteration 7/25 | Loss: 0.00109262
Iteration 8/25 | Loss: 0.00109262
Iteration 9/25 | Loss: 0.00109262
Iteration 10/25 | Loss: 0.00109262
Iteration 11/25 | Loss: 0.00109262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001092621241696179, 0.001092621241696179, 0.001092621241696179, 0.001092621241696179, 0.001092621241696179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001092621241696179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09168029
Iteration 2/25 | Loss: 0.00069019
Iteration 3/25 | Loss: 0.00069018
Iteration 4/25 | Loss: 0.00069018
Iteration 5/25 | Loss: 0.00069018
Iteration 6/25 | Loss: 0.00069018
Iteration 7/25 | Loss: 0.00069018
Iteration 8/25 | Loss: 0.00069018
Iteration 9/25 | Loss: 0.00069018
Iteration 10/25 | Loss: 0.00069018
Iteration 11/25 | Loss: 0.00069018
Iteration 12/25 | Loss: 0.00069017
Iteration 13/25 | Loss: 0.00069017
Iteration 14/25 | Loss: 0.00069017
Iteration 15/25 | Loss: 0.00069017
Iteration 16/25 | Loss: 0.00069017
Iteration 17/25 | Loss: 0.00069017
Iteration 18/25 | Loss: 0.00069017
Iteration 19/25 | Loss: 0.00069017
Iteration 20/25 | Loss: 0.00069017
Iteration 21/25 | Loss: 0.00069017
Iteration 22/25 | Loss: 0.00069017
Iteration 23/25 | Loss: 0.00069017
Iteration 24/25 | Loss: 0.00069017
Iteration 25/25 | Loss: 0.00069017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069017
Iteration 2/1000 | Loss: 0.00005357
Iteration 3/1000 | Loss: 0.00003618
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002610
Iteration 6/1000 | Loss: 0.00002501
Iteration 7/1000 | Loss: 0.00002434
Iteration 8/1000 | Loss: 0.00002357
Iteration 9/1000 | Loss: 0.00002316
Iteration 10/1000 | Loss: 0.00002289
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002171
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002141
Iteration 19/1000 | Loss: 0.00002134
Iteration 20/1000 | Loss: 0.00002132
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002126
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002100
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002099
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002098
Iteration 35/1000 | Loss: 0.00002097
Iteration 36/1000 | Loss: 0.00002097
Iteration 37/1000 | Loss: 0.00002096
Iteration 38/1000 | Loss: 0.00002096
Iteration 39/1000 | Loss: 0.00002096
Iteration 40/1000 | Loss: 0.00002096
Iteration 41/1000 | Loss: 0.00002096
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002095
Iteration 45/1000 | Loss: 0.00002095
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002094
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002092
Iteration 61/1000 | Loss: 0.00002092
Iteration 62/1000 | Loss: 0.00002092
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002091
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002091
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002091
Iteration 73/1000 | Loss: 0.00002091
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002090
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002090
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002089
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002088
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002088
Iteration 97/1000 | Loss: 0.00002088
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002088
Iteration 101/1000 | Loss: 0.00002088
Iteration 102/1000 | Loss: 0.00002088
Iteration 103/1000 | Loss: 0.00002088
Iteration 104/1000 | Loss: 0.00002088
Iteration 105/1000 | Loss: 0.00002088
Iteration 106/1000 | Loss: 0.00002088
Iteration 107/1000 | Loss: 0.00002088
Iteration 108/1000 | Loss: 0.00002088
Iteration 109/1000 | Loss: 0.00002088
Iteration 110/1000 | Loss: 0.00002088
Iteration 111/1000 | Loss: 0.00002088
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002088
Iteration 115/1000 | Loss: 0.00002088
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002088
Iteration 123/1000 | Loss: 0.00002088
Iteration 124/1000 | Loss: 0.00002088
Iteration 125/1000 | Loss: 0.00002088
Iteration 126/1000 | Loss: 0.00002088
Iteration 127/1000 | Loss: 0.00002088
Iteration 128/1000 | Loss: 0.00002088
Iteration 129/1000 | Loss: 0.00002088
Iteration 130/1000 | Loss: 0.00002088
Iteration 131/1000 | Loss: 0.00002088
Iteration 132/1000 | Loss: 0.00002088
Iteration 133/1000 | Loss: 0.00002088
Iteration 134/1000 | Loss: 0.00002088
Iteration 135/1000 | Loss: 0.00002088
Iteration 136/1000 | Loss: 0.00002088
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002088
Iteration 141/1000 | Loss: 0.00002088
Iteration 142/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.0882593162241392e-05, 2.0882593162241392e-05, 2.0882593162241392e-05, 2.0882593162241392e-05, 2.0882593162241392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0882593162241392e-05

Optimization complete. Final v2v error: 3.7210683822631836 mm

Highest mean error: 5.102896690368652 mm for frame 102

Lowest mean error: 2.904675006866455 mm for frame 122

Saving results

Total time: 45.46292757987976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894765
Iteration 2/25 | Loss: 0.00181959
Iteration 3/25 | Loss: 0.00134193
Iteration 4/25 | Loss: 0.00124630
Iteration 5/25 | Loss: 0.00118889
Iteration 6/25 | Loss: 0.00112999
Iteration 7/25 | Loss: 0.00111265
Iteration 8/25 | Loss: 0.00111025
Iteration 9/25 | Loss: 0.00110965
Iteration 10/25 | Loss: 0.00110952
Iteration 11/25 | Loss: 0.00110943
Iteration 12/25 | Loss: 0.00110941
Iteration 13/25 | Loss: 0.00110941
Iteration 14/25 | Loss: 0.00110941
Iteration 15/25 | Loss: 0.00110941
Iteration 16/25 | Loss: 0.00110941
Iteration 17/25 | Loss: 0.00110941
Iteration 18/25 | Loss: 0.00110941
Iteration 19/25 | Loss: 0.00110940
Iteration 20/25 | Loss: 0.00110940
Iteration 21/25 | Loss: 0.00110940
Iteration 22/25 | Loss: 0.00110940
Iteration 23/25 | Loss: 0.00110940
Iteration 24/25 | Loss: 0.00110940
Iteration 25/25 | Loss: 0.00110940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60268760
Iteration 2/25 | Loss: 0.00053720
Iteration 3/25 | Loss: 0.00053720
Iteration 4/25 | Loss: 0.00053720
Iteration 5/25 | Loss: 0.00053720
Iteration 6/25 | Loss: 0.00053720
Iteration 7/25 | Loss: 0.00053720
Iteration 8/25 | Loss: 0.00053720
Iteration 9/25 | Loss: 0.00053720
Iteration 10/25 | Loss: 0.00053720
Iteration 11/25 | Loss: 0.00053720
Iteration 12/25 | Loss: 0.00053720
Iteration 13/25 | Loss: 0.00053720
Iteration 14/25 | Loss: 0.00053720
Iteration 15/25 | Loss: 0.00053720
Iteration 16/25 | Loss: 0.00053720
Iteration 17/25 | Loss: 0.00053720
Iteration 18/25 | Loss: 0.00053720
Iteration 19/25 | Loss: 0.00053720
Iteration 20/25 | Loss: 0.00053720
Iteration 21/25 | Loss: 0.00053720
Iteration 22/25 | Loss: 0.00053720
Iteration 23/25 | Loss: 0.00053720
Iteration 24/25 | Loss: 0.00053720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005372002488002181, 0.0005372002488002181, 0.0005372002488002181, 0.0005372002488002181, 0.0005372002488002181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005372002488002181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053720
Iteration 2/1000 | Loss: 0.00004712
Iteration 3/1000 | Loss: 0.00003301
Iteration 4/1000 | Loss: 0.00002612
Iteration 5/1000 | Loss: 0.00002452
Iteration 6/1000 | Loss: 0.00002356
Iteration 7/1000 | Loss: 0.00002304
Iteration 8/1000 | Loss: 0.00002273
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002199
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002181
Iteration 15/1000 | Loss: 0.00002178
Iteration 16/1000 | Loss: 0.00002175
Iteration 17/1000 | Loss: 0.00002174
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002170
Iteration 21/1000 | Loss: 0.00002170
Iteration 22/1000 | Loss: 0.00002170
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002165
Iteration 25/1000 | Loss: 0.00002162
Iteration 26/1000 | Loss: 0.00002162
Iteration 27/1000 | Loss: 0.00002161
Iteration 28/1000 | Loss: 0.00002161
Iteration 29/1000 | Loss: 0.00002160
Iteration 30/1000 | Loss: 0.00002155
Iteration 31/1000 | Loss: 0.00002154
Iteration 32/1000 | Loss: 0.00002153
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00002152
Iteration 35/1000 | Loss: 0.00002152
Iteration 36/1000 | Loss: 0.00002152
Iteration 37/1000 | Loss: 0.00002151
Iteration 38/1000 | Loss: 0.00002151
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00002150
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002148
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002148
Iteration 50/1000 | Loss: 0.00002148
Iteration 51/1000 | Loss: 0.00002148
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002147
Iteration 54/1000 | Loss: 0.00002147
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002147
Iteration 57/1000 | Loss: 0.00002147
Iteration 58/1000 | Loss: 0.00002147
Iteration 59/1000 | Loss: 0.00002146
Iteration 60/1000 | Loss: 0.00002146
Iteration 61/1000 | Loss: 0.00002146
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002145
Iteration 64/1000 | Loss: 0.00002145
Iteration 65/1000 | Loss: 0.00002145
Iteration 66/1000 | Loss: 0.00002145
Iteration 67/1000 | Loss: 0.00002145
Iteration 68/1000 | Loss: 0.00002145
Iteration 69/1000 | Loss: 0.00002144
Iteration 70/1000 | Loss: 0.00002144
Iteration 71/1000 | Loss: 0.00002144
Iteration 72/1000 | Loss: 0.00002144
Iteration 73/1000 | Loss: 0.00002144
Iteration 74/1000 | Loss: 0.00002144
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002143
Iteration 78/1000 | Loss: 0.00002143
Iteration 79/1000 | Loss: 0.00002143
Iteration 80/1000 | Loss: 0.00002143
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002143
Iteration 85/1000 | Loss: 0.00002142
Iteration 86/1000 | Loss: 0.00002142
Iteration 87/1000 | Loss: 0.00002142
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002141
Iteration 90/1000 | Loss: 0.00002141
Iteration 91/1000 | Loss: 0.00002141
Iteration 92/1000 | Loss: 0.00002141
Iteration 93/1000 | Loss: 0.00002141
Iteration 94/1000 | Loss: 0.00002141
Iteration 95/1000 | Loss: 0.00002140
Iteration 96/1000 | Loss: 0.00002140
Iteration 97/1000 | Loss: 0.00002140
Iteration 98/1000 | Loss: 0.00002140
Iteration 99/1000 | Loss: 0.00002140
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002139
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002139
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002139
Iteration 121/1000 | Loss: 0.00002139
Iteration 122/1000 | Loss: 0.00002139
Iteration 123/1000 | Loss: 0.00002139
Iteration 124/1000 | Loss: 0.00002139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.1390018446254544e-05, 2.1390018446254544e-05, 2.1390018446254544e-05, 2.1390018446254544e-05, 2.1390018446254544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1390018446254544e-05

Optimization complete. Final v2v error: 3.712189197540283 mm

Highest mean error: 5.120625019073486 mm for frame 90

Lowest mean error: 2.63527250289917 mm for frame 5

Saving results

Total time: 45.58026623725891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764710
Iteration 2/25 | Loss: 0.00151374
Iteration 3/25 | Loss: 0.00126580
Iteration 4/25 | Loss: 0.00120051
Iteration 5/25 | Loss: 0.00118560
Iteration 6/25 | Loss: 0.00118173
Iteration 7/25 | Loss: 0.00118136
Iteration 8/25 | Loss: 0.00118136
Iteration 9/25 | Loss: 0.00118136
Iteration 10/25 | Loss: 0.00118136
Iteration 11/25 | Loss: 0.00118136
Iteration 12/25 | Loss: 0.00118136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011813640594482422, 0.0011813640594482422, 0.0011813640594482422, 0.0011813640594482422, 0.0011813640594482422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011813640594482422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15058482
Iteration 2/25 | Loss: 0.00112940
Iteration 3/25 | Loss: 0.00112938
Iteration 4/25 | Loss: 0.00112938
Iteration 5/25 | Loss: 0.00112938
Iteration 6/25 | Loss: 0.00112938
Iteration 7/25 | Loss: 0.00112938
Iteration 8/25 | Loss: 0.00112938
Iteration 9/25 | Loss: 0.00112938
Iteration 10/25 | Loss: 0.00112938
Iteration 11/25 | Loss: 0.00112938
Iteration 12/25 | Loss: 0.00112938
Iteration 13/25 | Loss: 0.00112938
Iteration 14/25 | Loss: 0.00112938
Iteration 15/25 | Loss: 0.00112938
Iteration 16/25 | Loss: 0.00112938
Iteration 17/25 | Loss: 0.00112938
Iteration 18/25 | Loss: 0.00112938
Iteration 19/25 | Loss: 0.00112938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011293779825791717, 0.0011293779825791717, 0.0011293779825791717, 0.0011293779825791717, 0.0011293779825791717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011293779825791717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112938
Iteration 2/1000 | Loss: 0.00008589
Iteration 3/1000 | Loss: 0.00005310
Iteration 4/1000 | Loss: 0.00004349
Iteration 5/1000 | Loss: 0.00004057
Iteration 6/1000 | Loss: 0.00003869
Iteration 7/1000 | Loss: 0.00003742
Iteration 8/1000 | Loss: 0.00003649
Iteration 9/1000 | Loss: 0.00003574
Iteration 10/1000 | Loss: 0.00003520
Iteration 11/1000 | Loss: 0.00003476
Iteration 12/1000 | Loss: 0.00003435
Iteration 13/1000 | Loss: 0.00003407
Iteration 14/1000 | Loss: 0.00003378
Iteration 15/1000 | Loss: 0.00003359
Iteration 16/1000 | Loss: 0.00003341
Iteration 17/1000 | Loss: 0.00003326
Iteration 18/1000 | Loss: 0.00003313
Iteration 19/1000 | Loss: 0.00003303
Iteration 20/1000 | Loss: 0.00003301
Iteration 21/1000 | Loss: 0.00003300
Iteration 22/1000 | Loss: 0.00003298
Iteration 23/1000 | Loss: 0.00003297
Iteration 24/1000 | Loss: 0.00003294
Iteration 25/1000 | Loss: 0.00003294
Iteration 26/1000 | Loss: 0.00003292
Iteration 27/1000 | Loss: 0.00003291
Iteration 28/1000 | Loss: 0.00003290
Iteration 29/1000 | Loss: 0.00003290
Iteration 30/1000 | Loss: 0.00003289
Iteration 31/1000 | Loss: 0.00003289
Iteration 32/1000 | Loss: 0.00003289
Iteration 33/1000 | Loss: 0.00003288
Iteration 34/1000 | Loss: 0.00003288
Iteration 35/1000 | Loss: 0.00003287
Iteration 36/1000 | Loss: 0.00003286
Iteration 37/1000 | Loss: 0.00003285
Iteration 38/1000 | Loss: 0.00003284
Iteration 39/1000 | Loss: 0.00003284
Iteration 40/1000 | Loss: 0.00003283
Iteration 41/1000 | Loss: 0.00003283
Iteration 42/1000 | Loss: 0.00003283
Iteration 43/1000 | Loss: 0.00003282
Iteration 44/1000 | Loss: 0.00003281
Iteration 45/1000 | Loss: 0.00003281
Iteration 46/1000 | Loss: 0.00003280
Iteration 47/1000 | Loss: 0.00003280
Iteration 48/1000 | Loss: 0.00003279
Iteration 49/1000 | Loss: 0.00003279
Iteration 50/1000 | Loss: 0.00003278
Iteration 51/1000 | Loss: 0.00003278
Iteration 52/1000 | Loss: 0.00003278
Iteration 53/1000 | Loss: 0.00003277
Iteration 54/1000 | Loss: 0.00003277
Iteration 55/1000 | Loss: 0.00003277
Iteration 56/1000 | Loss: 0.00003277
Iteration 57/1000 | Loss: 0.00003276
Iteration 58/1000 | Loss: 0.00003276
Iteration 59/1000 | Loss: 0.00003276
Iteration 60/1000 | Loss: 0.00003275
Iteration 61/1000 | Loss: 0.00003275
Iteration 62/1000 | Loss: 0.00003275
Iteration 63/1000 | Loss: 0.00003275
Iteration 64/1000 | Loss: 0.00003274
Iteration 65/1000 | Loss: 0.00003274
Iteration 66/1000 | Loss: 0.00003274
Iteration 67/1000 | Loss: 0.00003273
Iteration 68/1000 | Loss: 0.00003273
Iteration 69/1000 | Loss: 0.00003273
Iteration 70/1000 | Loss: 0.00003273
Iteration 71/1000 | Loss: 0.00003272
Iteration 72/1000 | Loss: 0.00003272
Iteration 73/1000 | Loss: 0.00003272
Iteration 74/1000 | Loss: 0.00003272
Iteration 75/1000 | Loss: 0.00003272
Iteration 76/1000 | Loss: 0.00003272
Iteration 77/1000 | Loss: 0.00003272
Iteration 78/1000 | Loss: 0.00003272
Iteration 79/1000 | Loss: 0.00003271
Iteration 80/1000 | Loss: 0.00003271
Iteration 81/1000 | Loss: 0.00003271
Iteration 82/1000 | Loss: 0.00003271
Iteration 83/1000 | Loss: 0.00003271
Iteration 84/1000 | Loss: 0.00003270
Iteration 85/1000 | Loss: 0.00003270
Iteration 86/1000 | Loss: 0.00003270
Iteration 87/1000 | Loss: 0.00003270
Iteration 88/1000 | Loss: 0.00003270
Iteration 89/1000 | Loss: 0.00003270
Iteration 90/1000 | Loss: 0.00003269
Iteration 91/1000 | Loss: 0.00003269
Iteration 92/1000 | Loss: 0.00003269
Iteration 93/1000 | Loss: 0.00003269
Iteration 94/1000 | Loss: 0.00003268
Iteration 95/1000 | Loss: 0.00003268
Iteration 96/1000 | Loss: 0.00003268
Iteration 97/1000 | Loss: 0.00003268
Iteration 98/1000 | Loss: 0.00003268
Iteration 99/1000 | Loss: 0.00003268
Iteration 100/1000 | Loss: 0.00003267
Iteration 101/1000 | Loss: 0.00003267
Iteration 102/1000 | Loss: 0.00003267
Iteration 103/1000 | Loss: 0.00003267
Iteration 104/1000 | Loss: 0.00003266
Iteration 105/1000 | Loss: 0.00003266
Iteration 106/1000 | Loss: 0.00003266
Iteration 107/1000 | Loss: 0.00003266
Iteration 108/1000 | Loss: 0.00003265
Iteration 109/1000 | Loss: 0.00003265
Iteration 110/1000 | Loss: 0.00003265
Iteration 111/1000 | Loss: 0.00003265
Iteration 112/1000 | Loss: 0.00003265
Iteration 113/1000 | Loss: 0.00003264
Iteration 114/1000 | Loss: 0.00003264
Iteration 115/1000 | Loss: 0.00003264
Iteration 116/1000 | Loss: 0.00003264
Iteration 117/1000 | Loss: 0.00003264
Iteration 118/1000 | Loss: 0.00003263
Iteration 119/1000 | Loss: 0.00003263
Iteration 120/1000 | Loss: 0.00003263
Iteration 121/1000 | Loss: 0.00003263
Iteration 122/1000 | Loss: 0.00003263
Iteration 123/1000 | Loss: 0.00003263
Iteration 124/1000 | Loss: 0.00003263
Iteration 125/1000 | Loss: 0.00003263
Iteration 126/1000 | Loss: 0.00003263
Iteration 127/1000 | Loss: 0.00003262
Iteration 128/1000 | Loss: 0.00003262
Iteration 129/1000 | Loss: 0.00003262
Iteration 130/1000 | Loss: 0.00003262
Iteration 131/1000 | Loss: 0.00003262
Iteration 132/1000 | Loss: 0.00003262
Iteration 133/1000 | Loss: 0.00003262
Iteration 134/1000 | Loss: 0.00003262
Iteration 135/1000 | Loss: 0.00003262
Iteration 136/1000 | Loss: 0.00003261
Iteration 137/1000 | Loss: 0.00003261
Iteration 138/1000 | Loss: 0.00003261
Iteration 139/1000 | Loss: 0.00003261
Iteration 140/1000 | Loss: 0.00003261
Iteration 141/1000 | Loss: 0.00003261
Iteration 142/1000 | Loss: 0.00003261
Iteration 143/1000 | Loss: 0.00003260
Iteration 144/1000 | Loss: 0.00003260
Iteration 145/1000 | Loss: 0.00003260
Iteration 146/1000 | Loss: 0.00003260
Iteration 147/1000 | Loss: 0.00003260
Iteration 148/1000 | Loss: 0.00003260
Iteration 149/1000 | Loss: 0.00003260
Iteration 150/1000 | Loss: 0.00003260
Iteration 151/1000 | Loss: 0.00003260
Iteration 152/1000 | Loss: 0.00003260
Iteration 153/1000 | Loss: 0.00003260
Iteration 154/1000 | Loss: 0.00003260
Iteration 155/1000 | Loss: 0.00003260
Iteration 156/1000 | Loss: 0.00003260
Iteration 157/1000 | Loss: 0.00003260
Iteration 158/1000 | Loss: 0.00003260
Iteration 159/1000 | Loss: 0.00003260
Iteration 160/1000 | Loss: 0.00003260
Iteration 161/1000 | Loss: 0.00003260
Iteration 162/1000 | Loss: 0.00003260
Iteration 163/1000 | Loss: 0.00003260
Iteration 164/1000 | Loss: 0.00003260
Iteration 165/1000 | Loss: 0.00003260
Iteration 166/1000 | Loss: 0.00003260
Iteration 167/1000 | Loss: 0.00003260
Iteration 168/1000 | Loss: 0.00003260
Iteration 169/1000 | Loss: 0.00003260
Iteration 170/1000 | Loss: 0.00003260
Iteration 171/1000 | Loss: 0.00003260
Iteration 172/1000 | Loss: 0.00003260
Iteration 173/1000 | Loss: 0.00003260
Iteration 174/1000 | Loss: 0.00003260
Iteration 175/1000 | Loss: 0.00003260
Iteration 176/1000 | Loss: 0.00003260
Iteration 177/1000 | Loss: 0.00003260
Iteration 178/1000 | Loss: 0.00003260
Iteration 179/1000 | Loss: 0.00003260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.2602609280729666e-05, 3.2602609280729666e-05, 3.2602609280729666e-05, 3.2602609280729666e-05, 3.2602609280729666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2602609280729666e-05

Optimization complete. Final v2v error: 4.634685039520264 mm

Highest mean error: 5.975747108459473 mm for frame 75

Lowest mean error: 3.3684496879577637 mm for frame 214

Saving results

Total time: 55.628705739974976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813482
Iteration 2/25 | Loss: 0.00119658
Iteration 3/25 | Loss: 0.00104390
Iteration 4/25 | Loss: 0.00101455
Iteration 5/25 | Loss: 0.00100415
Iteration 6/25 | Loss: 0.00100114
Iteration 7/25 | Loss: 0.00100032
Iteration 8/25 | Loss: 0.00100031
Iteration 9/25 | Loss: 0.00100031
Iteration 10/25 | Loss: 0.00100031
Iteration 11/25 | Loss: 0.00100031
Iteration 12/25 | Loss: 0.00100031
Iteration 13/25 | Loss: 0.00100031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010003057541325688, 0.0010003057541325688, 0.0010003057541325688, 0.0010003057541325688, 0.0010003057541325688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010003057541325688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36532879
Iteration 2/25 | Loss: 0.00098409
Iteration 3/25 | Loss: 0.00098408
Iteration 4/25 | Loss: 0.00098408
Iteration 5/25 | Loss: 0.00098408
Iteration 6/25 | Loss: 0.00098408
Iteration 7/25 | Loss: 0.00098408
Iteration 8/25 | Loss: 0.00098408
Iteration 9/25 | Loss: 0.00098408
Iteration 10/25 | Loss: 0.00098408
Iteration 11/25 | Loss: 0.00098408
Iteration 12/25 | Loss: 0.00098408
Iteration 13/25 | Loss: 0.00098408
Iteration 14/25 | Loss: 0.00098408
Iteration 15/25 | Loss: 0.00098408
Iteration 16/25 | Loss: 0.00098408
Iteration 17/25 | Loss: 0.00098408
Iteration 18/25 | Loss: 0.00098408
Iteration 19/25 | Loss: 0.00098408
Iteration 20/25 | Loss: 0.00098408
Iteration 21/25 | Loss: 0.00098408
Iteration 22/25 | Loss: 0.00098408
Iteration 23/25 | Loss: 0.00098408
Iteration 24/25 | Loss: 0.00098408
Iteration 25/25 | Loss: 0.00098408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098408
Iteration 2/1000 | Loss: 0.00003921
Iteration 3/1000 | Loss: 0.00002612
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002051
Iteration 7/1000 | Loss: 0.00001993
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001904
Iteration 10/1000 | Loss: 0.00001874
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001792
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001788
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001773
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001768
Iteration 131/1000 | Loss: 0.00001768
Iteration 132/1000 | Loss: 0.00001768
Iteration 133/1000 | Loss: 0.00001768
Iteration 134/1000 | Loss: 0.00001768
Iteration 135/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.7675112758297473e-05, 1.7675112758297473e-05, 1.7675112758297473e-05, 1.7675112758297473e-05, 1.7675112758297473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7675112758297473e-05

Optimization complete. Final v2v error: 3.5363240242004395 mm

Highest mean error: 4.418938159942627 mm for frame 60

Lowest mean error: 3.2937064170837402 mm for frame 110

Saving results

Total time: 40.80885410308838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770465
Iteration 2/25 | Loss: 0.00157677
Iteration 3/25 | Loss: 0.00087451
Iteration 4/25 | Loss: 0.00095522
Iteration 5/25 | Loss: 0.00077433
Iteration 6/25 | Loss: 0.00076439
Iteration 7/25 | Loss: 0.00075133
Iteration 8/25 | Loss: 0.00075400
Iteration 9/25 | Loss: 0.00074859
Iteration 10/25 | Loss: 0.00074798
Iteration 11/25 | Loss: 0.00074740
Iteration 12/25 | Loss: 0.00074705
Iteration 13/25 | Loss: 0.00074683
Iteration 14/25 | Loss: 0.00074671
Iteration 15/25 | Loss: 0.00074670
Iteration 16/25 | Loss: 0.00074670
Iteration 17/25 | Loss: 0.00074670
Iteration 18/25 | Loss: 0.00074670
Iteration 19/25 | Loss: 0.00074670
Iteration 20/25 | Loss: 0.00074670
Iteration 21/25 | Loss: 0.00074670
Iteration 22/25 | Loss: 0.00074670
Iteration 23/25 | Loss: 0.00074670
Iteration 24/25 | Loss: 0.00074670
Iteration 25/25 | Loss: 0.00074670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99046099
Iteration 2/25 | Loss: 0.00061225
Iteration 3/25 | Loss: 0.00061225
Iteration 4/25 | Loss: 0.00061225
Iteration 5/25 | Loss: 0.00061225
Iteration 6/25 | Loss: 0.00061225
Iteration 7/25 | Loss: 0.00061225
Iteration 8/25 | Loss: 0.00061225
Iteration 9/25 | Loss: 0.00061225
Iteration 10/25 | Loss: 0.00061225
Iteration 11/25 | Loss: 0.00061225
Iteration 12/25 | Loss: 0.00061225
Iteration 13/25 | Loss: 0.00061225
Iteration 14/25 | Loss: 0.00061225
Iteration 15/25 | Loss: 0.00061225
Iteration 16/25 | Loss: 0.00061225
Iteration 17/25 | Loss: 0.00061225
Iteration 18/25 | Loss: 0.00061225
Iteration 19/25 | Loss: 0.00061225
Iteration 20/25 | Loss: 0.00061225
Iteration 21/25 | Loss: 0.00061225
Iteration 22/25 | Loss: 0.00061225
Iteration 23/25 | Loss: 0.00061225
Iteration 24/25 | Loss: 0.00061225
Iteration 25/25 | Loss: 0.00061225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061225
Iteration 2/1000 | Loss: 0.00004825
Iteration 3/1000 | Loss: 0.00003042
Iteration 4/1000 | Loss: 0.00001649
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00010210
Iteration 7/1000 | Loss: 0.00001477
Iteration 8/1000 | Loss: 0.00010408
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00006921
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00008857
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00016723
Iteration 17/1000 | Loss: 0.00001392
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001366
Iteration 34/1000 | Loss: 0.00001366
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001355
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001355
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001345
Iteration 77/1000 | Loss: 0.00001345
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001342
Iteration 86/1000 | Loss: 0.00001341
Iteration 87/1000 | Loss: 0.00001341
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001341
Iteration 90/1000 | Loss: 0.00001341
Iteration 91/1000 | Loss: 0.00001341
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001334
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001332
Iteration 135/1000 | Loss: 0.00001332
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001332
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001331
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001330
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001327
Iteration 176/1000 | Loss: 0.00001327
Iteration 177/1000 | Loss: 0.00001327
Iteration 178/1000 | Loss: 0.00001327
Iteration 179/1000 | Loss: 0.00001327
Iteration 180/1000 | Loss: 0.00001327
Iteration 181/1000 | Loss: 0.00001327
Iteration 182/1000 | Loss: 0.00001327
Iteration 183/1000 | Loss: 0.00001327
Iteration 184/1000 | Loss: 0.00001327
Iteration 185/1000 | Loss: 0.00001327
Iteration 186/1000 | Loss: 0.00001327
Iteration 187/1000 | Loss: 0.00001327
Iteration 188/1000 | Loss: 0.00001327
Iteration 189/1000 | Loss: 0.00001327
Iteration 190/1000 | Loss: 0.00001327
Iteration 191/1000 | Loss: 0.00001327
Iteration 192/1000 | Loss: 0.00001327
Iteration 193/1000 | Loss: 0.00001327
Iteration 194/1000 | Loss: 0.00001327
Iteration 195/1000 | Loss: 0.00001327
Iteration 196/1000 | Loss: 0.00001327
Iteration 197/1000 | Loss: 0.00001327
Iteration 198/1000 | Loss: 0.00001327
Iteration 199/1000 | Loss: 0.00001327
Iteration 200/1000 | Loss: 0.00001327
Iteration 201/1000 | Loss: 0.00001327
Iteration 202/1000 | Loss: 0.00001327
Iteration 203/1000 | Loss: 0.00001327
Iteration 204/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.3265197594591882e-05, 1.3265197594591882e-05, 1.3265197594591882e-05, 1.3265197594591882e-05, 1.3265197594591882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3265197594591882e-05

Optimization complete. Final v2v error: 3.054950714111328 mm

Highest mean error: 3.466646671295166 mm for frame 64

Lowest mean error: 2.784334182739258 mm for frame 18

Saving results

Total time: 64.3018913269043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385675
Iteration 2/25 | Loss: 0.00087305
Iteration 3/25 | Loss: 0.00072208
Iteration 4/25 | Loss: 0.00070950
Iteration 5/25 | Loss: 0.00070576
Iteration 6/25 | Loss: 0.00070455
Iteration 7/25 | Loss: 0.00070429
Iteration 8/25 | Loss: 0.00070429
Iteration 9/25 | Loss: 0.00070429
Iteration 10/25 | Loss: 0.00070429
Iteration 11/25 | Loss: 0.00070429
Iteration 12/25 | Loss: 0.00070429
Iteration 13/25 | Loss: 0.00070429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000704286911059171, 0.000704286911059171, 0.000704286911059171, 0.000704286911059171, 0.000704286911059171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000704286911059171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50058007
Iteration 2/25 | Loss: 0.00041085
Iteration 3/25 | Loss: 0.00041085
Iteration 4/25 | Loss: 0.00041085
Iteration 5/25 | Loss: 0.00041085
Iteration 6/25 | Loss: 0.00041085
Iteration 7/25 | Loss: 0.00041085
Iteration 8/25 | Loss: 0.00041085
Iteration 9/25 | Loss: 0.00041085
Iteration 10/25 | Loss: 0.00041085
Iteration 11/25 | Loss: 0.00041084
Iteration 12/25 | Loss: 0.00041084
Iteration 13/25 | Loss: 0.00041084
Iteration 14/25 | Loss: 0.00041084
Iteration 15/25 | Loss: 0.00041084
Iteration 16/25 | Loss: 0.00041084
Iteration 17/25 | Loss: 0.00041084
Iteration 18/25 | Loss: 0.00041084
Iteration 19/25 | Loss: 0.00041084
Iteration 20/25 | Loss: 0.00041084
Iteration 21/25 | Loss: 0.00041084
Iteration 22/25 | Loss: 0.00041084
Iteration 23/25 | Loss: 0.00041084
Iteration 24/25 | Loss: 0.00041084
Iteration 25/25 | Loss: 0.00041084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041084
Iteration 2/1000 | Loss: 0.00001785
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001143
Iteration 5/1000 | Loss: 0.00001066
Iteration 6/1000 | Loss: 0.00001033
Iteration 7/1000 | Loss: 0.00001015
Iteration 8/1000 | Loss: 0.00001012
Iteration 9/1000 | Loss: 0.00001010
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00001007
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001006
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00001006
Iteration 16/1000 | Loss: 0.00001006
Iteration 17/1000 | Loss: 0.00001005
Iteration 18/1000 | Loss: 0.00001005
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001001
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000998
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000978
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000977
Iteration 37/1000 | Loss: 0.00000975
Iteration 38/1000 | Loss: 0.00000974
Iteration 39/1000 | Loss: 0.00000974
Iteration 40/1000 | Loss: 0.00000974
Iteration 41/1000 | Loss: 0.00000974
Iteration 42/1000 | Loss: 0.00000974
Iteration 43/1000 | Loss: 0.00000974
Iteration 44/1000 | Loss: 0.00000974
Iteration 45/1000 | Loss: 0.00000974
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000974
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000972
Iteration 50/1000 | Loss: 0.00000972
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000972
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000972
Iteration 55/1000 | Loss: 0.00000972
Iteration 56/1000 | Loss: 0.00000972
Iteration 57/1000 | Loss: 0.00000972
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000972
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000972
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [9.72389716480393e-06, 9.72389716480393e-06, 9.72389716480393e-06, 9.72389716480393e-06, 9.72389716480393e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.72389716480393e-06

Optimization complete. Final v2v error: 2.6363961696624756 mm

Highest mean error: 3.675900936126709 mm for frame 69

Lowest mean error: 2.464895009994507 mm for frame 102

Saving results

Total time: 27.745813369750977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791851
Iteration 2/25 | Loss: 0.00158382
Iteration 3/25 | Loss: 0.00098448
Iteration 4/25 | Loss: 0.00091041
Iteration 5/25 | Loss: 0.00088947
Iteration 6/25 | Loss: 0.00091193
Iteration 7/25 | Loss: 0.00086152
Iteration 8/25 | Loss: 0.00082838
Iteration 9/25 | Loss: 0.00080068
Iteration 10/25 | Loss: 0.00080758
Iteration 11/25 | Loss: 0.00079894
Iteration 12/25 | Loss: 0.00080031
Iteration 13/25 | Loss: 0.00079504
Iteration 14/25 | Loss: 0.00079786
Iteration 15/25 | Loss: 0.00079477
Iteration 16/25 | Loss: 0.00079208
Iteration 17/25 | Loss: 0.00079160
Iteration 18/25 | Loss: 0.00079142
Iteration 19/25 | Loss: 0.00079141
Iteration 20/25 | Loss: 0.00079140
Iteration 21/25 | Loss: 0.00079140
Iteration 22/25 | Loss: 0.00079140
Iteration 23/25 | Loss: 0.00079140
Iteration 24/25 | Loss: 0.00079140
Iteration 25/25 | Loss: 0.00079140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68647766
Iteration 2/25 | Loss: 0.00059272
Iteration 3/25 | Loss: 0.00059268
Iteration 4/25 | Loss: 0.00059267
Iteration 5/25 | Loss: 0.00059267
Iteration 6/25 | Loss: 0.00059267
Iteration 7/25 | Loss: 0.00059267
Iteration 8/25 | Loss: 0.00059267
Iteration 9/25 | Loss: 0.00059267
Iteration 10/25 | Loss: 0.00059267
Iteration 11/25 | Loss: 0.00059267
Iteration 12/25 | Loss: 0.00059267
Iteration 13/25 | Loss: 0.00059267
Iteration 14/25 | Loss: 0.00059267
Iteration 15/25 | Loss: 0.00059267
Iteration 16/25 | Loss: 0.00059267
Iteration 17/25 | Loss: 0.00059267
Iteration 18/25 | Loss: 0.00059267
Iteration 19/25 | Loss: 0.00059267
Iteration 20/25 | Loss: 0.00059267
Iteration 21/25 | Loss: 0.00059267
Iteration 22/25 | Loss: 0.00059267
Iteration 23/25 | Loss: 0.00059267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005926726153120399, 0.0005926726153120399, 0.0005926726153120399, 0.0005926726153120399, 0.0005926726153120399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005926726153120399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059267
Iteration 2/1000 | Loss: 0.00002561
Iteration 3/1000 | Loss: 0.00002124
Iteration 4/1000 | Loss: 0.00002446
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00002359
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00017271
Iteration 9/1000 | Loss: 0.00019396
Iteration 10/1000 | Loss: 0.00005817
Iteration 11/1000 | Loss: 0.00003224
Iteration 12/1000 | Loss: 0.00002191
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00034259
Iteration 15/1000 | Loss: 0.00011619
Iteration 16/1000 | Loss: 0.00007275
Iteration 17/1000 | Loss: 0.00001870
Iteration 18/1000 | Loss: 0.00003215
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00002387
Iteration 21/1000 | Loss: 0.00002960
Iteration 22/1000 | Loss: 0.00002440
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00008678
Iteration 26/1000 | Loss: 0.00034307
Iteration 27/1000 | Loss: 0.00009196
Iteration 28/1000 | Loss: 0.00014071
Iteration 29/1000 | Loss: 0.00009244
Iteration 30/1000 | Loss: 0.00002895
Iteration 31/1000 | Loss: 0.00009847
Iteration 32/1000 | Loss: 0.00069281
Iteration 33/1000 | Loss: 0.00061355
Iteration 34/1000 | Loss: 0.00005912
Iteration 35/1000 | Loss: 0.00032445
Iteration 36/1000 | Loss: 0.00002235
Iteration 37/1000 | Loss: 0.00005799
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00003638
Iteration 40/1000 | Loss: 0.00003414
Iteration 41/1000 | Loss: 0.00003522
Iteration 42/1000 | Loss: 0.00003345
Iteration 43/1000 | Loss: 0.00003548
Iteration 44/1000 | Loss: 0.00003114
Iteration 45/1000 | Loss: 0.00023875
Iteration 46/1000 | Loss: 0.00004263
Iteration 47/1000 | Loss: 0.00004997
Iteration 48/1000 | Loss: 0.00003235
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00013748
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001603
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00015621
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001546
Iteration 94/1000 | Loss: 0.00001546
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001545
Iteration 118/1000 | Loss: 0.00001545
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001545
Iteration 121/1000 | Loss: 0.00001545
Iteration 122/1000 | Loss: 0.00001545
Iteration 123/1000 | Loss: 0.00001545
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00001545
Iteration 126/1000 | Loss: 0.00001545
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001544
Iteration 131/1000 | Loss: 0.00001544
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001543
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001542
Iteration 145/1000 | Loss: 0.00001542
Iteration 146/1000 | Loss: 0.00001542
Iteration 147/1000 | Loss: 0.00001542
Iteration 148/1000 | Loss: 0.00001542
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001542
Iteration 151/1000 | Loss: 0.00001542
Iteration 152/1000 | Loss: 0.00001542
Iteration 153/1000 | Loss: 0.00001542
Iteration 154/1000 | Loss: 0.00001542
Iteration 155/1000 | Loss: 0.00001542
Iteration 156/1000 | Loss: 0.00001542
Iteration 157/1000 | Loss: 0.00001542
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001541
Iteration 160/1000 | Loss: 0.00001541
Iteration 161/1000 | Loss: 0.00001541
Iteration 162/1000 | Loss: 0.00001541
Iteration 163/1000 | Loss: 0.00001541
Iteration 164/1000 | Loss: 0.00001541
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001541
Iteration 173/1000 | Loss: 0.00001541
Iteration 174/1000 | Loss: 0.00001541
Iteration 175/1000 | Loss: 0.00001541
Iteration 176/1000 | Loss: 0.00001541
Iteration 177/1000 | Loss: 0.00001541
Iteration 178/1000 | Loss: 0.00001541
Iteration 179/1000 | Loss: 0.00001541
Iteration 180/1000 | Loss: 0.00001541
Iteration 181/1000 | Loss: 0.00001541
Iteration 182/1000 | Loss: 0.00001541
Iteration 183/1000 | Loss: 0.00001541
Iteration 184/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.540553421364166e-05, 1.540553421364166e-05, 1.540553421364166e-05, 1.540553421364166e-05, 1.540553421364166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.540553421364166e-05

Optimization complete. Final v2v error: 3.2851784229278564 mm

Highest mean error: 3.9173521995544434 mm for frame 153

Lowest mean error: 3.0535993576049805 mm for frame 134

Saving results

Total time: 140.2092125415802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068185
Iteration 2/25 | Loss: 0.00271435
Iteration 3/25 | Loss: 0.00155083
Iteration 4/25 | Loss: 0.00119026
Iteration 5/25 | Loss: 0.00105940
Iteration 6/25 | Loss: 0.00101728
Iteration 7/25 | Loss: 0.00098504
Iteration 8/25 | Loss: 0.00100088
Iteration 9/25 | Loss: 0.00094495
Iteration 10/25 | Loss: 0.00090508
Iteration 11/25 | Loss: 0.00086631
Iteration 12/25 | Loss: 0.00084598
Iteration 13/25 | Loss: 0.00084214
Iteration 14/25 | Loss: 0.00083195
Iteration 15/25 | Loss: 0.00083245
Iteration 16/25 | Loss: 0.00083083
Iteration 17/25 | Loss: 0.00082880
Iteration 18/25 | Loss: 0.00082521
Iteration 19/25 | Loss: 0.00082735
Iteration 20/25 | Loss: 0.00082383
Iteration 21/25 | Loss: 0.00082245
Iteration 22/25 | Loss: 0.00082293
Iteration 23/25 | Loss: 0.00082468
Iteration 24/25 | Loss: 0.00082386
Iteration 25/25 | Loss: 0.00082594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53796875
Iteration 2/25 | Loss: 0.00200761
Iteration 3/25 | Loss: 0.00157189
Iteration 4/25 | Loss: 0.00157188
Iteration 5/25 | Loss: 0.00157188
Iteration 6/25 | Loss: 0.00157188
Iteration 7/25 | Loss: 0.00157188
Iteration 8/25 | Loss: 0.00157188
Iteration 9/25 | Loss: 0.00157188
Iteration 10/25 | Loss: 0.00157188
Iteration 11/25 | Loss: 0.00157188
Iteration 12/25 | Loss: 0.00157188
Iteration 13/25 | Loss: 0.00157188
Iteration 14/25 | Loss: 0.00157188
Iteration 15/25 | Loss: 0.00157188
Iteration 16/25 | Loss: 0.00157188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015718827489763498, 0.0015718827489763498, 0.0015718827489763498, 0.0015718827489763498, 0.0015718827489763498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015718827489763498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157188
Iteration 2/1000 | Loss: 0.00080913
Iteration 3/1000 | Loss: 0.00102767
Iteration 4/1000 | Loss: 0.00080892
Iteration 5/1000 | Loss: 0.00069673
Iteration 6/1000 | Loss: 0.00035165
Iteration 7/1000 | Loss: 0.00026707
Iteration 8/1000 | Loss: 0.00045278
Iteration 9/1000 | Loss: 0.00029467
Iteration 10/1000 | Loss: 0.00015447
Iteration 11/1000 | Loss: 0.00032895
Iteration 12/1000 | Loss: 0.00026759
Iteration 13/1000 | Loss: 0.00026118
Iteration 14/1000 | Loss: 0.00050160
Iteration 15/1000 | Loss: 0.00097544
Iteration 16/1000 | Loss: 0.00042952
Iteration 17/1000 | Loss: 0.00019396
Iteration 18/1000 | Loss: 0.00016771
Iteration 19/1000 | Loss: 0.00024379
Iteration 20/1000 | Loss: 0.00019827
Iteration 21/1000 | Loss: 0.00018182
Iteration 22/1000 | Loss: 0.00025081
Iteration 23/1000 | Loss: 0.00024394
Iteration 24/1000 | Loss: 0.00008729
Iteration 25/1000 | Loss: 0.00015253
Iteration 26/1000 | Loss: 0.00014442
Iteration 27/1000 | Loss: 0.00007465
Iteration 28/1000 | Loss: 0.00015302
Iteration 29/1000 | Loss: 0.00022185
Iteration 30/1000 | Loss: 0.00014422
Iteration 31/1000 | Loss: 0.00053160
Iteration 32/1000 | Loss: 0.00016993
Iteration 33/1000 | Loss: 0.00008954
Iteration 34/1000 | Loss: 0.00047706
Iteration 35/1000 | Loss: 0.00060411
Iteration 36/1000 | Loss: 0.00032547
Iteration 37/1000 | Loss: 0.00071834
Iteration 38/1000 | Loss: 0.00005142
Iteration 39/1000 | Loss: 0.00011009
Iteration 40/1000 | Loss: 0.00002932
Iteration 41/1000 | Loss: 0.00004744
Iteration 42/1000 | Loss: 0.00007082
Iteration 43/1000 | Loss: 0.00002472
Iteration 44/1000 | Loss: 0.00016792
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00026986
Iteration 47/1000 | Loss: 0.00014969
Iteration 48/1000 | Loss: 0.00128537
Iteration 49/1000 | Loss: 0.00006329
Iteration 50/1000 | Loss: 0.00027703
Iteration 51/1000 | Loss: 0.00004166
Iteration 52/1000 | Loss: 0.00002442
Iteration 53/1000 | Loss: 0.00002366
Iteration 54/1000 | Loss: 0.00007106
Iteration 55/1000 | Loss: 0.00013706
Iteration 56/1000 | Loss: 0.00008622
Iteration 57/1000 | Loss: 0.00004363
Iteration 58/1000 | Loss: 0.00002299
Iteration 59/1000 | Loss: 0.00002254
Iteration 60/1000 | Loss: 0.00006780
Iteration 61/1000 | Loss: 0.00026854
Iteration 62/1000 | Loss: 0.00068400
Iteration 63/1000 | Loss: 0.00096551
Iteration 64/1000 | Loss: 0.00106181
Iteration 65/1000 | Loss: 0.00012013
Iteration 66/1000 | Loss: 0.00005410
Iteration 67/1000 | Loss: 0.00007121
Iteration 68/1000 | Loss: 0.00004963
Iteration 69/1000 | Loss: 0.00026764
Iteration 70/1000 | Loss: 0.00003950
Iteration 71/1000 | Loss: 0.00003617
Iteration 72/1000 | Loss: 0.00006660
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00010814
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00005801
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00009349
Iteration 79/1000 | Loss: 0.00084020
Iteration 80/1000 | Loss: 0.00038719
Iteration 81/1000 | Loss: 0.00095221
Iteration 82/1000 | Loss: 0.00061949
Iteration 83/1000 | Loss: 0.00013857
Iteration 84/1000 | Loss: 0.00002040
Iteration 85/1000 | Loss: 0.00039514
Iteration 86/1000 | Loss: 0.00024593
Iteration 87/1000 | Loss: 0.00001814
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00016964
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00031095
Iteration 95/1000 | Loss: 0.00025537
Iteration 96/1000 | Loss: 0.00012498
Iteration 97/1000 | Loss: 0.00035568
Iteration 98/1000 | Loss: 0.00027010
Iteration 99/1000 | Loss: 0.00029394
Iteration 100/1000 | Loss: 0.00052755
Iteration 101/1000 | Loss: 0.00025433
Iteration 102/1000 | Loss: 0.00037100
Iteration 103/1000 | Loss: 0.00175060
Iteration 104/1000 | Loss: 0.00005168
Iteration 105/1000 | Loss: 0.00006845
Iteration 106/1000 | Loss: 0.00001866
Iteration 107/1000 | Loss: 0.00034872
Iteration 108/1000 | Loss: 0.00171442
Iteration 109/1000 | Loss: 0.00218907
Iteration 110/1000 | Loss: 0.00202409
Iteration 111/1000 | Loss: 0.00010791
Iteration 112/1000 | Loss: 0.00138048
Iteration 113/1000 | Loss: 0.00003611
Iteration 114/1000 | Loss: 0.00007925
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00034716
Iteration 117/1000 | Loss: 0.00022692
Iteration 118/1000 | Loss: 0.00032090
Iteration 119/1000 | Loss: 0.00016114
Iteration 120/1000 | Loss: 0.00008011
Iteration 121/1000 | Loss: 0.00023306
Iteration 122/1000 | Loss: 0.00007541
Iteration 123/1000 | Loss: 0.00002172
Iteration 124/1000 | Loss: 0.00003014
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00016901
Iteration 127/1000 | Loss: 0.00019369
Iteration 128/1000 | Loss: 0.00002447
Iteration 129/1000 | Loss: 0.00012961
Iteration 130/1000 | Loss: 0.00001664
Iteration 131/1000 | Loss: 0.00006607
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00003157
Iteration 134/1000 | Loss: 0.00001463
Iteration 135/1000 | Loss: 0.00001460
Iteration 136/1000 | Loss: 0.00006847
Iteration 137/1000 | Loss: 0.00012102
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001439
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001434
Iteration 144/1000 | Loss: 0.00001434
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Iteration 149/1000 | Loss: 0.00001433
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001429
Iteration 159/1000 | Loss: 0.00001429
Iteration 160/1000 | Loss: 0.00001428
Iteration 161/1000 | Loss: 0.00001428
Iteration 162/1000 | Loss: 0.00001428
Iteration 163/1000 | Loss: 0.00001428
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00007941
Iteration 173/1000 | Loss: 0.00015532
Iteration 174/1000 | Loss: 0.00045249
Iteration 175/1000 | Loss: 0.00001984
Iteration 176/1000 | Loss: 0.00006128
Iteration 177/1000 | Loss: 0.00001451
Iteration 178/1000 | Loss: 0.00004894
Iteration 179/1000 | Loss: 0.00002123
Iteration 180/1000 | Loss: 0.00003674
Iteration 181/1000 | Loss: 0.00001421
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001414
Iteration 188/1000 | Loss: 0.00001413
Iteration 189/1000 | Loss: 0.00001413
Iteration 190/1000 | Loss: 0.00001413
Iteration 191/1000 | Loss: 0.00001413
Iteration 192/1000 | Loss: 0.00001413
Iteration 193/1000 | Loss: 0.00001413
Iteration 194/1000 | Loss: 0.00001413
Iteration 195/1000 | Loss: 0.00001413
Iteration 196/1000 | Loss: 0.00001412
Iteration 197/1000 | Loss: 0.00001412
Iteration 198/1000 | Loss: 0.00001412
Iteration 199/1000 | Loss: 0.00001412
Iteration 200/1000 | Loss: 0.00001411
Iteration 201/1000 | Loss: 0.00001411
Iteration 202/1000 | Loss: 0.00001411
Iteration 203/1000 | Loss: 0.00001410
Iteration 204/1000 | Loss: 0.00001410
Iteration 205/1000 | Loss: 0.00001409
Iteration 206/1000 | Loss: 0.00001408
Iteration 207/1000 | Loss: 0.00001408
Iteration 208/1000 | Loss: 0.00001408
Iteration 209/1000 | Loss: 0.00001408
Iteration 210/1000 | Loss: 0.00001408
Iteration 211/1000 | Loss: 0.00001408
Iteration 212/1000 | Loss: 0.00001407
Iteration 213/1000 | Loss: 0.00001407
Iteration 214/1000 | Loss: 0.00001406
Iteration 215/1000 | Loss: 0.00001406
Iteration 216/1000 | Loss: 0.00001405
Iteration 217/1000 | Loss: 0.00001405
Iteration 218/1000 | Loss: 0.00001405
Iteration 219/1000 | Loss: 0.00001404
Iteration 220/1000 | Loss: 0.00001404
Iteration 221/1000 | Loss: 0.00001399
Iteration 222/1000 | Loss: 0.00001398
Iteration 223/1000 | Loss: 0.00001380
Iteration 224/1000 | Loss: 0.00001353
Iteration 225/1000 | Loss: 0.00001337
Iteration 226/1000 | Loss: 0.00001325
Iteration 227/1000 | Loss: 0.00001317
Iteration 228/1000 | Loss: 0.00001314
Iteration 229/1000 | Loss: 0.00001311
Iteration 230/1000 | Loss: 0.00001311
Iteration 231/1000 | Loss: 0.00001309
Iteration 232/1000 | Loss: 0.00001308
Iteration 233/1000 | Loss: 0.00001308
Iteration 234/1000 | Loss: 0.00001307
Iteration 235/1000 | Loss: 0.00002578
Iteration 236/1000 | Loss: 0.00001302
Iteration 237/1000 | Loss: 0.00001300
Iteration 238/1000 | Loss: 0.00001300
Iteration 239/1000 | Loss: 0.00001300
Iteration 240/1000 | Loss: 0.00001300
Iteration 241/1000 | Loss: 0.00001300
Iteration 242/1000 | Loss: 0.00001300
Iteration 243/1000 | Loss: 0.00001299
Iteration 244/1000 | Loss: 0.00001299
Iteration 245/1000 | Loss: 0.00004947
Iteration 246/1000 | Loss: 0.00001443
Iteration 247/1000 | Loss: 0.00001300
Iteration 248/1000 | Loss: 0.00001300
Iteration 249/1000 | Loss: 0.00001299
Iteration 250/1000 | Loss: 0.00001299
Iteration 251/1000 | Loss: 0.00001299
Iteration 252/1000 | Loss: 0.00001299
Iteration 253/1000 | Loss: 0.00001299
Iteration 254/1000 | Loss: 0.00001298
Iteration 255/1000 | Loss: 0.00001298
Iteration 256/1000 | Loss: 0.00001298
Iteration 257/1000 | Loss: 0.00001298
Iteration 258/1000 | Loss: 0.00001298
Iteration 259/1000 | Loss: 0.00001298
Iteration 260/1000 | Loss: 0.00001298
Iteration 261/1000 | Loss: 0.00001298
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001297
Iteration 264/1000 | Loss: 0.00001297
Iteration 265/1000 | Loss: 0.00001297
Iteration 266/1000 | Loss: 0.00001297
Iteration 267/1000 | Loss: 0.00001297
Iteration 268/1000 | Loss: 0.00001297
Iteration 269/1000 | Loss: 0.00001297
Iteration 270/1000 | Loss: 0.00001297
Iteration 271/1000 | Loss: 0.00001297
Iteration 272/1000 | Loss: 0.00001297
Iteration 273/1000 | Loss: 0.00001297
Iteration 274/1000 | Loss: 0.00001297
Iteration 275/1000 | Loss: 0.00001297
Iteration 276/1000 | Loss: 0.00001297
Iteration 277/1000 | Loss: 0.00001297
Iteration 278/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.2968069313501474e-05, 1.2968069313501474e-05, 1.2968069313501474e-05, 1.2968069313501474e-05, 1.2968069313501474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2968069313501474e-05

Optimization complete. Final v2v error: 3.0553677082061768 mm

Highest mean error: 4.862344264984131 mm for frame 102

Lowest mean error: 2.6312761306762695 mm for frame 45

Saving results

Total time: 317.4582052230835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889952
Iteration 2/25 | Loss: 0.00217812
Iteration 3/25 | Loss: 0.00151889
Iteration 4/25 | Loss: 0.00110212
Iteration 5/25 | Loss: 0.00096453
Iteration 6/25 | Loss: 0.00093282
Iteration 7/25 | Loss: 0.00093059
Iteration 8/25 | Loss: 0.00094008
Iteration 9/25 | Loss: 0.00094776
Iteration 10/25 | Loss: 0.00094896
Iteration 11/25 | Loss: 0.00094059
Iteration 12/25 | Loss: 0.00093453
Iteration 13/25 | Loss: 0.00092894
Iteration 14/25 | Loss: 0.00092762
Iteration 15/25 | Loss: 0.00092775
Iteration 16/25 | Loss: 0.00092645
Iteration 17/25 | Loss: 0.00092725
Iteration 18/25 | Loss: 0.00092861
Iteration 19/25 | Loss: 0.00092795
Iteration 20/25 | Loss: 0.00092543
Iteration 21/25 | Loss: 0.00092581
Iteration 22/25 | Loss: 0.00092343
Iteration 23/25 | Loss: 0.00092341
Iteration 24/25 | Loss: 0.00092318
Iteration 25/25 | Loss: 0.00092323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24199975
Iteration 2/25 | Loss: 0.00070457
Iteration 3/25 | Loss: 0.00070457
Iteration 4/25 | Loss: 0.00070456
Iteration 5/25 | Loss: 0.00070456
Iteration 6/25 | Loss: 0.00070456
Iteration 7/25 | Loss: 0.00070456
Iteration 8/25 | Loss: 0.00070456
Iteration 9/25 | Loss: 0.00070456
Iteration 10/25 | Loss: 0.00070456
Iteration 11/25 | Loss: 0.00070456
Iteration 12/25 | Loss: 0.00070456
Iteration 13/25 | Loss: 0.00070456
Iteration 14/25 | Loss: 0.00070456
Iteration 15/25 | Loss: 0.00070456
Iteration 16/25 | Loss: 0.00070456
Iteration 17/25 | Loss: 0.00070456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007045624079182744, 0.0007045624079182744, 0.0007045624079182744, 0.0007045624079182744, 0.0007045624079182744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007045624079182744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070456
Iteration 2/1000 | Loss: 0.00020020
Iteration 3/1000 | Loss: 0.00021605
Iteration 4/1000 | Loss: 0.00019634
Iteration 5/1000 | Loss: 0.00015809
Iteration 6/1000 | Loss: 0.00023927
Iteration 7/1000 | Loss: 0.00027216
Iteration 8/1000 | Loss: 0.00014842
Iteration 9/1000 | Loss: 0.00017998
Iteration 10/1000 | Loss: 0.00018244
Iteration 11/1000 | Loss: 0.00018216
Iteration 12/1000 | Loss: 0.00026600
Iteration 13/1000 | Loss: 0.00027157
Iteration 14/1000 | Loss: 0.00025440
Iteration 15/1000 | Loss: 0.00026923
Iteration 16/1000 | Loss: 0.00019516
Iteration 17/1000 | Loss: 0.00017871
Iteration 18/1000 | Loss: 0.00011503
Iteration 19/1000 | Loss: 0.00015456
Iteration 20/1000 | Loss: 0.00014470
Iteration 21/1000 | Loss: 0.00017488
Iteration 22/1000 | Loss: 0.00016007
Iteration 23/1000 | Loss: 0.00017241
Iteration 24/1000 | Loss: 0.00020079
Iteration 25/1000 | Loss: 0.00016983
Iteration 26/1000 | Loss: 0.00023998
Iteration 27/1000 | Loss: 0.00014834
Iteration 28/1000 | Loss: 0.00009965
Iteration 29/1000 | Loss: 0.00012652
Iteration 30/1000 | Loss: 0.00014642
Iteration 31/1000 | Loss: 0.00015521
Iteration 32/1000 | Loss: 0.00016704
Iteration 33/1000 | Loss: 0.00017778
Iteration 34/1000 | Loss: 0.00019287
Iteration 35/1000 | Loss: 0.00015155
Iteration 36/1000 | Loss: 0.00016295
Iteration 37/1000 | Loss: 0.00019583
Iteration 38/1000 | Loss: 0.00013186
Iteration 39/1000 | Loss: 0.00018580
Iteration 40/1000 | Loss: 0.00018636
Iteration 41/1000 | Loss: 0.00019005
Iteration 42/1000 | Loss: 0.00020667
Iteration 43/1000 | Loss: 0.00015411
Iteration 44/1000 | Loss: 0.00010346
Iteration 45/1000 | Loss: 0.00008035
Iteration 46/1000 | Loss: 0.00008266
Iteration 47/1000 | Loss: 0.00013163
Iteration 48/1000 | Loss: 0.00008668
Iteration 49/1000 | Loss: 0.00012067
Iteration 50/1000 | Loss: 0.00010874
Iteration 51/1000 | Loss: 0.00023985
Iteration 52/1000 | Loss: 0.00009368
Iteration 53/1000 | Loss: 0.00025975
Iteration 54/1000 | Loss: 0.00014383
Iteration 55/1000 | Loss: 0.00010085
Iteration 56/1000 | Loss: 0.00020397
Iteration 57/1000 | Loss: 0.00011784
Iteration 58/1000 | Loss: 0.00010045
Iteration 59/1000 | Loss: 0.00011316
Iteration 60/1000 | Loss: 0.00010483
Iteration 61/1000 | Loss: 0.00009917
Iteration 62/1000 | Loss: 0.00022808
Iteration 63/1000 | Loss: 0.00009031
Iteration 64/1000 | Loss: 0.00005124
Iteration 65/1000 | Loss: 0.00007977
Iteration 66/1000 | Loss: 0.00018639
Iteration 67/1000 | Loss: 0.00011043
Iteration 68/1000 | Loss: 0.00016649
Iteration 69/1000 | Loss: 0.00018250
Iteration 70/1000 | Loss: 0.00015035
Iteration 71/1000 | Loss: 0.00022835
Iteration 72/1000 | Loss: 0.00005464
Iteration 73/1000 | Loss: 0.00032522
Iteration 74/1000 | Loss: 0.00007883
Iteration 75/1000 | Loss: 0.00006226
Iteration 76/1000 | Loss: 0.00010395
Iteration 77/1000 | Loss: 0.00007966
Iteration 78/1000 | Loss: 0.00011145
Iteration 79/1000 | Loss: 0.00009010
Iteration 80/1000 | Loss: 0.00005339
Iteration 81/1000 | Loss: 0.00005980
Iteration 82/1000 | Loss: 0.00007488
Iteration 83/1000 | Loss: 0.00004280
Iteration 84/1000 | Loss: 0.00008902
Iteration 85/1000 | Loss: 0.00008610
Iteration 86/1000 | Loss: 0.00023595
Iteration 87/1000 | Loss: 0.00020122
Iteration 88/1000 | Loss: 0.00008189
Iteration 89/1000 | Loss: 0.00005911
Iteration 90/1000 | Loss: 0.00009294
Iteration 91/1000 | Loss: 0.00007440
Iteration 92/1000 | Loss: 0.00009034
Iteration 93/1000 | Loss: 0.00004960
Iteration 94/1000 | Loss: 0.00004955
Iteration 95/1000 | Loss: 0.00004179
Iteration 96/1000 | Loss: 0.00003789
Iteration 97/1000 | Loss: 0.00004118
Iteration 98/1000 | Loss: 0.00017208
Iteration 99/1000 | Loss: 0.00005614
Iteration 100/1000 | Loss: 0.00005077
Iteration 101/1000 | Loss: 0.00005385
Iteration 102/1000 | Loss: 0.00006196
Iteration 103/1000 | Loss: 0.00005261
Iteration 104/1000 | Loss: 0.00006516
Iteration 105/1000 | Loss: 0.00004908
Iteration 106/1000 | Loss: 0.00004756
Iteration 107/1000 | Loss: 0.00003515
Iteration 108/1000 | Loss: 0.00003480
Iteration 109/1000 | Loss: 0.00005769
Iteration 110/1000 | Loss: 0.00004983
Iteration 111/1000 | Loss: 0.00003983
Iteration 112/1000 | Loss: 0.00003891
Iteration 113/1000 | Loss: 0.00003598
Iteration 114/1000 | Loss: 0.00003658
Iteration 115/1000 | Loss: 0.00003610
Iteration 116/1000 | Loss: 0.00003596
Iteration 117/1000 | Loss: 0.00003336
Iteration 118/1000 | Loss: 0.00003153
Iteration 119/1000 | Loss: 0.00003608
Iteration 120/1000 | Loss: 0.00005505
Iteration 121/1000 | Loss: 0.00004118
Iteration 122/1000 | Loss: 0.00005015
Iteration 123/1000 | Loss: 0.00002858
Iteration 124/1000 | Loss: 0.00003316
Iteration 125/1000 | Loss: 0.00002007
Iteration 126/1000 | Loss: 0.00002778
Iteration 127/1000 | Loss: 0.00002505
Iteration 128/1000 | Loss: 0.00002852
Iteration 129/1000 | Loss: 0.00002983
Iteration 130/1000 | Loss: 0.00002535
Iteration 131/1000 | Loss: 0.00002435
Iteration 132/1000 | Loss: 0.00002572
Iteration 133/1000 | Loss: 0.00002472
Iteration 134/1000 | Loss: 0.00002565
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00001917
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001770
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001644
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001624
Iteration 148/1000 | Loss: 0.00001622
Iteration 149/1000 | Loss: 0.00001621
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00001621
Iteration 152/1000 | Loss: 0.00001621
Iteration 153/1000 | Loss: 0.00001621
Iteration 154/1000 | Loss: 0.00001621
Iteration 155/1000 | Loss: 0.00001621
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001617
Iteration 174/1000 | Loss: 0.00001617
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001616
Iteration 179/1000 | Loss: 0.00001616
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001616
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001615
Iteration 192/1000 | Loss: 0.00001615
Iteration 193/1000 | Loss: 0.00001615
Iteration 194/1000 | Loss: 0.00001615
Iteration 195/1000 | Loss: 0.00001615
Iteration 196/1000 | Loss: 0.00001615
Iteration 197/1000 | Loss: 0.00001615
Iteration 198/1000 | Loss: 0.00001615
Iteration 199/1000 | Loss: 0.00001615
Iteration 200/1000 | Loss: 0.00001615
Iteration 201/1000 | Loss: 0.00001615
Iteration 202/1000 | Loss: 0.00001615
Iteration 203/1000 | Loss: 0.00001615
Iteration 204/1000 | Loss: 0.00001615
Iteration 205/1000 | Loss: 0.00001615
Iteration 206/1000 | Loss: 0.00001615
Iteration 207/1000 | Loss: 0.00001615
Iteration 208/1000 | Loss: 0.00001615
Iteration 209/1000 | Loss: 0.00001615
Iteration 210/1000 | Loss: 0.00001615
Iteration 211/1000 | Loss: 0.00001615
Iteration 212/1000 | Loss: 0.00001615
Iteration 213/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.6147048881975934e-05, 1.6147048881975934e-05, 1.6147048881975934e-05, 1.6147048881975934e-05, 1.6147048881975934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6147048881975934e-05

Optimization complete. Final v2v error: 3.385727643966675 mm

Highest mean error: 4.176045894622803 mm for frame 184

Lowest mean error: 3.211181640625 mm for frame 143

Saving results

Total time: 289.330593585968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457598
Iteration 2/25 | Loss: 0.00086478
Iteration 3/25 | Loss: 0.00074826
Iteration 4/25 | Loss: 0.00073200
Iteration 5/25 | Loss: 0.00072761
Iteration 6/25 | Loss: 0.00072586
Iteration 7/25 | Loss: 0.00072518
Iteration 8/25 | Loss: 0.00072503
Iteration 9/25 | Loss: 0.00072503
Iteration 10/25 | Loss: 0.00072503
Iteration 11/25 | Loss: 0.00072503
Iteration 12/25 | Loss: 0.00072503
Iteration 13/25 | Loss: 0.00072503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007250302587635815, 0.0007250302587635815, 0.0007250302587635815, 0.0007250302587635815, 0.0007250302587635815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007250302587635815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50376487
Iteration 2/25 | Loss: 0.00038217
Iteration 3/25 | Loss: 0.00038216
Iteration 4/25 | Loss: 0.00038216
Iteration 5/25 | Loss: 0.00038216
Iteration 6/25 | Loss: 0.00038216
Iteration 7/25 | Loss: 0.00038216
Iteration 8/25 | Loss: 0.00038216
Iteration 9/25 | Loss: 0.00038215
Iteration 10/25 | Loss: 0.00038215
Iteration 11/25 | Loss: 0.00038215
Iteration 12/25 | Loss: 0.00038215
Iteration 13/25 | Loss: 0.00038215
Iteration 14/25 | Loss: 0.00038215
Iteration 15/25 | Loss: 0.00038215
Iteration 16/25 | Loss: 0.00038215
Iteration 17/25 | Loss: 0.00038215
Iteration 18/25 | Loss: 0.00038215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00038215427775867283, 0.00038215427775867283, 0.00038215427775867283, 0.00038215427775867283, 0.00038215427775867283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038215427775867283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038215
Iteration 2/1000 | Loss: 0.00002762
Iteration 3/1000 | Loss: 0.00001625
Iteration 4/1000 | Loss: 0.00001358
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001178
Iteration 8/1000 | Loss: 0.00001158
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001150
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001139
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001127
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001108
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001108
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001105
Iteration 181/1000 | Loss: 0.00001105
Iteration 182/1000 | Loss: 0.00001105
Iteration 183/1000 | Loss: 0.00001105
Iteration 184/1000 | Loss: 0.00001105
Iteration 185/1000 | Loss: 0.00001105
Iteration 186/1000 | Loss: 0.00001105
Iteration 187/1000 | Loss: 0.00001105
Iteration 188/1000 | Loss: 0.00001105
Iteration 189/1000 | Loss: 0.00001105
Iteration 190/1000 | Loss: 0.00001105
Iteration 191/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.1052639820263721e-05, 1.1052639820263721e-05, 1.1052639820263721e-05, 1.1052639820263721e-05, 1.1052639820263721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1052639820263721e-05

Optimization complete. Final v2v error: 2.719775676727295 mm

Highest mean error: 3.317476511001587 mm for frame 59

Lowest mean error: 2.464879035949707 mm for frame 97

Saving results

Total time: 37.72413873672485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055416
Iteration 2/25 | Loss: 0.01055416
Iteration 3/25 | Loss: 0.00484848
Iteration 4/25 | Loss: 0.00180760
Iteration 5/25 | Loss: 0.00142114
Iteration 6/25 | Loss: 0.00138099
Iteration 7/25 | Loss: 0.00127216
Iteration 8/25 | Loss: 0.00111544
Iteration 9/25 | Loss: 0.00102737
Iteration 10/25 | Loss: 0.00094902
Iteration 11/25 | Loss: 0.00091116
Iteration 12/25 | Loss: 0.00086455
Iteration 13/25 | Loss: 0.00084403
Iteration 14/25 | Loss: 0.00082516
Iteration 15/25 | Loss: 0.00081545
Iteration 16/25 | Loss: 0.00081343
Iteration 17/25 | Loss: 0.00081427
Iteration 18/25 | Loss: 0.00080896
Iteration 19/25 | Loss: 0.00080522
Iteration 20/25 | Loss: 0.00080407
Iteration 21/25 | Loss: 0.00080154
Iteration 22/25 | Loss: 0.00079775
Iteration 23/25 | Loss: 0.00079372
Iteration 24/25 | Loss: 0.00079313
Iteration 25/25 | Loss: 0.00079431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49418390
Iteration 2/25 | Loss: 0.00059823
Iteration 3/25 | Loss: 0.00054300
Iteration 4/25 | Loss: 0.00054300
Iteration 5/25 | Loss: 0.00054300
Iteration 6/25 | Loss: 0.00054300
Iteration 7/25 | Loss: 0.00054300
Iteration 8/25 | Loss: 0.00054300
Iteration 9/25 | Loss: 0.00054300
Iteration 10/25 | Loss: 0.00054300
Iteration 11/25 | Loss: 0.00054300
Iteration 12/25 | Loss: 0.00054300
Iteration 13/25 | Loss: 0.00054300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005430030287243426, 0.0005430030287243426, 0.0005430030287243426, 0.0005430030287243426, 0.0005430030287243426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005430030287243426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054300
Iteration 2/1000 | Loss: 0.00005496
Iteration 3/1000 | Loss: 0.00011047
Iteration 4/1000 | Loss: 0.00004273
Iteration 5/1000 | Loss: 0.00003591
Iteration 6/1000 | Loss: 0.00063820
Iteration 7/1000 | Loss: 0.00037777
Iteration 8/1000 | Loss: 0.00052585
Iteration 9/1000 | Loss: 0.00015526
Iteration 10/1000 | Loss: 0.00009055
Iteration 11/1000 | Loss: 0.00008781
Iteration 12/1000 | Loss: 0.00003976
Iteration 13/1000 | Loss: 0.00004104
Iteration 14/1000 | Loss: 0.00004821
Iteration 15/1000 | Loss: 0.00002668
Iteration 16/1000 | Loss: 0.00003197
Iteration 17/1000 | Loss: 0.00007617
Iteration 18/1000 | Loss: 0.00004668
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00003730
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00003307
Iteration 25/1000 | Loss: 0.00008012
Iteration 26/1000 | Loss: 0.00003428
Iteration 27/1000 | Loss: 0.00003688
Iteration 28/1000 | Loss: 0.00002004
Iteration 29/1000 | Loss: 0.00001890
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001884
Iteration 40/1000 | Loss: 0.00001884
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001883
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00001882
Iteration 51/1000 | Loss: 0.00001882
Iteration 52/1000 | Loss: 0.00001882
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001881
Iteration 57/1000 | Loss: 0.00001881
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001880
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00003732
Iteration 64/1000 | Loss: 0.00001979
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001873
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001871
Iteration 77/1000 | Loss: 0.00001871
Iteration 78/1000 | Loss: 0.00001897
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001869
Iteration 85/1000 | Loss: 0.00001869
Iteration 86/1000 | Loss: 0.00001869
Iteration 87/1000 | Loss: 0.00001869
Iteration 88/1000 | Loss: 0.00001869
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00003617
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001866
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001866
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001866
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001865
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001864
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00002500
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001863
Iteration 114/1000 | Loss: 0.00001863
Iteration 115/1000 | Loss: 0.00001863
Iteration 116/1000 | Loss: 0.00001863
Iteration 117/1000 | Loss: 0.00001863
Iteration 118/1000 | Loss: 0.00001863
Iteration 119/1000 | Loss: 0.00001863
Iteration 120/1000 | Loss: 0.00001863
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001862
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001862
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001862
Iteration 132/1000 | Loss: 0.00001862
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001861
Iteration 135/1000 | Loss: 0.00001861
Iteration 136/1000 | Loss: 0.00001861
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001861
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001861
Iteration 148/1000 | Loss: 0.00001861
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8614246073411778e-05, 1.8614246073411778e-05, 1.8614246073411778e-05, 1.8614246073411778e-05, 1.8614246073411778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8614246073411778e-05

Optimization complete. Final v2v error: 3.6076500415802 mm

Highest mean error: 4.034460067749023 mm for frame 72

Lowest mean error: 3.150897741317749 mm for frame 45

Saving results

Total time: 114.2688717842102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082621
Iteration 2/25 | Loss: 0.00188918
Iteration 3/25 | Loss: 0.00141052
Iteration 4/25 | Loss: 0.00129235
Iteration 5/25 | Loss: 0.00115138
Iteration 6/25 | Loss: 0.00098915
Iteration 7/25 | Loss: 0.00097140
Iteration 8/25 | Loss: 0.00093555
Iteration 9/25 | Loss: 0.00098455
Iteration 10/25 | Loss: 0.00090892
Iteration 11/25 | Loss: 0.00090897
Iteration 12/25 | Loss: 0.00087945
Iteration 13/25 | Loss: 0.00089423
Iteration 14/25 | Loss: 0.00086589
Iteration 15/25 | Loss: 0.00086475
Iteration 16/25 | Loss: 0.00085655
Iteration 17/25 | Loss: 0.00086902
Iteration 18/25 | Loss: 0.00085226
Iteration 19/25 | Loss: 0.00083912
Iteration 20/25 | Loss: 0.00084135
Iteration 21/25 | Loss: 0.00083663
Iteration 22/25 | Loss: 0.00083126
Iteration 23/25 | Loss: 0.00082987
Iteration 24/25 | Loss: 0.00082886
Iteration 25/25 | Loss: 0.00083755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50750470
Iteration 2/25 | Loss: 0.00154759
Iteration 3/25 | Loss: 0.00154759
Iteration 4/25 | Loss: 0.00148026
Iteration 5/25 | Loss: 0.00148026
Iteration 6/25 | Loss: 0.00148026
Iteration 7/25 | Loss: 0.00148026
Iteration 8/25 | Loss: 0.00148026
Iteration 9/25 | Loss: 0.00148026
Iteration 10/25 | Loss: 0.00148026
Iteration 11/25 | Loss: 0.00148026
Iteration 12/25 | Loss: 0.00148026
Iteration 13/25 | Loss: 0.00148026
Iteration 14/25 | Loss: 0.00148026
Iteration 15/25 | Loss: 0.00148026
Iteration 16/25 | Loss: 0.00148026
Iteration 17/25 | Loss: 0.00148026
Iteration 18/25 | Loss: 0.00148026
Iteration 19/25 | Loss: 0.00148026
Iteration 20/25 | Loss: 0.00148026
Iteration 21/25 | Loss: 0.00148026
Iteration 22/25 | Loss: 0.00148026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014802617952227592, 0.0014802617952227592, 0.0014802617952227592, 0.0014802617952227592, 0.0014802617952227592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014802617952227592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148026
Iteration 2/1000 | Loss: 0.00171061
Iteration 3/1000 | Loss: 0.00035982
Iteration 4/1000 | Loss: 0.00022428
Iteration 5/1000 | Loss: 0.00006071
Iteration 6/1000 | Loss: 0.00012624
Iteration 7/1000 | Loss: 0.00036203
Iteration 8/1000 | Loss: 0.00018765
Iteration 9/1000 | Loss: 0.00008008
Iteration 10/1000 | Loss: 0.00004931
Iteration 11/1000 | Loss: 0.00022690
Iteration 12/1000 | Loss: 0.00015538
Iteration 13/1000 | Loss: 0.00021749
Iteration 14/1000 | Loss: 0.00014599
Iteration 15/1000 | Loss: 0.00005169
Iteration 16/1000 | Loss: 0.00075010
Iteration 17/1000 | Loss: 0.00068617
Iteration 18/1000 | Loss: 0.00006484
Iteration 19/1000 | Loss: 0.00021043
Iteration 20/1000 | Loss: 0.00040329
Iteration 21/1000 | Loss: 0.00081362
Iteration 22/1000 | Loss: 0.00133170
Iteration 23/1000 | Loss: 0.00091197
Iteration 24/1000 | Loss: 0.00071276
Iteration 25/1000 | Loss: 0.00060979
Iteration 26/1000 | Loss: 0.00059232
Iteration 27/1000 | Loss: 0.00043728
Iteration 28/1000 | Loss: 0.00005250
Iteration 29/1000 | Loss: 0.00015278
Iteration 30/1000 | Loss: 0.00016204
Iteration 31/1000 | Loss: 0.00024619
Iteration 32/1000 | Loss: 0.00003817
Iteration 33/1000 | Loss: 0.00008585
Iteration 34/1000 | Loss: 0.00019001
Iteration 35/1000 | Loss: 0.00003587
Iteration 36/1000 | Loss: 0.00002807
Iteration 37/1000 | Loss: 0.00002613
Iteration 38/1000 | Loss: 0.00037791
Iteration 39/1000 | Loss: 0.00003743
Iteration 40/1000 | Loss: 0.00002483
Iteration 41/1000 | Loss: 0.00002316
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00001958
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001666
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001660
Iteration 64/1000 | Loss: 0.00001660
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001655
Iteration 104/1000 | Loss: 0.00001655
Iteration 105/1000 | Loss: 0.00001655
Iteration 106/1000 | Loss: 0.00001655
Iteration 107/1000 | Loss: 0.00001655
Iteration 108/1000 | Loss: 0.00001655
Iteration 109/1000 | Loss: 0.00001655
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001655
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001655
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001654
Iteration 118/1000 | Loss: 0.00001654
Iteration 119/1000 | Loss: 0.00001654
Iteration 120/1000 | Loss: 0.00001654
Iteration 121/1000 | Loss: 0.00001654
Iteration 122/1000 | Loss: 0.00001654
Iteration 123/1000 | Loss: 0.00001653
Iteration 124/1000 | Loss: 0.00001653
Iteration 125/1000 | Loss: 0.00001653
Iteration 126/1000 | Loss: 0.00001653
Iteration 127/1000 | Loss: 0.00001653
Iteration 128/1000 | Loss: 0.00001653
Iteration 129/1000 | Loss: 0.00001653
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001652
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001651
Iteration 145/1000 | Loss: 0.00001651
Iteration 146/1000 | Loss: 0.00001651
Iteration 147/1000 | Loss: 0.00001651
Iteration 148/1000 | Loss: 0.00001651
Iteration 149/1000 | Loss: 0.00001651
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001648
Iteration 159/1000 | Loss: 0.00001648
Iteration 160/1000 | Loss: 0.00001648
Iteration 161/1000 | Loss: 0.00001648
Iteration 162/1000 | Loss: 0.00001648
Iteration 163/1000 | Loss: 0.00001648
Iteration 164/1000 | Loss: 0.00001647
Iteration 165/1000 | Loss: 0.00001647
Iteration 166/1000 | Loss: 0.00001647
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001646
Iteration 171/1000 | Loss: 0.00001646
Iteration 172/1000 | Loss: 0.00001646
Iteration 173/1000 | Loss: 0.00001645
Iteration 174/1000 | Loss: 0.00001645
Iteration 175/1000 | Loss: 0.00001645
Iteration 176/1000 | Loss: 0.00001645
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001643
Iteration 182/1000 | Loss: 0.00001643
Iteration 183/1000 | Loss: 0.00001642
Iteration 184/1000 | Loss: 0.00001642
Iteration 185/1000 | Loss: 0.00001642
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001642
Iteration 191/1000 | Loss: 0.00001642
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001641
Iteration 197/1000 | Loss: 0.00001641
Iteration 198/1000 | Loss: 0.00001641
Iteration 199/1000 | Loss: 0.00001641
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001640
Iteration 203/1000 | Loss: 0.00001640
Iteration 204/1000 | Loss: 0.00001640
Iteration 205/1000 | Loss: 0.00001640
Iteration 206/1000 | Loss: 0.00001640
Iteration 207/1000 | Loss: 0.00001640
Iteration 208/1000 | Loss: 0.00001640
Iteration 209/1000 | Loss: 0.00001640
Iteration 210/1000 | Loss: 0.00001640
Iteration 211/1000 | Loss: 0.00001640
Iteration 212/1000 | Loss: 0.00001640
Iteration 213/1000 | Loss: 0.00001640
Iteration 214/1000 | Loss: 0.00001640
Iteration 215/1000 | Loss: 0.00001640
Iteration 216/1000 | Loss: 0.00001640
Iteration 217/1000 | Loss: 0.00001640
Iteration 218/1000 | Loss: 0.00001640
Iteration 219/1000 | Loss: 0.00001640
Iteration 220/1000 | Loss: 0.00001640
Iteration 221/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.639653964957688e-05, 1.639653964957688e-05, 1.639653964957688e-05, 1.639653964957688e-05, 1.639653964957688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.639653964957688e-05

Optimization complete. Final v2v error: 3.351794719696045 mm

Highest mean error: 5.571041584014893 mm for frame 68

Lowest mean error: 2.9799551963806152 mm for frame 124

Saving results

Total time: 129.85646748542786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093829
Iteration 2/25 | Loss: 0.01093828
Iteration 3/25 | Loss: 0.01093828
Iteration 4/25 | Loss: 0.01093828
Iteration 5/25 | Loss: 0.01093828
Iteration 6/25 | Loss: 0.01093828
Iteration 7/25 | Loss: 0.01093828
Iteration 8/25 | Loss: 0.01093827
Iteration 9/25 | Loss: 0.01093827
Iteration 10/25 | Loss: 0.01093827
Iteration 11/25 | Loss: 0.01093827
Iteration 12/25 | Loss: 0.01093826
Iteration 13/25 | Loss: 0.01093826
Iteration 14/25 | Loss: 0.01093826
Iteration 15/25 | Loss: 0.01093826
Iteration 16/25 | Loss: 0.01093826
Iteration 17/25 | Loss: 0.01093826
Iteration 18/25 | Loss: 0.01093826
Iteration 19/25 | Loss: 0.01093825
Iteration 20/25 | Loss: 0.01093825
Iteration 21/25 | Loss: 0.01093825
Iteration 22/25 | Loss: 0.01093825
Iteration 23/25 | Loss: 0.01093824
Iteration 24/25 | Loss: 0.01093824
Iteration 25/25 | Loss: 0.01093824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02655268
Iteration 2/25 | Loss: 0.09337205
Iteration 3/25 | Loss: 0.08972959
Iteration 4/25 | Loss: 0.09261193
Iteration 5/25 | Loss: 0.09255288
Iteration 6/25 | Loss: 0.08972565
Iteration 7/25 | Loss: 0.08892024
Iteration 8/25 | Loss: 0.08892197
Iteration 9/25 | Loss: 0.08875818
Iteration 10/25 | Loss: 0.08875816
Iteration 11/25 | Loss: 0.08875816
Iteration 12/25 | Loss: 0.08875814
Iteration 13/25 | Loss: 0.08875814
Iteration 14/25 | Loss: 0.08875814
Iteration 15/25 | Loss: 0.08875814
Iteration 16/25 | Loss: 0.08875814
Iteration 17/25 | Loss: 0.08875813
Iteration 18/25 | Loss: 0.08875813
Iteration 19/25 | Loss: 0.08875813
Iteration 20/25 | Loss: 0.08875813
Iteration 21/25 | Loss: 0.08875813
Iteration 22/25 | Loss: 0.08875814
Iteration 23/25 | Loss: 0.08875814
Iteration 24/25 | Loss: 0.08875814
Iteration 25/25 | Loss: 0.08875813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08875813
Iteration 2/1000 | Loss: 0.00375032
Iteration 3/1000 | Loss: 0.00151965
Iteration 4/1000 | Loss: 0.00442233
Iteration 5/1000 | Loss: 0.00065993
Iteration 6/1000 | Loss: 0.00063193
Iteration 7/1000 | Loss: 0.00016108
Iteration 8/1000 | Loss: 0.00008834
Iteration 9/1000 | Loss: 0.00007953
Iteration 10/1000 | Loss: 0.00007235
Iteration 11/1000 | Loss: 0.00004788
Iteration 12/1000 | Loss: 0.00037340
Iteration 13/1000 | Loss: 0.00003764
Iteration 14/1000 | Loss: 0.00003243
Iteration 15/1000 | Loss: 0.00003762
Iteration 16/1000 | Loss: 0.00002693
Iteration 17/1000 | Loss: 0.00008490
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00003162
Iteration 20/1000 | Loss: 0.00009625
Iteration 21/1000 | Loss: 0.00005461
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00008276
Iteration 24/1000 | Loss: 0.00004630
Iteration 25/1000 | Loss: 0.00005931
Iteration 26/1000 | Loss: 0.00002566
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001908
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00003840
Iteration 32/1000 | Loss: 0.00002863
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00009790
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00005964
Iteration 43/1000 | Loss: 0.00005964
Iteration 44/1000 | Loss: 0.00005964
Iteration 45/1000 | Loss: 0.00005964
Iteration 46/1000 | Loss: 0.00002655
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00003091
Iteration 49/1000 | Loss: 0.00004264
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001401
Iteration 62/1000 | Loss: 0.00001401
Iteration 63/1000 | Loss: 0.00001401
Iteration 64/1000 | Loss: 0.00001401
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001396
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001380
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001378
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001378
Iteration 132/1000 | Loss: 0.00001378
Iteration 133/1000 | Loss: 0.00001378
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001376
Iteration 140/1000 | Loss: 0.00001376
Iteration 141/1000 | Loss: 0.00001376
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001375
Iteration 146/1000 | Loss: 0.00001375
Iteration 147/1000 | Loss: 0.00001375
Iteration 148/1000 | Loss: 0.00001375
Iteration 149/1000 | Loss: 0.00001375
Iteration 150/1000 | Loss: 0.00001375
Iteration 151/1000 | Loss: 0.00001375
Iteration 152/1000 | Loss: 0.00001375
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001374
Iteration 162/1000 | Loss: 0.00001374
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001372
Iteration 169/1000 | Loss: 0.00001372
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001371
Iteration 178/1000 | Loss: 0.00001371
Iteration 179/1000 | Loss: 0.00001371
Iteration 180/1000 | Loss: 0.00001371
Iteration 181/1000 | Loss: 0.00001371
Iteration 182/1000 | Loss: 0.00001371
Iteration 183/1000 | Loss: 0.00001371
Iteration 184/1000 | Loss: 0.00001371
Iteration 185/1000 | Loss: 0.00001371
Iteration 186/1000 | Loss: 0.00001371
Iteration 187/1000 | Loss: 0.00001371
Iteration 188/1000 | Loss: 0.00001371
Iteration 189/1000 | Loss: 0.00001371
Iteration 190/1000 | Loss: 0.00001370
Iteration 191/1000 | Loss: 0.00001370
Iteration 192/1000 | Loss: 0.00001370
Iteration 193/1000 | Loss: 0.00001370
Iteration 194/1000 | Loss: 0.00003323
Iteration 195/1000 | Loss: 0.00002752
Iteration 196/1000 | Loss: 0.00001379
Iteration 197/1000 | Loss: 0.00001589
Iteration 198/1000 | Loss: 0.00001873
Iteration 199/1000 | Loss: 0.00001455
Iteration 200/1000 | Loss: 0.00001595
Iteration 201/1000 | Loss: 0.00001425
Iteration 202/1000 | Loss: 0.00001371
Iteration 203/1000 | Loss: 0.00001371
Iteration 204/1000 | Loss: 0.00001371
Iteration 205/1000 | Loss: 0.00001370
Iteration 206/1000 | Loss: 0.00001370
Iteration 207/1000 | Loss: 0.00001370
Iteration 208/1000 | Loss: 0.00001370
Iteration 209/1000 | Loss: 0.00001370
Iteration 210/1000 | Loss: 0.00001370
Iteration 211/1000 | Loss: 0.00001369
Iteration 212/1000 | Loss: 0.00001369
Iteration 213/1000 | Loss: 0.00001369
Iteration 214/1000 | Loss: 0.00001369
Iteration 215/1000 | Loss: 0.00001369
Iteration 216/1000 | Loss: 0.00001369
Iteration 217/1000 | Loss: 0.00001369
Iteration 218/1000 | Loss: 0.00001369
Iteration 219/1000 | Loss: 0.00001369
Iteration 220/1000 | Loss: 0.00001369
Iteration 221/1000 | Loss: 0.00001369
Iteration 222/1000 | Loss: 0.00001369
Iteration 223/1000 | Loss: 0.00001369
Iteration 224/1000 | Loss: 0.00001369
Iteration 225/1000 | Loss: 0.00001369
Iteration 226/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3691533240489662e-05, 1.3691533240489662e-05, 1.3691533240489662e-05, 1.3691533240489662e-05, 1.3691533240489662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3691533240489662e-05

Optimization complete. Final v2v error: 3.1756410598754883 mm

Highest mean error: 3.800201892852783 mm for frame 61

Lowest mean error: 2.669679880142212 mm for frame 236

Saving results

Total time: 111.47739267349243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392627
Iteration 2/25 | Loss: 0.00094085
Iteration 3/25 | Loss: 0.00078327
Iteration 4/25 | Loss: 0.00076928
Iteration 5/25 | Loss: 0.00076103
Iteration 6/25 | Loss: 0.00075915
Iteration 7/25 | Loss: 0.00075915
Iteration 8/25 | Loss: 0.00075915
Iteration 9/25 | Loss: 0.00075915
Iteration 10/25 | Loss: 0.00075915
Iteration 11/25 | Loss: 0.00075915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000759146932978183, 0.000759146932978183, 0.000759146932978183, 0.000759146932978183, 0.000759146932978183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000759146932978183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75650740
Iteration 2/25 | Loss: 0.00055954
Iteration 3/25 | Loss: 0.00055954
Iteration 4/25 | Loss: 0.00055954
Iteration 5/25 | Loss: 0.00055954
Iteration 6/25 | Loss: 0.00055954
Iteration 7/25 | Loss: 0.00055953
Iteration 8/25 | Loss: 0.00055953
Iteration 9/25 | Loss: 0.00055953
Iteration 10/25 | Loss: 0.00055953
Iteration 11/25 | Loss: 0.00055953
Iteration 12/25 | Loss: 0.00055953
Iteration 13/25 | Loss: 0.00055953
Iteration 14/25 | Loss: 0.00055953
Iteration 15/25 | Loss: 0.00055953
Iteration 16/25 | Loss: 0.00055953
Iteration 17/25 | Loss: 0.00055953
Iteration 18/25 | Loss: 0.00055953
Iteration 19/25 | Loss: 0.00055953
Iteration 20/25 | Loss: 0.00055953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005595335969701409, 0.0005595335969701409, 0.0005595335969701409, 0.0005595335969701409, 0.0005595335969701409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005595335969701409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055953
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00001825
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001480
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001304
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001293
Iteration 64/1000 | Loss: 0.00001293
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001292
Iteration 69/1000 | Loss: 0.00001292
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001292
Iteration 74/1000 | Loss: 0.00001292
Iteration 75/1000 | Loss: 0.00001292
Iteration 76/1000 | Loss: 0.00001292
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001292
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001292
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001292
Iteration 86/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.2921492270834278e-05, 1.2921492270834278e-05, 1.2921492270834278e-05, 1.2921492270834278e-05, 1.2921492270834278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2921492270834278e-05

Optimization complete. Final v2v error: 3.0602688789367676 mm

Highest mean error: 3.316187858581543 mm for frame 62

Lowest mean error: 2.8223953247070312 mm for frame 265

Saving results

Total time: 34.46372032165527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101328
Iteration 2/25 | Loss: 0.01101328
Iteration 3/25 | Loss: 0.00149563
Iteration 4/25 | Loss: 0.00094500
Iteration 5/25 | Loss: 0.00081703
Iteration 6/25 | Loss: 0.00076925
Iteration 7/25 | Loss: 0.00075558
Iteration 8/25 | Loss: 0.00075108
Iteration 9/25 | Loss: 0.00075096
Iteration 10/25 | Loss: 0.00074936
Iteration 11/25 | Loss: 0.00074877
Iteration 12/25 | Loss: 0.00074854
Iteration 13/25 | Loss: 0.00074846
Iteration 14/25 | Loss: 0.00074842
Iteration 15/25 | Loss: 0.00074841
Iteration 16/25 | Loss: 0.00074834
Iteration 17/25 | Loss: 0.00074823
Iteration 18/25 | Loss: 0.00074823
Iteration 19/25 | Loss: 0.00074823
Iteration 20/25 | Loss: 0.00074823
Iteration 21/25 | Loss: 0.00074823
Iteration 22/25 | Loss: 0.00074823
Iteration 23/25 | Loss: 0.00074823
Iteration 24/25 | Loss: 0.00074823
Iteration 25/25 | Loss: 0.00074822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.57485104
Iteration 2/25 | Loss: 0.00047310
Iteration 3/25 | Loss: 0.00047310
Iteration 4/25 | Loss: 0.00047310
Iteration 5/25 | Loss: 0.00047310
Iteration 6/25 | Loss: 0.00047310
Iteration 7/25 | Loss: 0.00047310
Iteration 8/25 | Loss: 0.00047310
Iteration 9/25 | Loss: 0.00047310
Iteration 10/25 | Loss: 0.00047310
Iteration 11/25 | Loss: 0.00047310
Iteration 12/25 | Loss: 0.00047310
Iteration 13/25 | Loss: 0.00047310
Iteration 14/25 | Loss: 0.00047310
Iteration 15/25 | Loss: 0.00047310
Iteration 16/25 | Loss: 0.00047310
Iteration 17/25 | Loss: 0.00047310
Iteration 18/25 | Loss: 0.00047310
Iteration 19/25 | Loss: 0.00047310
Iteration 20/25 | Loss: 0.00047310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004730993532575667, 0.0004730993532575667, 0.0004730993532575667, 0.0004730993532575667, 0.0004730993532575667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004730993532575667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047310
Iteration 2/1000 | Loss: 0.00009701
Iteration 3/1000 | Loss: 0.00009479
Iteration 4/1000 | Loss: 0.00018757
Iteration 5/1000 | Loss: 0.00010786
Iteration 6/1000 | Loss: 0.00001469
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00016070
Iteration 9/1000 | Loss: 0.00027401
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00017728
Iteration 12/1000 | Loss: 0.00012737
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00009763
Iteration 15/1000 | Loss: 0.00010065
Iteration 16/1000 | Loss: 0.00008432
Iteration 17/1000 | Loss: 0.00010423
Iteration 18/1000 | Loss: 0.00012523
Iteration 19/1000 | Loss: 0.00012589
Iteration 20/1000 | Loss: 0.00029405
Iteration 21/1000 | Loss: 0.00004574
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00012214
Iteration 25/1000 | Loss: 0.00002127
Iteration 26/1000 | Loss: 0.00003864
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001289
Iteration 33/1000 | Loss: 0.00010422
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00013734
Iteration 41/1000 | Loss: 0.00013734
Iteration 42/1000 | Loss: 0.00007456
Iteration 43/1000 | Loss: 0.00003664
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00006717
Iteration 46/1000 | Loss: 0.00001207
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001179
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001171
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001170
Iteration 80/1000 | Loss: 0.00001170
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001168
Iteration 106/1000 | Loss: 0.00001168
Iteration 107/1000 | Loss: 0.00001168
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001167
Iteration 117/1000 | Loss: 0.00001167
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001166
Iteration 144/1000 | Loss: 0.00001166
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001165
Iteration 154/1000 | Loss: 0.00001165
Iteration 155/1000 | Loss: 0.00001165
Iteration 156/1000 | Loss: 0.00001165
Iteration 157/1000 | Loss: 0.00001165
Iteration 158/1000 | Loss: 0.00001165
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001165
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001165
Iteration 167/1000 | Loss: 0.00001165
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001165
Iteration 171/1000 | Loss: 0.00001165
Iteration 172/1000 | Loss: 0.00001165
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1648889994830824e-05, 1.1648889994830824e-05, 1.1648889994830824e-05, 1.1648889994830824e-05, 1.1648889994830824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1648889994830824e-05

Optimization complete. Final v2v error: 2.897477626800537 mm

Highest mean error: 3.7995402812957764 mm for frame 95

Lowest mean error: 2.6669580936431885 mm for frame 153

Saving results

Total time: 99.54688239097595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428317
Iteration 2/25 | Loss: 0.00095925
Iteration 3/25 | Loss: 0.00079268
Iteration 4/25 | Loss: 0.00077223
Iteration 5/25 | Loss: 0.00076706
Iteration 6/25 | Loss: 0.00076493
Iteration 7/25 | Loss: 0.00076478
Iteration 8/25 | Loss: 0.00076478
Iteration 9/25 | Loss: 0.00076478
Iteration 10/25 | Loss: 0.00076478
Iteration 11/25 | Loss: 0.00076478
Iteration 12/25 | Loss: 0.00076478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007647840538993478, 0.0007647840538993478, 0.0007647840538993478, 0.0007647840538993478, 0.0007647840538993478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007647840538993478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48959851
Iteration 2/25 | Loss: 0.00055945
Iteration 3/25 | Loss: 0.00055945
Iteration 4/25 | Loss: 0.00055944
Iteration 5/25 | Loss: 0.00055944
Iteration 6/25 | Loss: 0.00055944
Iteration 7/25 | Loss: 0.00055944
Iteration 8/25 | Loss: 0.00055944
Iteration 9/25 | Loss: 0.00055944
Iteration 10/25 | Loss: 0.00055944
Iteration 11/25 | Loss: 0.00055944
Iteration 12/25 | Loss: 0.00055944
Iteration 13/25 | Loss: 0.00055944
Iteration 14/25 | Loss: 0.00055944
Iteration 15/25 | Loss: 0.00055944
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005594431422650814, 0.0005594431422650814, 0.0005594431422650814, 0.0005594431422650814, 0.0005594431422650814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005594431422650814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055944
Iteration 2/1000 | Loss: 0.00002405
Iteration 3/1000 | Loss: 0.00001659
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001366
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001355
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001354
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001348
Iteration 87/1000 | Loss: 0.00001348
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001348
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001345
Iteration 114/1000 | Loss: 0.00001345
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001343
Iteration 121/1000 | Loss: 0.00001343
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3426325494947378e-05, 1.3426325494947378e-05, 1.3426325494947378e-05, 1.3426325494947378e-05, 1.3426325494947378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3426325494947378e-05

Optimization complete. Final v2v error: 3.086749315261841 mm

Highest mean error: 3.6156792640686035 mm for frame 8

Lowest mean error: 2.8773374557495117 mm for frame 20

Saving results

Total time: 33.22039294242859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673388
Iteration 2/25 | Loss: 0.00102266
Iteration 3/25 | Loss: 0.00087476
Iteration 4/25 | Loss: 0.00083232
Iteration 5/25 | Loss: 0.00081841
Iteration 6/25 | Loss: 0.00081630
Iteration 7/25 | Loss: 0.00081550
Iteration 8/25 | Loss: 0.00081550
Iteration 9/25 | Loss: 0.00081550
Iteration 10/25 | Loss: 0.00081550
Iteration 11/25 | Loss: 0.00081550
Iteration 12/25 | Loss: 0.00081550
Iteration 13/25 | Loss: 0.00081550
Iteration 14/25 | Loss: 0.00081550
Iteration 15/25 | Loss: 0.00081550
Iteration 16/25 | Loss: 0.00081550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008155003888532519, 0.0008155003888532519, 0.0008155003888532519, 0.0008155003888532519, 0.0008155003888532519]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008155003888532519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49744022
Iteration 2/25 | Loss: 0.00056449
Iteration 3/25 | Loss: 0.00056448
Iteration 4/25 | Loss: 0.00056448
Iteration 5/25 | Loss: 0.00056448
Iteration 6/25 | Loss: 0.00056448
Iteration 7/25 | Loss: 0.00056448
Iteration 8/25 | Loss: 0.00056448
Iteration 9/25 | Loss: 0.00056448
Iteration 10/25 | Loss: 0.00056448
Iteration 11/25 | Loss: 0.00056448
Iteration 12/25 | Loss: 0.00056448
Iteration 13/25 | Loss: 0.00056448
Iteration 14/25 | Loss: 0.00056448
Iteration 15/25 | Loss: 0.00056448
Iteration 16/25 | Loss: 0.00056448
Iteration 17/25 | Loss: 0.00056448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005644802586175501, 0.0005644802586175501, 0.0005644802586175501, 0.0005644802586175501, 0.0005644802586175501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005644802586175501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056448
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00003570
Iteration 4/1000 | Loss: 0.00003272
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00003043
Iteration 7/1000 | Loss: 0.00002979
Iteration 8/1000 | Loss: 0.00002925
Iteration 9/1000 | Loss: 0.00002879
Iteration 10/1000 | Loss: 0.00002850
Iteration 11/1000 | Loss: 0.00002825
Iteration 12/1000 | Loss: 0.00002811
Iteration 13/1000 | Loss: 0.00002800
Iteration 14/1000 | Loss: 0.00002784
Iteration 15/1000 | Loss: 0.00002784
Iteration 16/1000 | Loss: 0.00002783
Iteration 17/1000 | Loss: 0.00002783
Iteration 18/1000 | Loss: 0.00002783
Iteration 19/1000 | Loss: 0.00002782
Iteration 20/1000 | Loss: 0.00002782
Iteration 21/1000 | Loss: 0.00002781
Iteration 22/1000 | Loss: 0.00002781
Iteration 23/1000 | Loss: 0.00002781
Iteration 24/1000 | Loss: 0.00002781
Iteration 25/1000 | Loss: 0.00002781
Iteration 26/1000 | Loss: 0.00002781
Iteration 27/1000 | Loss: 0.00002781
Iteration 28/1000 | Loss: 0.00002780
Iteration 29/1000 | Loss: 0.00002780
Iteration 30/1000 | Loss: 0.00002780
Iteration 31/1000 | Loss: 0.00002780
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002778
Iteration 35/1000 | Loss: 0.00002778
Iteration 36/1000 | Loss: 0.00002778
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00002777
Iteration 39/1000 | Loss: 0.00002777
Iteration 40/1000 | Loss: 0.00002777
Iteration 41/1000 | Loss: 0.00002776
Iteration 42/1000 | Loss: 0.00002776
Iteration 43/1000 | Loss: 0.00002775
Iteration 44/1000 | Loss: 0.00002775
Iteration 45/1000 | Loss: 0.00002775
Iteration 46/1000 | Loss: 0.00002775
Iteration 47/1000 | Loss: 0.00002775
Iteration 48/1000 | Loss: 0.00002775
Iteration 49/1000 | Loss: 0.00002775
Iteration 50/1000 | Loss: 0.00002775
Iteration 51/1000 | Loss: 0.00002775
Iteration 52/1000 | Loss: 0.00002775
Iteration 53/1000 | Loss: 0.00002775
Iteration 54/1000 | Loss: 0.00002775
Iteration 55/1000 | Loss: 0.00002774
Iteration 56/1000 | Loss: 0.00002774
Iteration 57/1000 | Loss: 0.00002774
Iteration 58/1000 | Loss: 0.00002773
Iteration 59/1000 | Loss: 0.00002773
Iteration 60/1000 | Loss: 0.00002773
Iteration 61/1000 | Loss: 0.00002773
Iteration 62/1000 | Loss: 0.00002772
Iteration 63/1000 | Loss: 0.00002772
Iteration 64/1000 | Loss: 0.00002772
Iteration 65/1000 | Loss: 0.00002771
Iteration 66/1000 | Loss: 0.00002771
Iteration 67/1000 | Loss: 0.00002771
Iteration 68/1000 | Loss: 0.00002771
Iteration 69/1000 | Loss: 0.00002771
Iteration 70/1000 | Loss: 0.00002770
Iteration 71/1000 | Loss: 0.00002770
Iteration 72/1000 | Loss: 0.00002769
Iteration 73/1000 | Loss: 0.00002769
Iteration 74/1000 | Loss: 0.00002769
Iteration 75/1000 | Loss: 0.00002769
Iteration 76/1000 | Loss: 0.00002769
Iteration 77/1000 | Loss: 0.00002768
Iteration 78/1000 | Loss: 0.00002768
Iteration 79/1000 | Loss: 0.00002768
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002765
Iteration 82/1000 | Loss: 0.00002765
Iteration 83/1000 | Loss: 0.00002765
Iteration 84/1000 | Loss: 0.00002764
Iteration 85/1000 | Loss: 0.00002764
Iteration 86/1000 | Loss: 0.00002764
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00002763
Iteration 89/1000 | Loss: 0.00002763
Iteration 90/1000 | Loss: 0.00002762
Iteration 91/1000 | Loss: 0.00002762
Iteration 92/1000 | Loss: 0.00002762
Iteration 93/1000 | Loss: 0.00002762
Iteration 94/1000 | Loss: 0.00002762
Iteration 95/1000 | Loss: 0.00002761
Iteration 96/1000 | Loss: 0.00002761
Iteration 97/1000 | Loss: 0.00002761
Iteration 98/1000 | Loss: 0.00002761
Iteration 99/1000 | Loss: 0.00002761
Iteration 100/1000 | Loss: 0.00002761
Iteration 101/1000 | Loss: 0.00002761
Iteration 102/1000 | Loss: 0.00002761
Iteration 103/1000 | Loss: 0.00002761
Iteration 104/1000 | Loss: 0.00002761
Iteration 105/1000 | Loss: 0.00002761
Iteration 106/1000 | Loss: 0.00002761
Iteration 107/1000 | Loss: 0.00002760
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002760
Iteration 110/1000 | Loss: 0.00002760
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002760
Iteration 114/1000 | Loss: 0.00002760
Iteration 115/1000 | Loss: 0.00002760
Iteration 116/1000 | Loss: 0.00002760
Iteration 117/1000 | Loss: 0.00002760
Iteration 118/1000 | Loss: 0.00002760
Iteration 119/1000 | Loss: 0.00002760
Iteration 120/1000 | Loss: 0.00002760
Iteration 121/1000 | Loss: 0.00002760
Iteration 122/1000 | Loss: 0.00002760
Iteration 123/1000 | Loss: 0.00002760
Iteration 124/1000 | Loss: 0.00002760
Iteration 125/1000 | Loss: 0.00002760
Iteration 126/1000 | Loss: 0.00002760
Iteration 127/1000 | Loss: 0.00002760
Iteration 128/1000 | Loss: 0.00002760
Iteration 129/1000 | Loss: 0.00002760
Iteration 130/1000 | Loss: 0.00002760
Iteration 131/1000 | Loss: 0.00002760
Iteration 132/1000 | Loss: 0.00002760
Iteration 133/1000 | Loss: 0.00002760
Iteration 134/1000 | Loss: 0.00002760
Iteration 135/1000 | Loss: 0.00002760
Iteration 136/1000 | Loss: 0.00002760
Iteration 137/1000 | Loss: 0.00002760
Iteration 138/1000 | Loss: 0.00002760
Iteration 139/1000 | Loss: 0.00002760
Iteration 140/1000 | Loss: 0.00002760
Iteration 141/1000 | Loss: 0.00002760
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002760
Iteration 144/1000 | Loss: 0.00002760
Iteration 145/1000 | Loss: 0.00002760
Iteration 146/1000 | Loss: 0.00002760
Iteration 147/1000 | Loss: 0.00002760
Iteration 148/1000 | Loss: 0.00002760
Iteration 149/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.759940798569005e-05, 2.759940798569005e-05, 2.759940798569005e-05, 2.759940798569005e-05, 2.759940798569005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.759940798569005e-05

Optimization complete. Final v2v error: 4.312824726104736 mm

Highest mean error: 4.686249732971191 mm for frame 27

Lowest mean error: 3.8103017807006836 mm for frame 205

Saving results

Total time: 43.01934623718262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101766
Iteration 2/25 | Loss: 0.00191638
Iteration 3/25 | Loss: 0.00112235
Iteration 4/25 | Loss: 0.00099537
Iteration 5/25 | Loss: 0.00092111
Iteration 6/25 | Loss: 0.00094550
Iteration 7/25 | Loss: 0.00090138
Iteration 8/25 | Loss: 0.00086975
Iteration 9/25 | Loss: 0.00084488
Iteration 10/25 | Loss: 0.00083685
Iteration 11/25 | Loss: 0.00083277
Iteration 12/25 | Loss: 0.00083730
Iteration 13/25 | Loss: 0.00081936
Iteration 14/25 | Loss: 0.00081709
Iteration 15/25 | Loss: 0.00081614
Iteration 16/25 | Loss: 0.00081598
Iteration 17/25 | Loss: 0.00081588
Iteration 18/25 | Loss: 0.00081583
Iteration 19/25 | Loss: 0.00081583
Iteration 20/25 | Loss: 0.00081583
Iteration 21/25 | Loss: 0.00081583
Iteration 22/25 | Loss: 0.00081583
Iteration 23/25 | Loss: 0.00081583
Iteration 24/25 | Loss: 0.00081583
Iteration 25/25 | Loss: 0.00081583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30495358
Iteration 2/25 | Loss: 0.00045222
Iteration 3/25 | Loss: 0.00045221
Iteration 4/25 | Loss: 0.00045221
Iteration 5/25 | Loss: 0.00045221
Iteration 6/25 | Loss: 0.00045221
Iteration 7/25 | Loss: 0.00045221
Iteration 8/25 | Loss: 0.00045221
Iteration 9/25 | Loss: 0.00045221
Iteration 10/25 | Loss: 0.00045221
Iteration 11/25 | Loss: 0.00045221
Iteration 12/25 | Loss: 0.00045221
Iteration 13/25 | Loss: 0.00045221
Iteration 14/25 | Loss: 0.00045221
Iteration 15/25 | Loss: 0.00045221
Iteration 16/25 | Loss: 0.00045221
Iteration 17/25 | Loss: 0.00045221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00045220667379908264, 0.00045220667379908264, 0.00045220667379908264, 0.00045220667379908264, 0.00045220667379908264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045220667379908264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045221
Iteration 2/1000 | Loss: 0.00004220
Iteration 3/1000 | Loss: 0.00003781
Iteration 4/1000 | Loss: 0.00006077
Iteration 5/1000 | Loss: 0.00002595
Iteration 6/1000 | Loss: 0.00004608
Iteration 7/1000 | Loss: 0.00002417
Iteration 8/1000 | Loss: 0.00004724
Iteration 9/1000 | Loss: 0.00002346
Iteration 10/1000 | Loss: 0.00002313
Iteration 11/1000 | Loss: 0.00002284
Iteration 12/1000 | Loss: 0.00008183
Iteration 13/1000 | Loss: 0.00012280
Iteration 14/1000 | Loss: 0.00002251
Iteration 15/1000 | Loss: 0.00005143
Iteration 16/1000 | Loss: 0.00002236
Iteration 17/1000 | Loss: 0.00002227
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00005114
Iteration 20/1000 | Loss: 0.00002209
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002200
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002200
Iteration 27/1000 | Loss: 0.00002200
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002200
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002199
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00004033
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002194
Iteration 42/1000 | Loss: 0.00002194
Iteration 43/1000 | Loss: 0.00002193
Iteration 44/1000 | Loss: 0.00002193
Iteration 45/1000 | Loss: 0.00002193
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002192
Iteration 49/1000 | Loss: 0.00002192
Iteration 50/1000 | Loss: 0.00002192
Iteration 51/1000 | Loss: 0.00002192
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002191
Iteration 55/1000 | Loss: 0.00002190
Iteration 56/1000 | Loss: 0.00002190
Iteration 57/1000 | Loss: 0.00002189
Iteration 58/1000 | Loss: 0.00002189
Iteration 59/1000 | Loss: 0.00002188
Iteration 60/1000 | Loss: 0.00002188
Iteration 61/1000 | Loss: 0.00002188
Iteration 62/1000 | Loss: 0.00002188
Iteration 63/1000 | Loss: 0.00002188
Iteration 64/1000 | Loss: 0.00002188
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002187
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002187
Iteration 81/1000 | Loss: 0.00002187
Iteration 82/1000 | Loss: 0.00002187
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002187
Iteration 85/1000 | Loss: 0.00002187
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002186
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002186
Iteration 92/1000 | Loss: 0.00002186
Iteration 93/1000 | Loss: 0.00002186
Iteration 94/1000 | Loss: 0.00002186
Iteration 95/1000 | Loss: 0.00002186
Iteration 96/1000 | Loss: 0.00002186
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002184
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002184
Iteration 111/1000 | Loss: 0.00002184
Iteration 112/1000 | Loss: 0.00002184
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00002183
Iteration 117/1000 | Loss: 0.00002183
Iteration 118/1000 | Loss: 0.00002183
Iteration 119/1000 | Loss: 0.00002183
Iteration 120/1000 | Loss: 0.00002182
Iteration 121/1000 | Loss: 0.00002182
Iteration 122/1000 | Loss: 0.00002182
Iteration 123/1000 | Loss: 0.00002182
Iteration 124/1000 | Loss: 0.00002181
Iteration 125/1000 | Loss: 0.00002181
Iteration 126/1000 | Loss: 0.00005636
Iteration 127/1000 | Loss: 0.00002261
Iteration 128/1000 | Loss: 0.00002378
Iteration 129/1000 | Loss: 0.00002183
Iteration 130/1000 | Loss: 0.00002183
Iteration 131/1000 | Loss: 0.00002183
Iteration 132/1000 | Loss: 0.00002182
Iteration 133/1000 | Loss: 0.00002182
Iteration 134/1000 | Loss: 0.00002182
Iteration 135/1000 | Loss: 0.00002182
Iteration 136/1000 | Loss: 0.00002182
Iteration 137/1000 | Loss: 0.00002182
Iteration 138/1000 | Loss: 0.00002182
Iteration 139/1000 | Loss: 0.00002182
Iteration 140/1000 | Loss: 0.00002182
Iteration 141/1000 | Loss: 0.00002182
Iteration 142/1000 | Loss: 0.00002181
Iteration 143/1000 | Loss: 0.00002180
Iteration 144/1000 | Loss: 0.00002180
Iteration 145/1000 | Loss: 0.00002180
Iteration 146/1000 | Loss: 0.00002180
Iteration 147/1000 | Loss: 0.00002180
Iteration 148/1000 | Loss: 0.00002179
Iteration 149/1000 | Loss: 0.00002179
Iteration 150/1000 | Loss: 0.00002179
Iteration 151/1000 | Loss: 0.00002179
Iteration 152/1000 | Loss: 0.00002179
Iteration 153/1000 | Loss: 0.00002179
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00002178
Iteration 156/1000 | Loss: 0.00002178
Iteration 157/1000 | Loss: 0.00002178
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002177
Iteration 161/1000 | Loss: 0.00002177
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002176
Iteration 167/1000 | Loss: 0.00002176
Iteration 168/1000 | Loss: 0.00002176
Iteration 169/1000 | Loss: 0.00002176
Iteration 170/1000 | Loss: 0.00002176
Iteration 171/1000 | Loss: 0.00002176
Iteration 172/1000 | Loss: 0.00002176
Iteration 173/1000 | Loss: 0.00002175
Iteration 174/1000 | Loss: 0.00002175
Iteration 175/1000 | Loss: 0.00002175
Iteration 176/1000 | Loss: 0.00002175
Iteration 177/1000 | Loss: 0.00002175
Iteration 178/1000 | Loss: 0.00002175
Iteration 179/1000 | Loss: 0.00002175
Iteration 180/1000 | Loss: 0.00002175
Iteration 181/1000 | Loss: 0.00002175
Iteration 182/1000 | Loss: 0.00002175
Iteration 183/1000 | Loss: 0.00002175
Iteration 184/1000 | Loss: 0.00002175
Iteration 185/1000 | Loss: 0.00002175
Iteration 186/1000 | Loss: 0.00002175
Iteration 187/1000 | Loss: 0.00002175
Iteration 188/1000 | Loss: 0.00002175
Iteration 189/1000 | Loss: 0.00002175
Iteration 190/1000 | Loss: 0.00002175
Iteration 191/1000 | Loss: 0.00002175
Iteration 192/1000 | Loss: 0.00002175
Iteration 193/1000 | Loss: 0.00002175
Iteration 194/1000 | Loss: 0.00002175
Iteration 195/1000 | Loss: 0.00002175
Iteration 196/1000 | Loss: 0.00002175
Iteration 197/1000 | Loss: 0.00002175
Iteration 198/1000 | Loss: 0.00002175
Iteration 199/1000 | Loss: 0.00002175
Iteration 200/1000 | Loss: 0.00002175
Iteration 201/1000 | Loss: 0.00002175
Iteration 202/1000 | Loss: 0.00002175
Iteration 203/1000 | Loss: 0.00002175
Iteration 204/1000 | Loss: 0.00002175
Iteration 205/1000 | Loss: 0.00002175
Iteration 206/1000 | Loss: 0.00002175
Iteration 207/1000 | Loss: 0.00002175
Iteration 208/1000 | Loss: 0.00002175
Iteration 209/1000 | Loss: 0.00002175
Iteration 210/1000 | Loss: 0.00002175
Iteration 211/1000 | Loss: 0.00002175
Iteration 212/1000 | Loss: 0.00002175
Iteration 213/1000 | Loss: 0.00002175
Iteration 214/1000 | Loss: 0.00002175
Iteration 215/1000 | Loss: 0.00002175
Iteration 216/1000 | Loss: 0.00002175
Iteration 217/1000 | Loss: 0.00002175
Iteration 218/1000 | Loss: 0.00002175
Iteration 219/1000 | Loss: 0.00002175
Iteration 220/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.1748690414824523e-05, 2.1748690414824523e-05, 2.1748690414824523e-05, 2.1748690414824523e-05, 2.1748690414824523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1748690414824523e-05

Optimization complete. Final v2v error: 3.647153854370117 mm

Highest mean error: 6.11308479309082 mm for frame 73

Lowest mean error: 2.6688103675842285 mm for frame 0

Saving results

Total time: 77.34794807434082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788789
Iteration 2/25 | Loss: 0.00120385
Iteration 3/25 | Loss: 0.00091140
Iteration 4/25 | Loss: 0.00084686
Iteration 5/25 | Loss: 0.00082321
Iteration 6/25 | Loss: 0.00080806
Iteration 7/25 | Loss: 0.00082555
Iteration 8/25 | Loss: 0.00081564
Iteration 9/25 | Loss: 0.00079879
Iteration 10/25 | Loss: 0.00079019
Iteration 11/25 | Loss: 0.00078523
Iteration 12/25 | Loss: 0.00078279
Iteration 13/25 | Loss: 0.00078472
Iteration 14/25 | Loss: 0.00078206
Iteration 15/25 | Loss: 0.00078156
Iteration 16/25 | Loss: 0.00078154
Iteration 17/25 | Loss: 0.00078154
Iteration 18/25 | Loss: 0.00078154
Iteration 19/25 | Loss: 0.00078154
Iteration 20/25 | Loss: 0.00078154
Iteration 21/25 | Loss: 0.00078153
Iteration 22/25 | Loss: 0.00078153
Iteration 23/25 | Loss: 0.00078153
Iteration 24/25 | Loss: 0.00078153
Iteration 25/25 | Loss: 0.00078153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94473314
Iteration 2/25 | Loss: 0.00050891
Iteration 3/25 | Loss: 0.00050891
Iteration 4/25 | Loss: 0.00050890
Iteration 5/25 | Loss: 0.00050890
Iteration 6/25 | Loss: 0.00050890
Iteration 7/25 | Loss: 0.00050890
Iteration 8/25 | Loss: 0.00050890
Iteration 9/25 | Loss: 0.00050890
Iteration 10/25 | Loss: 0.00050890
Iteration 11/25 | Loss: 0.00050890
Iteration 12/25 | Loss: 0.00050890
Iteration 13/25 | Loss: 0.00050890
Iteration 14/25 | Loss: 0.00050890
Iteration 15/25 | Loss: 0.00050890
Iteration 16/25 | Loss: 0.00050890
Iteration 17/25 | Loss: 0.00050890
Iteration 18/25 | Loss: 0.00050890
Iteration 19/25 | Loss: 0.00050890
Iteration 20/25 | Loss: 0.00050890
Iteration 21/25 | Loss: 0.00050890
Iteration 22/25 | Loss: 0.00050890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005089028272777796, 0.0005089028272777796, 0.0005089028272777796, 0.0005089028272777796, 0.0005089028272777796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005089028272777796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050890
Iteration 2/1000 | Loss: 0.00005779
Iteration 3/1000 | Loss: 0.00002576
Iteration 4/1000 | Loss: 0.00004586
Iteration 5/1000 | Loss: 0.00002307
Iteration 6/1000 | Loss: 0.00006632
Iteration 7/1000 | Loss: 0.00003497
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00067897
Iteration 11/1000 | Loss: 0.00002250
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001802
Iteration 19/1000 | Loss: 0.00001801
Iteration 20/1000 | Loss: 0.00001801
Iteration 21/1000 | Loss: 0.00001800
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001793
Iteration 24/1000 | Loss: 0.00001788
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00001773
Iteration 31/1000 | Loss: 0.00001772
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001758
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001751
Iteration 70/1000 | Loss: 0.00001751
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001749
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001749
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001748
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001747
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001746
Iteration 101/1000 | Loss: 0.00001746
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001746
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001744
Iteration 119/1000 | Loss: 0.00001744
Iteration 120/1000 | Loss: 0.00001744
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.7436641428503208e-05, 1.7436641428503208e-05, 1.7436641428503208e-05, 1.7436641428503208e-05, 1.7436641428503208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7436641428503208e-05

Optimization complete. Final v2v error: 3.4685146808624268 mm

Highest mean error: 4.305491924285889 mm for frame 41

Lowest mean error: 2.967458486557007 mm for frame 75

Saving results

Total time: 61.01682138442993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571494
Iteration 2/25 | Loss: 0.00095997
Iteration 3/25 | Loss: 0.00083887
Iteration 4/25 | Loss: 0.00081290
Iteration 5/25 | Loss: 0.00080307
Iteration 6/25 | Loss: 0.00080147
Iteration 7/25 | Loss: 0.00080111
Iteration 8/25 | Loss: 0.00080111
Iteration 9/25 | Loss: 0.00080111
Iteration 10/25 | Loss: 0.00080111
Iteration 11/25 | Loss: 0.00080111
Iteration 12/25 | Loss: 0.00080111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008011112222447991, 0.0008011112222447991, 0.0008011112222447991, 0.0008011112222447991, 0.0008011112222447991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008011112222447991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45159149
Iteration 2/25 | Loss: 0.00047113
Iteration 3/25 | Loss: 0.00047109
Iteration 4/25 | Loss: 0.00047109
Iteration 5/25 | Loss: 0.00047109
Iteration 6/25 | Loss: 0.00047109
Iteration 7/25 | Loss: 0.00047109
Iteration 8/25 | Loss: 0.00047109
Iteration 9/25 | Loss: 0.00047109
Iteration 10/25 | Loss: 0.00047109
Iteration 11/25 | Loss: 0.00047109
Iteration 12/25 | Loss: 0.00047109
Iteration 13/25 | Loss: 0.00047109
Iteration 14/25 | Loss: 0.00047109
Iteration 15/25 | Loss: 0.00047109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004710904904641211, 0.0004710904904641211, 0.0004710904904641211, 0.0004710904904641211, 0.0004710904904641211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004710904904641211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047109
Iteration 2/1000 | Loss: 0.00004112
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002275
Iteration 6/1000 | Loss: 0.00002198
Iteration 7/1000 | Loss: 0.00002136
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00002063
Iteration 10/1000 | Loss: 0.00002060
Iteration 11/1000 | Loss: 0.00002039
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002035
Iteration 14/1000 | Loss: 0.00002026
Iteration 15/1000 | Loss: 0.00002018
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00002008
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001995
Iteration 28/1000 | Loss: 0.00001995
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001994
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001993
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00001993
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00001993
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001991
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001989
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001988
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001986
Iteration 65/1000 | Loss: 0.00001986
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001982
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001979
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001972
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001969
Iteration 103/1000 | Loss: 0.00001969
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001969
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001968
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001968
Iteration 130/1000 | Loss: 0.00001968
Iteration 131/1000 | Loss: 0.00001968
Iteration 132/1000 | Loss: 0.00001968
Iteration 133/1000 | Loss: 0.00001968
Iteration 134/1000 | Loss: 0.00001968
Iteration 135/1000 | Loss: 0.00001968
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001968
Iteration 139/1000 | Loss: 0.00001968
Iteration 140/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.967643947864417e-05, 1.967643947864417e-05, 1.967643947864417e-05, 1.967643947864417e-05, 1.967643947864417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.967643947864417e-05

Optimization complete. Final v2v error: 3.7563211917877197 mm

Highest mean error: 4.15610408782959 mm for frame 122

Lowest mean error: 3.261143207550049 mm for frame 36

Saving results

Total time: 38.029234886169434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808643
Iteration 2/25 | Loss: 0.00118611
Iteration 3/25 | Loss: 0.00084452
Iteration 4/25 | Loss: 0.00081951
Iteration 5/25 | Loss: 0.00081671
Iteration 6/25 | Loss: 0.00081671
Iteration 7/25 | Loss: 0.00081671
Iteration 8/25 | Loss: 0.00081671
Iteration 9/25 | Loss: 0.00081671
Iteration 10/25 | Loss: 0.00081671
Iteration 11/25 | Loss: 0.00081671
Iteration 12/25 | Loss: 0.00081671
Iteration 13/25 | Loss: 0.00081671
Iteration 14/25 | Loss: 0.00081671
Iteration 15/25 | Loss: 0.00081671
Iteration 16/25 | Loss: 0.00081671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008167113992385566, 0.0008167113992385566, 0.0008167113992385566, 0.0008167113992385566, 0.0008167113992385566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008167113992385566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47341251
Iteration 2/25 | Loss: 0.00057445
Iteration 3/25 | Loss: 0.00057445
Iteration 4/25 | Loss: 0.00057445
Iteration 5/25 | Loss: 0.00057445
Iteration 6/25 | Loss: 0.00057445
Iteration 7/25 | Loss: 0.00057445
Iteration 8/25 | Loss: 0.00057445
Iteration 9/25 | Loss: 0.00057445
Iteration 10/25 | Loss: 0.00057445
Iteration 11/25 | Loss: 0.00057445
Iteration 12/25 | Loss: 0.00057445
Iteration 13/25 | Loss: 0.00057445
Iteration 14/25 | Loss: 0.00057445
Iteration 15/25 | Loss: 0.00057445
Iteration 16/25 | Loss: 0.00057445
Iteration 17/25 | Loss: 0.00057445
Iteration 18/25 | Loss: 0.00057445
Iteration 19/25 | Loss: 0.00057445
Iteration 20/25 | Loss: 0.00057445
Iteration 21/25 | Loss: 0.00057445
Iteration 22/25 | Loss: 0.00057445
Iteration 23/25 | Loss: 0.00057445
Iteration 24/25 | Loss: 0.00057445
Iteration 25/25 | Loss: 0.00057445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057445
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001969
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001769
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001719
Iteration 10/1000 | Loss: 0.00001713
Iteration 11/1000 | Loss: 0.00001704
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001704
Iteration 16/1000 | Loss: 0.00001704
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001700
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001700
Iteration 32/1000 | Loss: 0.00001700
Iteration 33/1000 | Loss: 0.00001700
Iteration 34/1000 | Loss: 0.00001699
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001698
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001694
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001691
Iteration 103/1000 | Loss: 0.00001691
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001689
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001686
Iteration 151/1000 | Loss: 0.00001686
Iteration 152/1000 | Loss: 0.00001686
Iteration 153/1000 | Loss: 0.00001686
Iteration 154/1000 | Loss: 0.00001686
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6860889445524663e-05, 1.6860889445524663e-05, 1.6860889445524663e-05, 1.6860889445524663e-05, 1.6860889445524663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6860889445524663e-05

Optimization complete. Final v2v error: 3.461372137069702 mm

Highest mean error: 3.7101964950561523 mm for frame 23

Lowest mean error: 3.128922462463379 mm for frame 89

Saving results

Total time: 34.98041486740112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835395
Iteration 2/25 | Loss: 0.00115223
Iteration 3/25 | Loss: 0.00082153
Iteration 4/25 | Loss: 0.00079978
Iteration 5/25 | Loss: 0.00079625
Iteration 6/25 | Loss: 0.00079445
Iteration 7/25 | Loss: 0.00079439
Iteration 8/25 | Loss: 0.00079439
Iteration 9/25 | Loss: 0.00079439
Iteration 10/25 | Loss: 0.00079439
Iteration 11/25 | Loss: 0.00079439
Iteration 12/25 | Loss: 0.00079439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007943866075947881, 0.0007943866075947881, 0.0007943866075947881, 0.0007943866075947881, 0.0007943866075947881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007943866075947881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39469910
Iteration 2/25 | Loss: 0.00050533
Iteration 3/25 | Loss: 0.00050532
Iteration 4/25 | Loss: 0.00050532
Iteration 5/25 | Loss: 0.00050532
Iteration 6/25 | Loss: 0.00050532
Iteration 7/25 | Loss: 0.00050532
Iteration 8/25 | Loss: 0.00050532
Iteration 9/25 | Loss: 0.00050532
Iteration 10/25 | Loss: 0.00050532
Iteration 11/25 | Loss: 0.00050532
Iteration 12/25 | Loss: 0.00050532
Iteration 13/25 | Loss: 0.00050532
Iteration 14/25 | Loss: 0.00050532
Iteration 15/25 | Loss: 0.00050532
Iteration 16/25 | Loss: 0.00050532
Iteration 17/25 | Loss: 0.00050532
Iteration 18/25 | Loss: 0.00050532
Iteration 19/25 | Loss: 0.00050532
Iteration 20/25 | Loss: 0.00050532
Iteration 21/25 | Loss: 0.00050532
Iteration 22/25 | Loss: 0.00050532
Iteration 23/25 | Loss: 0.00050532
Iteration 24/25 | Loss: 0.00050532
Iteration 25/25 | Loss: 0.00050532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050532
Iteration 2/1000 | Loss: 0.00002169
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001515
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00001391
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001391
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001391
Iteration 24/1000 | Loss: 0.00001390
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001383
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001378
Iteration 59/1000 | Loss: 0.00001378
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001373
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001372
Iteration 89/1000 | Loss: 0.00001372
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001371
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001368
Iteration 132/1000 | Loss: 0.00001368
Iteration 133/1000 | Loss: 0.00001368
Iteration 134/1000 | Loss: 0.00001368
Iteration 135/1000 | Loss: 0.00001368
Iteration 136/1000 | Loss: 0.00001368
Iteration 137/1000 | Loss: 0.00001368
Iteration 138/1000 | Loss: 0.00001368
Iteration 139/1000 | Loss: 0.00001368
Iteration 140/1000 | Loss: 0.00001368
Iteration 141/1000 | Loss: 0.00001368
Iteration 142/1000 | Loss: 0.00001368
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001367
Iteration 148/1000 | Loss: 0.00001367
Iteration 149/1000 | Loss: 0.00001367
Iteration 150/1000 | Loss: 0.00001367
Iteration 151/1000 | Loss: 0.00001367
Iteration 152/1000 | Loss: 0.00001367
Iteration 153/1000 | Loss: 0.00001367
Iteration 154/1000 | Loss: 0.00001367
Iteration 155/1000 | Loss: 0.00001367
Iteration 156/1000 | Loss: 0.00001367
Iteration 157/1000 | Loss: 0.00001367
Iteration 158/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.367487038805848e-05, 1.367487038805848e-05, 1.367487038805848e-05, 1.367487038805848e-05, 1.367487038805848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.367487038805848e-05

Optimization complete. Final v2v error: 3.1624953746795654 mm

Highest mean error: 3.49603009223938 mm for frame 61

Lowest mean error: 3.0224928855895996 mm for frame 43

Saving results

Total time: 32.403621673583984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423711
Iteration 2/25 | Loss: 0.00091262
Iteration 3/25 | Loss: 0.00077096
Iteration 4/25 | Loss: 0.00075383
Iteration 5/25 | Loss: 0.00074802
Iteration 6/25 | Loss: 0.00074669
Iteration 7/25 | Loss: 0.00074625
Iteration 8/25 | Loss: 0.00074624
Iteration 9/25 | Loss: 0.00074624
Iteration 10/25 | Loss: 0.00074624
Iteration 11/25 | Loss: 0.00074624
Iteration 12/25 | Loss: 0.00074624
Iteration 13/25 | Loss: 0.00074624
Iteration 14/25 | Loss: 0.00074624
Iteration 15/25 | Loss: 0.00074624
Iteration 16/25 | Loss: 0.00074624
Iteration 17/25 | Loss: 0.00074624
Iteration 18/25 | Loss: 0.00074624
Iteration 19/25 | Loss: 0.00074624
Iteration 20/25 | Loss: 0.00074624
Iteration 21/25 | Loss: 0.00074624
Iteration 22/25 | Loss: 0.00074624
Iteration 23/25 | Loss: 0.00074624
Iteration 24/25 | Loss: 0.00074624
Iteration 25/25 | Loss: 0.00074624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72689342
Iteration 2/25 | Loss: 0.00049399
Iteration 3/25 | Loss: 0.00049398
Iteration 4/25 | Loss: 0.00049398
Iteration 5/25 | Loss: 0.00049398
Iteration 6/25 | Loss: 0.00049398
Iteration 7/25 | Loss: 0.00049398
Iteration 8/25 | Loss: 0.00049398
Iteration 9/25 | Loss: 0.00049398
Iteration 10/25 | Loss: 0.00049398
Iteration 11/25 | Loss: 0.00049398
Iteration 12/25 | Loss: 0.00049398
Iteration 13/25 | Loss: 0.00049398
Iteration 14/25 | Loss: 0.00049398
Iteration 15/25 | Loss: 0.00049398
Iteration 16/25 | Loss: 0.00049398
Iteration 17/25 | Loss: 0.00049398
Iteration 18/25 | Loss: 0.00049398
Iteration 19/25 | Loss: 0.00049398
Iteration 20/25 | Loss: 0.00049398
Iteration 21/25 | Loss: 0.00049398
Iteration 22/25 | Loss: 0.00049398
Iteration 23/25 | Loss: 0.00049398
Iteration 24/25 | Loss: 0.00049398
Iteration 25/25 | Loss: 0.00049398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049398
Iteration 2/1000 | Loss: 0.00002707
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001536
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001423
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001422
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001394
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001390
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001387
Iteration 115/1000 | Loss: 0.00001387
Iteration 116/1000 | Loss: 0.00001387
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001386
Iteration 121/1000 | Loss: 0.00001386
Iteration 122/1000 | Loss: 0.00001386
Iteration 123/1000 | Loss: 0.00001386
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001384
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001382
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00001382
Iteration 174/1000 | Loss: 0.00001382
Iteration 175/1000 | Loss: 0.00001382
Iteration 176/1000 | Loss: 0.00001382
Iteration 177/1000 | Loss: 0.00001382
Iteration 178/1000 | Loss: 0.00001382
Iteration 179/1000 | Loss: 0.00001382
Iteration 180/1000 | Loss: 0.00001382
Iteration 181/1000 | Loss: 0.00001382
Iteration 182/1000 | Loss: 0.00001382
Iteration 183/1000 | Loss: 0.00001382
Iteration 184/1000 | Loss: 0.00001382
Iteration 185/1000 | Loss: 0.00001382
Iteration 186/1000 | Loss: 0.00001382
Iteration 187/1000 | Loss: 0.00001382
Iteration 188/1000 | Loss: 0.00001382
Iteration 189/1000 | Loss: 0.00001382
Iteration 190/1000 | Loss: 0.00001382
Iteration 191/1000 | Loss: 0.00001381
Iteration 192/1000 | Loss: 0.00001381
Iteration 193/1000 | Loss: 0.00001381
Iteration 194/1000 | Loss: 0.00001381
Iteration 195/1000 | Loss: 0.00001381
Iteration 196/1000 | Loss: 0.00001381
Iteration 197/1000 | Loss: 0.00001381
Iteration 198/1000 | Loss: 0.00001381
Iteration 199/1000 | Loss: 0.00001381
Iteration 200/1000 | Loss: 0.00001381
Iteration 201/1000 | Loss: 0.00001381
Iteration 202/1000 | Loss: 0.00001381
Iteration 203/1000 | Loss: 0.00001381
Iteration 204/1000 | Loss: 0.00001381
Iteration 205/1000 | Loss: 0.00001381
Iteration 206/1000 | Loss: 0.00001381
Iteration 207/1000 | Loss: 0.00001381
Iteration 208/1000 | Loss: 0.00001380
Iteration 209/1000 | Loss: 0.00001380
Iteration 210/1000 | Loss: 0.00001380
Iteration 211/1000 | Loss: 0.00001380
Iteration 212/1000 | Loss: 0.00001380
Iteration 213/1000 | Loss: 0.00001380
Iteration 214/1000 | Loss: 0.00001380
Iteration 215/1000 | Loss: 0.00001380
Iteration 216/1000 | Loss: 0.00001380
Iteration 217/1000 | Loss: 0.00001380
Iteration 218/1000 | Loss: 0.00001380
Iteration 219/1000 | Loss: 0.00001380
Iteration 220/1000 | Loss: 0.00001380
Iteration 221/1000 | Loss: 0.00001380
Iteration 222/1000 | Loss: 0.00001380
Iteration 223/1000 | Loss: 0.00001380
Iteration 224/1000 | Loss: 0.00001380
Iteration 225/1000 | Loss: 0.00001380
Iteration 226/1000 | Loss: 0.00001380
Iteration 227/1000 | Loss: 0.00001380
Iteration 228/1000 | Loss: 0.00001380
Iteration 229/1000 | Loss: 0.00001380
Iteration 230/1000 | Loss: 0.00001380
Iteration 231/1000 | Loss: 0.00001380
Iteration 232/1000 | Loss: 0.00001380
Iteration 233/1000 | Loss: 0.00001380
Iteration 234/1000 | Loss: 0.00001380
Iteration 235/1000 | Loss: 0.00001380
Iteration 236/1000 | Loss: 0.00001380
Iteration 237/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.3804151421936695e-05, 1.3804151421936695e-05, 1.3804151421936695e-05, 1.3804151421936695e-05, 1.3804151421936695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3804151421936695e-05

Optimization complete. Final v2v error: 3.194394111633301 mm

Highest mean error: 3.696221113204956 mm for frame 82

Lowest mean error: 2.887343406677246 mm for frame 34

Saving results

Total time: 41.765308141708374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846052
Iteration 2/25 | Loss: 0.00114856
Iteration 3/25 | Loss: 0.00078636
Iteration 4/25 | Loss: 0.00076551
Iteration 5/25 | Loss: 0.00073638
Iteration 6/25 | Loss: 0.00073361
Iteration 7/25 | Loss: 0.00073287
Iteration 8/25 | Loss: 0.00073264
Iteration 9/25 | Loss: 0.00073260
Iteration 10/25 | Loss: 0.00073260
Iteration 11/25 | Loss: 0.00073260
Iteration 12/25 | Loss: 0.00073260
Iteration 13/25 | Loss: 0.00073260
Iteration 14/25 | Loss: 0.00073260
Iteration 15/25 | Loss: 0.00073260
Iteration 16/25 | Loss: 0.00073259
Iteration 17/25 | Loss: 0.00073259
Iteration 18/25 | Loss: 0.00073259
Iteration 19/25 | Loss: 0.00073259
Iteration 20/25 | Loss: 0.00073259
Iteration 21/25 | Loss: 0.00073259
Iteration 22/25 | Loss: 0.00073259
Iteration 23/25 | Loss: 0.00073259
Iteration 24/25 | Loss: 0.00073259
Iteration 25/25 | Loss: 0.00073259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68092823
Iteration 2/25 | Loss: 0.00048396
Iteration 3/25 | Loss: 0.00048395
Iteration 4/25 | Loss: 0.00048395
Iteration 5/25 | Loss: 0.00048395
Iteration 6/25 | Loss: 0.00048395
Iteration 7/25 | Loss: 0.00048395
Iteration 8/25 | Loss: 0.00048395
Iteration 9/25 | Loss: 0.00048395
Iteration 10/25 | Loss: 0.00048395
Iteration 11/25 | Loss: 0.00048395
Iteration 12/25 | Loss: 0.00048395
Iteration 13/25 | Loss: 0.00048395
Iteration 14/25 | Loss: 0.00048395
Iteration 15/25 | Loss: 0.00048395
Iteration 16/25 | Loss: 0.00048395
Iteration 17/25 | Loss: 0.00048395
Iteration 18/25 | Loss: 0.00048395
Iteration 19/25 | Loss: 0.00048395
Iteration 20/25 | Loss: 0.00048395
Iteration 21/25 | Loss: 0.00048395
Iteration 22/25 | Loss: 0.00048395
Iteration 23/25 | Loss: 0.00048395
Iteration 24/25 | Loss: 0.00048395
Iteration 25/25 | Loss: 0.00048395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048395
Iteration 2/1000 | Loss: 0.00002246
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001320
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001308
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001307
Iteration 67/1000 | Loss: 0.00001307
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001300
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001290
Iteration 160/1000 | Loss: 0.00001290
Iteration 161/1000 | Loss: 0.00001290
Iteration 162/1000 | Loss: 0.00001290
Iteration 163/1000 | Loss: 0.00001290
Iteration 164/1000 | Loss: 0.00001290
Iteration 165/1000 | Loss: 0.00001290
Iteration 166/1000 | Loss: 0.00001290
Iteration 167/1000 | Loss: 0.00001290
Iteration 168/1000 | Loss: 0.00001290
Iteration 169/1000 | Loss: 0.00001290
Iteration 170/1000 | Loss: 0.00001290
Iteration 171/1000 | Loss: 0.00001290
Iteration 172/1000 | Loss: 0.00001290
Iteration 173/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.290413365495624e-05, 1.290413365495624e-05, 1.290413365495624e-05, 1.290413365495624e-05, 1.290413365495624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.290413365495624e-05

Optimization complete. Final v2v error: 3.072235107421875 mm

Highest mean error: 3.4013586044311523 mm for frame 220

Lowest mean error: 2.7335333824157715 mm for frame 180

Saving results

Total time: 45.666940450668335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394709
Iteration 2/25 | Loss: 0.00090377
Iteration 3/25 | Loss: 0.00077320
Iteration 4/25 | Loss: 0.00075701
Iteration 5/25 | Loss: 0.00075185
Iteration 6/25 | Loss: 0.00075053
Iteration 7/25 | Loss: 0.00075016
Iteration 8/25 | Loss: 0.00075016
Iteration 9/25 | Loss: 0.00075016
Iteration 10/25 | Loss: 0.00075016
Iteration 11/25 | Loss: 0.00075016
Iteration 12/25 | Loss: 0.00075016
Iteration 13/25 | Loss: 0.00075016
Iteration 14/25 | Loss: 0.00075016
Iteration 15/25 | Loss: 0.00075016
Iteration 16/25 | Loss: 0.00075016
Iteration 17/25 | Loss: 0.00075016
Iteration 18/25 | Loss: 0.00075016
Iteration 19/25 | Loss: 0.00075016
Iteration 20/25 | Loss: 0.00075016
Iteration 21/25 | Loss: 0.00075016
Iteration 22/25 | Loss: 0.00075016
Iteration 23/25 | Loss: 0.00075016
Iteration 24/25 | Loss: 0.00075016
Iteration 25/25 | Loss: 0.00075016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007501632790081203, 0.0007501632790081203, 0.0007501632790081203, 0.0007501632790081203, 0.0007501632790081203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007501632790081203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75168741
Iteration 2/25 | Loss: 0.00054249
Iteration 3/25 | Loss: 0.00054249
Iteration 4/25 | Loss: 0.00054249
Iteration 5/25 | Loss: 0.00054249
Iteration 6/25 | Loss: 0.00054249
Iteration 7/25 | Loss: 0.00054249
Iteration 8/25 | Loss: 0.00054249
Iteration 9/25 | Loss: 0.00054249
Iteration 10/25 | Loss: 0.00054249
Iteration 11/25 | Loss: 0.00054249
Iteration 12/25 | Loss: 0.00054249
Iteration 13/25 | Loss: 0.00054249
Iteration 14/25 | Loss: 0.00054249
Iteration 15/25 | Loss: 0.00054249
Iteration 16/25 | Loss: 0.00054249
Iteration 17/25 | Loss: 0.00054249
Iteration 18/25 | Loss: 0.00054249
Iteration 19/25 | Loss: 0.00054249
Iteration 20/25 | Loss: 0.00054249
Iteration 21/25 | Loss: 0.00054249
Iteration 22/25 | Loss: 0.00054249
Iteration 23/25 | Loss: 0.00054249
Iteration 24/25 | Loss: 0.00054249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005424871924333274, 0.0005424871924333274, 0.0005424871924333274, 0.0005424871924333274, 0.0005424871924333274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005424871924333274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054249
Iteration 2/1000 | Loss: 0.00002711
Iteration 3/1000 | Loss: 0.00001813
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001245
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001184
Iteration 11/1000 | Loss: 0.00001166
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001163
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001145
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001142
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001131
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001130
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001126
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001126
Iteration 74/1000 | Loss: 0.00001126
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001125
Iteration 78/1000 | Loss: 0.00001125
Iteration 79/1000 | Loss: 0.00001125
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001125
Iteration 83/1000 | Loss: 0.00001125
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001125
Iteration 88/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.1251512660237495e-05, 1.1251512660237495e-05, 1.1251512660237495e-05, 1.1251512660237495e-05, 1.1251512660237495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1251512660237495e-05

Optimization complete. Final v2v error: 2.862030029296875 mm

Highest mean error: 3.1001057624816895 mm for frame 52

Lowest mean error: 2.6942267417907715 mm for frame 137

Saving results

Total time: 32.71488094329834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052229
Iteration 2/25 | Loss: 0.01052228
Iteration 3/25 | Loss: 0.01052227
Iteration 4/25 | Loss: 0.01052227
Iteration 5/25 | Loss: 0.01052226
Iteration 6/25 | Loss: 0.01052226
Iteration 7/25 | Loss: 0.01052226
Iteration 8/25 | Loss: 0.01052225
Iteration 9/25 | Loss: 0.01052225
Iteration 10/25 | Loss: 0.01052224
Iteration 11/25 | Loss: 0.00260826
Iteration 12/25 | Loss: 0.00162533
Iteration 13/25 | Loss: 0.00135595
Iteration 14/25 | Loss: 0.00119654
Iteration 15/25 | Loss: 0.00116468
Iteration 16/25 | Loss: 0.00113395
Iteration 17/25 | Loss: 0.00107037
Iteration 18/25 | Loss: 0.00105029
Iteration 19/25 | Loss: 0.00110865
Iteration 20/25 | Loss: 0.00103407
Iteration 21/25 | Loss: 0.00093382
Iteration 22/25 | Loss: 0.00090069
Iteration 23/25 | Loss: 0.00089362
Iteration 24/25 | Loss: 0.00089063
Iteration 25/25 | Loss: 0.00088618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52374721
Iteration 2/25 | Loss: 0.00091355
Iteration 3/25 | Loss: 0.00090955
Iteration 4/25 | Loss: 0.00090955
Iteration 5/25 | Loss: 0.00090955
Iteration 6/25 | Loss: 0.00090955
Iteration 7/25 | Loss: 0.00090955
Iteration 8/25 | Loss: 0.00090955
Iteration 9/25 | Loss: 0.00090955
Iteration 10/25 | Loss: 0.00090955
Iteration 11/25 | Loss: 0.00090955
Iteration 12/25 | Loss: 0.00090955
Iteration 13/25 | Loss: 0.00090954
Iteration 14/25 | Loss: 0.00090954
Iteration 15/25 | Loss: 0.00090954
Iteration 16/25 | Loss: 0.00090954
Iteration 17/25 | Loss: 0.00090954
Iteration 18/25 | Loss: 0.00090954
Iteration 19/25 | Loss: 0.00090954
Iteration 20/25 | Loss: 0.00090954
Iteration 21/25 | Loss: 0.00090954
Iteration 22/25 | Loss: 0.00090954
Iteration 23/25 | Loss: 0.00090954
Iteration 24/25 | Loss: 0.00090954
Iteration 25/25 | Loss: 0.00090954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090954
Iteration 2/1000 | Loss: 0.00053209
Iteration 3/1000 | Loss: 0.00022912
Iteration 4/1000 | Loss: 0.00009746
Iteration 5/1000 | Loss: 0.00007437
Iteration 6/1000 | Loss: 0.00019848
Iteration 7/1000 | Loss: 0.00059948
Iteration 8/1000 | Loss: 0.00046132
Iteration 9/1000 | Loss: 0.00020135
Iteration 10/1000 | Loss: 0.00006604
Iteration 11/1000 | Loss: 0.00014301
Iteration 12/1000 | Loss: 0.00056648
Iteration 13/1000 | Loss: 0.00048127
Iteration 14/1000 | Loss: 0.00014032
Iteration 15/1000 | Loss: 0.00018340
Iteration 16/1000 | Loss: 0.00007817
Iteration 17/1000 | Loss: 0.00010592
Iteration 18/1000 | Loss: 0.00009018
Iteration 19/1000 | Loss: 0.00009462
Iteration 20/1000 | Loss: 0.00014235
Iteration 21/1000 | Loss: 0.00008592
Iteration 22/1000 | Loss: 0.00003798
Iteration 23/1000 | Loss: 0.00014860
Iteration 24/1000 | Loss: 0.00008844
Iteration 25/1000 | Loss: 0.00043251
Iteration 26/1000 | Loss: 0.00008030
Iteration 27/1000 | Loss: 0.00008468
Iteration 28/1000 | Loss: 0.00015028
Iteration 29/1000 | Loss: 0.00013089
Iteration 30/1000 | Loss: 0.00015224
Iteration 31/1000 | Loss: 0.00006962
Iteration 32/1000 | Loss: 0.00010656
Iteration 33/1000 | Loss: 0.00030783
Iteration 34/1000 | Loss: 0.00072913
Iteration 35/1000 | Loss: 0.00027632
Iteration 36/1000 | Loss: 0.00061976
Iteration 37/1000 | Loss: 0.00010848
Iteration 38/1000 | Loss: 0.00005131
Iteration 39/1000 | Loss: 0.00004115
Iteration 40/1000 | Loss: 0.00004530
Iteration 41/1000 | Loss: 0.00003034
Iteration 42/1000 | Loss: 0.00009044
Iteration 43/1000 | Loss: 0.00017220
Iteration 44/1000 | Loss: 0.00017455
Iteration 45/1000 | Loss: 0.00011208
Iteration 46/1000 | Loss: 0.00015621
Iteration 47/1000 | Loss: 0.00003521
Iteration 48/1000 | Loss: 0.00003529
Iteration 49/1000 | Loss: 0.00004188
Iteration 50/1000 | Loss: 0.00031148
Iteration 51/1000 | Loss: 0.00014708
Iteration 52/1000 | Loss: 0.00008996
Iteration 53/1000 | Loss: 0.00008815
Iteration 54/1000 | Loss: 0.00008498
Iteration 55/1000 | Loss: 0.00012172
Iteration 56/1000 | Loss: 0.00036614
Iteration 57/1000 | Loss: 0.00007456
Iteration 58/1000 | Loss: 0.00004152
Iteration 59/1000 | Loss: 0.00019686
Iteration 60/1000 | Loss: 0.00004832
Iteration 61/1000 | Loss: 0.00005053
Iteration 62/1000 | Loss: 0.00004909
Iteration 63/1000 | Loss: 0.00005141
Iteration 64/1000 | Loss: 0.00024542
Iteration 65/1000 | Loss: 0.00003151
Iteration 66/1000 | Loss: 0.00002474
Iteration 67/1000 | Loss: 0.00002222
Iteration 68/1000 | Loss: 0.00003495
Iteration 69/1000 | Loss: 0.00002934
Iteration 70/1000 | Loss: 0.00003595
Iteration 71/1000 | Loss: 0.00002874
Iteration 72/1000 | Loss: 0.00003688
Iteration 73/1000 | Loss: 0.00003179
Iteration 74/1000 | Loss: 0.00003597
Iteration 75/1000 | Loss: 0.00003090
Iteration 76/1000 | Loss: 0.00003637
Iteration 77/1000 | Loss: 0.00002956
Iteration 78/1000 | Loss: 0.00003618
Iteration 79/1000 | Loss: 0.00002827
Iteration 80/1000 | Loss: 0.00005080
Iteration 81/1000 | Loss: 0.00004060
Iteration 82/1000 | Loss: 0.00005056
Iteration 83/1000 | Loss: 0.00003421
Iteration 84/1000 | Loss: 0.00004234
Iteration 85/1000 | Loss: 0.00004202
Iteration 86/1000 | Loss: 0.00004934
Iteration 87/1000 | Loss: 0.00004281
Iteration 88/1000 | Loss: 0.00004628
Iteration 89/1000 | Loss: 0.00003922
Iteration 90/1000 | Loss: 0.00004634
Iteration 91/1000 | Loss: 0.00002268
Iteration 92/1000 | Loss: 0.00001866
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001751
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001735
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00001703
Iteration 155/1000 | Loss: 0.00001703
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Iteration 172/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.7027088688337244e-05, 1.7027088688337244e-05, 1.7027088688337244e-05, 1.7027088688337244e-05, 1.7027088688337244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7027088688337244e-05

Optimization complete. Final v2v error: 3.3948287963867188 mm

Highest mean error: 6.0542683601379395 mm for frame 6

Lowest mean error: 2.960362195968628 mm for frame 21

Saving results

Total time: 197.48654651641846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460819
Iteration 2/25 | Loss: 0.00108338
Iteration 3/25 | Loss: 0.00080170
Iteration 4/25 | Loss: 0.00076380
Iteration 5/25 | Loss: 0.00075302
Iteration 6/25 | Loss: 0.00075003
Iteration 7/25 | Loss: 0.00074927
Iteration 8/25 | Loss: 0.00074927
Iteration 9/25 | Loss: 0.00074927
Iteration 10/25 | Loss: 0.00074927
Iteration 11/25 | Loss: 0.00074927
Iteration 12/25 | Loss: 0.00074927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007492711883969605, 0.0007492711883969605, 0.0007492711883969605, 0.0007492711883969605, 0.0007492711883969605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007492711883969605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49975455
Iteration 2/25 | Loss: 0.00045247
Iteration 3/25 | Loss: 0.00045247
Iteration 4/25 | Loss: 0.00045247
Iteration 5/25 | Loss: 0.00045247
Iteration 6/25 | Loss: 0.00045247
Iteration 7/25 | Loss: 0.00045246
Iteration 8/25 | Loss: 0.00045246
Iteration 9/25 | Loss: 0.00045246
Iteration 10/25 | Loss: 0.00045246
Iteration 11/25 | Loss: 0.00045246
Iteration 12/25 | Loss: 0.00045246
Iteration 13/25 | Loss: 0.00045246
Iteration 14/25 | Loss: 0.00045246
Iteration 15/25 | Loss: 0.00045246
Iteration 16/25 | Loss: 0.00045246
Iteration 17/25 | Loss: 0.00045246
Iteration 18/25 | Loss: 0.00045246
Iteration 19/25 | Loss: 0.00045246
Iteration 20/25 | Loss: 0.00045246
Iteration 21/25 | Loss: 0.00045246
Iteration 22/25 | Loss: 0.00045246
Iteration 23/25 | Loss: 0.00045246
Iteration 24/25 | Loss: 0.00045246
Iteration 25/25 | Loss: 0.00045246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045246
Iteration 2/1000 | Loss: 0.00003505
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001978
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001844
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001769
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001759
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001758
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001755
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001754
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001751
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001750
Iteration 80/1000 | Loss: 0.00001750
Iteration 81/1000 | Loss: 0.00001750
Iteration 82/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.7501493857707828e-05, 1.7501493857707828e-05, 1.7501493857707828e-05, 1.7501493857707828e-05, 1.7501493857707828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7501493857707828e-05

Optimization complete. Final v2v error: 3.486069440841675 mm

Highest mean error: 4.567078590393066 mm for frame 205

Lowest mean error: 2.771575689315796 mm for frame 56

Saving results

Total time: 40.946532249450684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358080
Iteration 2/25 | Loss: 0.00078268
Iteration 3/25 | Loss: 0.00070014
Iteration 4/25 | Loss: 0.00068752
Iteration 5/25 | Loss: 0.00068360
Iteration 6/25 | Loss: 0.00068244
Iteration 7/25 | Loss: 0.00068228
Iteration 8/25 | Loss: 0.00068228
Iteration 9/25 | Loss: 0.00068228
Iteration 10/25 | Loss: 0.00068228
Iteration 11/25 | Loss: 0.00068228
Iteration 12/25 | Loss: 0.00068228
Iteration 13/25 | Loss: 0.00068228
Iteration 14/25 | Loss: 0.00068228
Iteration 15/25 | Loss: 0.00068228
Iteration 16/25 | Loss: 0.00068228
Iteration 17/25 | Loss: 0.00068228
Iteration 18/25 | Loss: 0.00068228
Iteration 19/25 | Loss: 0.00068228
Iteration 20/25 | Loss: 0.00068228
Iteration 21/25 | Loss: 0.00068228
Iteration 22/25 | Loss: 0.00068228
Iteration 23/25 | Loss: 0.00068228
Iteration 24/25 | Loss: 0.00068228
Iteration 25/25 | Loss: 0.00068228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50801909
Iteration 2/25 | Loss: 0.00044658
Iteration 3/25 | Loss: 0.00044657
Iteration 4/25 | Loss: 0.00044657
Iteration 5/25 | Loss: 0.00044657
Iteration 6/25 | Loss: 0.00044657
Iteration 7/25 | Loss: 0.00044657
Iteration 8/25 | Loss: 0.00044657
Iteration 9/25 | Loss: 0.00044657
Iteration 10/25 | Loss: 0.00044657
Iteration 11/25 | Loss: 0.00044657
Iteration 12/25 | Loss: 0.00044657
Iteration 13/25 | Loss: 0.00044657
Iteration 14/25 | Loss: 0.00044657
Iteration 15/25 | Loss: 0.00044657
Iteration 16/25 | Loss: 0.00044657
Iteration 17/25 | Loss: 0.00044657
Iteration 18/25 | Loss: 0.00044657
Iteration 19/25 | Loss: 0.00044657
Iteration 20/25 | Loss: 0.00044657
Iteration 21/25 | Loss: 0.00044657
Iteration 22/25 | Loss: 0.00044657
Iteration 23/25 | Loss: 0.00044657
Iteration 24/25 | Loss: 0.00044657
Iteration 25/25 | Loss: 0.00044657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044657
Iteration 2/1000 | Loss: 0.00002088
Iteration 3/1000 | Loss: 0.00001258
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001079
Iteration 6/1000 | Loss: 0.00001041
Iteration 7/1000 | Loss: 0.00001030
Iteration 8/1000 | Loss: 0.00001029
Iteration 9/1000 | Loss: 0.00001027
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001014
Iteration 15/1000 | Loss: 0.00001010
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001007
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00000996
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000992
Iteration 24/1000 | Loss: 0.00000992
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000977
Iteration 54/1000 | Loss: 0.00000977
Iteration 55/1000 | Loss: 0.00000977
Iteration 56/1000 | Loss: 0.00000977
Iteration 57/1000 | Loss: 0.00000977
Iteration 58/1000 | Loss: 0.00000977
Iteration 59/1000 | Loss: 0.00000977
Iteration 60/1000 | Loss: 0.00000977
Iteration 61/1000 | Loss: 0.00000977
Iteration 62/1000 | Loss: 0.00000977
Iteration 63/1000 | Loss: 0.00000977
Iteration 64/1000 | Loss: 0.00000977
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000977
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [9.765549293661024e-06, 9.765549293661024e-06, 9.765549293661024e-06, 9.765549293661024e-06, 9.765549293661024e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.765549293661024e-06

Optimization complete. Final v2v error: 2.6747725009918213 mm

Highest mean error: 2.835836410522461 mm for frame 146

Lowest mean error: 2.6359364986419678 mm for frame 154

Saving results

Total time: 28.21794557571411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089053
Iteration 2/25 | Loss: 0.01089053
Iteration 3/25 | Loss: 0.00222052
Iteration 4/25 | Loss: 0.00152364
Iteration 5/25 | Loss: 0.00126730
Iteration 6/25 | Loss: 0.00116518
Iteration 7/25 | Loss: 0.00121862
Iteration 8/25 | Loss: 0.00119086
Iteration 9/25 | Loss: 0.00105198
Iteration 10/25 | Loss: 0.00100502
Iteration 11/25 | Loss: 0.00096391
Iteration 12/25 | Loss: 0.00091660
Iteration 13/25 | Loss: 0.00088536
Iteration 14/25 | Loss: 0.00088271
Iteration 15/25 | Loss: 0.00085839
Iteration 16/25 | Loss: 0.00085229
Iteration 17/25 | Loss: 0.00085437
Iteration 18/25 | Loss: 0.00085778
Iteration 19/25 | Loss: 0.00085283
Iteration 20/25 | Loss: 0.00086544
Iteration 21/25 | Loss: 0.00087435
Iteration 22/25 | Loss: 0.00087348
Iteration 23/25 | Loss: 0.00086418
Iteration 24/25 | Loss: 0.00085905
Iteration 25/25 | Loss: 0.00086179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48272145
Iteration 2/25 | Loss: 0.00143922
Iteration 3/25 | Loss: 0.00113955
Iteration 4/25 | Loss: 0.00113955
Iteration 5/25 | Loss: 0.00113955
Iteration 6/25 | Loss: 0.00113955
Iteration 7/25 | Loss: 0.00113955
Iteration 8/25 | Loss: 0.00113955
Iteration 9/25 | Loss: 0.00113955
Iteration 10/25 | Loss: 0.00113955
Iteration 11/25 | Loss: 0.00113955
Iteration 12/25 | Loss: 0.00113955
Iteration 13/25 | Loss: 0.00113955
Iteration 14/25 | Loss: 0.00113955
Iteration 15/25 | Loss: 0.00113955
Iteration 16/25 | Loss: 0.00113955
Iteration 17/25 | Loss: 0.00113955
Iteration 18/25 | Loss: 0.00113955
Iteration 19/25 | Loss: 0.00113955
Iteration 20/25 | Loss: 0.00113955
Iteration 21/25 | Loss: 0.00113955
Iteration 22/25 | Loss: 0.00113955
Iteration 23/25 | Loss: 0.00113955
Iteration 24/25 | Loss: 0.00113955
Iteration 25/25 | Loss: 0.00113955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113955
Iteration 2/1000 | Loss: 0.00087236
Iteration 3/1000 | Loss: 0.00085172
Iteration 4/1000 | Loss: 0.00082062
Iteration 5/1000 | Loss: 0.00083305
Iteration 6/1000 | Loss: 0.00077753
Iteration 7/1000 | Loss: 0.00088736
Iteration 8/1000 | Loss: 0.00109102
Iteration 9/1000 | Loss: 0.00110939
Iteration 10/1000 | Loss: 0.00084517
Iteration 11/1000 | Loss: 0.00092469
Iteration 12/1000 | Loss: 0.00084470
Iteration 13/1000 | Loss: 0.00115642
Iteration 14/1000 | Loss: 0.00093251
Iteration 15/1000 | Loss: 0.00107436
Iteration 16/1000 | Loss: 0.00098341
Iteration 17/1000 | Loss: 0.00085512
Iteration 18/1000 | Loss: 0.00088019
Iteration 19/1000 | Loss: 0.00061858
Iteration 20/1000 | Loss: 0.00071982
Iteration 21/1000 | Loss: 0.00069609
Iteration 22/1000 | Loss: 0.00082685
Iteration 23/1000 | Loss: 0.00066206
Iteration 24/1000 | Loss: 0.00064691
Iteration 25/1000 | Loss: 0.00062557
Iteration 26/1000 | Loss: 0.00066574
Iteration 27/1000 | Loss: 0.00078471
Iteration 28/1000 | Loss: 0.00072801
Iteration 29/1000 | Loss: 0.00066375
Iteration 30/1000 | Loss: 0.00082604
Iteration 31/1000 | Loss: 0.00089411
Iteration 32/1000 | Loss: 0.00070174
Iteration 33/1000 | Loss: 0.00076130
Iteration 34/1000 | Loss: 0.00077136
Iteration 35/1000 | Loss: 0.00059905
Iteration 36/1000 | Loss: 0.00071334
Iteration 37/1000 | Loss: 0.00076839
Iteration 38/1000 | Loss: 0.00062891
Iteration 39/1000 | Loss: 0.00091074
Iteration 40/1000 | Loss: 0.00090151
Iteration 41/1000 | Loss: 0.00089503
Iteration 42/1000 | Loss: 0.00089223
Iteration 43/1000 | Loss: 0.00057629
Iteration 44/1000 | Loss: 0.00043839
Iteration 45/1000 | Loss: 0.00025565
Iteration 46/1000 | Loss: 0.00047289
Iteration 47/1000 | Loss: 0.00063979
Iteration 48/1000 | Loss: 0.00056813
Iteration 49/1000 | Loss: 0.00055414
Iteration 50/1000 | Loss: 0.00024115
Iteration 51/1000 | Loss: 0.00053286
Iteration 52/1000 | Loss: 0.00044946
Iteration 53/1000 | Loss: 0.00035013
Iteration 54/1000 | Loss: 0.00034889
Iteration 55/1000 | Loss: 0.00037161
Iteration 56/1000 | Loss: 0.00036464
Iteration 57/1000 | Loss: 0.00040330
Iteration 58/1000 | Loss: 0.00043095
Iteration 59/1000 | Loss: 0.00044488
Iteration 60/1000 | Loss: 0.00028878
Iteration 61/1000 | Loss: 0.00026654
Iteration 62/1000 | Loss: 0.00023727
Iteration 63/1000 | Loss: 0.00042504
Iteration 64/1000 | Loss: 0.00038495
Iteration 65/1000 | Loss: 0.00041589
Iteration 66/1000 | Loss: 0.00036176
Iteration 67/1000 | Loss: 0.00039127
Iteration 68/1000 | Loss: 0.00039357
Iteration 69/1000 | Loss: 0.00024871
Iteration 70/1000 | Loss: 0.00026305
Iteration 71/1000 | Loss: 0.00031072
Iteration 72/1000 | Loss: 0.00025078
Iteration 73/1000 | Loss: 0.00037474
Iteration 74/1000 | Loss: 0.00034293
Iteration 75/1000 | Loss: 0.00031089
Iteration 76/1000 | Loss: 0.00040892
Iteration 77/1000 | Loss: 0.00060964
Iteration 78/1000 | Loss: 0.00061406
Iteration 79/1000 | Loss: 0.00014963
Iteration 80/1000 | Loss: 0.00025883
Iteration 81/1000 | Loss: 0.00036012
Iteration 82/1000 | Loss: 0.00020270
Iteration 83/1000 | Loss: 0.00025783
Iteration 84/1000 | Loss: 0.00019551
Iteration 85/1000 | Loss: 0.00024725
Iteration 86/1000 | Loss: 0.00022401
Iteration 87/1000 | Loss: 0.00023725
Iteration 88/1000 | Loss: 0.00019740
Iteration 89/1000 | Loss: 0.00033340
Iteration 90/1000 | Loss: 0.00027867
Iteration 91/1000 | Loss: 0.00014185
Iteration 92/1000 | Loss: 0.00013650
Iteration 93/1000 | Loss: 0.00014053
Iteration 94/1000 | Loss: 0.00019755
Iteration 95/1000 | Loss: 0.00022855
Iteration 96/1000 | Loss: 0.00026362
Iteration 97/1000 | Loss: 0.00032328
Iteration 98/1000 | Loss: 0.00033462
Iteration 99/1000 | Loss: 0.00039028
Iteration 100/1000 | Loss: 0.00032825
Iteration 101/1000 | Loss: 0.00029242
Iteration 102/1000 | Loss: 0.00034924
Iteration 103/1000 | Loss: 0.00037703
Iteration 104/1000 | Loss: 0.00035402
Iteration 105/1000 | Loss: 0.00035595
Iteration 106/1000 | Loss: 0.00037422
Iteration 107/1000 | Loss: 0.00042363
Iteration 108/1000 | Loss: 0.00036149
Iteration 109/1000 | Loss: 0.00050594
Iteration 110/1000 | Loss: 0.00041499
Iteration 111/1000 | Loss: 0.00037581
Iteration 112/1000 | Loss: 0.00040365
Iteration 113/1000 | Loss: 0.00038018
Iteration 114/1000 | Loss: 0.00053619
Iteration 115/1000 | Loss: 0.00053423
Iteration 116/1000 | Loss: 0.00064314
Iteration 117/1000 | Loss: 0.00064439
Iteration 118/1000 | Loss: 0.00043770
Iteration 119/1000 | Loss: 0.00040896
Iteration 120/1000 | Loss: 0.00034636
Iteration 121/1000 | Loss: 0.00034429
Iteration 122/1000 | Loss: 0.00045054
Iteration 123/1000 | Loss: 0.00040255
Iteration 124/1000 | Loss: 0.00048633
Iteration 125/1000 | Loss: 0.00043440
Iteration 126/1000 | Loss: 0.00030426
Iteration 127/1000 | Loss: 0.00037413
Iteration 128/1000 | Loss: 0.00037154
Iteration 129/1000 | Loss: 0.00050651
Iteration 130/1000 | Loss: 0.00018347
Iteration 131/1000 | Loss: 0.00037744
Iteration 132/1000 | Loss: 0.00033676
Iteration 133/1000 | Loss: 0.00028565
Iteration 134/1000 | Loss: 0.00037455
Iteration 135/1000 | Loss: 0.00028841
Iteration 136/1000 | Loss: 0.00037610
Iteration 137/1000 | Loss: 0.00030858
Iteration 138/1000 | Loss: 0.00027606
Iteration 139/1000 | Loss: 0.00032072
Iteration 140/1000 | Loss: 0.00033351
Iteration 141/1000 | Loss: 0.00039776
Iteration 142/1000 | Loss: 0.00020733
Iteration 143/1000 | Loss: 0.00020229
Iteration 144/1000 | Loss: 0.00037003
Iteration 145/1000 | Loss: 0.00067380
Iteration 146/1000 | Loss: 0.00010318
Iteration 147/1000 | Loss: 0.00018224
Iteration 148/1000 | Loss: 0.00020683
Iteration 149/1000 | Loss: 0.00022174
Iteration 150/1000 | Loss: 0.00015287
Iteration 151/1000 | Loss: 0.00022254
Iteration 152/1000 | Loss: 0.00025627
Iteration 153/1000 | Loss: 0.00026364
Iteration 154/1000 | Loss: 0.00034855
Iteration 155/1000 | Loss: 0.00029168
Iteration 156/1000 | Loss: 0.00012974
Iteration 157/1000 | Loss: 0.00044162
Iteration 158/1000 | Loss: 0.00027066
Iteration 159/1000 | Loss: 0.00008394
Iteration 160/1000 | Loss: 0.00038912
Iteration 161/1000 | Loss: 0.00032278
Iteration 162/1000 | Loss: 0.00043933
Iteration 163/1000 | Loss: 0.00037009
Iteration 164/1000 | Loss: 0.00017822
Iteration 165/1000 | Loss: 0.00037825
Iteration 166/1000 | Loss: 0.00030157
Iteration 167/1000 | Loss: 0.00013508
Iteration 168/1000 | Loss: 0.00027952
Iteration 169/1000 | Loss: 0.00033139
Iteration 170/1000 | Loss: 0.00031688
Iteration 171/1000 | Loss: 0.00057106
Iteration 172/1000 | Loss: 0.00028181
Iteration 173/1000 | Loss: 0.00041793
Iteration 174/1000 | Loss: 0.00115377
Iteration 175/1000 | Loss: 0.00035438
Iteration 176/1000 | Loss: 0.00012054
Iteration 177/1000 | Loss: 0.00024619
Iteration 178/1000 | Loss: 0.00062020
Iteration 179/1000 | Loss: 0.00050110
Iteration 180/1000 | Loss: 0.00047379
Iteration 181/1000 | Loss: 0.00041156
Iteration 182/1000 | Loss: 0.00025822
Iteration 183/1000 | Loss: 0.00033605
Iteration 184/1000 | Loss: 0.00032972
Iteration 185/1000 | Loss: 0.00074172
Iteration 186/1000 | Loss: 0.00065020
Iteration 187/1000 | Loss: 0.00042091
Iteration 188/1000 | Loss: 0.00039482
Iteration 189/1000 | Loss: 0.00040149
Iteration 190/1000 | Loss: 0.00088261
Iteration 191/1000 | Loss: 0.00053457
Iteration 192/1000 | Loss: 0.00074176
Iteration 193/1000 | Loss: 0.00056248
Iteration 194/1000 | Loss: 0.00042328
Iteration 195/1000 | Loss: 0.00159812
Iteration 196/1000 | Loss: 0.00060640
Iteration 197/1000 | Loss: 0.00025586
Iteration 198/1000 | Loss: 0.00028886
Iteration 199/1000 | Loss: 0.00043197
Iteration 200/1000 | Loss: 0.00020249
Iteration 201/1000 | Loss: 0.00028640
Iteration 202/1000 | Loss: 0.00028664
Iteration 203/1000 | Loss: 0.00030225
Iteration 204/1000 | Loss: 0.00034482
Iteration 205/1000 | Loss: 0.00033940
Iteration 206/1000 | Loss: 0.00033036
Iteration 207/1000 | Loss: 0.00037284
Iteration 208/1000 | Loss: 0.00021880
Iteration 209/1000 | Loss: 0.00030387
Iteration 210/1000 | Loss: 0.00029102
Iteration 211/1000 | Loss: 0.00028545
Iteration 212/1000 | Loss: 0.00026199
Iteration 213/1000 | Loss: 0.00040887
Iteration 214/1000 | Loss: 0.00036557
Iteration 215/1000 | Loss: 0.00011490
Iteration 216/1000 | Loss: 0.00025669
Iteration 217/1000 | Loss: 0.00019438
Iteration 218/1000 | Loss: 0.00014038
Iteration 219/1000 | Loss: 0.00015747
Iteration 220/1000 | Loss: 0.00023491
Iteration 221/1000 | Loss: 0.00025621
Iteration 222/1000 | Loss: 0.00026417
Iteration 223/1000 | Loss: 0.00023598
Iteration 224/1000 | Loss: 0.00025739
Iteration 225/1000 | Loss: 0.00024380
Iteration 226/1000 | Loss: 0.00027690
Iteration 227/1000 | Loss: 0.00030245
Iteration 228/1000 | Loss: 0.00033453
Iteration 229/1000 | Loss: 0.00037940
Iteration 230/1000 | Loss: 0.00028813
Iteration 231/1000 | Loss: 0.00030877
Iteration 232/1000 | Loss: 0.00027421
Iteration 233/1000 | Loss: 0.00026412
Iteration 234/1000 | Loss: 0.00017582
Iteration 235/1000 | Loss: 0.00025292
Iteration 236/1000 | Loss: 0.00012699
Iteration 237/1000 | Loss: 0.00027162
Iteration 238/1000 | Loss: 0.00021483
Iteration 239/1000 | Loss: 0.00024127
Iteration 240/1000 | Loss: 0.00038799
Iteration 241/1000 | Loss: 0.00042381
Iteration 242/1000 | Loss: 0.00027437
Iteration 243/1000 | Loss: 0.00026074
Iteration 244/1000 | Loss: 0.00038182
Iteration 245/1000 | Loss: 0.00049239
Iteration 246/1000 | Loss: 0.00027142
Iteration 247/1000 | Loss: 0.00028653
Iteration 248/1000 | Loss: 0.00021988
Iteration 249/1000 | Loss: 0.00024776
Iteration 250/1000 | Loss: 0.00024222
Iteration 251/1000 | Loss: 0.00026477
Iteration 252/1000 | Loss: 0.00029360
Iteration 253/1000 | Loss: 0.00026190
Iteration 254/1000 | Loss: 0.00028106
Iteration 255/1000 | Loss: 0.00025629
Iteration 256/1000 | Loss: 0.00027863
Iteration 257/1000 | Loss: 0.00033746
Iteration 258/1000 | Loss: 0.00031483
Iteration 259/1000 | Loss: 0.00034162
Iteration 260/1000 | Loss: 0.00043246
Iteration 261/1000 | Loss: 0.00033961
Iteration 262/1000 | Loss: 0.00038208
Iteration 263/1000 | Loss: 0.00034038
Iteration 264/1000 | Loss: 0.00037059
Iteration 265/1000 | Loss: 0.00023928
Iteration 266/1000 | Loss: 0.00023359
Iteration 267/1000 | Loss: 0.00035459
Iteration 268/1000 | Loss: 0.00018877
Iteration 269/1000 | Loss: 0.00015124
Iteration 270/1000 | Loss: 0.00014828
Iteration 271/1000 | Loss: 0.00021244
Iteration 272/1000 | Loss: 0.00064888
Iteration 273/1000 | Loss: 0.00026448
Iteration 274/1000 | Loss: 0.00046650
Iteration 275/1000 | Loss: 0.00027979
Iteration 276/1000 | Loss: 0.00020581
Iteration 277/1000 | Loss: 0.00027615
Iteration 278/1000 | Loss: 0.00029516
Iteration 279/1000 | Loss: 0.00022578
Iteration 280/1000 | Loss: 0.00018530
Iteration 281/1000 | Loss: 0.00031770
Iteration 282/1000 | Loss: 0.00038388
Iteration 283/1000 | Loss: 0.00034741
Iteration 284/1000 | Loss: 0.00027470
Iteration 285/1000 | Loss: 0.00033825
Iteration 286/1000 | Loss: 0.00035242
Iteration 287/1000 | Loss: 0.00044932
Iteration 288/1000 | Loss: 0.00043412
Iteration 289/1000 | Loss: 0.00037288
Iteration 290/1000 | Loss: 0.00039131
Iteration 291/1000 | Loss: 0.00042417
Iteration 292/1000 | Loss: 0.00019174
Iteration 293/1000 | Loss: 0.00030434
Iteration 294/1000 | Loss: 0.00028097
Iteration 295/1000 | Loss: 0.00021343
Iteration 296/1000 | Loss: 0.00019184
Iteration 297/1000 | Loss: 0.00026812
Iteration 298/1000 | Loss: 0.00017704
Iteration 299/1000 | Loss: 0.00028062
Iteration 300/1000 | Loss: 0.00028528
Iteration 301/1000 | Loss: 0.00044519
Iteration 302/1000 | Loss: 0.00030036
Iteration 303/1000 | Loss: 0.00016145
Iteration 304/1000 | Loss: 0.00015939
Iteration 305/1000 | Loss: 0.00017167
Iteration 306/1000 | Loss: 0.00011028
Iteration 307/1000 | Loss: 0.00009737
Iteration 308/1000 | Loss: 0.00021990
Iteration 309/1000 | Loss: 0.00017977
Iteration 310/1000 | Loss: 0.00020924
Iteration 311/1000 | Loss: 0.00019636
Iteration 312/1000 | Loss: 0.00044914
Iteration 313/1000 | Loss: 0.00047519
Iteration 314/1000 | Loss: 0.00032898
Iteration 315/1000 | Loss: 0.00030484
Iteration 316/1000 | Loss: 0.00077956
Iteration 317/1000 | Loss: 0.00019168
Iteration 318/1000 | Loss: 0.00066980
Iteration 319/1000 | Loss: 0.00025126
Iteration 320/1000 | Loss: 0.00017526
Iteration 321/1000 | Loss: 0.00014501
Iteration 322/1000 | Loss: 0.00015818
Iteration 323/1000 | Loss: 0.00031010
Iteration 324/1000 | Loss: 0.00024236
Iteration 325/1000 | Loss: 0.00028992
Iteration 326/1000 | Loss: 0.00018107
Iteration 327/1000 | Loss: 0.00024174
Iteration 328/1000 | Loss: 0.00032694
Iteration 329/1000 | Loss: 0.00041296
Iteration 330/1000 | Loss: 0.00025062
Iteration 331/1000 | Loss: 0.00023505
Iteration 332/1000 | Loss: 0.00029183
Iteration 333/1000 | Loss: 0.00025658
Iteration 334/1000 | Loss: 0.00030612
Iteration 335/1000 | Loss: 0.00066645
Iteration 336/1000 | Loss: 0.00024137
Iteration 337/1000 | Loss: 0.00009834
Iteration 338/1000 | Loss: 0.00004050
Iteration 339/1000 | Loss: 0.00012738
Iteration 340/1000 | Loss: 0.00006326
Iteration 341/1000 | Loss: 0.00006529
Iteration 342/1000 | Loss: 0.00006464
Iteration 343/1000 | Loss: 0.00006466
Iteration 344/1000 | Loss: 0.00004739
Iteration 345/1000 | Loss: 0.00007041
Iteration 346/1000 | Loss: 0.00006855
Iteration 347/1000 | Loss: 0.00006766
Iteration 348/1000 | Loss: 0.00007548
Iteration 349/1000 | Loss: 0.00007270
Iteration 350/1000 | Loss: 0.00007461
Iteration 351/1000 | Loss: 0.00007086
Iteration 352/1000 | Loss: 0.00006777
Iteration 353/1000 | Loss: 0.00006076
Iteration 354/1000 | Loss: 0.00007505
Iteration 355/1000 | Loss: 0.00006884
Iteration 356/1000 | Loss: 0.00007072
Iteration 357/1000 | Loss: 0.00010802
Iteration 358/1000 | Loss: 0.00007058
Iteration 359/1000 | Loss: 0.00005429
Iteration 360/1000 | Loss: 0.00006687
Iteration 361/1000 | Loss: 0.00003853
Iteration 362/1000 | Loss: 0.00007118
Iteration 363/1000 | Loss: 0.00007437
Iteration 364/1000 | Loss: 0.00007814
Iteration 365/1000 | Loss: 0.00006527
Iteration 366/1000 | Loss: 0.00007290
Iteration 367/1000 | Loss: 0.00008148
Iteration 368/1000 | Loss: 0.00006968
Iteration 369/1000 | Loss: 0.00005367
Iteration 370/1000 | Loss: 0.00005576
Iteration 371/1000 | Loss: 0.00005907
Iteration 372/1000 | Loss: 0.00004947
Iteration 373/1000 | Loss: 0.00007138
Iteration 374/1000 | Loss: 0.00007058
Iteration 375/1000 | Loss: 0.00006055
Iteration 376/1000 | Loss: 0.00006933
Iteration 377/1000 | Loss: 0.00006717
Iteration 378/1000 | Loss: 0.00006735
Iteration 379/1000 | Loss: 0.00007341
Iteration 380/1000 | Loss: 0.00006931
Iteration 381/1000 | Loss: 0.00007079
Iteration 382/1000 | Loss: 0.00006770
Iteration 383/1000 | Loss: 0.00008678
Iteration 384/1000 | Loss: 0.00007074
Iteration 385/1000 | Loss: 0.00006199
Iteration 386/1000 | Loss: 0.00006550
Iteration 387/1000 | Loss: 0.00006651
Iteration 388/1000 | Loss: 0.00007150
Iteration 389/1000 | Loss: 0.00007008
Iteration 390/1000 | Loss: 0.00006357
Iteration 391/1000 | Loss: 0.00005926
Iteration 392/1000 | Loss: 0.00007860
Iteration 393/1000 | Loss: 0.00006341
Iteration 394/1000 | Loss: 0.00006322
Iteration 395/1000 | Loss: 0.00005960
Iteration 396/1000 | Loss: 0.00005871
Iteration 397/1000 | Loss: 0.00006334
Iteration 398/1000 | Loss: 0.00005448
Iteration 399/1000 | Loss: 0.00004798
Iteration 400/1000 | Loss: 0.00005592
Iteration 401/1000 | Loss: 0.00006379
Iteration 402/1000 | Loss: 0.00006065
Iteration 403/1000 | Loss: 0.00006814
Iteration 404/1000 | Loss: 0.00006261
Iteration 405/1000 | Loss: 0.00010768
Iteration 406/1000 | Loss: 0.00006483
Iteration 407/1000 | Loss: 0.00006464
Iteration 408/1000 | Loss: 0.00007187
Iteration 409/1000 | Loss: 0.00006079
Iteration 410/1000 | Loss: 0.00006775
Iteration 411/1000 | Loss: 0.00006126
Iteration 412/1000 | Loss: 0.00007209
Iteration 413/1000 | Loss: 0.00006643
Iteration 414/1000 | Loss: 0.00007409
Iteration 415/1000 | Loss: 0.00005955
Iteration 416/1000 | Loss: 0.00005593
Iteration 417/1000 | Loss: 0.00005967
Iteration 418/1000 | Loss: 0.00006422
Iteration 419/1000 | Loss: 0.00007640
Iteration 420/1000 | Loss: 0.00005275
Iteration 421/1000 | Loss: 0.00003296
Iteration 422/1000 | Loss: 0.00002865
Iteration 423/1000 | Loss: 0.00002528
Iteration 424/1000 | Loss: 0.00002354
Iteration 425/1000 | Loss: 0.00002268
Iteration 426/1000 | Loss: 0.00002191
Iteration 427/1000 | Loss: 0.00002135
Iteration 428/1000 | Loss: 0.00002097
Iteration 429/1000 | Loss: 0.00002074
Iteration 430/1000 | Loss: 0.00002049
Iteration 431/1000 | Loss: 0.00002045
Iteration 432/1000 | Loss: 0.00002029
Iteration 433/1000 | Loss: 0.00002017
Iteration 434/1000 | Loss: 0.00002016
Iteration 435/1000 | Loss: 0.00002015
Iteration 436/1000 | Loss: 0.00002011
Iteration 437/1000 | Loss: 0.00002010
Iteration 438/1000 | Loss: 0.00002009
Iteration 439/1000 | Loss: 0.00002008
Iteration 440/1000 | Loss: 0.00002007
Iteration 441/1000 | Loss: 0.00010502
Iteration 442/1000 | Loss: 0.00002505
Iteration 443/1000 | Loss: 0.00002266
Iteration 444/1000 | Loss: 0.00013293
Iteration 445/1000 | Loss: 0.00004274
Iteration 446/1000 | Loss: 0.00002585
Iteration 447/1000 | Loss: 0.00004617
Iteration 448/1000 | Loss: 0.00010609
Iteration 449/1000 | Loss: 0.00002812
Iteration 450/1000 | Loss: 0.00003119
Iteration 451/1000 | Loss: 0.00010047
Iteration 452/1000 | Loss: 0.00018938
Iteration 453/1000 | Loss: 0.00014293
Iteration 454/1000 | Loss: 0.00014339
Iteration 455/1000 | Loss: 0.00010500
Iteration 456/1000 | Loss: 0.00008475
Iteration 457/1000 | Loss: 0.00016423
Iteration 458/1000 | Loss: 0.00012140
Iteration 459/1000 | Loss: 0.00005900
Iteration 460/1000 | Loss: 0.00007584
Iteration 461/1000 | Loss: 0.00015325
Iteration 462/1000 | Loss: 0.00023979
Iteration 463/1000 | Loss: 0.00018509
Iteration 464/1000 | Loss: 0.00016619
Iteration 465/1000 | Loss: 0.00024141
Iteration 466/1000 | Loss: 0.00007407
Iteration 467/1000 | Loss: 0.00017521
Iteration 468/1000 | Loss: 0.00020540
Iteration 469/1000 | Loss: 0.00018615
Iteration 470/1000 | Loss: 0.00017654
Iteration 471/1000 | Loss: 0.00013507
Iteration 472/1000 | Loss: 0.00015860
Iteration 473/1000 | Loss: 0.00021283
Iteration 474/1000 | Loss: 0.00002674
Iteration 475/1000 | Loss: 0.00002262
Iteration 476/1000 | Loss: 0.00002070
Iteration 477/1000 | Loss: 0.00001990
Iteration 478/1000 | Loss: 0.00001956
Iteration 479/1000 | Loss: 0.00001949
Iteration 480/1000 | Loss: 0.00001948
Iteration 481/1000 | Loss: 0.00001948
Iteration 482/1000 | Loss: 0.00001947
Iteration 483/1000 | Loss: 0.00001946
Iteration 484/1000 | Loss: 0.00001944
Iteration 485/1000 | Loss: 0.00001942
Iteration 486/1000 | Loss: 0.00001942
Iteration 487/1000 | Loss: 0.00001942
Iteration 488/1000 | Loss: 0.00001942
Iteration 489/1000 | Loss: 0.00001942
Iteration 490/1000 | Loss: 0.00001942
Iteration 491/1000 | Loss: 0.00001942
Iteration 492/1000 | Loss: 0.00001941
Iteration 493/1000 | Loss: 0.00001941
Iteration 494/1000 | Loss: 0.00001941
Iteration 495/1000 | Loss: 0.00001941
Iteration 496/1000 | Loss: 0.00001941
Iteration 497/1000 | Loss: 0.00001941
Iteration 498/1000 | Loss: 0.00001941
Iteration 499/1000 | Loss: 0.00001940
Iteration 500/1000 | Loss: 0.00001939
Iteration 501/1000 | Loss: 0.00001938
Iteration 502/1000 | Loss: 0.00001938
Iteration 503/1000 | Loss: 0.00001938
Iteration 504/1000 | Loss: 0.00001938
Iteration 505/1000 | Loss: 0.00001937
Iteration 506/1000 | Loss: 0.00001937
Iteration 507/1000 | Loss: 0.00001936
Iteration 508/1000 | Loss: 0.00001936
Iteration 509/1000 | Loss: 0.00001935
Iteration 510/1000 | Loss: 0.00001935
Iteration 511/1000 | Loss: 0.00001935
Iteration 512/1000 | Loss: 0.00001935
Iteration 513/1000 | Loss: 0.00001935
Iteration 514/1000 | Loss: 0.00001935
Iteration 515/1000 | Loss: 0.00001934
Iteration 516/1000 | Loss: 0.00001934
Iteration 517/1000 | Loss: 0.00001934
Iteration 518/1000 | Loss: 0.00001934
Iteration 519/1000 | Loss: 0.00001934
Iteration 520/1000 | Loss: 0.00001934
Iteration 521/1000 | Loss: 0.00001934
Iteration 522/1000 | Loss: 0.00001934
Iteration 523/1000 | Loss: 0.00001934
Iteration 524/1000 | Loss: 0.00001934
Iteration 525/1000 | Loss: 0.00001934
Iteration 526/1000 | Loss: 0.00001934
Iteration 527/1000 | Loss: 0.00001934
Iteration 528/1000 | Loss: 0.00001934
Iteration 529/1000 | Loss: 0.00001933
Iteration 530/1000 | Loss: 0.00001933
Iteration 531/1000 | Loss: 0.00001933
Iteration 532/1000 | Loss: 0.00001933
Iteration 533/1000 | Loss: 0.00001933
Iteration 534/1000 | Loss: 0.00001933
Iteration 535/1000 | Loss: 0.00001933
Iteration 536/1000 | Loss: 0.00001933
Iteration 537/1000 | Loss: 0.00001933
Iteration 538/1000 | Loss: 0.00001933
Iteration 539/1000 | Loss: 0.00001933
Iteration 540/1000 | Loss: 0.00001932
Iteration 541/1000 | Loss: 0.00001932
Iteration 542/1000 | Loss: 0.00001932
Iteration 543/1000 | Loss: 0.00001932
Iteration 544/1000 | Loss: 0.00001932
Iteration 545/1000 | Loss: 0.00001932
Iteration 546/1000 | Loss: 0.00001932
Iteration 547/1000 | Loss: 0.00001932
Iteration 548/1000 | Loss: 0.00001932
Iteration 549/1000 | Loss: 0.00001932
Iteration 550/1000 | Loss: 0.00001932
Iteration 551/1000 | Loss: 0.00001932
Iteration 552/1000 | Loss: 0.00001932
Iteration 553/1000 | Loss: 0.00001931
Iteration 554/1000 | Loss: 0.00001931
Iteration 555/1000 | Loss: 0.00001931
Iteration 556/1000 | Loss: 0.00001931
Iteration 557/1000 | Loss: 0.00001931
Iteration 558/1000 | Loss: 0.00001931
Iteration 559/1000 | Loss: 0.00001931
Iteration 560/1000 | Loss: 0.00001931
Iteration 561/1000 | Loss: 0.00001931
Iteration 562/1000 | Loss: 0.00001931
Iteration 563/1000 | Loss: 0.00001931
Iteration 564/1000 | Loss: 0.00001931
Iteration 565/1000 | Loss: 0.00001931
Iteration 566/1000 | Loss: 0.00001930
Iteration 567/1000 | Loss: 0.00001930
Iteration 568/1000 | Loss: 0.00001930
Iteration 569/1000 | Loss: 0.00001930
Iteration 570/1000 | Loss: 0.00001930
Iteration 571/1000 | Loss: 0.00001930
Iteration 572/1000 | Loss: 0.00001930
Iteration 573/1000 | Loss: 0.00001930
Iteration 574/1000 | Loss: 0.00001930
Iteration 575/1000 | Loss: 0.00001930
Iteration 576/1000 | Loss: 0.00001930
Iteration 577/1000 | Loss: 0.00001930
Iteration 578/1000 | Loss: 0.00001930
Iteration 579/1000 | Loss: 0.00001930
Iteration 580/1000 | Loss: 0.00001930
Iteration 581/1000 | Loss: 0.00001930
Iteration 582/1000 | Loss: 0.00001930
Iteration 583/1000 | Loss: 0.00001930
Iteration 584/1000 | Loss: 0.00001930
Iteration 585/1000 | Loss: 0.00001930
Iteration 586/1000 | Loss: 0.00001930
Iteration 587/1000 | Loss: 0.00001930
Iteration 588/1000 | Loss: 0.00001930
Iteration 589/1000 | Loss: 0.00001930
Iteration 590/1000 | Loss: 0.00001930
Iteration 591/1000 | Loss: 0.00001930
Iteration 592/1000 | Loss: 0.00001930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 592. Stopping optimization.
Last 5 losses: [1.9303999579278752e-05, 1.9303999579278752e-05, 1.9303999579278752e-05, 1.9303999579278752e-05, 1.9303999579278752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9303999579278752e-05

Optimization complete. Final v2v error: 3.5816335678100586 mm

Highest mean error: 7.283358097076416 mm for frame 54

Lowest mean error: 3.1040539741516113 mm for frame 133

Saving results

Total time: 812.4602530002594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514089
Iteration 2/25 | Loss: 0.00129347
Iteration 3/25 | Loss: 0.00087440
Iteration 4/25 | Loss: 0.00083875
Iteration 5/25 | Loss: 0.00082981
Iteration 6/25 | Loss: 0.00082802
Iteration 7/25 | Loss: 0.00082792
Iteration 8/25 | Loss: 0.00082792
Iteration 9/25 | Loss: 0.00082792
Iteration 10/25 | Loss: 0.00082792
Iteration 11/25 | Loss: 0.00082792
Iteration 12/25 | Loss: 0.00082792
Iteration 13/25 | Loss: 0.00082792
Iteration 14/25 | Loss: 0.00082792
Iteration 15/25 | Loss: 0.00082792
Iteration 16/25 | Loss: 0.00082792
Iteration 17/25 | Loss: 0.00082792
Iteration 18/25 | Loss: 0.00082792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000827915733680129, 0.000827915733680129, 0.000827915733680129, 0.000827915733680129, 0.000827915733680129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000827915733680129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15774143
Iteration 2/25 | Loss: 0.00052013
Iteration 3/25 | Loss: 0.00052013
Iteration 4/25 | Loss: 0.00052013
Iteration 5/25 | Loss: 0.00052013
Iteration 6/25 | Loss: 0.00052013
Iteration 7/25 | Loss: 0.00052013
Iteration 8/25 | Loss: 0.00052013
Iteration 9/25 | Loss: 0.00052013
Iteration 10/25 | Loss: 0.00052013
Iteration 11/25 | Loss: 0.00052013
Iteration 12/25 | Loss: 0.00052013
Iteration 13/25 | Loss: 0.00052013
Iteration 14/25 | Loss: 0.00052013
Iteration 15/25 | Loss: 0.00052013
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005201292224228382, 0.0005201292224228382, 0.0005201292224228382, 0.0005201292224228382, 0.0005201292224228382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005201292224228382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052013
Iteration 2/1000 | Loss: 0.00005206
Iteration 3/1000 | Loss: 0.00003706
Iteration 4/1000 | Loss: 0.00003129
Iteration 5/1000 | Loss: 0.00002851
Iteration 6/1000 | Loss: 0.00002721
Iteration 7/1000 | Loss: 0.00002577
Iteration 8/1000 | Loss: 0.00002524
Iteration 9/1000 | Loss: 0.00002437
Iteration 10/1000 | Loss: 0.00002391
Iteration 11/1000 | Loss: 0.00002366
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002312
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002268
Iteration 17/1000 | Loss: 0.00002248
Iteration 18/1000 | Loss: 0.00002245
Iteration 19/1000 | Loss: 0.00002244
Iteration 20/1000 | Loss: 0.00002243
Iteration 21/1000 | Loss: 0.00002242
Iteration 22/1000 | Loss: 0.00002241
Iteration 23/1000 | Loss: 0.00002230
Iteration 24/1000 | Loss: 0.00002225
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002211
Iteration 28/1000 | Loss: 0.00002211
Iteration 29/1000 | Loss: 0.00002210
Iteration 30/1000 | Loss: 0.00002210
Iteration 31/1000 | Loss: 0.00002209
Iteration 32/1000 | Loss: 0.00002207
Iteration 33/1000 | Loss: 0.00002207
Iteration 34/1000 | Loss: 0.00002206
Iteration 35/1000 | Loss: 0.00002206
Iteration 36/1000 | Loss: 0.00002206
Iteration 37/1000 | Loss: 0.00002205
Iteration 38/1000 | Loss: 0.00002205
Iteration 39/1000 | Loss: 0.00002205
Iteration 40/1000 | Loss: 0.00002205
Iteration 41/1000 | Loss: 0.00002205
Iteration 42/1000 | Loss: 0.00002204
Iteration 43/1000 | Loss: 0.00002203
Iteration 44/1000 | Loss: 0.00002203
Iteration 45/1000 | Loss: 0.00002202
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002202
Iteration 48/1000 | Loss: 0.00002201
Iteration 49/1000 | Loss: 0.00002201
Iteration 50/1000 | Loss: 0.00002201
Iteration 51/1000 | Loss: 0.00002200
Iteration 52/1000 | Loss: 0.00002200
Iteration 53/1000 | Loss: 0.00002200
Iteration 54/1000 | Loss: 0.00002200
Iteration 55/1000 | Loss: 0.00002199
Iteration 56/1000 | Loss: 0.00002199
Iteration 57/1000 | Loss: 0.00002199
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00002198
Iteration 60/1000 | Loss: 0.00002198
Iteration 61/1000 | Loss: 0.00002198
Iteration 62/1000 | Loss: 0.00002198
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002197
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002197
Iteration 68/1000 | Loss: 0.00002197
Iteration 69/1000 | Loss: 0.00002197
Iteration 70/1000 | Loss: 0.00002197
Iteration 71/1000 | Loss: 0.00002197
Iteration 72/1000 | Loss: 0.00002197
Iteration 73/1000 | Loss: 0.00002197
Iteration 74/1000 | Loss: 0.00002197
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002196
Iteration 78/1000 | Loss: 0.00002196
Iteration 79/1000 | Loss: 0.00002196
Iteration 80/1000 | Loss: 0.00002196
Iteration 81/1000 | Loss: 0.00002196
Iteration 82/1000 | Loss: 0.00002196
Iteration 83/1000 | Loss: 0.00002196
Iteration 84/1000 | Loss: 0.00002195
Iteration 85/1000 | Loss: 0.00002195
Iteration 86/1000 | Loss: 0.00002195
Iteration 87/1000 | Loss: 0.00002195
Iteration 88/1000 | Loss: 0.00002195
Iteration 89/1000 | Loss: 0.00002195
Iteration 90/1000 | Loss: 0.00002194
Iteration 91/1000 | Loss: 0.00002194
Iteration 92/1000 | Loss: 0.00002194
Iteration 93/1000 | Loss: 0.00002194
Iteration 94/1000 | Loss: 0.00002194
Iteration 95/1000 | Loss: 0.00002194
Iteration 96/1000 | Loss: 0.00002194
Iteration 97/1000 | Loss: 0.00002194
Iteration 98/1000 | Loss: 0.00002194
Iteration 99/1000 | Loss: 0.00002194
Iteration 100/1000 | Loss: 0.00002194
Iteration 101/1000 | Loss: 0.00002194
Iteration 102/1000 | Loss: 0.00002194
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002193
Iteration 106/1000 | Loss: 0.00002193
Iteration 107/1000 | Loss: 0.00002193
Iteration 108/1000 | Loss: 0.00002193
Iteration 109/1000 | Loss: 0.00002193
Iteration 110/1000 | Loss: 0.00002193
Iteration 111/1000 | Loss: 0.00002192
Iteration 112/1000 | Loss: 0.00002192
Iteration 113/1000 | Loss: 0.00002192
Iteration 114/1000 | Loss: 0.00002192
Iteration 115/1000 | Loss: 0.00002192
Iteration 116/1000 | Loss: 0.00002192
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002192
Iteration 119/1000 | Loss: 0.00002192
Iteration 120/1000 | Loss: 0.00002192
Iteration 121/1000 | Loss: 0.00002191
Iteration 122/1000 | Loss: 0.00002191
Iteration 123/1000 | Loss: 0.00002191
Iteration 124/1000 | Loss: 0.00002191
Iteration 125/1000 | Loss: 0.00002191
Iteration 126/1000 | Loss: 0.00002190
Iteration 127/1000 | Loss: 0.00002190
Iteration 128/1000 | Loss: 0.00002190
Iteration 129/1000 | Loss: 0.00002190
Iteration 130/1000 | Loss: 0.00002190
Iteration 131/1000 | Loss: 0.00002190
Iteration 132/1000 | Loss: 0.00002190
Iteration 133/1000 | Loss: 0.00002190
Iteration 134/1000 | Loss: 0.00002190
Iteration 135/1000 | Loss: 0.00002189
Iteration 136/1000 | Loss: 0.00002189
Iteration 137/1000 | Loss: 0.00002189
Iteration 138/1000 | Loss: 0.00002189
Iteration 139/1000 | Loss: 0.00002189
Iteration 140/1000 | Loss: 0.00002189
Iteration 141/1000 | Loss: 0.00002189
Iteration 142/1000 | Loss: 0.00002189
Iteration 143/1000 | Loss: 0.00002189
Iteration 144/1000 | Loss: 0.00002189
Iteration 145/1000 | Loss: 0.00002188
Iteration 146/1000 | Loss: 0.00002188
Iteration 147/1000 | Loss: 0.00002188
Iteration 148/1000 | Loss: 0.00002188
Iteration 149/1000 | Loss: 0.00002188
Iteration 150/1000 | Loss: 0.00002188
Iteration 151/1000 | Loss: 0.00002188
Iteration 152/1000 | Loss: 0.00002188
Iteration 153/1000 | Loss: 0.00002188
Iteration 154/1000 | Loss: 0.00002188
Iteration 155/1000 | Loss: 0.00002188
Iteration 156/1000 | Loss: 0.00002188
Iteration 157/1000 | Loss: 0.00002187
Iteration 158/1000 | Loss: 0.00002187
Iteration 159/1000 | Loss: 0.00002187
Iteration 160/1000 | Loss: 0.00002187
Iteration 161/1000 | Loss: 0.00002187
Iteration 162/1000 | Loss: 0.00002187
Iteration 163/1000 | Loss: 0.00002187
Iteration 164/1000 | Loss: 0.00002187
Iteration 165/1000 | Loss: 0.00002187
Iteration 166/1000 | Loss: 0.00002187
Iteration 167/1000 | Loss: 0.00002187
Iteration 168/1000 | Loss: 0.00002186
Iteration 169/1000 | Loss: 0.00002186
Iteration 170/1000 | Loss: 0.00002186
Iteration 171/1000 | Loss: 0.00002186
Iteration 172/1000 | Loss: 0.00002186
Iteration 173/1000 | Loss: 0.00002186
Iteration 174/1000 | Loss: 0.00002186
Iteration 175/1000 | Loss: 0.00002186
Iteration 176/1000 | Loss: 0.00002186
Iteration 177/1000 | Loss: 0.00002186
Iteration 178/1000 | Loss: 0.00002186
Iteration 179/1000 | Loss: 0.00002186
Iteration 180/1000 | Loss: 0.00002186
Iteration 181/1000 | Loss: 0.00002185
Iteration 182/1000 | Loss: 0.00002185
Iteration 183/1000 | Loss: 0.00002185
Iteration 184/1000 | Loss: 0.00002185
Iteration 185/1000 | Loss: 0.00002185
Iteration 186/1000 | Loss: 0.00002185
Iteration 187/1000 | Loss: 0.00002185
Iteration 188/1000 | Loss: 0.00002185
Iteration 189/1000 | Loss: 0.00002185
Iteration 190/1000 | Loss: 0.00002185
Iteration 191/1000 | Loss: 0.00002185
Iteration 192/1000 | Loss: 0.00002185
Iteration 193/1000 | Loss: 0.00002185
Iteration 194/1000 | Loss: 0.00002184
Iteration 195/1000 | Loss: 0.00002184
Iteration 196/1000 | Loss: 0.00002184
Iteration 197/1000 | Loss: 0.00002184
Iteration 198/1000 | Loss: 0.00002184
Iteration 199/1000 | Loss: 0.00002184
Iteration 200/1000 | Loss: 0.00002184
Iteration 201/1000 | Loss: 0.00002184
Iteration 202/1000 | Loss: 0.00002184
Iteration 203/1000 | Loss: 0.00002184
Iteration 204/1000 | Loss: 0.00002184
Iteration 205/1000 | Loss: 0.00002184
Iteration 206/1000 | Loss: 0.00002184
Iteration 207/1000 | Loss: 0.00002184
Iteration 208/1000 | Loss: 0.00002183
Iteration 209/1000 | Loss: 0.00002183
Iteration 210/1000 | Loss: 0.00002183
Iteration 211/1000 | Loss: 0.00002183
Iteration 212/1000 | Loss: 0.00002183
Iteration 213/1000 | Loss: 0.00002183
Iteration 214/1000 | Loss: 0.00002183
Iteration 215/1000 | Loss: 0.00002183
Iteration 216/1000 | Loss: 0.00002183
Iteration 217/1000 | Loss: 0.00002183
Iteration 218/1000 | Loss: 0.00002183
Iteration 219/1000 | Loss: 0.00002183
Iteration 220/1000 | Loss: 0.00002183
Iteration 221/1000 | Loss: 0.00002183
Iteration 222/1000 | Loss: 0.00002183
Iteration 223/1000 | Loss: 0.00002183
Iteration 224/1000 | Loss: 0.00002183
Iteration 225/1000 | Loss: 0.00002183
Iteration 226/1000 | Loss: 0.00002183
Iteration 227/1000 | Loss: 0.00002183
Iteration 228/1000 | Loss: 0.00002183
Iteration 229/1000 | Loss: 0.00002183
Iteration 230/1000 | Loss: 0.00002183
Iteration 231/1000 | Loss: 0.00002183
Iteration 232/1000 | Loss: 0.00002183
Iteration 233/1000 | Loss: 0.00002183
Iteration 234/1000 | Loss: 0.00002183
Iteration 235/1000 | Loss: 0.00002183
Iteration 236/1000 | Loss: 0.00002183
Iteration 237/1000 | Loss: 0.00002183
Iteration 238/1000 | Loss: 0.00002183
Iteration 239/1000 | Loss: 0.00002183
Iteration 240/1000 | Loss: 0.00002183
Iteration 241/1000 | Loss: 0.00002183
Iteration 242/1000 | Loss: 0.00002183
Iteration 243/1000 | Loss: 0.00002183
Iteration 244/1000 | Loss: 0.00002183
Iteration 245/1000 | Loss: 0.00002183
Iteration 246/1000 | Loss: 0.00002183
Iteration 247/1000 | Loss: 0.00002183
Iteration 248/1000 | Loss: 0.00002183
Iteration 249/1000 | Loss: 0.00002183
Iteration 250/1000 | Loss: 0.00002183
Iteration 251/1000 | Loss: 0.00002183
Iteration 252/1000 | Loss: 0.00002183
Iteration 253/1000 | Loss: 0.00002183
Iteration 254/1000 | Loss: 0.00002183
Iteration 255/1000 | Loss: 0.00002183
Iteration 256/1000 | Loss: 0.00002183
Iteration 257/1000 | Loss: 0.00002183
Iteration 258/1000 | Loss: 0.00002183
Iteration 259/1000 | Loss: 0.00002183
Iteration 260/1000 | Loss: 0.00002183
Iteration 261/1000 | Loss: 0.00002183
Iteration 262/1000 | Loss: 0.00002183
Iteration 263/1000 | Loss: 0.00002183
Iteration 264/1000 | Loss: 0.00002183
Iteration 265/1000 | Loss: 0.00002183
Iteration 266/1000 | Loss: 0.00002183
Iteration 267/1000 | Loss: 0.00002183
Iteration 268/1000 | Loss: 0.00002183
Iteration 269/1000 | Loss: 0.00002183
Iteration 270/1000 | Loss: 0.00002183
Iteration 271/1000 | Loss: 0.00002183
Iteration 272/1000 | Loss: 0.00002183
Iteration 273/1000 | Loss: 0.00002183
Iteration 274/1000 | Loss: 0.00002183
Iteration 275/1000 | Loss: 0.00002183
Iteration 276/1000 | Loss: 0.00002183
Iteration 277/1000 | Loss: 0.00002183
Iteration 278/1000 | Loss: 0.00002183
Iteration 279/1000 | Loss: 0.00002183
Iteration 280/1000 | Loss: 0.00002183
Iteration 281/1000 | Loss: 0.00002183
Iteration 282/1000 | Loss: 0.00002183
Iteration 283/1000 | Loss: 0.00002183
Iteration 284/1000 | Loss: 0.00002183
Iteration 285/1000 | Loss: 0.00002183
Iteration 286/1000 | Loss: 0.00002183
Iteration 287/1000 | Loss: 0.00002183
Iteration 288/1000 | Loss: 0.00002183
Iteration 289/1000 | Loss: 0.00002183
Iteration 290/1000 | Loss: 0.00002183
Iteration 291/1000 | Loss: 0.00002183
Iteration 292/1000 | Loss: 0.00002183
Iteration 293/1000 | Loss: 0.00002183
Iteration 294/1000 | Loss: 0.00002183
Iteration 295/1000 | Loss: 0.00002183
Iteration 296/1000 | Loss: 0.00002183
Iteration 297/1000 | Loss: 0.00002183
Iteration 298/1000 | Loss: 0.00002183
Iteration 299/1000 | Loss: 0.00002183
Iteration 300/1000 | Loss: 0.00002183
Iteration 301/1000 | Loss: 0.00002183
Iteration 302/1000 | Loss: 0.00002183
Iteration 303/1000 | Loss: 0.00002183
Iteration 304/1000 | Loss: 0.00002183
Iteration 305/1000 | Loss: 0.00002183
Iteration 306/1000 | Loss: 0.00002183
Iteration 307/1000 | Loss: 0.00002183
Iteration 308/1000 | Loss: 0.00002183
Iteration 309/1000 | Loss: 0.00002183
Iteration 310/1000 | Loss: 0.00002183
Iteration 311/1000 | Loss: 0.00002183
Iteration 312/1000 | Loss: 0.00002183
Iteration 313/1000 | Loss: 0.00002183
Iteration 314/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [2.1828056560480036e-05, 2.1828056560480036e-05, 2.1828056560480036e-05, 2.1828056560480036e-05, 2.1828056560480036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1828056560480036e-05

Optimization complete. Final v2v error: 3.875344753265381 mm

Highest mean error: 5.464512348175049 mm for frame 161

Lowest mean error: 2.711789131164551 mm for frame 194

Saving results

Total time: 54.553202390670776
