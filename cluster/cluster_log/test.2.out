Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=2, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 112-167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392428
Iteration 2/25 | Loss: 0.00131973
Iteration 3/25 | Loss: 0.00124888
Iteration 4/25 | Loss: 0.00124064
Iteration 5/25 | Loss: 0.00123724
Iteration 6/25 | Loss: 0.00123668
Iteration 7/25 | Loss: 0.00123668
Iteration 8/25 | Loss: 0.00123668
Iteration 9/25 | Loss: 0.00123668
Iteration 10/25 | Loss: 0.00123668
Iteration 11/25 | Loss: 0.00123668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012366847367957234, 0.0012366847367957234, 0.0012366847367957234, 0.0012366847367957234, 0.0012366847367957234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012366847367957234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61172557
Iteration 2/25 | Loss: 0.00295174
Iteration 3/25 | Loss: 0.00295174
Iteration 4/25 | Loss: 0.00295174
Iteration 5/25 | Loss: 0.00295174
Iteration 6/25 | Loss: 0.00295174
Iteration 7/25 | Loss: 0.00295174
Iteration 8/25 | Loss: 0.00295174
Iteration 9/25 | Loss: 0.00295174
Iteration 10/25 | Loss: 0.00295174
Iteration 11/25 | Loss: 0.00295174
Iteration 12/25 | Loss: 0.00295174
Iteration 13/25 | Loss: 0.00295174
Iteration 14/25 | Loss: 0.00295174
Iteration 15/25 | Loss: 0.00295174
Iteration 16/25 | Loss: 0.00295174
Iteration 17/25 | Loss: 0.00295174
Iteration 18/25 | Loss: 0.00295174
Iteration 19/25 | Loss: 0.00295174
Iteration 20/25 | Loss: 0.00295174
Iteration 21/25 | Loss: 0.00295174
Iteration 22/25 | Loss: 0.00295174
Iteration 23/25 | Loss: 0.00295174
Iteration 24/25 | Loss: 0.00295174
Iteration 25/25 | Loss: 0.00295174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295174
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001656
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001560
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001501
Iteration 16/1000 | Loss: 0.00001493
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001472
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.4704810382681899e-05, 1.4704810382681899e-05, 1.4704810382681899e-05, 1.4704810382681899e-05, 1.4704810382681899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4704810382681899e-05

Optimization complete. Final v2v error: 3.220505952835083 mm

Highest mean error: 3.4084086418151855 mm for frame 55

Lowest mean error: 3.010822296142578 mm for frame 157

Saving results

Total time: 31.678600549697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876460
Iteration 2/25 | Loss: 0.00155542
Iteration 3/25 | Loss: 0.00132098
Iteration 4/25 | Loss: 0.00129760
Iteration 5/25 | Loss: 0.00129504
Iteration 6/25 | Loss: 0.00129504
Iteration 7/25 | Loss: 0.00129504
Iteration 8/25 | Loss: 0.00129504
Iteration 9/25 | Loss: 0.00129504
Iteration 10/25 | Loss: 0.00129504
Iteration 11/25 | Loss: 0.00129504
Iteration 12/25 | Loss: 0.00129504
Iteration 13/25 | Loss: 0.00129504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012950366362929344, 0.0012950366362929344, 0.0012950366362929344, 0.0012950366362929344, 0.0012950366362929344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012950366362929344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06228733
Iteration 2/25 | Loss: 0.00227863
Iteration 3/25 | Loss: 0.00227862
Iteration 4/25 | Loss: 0.00227862
Iteration 5/25 | Loss: 0.00227862
Iteration 6/25 | Loss: 0.00227862
Iteration 7/25 | Loss: 0.00227862
Iteration 8/25 | Loss: 0.00227862
Iteration 9/25 | Loss: 0.00227862
Iteration 10/25 | Loss: 0.00227862
Iteration 11/25 | Loss: 0.00227862
Iteration 12/25 | Loss: 0.00227862
Iteration 13/25 | Loss: 0.00227862
Iteration 14/25 | Loss: 0.00227862
Iteration 15/25 | Loss: 0.00227862
Iteration 16/25 | Loss: 0.00227862
Iteration 17/25 | Loss: 0.00227862
Iteration 18/25 | Loss: 0.00227862
Iteration 19/25 | Loss: 0.00227862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022786189801990986, 0.0022786189801990986, 0.0022786189801990986, 0.0022786189801990986, 0.0022786189801990986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022786189801990986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227862
Iteration 2/1000 | Loss: 0.00004636
Iteration 3/1000 | Loss: 0.00002942
Iteration 4/1000 | Loss: 0.00002532
Iteration 5/1000 | Loss: 0.00002384
Iteration 6/1000 | Loss: 0.00002302
Iteration 7/1000 | Loss: 0.00002244
Iteration 8/1000 | Loss: 0.00002197
Iteration 9/1000 | Loss: 0.00002171
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002136
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002097
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002088
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002087
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002087
Iteration 20/1000 | Loss: 0.00002087
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002081
Iteration 27/1000 | Loss: 0.00002075
Iteration 28/1000 | Loss: 0.00002074
Iteration 29/1000 | Loss: 0.00002074
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00002074
Iteration 37/1000 | Loss: 0.00002074
Iteration 38/1000 | Loss: 0.00002073
Iteration 39/1000 | Loss: 0.00002073
Iteration 40/1000 | Loss: 0.00002071
Iteration 41/1000 | Loss: 0.00002070
Iteration 42/1000 | Loss: 0.00002070
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002069
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002068
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002067
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002054
Iteration 61/1000 | Loss: 0.00002054
Iteration 62/1000 | Loss: 0.00002054
Iteration 63/1000 | Loss: 0.00002053
Iteration 64/1000 | Loss: 0.00002053
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002052
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002051
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002050
Iteration 81/1000 | Loss: 0.00002050
Iteration 82/1000 | Loss: 0.00002049
Iteration 83/1000 | Loss: 0.00002049
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00002049
Iteration 86/1000 | Loss: 0.00002049
Iteration 87/1000 | Loss: 0.00002049
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002048
Iteration 94/1000 | Loss: 0.00002048
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002045
Iteration 104/1000 | Loss: 0.00002045
Iteration 105/1000 | Loss: 0.00002045
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002043
Iteration 112/1000 | Loss: 0.00002043
Iteration 113/1000 | Loss: 0.00002043
Iteration 114/1000 | Loss: 0.00002043
Iteration 115/1000 | Loss: 0.00002043
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002042
Iteration 119/1000 | Loss: 0.00002042
Iteration 120/1000 | Loss: 0.00002042
Iteration 121/1000 | Loss: 0.00002042
Iteration 122/1000 | Loss: 0.00002042
Iteration 123/1000 | Loss: 0.00002042
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Iteration 128/1000 | Loss: 0.00002041
Iteration 129/1000 | Loss: 0.00002041
Iteration 130/1000 | Loss: 0.00002041
Iteration 131/1000 | Loss: 0.00002041
Iteration 132/1000 | Loss: 0.00002041
Iteration 133/1000 | Loss: 0.00002041
Iteration 134/1000 | Loss: 0.00002041
Iteration 135/1000 | Loss: 0.00002041
Iteration 136/1000 | Loss: 0.00002041
Iteration 137/1000 | Loss: 0.00002041
Iteration 138/1000 | Loss: 0.00002041
Iteration 139/1000 | Loss: 0.00002041
Iteration 140/1000 | Loss: 0.00002041
Iteration 141/1000 | Loss: 0.00002041
Iteration 142/1000 | Loss: 0.00002041
Iteration 143/1000 | Loss: 0.00002041
Iteration 144/1000 | Loss: 0.00002041
Iteration 145/1000 | Loss: 0.00002041
Iteration 146/1000 | Loss: 0.00002041
Iteration 147/1000 | Loss: 0.00002041
Iteration 148/1000 | Loss: 0.00002041
Iteration 149/1000 | Loss: 0.00002041
Iteration 150/1000 | Loss: 0.00002041
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002041
Iteration 153/1000 | Loss: 0.00002041
Iteration 154/1000 | Loss: 0.00002041
Iteration 155/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0406223484314978e-05, 2.0406223484314978e-05, 2.0406223484314978e-05, 2.0406223484314978e-05, 2.0406223484314978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0406223484314978e-05

Optimization complete. Final v2v error: 3.8893182277679443 mm

Highest mean error: 4.189343452453613 mm for frame 24

Lowest mean error: 3.7071521282196045 mm for frame 107

Saving results

Total time: 43.56127977371216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583838
Iteration 2/25 | Loss: 0.00143143
Iteration 3/25 | Loss: 0.00130646
Iteration 4/25 | Loss: 0.00128614
Iteration 5/25 | Loss: 0.00127722
Iteration 6/25 | Loss: 0.00127448
Iteration 7/25 | Loss: 0.00127397
Iteration 8/25 | Loss: 0.00127397
Iteration 9/25 | Loss: 0.00127397
Iteration 10/25 | Loss: 0.00127397
Iteration 11/25 | Loss: 0.00127397
Iteration 12/25 | Loss: 0.00127397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012739748926833272, 0.0012739748926833272, 0.0012739748926833272, 0.0012739748926833272, 0.0012739748926833272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012739748926833272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.55560017
Iteration 2/25 | Loss: 0.00253506
Iteration 3/25 | Loss: 0.00253506
Iteration 4/25 | Loss: 0.00253506
Iteration 5/25 | Loss: 0.00253506
Iteration 6/25 | Loss: 0.00253506
Iteration 7/25 | Loss: 0.00253506
Iteration 8/25 | Loss: 0.00253506
Iteration 9/25 | Loss: 0.00253506
Iteration 10/25 | Loss: 0.00253506
Iteration 11/25 | Loss: 0.00253506
Iteration 12/25 | Loss: 0.00253506
Iteration 13/25 | Loss: 0.00253506
Iteration 14/25 | Loss: 0.00253506
Iteration 15/25 | Loss: 0.00253506
Iteration 16/25 | Loss: 0.00253506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002535057719796896, 0.002535057719796896, 0.002535057719796896, 0.002535057719796896, 0.002535057719796896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002535057719796896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253506
Iteration 2/1000 | Loss: 0.00007084
Iteration 3/1000 | Loss: 0.00004284
Iteration 4/1000 | Loss: 0.00003078
Iteration 5/1000 | Loss: 0.00002802
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002462
Iteration 9/1000 | Loss: 0.00002409
Iteration 10/1000 | Loss: 0.00002379
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002290
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002254
Iteration 16/1000 | Loss: 0.00002253
Iteration 17/1000 | Loss: 0.00002248
Iteration 18/1000 | Loss: 0.00002247
Iteration 19/1000 | Loss: 0.00002242
Iteration 20/1000 | Loss: 0.00002238
Iteration 21/1000 | Loss: 0.00002233
Iteration 22/1000 | Loss: 0.00002233
Iteration 23/1000 | Loss: 0.00002233
Iteration 24/1000 | Loss: 0.00002233
Iteration 25/1000 | Loss: 0.00002232
Iteration 26/1000 | Loss: 0.00002229
Iteration 27/1000 | Loss: 0.00002228
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002227
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002224
Iteration 34/1000 | Loss: 0.00002224
Iteration 35/1000 | Loss: 0.00002224
Iteration 36/1000 | Loss: 0.00002224
Iteration 37/1000 | Loss: 0.00002224
Iteration 38/1000 | Loss: 0.00002224
Iteration 39/1000 | Loss: 0.00002224
Iteration 40/1000 | Loss: 0.00002224
Iteration 41/1000 | Loss: 0.00002223
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002223
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002223
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002223
Iteration 48/1000 | Loss: 0.00002223
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002221
Iteration 52/1000 | Loss: 0.00002221
Iteration 53/1000 | Loss: 0.00002221
Iteration 54/1000 | Loss: 0.00002221
Iteration 55/1000 | Loss: 0.00002221
Iteration 56/1000 | Loss: 0.00002221
Iteration 57/1000 | Loss: 0.00002221
Iteration 58/1000 | Loss: 0.00002221
Iteration 59/1000 | Loss: 0.00002220
Iteration 60/1000 | Loss: 0.00002220
Iteration 61/1000 | Loss: 0.00002220
Iteration 62/1000 | Loss: 0.00002220
Iteration 63/1000 | Loss: 0.00002220
Iteration 64/1000 | Loss: 0.00002220
Iteration 65/1000 | Loss: 0.00002220
Iteration 66/1000 | Loss: 0.00002219
Iteration 67/1000 | Loss: 0.00002218
Iteration 68/1000 | Loss: 0.00002218
Iteration 69/1000 | Loss: 0.00002218
Iteration 70/1000 | Loss: 0.00002218
Iteration 71/1000 | Loss: 0.00002217
Iteration 72/1000 | Loss: 0.00002217
Iteration 73/1000 | Loss: 0.00002216
Iteration 74/1000 | Loss: 0.00002216
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00002216
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002215
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002214
Iteration 81/1000 | Loss: 0.00002214
Iteration 82/1000 | Loss: 0.00002214
Iteration 83/1000 | Loss: 0.00002214
Iteration 84/1000 | Loss: 0.00002214
Iteration 85/1000 | Loss: 0.00002214
Iteration 86/1000 | Loss: 0.00002214
Iteration 87/1000 | Loss: 0.00002214
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002214
Iteration 91/1000 | Loss: 0.00002213
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002213
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002213
Iteration 98/1000 | Loss: 0.00002213
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002213
Iteration 101/1000 | Loss: 0.00002212
Iteration 102/1000 | Loss: 0.00002212
Iteration 103/1000 | Loss: 0.00002212
Iteration 104/1000 | Loss: 0.00002212
Iteration 105/1000 | Loss: 0.00002212
Iteration 106/1000 | Loss: 0.00002212
Iteration 107/1000 | Loss: 0.00002212
Iteration 108/1000 | Loss: 0.00002212
Iteration 109/1000 | Loss: 0.00002212
Iteration 110/1000 | Loss: 0.00002212
Iteration 111/1000 | Loss: 0.00002212
Iteration 112/1000 | Loss: 0.00002212
Iteration 113/1000 | Loss: 0.00002211
Iteration 114/1000 | Loss: 0.00002211
Iteration 115/1000 | Loss: 0.00002211
Iteration 116/1000 | Loss: 0.00002211
Iteration 117/1000 | Loss: 0.00002211
Iteration 118/1000 | Loss: 0.00002211
Iteration 119/1000 | Loss: 0.00002211
Iteration 120/1000 | Loss: 0.00002211
Iteration 121/1000 | Loss: 0.00002211
Iteration 122/1000 | Loss: 0.00002211
Iteration 123/1000 | Loss: 0.00002211
Iteration 124/1000 | Loss: 0.00002211
Iteration 125/1000 | Loss: 0.00002211
Iteration 126/1000 | Loss: 0.00002211
Iteration 127/1000 | Loss: 0.00002211
Iteration 128/1000 | Loss: 0.00002211
Iteration 129/1000 | Loss: 0.00002211
Iteration 130/1000 | Loss: 0.00002211
Iteration 131/1000 | Loss: 0.00002211
Iteration 132/1000 | Loss: 0.00002211
Iteration 133/1000 | Loss: 0.00002211
Iteration 134/1000 | Loss: 0.00002211
Iteration 135/1000 | Loss: 0.00002211
Iteration 136/1000 | Loss: 0.00002211
Iteration 137/1000 | Loss: 0.00002211
Iteration 138/1000 | Loss: 0.00002211
Iteration 139/1000 | Loss: 0.00002211
Iteration 140/1000 | Loss: 0.00002211
Iteration 141/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.210708407801576e-05, 2.210708407801576e-05, 2.210708407801576e-05, 2.210708407801576e-05, 2.210708407801576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.210708407801576e-05

Optimization complete. Final v2v error: 3.9016475677490234 mm

Highest mean error: 5.407513618469238 mm for frame 98

Lowest mean error: 3.3295717239379883 mm for frame 133

Saving results

Total time: 41.28571629524231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957990
Iteration 2/25 | Loss: 0.00142913
Iteration 3/25 | Loss: 0.00128741
Iteration 4/25 | Loss: 0.00126754
Iteration 5/25 | Loss: 0.00126361
Iteration 6/25 | Loss: 0.00126317
Iteration 7/25 | Loss: 0.00126317
Iteration 8/25 | Loss: 0.00126317
Iteration 9/25 | Loss: 0.00126317
Iteration 10/25 | Loss: 0.00126317
Iteration 11/25 | Loss: 0.00126317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012631712015718222, 0.0012631712015718222, 0.0012631712015718222, 0.0012631712015718222, 0.0012631712015718222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012631712015718222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16391790
Iteration 2/25 | Loss: 0.00300427
Iteration 3/25 | Loss: 0.00300427
Iteration 4/25 | Loss: 0.00300427
Iteration 5/25 | Loss: 0.00300427
Iteration 6/25 | Loss: 0.00300427
Iteration 7/25 | Loss: 0.00300427
Iteration 8/25 | Loss: 0.00300427
Iteration 9/25 | Loss: 0.00300427
Iteration 10/25 | Loss: 0.00300427
Iteration 11/25 | Loss: 0.00300427
Iteration 12/25 | Loss: 0.00300427
Iteration 13/25 | Loss: 0.00300427
Iteration 14/25 | Loss: 0.00300427
Iteration 15/25 | Loss: 0.00300427
Iteration 16/25 | Loss: 0.00300427
Iteration 17/25 | Loss: 0.00300427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003004265483468771, 0.003004265483468771, 0.003004265483468771, 0.003004265483468771, 0.003004265483468771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003004265483468771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300427
Iteration 2/1000 | Loss: 0.00007338
Iteration 3/1000 | Loss: 0.00003235
Iteration 4/1000 | Loss: 0.00002363
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00001962
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001860
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001749
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001738
Iteration 18/1000 | Loss: 0.00001737
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00001718
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001712
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001709
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001701
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001698
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001698
Iteration 95/1000 | Loss: 0.00001698
Iteration 96/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.69784343597712e-05, 1.69784343597712e-05, 1.69784343597712e-05, 1.69784343597712e-05, 1.69784343597712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69784343597712e-05

Optimization complete. Final v2v error: 3.4885547161102295 mm

Highest mean error: 4.265454292297363 mm for frame 176

Lowest mean error: 3.16139554977417 mm for frame 75

Saving results

Total time: 39.937317848205566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934489
Iteration 2/25 | Loss: 0.00163162
Iteration 3/25 | Loss: 0.00140360
Iteration 4/25 | Loss: 0.00138457
Iteration 5/25 | Loss: 0.00138144
Iteration 6/25 | Loss: 0.00138131
Iteration 7/25 | Loss: 0.00137192
Iteration 8/25 | Loss: 0.00136662
Iteration 9/25 | Loss: 0.00136506
Iteration 10/25 | Loss: 0.00136467
Iteration 11/25 | Loss: 0.00136457
Iteration 12/25 | Loss: 0.00136457
Iteration 13/25 | Loss: 0.00136457
Iteration 14/25 | Loss: 0.00136457
Iteration 15/25 | Loss: 0.00136457
Iteration 16/25 | Loss: 0.00136456
Iteration 17/25 | Loss: 0.00136456
Iteration 18/25 | Loss: 0.00136456
Iteration 19/25 | Loss: 0.00136456
Iteration 20/25 | Loss: 0.00136456
Iteration 21/25 | Loss: 0.00136456
Iteration 22/25 | Loss: 0.00136456
Iteration 23/25 | Loss: 0.00136456
Iteration 24/25 | Loss: 0.00136456
Iteration 25/25 | Loss: 0.00136456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73954141
Iteration 2/25 | Loss: 0.00173672
Iteration 3/25 | Loss: 0.00173672
Iteration 4/25 | Loss: 0.00173672
Iteration 5/25 | Loss: 0.00173672
Iteration 6/25 | Loss: 0.00173671
Iteration 7/25 | Loss: 0.00173671
Iteration 8/25 | Loss: 0.00173671
Iteration 9/25 | Loss: 0.00173671
Iteration 10/25 | Loss: 0.00173671
Iteration 11/25 | Loss: 0.00173671
Iteration 12/25 | Loss: 0.00173671
Iteration 13/25 | Loss: 0.00173671
Iteration 14/25 | Loss: 0.00173671
Iteration 15/25 | Loss: 0.00173671
Iteration 16/25 | Loss: 0.00173671
Iteration 17/25 | Loss: 0.00173671
Iteration 18/25 | Loss: 0.00173671
Iteration 19/25 | Loss: 0.00173671
Iteration 20/25 | Loss: 0.00173671
Iteration 21/25 | Loss: 0.00173671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017367122927680612, 0.0017367122927680612, 0.0017367122927680612, 0.0017367122927680612, 0.0017367122927680612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017367122927680612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173671
Iteration 2/1000 | Loss: 0.00006960
Iteration 3/1000 | Loss: 0.00005249
Iteration 4/1000 | Loss: 0.00004705
Iteration 5/1000 | Loss: 0.00004505
Iteration 6/1000 | Loss: 0.00004394
Iteration 7/1000 | Loss: 0.00004310
Iteration 8/1000 | Loss: 0.00004262
Iteration 9/1000 | Loss: 0.00004213
Iteration 10/1000 | Loss: 0.00004170
Iteration 11/1000 | Loss: 0.00004148
Iteration 12/1000 | Loss: 0.00004113
Iteration 13/1000 | Loss: 0.00004087
Iteration 14/1000 | Loss: 0.00004068
Iteration 15/1000 | Loss: 0.00004066
Iteration 16/1000 | Loss: 0.00004063
Iteration 17/1000 | Loss: 0.00004062
Iteration 18/1000 | Loss: 0.00004058
Iteration 19/1000 | Loss: 0.00004058
Iteration 20/1000 | Loss: 0.00004058
Iteration 21/1000 | Loss: 0.00004057
Iteration 22/1000 | Loss: 0.00004057
Iteration 23/1000 | Loss: 0.00004057
Iteration 24/1000 | Loss: 0.00004057
Iteration 25/1000 | Loss: 0.00004057
Iteration 26/1000 | Loss: 0.00004057
Iteration 27/1000 | Loss: 0.00004057
Iteration 28/1000 | Loss: 0.00004057
Iteration 29/1000 | Loss: 0.00004057
Iteration 30/1000 | Loss: 0.00004057
Iteration 31/1000 | Loss: 0.00004057
Iteration 32/1000 | Loss: 0.00004057
Iteration 33/1000 | Loss: 0.00004057
Iteration 34/1000 | Loss: 0.00004057
Iteration 35/1000 | Loss: 0.00004057
Iteration 36/1000 | Loss: 0.00004057
Iteration 37/1000 | Loss: 0.00004057
Iteration 38/1000 | Loss: 0.00004057
Iteration 39/1000 | Loss: 0.00004057
Iteration 40/1000 | Loss: 0.00004057
Iteration 41/1000 | Loss: 0.00004057
Iteration 42/1000 | Loss: 0.00004057
Iteration 43/1000 | Loss: 0.00004057
Iteration 44/1000 | Loss: 0.00004057
Iteration 45/1000 | Loss: 0.00004057
Iteration 46/1000 | Loss: 0.00004057
Iteration 47/1000 | Loss: 0.00004057
Iteration 48/1000 | Loss: 0.00004057
Iteration 49/1000 | Loss: 0.00004057
Iteration 50/1000 | Loss: 0.00004057
Iteration 51/1000 | Loss: 0.00004057
Iteration 52/1000 | Loss: 0.00004057
Iteration 53/1000 | Loss: 0.00004057
Iteration 54/1000 | Loss: 0.00004057
Iteration 55/1000 | Loss: 0.00004057
Iteration 56/1000 | Loss: 0.00004057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [4.05676692025736e-05, 4.05676692025736e-05, 4.05676692025736e-05, 4.05676692025736e-05, 4.05676692025736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.05676692025736e-05

Optimization complete. Final v2v error: 5.376246929168701 mm

Highest mean error: 5.561770915985107 mm for frame 150

Lowest mean error: 5.227694034576416 mm for frame 90

Saving results

Total time: 40.52299499511719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823223
Iteration 2/25 | Loss: 0.00186548
Iteration 3/25 | Loss: 0.00148323
Iteration 4/25 | Loss: 0.00138405
Iteration 5/25 | Loss: 0.00137407
Iteration 6/25 | Loss: 0.00137023
Iteration 7/25 | Loss: 0.00136910
Iteration 8/25 | Loss: 0.00136801
Iteration 9/25 | Loss: 0.00136410
Iteration 10/25 | Loss: 0.00136524
Iteration 11/25 | Loss: 0.00135991
Iteration 12/25 | Loss: 0.00136087
Iteration 13/25 | Loss: 0.00136404
Iteration 14/25 | Loss: 0.00136327
Iteration 15/25 | Loss: 0.00136003
Iteration 16/25 | Loss: 0.00136084
Iteration 17/25 | Loss: 0.00136107
Iteration 18/25 | Loss: 0.00135831
Iteration 19/25 | Loss: 0.00135432
Iteration 20/25 | Loss: 0.00135317
Iteration 21/25 | Loss: 0.00135152
Iteration 22/25 | Loss: 0.00134792
Iteration 23/25 | Loss: 0.00134752
Iteration 24/25 | Loss: 0.00134742
Iteration 25/25 | Loss: 0.00134712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58106041
Iteration 2/25 | Loss: 0.00305276
Iteration 3/25 | Loss: 0.00293304
Iteration 4/25 | Loss: 0.00293304
Iteration 5/25 | Loss: 0.00293304
Iteration 6/25 | Loss: 0.00293304
Iteration 7/25 | Loss: 0.00293304
Iteration 8/25 | Loss: 0.00293304
Iteration 9/25 | Loss: 0.00293304
Iteration 10/25 | Loss: 0.00293304
Iteration 11/25 | Loss: 0.00293304
Iteration 12/25 | Loss: 0.00293304
Iteration 13/25 | Loss: 0.00293304
Iteration 14/25 | Loss: 0.00293304
Iteration 15/25 | Loss: 0.00293304
Iteration 16/25 | Loss: 0.00293304
Iteration 17/25 | Loss: 0.00293304
Iteration 18/25 | Loss: 0.00293304
Iteration 19/25 | Loss: 0.00293304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029330432880669832, 0.0029330432880669832, 0.0029330432880669832, 0.0029330432880669832, 0.0029330432880669832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029330432880669832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293304
Iteration 2/1000 | Loss: 0.00016408
Iteration 3/1000 | Loss: 0.00012518
Iteration 4/1000 | Loss: 0.00024297
Iteration 5/1000 | Loss: 0.00012259
Iteration 6/1000 | Loss: 0.00017479
Iteration 7/1000 | Loss: 0.00009853
Iteration 8/1000 | Loss: 0.00014725
Iteration 9/1000 | Loss: 0.00017001
Iteration 10/1000 | Loss: 0.00017639
Iteration 11/1000 | Loss: 0.00018618
Iteration 12/1000 | Loss: 0.00017041
Iteration 13/1000 | Loss: 0.00021171
Iteration 14/1000 | Loss: 0.00021449
Iteration 15/1000 | Loss: 0.00017383
Iteration 16/1000 | Loss: 0.00015145
Iteration 17/1000 | Loss: 0.00019197
Iteration 18/1000 | Loss: 0.00013728
Iteration 19/1000 | Loss: 0.00011980
Iteration 20/1000 | Loss: 0.00010108
Iteration 21/1000 | Loss: 0.00023590
Iteration 22/1000 | Loss: 0.00006609
Iteration 23/1000 | Loss: 0.00010689
Iteration 24/1000 | Loss: 0.00013235
Iteration 25/1000 | Loss: 0.00017777
Iteration 26/1000 | Loss: 0.00005190
Iteration 27/1000 | Loss: 0.00006850
Iteration 28/1000 | Loss: 0.00006759
Iteration 29/1000 | Loss: 0.00009484
Iteration 30/1000 | Loss: 0.00010661
Iteration 31/1000 | Loss: 0.00012198
Iteration 32/1000 | Loss: 0.00018547
Iteration 33/1000 | Loss: 0.00013196
Iteration 34/1000 | Loss: 0.00015308
Iteration 35/1000 | Loss: 0.00014795
Iteration 36/1000 | Loss: 0.00008954
Iteration 37/1000 | Loss: 0.00011288
Iteration 38/1000 | Loss: 0.00016425
Iteration 39/1000 | Loss: 0.00010295
Iteration 40/1000 | Loss: 0.00011360
Iteration 41/1000 | Loss: 0.00010682
Iteration 42/1000 | Loss: 0.00023298
Iteration 43/1000 | Loss: 0.00012450
Iteration 44/1000 | Loss: 0.00004546
Iteration 45/1000 | Loss: 0.00015626
Iteration 46/1000 | Loss: 0.00004267
Iteration 47/1000 | Loss: 0.00003287
Iteration 48/1000 | Loss: 0.00006857
Iteration 49/1000 | Loss: 0.00008807
Iteration 50/1000 | Loss: 0.00008929
Iteration 51/1000 | Loss: 0.00008689
Iteration 52/1000 | Loss: 0.00003201
Iteration 53/1000 | Loss: 0.00005185
Iteration 54/1000 | Loss: 0.00003134
Iteration 55/1000 | Loss: 0.00003004
Iteration 56/1000 | Loss: 0.00002922
Iteration 57/1000 | Loss: 0.00002842
Iteration 58/1000 | Loss: 0.00002774
Iteration 59/1000 | Loss: 0.00002722
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002637
Iteration 62/1000 | Loss: 0.00002624
Iteration 63/1000 | Loss: 0.00002611
Iteration 64/1000 | Loss: 0.00002602
Iteration 65/1000 | Loss: 0.00002592
Iteration 66/1000 | Loss: 0.00002592
Iteration 67/1000 | Loss: 0.00002590
Iteration 68/1000 | Loss: 0.00002590
Iteration 69/1000 | Loss: 0.00002590
Iteration 70/1000 | Loss: 0.00002590
Iteration 71/1000 | Loss: 0.00002590
Iteration 72/1000 | Loss: 0.00002590
Iteration 73/1000 | Loss: 0.00002589
Iteration 74/1000 | Loss: 0.00002588
Iteration 75/1000 | Loss: 0.00002587
Iteration 76/1000 | Loss: 0.00002587
Iteration 77/1000 | Loss: 0.00002877
Iteration 78/1000 | Loss: 0.00002876
Iteration 79/1000 | Loss: 0.00002757
Iteration 80/1000 | Loss: 0.00002643
Iteration 81/1000 | Loss: 0.00002592
Iteration 82/1000 | Loss: 0.00002662
Iteration 83/1000 | Loss: 0.00002573
Iteration 84/1000 | Loss: 0.00002573
Iteration 85/1000 | Loss: 0.00002573
Iteration 86/1000 | Loss: 0.00002731
Iteration 87/1000 | Loss: 0.00002614
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002572
Iteration 91/1000 | Loss: 0.00002572
Iteration 92/1000 | Loss: 0.00002572
Iteration 93/1000 | Loss: 0.00002572
Iteration 94/1000 | Loss: 0.00002572
Iteration 95/1000 | Loss: 0.00002572
Iteration 96/1000 | Loss: 0.00002572
Iteration 97/1000 | Loss: 0.00002572
Iteration 98/1000 | Loss: 0.00002572
Iteration 99/1000 | Loss: 0.00002572
Iteration 100/1000 | Loss: 0.00002572
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002572
Iteration 105/1000 | Loss: 0.00002572
Iteration 106/1000 | Loss: 0.00002572
Iteration 107/1000 | Loss: 0.00002572
Iteration 108/1000 | Loss: 0.00002572
Iteration 109/1000 | Loss: 0.00002572
Iteration 110/1000 | Loss: 0.00002572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.5719389668665826e-05, 2.5719389668665826e-05, 2.5719389668665826e-05, 2.5719389668665826e-05, 2.5719389668665826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5719389668665826e-05

Optimization complete. Final v2v error: 4.103849411010742 mm

Highest mean error: 11.915655136108398 mm for frame 23

Lowest mean error: 3.5527071952819824 mm for frame 139

Saving results

Total time: 163.68671917915344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499508
Iteration 2/25 | Loss: 0.00146720
Iteration 3/25 | Loss: 0.00130985
Iteration 4/25 | Loss: 0.00129261
Iteration 5/25 | Loss: 0.00128790
Iteration 6/25 | Loss: 0.00128636
Iteration 7/25 | Loss: 0.00128631
Iteration 8/25 | Loss: 0.00128631
Iteration 9/25 | Loss: 0.00128631
Iteration 10/25 | Loss: 0.00128631
Iteration 11/25 | Loss: 0.00128631
Iteration 12/25 | Loss: 0.00128631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012863127049058676, 0.0012863127049058676, 0.0012863127049058676, 0.0012863127049058676, 0.0012863127049058676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012863127049058676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24086463
Iteration 2/25 | Loss: 0.00245827
Iteration 3/25 | Loss: 0.00245826
Iteration 4/25 | Loss: 0.00245825
Iteration 5/25 | Loss: 0.00245825
Iteration 6/25 | Loss: 0.00245825
Iteration 7/25 | Loss: 0.00245825
Iteration 8/25 | Loss: 0.00245825
Iteration 9/25 | Loss: 0.00245825
Iteration 10/25 | Loss: 0.00245825
Iteration 11/25 | Loss: 0.00245825
Iteration 12/25 | Loss: 0.00245825
Iteration 13/25 | Loss: 0.00245825
Iteration 14/25 | Loss: 0.00245825
Iteration 15/25 | Loss: 0.00245825
Iteration 16/25 | Loss: 0.00245825
Iteration 17/25 | Loss: 0.00245825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024582520127296448, 0.0024582520127296448, 0.0024582520127296448, 0.0024582520127296448, 0.0024582520127296448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024582520127296448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245825
Iteration 2/1000 | Loss: 0.00005749
Iteration 3/1000 | Loss: 0.00003089
Iteration 4/1000 | Loss: 0.00002536
Iteration 5/1000 | Loss: 0.00002333
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002124
Iteration 9/1000 | Loss: 0.00002093
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002046
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001978
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001976
Iteration 20/1000 | Loss: 0.00001974
Iteration 21/1000 | Loss: 0.00001973
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001972
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001949
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.9461371266515926e-05, 1.9461371266515926e-05, 1.9461371266515926e-05, 1.9461371266515926e-05, 1.9461371266515926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9461371266515926e-05

Optimization complete. Final v2v error: 3.667555332183838 mm

Highest mean error: 4.2828569412231445 mm for frame 67

Lowest mean error: 3.161055326461792 mm for frame 0

Saving results

Total time: 36.09885025024414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758606
Iteration 2/25 | Loss: 0.00149520
Iteration 3/25 | Loss: 0.00132301
Iteration 4/25 | Loss: 0.00128920
Iteration 5/25 | Loss: 0.00128025
Iteration 6/25 | Loss: 0.00127619
Iteration 7/25 | Loss: 0.00127502
Iteration 8/25 | Loss: 0.00127425
Iteration 9/25 | Loss: 0.00127386
Iteration 10/25 | Loss: 0.00127371
Iteration 11/25 | Loss: 0.00127362
Iteration 12/25 | Loss: 0.00127360
Iteration 13/25 | Loss: 0.00127360
Iteration 14/25 | Loss: 0.00127358
Iteration 15/25 | Loss: 0.00127358
Iteration 16/25 | Loss: 0.00127358
Iteration 17/25 | Loss: 0.00127358
Iteration 18/25 | Loss: 0.00127358
Iteration 19/25 | Loss: 0.00127358
Iteration 20/25 | Loss: 0.00127357
Iteration 21/25 | Loss: 0.00127357
Iteration 22/25 | Loss: 0.00127357
Iteration 23/25 | Loss: 0.00127357
Iteration 24/25 | Loss: 0.00127357
Iteration 25/25 | Loss: 0.00127357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63356042
Iteration 2/25 | Loss: 0.00293215
Iteration 3/25 | Loss: 0.00293214
Iteration 4/25 | Loss: 0.00293214
Iteration 5/25 | Loss: 0.00293214
Iteration 6/25 | Loss: 0.00293214
Iteration 7/25 | Loss: 0.00293214
Iteration 8/25 | Loss: 0.00293214
Iteration 9/25 | Loss: 0.00293214
Iteration 10/25 | Loss: 0.00293214
Iteration 11/25 | Loss: 0.00293214
Iteration 12/25 | Loss: 0.00293214
Iteration 13/25 | Loss: 0.00293214
Iteration 14/25 | Loss: 0.00293214
Iteration 15/25 | Loss: 0.00293214
Iteration 16/25 | Loss: 0.00293214
Iteration 17/25 | Loss: 0.00293214
Iteration 18/25 | Loss: 0.00293214
Iteration 19/25 | Loss: 0.00293214
Iteration 20/25 | Loss: 0.00293214
Iteration 21/25 | Loss: 0.00293214
Iteration 22/25 | Loss: 0.00293214
Iteration 23/25 | Loss: 0.00293214
Iteration 24/25 | Loss: 0.00293214
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0029321417678147554, 0.0029321417678147554, 0.0029321417678147554, 0.0029321417678147554, 0.0029321417678147554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029321417678147554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293214
Iteration 2/1000 | Loss: 0.00004778
Iteration 3/1000 | Loss: 0.00002869
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002109
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002012
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001907
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001897
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001885
Iteration 26/1000 | Loss: 0.00001885
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001884
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001883
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001882
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001880
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001877
Iteration 81/1000 | Loss: 0.00001877
Iteration 82/1000 | Loss: 0.00001877
Iteration 83/1000 | Loss: 0.00001877
Iteration 84/1000 | Loss: 0.00001877
Iteration 85/1000 | Loss: 0.00001877
Iteration 86/1000 | Loss: 0.00001877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.8769951566355303e-05, 1.8769951566355303e-05, 1.8769951566355303e-05, 1.8769951566355303e-05, 1.8769951566355303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8769951566355303e-05

Optimization complete. Final v2v error: 3.7574427127838135 mm

Highest mean error: 4.226778507232666 mm for frame 151

Lowest mean error: 3.3216052055358887 mm for frame 31

Saving results

Total time: 50.29439091682434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01133208
Iteration 2/25 | Loss: 0.01133208
Iteration 3/25 | Loss: 0.01133207
Iteration 4/25 | Loss: 0.01133207
Iteration 5/25 | Loss: 0.00149235
Iteration 6/25 | Loss: 0.00129411
Iteration 7/25 | Loss: 0.00127559
Iteration 8/25 | Loss: 0.00127167
Iteration 9/25 | Loss: 0.00127167
Iteration 10/25 | Loss: 0.00127167
Iteration 11/25 | Loss: 0.00127167
Iteration 12/25 | Loss: 0.00127167
Iteration 13/25 | Loss: 0.00127167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012716678902506828, 0.0012716678902506828, 0.0012716678902506828, 0.0012716678902506828, 0.0012716678902506828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012716678902506828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21150255
Iteration 2/25 | Loss: 0.00231367
Iteration 3/25 | Loss: 0.00231367
Iteration 4/25 | Loss: 0.00231366
Iteration 5/25 | Loss: 0.00231366
Iteration 6/25 | Loss: 0.00231366
Iteration 7/25 | Loss: 0.00231366
Iteration 8/25 | Loss: 0.00231366
Iteration 9/25 | Loss: 0.00231366
Iteration 10/25 | Loss: 0.00231366
Iteration 11/25 | Loss: 0.00231366
Iteration 12/25 | Loss: 0.00231366
Iteration 13/25 | Loss: 0.00231366
Iteration 14/25 | Loss: 0.00231366
Iteration 15/25 | Loss: 0.00231366
Iteration 16/25 | Loss: 0.00231366
Iteration 17/25 | Loss: 0.00231366
Iteration 18/25 | Loss: 0.00231366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002313662087544799, 0.002313662087544799, 0.002313662087544799, 0.002313662087544799, 0.002313662087544799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002313662087544799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231366
Iteration 2/1000 | Loss: 0.00004741
Iteration 3/1000 | Loss: 0.00002973
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002452
Iteration 6/1000 | Loss: 0.00002349
Iteration 7/1000 | Loss: 0.00002259
Iteration 8/1000 | Loss: 0.00002209
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002147
Iteration 11/1000 | Loss: 0.00002121
Iteration 12/1000 | Loss: 0.00002102
Iteration 13/1000 | Loss: 0.00002097
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002097
Iteration 16/1000 | Loss: 0.00002097
Iteration 17/1000 | Loss: 0.00002095
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002095
Iteration 20/1000 | Loss: 0.00002094
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002094
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002089
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002088
Iteration 33/1000 | Loss: 0.00002088
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002087
Iteration 38/1000 | Loss: 0.00002087
Iteration 39/1000 | Loss: 0.00002086
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002077
Iteration 60/1000 | Loss: 0.00002077
Iteration 61/1000 | Loss: 0.00002077
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002077
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002077
Iteration 71/1000 | Loss: 0.00002077
Iteration 72/1000 | Loss: 0.00002077
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002077
Iteration 77/1000 | Loss: 0.00002077
Iteration 78/1000 | Loss: 0.00002077
Iteration 79/1000 | Loss: 0.00002077
Iteration 80/1000 | Loss: 0.00002077
Iteration 81/1000 | Loss: 0.00002077
Iteration 82/1000 | Loss: 0.00002077
Iteration 83/1000 | Loss: 0.00002077
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.0766126908711158e-05, 2.0766126908711158e-05, 2.0766126908711158e-05, 2.0766126908711158e-05, 2.0766126908711158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0766126908711158e-05

Optimization complete. Final v2v error: 3.9111948013305664 mm

Highest mean error: 4.219491004943848 mm for frame 200

Lowest mean error: 3.6843225955963135 mm for frame 237

Saving results

Total time: 33.72200679779053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112497
Iteration 2/25 | Loss: 0.00176928
Iteration 3/25 | Loss: 0.00150556
Iteration 4/25 | Loss: 0.00154503
Iteration 5/25 | Loss: 0.00149798
Iteration 6/25 | Loss: 0.00138490
Iteration 7/25 | Loss: 0.00140918
Iteration 8/25 | Loss: 0.00135654
Iteration 9/25 | Loss: 0.00133903
Iteration 10/25 | Loss: 0.00134585
Iteration 11/25 | Loss: 0.00133779
Iteration 12/25 | Loss: 0.00132334
Iteration 13/25 | Loss: 0.00132270
Iteration 14/25 | Loss: 0.00132761
Iteration 15/25 | Loss: 0.00132174
Iteration 16/25 | Loss: 0.00132006
Iteration 17/25 | Loss: 0.00131943
Iteration 18/25 | Loss: 0.00131922
Iteration 19/25 | Loss: 0.00131921
Iteration 20/25 | Loss: 0.00131921
Iteration 21/25 | Loss: 0.00131921
Iteration 22/25 | Loss: 0.00131921
Iteration 23/25 | Loss: 0.00131921
Iteration 24/25 | Loss: 0.00131920
Iteration 25/25 | Loss: 0.00131920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32495892
Iteration 2/25 | Loss: 0.00470389
Iteration 3/25 | Loss: 0.00293108
Iteration 4/25 | Loss: 0.00293106
Iteration 5/25 | Loss: 0.00293106
Iteration 6/25 | Loss: 0.00293106
Iteration 7/25 | Loss: 0.00293105
Iteration 8/25 | Loss: 0.00293105
Iteration 9/25 | Loss: 0.00293105
Iteration 10/25 | Loss: 0.00293105
Iteration 11/25 | Loss: 0.00293105
Iteration 12/25 | Loss: 0.00293105
Iteration 13/25 | Loss: 0.00293105
Iteration 14/25 | Loss: 0.00293105
Iteration 15/25 | Loss: 0.00293105
Iteration 16/25 | Loss: 0.00293105
Iteration 17/25 | Loss: 0.00293105
Iteration 18/25 | Loss: 0.00293105
Iteration 19/25 | Loss: 0.00293105
Iteration 20/25 | Loss: 0.00293105
Iteration 21/25 | Loss: 0.00293105
Iteration 22/25 | Loss: 0.00293105
Iteration 23/25 | Loss: 0.00293105
Iteration 24/25 | Loss: 0.00293105
Iteration 25/25 | Loss: 0.00293105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293105
Iteration 2/1000 | Loss: 0.00006346
Iteration 3/1000 | Loss: 0.00004073
Iteration 4/1000 | Loss: 0.00003512
Iteration 5/1000 | Loss: 0.00003301
Iteration 6/1000 | Loss: 0.00015038
Iteration 7/1000 | Loss: 0.00003945
Iteration 8/1000 | Loss: 0.00003173
Iteration 9/1000 | Loss: 0.00002921
Iteration 10/1000 | Loss: 0.00027378
Iteration 11/1000 | Loss: 0.00003226
Iteration 12/1000 | Loss: 0.00004105
Iteration 13/1000 | Loss: 0.00003158
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002301
Iteration 18/1000 | Loss: 0.00002272
Iteration 19/1000 | Loss: 0.00002254
Iteration 20/1000 | Loss: 0.00004063
Iteration 21/1000 | Loss: 0.00002236
Iteration 22/1000 | Loss: 0.00002232
Iteration 23/1000 | Loss: 0.00002231
Iteration 24/1000 | Loss: 0.00002225
Iteration 25/1000 | Loss: 0.00002224
Iteration 26/1000 | Loss: 0.00002224
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002219
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002216
Iteration 32/1000 | Loss: 0.00003726
Iteration 33/1000 | Loss: 0.00002249
Iteration 34/1000 | Loss: 0.00002206
Iteration 35/1000 | Loss: 0.00002202
Iteration 36/1000 | Loss: 0.00002202
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00002201
Iteration 43/1000 | Loss: 0.00002201
Iteration 44/1000 | Loss: 0.00002201
Iteration 45/1000 | Loss: 0.00002201
Iteration 46/1000 | Loss: 0.00002201
Iteration 47/1000 | Loss: 0.00002201
Iteration 48/1000 | Loss: 0.00002201
Iteration 49/1000 | Loss: 0.00002201
Iteration 50/1000 | Loss: 0.00002201
Iteration 51/1000 | Loss: 0.00002201
Iteration 52/1000 | Loss: 0.00002201
Iteration 53/1000 | Loss: 0.00002201
Iteration 54/1000 | Loss: 0.00002201
Iteration 55/1000 | Loss: 0.00002201
Iteration 56/1000 | Loss: 0.00002201
Iteration 57/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [2.201205825258512e-05, 2.201205825258512e-05, 2.201205825258512e-05, 2.201205825258512e-05, 2.201205825258512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.201205825258512e-05

Optimization complete. Final v2v error: 4.104152202606201 mm

Highest mean error: 5.132160663604736 mm for frame 72

Lowest mean error: 3.7724575996398926 mm for frame 11

Saving results

Total time: 66.49425411224365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828976
Iteration 2/25 | Loss: 0.00150201
Iteration 3/25 | Loss: 0.00134645
Iteration 4/25 | Loss: 0.00132500
Iteration 5/25 | Loss: 0.00131990
Iteration 6/25 | Loss: 0.00131892
Iteration 7/25 | Loss: 0.00131892
Iteration 8/25 | Loss: 0.00131892
Iteration 9/25 | Loss: 0.00131892
Iteration 10/25 | Loss: 0.00131892
Iteration 11/25 | Loss: 0.00131892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013189237797632813, 0.0013189237797632813, 0.0013189237797632813, 0.0013189237797632813, 0.0013189237797632813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013189237797632813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10874438
Iteration 2/25 | Loss: 0.00351120
Iteration 3/25 | Loss: 0.00351117
Iteration 4/25 | Loss: 0.00351117
Iteration 5/25 | Loss: 0.00351117
Iteration 6/25 | Loss: 0.00351117
Iteration 7/25 | Loss: 0.00351117
Iteration 8/25 | Loss: 0.00351117
Iteration 9/25 | Loss: 0.00351117
Iteration 10/25 | Loss: 0.00351117
Iteration 11/25 | Loss: 0.00351117
Iteration 12/25 | Loss: 0.00351117
Iteration 13/25 | Loss: 0.00351117
Iteration 14/25 | Loss: 0.00351117
Iteration 15/25 | Loss: 0.00351117
Iteration 16/25 | Loss: 0.00351117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0035111692268401384, 0.0035111692268401384, 0.0035111692268401384, 0.0035111692268401384, 0.0035111692268401384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035111692268401384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351117
Iteration 2/1000 | Loss: 0.00006542
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002413
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00001974
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001853
Iteration 10/1000 | Loss: 0.00001813
Iteration 11/1000 | Loss: 0.00001787
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001694
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001682
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001678
Iteration 38/1000 | Loss: 0.00001678
Iteration 39/1000 | Loss: 0.00001678
Iteration 40/1000 | Loss: 0.00001678
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001674
Iteration 47/1000 | Loss: 0.00001674
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001673
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001672
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001670
Iteration 63/1000 | Loss: 0.00001670
Iteration 64/1000 | Loss: 0.00001669
Iteration 65/1000 | Loss: 0.00001669
Iteration 66/1000 | Loss: 0.00001669
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001668
Iteration 69/1000 | Loss: 0.00001668
Iteration 70/1000 | Loss: 0.00001668
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001668
Iteration 73/1000 | Loss: 0.00001668
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001667
Iteration 76/1000 | Loss: 0.00001667
Iteration 77/1000 | Loss: 0.00001667
Iteration 78/1000 | Loss: 0.00001667
Iteration 79/1000 | Loss: 0.00001667
Iteration 80/1000 | Loss: 0.00001667
Iteration 81/1000 | Loss: 0.00001667
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001662
Iteration 117/1000 | Loss: 0.00001662
Iteration 118/1000 | Loss: 0.00001662
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6610823877272196e-05, 1.6610823877272196e-05, 1.6610823877272196e-05, 1.6610823877272196e-05, 1.6610823877272196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6610823877272196e-05

Optimization complete. Final v2v error: 3.564223051071167 mm

Highest mean error: 3.8090696334838867 mm for frame 97

Lowest mean error: 3.2147350311279297 mm for frame 14

Saving results

Total time: 42.44926714897156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414959
Iteration 2/25 | Loss: 0.00140893
Iteration 3/25 | Loss: 0.00129132
Iteration 4/25 | Loss: 0.00127713
Iteration 5/25 | Loss: 0.00127324
Iteration 6/25 | Loss: 0.00127262
Iteration 7/25 | Loss: 0.00127262
Iteration 8/25 | Loss: 0.00127262
Iteration 9/25 | Loss: 0.00127262
Iteration 10/25 | Loss: 0.00127262
Iteration 11/25 | Loss: 0.00127262
Iteration 12/25 | Loss: 0.00127262
Iteration 13/25 | Loss: 0.00127262
Iteration 14/25 | Loss: 0.00127262
Iteration 15/25 | Loss: 0.00127262
Iteration 16/25 | Loss: 0.00127262
Iteration 17/25 | Loss: 0.00127262
Iteration 18/25 | Loss: 0.00127262
Iteration 19/25 | Loss: 0.00127262
Iteration 20/25 | Loss: 0.00127262
Iteration 21/25 | Loss: 0.00127262
Iteration 22/25 | Loss: 0.00127262
Iteration 23/25 | Loss: 0.00127262
Iteration 24/25 | Loss: 0.00127262
Iteration 25/25 | Loss: 0.00127262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15252471
Iteration 2/25 | Loss: 0.00287761
Iteration 3/25 | Loss: 0.00287761
Iteration 4/25 | Loss: 0.00287761
Iteration 5/25 | Loss: 0.00287761
Iteration 6/25 | Loss: 0.00287761
Iteration 7/25 | Loss: 0.00287761
Iteration 8/25 | Loss: 0.00287761
Iteration 9/25 | Loss: 0.00287761
Iteration 10/25 | Loss: 0.00287761
Iteration 11/25 | Loss: 0.00287761
Iteration 12/25 | Loss: 0.00287761
Iteration 13/25 | Loss: 0.00287761
Iteration 14/25 | Loss: 0.00287761
Iteration 15/25 | Loss: 0.00287761
Iteration 16/25 | Loss: 0.00287761
Iteration 17/25 | Loss: 0.00287761
Iteration 18/25 | Loss: 0.00287761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0028776053804904222, 0.0028776053804904222, 0.0028776053804904222, 0.0028776053804904222, 0.0028776053804904222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028776053804904222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287761
Iteration 2/1000 | Loss: 0.00004668
Iteration 3/1000 | Loss: 0.00002756
Iteration 4/1000 | Loss: 0.00002316
Iteration 5/1000 | Loss: 0.00002178
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001926
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001832
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001785
Iteration 16/1000 | Loss: 0.00001785
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001775
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001766
Iteration 37/1000 | Loss: 0.00001766
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001759
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001757
Iteration 77/1000 | Loss: 0.00001757
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001755
Iteration 105/1000 | Loss: 0.00001755
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001753
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.752881689753849e-05, 1.752881689753849e-05, 1.752881689753849e-05, 1.752881689753849e-05, 1.752881689753849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.752881689753849e-05

Optimization complete. Final v2v error: 3.6457130908966064 mm

Highest mean error: 3.9516849517822266 mm for frame 63

Lowest mean error: 3.3492841720581055 mm for frame 177

Saving results

Total time: 35.68271446228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_2859/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_2859/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637652
Iteration 2/25 | Loss: 0.00141016
Iteration 3/25 | Loss: 0.00128420
Iteration 4/25 | Loss: 0.00126768
Iteration 5/25 | Loss: 0.00125998
Iteration 6/25 | Loss: 0.00125746
Iteration 7/25 | Loss: 0.00125725
Iteration 8/25 | Loss: 0.00125725
Iteration 9/25 | Loss: 0.00125725
Iteration 10/25 | Loss: 0.00125725
Iteration 11/25 | Loss: 0.00125725
Iteration 12/25 | Loss: 0.00125725
Iteration 13/25 | Loss: 0.00125725
Iteration 14/25 | Loss: 0.00125725
Iteration 15/25 | Loss: 0.00125725
Iteration 16/25 | Loss: 0.00125725
Iteration 17/25 | Loss: 0.00125725
Iteration 18/25 | Loss: 0.00125725
Iteration 19/25 | Loss: 0.00125725
Iteration 20/25 | Loss: 0.00125725
Iteration 21/25 | Loss: 0.00125725
Iteration 22/25 | Loss: 0.00125725
Iteration 23/25 | Loss: 0.00125725
Iteration 24/25 | Loss: 0.00125725
Iteration 25/25 | Loss: 0.00125725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.27119780
Iteration 2/25 | Loss: 0.00286838
Iteration 3/25 | Loss: 0.00286838
Iteration 4/25 | Loss: 0.00286838
Iteration 5/25 | Loss: 0.00286838
Iteration 6/25 | Loss: 0.00286838
Iteration 7/25 | Loss: 0.00286838
Iteration 8/25 | Loss: 0.00286838
Iteration 9/25 | Loss: 0.00286838
Iteration 10/25 | Loss: 0.00286838
Iteration 11/25 | Loss: 0.00286838
Iteration 12/25 | Loss: 0.00286838
Iteration 13/25 | Loss: 0.00286838
Iteration 14/25 | Loss: 0.00286838
Iteration 15/25 | Loss: 0.00286838
Iteration 16/25 | Loss: 0.00286838
Iteration 17/25 | Loss: 0.00286838
Iteration 18/25 | Loss: 0.00286838
Iteration 19/25 | Loss: 0.00286838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002868381794542074, 0.002868381794542074, 0.002868381794542074, 0.002868381794542074, 0.002868381794542074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002868381794542074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286838
Iteration 2/1000 | Loss: 0.00005108
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001934
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001792
Iteration 9/1000 | Loss: 0.00001740
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001675
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001615
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001609
Iteration 30/1000 | Loss: 0.00001609
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001607
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001605
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001604
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001599
Iteration 67/1000 | Loss: 0.00001599
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001598
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001598
Iteration 94/1000 | Loss: 0.00001598
Iteration 95/1000 | Loss: 0.00001598
Iteration 96/1000 | Loss: 0.00001598
Iteration 97/1000 | Loss: 0.00001598
Iteration 98/1000 | Loss: 0.00001598
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001597
Iteration 103/1000 | Loss: 0.00001597
Iteration 104/1000 | Loss: 0.00001597
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001596
Iteration 113/1000 | Loss: 0.00001596
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001596
Iteration 116/1000 | Loss: 0.00001596
Iteration 117/1000 | Loss: 0.00001596
Iteration 118/1000 | Loss: 0.00001596
Iteration 119/1000 | Loss: 0.00001596
Iteration 120/1000 | Loss: 0.00001596
Iteration 121/1000 | Loss: 0.00001596
Iteration 122/1000 | Loss: 0.00001596
Iteration 123/1000 | Loss: 0.00001596
Iteration 124/1000 | Loss: 0.00001596
Iteration 125/1000 | Loss: 0.00001596
Iteration 126/1000 | Loss: 0.00001596
Iteration 127/1000 | Loss: 0.00001596
Iteration 128/1000 | Loss: 0.00001596
Iteration 129/1000 | Loss: 0.00001596
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001595
Iteration 134/1000 | Loss: 0.00001595
Iteration 135/1000 | Loss: 0.00001595
Iteration 136/1000 | Loss: 0.00001595
Iteration 137/1000 | Loss: 0.00001595
Iteration 138/1000 | Loss: 0.00001595
Iteration 139/1000 | Loss: 0.00001595
Iteration 140/1000 | Loss: 0.00001595
Iteration 141/1000 | Loss: 0.00001595
Iteration 142/1000 | Loss: 0.00001595
Iteration 143/1000 | Loss: 0.00001595
Iteration 144/1000 | Loss: 0.00001595
Iteration 145/1000 | Loss: 0.00001595
Iteration 146/1000 | Loss: 0.00001595
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.5948682630551048e-05, 1.5948682630551048e-05, 1.5948682630551048e-05, 1.5948682630551048e-05, 1.5948682630551048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5948682630551048e-05

Optimization complete. Final v2v error: 3.4220848083496094 mm

Highest mean error: 3.868713617324829 mm for frame 147

Lowest mean error: 3.1494686603546143 mm for frame 7

Saving results

Total time: 36.30494976043701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013874
Iteration 2/25 | Loss: 0.00204104
Iteration 3/25 | Loss: 0.00128463
Iteration 4/25 | Loss: 0.00114383
Iteration 5/25 | Loss: 0.00105249
Iteration 6/25 | Loss: 0.00103862
Iteration 7/25 | Loss: 0.00098210
Iteration 8/25 | Loss: 0.00097042
Iteration 9/25 | Loss: 0.00097065
Iteration 10/25 | Loss: 0.00096725
Iteration 11/25 | Loss: 0.00094952
Iteration 12/25 | Loss: 0.00094799
Iteration 13/25 | Loss: 0.00094493
Iteration 14/25 | Loss: 0.00093932
Iteration 15/25 | Loss: 0.00093627
Iteration 16/25 | Loss: 0.00093395
Iteration 17/25 | Loss: 0.00093262
Iteration 18/25 | Loss: 0.00093557
Iteration 19/25 | Loss: 0.00093309
Iteration 20/25 | Loss: 0.00092993
Iteration 21/25 | Loss: 0.00092856
Iteration 22/25 | Loss: 0.00092792
Iteration 23/25 | Loss: 0.00093254
Iteration 24/25 | Loss: 0.00093258
Iteration 25/25 | Loss: 0.00092291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44629991
Iteration 2/25 | Loss: 0.00139076
Iteration 3/25 | Loss: 0.00091142
Iteration 4/25 | Loss: 0.00091142
Iteration 5/25 | Loss: 0.00091142
Iteration 6/25 | Loss: 0.00091142
Iteration 7/25 | Loss: 0.00091142
Iteration 8/25 | Loss: 0.00091142
Iteration 9/25 | Loss: 0.00091142
Iteration 10/25 | Loss: 0.00091142
Iteration 11/25 | Loss: 0.00091142
Iteration 12/25 | Loss: 0.00091142
Iteration 13/25 | Loss: 0.00091142
Iteration 14/25 | Loss: 0.00091142
Iteration 15/25 | Loss: 0.00091142
Iteration 16/25 | Loss: 0.00091142
Iteration 17/25 | Loss: 0.00091142
Iteration 18/25 | Loss: 0.00091142
Iteration 19/25 | Loss: 0.00091142
Iteration 20/25 | Loss: 0.00091142
Iteration 21/25 | Loss: 0.00091142
Iteration 22/25 | Loss: 0.00091142
Iteration 23/25 | Loss: 0.00091142
Iteration 24/25 | Loss: 0.00091142
Iteration 25/25 | Loss: 0.00091142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091142
Iteration 2/1000 | Loss: 0.00096879
Iteration 3/1000 | Loss: 0.00076133
Iteration 4/1000 | Loss: 0.00038607
Iteration 5/1000 | Loss: 0.00007226
Iteration 6/1000 | Loss: 0.00089341
Iteration 7/1000 | Loss: 0.00012079
Iteration 8/1000 | Loss: 0.00003287
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00036849
Iteration 11/1000 | Loss: 0.00003082
Iteration 12/1000 | Loss: 0.00013811
Iteration 13/1000 | Loss: 0.00003708
Iteration 14/1000 | Loss: 0.00002786
Iteration 15/1000 | Loss: 0.00014807
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00021120
Iteration 18/1000 | Loss: 0.00080427
Iteration 19/1000 | Loss: 0.00020858
Iteration 20/1000 | Loss: 0.00013679
Iteration 21/1000 | Loss: 0.00001847
Iteration 22/1000 | Loss: 0.00005550
Iteration 23/1000 | Loss: 0.00001829
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001737
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001725
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001724
Iteration 68/1000 | Loss: 0.00001724
Iteration 69/1000 | Loss: 0.00001724
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001721
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001716
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7163964002975263e-05, 1.7163964002975263e-05, 1.7163964002975263e-05, 1.7163964002975263e-05, 1.7163964002975263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7163964002975263e-05

Optimization complete. Final v2v error: 3.498506784439087 mm

Highest mean error: 4.0868611335754395 mm for frame 134

Lowest mean error: 3.0182735919952393 mm for frame 181

Saving results

Total time: 104.07657289505005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845812
Iteration 2/25 | Loss: 0.00136562
Iteration 3/25 | Loss: 0.00098979
Iteration 4/25 | Loss: 0.00093952
Iteration 5/25 | Loss: 0.00092903
Iteration 6/25 | Loss: 0.00092758
Iteration 7/25 | Loss: 0.00092726
Iteration 8/25 | Loss: 0.00092726
Iteration 9/25 | Loss: 0.00092726
Iteration 10/25 | Loss: 0.00092726
Iteration 11/25 | Loss: 0.00092726
Iteration 12/25 | Loss: 0.00092726
Iteration 13/25 | Loss: 0.00092726
Iteration 14/25 | Loss: 0.00092726
Iteration 15/25 | Loss: 0.00092726
Iteration 16/25 | Loss: 0.00092726
Iteration 17/25 | Loss: 0.00092726
Iteration 18/25 | Loss: 0.00092726
Iteration 19/25 | Loss: 0.00092726
Iteration 20/25 | Loss: 0.00092726
Iteration 21/25 | Loss: 0.00092726
Iteration 22/25 | Loss: 0.00092726
Iteration 23/25 | Loss: 0.00092726
Iteration 24/25 | Loss: 0.00092726
Iteration 25/25 | Loss: 0.00092726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44736016
Iteration 2/25 | Loss: 0.00041008
Iteration 3/25 | Loss: 0.00041002
Iteration 4/25 | Loss: 0.00041002
Iteration 5/25 | Loss: 0.00041002
Iteration 6/25 | Loss: 0.00041002
Iteration 7/25 | Loss: 0.00041002
Iteration 8/25 | Loss: 0.00041002
Iteration 9/25 | Loss: 0.00041002
Iteration 10/25 | Loss: 0.00041002
Iteration 11/25 | Loss: 0.00041002
Iteration 12/25 | Loss: 0.00041002
Iteration 13/25 | Loss: 0.00041002
Iteration 14/25 | Loss: 0.00041002
Iteration 15/25 | Loss: 0.00041002
Iteration 16/25 | Loss: 0.00041002
Iteration 17/25 | Loss: 0.00041002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00041001985664479434, 0.00041001985664479434, 0.00041001985664479434, 0.00041001985664479434, 0.00041001985664479434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041001985664479434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041002
Iteration 2/1000 | Loss: 0.00003373
Iteration 3/1000 | Loss: 0.00002530
Iteration 4/1000 | Loss: 0.00002180
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001934
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001790
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001705
Iteration 37/1000 | Loss: 0.00001705
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001704
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6999187209876254e-05, 1.6999187209876254e-05, 1.6999187209876254e-05, 1.6999187209876254e-05, 1.6999187209876254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6999187209876254e-05

Optimization complete. Final v2v error: 3.534177780151367 mm

Highest mean error: 3.9122798442840576 mm for frame 137

Lowest mean error: 3.172116994857788 mm for frame 14

Saving results

Total time: 31.677721738815308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373939
Iteration 2/25 | Loss: 0.00090803
Iteration 3/25 | Loss: 0.00081855
Iteration 4/25 | Loss: 0.00081056
Iteration 5/25 | Loss: 0.00080780
Iteration 6/25 | Loss: 0.00080691
Iteration 7/25 | Loss: 0.00080678
Iteration 8/25 | Loss: 0.00080678
Iteration 9/25 | Loss: 0.00080678
Iteration 10/25 | Loss: 0.00080678
Iteration 11/25 | Loss: 0.00080678
Iteration 12/25 | Loss: 0.00080678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008067799499258399, 0.0008067799499258399, 0.0008067799499258399, 0.0008067799499258399, 0.0008067799499258399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008067799499258399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75651264
Iteration 2/25 | Loss: 0.00053661
Iteration 3/25 | Loss: 0.00053661
Iteration 4/25 | Loss: 0.00053661
Iteration 5/25 | Loss: 0.00053661
Iteration 6/25 | Loss: 0.00053660
Iteration 7/25 | Loss: 0.00053660
Iteration 8/25 | Loss: 0.00053660
Iteration 9/25 | Loss: 0.00053660
Iteration 10/25 | Loss: 0.00053660
Iteration 11/25 | Loss: 0.00053660
Iteration 12/25 | Loss: 0.00053660
Iteration 13/25 | Loss: 0.00053660
Iteration 14/25 | Loss: 0.00053660
Iteration 15/25 | Loss: 0.00053660
Iteration 16/25 | Loss: 0.00053660
Iteration 17/25 | Loss: 0.00053660
Iteration 18/25 | Loss: 0.00053660
Iteration 19/25 | Loss: 0.00053660
Iteration 20/25 | Loss: 0.00053660
Iteration 21/25 | Loss: 0.00053660
Iteration 22/25 | Loss: 0.00053660
Iteration 23/25 | Loss: 0.00053660
Iteration 24/25 | Loss: 0.00053660
Iteration 25/25 | Loss: 0.00053660

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053660
Iteration 2/1000 | Loss: 0.00002110
Iteration 3/1000 | Loss: 0.00001230
Iteration 4/1000 | Loss: 0.00001135
Iteration 5/1000 | Loss: 0.00001059
Iteration 6/1000 | Loss: 0.00001031
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00001006
Iteration 9/1000 | Loss: 0.00000995
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000990
Iteration 13/1000 | Loss: 0.00000990
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000990
Iteration 16/1000 | Loss: 0.00000989
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000984
Iteration 20/1000 | Loss: 0.00000984
Iteration 21/1000 | Loss: 0.00000983
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000982
Iteration 24/1000 | Loss: 0.00000980
Iteration 25/1000 | Loss: 0.00000979
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000977
Iteration 30/1000 | Loss: 0.00000977
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000967
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000952
Iteration 59/1000 | Loss: 0.00000952
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000950
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000948
Iteration 69/1000 | Loss: 0.00000948
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000948
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000947
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000945
Iteration 82/1000 | Loss: 0.00000945
Iteration 83/1000 | Loss: 0.00000945
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000944
Iteration 87/1000 | Loss: 0.00000944
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000942
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000941
Iteration 115/1000 | Loss: 0.00000941
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000939
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000939
Iteration 124/1000 | Loss: 0.00000938
Iteration 125/1000 | Loss: 0.00000938
Iteration 126/1000 | Loss: 0.00000938
Iteration 127/1000 | Loss: 0.00000938
Iteration 128/1000 | Loss: 0.00000937
Iteration 129/1000 | Loss: 0.00000937
Iteration 130/1000 | Loss: 0.00000937
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000935
Iteration 139/1000 | Loss: 0.00000935
Iteration 140/1000 | Loss: 0.00000935
Iteration 141/1000 | Loss: 0.00000935
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000934
Iteration 144/1000 | Loss: 0.00000934
Iteration 145/1000 | Loss: 0.00000934
Iteration 146/1000 | Loss: 0.00000933
Iteration 147/1000 | Loss: 0.00000933
Iteration 148/1000 | Loss: 0.00000933
Iteration 149/1000 | Loss: 0.00000933
Iteration 150/1000 | Loss: 0.00000933
Iteration 151/1000 | Loss: 0.00000933
Iteration 152/1000 | Loss: 0.00000933
Iteration 153/1000 | Loss: 0.00000933
Iteration 154/1000 | Loss: 0.00000933
Iteration 155/1000 | Loss: 0.00000933
Iteration 156/1000 | Loss: 0.00000933
Iteration 157/1000 | Loss: 0.00000932
Iteration 158/1000 | Loss: 0.00000932
Iteration 159/1000 | Loss: 0.00000932
Iteration 160/1000 | Loss: 0.00000932
Iteration 161/1000 | Loss: 0.00000932
Iteration 162/1000 | Loss: 0.00000932
Iteration 163/1000 | Loss: 0.00000932
Iteration 164/1000 | Loss: 0.00000932
Iteration 165/1000 | Loss: 0.00000932
Iteration 166/1000 | Loss: 0.00000932
Iteration 167/1000 | Loss: 0.00000932
Iteration 168/1000 | Loss: 0.00000932
Iteration 169/1000 | Loss: 0.00000932
Iteration 170/1000 | Loss: 0.00000932
Iteration 171/1000 | Loss: 0.00000932
Iteration 172/1000 | Loss: 0.00000932
Iteration 173/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [9.32475541048916e-06, 9.32475541048916e-06, 9.32475541048916e-06, 9.32475541048916e-06, 9.32475541048916e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.32475541048916e-06

Optimization complete. Final v2v error: 2.626065492630005 mm

Highest mean error: 3.0288331508636475 mm for frame 85

Lowest mean error: 2.544403553009033 mm for frame 70

Saving results

Total time: 34.733370780944824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818775
Iteration 2/25 | Loss: 0.00179186
Iteration 3/25 | Loss: 0.00125949
Iteration 4/25 | Loss: 0.00117119
Iteration 5/25 | Loss: 0.00112012
Iteration 6/25 | Loss: 0.00105035
Iteration 7/25 | Loss: 0.00103292
Iteration 8/25 | Loss: 0.00099975
Iteration 9/25 | Loss: 0.00098683
Iteration 10/25 | Loss: 0.00097403
Iteration 11/25 | Loss: 0.00097359
Iteration 12/25 | Loss: 0.00096403
Iteration 13/25 | Loss: 0.00096094
Iteration 14/25 | Loss: 0.00095931
Iteration 15/25 | Loss: 0.00096252
Iteration 16/25 | Loss: 0.00095609
Iteration 17/25 | Loss: 0.00095449
Iteration 18/25 | Loss: 0.00095418
Iteration 19/25 | Loss: 0.00095413
Iteration 20/25 | Loss: 0.00095412
Iteration 21/25 | Loss: 0.00095412
Iteration 22/25 | Loss: 0.00095412
Iteration 23/25 | Loss: 0.00095412
Iteration 24/25 | Loss: 0.00095412
Iteration 25/25 | Loss: 0.00095412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53661704
Iteration 2/25 | Loss: 0.00088877
Iteration 3/25 | Loss: 0.00088877
Iteration 4/25 | Loss: 0.00088876
Iteration 5/25 | Loss: 0.00088876
Iteration 6/25 | Loss: 0.00088876
Iteration 7/25 | Loss: 0.00088876
Iteration 8/25 | Loss: 0.00088876
Iteration 9/25 | Loss: 0.00088876
Iteration 10/25 | Loss: 0.00088876
Iteration 11/25 | Loss: 0.00088876
Iteration 12/25 | Loss: 0.00088876
Iteration 13/25 | Loss: 0.00088876
Iteration 14/25 | Loss: 0.00088876
Iteration 15/25 | Loss: 0.00088876
Iteration 16/25 | Loss: 0.00088876
Iteration 17/25 | Loss: 0.00088876
Iteration 18/25 | Loss: 0.00088876
Iteration 19/25 | Loss: 0.00088876
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008887611911632121, 0.0008887611911632121, 0.0008887611911632121, 0.0008887611911632121, 0.0008887611911632121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008887611911632121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088876
Iteration 2/1000 | Loss: 0.00025985
Iteration 3/1000 | Loss: 0.00073454
Iteration 4/1000 | Loss: 0.00008865
Iteration 5/1000 | Loss: 0.00006359
Iteration 6/1000 | Loss: 0.00004680
Iteration 7/1000 | Loss: 0.00003814
Iteration 8/1000 | Loss: 0.00003285
Iteration 9/1000 | Loss: 0.00003041
Iteration 10/1000 | Loss: 0.00056923
Iteration 11/1000 | Loss: 0.00029440
Iteration 12/1000 | Loss: 0.00018483
Iteration 13/1000 | Loss: 0.00023246
Iteration 14/1000 | Loss: 0.00015896
Iteration 15/1000 | Loss: 0.00003010
Iteration 16/1000 | Loss: 0.00002734
Iteration 17/1000 | Loss: 0.00002651
Iteration 18/1000 | Loss: 0.00002584
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002450
Iteration 22/1000 | Loss: 0.00002420
Iteration 23/1000 | Loss: 0.00002416
Iteration 24/1000 | Loss: 0.00002411
Iteration 25/1000 | Loss: 0.00002402
Iteration 26/1000 | Loss: 0.00002386
Iteration 27/1000 | Loss: 0.00002378
Iteration 28/1000 | Loss: 0.00002374
Iteration 29/1000 | Loss: 0.00002372
Iteration 30/1000 | Loss: 0.00002367
Iteration 31/1000 | Loss: 0.00002362
Iteration 32/1000 | Loss: 0.00002361
Iteration 33/1000 | Loss: 0.00002360
Iteration 34/1000 | Loss: 0.00002359
Iteration 35/1000 | Loss: 0.00002355
Iteration 36/1000 | Loss: 0.00002353
Iteration 37/1000 | Loss: 0.00002352
Iteration 38/1000 | Loss: 0.00002352
Iteration 39/1000 | Loss: 0.00002352
Iteration 40/1000 | Loss: 0.00002352
Iteration 41/1000 | Loss: 0.00002352
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00002351
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002350
Iteration 48/1000 | Loss: 0.00002349
Iteration 49/1000 | Loss: 0.00002349
Iteration 50/1000 | Loss: 0.00002349
Iteration 51/1000 | Loss: 0.00002349
Iteration 52/1000 | Loss: 0.00002348
Iteration 53/1000 | Loss: 0.00002348
Iteration 54/1000 | Loss: 0.00002348
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002347
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002344
Iteration 65/1000 | Loss: 0.00002344
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002343
Iteration 68/1000 | Loss: 0.00002343
Iteration 69/1000 | Loss: 0.00002342
Iteration 70/1000 | Loss: 0.00002342
Iteration 71/1000 | Loss: 0.00002341
Iteration 72/1000 | Loss: 0.00002340
Iteration 73/1000 | Loss: 0.00002340
Iteration 74/1000 | Loss: 0.00002340
Iteration 75/1000 | Loss: 0.00002339
Iteration 76/1000 | Loss: 0.00002339
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002338
Iteration 80/1000 | Loss: 0.00002337
Iteration 81/1000 | Loss: 0.00002337
Iteration 82/1000 | Loss: 0.00002337
Iteration 83/1000 | Loss: 0.00002336
Iteration 84/1000 | Loss: 0.00002336
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002335
Iteration 88/1000 | Loss: 0.00002334
Iteration 89/1000 | Loss: 0.00002334
Iteration 90/1000 | Loss: 0.00002333
Iteration 91/1000 | Loss: 0.00002333
Iteration 92/1000 | Loss: 0.00002333
Iteration 93/1000 | Loss: 0.00002333
Iteration 94/1000 | Loss: 0.00002333
Iteration 95/1000 | Loss: 0.00002333
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002332
Iteration 102/1000 | Loss: 0.00002332
Iteration 103/1000 | Loss: 0.00002332
Iteration 104/1000 | Loss: 0.00002332
Iteration 105/1000 | Loss: 0.00002332
Iteration 106/1000 | Loss: 0.00002331
Iteration 107/1000 | Loss: 0.00002331
Iteration 108/1000 | Loss: 0.00002331
Iteration 109/1000 | Loss: 0.00002331
Iteration 110/1000 | Loss: 0.00002331
Iteration 111/1000 | Loss: 0.00002331
Iteration 112/1000 | Loss: 0.00002331
Iteration 113/1000 | Loss: 0.00002331
Iteration 114/1000 | Loss: 0.00002331
Iteration 115/1000 | Loss: 0.00002331
Iteration 116/1000 | Loss: 0.00002330
Iteration 117/1000 | Loss: 0.00002330
Iteration 118/1000 | Loss: 0.00002330
Iteration 119/1000 | Loss: 0.00002330
Iteration 120/1000 | Loss: 0.00002330
Iteration 121/1000 | Loss: 0.00002330
Iteration 122/1000 | Loss: 0.00002330
Iteration 123/1000 | Loss: 0.00002330
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002330
Iteration 127/1000 | Loss: 0.00002330
Iteration 128/1000 | Loss: 0.00002330
Iteration 129/1000 | Loss: 0.00002330
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002329
Iteration 132/1000 | Loss: 0.00002329
Iteration 133/1000 | Loss: 0.00002329
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002329
Iteration 136/1000 | Loss: 0.00002329
Iteration 137/1000 | Loss: 0.00002329
Iteration 138/1000 | Loss: 0.00002329
Iteration 139/1000 | Loss: 0.00002329
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002329
Iteration 142/1000 | Loss: 0.00002328
Iteration 143/1000 | Loss: 0.00002328
Iteration 144/1000 | Loss: 0.00002328
Iteration 145/1000 | Loss: 0.00002328
Iteration 146/1000 | Loss: 0.00002328
Iteration 147/1000 | Loss: 0.00002328
Iteration 148/1000 | Loss: 0.00002328
Iteration 149/1000 | Loss: 0.00002328
Iteration 150/1000 | Loss: 0.00002328
Iteration 151/1000 | Loss: 0.00002328
Iteration 152/1000 | Loss: 0.00002328
Iteration 153/1000 | Loss: 0.00002328
Iteration 154/1000 | Loss: 0.00002328
Iteration 155/1000 | Loss: 0.00002328
Iteration 156/1000 | Loss: 0.00002328
Iteration 157/1000 | Loss: 0.00002327
Iteration 158/1000 | Loss: 0.00002327
Iteration 159/1000 | Loss: 0.00002327
Iteration 160/1000 | Loss: 0.00002327
Iteration 161/1000 | Loss: 0.00002327
Iteration 162/1000 | Loss: 0.00002327
Iteration 163/1000 | Loss: 0.00002327
Iteration 164/1000 | Loss: 0.00002327
Iteration 165/1000 | Loss: 0.00002327
Iteration 166/1000 | Loss: 0.00002327
Iteration 167/1000 | Loss: 0.00002326
Iteration 168/1000 | Loss: 0.00002326
Iteration 169/1000 | Loss: 0.00002326
Iteration 170/1000 | Loss: 0.00002326
Iteration 171/1000 | Loss: 0.00002326
Iteration 172/1000 | Loss: 0.00002326
Iteration 173/1000 | Loss: 0.00002326
Iteration 174/1000 | Loss: 0.00002325
Iteration 175/1000 | Loss: 0.00002325
Iteration 176/1000 | Loss: 0.00002325
Iteration 177/1000 | Loss: 0.00002325
Iteration 178/1000 | Loss: 0.00002325
Iteration 179/1000 | Loss: 0.00002325
Iteration 180/1000 | Loss: 0.00002325
Iteration 181/1000 | Loss: 0.00002325
Iteration 182/1000 | Loss: 0.00002325
Iteration 183/1000 | Loss: 0.00002325
Iteration 184/1000 | Loss: 0.00002325
Iteration 185/1000 | Loss: 0.00002325
Iteration 186/1000 | Loss: 0.00002325
Iteration 187/1000 | Loss: 0.00002325
Iteration 188/1000 | Loss: 0.00002325
Iteration 189/1000 | Loss: 0.00002325
Iteration 190/1000 | Loss: 0.00002324
Iteration 191/1000 | Loss: 0.00002324
Iteration 192/1000 | Loss: 0.00002324
Iteration 193/1000 | Loss: 0.00002324
Iteration 194/1000 | Loss: 0.00002324
Iteration 195/1000 | Loss: 0.00002324
Iteration 196/1000 | Loss: 0.00002324
Iteration 197/1000 | Loss: 0.00002324
Iteration 198/1000 | Loss: 0.00002324
Iteration 199/1000 | Loss: 0.00002324
Iteration 200/1000 | Loss: 0.00002324
Iteration 201/1000 | Loss: 0.00002324
Iteration 202/1000 | Loss: 0.00002324
Iteration 203/1000 | Loss: 0.00002324
Iteration 204/1000 | Loss: 0.00002324
Iteration 205/1000 | Loss: 0.00002323
Iteration 206/1000 | Loss: 0.00002323
Iteration 207/1000 | Loss: 0.00002323
Iteration 208/1000 | Loss: 0.00002323
Iteration 209/1000 | Loss: 0.00002323
Iteration 210/1000 | Loss: 0.00002323
Iteration 211/1000 | Loss: 0.00002323
Iteration 212/1000 | Loss: 0.00002323
Iteration 213/1000 | Loss: 0.00002323
Iteration 214/1000 | Loss: 0.00002323
Iteration 215/1000 | Loss: 0.00002323
Iteration 216/1000 | Loss: 0.00002323
Iteration 217/1000 | Loss: 0.00002323
Iteration 218/1000 | Loss: 0.00002323
Iteration 219/1000 | Loss: 0.00002323
Iteration 220/1000 | Loss: 0.00002323
Iteration 221/1000 | Loss: 0.00002323
Iteration 222/1000 | Loss: 0.00002323
Iteration 223/1000 | Loss: 0.00002323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.3225922632263973e-05, 2.3225922632263973e-05, 2.3225922632263973e-05, 2.3225922632263973e-05, 2.3225922632263973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3225922632263973e-05

Optimization complete. Final v2v error: 4.010889053344727 mm

Highest mean error: 4.574060440063477 mm for frame 0

Lowest mean error: 3.333163022994995 mm for frame 34

Saving results

Total time: 92.71421885490417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619336
Iteration 2/25 | Loss: 0.00116715
Iteration 3/25 | Loss: 0.00089917
Iteration 4/25 | Loss: 0.00086449
Iteration 5/25 | Loss: 0.00085830
Iteration 6/25 | Loss: 0.00085658
Iteration 7/25 | Loss: 0.00085633
Iteration 8/25 | Loss: 0.00085633
Iteration 9/25 | Loss: 0.00085633
Iteration 10/25 | Loss: 0.00085633
Iteration 11/25 | Loss: 0.00085633
Iteration 12/25 | Loss: 0.00085633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008563299779780209, 0.0008563299779780209, 0.0008563299779780209, 0.0008563299779780209, 0.0008563299779780209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008563299779780209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23033237
Iteration 2/25 | Loss: 0.00049208
Iteration 3/25 | Loss: 0.00049208
Iteration 4/25 | Loss: 0.00049208
Iteration 5/25 | Loss: 0.00049208
Iteration 6/25 | Loss: 0.00049208
Iteration 7/25 | Loss: 0.00049208
Iteration 8/25 | Loss: 0.00049208
Iteration 9/25 | Loss: 0.00049208
Iteration 10/25 | Loss: 0.00049208
Iteration 11/25 | Loss: 0.00049208
Iteration 12/25 | Loss: 0.00049208
Iteration 13/25 | Loss: 0.00049208
Iteration 14/25 | Loss: 0.00049208
Iteration 15/25 | Loss: 0.00049208
Iteration 16/25 | Loss: 0.00049208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004920752835460007, 0.0004920752835460007, 0.0004920752835460007, 0.0004920752835460007, 0.0004920752835460007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004920752835460007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049208
Iteration 2/1000 | Loss: 0.00002210
Iteration 3/1000 | Loss: 0.00001739
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001301
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001297
Iteration 20/1000 | Loss: 0.00001296
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001283
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001281
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001272
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001270
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001270
Iteration 62/1000 | Loss: 0.00001270
Iteration 63/1000 | Loss: 0.00001270
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001266
Iteration 77/1000 | Loss: 0.00001266
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.2637596228159964e-05, 1.2637596228159964e-05, 1.2637596228159964e-05, 1.2637596228159964e-05, 1.2637596228159964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2637596228159964e-05

Optimization complete. Final v2v error: 2.987612247467041 mm

Highest mean error: 4.013730049133301 mm for frame 33

Lowest mean error: 2.6541717052459717 mm for frame 164

Saving results

Total time: 31.145925760269165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649457
Iteration 2/25 | Loss: 0.00133229
Iteration 3/25 | Loss: 0.00104889
Iteration 4/25 | Loss: 0.00102132
Iteration 5/25 | Loss: 0.00101338
Iteration 6/25 | Loss: 0.00101126
Iteration 7/25 | Loss: 0.00101079
Iteration 8/25 | Loss: 0.00101079
Iteration 9/25 | Loss: 0.00101079
Iteration 10/25 | Loss: 0.00101079
Iteration 11/25 | Loss: 0.00101079
Iteration 12/25 | Loss: 0.00101079
Iteration 13/25 | Loss: 0.00101079
Iteration 14/25 | Loss: 0.00101079
Iteration 15/25 | Loss: 0.00101079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001010791165754199, 0.001010791165754199, 0.001010791165754199, 0.001010791165754199, 0.001010791165754199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001010791165754199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50702465
Iteration 2/25 | Loss: 0.00081122
Iteration 3/25 | Loss: 0.00081118
Iteration 4/25 | Loss: 0.00081118
Iteration 5/25 | Loss: 0.00081118
Iteration 6/25 | Loss: 0.00081118
Iteration 7/25 | Loss: 0.00081118
Iteration 8/25 | Loss: 0.00081118
Iteration 9/25 | Loss: 0.00081118
Iteration 10/25 | Loss: 0.00081118
Iteration 11/25 | Loss: 0.00081118
Iteration 12/25 | Loss: 0.00081118
Iteration 13/25 | Loss: 0.00081118
Iteration 14/25 | Loss: 0.00081118
Iteration 15/25 | Loss: 0.00081118
Iteration 16/25 | Loss: 0.00081118
Iteration 17/25 | Loss: 0.00081118
Iteration 18/25 | Loss: 0.00081118
Iteration 19/25 | Loss: 0.00081118
Iteration 20/25 | Loss: 0.00081118
Iteration 21/25 | Loss: 0.00081118
Iteration 22/25 | Loss: 0.00081118
Iteration 23/25 | Loss: 0.00081118
Iteration 24/25 | Loss: 0.00081118
Iteration 25/25 | Loss: 0.00081118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081118
Iteration 2/1000 | Loss: 0.00004323
Iteration 3/1000 | Loss: 0.00003213
Iteration 4/1000 | Loss: 0.00002995
Iteration 5/1000 | Loss: 0.00002876
Iteration 6/1000 | Loss: 0.00002791
Iteration 7/1000 | Loss: 0.00002734
Iteration 8/1000 | Loss: 0.00002687
Iteration 9/1000 | Loss: 0.00002651
Iteration 10/1000 | Loss: 0.00002627
Iteration 11/1000 | Loss: 0.00002602
Iteration 12/1000 | Loss: 0.00002580
Iteration 13/1000 | Loss: 0.00002567
Iteration 14/1000 | Loss: 0.00002563
Iteration 15/1000 | Loss: 0.00002562
Iteration 16/1000 | Loss: 0.00002562
Iteration 17/1000 | Loss: 0.00002561
Iteration 18/1000 | Loss: 0.00002561
Iteration 19/1000 | Loss: 0.00002560
Iteration 20/1000 | Loss: 0.00002559
Iteration 21/1000 | Loss: 0.00002557
Iteration 22/1000 | Loss: 0.00002556
Iteration 23/1000 | Loss: 0.00002555
Iteration 24/1000 | Loss: 0.00002550
Iteration 25/1000 | Loss: 0.00002550
Iteration 26/1000 | Loss: 0.00002548
Iteration 27/1000 | Loss: 0.00002548
Iteration 28/1000 | Loss: 0.00002547
Iteration 29/1000 | Loss: 0.00002547
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002546
Iteration 32/1000 | Loss: 0.00002546
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002545
Iteration 35/1000 | Loss: 0.00002545
Iteration 36/1000 | Loss: 0.00002544
Iteration 37/1000 | Loss: 0.00002543
Iteration 38/1000 | Loss: 0.00002543
Iteration 39/1000 | Loss: 0.00002543
Iteration 40/1000 | Loss: 0.00002543
Iteration 41/1000 | Loss: 0.00002543
Iteration 42/1000 | Loss: 0.00002543
Iteration 43/1000 | Loss: 0.00002543
Iteration 44/1000 | Loss: 0.00002543
Iteration 45/1000 | Loss: 0.00002543
Iteration 46/1000 | Loss: 0.00002543
Iteration 47/1000 | Loss: 0.00002543
Iteration 48/1000 | Loss: 0.00002543
Iteration 49/1000 | Loss: 0.00002543
Iteration 50/1000 | Loss: 0.00002543
Iteration 51/1000 | Loss: 0.00002543
Iteration 52/1000 | Loss: 0.00002543
Iteration 53/1000 | Loss: 0.00002543
Iteration 54/1000 | Loss: 0.00002543
Iteration 55/1000 | Loss: 0.00002543
Iteration 56/1000 | Loss: 0.00002543
Iteration 57/1000 | Loss: 0.00002543
Iteration 58/1000 | Loss: 0.00002543
Iteration 59/1000 | Loss: 0.00002543
Iteration 60/1000 | Loss: 0.00002543
Iteration 61/1000 | Loss: 0.00002543
Iteration 62/1000 | Loss: 0.00002543
Iteration 63/1000 | Loss: 0.00002543
Iteration 64/1000 | Loss: 0.00002543
Iteration 65/1000 | Loss: 0.00002543
Iteration 66/1000 | Loss: 0.00002543
Iteration 67/1000 | Loss: 0.00002543
Iteration 68/1000 | Loss: 0.00002543
Iteration 69/1000 | Loss: 0.00002543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.5429026209167205e-05, 2.5429026209167205e-05, 2.5429026209167205e-05, 2.5429026209167205e-05, 2.5429026209167205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5429026209167205e-05

Optimization complete. Final v2v error: 4.233999729156494 mm

Highest mean error: 4.605103969573975 mm for frame 205

Lowest mean error: 3.8483400344848633 mm for frame 235

Saving results

Total time: 37.28241038322449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038537
Iteration 2/25 | Loss: 0.00236639
Iteration 3/25 | Loss: 0.00176443
Iteration 4/25 | Loss: 0.00161280
Iteration 5/25 | Loss: 0.00154308
Iteration 6/25 | Loss: 0.00145544
Iteration 7/25 | Loss: 0.00139484
Iteration 8/25 | Loss: 0.00132438
Iteration 9/25 | Loss: 0.00129712
Iteration 10/25 | Loss: 0.00128467
Iteration 11/25 | Loss: 0.00124088
Iteration 12/25 | Loss: 0.00122661
Iteration 13/25 | Loss: 0.00120623
Iteration 14/25 | Loss: 0.00119540
Iteration 15/25 | Loss: 0.00119325
Iteration 16/25 | Loss: 0.00118112
Iteration 17/25 | Loss: 0.00117916
Iteration 18/25 | Loss: 0.00117996
Iteration 19/25 | Loss: 0.00117964
Iteration 20/25 | Loss: 0.00117421
Iteration 21/25 | Loss: 0.00117240
Iteration 22/25 | Loss: 0.00117196
Iteration 23/25 | Loss: 0.00117170
Iteration 24/25 | Loss: 0.00117307
Iteration 25/25 | Loss: 0.00117025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50222814
Iteration 2/25 | Loss: 0.00283734
Iteration 3/25 | Loss: 0.00283734
Iteration 4/25 | Loss: 0.00283733
Iteration 5/25 | Loss: 0.00283733
Iteration 6/25 | Loss: 0.00283733
Iteration 7/25 | Loss: 0.00283733
Iteration 8/25 | Loss: 0.00283733
Iteration 9/25 | Loss: 0.00283733
Iteration 10/25 | Loss: 0.00283733
Iteration 11/25 | Loss: 0.00283733
Iteration 12/25 | Loss: 0.00283733
Iteration 13/25 | Loss: 0.00283733
Iteration 14/25 | Loss: 0.00283733
Iteration 15/25 | Loss: 0.00283733
Iteration 16/25 | Loss: 0.00283733
Iteration 17/25 | Loss: 0.00283733
Iteration 18/25 | Loss: 0.00283733
Iteration 19/25 | Loss: 0.00283733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0028373333625495434, 0.0028373333625495434, 0.0028373333625495434, 0.0028373333625495434, 0.0028373333625495434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028373333625495434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00283733
Iteration 2/1000 | Loss: 0.00063279
Iteration 3/1000 | Loss: 0.00088975
Iteration 4/1000 | Loss: 0.00060077
Iteration 5/1000 | Loss: 0.00059122
Iteration 6/1000 | Loss: 0.00053632
Iteration 7/1000 | Loss: 0.00025861
Iteration 8/1000 | Loss: 0.00023005
Iteration 9/1000 | Loss: 0.00018497
Iteration 10/1000 | Loss: 0.00015284
Iteration 11/1000 | Loss: 0.00013895
Iteration 12/1000 | Loss: 0.00012709
Iteration 13/1000 | Loss: 0.00047681
Iteration 14/1000 | Loss: 0.00033889
Iteration 15/1000 | Loss: 0.00013250
Iteration 16/1000 | Loss: 0.00025213
Iteration 17/1000 | Loss: 0.00022570
Iteration 18/1000 | Loss: 0.00011656
Iteration 19/1000 | Loss: 0.00026835
Iteration 20/1000 | Loss: 0.00014115
Iteration 21/1000 | Loss: 0.00011536
Iteration 22/1000 | Loss: 0.00010774
Iteration 23/1000 | Loss: 0.00010351
Iteration 24/1000 | Loss: 0.00010192
Iteration 25/1000 | Loss: 0.00015760
Iteration 26/1000 | Loss: 0.00010801
Iteration 27/1000 | Loss: 0.00022437
Iteration 28/1000 | Loss: 0.00043134
Iteration 29/1000 | Loss: 0.00049275
Iteration 30/1000 | Loss: 0.00144119
Iteration 31/1000 | Loss: 0.00127367
Iteration 32/1000 | Loss: 0.00226518
Iteration 33/1000 | Loss: 0.00178720
Iteration 34/1000 | Loss: 0.00050398
Iteration 35/1000 | Loss: 0.00023320
Iteration 36/1000 | Loss: 0.00030574
Iteration 37/1000 | Loss: 0.00011536
Iteration 38/1000 | Loss: 0.00009182
Iteration 39/1000 | Loss: 0.00005509
Iteration 40/1000 | Loss: 0.00010491
Iteration 41/1000 | Loss: 0.00004205
Iteration 42/1000 | Loss: 0.00003462
Iteration 43/1000 | Loss: 0.00003040
Iteration 44/1000 | Loss: 0.00002626
Iteration 45/1000 | Loss: 0.00002314
Iteration 46/1000 | Loss: 0.00002101
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001560
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00001546
Iteration 93/1000 | Loss: 0.00001546
Iteration 94/1000 | Loss: 0.00001546
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5430390703841113e-05, 1.5430390703841113e-05, 1.5430390703841113e-05, 1.5430390703841113e-05, 1.5430390703841113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5430390703841113e-05

Optimization complete. Final v2v error: 3.039917469024658 mm

Highest mean error: 11.15471076965332 mm for frame 69

Lowest mean error: 2.7961554527282715 mm for frame 178

Saving results

Total time: 143.6285798549652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050756
Iteration 2/25 | Loss: 0.00317845
Iteration 3/25 | Loss: 0.00233552
Iteration 4/25 | Loss: 0.00207073
Iteration 5/25 | Loss: 0.00183734
Iteration 6/25 | Loss: 0.00170602
Iteration 7/25 | Loss: 0.00161109
Iteration 8/25 | Loss: 0.00155264
Iteration 9/25 | Loss: 0.00137533
Iteration 10/25 | Loss: 0.00132300
Iteration 11/25 | Loss: 0.00129340
Iteration 12/25 | Loss: 0.00122468
Iteration 13/25 | Loss: 0.00123155
Iteration 14/25 | Loss: 0.00123952
Iteration 15/25 | Loss: 0.00119928
Iteration 16/25 | Loss: 0.00120511
Iteration 17/25 | Loss: 0.00119183
Iteration 18/25 | Loss: 0.00117452
Iteration 19/25 | Loss: 0.00117884
Iteration 20/25 | Loss: 0.00116447
Iteration 21/25 | Loss: 0.00116055
Iteration 22/25 | Loss: 0.00115895
Iteration 23/25 | Loss: 0.00115815
Iteration 24/25 | Loss: 0.00116378
Iteration 25/25 | Loss: 0.00116006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47859967
Iteration 2/25 | Loss: 0.00427281
Iteration 3/25 | Loss: 0.00348538
Iteration 4/25 | Loss: 0.00348538
Iteration 5/25 | Loss: 0.00348538
Iteration 6/25 | Loss: 0.00348538
Iteration 7/25 | Loss: 0.00348538
Iteration 8/25 | Loss: 0.00348538
Iteration 9/25 | Loss: 0.00348538
Iteration 10/25 | Loss: 0.00348538
Iteration 11/25 | Loss: 0.00348538
Iteration 12/25 | Loss: 0.00348538
Iteration 13/25 | Loss: 0.00348538
Iteration 14/25 | Loss: 0.00348538
Iteration 15/25 | Loss: 0.00348538
Iteration 16/25 | Loss: 0.00348538
Iteration 17/25 | Loss: 0.00348538
Iteration 18/25 | Loss: 0.00348538
Iteration 19/25 | Loss: 0.00348538
Iteration 20/25 | Loss: 0.00348538
Iteration 21/25 | Loss: 0.00348538
Iteration 22/25 | Loss: 0.00348538
Iteration 23/25 | Loss: 0.00348538
Iteration 24/25 | Loss: 0.00348538
Iteration 25/25 | Loss: 0.00348538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348538
Iteration 2/1000 | Loss: 0.00060541
Iteration 3/1000 | Loss: 0.00064887
Iteration 4/1000 | Loss: 0.00067078
Iteration 5/1000 | Loss: 0.00029196
Iteration 6/1000 | Loss: 0.00058026
Iteration 7/1000 | Loss: 0.00061966
Iteration 8/1000 | Loss: 0.00071351
Iteration 9/1000 | Loss: 0.00053659
Iteration 10/1000 | Loss: 0.00227242
Iteration 11/1000 | Loss: 0.00174187
Iteration 12/1000 | Loss: 0.00091190
Iteration 13/1000 | Loss: 0.00202064
Iteration 14/1000 | Loss: 0.00053990
Iteration 15/1000 | Loss: 0.00063693
Iteration 16/1000 | Loss: 0.00039007
Iteration 17/1000 | Loss: 0.00097263
Iteration 18/1000 | Loss: 0.00095016
Iteration 19/1000 | Loss: 0.00120244
Iteration 20/1000 | Loss: 0.00094586
Iteration 21/1000 | Loss: 0.00021169
Iteration 22/1000 | Loss: 0.00037392
Iteration 23/1000 | Loss: 0.00019944
Iteration 24/1000 | Loss: 0.00019025
Iteration 25/1000 | Loss: 0.00018453
Iteration 26/1000 | Loss: 0.00060600
Iteration 27/1000 | Loss: 0.00027866
Iteration 28/1000 | Loss: 0.00017810
Iteration 29/1000 | Loss: 0.00017670
Iteration 30/1000 | Loss: 0.00031848
Iteration 31/1000 | Loss: 0.00017403
Iteration 32/1000 | Loss: 0.00017235
Iteration 33/1000 | Loss: 0.00017037
Iteration 34/1000 | Loss: 0.00065829
Iteration 35/1000 | Loss: 0.00034771
Iteration 36/1000 | Loss: 0.00018092
Iteration 37/1000 | Loss: 0.00047965
Iteration 38/1000 | Loss: 0.00016287
Iteration 39/1000 | Loss: 0.00029913
Iteration 40/1000 | Loss: 0.00015621
Iteration 41/1000 | Loss: 0.00015220
Iteration 42/1000 | Loss: 0.00014833
Iteration 43/1000 | Loss: 0.00033759
Iteration 44/1000 | Loss: 0.00014326
Iteration 45/1000 | Loss: 0.00013701
Iteration 46/1000 | Loss: 0.00037694
Iteration 47/1000 | Loss: 0.00013680
Iteration 48/1000 | Loss: 0.00031082
Iteration 49/1000 | Loss: 0.00012377
Iteration 50/1000 | Loss: 0.00011959
Iteration 51/1000 | Loss: 0.00032095
Iteration 52/1000 | Loss: 0.00036990
Iteration 53/1000 | Loss: 0.00104671
Iteration 54/1000 | Loss: 0.00058401
Iteration 55/1000 | Loss: 0.00076756
Iteration 56/1000 | Loss: 0.00022718
Iteration 57/1000 | Loss: 0.00024098
Iteration 58/1000 | Loss: 0.00043919
Iteration 59/1000 | Loss: 0.00021897
Iteration 60/1000 | Loss: 0.00018229
Iteration 61/1000 | Loss: 0.00014497
Iteration 62/1000 | Loss: 0.00023069
Iteration 63/1000 | Loss: 0.00031092
Iteration 64/1000 | Loss: 0.00011589
Iteration 65/1000 | Loss: 0.00028385
Iteration 66/1000 | Loss: 0.00035644
Iteration 67/1000 | Loss: 0.00037116
Iteration 68/1000 | Loss: 0.00096871
Iteration 69/1000 | Loss: 0.00075309
Iteration 70/1000 | Loss: 0.00066595
Iteration 71/1000 | Loss: 0.00012782
Iteration 72/1000 | Loss: 0.00010796
Iteration 73/1000 | Loss: 0.00010231
Iteration 74/1000 | Loss: 0.00028347
Iteration 75/1000 | Loss: 0.00009841
Iteration 76/1000 | Loss: 0.00009671
Iteration 77/1000 | Loss: 0.00009493
Iteration 78/1000 | Loss: 0.00009410
Iteration 79/1000 | Loss: 0.00009351
Iteration 80/1000 | Loss: 0.00009314
Iteration 81/1000 | Loss: 0.00009276
Iteration 82/1000 | Loss: 0.00009254
Iteration 83/1000 | Loss: 0.00009238
Iteration 84/1000 | Loss: 0.00009230
Iteration 85/1000 | Loss: 0.00009228
Iteration 86/1000 | Loss: 0.00009226
Iteration 87/1000 | Loss: 0.00009226
Iteration 88/1000 | Loss: 0.00009218
Iteration 89/1000 | Loss: 0.00009218
Iteration 90/1000 | Loss: 0.00009217
Iteration 91/1000 | Loss: 0.00009215
Iteration 92/1000 | Loss: 0.00023355
Iteration 93/1000 | Loss: 0.00010076
Iteration 94/1000 | Loss: 0.00014966
Iteration 95/1000 | Loss: 0.00009252
Iteration 96/1000 | Loss: 0.00009217
Iteration 97/1000 | Loss: 0.00009210
Iteration 98/1000 | Loss: 0.00009209
Iteration 99/1000 | Loss: 0.00009208
Iteration 100/1000 | Loss: 0.00009206
Iteration 101/1000 | Loss: 0.00009205
Iteration 102/1000 | Loss: 0.00009205
Iteration 103/1000 | Loss: 0.00009204
Iteration 104/1000 | Loss: 0.00009204
Iteration 105/1000 | Loss: 0.00009203
Iteration 106/1000 | Loss: 0.00009203
Iteration 107/1000 | Loss: 0.00009202
Iteration 108/1000 | Loss: 0.00009201
Iteration 109/1000 | Loss: 0.00009199
Iteration 110/1000 | Loss: 0.00009198
Iteration 111/1000 | Loss: 0.00009198
Iteration 112/1000 | Loss: 0.00009198
Iteration 113/1000 | Loss: 0.00009198
Iteration 114/1000 | Loss: 0.00009198
Iteration 115/1000 | Loss: 0.00009198
Iteration 116/1000 | Loss: 0.00009198
Iteration 117/1000 | Loss: 0.00009198
Iteration 118/1000 | Loss: 0.00009197
Iteration 119/1000 | Loss: 0.00009197
Iteration 120/1000 | Loss: 0.00009197
Iteration 121/1000 | Loss: 0.00009197
Iteration 122/1000 | Loss: 0.00009197
Iteration 123/1000 | Loss: 0.00009197
Iteration 124/1000 | Loss: 0.00009197
Iteration 125/1000 | Loss: 0.00009197
Iteration 126/1000 | Loss: 0.00009197
Iteration 127/1000 | Loss: 0.00009197
Iteration 128/1000 | Loss: 0.00009197
Iteration 129/1000 | Loss: 0.00009197
Iteration 130/1000 | Loss: 0.00009197
Iteration 131/1000 | Loss: 0.00009197
Iteration 132/1000 | Loss: 0.00009196
Iteration 133/1000 | Loss: 0.00009196
Iteration 134/1000 | Loss: 0.00009196
Iteration 135/1000 | Loss: 0.00009196
Iteration 136/1000 | Loss: 0.00009196
Iteration 137/1000 | Loss: 0.00009195
Iteration 138/1000 | Loss: 0.00009195
Iteration 139/1000 | Loss: 0.00009195
Iteration 140/1000 | Loss: 0.00009195
Iteration 141/1000 | Loss: 0.00009195
Iteration 142/1000 | Loss: 0.00009195
Iteration 143/1000 | Loss: 0.00009194
Iteration 144/1000 | Loss: 0.00009194
Iteration 145/1000 | Loss: 0.00009194
Iteration 146/1000 | Loss: 0.00009194
Iteration 147/1000 | Loss: 0.00009194
Iteration 148/1000 | Loss: 0.00009194
Iteration 149/1000 | Loss: 0.00009193
Iteration 150/1000 | Loss: 0.00009193
Iteration 151/1000 | Loss: 0.00009193
Iteration 152/1000 | Loss: 0.00009193
Iteration 153/1000 | Loss: 0.00009193
Iteration 154/1000 | Loss: 0.00009192
Iteration 155/1000 | Loss: 0.00009192
Iteration 156/1000 | Loss: 0.00009192
Iteration 157/1000 | Loss: 0.00009192
Iteration 158/1000 | Loss: 0.00009192
Iteration 159/1000 | Loss: 0.00009191
Iteration 160/1000 | Loss: 0.00009191
Iteration 161/1000 | Loss: 0.00009191
Iteration 162/1000 | Loss: 0.00009191
Iteration 163/1000 | Loss: 0.00009191
Iteration 164/1000 | Loss: 0.00009191
Iteration 165/1000 | Loss: 0.00009191
Iteration 166/1000 | Loss: 0.00009191
Iteration 167/1000 | Loss: 0.00009191
Iteration 168/1000 | Loss: 0.00009190
Iteration 169/1000 | Loss: 0.00009190
Iteration 170/1000 | Loss: 0.00009190
Iteration 171/1000 | Loss: 0.00009190
Iteration 172/1000 | Loss: 0.00009190
Iteration 173/1000 | Loss: 0.00009190
Iteration 174/1000 | Loss: 0.00009190
Iteration 175/1000 | Loss: 0.00009190
Iteration 176/1000 | Loss: 0.00009190
Iteration 177/1000 | Loss: 0.00009190
Iteration 178/1000 | Loss: 0.00009190
Iteration 179/1000 | Loss: 0.00009190
Iteration 180/1000 | Loss: 0.00009189
Iteration 181/1000 | Loss: 0.00009189
Iteration 182/1000 | Loss: 0.00009189
Iteration 183/1000 | Loss: 0.00009189
Iteration 184/1000 | Loss: 0.00009189
Iteration 185/1000 | Loss: 0.00009188
Iteration 186/1000 | Loss: 0.00009188
Iteration 187/1000 | Loss: 0.00009188
Iteration 188/1000 | Loss: 0.00009188
Iteration 189/1000 | Loss: 0.00009188
Iteration 190/1000 | Loss: 0.00009187
Iteration 191/1000 | Loss: 0.00009187
Iteration 192/1000 | Loss: 0.00009187
Iteration 193/1000 | Loss: 0.00009186
Iteration 194/1000 | Loss: 0.00009186
Iteration 195/1000 | Loss: 0.00009185
Iteration 196/1000 | Loss: 0.00009185
Iteration 197/1000 | Loss: 0.00009184
Iteration 198/1000 | Loss: 0.00009184
Iteration 199/1000 | Loss: 0.00028239
Iteration 200/1000 | Loss: 0.00009215
Iteration 201/1000 | Loss: 0.00009180
Iteration 202/1000 | Loss: 0.00009180
Iteration 203/1000 | Loss: 0.00009179
Iteration 204/1000 | Loss: 0.00009179
Iteration 205/1000 | Loss: 0.00009179
Iteration 206/1000 | Loss: 0.00009179
Iteration 207/1000 | Loss: 0.00009179
Iteration 208/1000 | Loss: 0.00009179
Iteration 209/1000 | Loss: 0.00009179
Iteration 210/1000 | Loss: 0.00009179
Iteration 211/1000 | Loss: 0.00009179
Iteration 212/1000 | Loss: 0.00009179
Iteration 213/1000 | Loss: 0.00009179
Iteration 214/1000 | Loss: 0.00009178
Iteration 215/1000 | Loss: 0.00009178
Iteration 216/1000 | Loss: 0.00009178
Iteration 217/1000 | Loss: 0.00009178
Iteration 218/1000 | Loss: 0.00009178
Iteration 219/1000 | Loss: 0.00009177
Iteration 220/1000 | Loss: 0.00009177
Iteration 221/1000 | Loss: 0.00009177
Iteration 222/1000 | Loss: 0.00009177
Iteration 223/1000 | Loss: 0.00009177
Iteration 224/1000 | Loss: 0.00009177
Iteration 225/1000 | Loss: 0.00009177
Iteration 226/1000 | Loss: 0.00009177
Iteration 227/1000 | Loss: 0.00009177
Iteration 228/1000 | Loss: 0.00009177
Iteration 229/1000 | Loss: 0.00009177
Iteration 230/1000 | Loss: 0.00009177
Iteration 231/1000 | Loss: 0.00009177
Iteration 232/1000 | Loss: 0.00009177
Iteration 233/1000 | Loss: 0.00009177
Iteration 234/1000 | Loss: 0.00009177
Iteration 235/1000 | Loss: 0.00009177
Iteration 236/1000 | Loss: 0.00009177
Iteration 237/1000 | Loss: 0.00009177
Iteration 238/1000 | Loss: 0.00009177
Iteration 239/1000 | Loss: 0.00009177
Iteration 240/1000 | Loss: 0.00009177
Iteration 241/1000 | Loss: 0.00009177
Iteration 242/1000 | Loss: 0.00009177
Iteration 243/1000 | Loss: 0.00009177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [9.177191532216966e-05, 9.177191532216966e-05, 9.177191532216966e-05, 9.177191532216966e-05, 9.177191532216966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.177191532216966e-05

Optimization complete. Final v2v error: 5.153992652893066 mm

Highest mean error: 11.786653518676758 mm for frame 54

Lowest mean error: 3.482304334640503 mm for frame 149

Saving results

Total time: 179.9795365333557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389076
Iteration 2/25 | Loss: 0.00091538
Iteration 3/25 | Loss: 0.00083717
Iteration 4/25 | Loss: 0.00082147
Iteration 5/25 | Loss: 0.00081621
Iteration 6/25 | Loss: 0.00081582
Iteration 7/25 | Loss: 0.00081582
Iteration 8/25 | Loss: 0.00081582
Iteration 9/25 | Loss: 0.00081582
Iteration 10/25 | Loss: 0.00081582
Iteration 11/25 | Loss: 0.00081582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008158183190971613, 0.0008158183190971613, 0.0008158183190971613, 0.0008158183190971613, 0.0008158183190971613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008158183190971613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48699903
Iteration 2/25 | Loss: 0.00053231
Iteration 3/25 | Loss: 0.00053231
Iteration 4/25 | Loss: 0.00053231
Iteration 5/25 | Loss: 0.00053231
Iteration 6/25 | Loss: 0.00053230
Iteration 7/25 | Loss: 0.00053230
Iteration 8/25 | Loss: 0.00053230
Iteration 9/25 | Loss: 0.00053230
Iteration 10/25 | Loss: 0.00053230
Iteration 11/25 | Loss: 0.00053230
Iteration 12/25 | Loss: 0.00053230
Iteration 13/25 | Loss: 0.00053230
Iteration 14/25 | Loss: 0.00053230
Iteration 15/25 | Loss: 0.00053230
Iteration 16/25 | Loss: 0.00053230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005323035875335336, 0.0005323035875335336, 0.0005323035875335336, 0.0005323035875335336, 0.0005323035875335336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005323035875335336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053230
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001624
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001369
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001324
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001264
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001260
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001260
Iteration 80/1000 | Loss: 0.00001260
Iteration 81/1000 | Loss: 0.00001260
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.2600663467310369e-05, 1.2600663467310369e-05, 1.2600663467310369e-05, 1.2600663467310369e-05, 1.2600663467310369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2600663467310369e-05

Optimization complete. Final v2v error: 3.0382187366485596 mm

Highest mean error: 3.0786972045898438 mm for frame 262

Lowest mean error: 3.0005569458007812 mm for frame 33

Saving results

Total time: 34.828319787979126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034381
Iteration 2/25 | Loss: 0.00426276
Iteration 3/25 | Loss: 0.00259617
Iteration 4/25 | Loss: 0.00237088
Iteration 5/25 | Loss: 0.00216093
Iteration 6/25 | Loss: 0.00183601
Iteration 7/25 | Loss: 0.00170353
Iteration 8/25 | Loss: 0.00168963
Iteration 9/25 | Loss: 0.00162047
Iteration 10/25 | Loss: 0.00156416
Iteration 11/25 | Loss: 0.00146760
Iteration 12/25 | Loss: 0.00142069
Iteration 13/25 | Loss: 0.00141487
Iteration 14/25 | Loss: 0.00144449
Iteration 15/25 | Loss: 0.00129186
Iteration 16/25 | Loss: 0.00127186
Iteration 17/25 | Loss: 0.00125357
Iteration 18/25 | Loss: 0.00123388
Iteration 19/25 | Loss: 0.00122056
Iteration 20/25 | Loss: 0.00121430
Iteration 21/25 | Loss: 0.00121764
Iteration 22/25 | Loss: 0.00120484
Iteration 23/25 | Loss: 0.00120173
Iteration 24/25 | Loss: 0.00119836
Iteration 25/25 | Loss: 0.00119371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63831222
Iteration 2/25 | Loss: 0.00388611
Iteration 3/25 | Loss: 0.00322670
Iteration 4/25 | Loss: 0.00322670
Iteration 5/25 | Loss: 0.00322670
Iteration 6/25 | Loss: 0.00322669
Iteration 7/25 | Loss: 0.00322669
Iteration 8/25 | Loss: 0.00322669
Iteration 9/25 | Loss: 0.00322669
Iteration 10/25 | Loss: 0.00322669
Iteration 11/25 | Loss: 0.00322669
Iteration 12/25 | Loss: 0.00322669
Iteration 13/25 | Loss: 0.00322669
Iteration 14/25 | Loss: 0.00322669
Iteration 15/25 | Loss: 0.00322669
Iteration 16/25 | Loss: 0.00322669
Iteration 17/25 | Loss: 0.00322669
Iteration 18/25 | Loss: 0.00322669
Iteration 19/25 | Loss: 0.00322669
Iteration 20/25 | Loss: 0.00322669
Iteration 21/25 | Loss: 0.00322669
Iteration 22/25 | Loss: 0.00322669
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0032266932539641857, 0.0032266932539641857, 0.0032266932539641857, 0.0032266932539641857, 0.0032266932539641857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032266932539641857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322669
Iteration 2/1000 | Loss: 0.00351498
Iteration 3/1000 | Loss: 0.00090102
Iteration 4/1000 | Loss: 0.00230664
Iteration 5/1000 | Loss: 0.00096352
Iteration 6/1000 | Loss: 0.00119898
Iteration 7/1000 | Loss: 0.00098436
Iteration 8/1000 | Loss: 0.00067239
Iteration 9/1000 | Loss: 0.00056519
Iteration 10/1000 | Loss: 0.00081448
Iteration 11/1000 | Loss: 0.00040421
Iteration 12/1000 | Loss: 0.00035992
Iteration 13/1000 | Loss: 0.00015219
Iteration 14/1000 | Loss: 0.00011236
Iteration 15/1000 | Loss: 0.00010811
Iteration 16/1000 | Loss: 0.00005392
Iteration 17/1000 | Loss: 0.00027285
Iteration 18/1000 | Loss: 0.00035304
Iteration 19/1000 | Loss: 0.00029005
Iteration 20/1000 | Loss: 0.00005085
Iteration 21/1000 | Loss: 0.00029155
Iteration 22/1000 | Loss: 0.00005306
Iteration 23/1000 | Loss: 0.00032759
Iteration 24/1000 | Loss: 0.00005132
Iteration 25/1000 | Loss: 0.00007086
Iteration 26/1000 | Loss: 0.00022611
Iteration 27/1000 | Loss: 0.00041724
Iteration 28/1000 | Loss: 0.00031241
Iteration 29/1000 | Loss: 0.00034118
Iteration 30/1000 | Loss: 0.00014126
Iteration 31/1000 | Loss: 0.00023134
Iteration 32/1000 | Loss: 0.00005139
Iteration 33/1000 | Loss: 0.00004819
Iteration 34/1000 | Loss: 0.00004614
Iteration 35/1000 | Loss: 0.00007151
Iteration 36/1000 | Loss: 0.00004380
Iteration 37/1000 | Loss: 0.00004057
Iteration 38/1000 | Loss: 0.00003906
Iteration 39/1000 | Loss: 0.00003759
Iteration 40/1000 | Loss: 0.00040164
Iteration 41/1000 | Loss: 0.00016367
Iteration 42/1000 | Loss: 0.00009637
Iteration 43/1000 | Loss: 0.00052526
Iteration 44/1000 | Loss: 0.00070686
Iteration 45/1000 | Loss: 0.00058249
Iteration 46/1000 | Loss: 0.00009397
Iteration 47/1000 | Loss: 0.00010713
Iteration 48/1000 | Loss: 0.00004897
Iteration 49/1000 | Loss: 0.00017823
Iteration 50/1000 | Loss: 0.00004336
Iteration 51/1000 | Loss: 0.00004074
Iteration 52/1000 | Loss: 0.00003860
Iteration 53/1000 | Loss: 0.00003760
Iteration 54/1000 | Loss: 0.00003712
Iteration 55/1000 | Loss: 0.00021208
Iteration 56/1000 | Loss: 0.00034666
Iteration 57/1000 | Loss: 0.00004101
Iteration 58/1000 | Loss: 0.00003654
Iteration 59/1000 | Loss: 0.00003553
Iteration 60/1000 | Loss: 0.00003494
Iteration 61/1000 | Loss: 0.00003456
Iteration 62/1000 | Loss: 0.00003419
Iteration 63/1000 | Loss: 0.00003390
Iteration 64/1000 | Loss: 0.00024567
Iteration 65/1000 | Loss: 0.00006480
Iteration 66/1000 | Loss: 0.00004391
Iteration 67/1000 | Loss: 0.00005218
Iteration 68/1000 | Loss: 0.00003302
Iteration 69/1000 | Loss: 0.00005898
Iteration 70/1000 | Loss: 0.00003161
Iteration 71/1000 | Loss: 0.00003104
Iteration 72/1000 | Loss: 0.00003075
Iteration 73/1000 | Loss: 0.00003055
Iteration 74/1000 | Loss: 0.00003055
Iteration 75/1000 | Loss: 0.00003046
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003040
Iteration 78/1000 | Loss: 0.00003033
Iteration 79/1000 | Loss: 0.00003033
Iteration 80/1000 | Loss: 0.00003033
Iteration 81/1000 | Loss: 0.00003033
Iteration 82/1000 | Loss: 0.00003032
Iteration 83/1000 | Loss: 0.00003032
Iteration 84/1000 | Loss: 0.00003032
Iteration 85/1000 | Loss: 0.00003031
Iteration 86/1000 | Loss: 0.00003031
Iteration 87/1000 | Loss: 0.00003030
Iteration 88/1000 | Loss: 0.00003030
Iteration 89/1000 | Loss: 0.00003030
Iteration 90/1000 | Loss: 0.00003030
Iteration 91/1000 | Loss: 0.00003030
Iteration 92/1000 | Loss: 0.00003030
Iteration 93/1000 | Loss: 0.00003030
Iteration 94/1000 | Loss: 0.00003030
Iteration 95/1000 | Loss: 0.00003030
Iteration 96/1000 | Loss: 0.00003030
Iteration 97/1000 | Loss: 0.00003030
Iteration 98/1000 | Loss: 0.00003030
Iteration 99/1000 | Loss: 0.00003029
Iteration 100/1000 | Loss: 0.00003029
Iteration 101/1000 | Loss: 0.00003028
Iteration 102/1000 | Loss: 0.00003028
Iteration 103/1000 | Loss: 0.00003028
Iteration 104/1000 | Loss: 0.00003027
Iteration 105/1000 | Loss: 0.00003027
Iteration 106/1000 | Loss: 0.00003026
Iteration 107/1000 | Loss: 0.00003026
Iteration 108/1000 | Loss: 0.00003026
Iteration 109/1000 | Loss: 0.00003026
Iteration 110/1000 | Loss: 0.00003025
Iteration 111/1000 | Loss: 0.00003024
Iteration 112/1000 | Loss: 0.00003024
Iteration 113/1000 | Loss: 0.00003023
Iteration 114/1000 | Loss: 0.00003023
Iteration 115/1000 | Loss: 0.00003023
Iteration 116/1000 | Loss: 0.00003023
Iteration 117/1000 | Loss: 0.00003022
Iteration 118/1000 | Loss: 0.00003022
Iteration 119/1000 | Loss: 0.00003021
Iteration 120/1000 | Loss: 0.00003021
Iteration 121/1000 | Loss: 0.00003020
Iteration 122/1000 | Loss: 0.00003020
Iteration 123/1000 | Loss: 0.00003020
Iteration 124/1000 | Loss: 0.00003020
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003018
Iteration 131/1000 | Loss: 0.00003018
Iteration 132/1000 | Loss: 0.00003018
Iteration 133/1000 | Loss: 0.00003018
Iteration 134/1000 | Loss: 0.00003018
Iteration 135/1000 | Loss: 0.00003018
Iteration 136/1000 | Loss: 0.00003018
Iteration 137/1000 | Loss: 0.00003018
Iteration 138/1000 | Loss: 0.00003018
Iteration 139/1000 | Loss: 0.00003018
Iteration 140/1000 | Loss: 0.00003018
Iteration 141/1000 | Loss: 0.00003018
Iteration 142/1000 | Loss: 0.00003017
Iteration 143/1000 | Loss: 0.00003017
Iteration 144/1000 | Loss: 0.00003017
Iteration 145/1000 | Loss: 0.00003017
Iteration 146/1000 | Loss: 0.00003017
Iteration 147/1000 | Loss: 0.00003016
Iteration 148/1000 | Loss: 0.00003016
Iteration 149/1000 | Loss: 0.00003016
Iteration 150/1000 | Loss: 0.00003016
Iteration 151/1000 | Loss: 0.00003015
Iteration 152/1000 | Loss: 0.00003015
Iteration 153/1000 | Loss: 0.00003015
Iteration 154/1000 | Loss: 0.00003015
Iteration 155/1000 | Loss: 0.00003015
Iteration 156/1000 | Loss: 0.00003015
Iteration 157/1000 | Loss: 0.00003015
Iteration 158/1000 | Loss: 0.00003015
Iteration 159/1000 | Loss: 0.00003015
Iteration 160/1000 | Loss: 0.00003015
Iteration 161/1000 | Loss: 0.00003015
Iteration 162/1000 | Loss: 0.00003015
Iteration 163/1000 | Loss: 0.00003015
Iteration 164/1000 | Loss: 0.00003015
Iteration 165/1000 | Loss: 0.00003015
Iteration 166/1000 | Loss: 0.00003015
Iteration 167/1000 | Loss: 0.00003015
Iteration 168/1000 | Loss: 0.00003015
Iteration 169/1000 | Loss: 0.00003015
Iteration 170/1000 | Loss: 0.00003015
Iteration 171/1000 | Loss: 0.00003015
Iteration 172/1000 | Loss: 0.00003015
Iteration 173/1000 | Loss: 0.00003015
Iteration 174/1000 | Loss: 0.00003015
Iteration 175/1000 | Loss: 0.00003015
Iteration 176/1000 | Loss: 0.00003015
Iteration 177/1000 | Loss: 0.00003015
Iteration 178/1000 | Loss: 0.00003015
Iteration 179/1000 | Loss: 0.00003015
Iteration 180/1000 | Loss: 0.00003015
Iteration 181/1000 | Loss: 0.00003015
Iteration 182/1000 | Loss: 0.00003015
Iteration 183/1000 | Loss: 0.00003015
Iteration 184/1000 | Loss: 0.00003015
Iteration 185/1000 | Loss: 0.00003015
Iteration 186/1000 | Loss: 0.00003015
Iteration 187/1000 | Loss: 0.00003015
Iteration 188/1000 | Loss: 0.00003015
Iteration 189/1000 | Loss: 0.00003015
Iteration 190/1000 | Loss: 0.00003015
Iteration 191/1000 | Loss: 0.00003015
Iteration 192/1000 | Loss: 0.00003015
Iteration 193/1000 | Loss: 0.00003015
Iteration 194/1000 | Loss: 0.00003015
Iteration 195/1000 | Loss: 0.00003015
Iteration 196/1000 | Loss: 0.00003015
Iteration 197/1000 | Loss: 0.00003015
Iteration 198/1000 | Loss: 0.00003015
Iteration 199/1000 | Loss: 0.00003015
Iteration 200/1000 | Loss: 0.00003015
Iteration 201/1000 | Loss: 0.00003015
Iteration 202/1000 | Loss: 0.00003015
Iteration 203/1000 | Loss: 0.00003015
Iteration 204/1000 | Loss: 0.00003015
Iteration 205/1000 | Loss: 0.00003015
Iteration 206/1000 | Loss: 0.00003015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.014844514837023e-05, 3.014844514837023e-05, 3.014844514837023e-05, 3.014844514837023e-05, 3.014844514837023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.014844514837023e-05

Optimization complete. Final v2v error: 3.846306324005127 mm

Highest mean error: 11.1582670211792 mm for frame 116

Lowest mean error: 3.3250880241394043 mm for frame 32

Saving results

Total time: 153.8349232673645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064198
Iteration 2/25 | Loss: 0.00279772
Iteration 3/25 | Loss: 0.00189601
Iteration 4/25 | Loss: 0.00174868
Iteration 5/25 | Loss: 0.00170025
Iteration 6/25 | Loss: 0.00163154
Iteration 7/25 | Loss: 0.00156641
Iteration 8/25 | Loss: 0.00155417
Iteration 9/25 | Loss: 0.00150313
Iteration 10/25 | Loss: 0.00145857
Iteration 11/25 | Loss: 0.00143866
Iteration 12/25 | Loss: 0.00136346
Iteration 13/25 | Loss: 0.00131520
Iteration 14/25 | Loss: 0.00131040
Iteration 15/25 | Loss: 0.00125047
Iteration 16/25 | Loss: 0.00121920
Iteration 17/25 | Loss: 0.00121290
Iteration 18/25 | Loss: 0.00120876
Iteration 19/25 | Loss: 0.00122446
Iteration 20/25 | Loss: 0.00119739
Iteration 21/25 | Loss: 0.00118642
Iteration 22/25 | Loss: 0.00118934
Iteration 23/25 | Loss: 0.00118362
Iteration 24/25 | Loss: 0.00118096
Iteration 25/25 | Loss: 0.00118352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48745656
Iteration 2/25 | Loss: 0.00331575
Iteration 3/25 | Loss: 0.00238769
Iteration 4/25 | Loss: 0.00238769
Iteration 5/25 | Loss: 0.00238769
Iteration 6/25 | Loss: 0.00238768
Iteration 7/25 | Loss: 0.00238768
Iteration 8/25 | Loss: 0.00238768
Iteration 9/25 | Loss: 0.00238768
Iteration 10/25 | Loss: 0.00238768
Iteration 11/25 | Loss: 0.00238768
Iteration 12/25 | Loss: 0.00238768
Iteration 13/25 | Loss: 0.00238768
Iteration 14/25 | Loss: 0.00238768
Iteration 15/25 | Loss: 0.00238768
Iteration 16/25 | Loss: 0.00238768
Iteration 17/25 | Loss: 0.00238768
Iteration 18/25 | Loss: 0.00238768
Iteration 19/25 | Loss: 0.00238768
Iteration 20/25 | Loss: 0.00238768
Iteration 21/25 | Loss: 0.00238768
Iteration 22/25 | Loss: 0.00238768
Iteration 23/25 | Loss: 0.00238768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002387681044638157, 0.002387681044638157, 0.002387681044638157, 0.002387681044638157, 0.002387681044638157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002387681044638157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238768
Iteration 2/1000 | Loss: 0.00094702
Iteration 3/1000 | Loss: 0.00037995
Iteration 4/1000 | Loss: 0.00106496
Iteration 5/1000 | Loss: 0.00041291
Iteration 6/1000 | Loss: 0.00023801
Iteration 7/1000 | Loss: 0.00040595
Iteration 8/1000 | Loss: 0.00020200
Iteration 9/1000 | Loss: 0.00041198
Iteration 10/1000 | Loss: 0.00153247
Iteration 11/1000 | Loss: 0.01293515
Iteration 12/1000 | Loss: 0.01314038
Iteration 13/1000 | Loss: 0.00160365
Iteration 14/1000 | Loss: 0.00045090
Iteration 15/1000 | Loss: 0.00144989
Iteration 16/1000 | Loss: 0.00068818
Iteration 17/1000 | Loss: 0.00044263
Iteration 18/1000 | Loss: 0.00012460
Iteration 19/1000 | Loss: 0.00021666
Iteration 20/1000 | Loss: 0.00075865
Iteration 21/1000 | Loss: 0.00017880
Iteration 22/1000 | Loss: 0.00015745
Iteration 23/1000 | Loss: 0.00025273
Iteration 24/1000 | Loss: 0.00004956
Iteration 25/1000 | Loss: 0.00012411
Iteration 26/1000 | Loss: 0.00037633
Iteration 27/1000 | Loss: 0.00002815
Iteration 28/1000 | Loss: 0.00022985
Iteration 29/1000 | Loss: 0.00189489
Iteration 30/1000 | Loss: 0.00007282
Iteration 31/1000 | Loss: 0.00014396
Iteration 32/1000 | Loss: 0.00005895
Iteration 33/1000 | Loss: 0.00012623
Iteration 34/1000 | Loss: 0.00005107
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00014067
Iteration 37/1000 | Loss: 0.00010619
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00003532
Iteration 40/1000 | Loss: 0.00003073
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001141
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00004158
Iteration 48/1000 | Loss: 0.00016132
Iteration 49/1000 | Loss: 0.00001840
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001079
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001079
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00003789
Iteration 93/1000 | Loss: 0.00020891
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00010165
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00002804
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.0752497473731637e-05, 1.0752497473731637e-05, 1.0752497473731637e-05, 1.0752497473731637e-05, 1.0752497473731637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0752497473731637e-05

Optimization complete. Final v2v error: 2.823746919631958 mm

Highest mean error: 3.3741300106048584 mm for frame 126

Lowest mean error: 2.70892596244812 mm for frame 30

Saving results

Total time: 118.22140574455261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485719
Iteration 2/25 | Loss: 0.00096784
Iteration 3/25 | Loss: 0.00084521
Iteration 4/25 | Loss: 0.00083486
Iteration 5/25 | Loss: 0.00083108
Iteration 6/25 | Loss: 0.00083076
Iteration 7/25 | Loss: 0.00083076
Iteration 8/25 | Loss: 0.00083076
Iteration 9/25 | Loss: 0.00083076
Iteration 10/25 | Loss: 0.00083076
Iteration 11/25 | Loss: 0.00083076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008307630196213722, 0.0008307630196213722, 0.0008307630196213722, 0.0008307630196213722, 0.0008307630196213722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008307630196213722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.86672878
Iteration 2/25 | Loss: 0.00051594
Iteration 3/25 | Loss: 0.00051593
Iteration 4/25 | Loss: 0.00051593
Iteration 5/25 | Loss: 0.00051593
Iteration 6/25 | Loss: 0.00051593
Iteration 7/25 | Loss: 0.00051593
Iteration 8/25 | Loss: 0.00051593
Iteration 9/25 | Loss: 0.00051593
Iteration 10/25 | Loss: 0.00051593
Iteration 11/25 | Loss: 0.00051593
Iteration 12/25 | Loss: 0.00051593
Iteration 13/25 | Loss: 0.00051592
Iteration 14/25 | Loss: 0.00051592
Iteration 15/25 | Loss: 0.00051592
Iteration 16/25 | Loss: 0.00051592
Iteration 17/25 | Loss: 0.00051592
Iteration 18/25 | Loss: 0.00051592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005159249994903803, 0.0005159249994903803, 0.0005159249994903803, 0.0005159249994903803, 0.0005159249994903803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005159249994903803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051592
Iteration 2/1000 | Loss: 0.00002271
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001419
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001373
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001360
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001352
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001345
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001333
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.3272458090796135e-05, 1.3272458090796135e-05, 1.3272458090796135e-05, 1.3272458090796135e-05, 1.3272458090796135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3272458090796135e-05

Optimization complete. Final v2v error: 3.1014130115509033 mm

Highest mean error: 3.515925645828247 mm for frame 191

Lowest mean error: 2.87933087348938 mm for frame 215

Saving results

Total time: 35.74783229827881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858499
Iteration 2/25 | Loss: 0.00157626
Iteration 3/25 | Loss: 0.00103312
Iteration 4/25 | Loss: 0.00097102
Iteration 5/25 | Loss: 0.00094994
Iteration 6/25 | Loss: 0.00094983
Iteration 7/25 | Loss: 0.00094690
Iteration 8/25 | Loss: 0.00093378
Iteration 9/25 | Loss: 0.00092092
Iteration 10/25 | Loss: 0.00092116
Iteration 11/25 | Loss: 0.00091959
Iteration 12/25 | Loss: 0.00092719
Iteration 13/25 | Loss: 0.00091667
Iteration 14/25 | Loss: 0.00091663
Iteration 15/25 | Loss: 0.00091663
Iteration 16/25 | Loss: 0.00091663
Iteration 17/25 | Loss: 0.00091663
Iteration 18/25 | Loss: 0.00091662
Iteration 19/25 | Loss: 0.00091662
Iteration 20/25 | Loss: 0.00091662
Iteration 21/25 | Loss: 0.00091662
Iteration 22/25 | Loss: 0.00091662
Iteration 23/25 | Loss: 0.00091661
Iteration 24/25 | Loss: 0.00091661
Iteration 25/25 | Loss: 0.00091661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62516856
Iteration 2/25 | Loss: 0.00092804
Iteration 3/25 | Loss: 0.00083791
Iteration 4/25 | Loss: 0.00083791
Iteration 5/25 | Loss: 0.00083791
Iteration 6/25 | Loss: 0.00083791
Iteration 7/25 | Loss: 0.00083790
Iteration 8/25 | Loss: 0.00083790
Iteration 9/25 | Loss: 0.00083790
Iteration 10/25 | Loss: 0.00083790
Iteration 11/25 | Loss: 0.00083790
Iteration 12/25 | Loss: 0.00083790
Iteration 13/25 | Loss: 0.00083790
Iteration 14/25 | Loss: 0.00083790
Iteration 15/25 | Loss: 0.00083790
Iteration 16/25 | Loss: 0.00083790
Iteration 17/25 | Loss: 0.00083790
Iteration 18/25 | Loss: 0.00083790
Iteration 19/25 | Loss: 0.00083790
Iteration 20/25 | Loss: 0.00083790
Iteration 21/25 | Loss: 0.00083790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008379041682928801, 0.0008379041682928801, 0.0008379041682928801, 0.0008379041682928801, 0.0008379041682928801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008379041682928801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083790
Iteration 2/1000 | Loss: 0.00027817
Iteration 3/1000 | Loss: 0.00004106
Iteration 4/1000 | Loss: 0.00002775
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002259
Iteration 7/1000 | Loss: 0.00002131
Iteration 8/1000 | Loss: 0.00002050
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001907
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001882
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001866
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001829
Iteration 49/1000 | Loss: 0.00001826
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001818
Iteration 53/1000 | Loss: 0.00022452
Iteration 54/1000 | Loss: 0.00012632
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001817
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001815
Iteration 61/1000 | Loss: 0.00001815
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00022753
Iteration 71/1000 | Loss: 0.00014018
Iteration 72/1000 | Loss: 0.00016411
Iteration 73/1000 | Loss: 0.00003483
Iteration 74/1000 | Loss: 0.00015448
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001832
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001811
Iteration 79/1000 | Loss: 0.00024716
Iteration 80/1000 | Loss: 0.00010108
Iteration 81/1000 | Loss: 0.00023279
Iteration 82/1000 | Loss: 0.00019451
Iteration 83/1000 | Loss: 0.00027112
Iteration 84/1000 | Loss: 0.00017685
Iteration 85/1000 | Loss: 0.00003271
Iteration 86/1000 | Loss: 0.00001942
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001700
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6999807485262863e-05, 1.6999807485262863e-05, 1.6999807485262863e-05, 1.6999807485262863e-05, 1.6999807485262863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6999807485262863e-05

Optimization complete. Final v2v error: 3.50101637840271 mm

Highest mean error: 4.908659934997559 mm for frame 185

Lowest mean error: 3.0333409309387207 mm for frame 203

Saving results

Total time: 97.96461653709412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959553
Iteration 2/25 | Loss: 0.00308615
Iteration 3/25 | Loss: 0.00178179
Iteration 4/25 | Loss: 0.00140384
Iteration 5/25 | Loss: 0.00129902
Iteration 6/25 | Loss: 0.00134564
Iteration 7/25 | Loss: 0.00135356
Iteration 8/25 | Loss: 0.00130091
Iteration 9/25 | Loss: 0.00125714
Iteration 10/25 | Loss: 0.00123552
Iteration 11/25 | Loss: 0.00122355
Iteration 12/25 | Loss: 0.00120436
Iteration 13/25 | Loss: 0.00119309
Iteration 14/25 | Loss: 0.00120827
Iteration 15/25 | Loss: 0.00121323
Iteration 16/25 | Loss: 0.00119882
Iteration 17/25 | Loss: 0.00119093
Iteration 18/25 | Loss: 0.00117554
Iteration 19/25 | Loss: 0.00117020
Iteration 20/25 | Loss: 0.00117314
Iteration 21/25 | Loss: 0.00117695
Iteration 22/25 | Loss: 0.00116452
Iteration 23/25 | Loss: 0.00117059
Iteration 24/25 | Loss: 0.00116569
Iteration 25/25 | Loss: 0.00116520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86381841
Iteration 2/25 | Loss: 0.00355542
Iteration 3/25 | Loss: 0.00355542
Iteration 4/25 | Loss: 0.00355541
Iteration 5/25 | Loss: 0.00355541
Iteration 6/25 | Loss: 0.00355541
Iteration 7/25 | Loss: 0.00355541
Iteration 8/25 | Loss: 0.00355541
Iteration 9/25 | Loss: 0.00355541
Iteration 10/25 | Loss: 0.00355541
Iteration 11/25 | Loss: 0.00355541
Iteration 12/25 | Loss: 0.00355541
Iteration 13/25 | Loss: 0.00355541
Iteration 14/25 | Loss: 0.00355541
Iteration 15/25 | Loss: 0.00355541
Iteration 16/25 | Loss: 0.00355541
Iteration 17/25 | Loss: 0.00355541
Iteration 18/25 | Loss: 0.00355541
Iteration 19/25 | Loss: 0.00355541
Iteration 20/25 | Loss: 0.00355541
Iteration 21/25 | Loss: 0.00355541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0035554051864892244, 0.0035554051864892244, 0.0035554051864892244, 0.0035554051864892244, 0.0035554051864892244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035554051864892244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00355541
Iteration 2/1000 | Loss: 0.00096518
Iteration 3/1000 | Loss: 0.00090261
Iteration 4/1000 | Loss: 0.00095045
Iteration 5/1000 | Loss: 0.00099369
Iteration 6/1000 | Loss: 0.00109296
Iteration 7/1000 | Loss: 0.00371231
Iteration 8/1000 | Loss: 0.00052951
Iteration 9/1000 | Loss: 0.00459287
Iteration 10/1000 | Loss: 0.00145961
Iteration 11/1000 | Loss: 0.00088275
Iteration 12/1000 | Loss: 0.00081650
Iteration 13/1000 | Loss: 0.00167895
Iteration 14/1000 | Loss: 0.00132039
Iteration 15/1000 | Loss: 0.00189969
Iteration 16/1000 | Loss: 0.00300858
Iteration 17/1000 | Loss: 0.00102403
Iteration 18/1000 | Loss: 0.00083435
Iteration 19/1000 | Loss: 0.00044501
Iteration 20/1000 | Loss: 0.00106029
Iteration 21/1000 | Loss: 0.00080065
Iteration 22/1000 | Loss: 0.00056181
Iteration 23/1000 | Loss: 0.00050908
Iteration 24/1000 | Loss: 0.00045538
Iteration 25/1000 | Loss: 0.00153871
Iteration 26/1000 | Loss: 0.00065494
Iteration 27/1000 | Loss: 0.00094480
Iteration 28/1000 | Loss: 0.00079086
Iteration 29/1000 | Loss: 0.00082263
Iteration 30/1000 | Loss: 0.00137956
Iteration 31/1000 | Loss: 0.00032586
Iteration 32/1000 | Loss: 0.00056690
Iteration 33/1000 | Loss: 0.00046271
Iteration 34/1000 | Loss: 0.00047761
Iteration 35/1000 | Loss: 0.00061679
Iteration 36/1000 | Loss: 0.00048960
Iteration 37/1000 | Loss: 0.00064837
Iteration 38/1000 | Loss: 0.00124101
Iteration 39/1000 | Loss: 0.00100521
Iteration 40/1000 | Loss: 0.00108036
Iteration 41/1000 | Loss: 0.00084774
Iteration 42/1000 | Loss: 0.00063544
Iteration 43/1000 | Loss: 0.00061837
Iteration 44/1000 | Loss: 0.00047396
Iteration 45/1000 | Loss: 0.00059254
Iteration 46/1000 | Loss: 0.00035911
Iteration 47/1000 | Loss: 0.00114657
Iteration 48/1000 | Loss: 0.00172689
Iteration 49/1000 | Loss: 0.00065462
Iteration 50/1000 | Loss: 0.00115252
Iteration 51/1000 | Loss: 0.00063632
Iteration 52/1000 | Loss: 0.00204923
Iteration 53/1000 | Loss: 0.00084149
Iteration 54/1000 | Loss: 0.00044109
Iteration 55/1000 | Loss: 0.00029927
Iteration 56/1000 | Loss: 0.00035311
Iteration 57/1000 | Loss: 0.00011678
Iteration 58/1000 | Loss: 0.00021009
Iteration 59/1000 | Loss: 0.00024634
Iteration 60/1000 | Loss: 0.00020340
Iteration 61/1000 | Loss: 0.00041314
Iteration 62/1000 | Loss: 0.00044300
Iteration 63/1000 | Loss: 0.00080205
Iteration 64/1000 | Loss: 0.00086087
Iteration 65/1000 | Loss: 0.00079599
Iteration 66/1000 | Loss: 0.00041438
Iteration 67/1000 | Loss: 0.00099506
Iteration 68/1000 | Loss: 0.00037672
Iteration 69/1000 | Loss: 0.00013311
Iteration 70/1000 | Loss: 0.00009923
Iteration 71/1000 | Loss: 0.00089911
Iteration 72/1000 | Loss: 0.00033795
Iteration 73/1000 | Loss: 0.00074651
Iteration 74/1000 | Loss: 0.00075678
Iteration 75/1000 | Loss: 0.00017913
Iteration 76/1000 | Loss: 0.00012363
Iteration 77/1000 | Loss: 0.00017866
Iteration 78/1000 | Loss: 0.00026722
Iteration 79/1000 | Loss: 0.00076627
Iteration 80/1000 | Loss: 0.00027126
Iteration 81/1000 | Loss: 0.00050809
Iteration 82/1000 | Loss: 0.00082505
Iteration 83/1000 | Loss: 0.00049523
Iteration 84/1000 | Loss: 0.00009205
Iteration 85/1000 | Loss: 0.00049730
Iteration 86/1000 | Loss: 0.00085207
Iteration 87/1000 | Loss: 0.00102103
Iteration 88/1000 | Loss: 0.00068718
Iteration 89/1000 | Loss: 0.00060985
Iteration 90/1000 | Loss: 0.00078942
Iteration 91/1000 | Loss: 0.00026359
Iteration 92/1000 | Loss: 0.00014234
Iteration 93/1000 | Loss: 0.00066015
Iteration 94/1000 | Loss: 0.00099091
Iteration 95/1000 | Loss: 0.00054675
Iteration 96/1000 | Loss: 0.00027924
Iteration 97/1000 | Loss: 0.00036242
Iteration 98/1000 | Loss: 0.00017517
Iteration 99/1000 | Loss: 0.00034513
Iteration 100/1000 | Loss: 0.00011749
Iteration 101/1000 | Loss: 0.00010273
Iteration 102/1000 | Loss: 0.00008652
Iteration 103/1000 | Loss: 0.00048021
Iteration 104/1000 | Loss: 0.00045136
Iteration 105/1000 | Loss: 0.00036823
Iteration 106/1000 | Loss: 0.00021248
Iteration 107/1000 | Loss: 0.00007908
Iteration 108/1000 | Loss: 0.00031750
Iteration 109/1000 | Loss: 0.00011101
Iteration 110/1000 | Loss: 0.00009177
Iteration 111/1000 | Loss: 0.00006676
Iteration 112/1000 | Loss: 0.00006515
Iteration 113/1000 | Loss: 0.00006569
Iteration 114/1000 | Loss: 0.00027277
Iteration 115/1000 | Loss: 0.00023726
Iteration 116/1000 | Loss: 0.00021667
Iteration 117/1000 | Loss: 0.00014667
Iteration 118/1000 | Loss: 0.00005915
Iteration 119/1000 | Loss: 0.00033969
Iteration 120/1000 | Loss: 0.00029390
Iteration 121/1000 | Loss: 0.00035156
Iteration 122/1000 | Loss: 0.00028299
Iteration 123/1000 | Loss: 0.00046746
Iteration 124/1000 | Loss: 0.00027501
Iteration 125/1000 | Loss: 0.00031350
Iteration 126/1000 | Loss: 0.00007637
Iteration 127/1000 | Loss: 0.00006899
Iteration 128/1000 | Loss: 0.00005477
Iteration 129/1000 | Loss: 0.00005270
Iteration 130/1000 | Loss: 0.00030098
Iteration 131/1000 | Loss: 0.00020229
Iteration 132/1000 | Loss: 0.00020887
Iteration 133/1000 | Loss: 0.00017864
Iteration 134/1000 | Loss: 0.00018233
Iteration 135/1000 | Loss: 0.00006503
Iteration 136/1000 | Loss: 0.00007154
Iteration 137/1000 | Loss: 0.00005882
Iteration 138/1000 | Loss: 0.00035832
Iteration 139/1000 | Loss: 0.00031791
Iteration 140/1000 | Loss: 0.00032708
Iteration 141/1000 | Loss: 0.00027421
Iteration 142/1000 | Loss: 0.00024303
Iteration 143/1000 | Loss: 0.00021925
Iteration 144/1000 | Loss: 0.00023278
Iteration 145/1000 | Loss: 0.00006228
Iteration 146/1000 | Loss: 0.00004881
Iteration 147/1000 | Loss: 0.00005182
Iteration 148/1000 | Loss: 0.00008256
Iteration 149/1000 | Loss: 0.00007247
Iteration 150/1000 | Loss: 0.00006841
Iteration 151/1000 | Loss: 0.00006576
Iteration 152/1000 | Loss: 0.00045136
Iteration 153/1000 | Loss: 0.00007425
Iteration 154/1000 | Loss: 0.00041577
Iteration 155/1000 | Loss: 0.00032747
Iteration 156/1000 | Loss: 0.00054679
Iteration 157/1000 | Loss: 0.00007067
Iteration 158/1000 | Loss: 0.00005515
Iteration 159/1000 | Loss: 0.00005172
Iteration 160/1000 | Loss: 0.00080518
Iteration 161/1000 | Loss: 0.00042346
Iteration 162/1000 | Loss: 0.00019071
Iteration 163/1000 | Loss: 0.00016133
Iteration 164/1000 | Loss: 0.00006832
Iteration 165/1000 | Loss: 0.00014075
Iteration 166/1000 | Loss: 0.00015344
Iteration 167/1000 | Loss: 0.00020247
Iteration 168/1000 | Loss: 0.00007260
Iteration 169/1000 | Loss: 0.00005722
Iteration 170/1000 | Loss: 0.00005225
Iteration 171/1000 | Loss: 0.00004983
Iteration 172/1000 | Loss: 0.00004758
Iteration 173/1000 | Loss: 0.00029090
Iteration 174/1000 | Loss: 0.00012777
Iteration 175/1000 | Loss: 0.00018251
Iteration 176/1000 | Loss: 0.00012565
Iteration 177/1000 | Loss: 0.00074992
Iteration 178/1000 | Loss: 0.00122607
Iteration 179/1000 | Loss: 0.00061558
Iteration 180/1000 | Loss: 0.00005443
Iteration 181/1000 | Loss: 0.00007014
Iteration 182/1000 | Loss: 0.00004442
Iteration 183/1000 | Loss: 0.00007311
Iteration 184/1000 | Loss: 0.00006478
Iteration 185/1000 | Loss: 0.00007937
Iteration 186/1000 | Loss: 0.00006245
Iteration 187/1000 | Loss: 0.00008414
Iteration 188/1000 | Loss: 0.00004809
Iteration 189/1000 | Loss: 0.00004352
Iteration 190/1000 | Loss: 0.00004191
Iteration 191/1000 | Loss: 0.00004127
Iteration 192/1000 | Loss: 0.00004061
Iteration 193/1000 | Loss: 0.00004026
Iteration 194/1000 | Loss: 0.00028264
Iteration 195/1000 | Loss: 0.00015960
Iteration 196/1000 | Loss: 0.00004107
Iteration 197/1000 | Loss: 0.00027920
Iteration 198/1000 | Loss: 0.00016116
Iteration 199/1000 | Loss: 0.00004031
Iteration 200/1000 | Loss: 0.00028037
Iteration 201/1000 | Loss: 0.00013561
Iteration 202/1000 | Loss: 0.00004028
Iteration 203/1000 | Loss: 0.00027699
Iteration 204/1000 | Loss: 0.00073968
Iteration 205/1000 | Loss: 0.00006984
Iteration 206/1000 | Loss: 0.00024494
Iteration 207/1000 | Loss: 0.00005293
Iteration 208/1000 | Loss: 0.00004587
Iteration 209/1000 | Loss: 0.00004264
Iteration 210/1000 | Loss: 0.00003977
Iteration 211/1000 | Loss: 0.00003884
Iteration 212/1000 | Loss: 0.00003794
Iteration 213/1000 | Loss: 0.00003747
Iteration 214/1000 | Loss: 0.00003704
Iteration 215/1000 | Loss: 0.00003667
Iteration 216/1000 | Loss: 0.00080051
Iteration 217/1000 | Loss: 0.00003944
Iteration 218/1000 | Loss: 0.00003715
Iteration 219/1000 | Loss: 0.00003587
Iteration 220/1000 | Loss: 0.00003450
Iteration 221/1000 | Loss: 0.00003380
Iteration 222/1000 | Loss: 0.00003336
Iteration 223/1000 | Loss: 0.00003310
Iteration 224/1000 | Loss: 0.00003284
Iteration 225/1000 | Loss: 0.00003283
Iteration 226/1000 | Loss: 0.00003260
Iteration 227/1000 | Loss: 0.00003237
Iteration 228/1000 | Loss: 0.00079139
Iteration 229/1000 | Loss: 0.00004061
Iteration 230/1000 | Loss: 0.00003416
Iteration 231/1000 | Loss: 0.00003245
Iteration 232/1000 | Loss: 0.00003093
Iteration 233/1000 | Loss: 0.00003000
Iteration 234/1000 | Loss: 0.00002965
Iteration 235/1000 | Loss: 0.00002940
Iteration 236/1000 | Loss: 0.00002936
Iteration 237/1000 | Loss: 0.00002931
Iteration 238/1000 | Loss: 0.00002926
Iteration 239/1000 | Loss: 0.00002919
Iteration 240/1000 | Loss: 0.00002918
Iteration 241/1000 | Loss: 0.00002918
Iteration 242/1000 | Loss: 0.00002918
Iteration 243/1000 | Loss: 0.00002918
Iteration 244/1000 | Loss: 0.00002918
Iteration 245/1000 | Loss: 0.00002918
Iteration 246/1000 | Loss: 0.00002917
Iteration 247/1000 | Loss: 0.00002917
Iteration 248/1000 | Loss: 0.00002917
Iteration 249/1000 | Loss: 0.00002916
Iteration 250/1000 | Loss: 0.00002916
Iteration 251/1000 | Loss: 0.00002916
Iteration 252/1000 | Loss: 0.00002915
Iteration 253/1000 | Loss: 0.00002914
Iteration 254/1000 | Loss: 0.00002913
Iteration 255/1000 | Loss: 0.00002913
Iteration 256/1000 | Loss: 0.00002912
Iteration 257/1000 | Loss: 0.00002912
Iteration 258/1000 | Loss: 0.00002912
Iteration 259/1000 | Loss: 0.00002910
Iteration 260/1000 | Loss: 0.00002910
Iteration 261/1000 | Loss: 0.00002909
Iteration 262/1000 | Loss: 0.00002908
Iteration 263/1000 | Loss: 0.00002908
Iteration 264/1000 | Loss: 0.00002908
Iteration 265/1000 | Loss: 0.00002908
Iteration 266/1000 | Loss: 0.00002908
Iteration 267/1000 | Loss: 0.00002908
Iteration 268/1000 | Loss: 0.00002907
Iteration 269/1000 | Loss: 0.00002907
Iteration 270/1000 | Loss: 0.00002906
Iteration 271/1000 | Loss: 0.00002906
Iteration 272/1000 | Loss: 0.00002906
Iteration 273/1000 | Loss: 0.00002905
Iteration 274/1000 | Loss: 0.00002905
Iteration 275/1000 | Loss: 0.00002905
Iteration 276/1000 | Loss: 0.00002905
Iteration 277/1000 | Loss: 0.00002904
Iteration 278/1000 | Loss: 0.00002904
Iteration 279/1000 | Loss: 0.00002903
Iteration 280/1000 | Loss: 0.00002903
Iteration 281/1000 | Loss: 0.00002903
Iteration 282/1000 | Loss: 0.00002902
Iteration 283/1000 | Loss: 0.00002902
Iteration 284/1000 | Loss: 0.00002901
Iteration 285/1000 | Loss: 0.00002901
Iteration 286/1000 | Loss: 0.00002901
Iteration 287/1000 | Loss: 0.00002901
Iteration 288/1000 | Loss: 0.00002901
Iteration 289/1000 | Loss: 0.00002901
Iteration 290/1000 | Loss: 0.00002901
Iteration 291/1000 | Loss: 0.00002901
Iteration 292/1000 | Loss: 0.00002900
Iteration 293/1000 | Loss: 0.00002900
Iteration 294/1000 | Loss: 0.00002900
Iteration 295/1000 | Loss: 0.00002899
Iteration 296/1000 | Loss: 0.00002899
Iteration 297/1000 | Loss: 0.00002899
Iteration 298/1000 | Loss: 0.00002899
Iteration 299/1000 | Loss: 0.00002898
Iteration 300/1000 | Loss: 0.00002898
Iteration 301/1000 | Loss: 0.00002898
Iteration 302/1000 | Loss: 0.00002898
Iteration 303/1000 | Loss: 0.00002898
Iteration 304/1000 | Loss: 0.00002898
Iteration 305/1000 | Loss: 0.00002898
Iteration 306/1000 | Loss: 0.00002898
Iteration 307/1000 | Loss: 0.00002898
Iteration 308/1000 | Loss: 0.00002898
Iteration 309/1000 | Loss: 0.00002898
Iteration 310/1000 | Loss: 0.00002898
Iteration 311/1000 | Loss: 0.00002898
Iteration 312/1000 | Loss: 0.00002897
Iteration 313/1000 | Loss: 0.00002897
Iteration 314/1000 | Loss: 0.00002897
Iteration 315/1000 | Loss: 0.00002897
Iteration 316/1000 | Loss: 0.00002897
Iteration 317/1000 | Loss: 0.00002897
Iteration 318/1000 | Loss: 0.00002897
Iteration 319/1000 | Loss: 0.00002897
Iteration 320/1000 | Loss: 0.00002897
Iteration 321/1000 | Loss: 0.00002897
Iteration 322/1000 | Loss: 0.00002897
Iteration 323/1000 | Loss: 0.00002896
Iteration 324/1000 | Loss: 0.00002896
Iteration 325/1000 | Loss: 0.00002896
Iteration 326/1000 | Loss: 0.00002896
Iteration 327/1000 | Loss: 0.00002896
Iteration 328/1000 | Loss: 0.00002896
Iteration 329/1000 | Loss: 0.00002896
Iteration 330/1000 | Loss: 0.00002896
Iteration 331/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [2.8963406293769367e-05, 2.8963406293769367e-05, 2.8963406293769367e-05, 2.8963406293769367e-05, 2.8963406293769367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8963406293769367e-05

Optimization complete. Final v2v error: 3.7431416511535645 mm

Highest mean error: 13.206277847290039 mm for frame 40

Lowest mean error: 2.8595147132873535 mm for frame 160

Saving results

Total time: 384.24215149879456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840119
Iteration 2/25 | Loss: 0.00096419
Iteration 3/25 | Loss: 0.00083198
Iteration 4/25 | Loss: 0.00081450
Iteration 5/25 | Loss: 0.00080993
Iteration 6/25 | Loss: 0.00080858
Iteration 7/25 | Loss: 0.00080858
Iteration 8/25 | Loss: 0.00080858
Iteration 9/25 | Loss: 0.00080858
Iteration 10/25 | Loss: 0.00080858
Iteration 11/25 | Loss: 0.00080858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008085845038294792, 0.0008085845038294792, 0.0008085845038294792, 0.0008085845038294792, 0.0008085845038294792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008085845038294792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50388062
Iteration 2/25 | Loss: 0.00050179
Iteration 3/25 | Loss: 0.00050179
Iteration 4/25 | Loss: 0.00050179
Iteration 5/25 | Loss: 0.00050179
Iteration 6/25 | Loss: 0.00050178
Iteration 7/25 | Loss: 0.00050178
Iteration 8/25 | Loss: 0.00050178
Iteration 9/25 | Loss: 0.00050178
Iteration 10/25 | Loss: 0.00050178
Iteration 11/25 | Loss: 0.00050178
Iteration 12/25 | Loss: 0.00050178
Iteration 13/25 | Loss: 0.00050178
Iteration 14/25 | Loss: 0.00050178
Iteration 15/25 | Loss: 0.00050178
Iteration 16/25 | Loss: 0.00050178
Iteration 17/25 | Loss: 0.00050178
Iteration 18/25 | Loss: 0.00050178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000501783040817827, 0.000501783040817827, 0.000501783040817827, 0.000501783040817827, 0.000501783040817827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000501783040817827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050178
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001364
Iteration 5/1000 | Loss: 0.00001258
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001172
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001127
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001124
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001113
Iteration 25/1000 | Loss: 0.00001113
Iteration 26/1000 | Loss: 0.00001109
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001092
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001086
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001078
Iteration 95/1000 | Loss: 0.00001078
Iteration 96/1000 | Loss: 0.00001078
Iteration 97/1000 | Loss: 0.00001078
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001078
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001078
Iteration 106/1000 | Loss: 0.00001078
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.0776182534755208e-05, 1.0776182534755208e-05, 1.0776182534755208e-05, 1.0776182534755208e-05, 1.0776182534755208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0776182534755208e-05

Optimization complete. Final v2v error: 2.778611421585083 mm

Highest mean error: 3.021880626678467 mm for frame 109

Lowest mean error: 2.600895643234253 mm for frame 197

Saving results

Total time: 35.50282335281372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039273
Iteration 2/25 | Loss: 0.01039273
Iteration 3/25 | Loss: 0.01039273
Iteration 4/25 | Loss: 0.01039272
Iteration 5/25 | Loss: 0.01039272
Iteration 6/25 | Loss: 0.01039272
Iteration 7/25 | Loss: 0.01039272
Iteration 8/25 | Loss: 0.01039272
Iteration 9/25 | Loss: 0.01039272
Iteration 10/25 | Loss: 0.01039272
Iteration 11/25 | Loss: 0.01039272
Iteration 12/25 | Loss: 0.01039272
Iteration 13/25 | Loss: 0.01039272
Iteration 14/25 | Loss: 0.01039271
Iteration 15/25 | Loss: 0.01039271
Iteration 16/25 | Loss: 0.01039271
Iteration 17/25 | Loss: 0.01039271
Iteration 18/25 | Loss: 0.01039271
Iteration 19/25 | Loss: 0.01039271
Iteration 20/25 | Loss: 0.01039271
Iteration 21/25 | Loss: 0.01039271
Iteration 22/25 | Loss: 0.01039271
Iteration 23/25 | Loss: 0.01039270
Iteration 24/25 | Loss: 0.01039270
Iteration 25/25 | Loss: 0.01039270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81274605
Iteration 2/25 | Loss: 0.12111381
Iteration 3/25 | Loss: 0.11283951
Iteration 4/25 | Loss: 0.10850684
Iteration 5/25 | Loss: 0.10799205
Iteration 6/25 | Loss: 0.10799200
Iteration 7/25 | Loss: 0.10799199
Iteration 8/25 | Loss: 0.10799199
Iteration 9/25 | Loss: 0.10799199
Iteration 10/25 | Loss: 0.10799199
Iteration 11/25 | Loss: 0.10799199
Iteration 12/25 | Loss: 0.10799199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.10799198597669601, 0.10799198597669601, 0.10799198597669601, 0.10799198597669601, 0.10799198597669601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10799198597669601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10799199
Iteration 2/1000 | Loss: 0.00348391
Iteration 3/1000 | Loss: 0.00653343
Iteration 4/1000 | Loss: 0.03672973
Iteration 5/1000 | Loss: 0.00058665
Iteration 6/1000 | Loss: 0.00168224
Iteration 7/1000 | Loss: 0.00154413
Iteration 8/1000 | Loss: 0.00058610
Iteration 9/1000 | Loss: 0.00020565
Iteration 10/1000 | Loss: 0.00016993
Iteration 11/1000 | Loss: 0.00008121
Iteration 12/1000 | Loss: 0.00035348
Iteration 13/1000 | Loss: 0.00018709
Iteration 14/1000 | Loss: 0.00126310
Iteration 15/1000 | Loss: 0.00009756
Iteration 16/1000 | Loss: 0.00009556
Iteration 17/1000 | Loss: 0.00010322
Iteration 18/1000 | Loss: 0.00005160
Iteration 19/1000 | Loss: 0.00011794
Iteration 20/1000 | Loss: 0.00015253
Iteration 21/1000 | Loss: 0.00005286
Iteration 22/1000 | Loss: 0.00023227
Iteration 23/1000 | Loss: 0.00018170
Iteration 24/1000 | Loss: 0.00051646
Iteration 25/1000 | Loss: 0.00005814
Iteration 26/1000 | Loss: 0.00018717
Iteration 27/1000 | Loss: 0.00013667
Iteration 28/1000 | Loss: 0.00009845
Iteration 29/1000 | Loss: 0.00027205
Iteration 30/1000 | Loss: 0.00005469
Iteration 31/1000 | Loss: 0.00008853
Iteration 32/1000 | Loss: 0.00005131
Iteration 33/1000 | Loss: 0.00007189
Iteration 34/1000 | Loss: 0.00002835
Iteration 35/1000 | Loss: 0.00004485
Iteration 36/1000 | Loss: 0.00005017
Iteration 37/1000 | Loss: 0.00016252
Iteration 38/1000 | Loss: 0.00015043
Iteration 39/1000 | Loss: 0.00009369
Iteration 40/1000 | Loss: 0.00003214
Iteration 41/1000 | Loss: 0.00003455
Iteration 42/1000 | Loss: 0.00008005
Iteration 43/1000 | Loss: 0.00078526
Iteration 44/1000 | Loss: 0.00002475
Iteration 45/1000 | Loss: 0.00004444
Iteration 46/1000 | Loss: 0.00004814
Iteration 47/1000 | Loss: 0.00007582
Iteration 48/1000 | Loss: 0.00004778
Iteration 49/1000 | Loss: 0.00003877
Iteration 50/1000 | Loss: 0.00070257
Iteration 51/1000 | Loss: 0.00008964
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00006280
Iteration 54/1000 | Loss: 0.00002157
Iteration 55/1000 | Loss: 0.00003263
Iteration 56/1000 | Loss: 0.00003614
Iteration 57/1000 | Loss: 0.00002209
Iteration 58/1000 | Loss: 0.00002382
Iteration 59/1000 | Loss: 0.00002954
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00002301
Iteration 89/1000 | Loss: 0.00005300
Iteration 90/1000 | Loss: 0.00009524
Iteration 91/1000 | Loss: 0.00002719
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00013443
Iteration 96/1000 | Loss: 0.00003438
Iteration 97/1000 | Loss: 0.00002171
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002598
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00002256
Iteration 103/1000 | Loss: 0.00006583
Iteration 104/1000 | Loss: 0.00007218
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00003588
Iteration 107/1000 | Loss: 0.00002103
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002269
Iteration 111/1000 | Loss: 0.00002269
Iteration 112/1000 | Loss: 0.00002269
Iteration 113/1000 | Loss: 0.00002269
Iteration 114/1000 | Loss: 0.00002269
Iteration 115/1000 | Loss: 0.00002269
Iteration 116/1000 | Loss: 0.00002269
Iteration 117/1000 | Loss: 0.00002269
Iteration 118/1000 | Loss: 0.00002269
Iteration 119/1000 | Loss: 0.00002269
Iteration 120/1000 | Loss: 0.00002269
Iteration 121/1000 | Loss: 0.00002269
Iteration 122/1000 | Loss: 0.00002269
Iteration 123/1000 | Loss: 0.00002269
Iteration 124/1000 | Loss: 0.00002269
Iteration 125/1000 | Loss: 0.00002269
Iteration 126/1000 | Loss: 0.00002269
Iteration 127/1000 | Loss: 0.00002269
Iteration 128/1000 | Loss: 0.00002269
Iteration 129/1000 | Loss: 0.00002269
Iteration 130/1000 | Loss: 0.00002269
Iteration 131/1000 | Loss: 0.00002269
Iteration 132/1000 | Loss: 0.00002269
Iteration 133/1000 | Loss: 0.00002269
Iteration 134/1000 | Loss: 0.00002269
Iteration 135/1000 | Loss: 0.00002269
Iteration 136/1000 | Loss: 0.00002269
Iteration 137/1000 | Loss: 0.00002269
Iteration 138/1000 | Loss: 0.00002269
Iteration 139/1000 | Loss: 0.00002269
Iteration 140/1000 | Loss: 0.00002269
Iteration 141/1000 | Loss: 0.00002269
Iteration 142/1000 | Loss: 0.00002269
Iteration 143/1000 | Loss: 0.00002269
Iteration 144/1000 | Loss: 0.00002269
Iteration 145/1000 | Loss: 0.00002269
Iteration 146/1000 | Loss: 0.00002269
Iteration 147/1000 | Loss: 0.00002269
Iteration 148/1000 | Loss: 0.00002269
Iteration 149/1000 | Loss: 0.00002269
Iteration 150/1000 | Loss: 0.00002269
Iteration 151/1000 | Loss: 0.00002269
Iteration 152/1000 | Loss: 0.00002269
Iteration 153/1000 | Loss: 0.00002269
Iteration 154/1000 | Loss: 0.00002269
Iteration 155/1000 | Loss: 0.00002269
Iteration 156/1000 | Loss: 0.00002269
Iteration 157/1000 | Loss: 0.00002269
Iteration 158/1000 | Loss: 0.00002269
Iteration 159/1000 | Loss: 0.00002269
Iteration 160/1000 | Loss: 0.00002269
Iteration 161/1000 | Loss: 0.00002269
Iteration 162/1000 | Loss: 0.00002269
Iteration 163/1000 | Loss: 0.00002269
Iteration 164/1000 | Loss: 0.00002269
Iteration 165/1000 | Loss: 0.00002269
Iteration 166/1000 | Loss: 0.00002269
Iteration 167/1000 | Loss: 0.00002269
Iteration 168/1000 | Loss: 0.00002269
Iteration 169/1000 | Loss: 0.00002269
Iteration 170/1000 | Loss: 0.00002269
Iteration 171/1000 | Loss: 0.00002269
Iteration 172/1000 | Loss: 0.00002269
Iteration 173/1000 | Loss: 0.00002269
Iteration 174/1000 | Loss: 0.00002269
Iteration 175/1000 | Loss: 0.00002269
Iteration 176/1000 | Loss: 0.00002269
Iteration 177/1000 | Loss: 0.00002269
Iteration 178/1000 | Loss: 0.00002269
Iteration 179/1000 | Loss: 0.00002269
Iteration 180/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.2693315258948132e-05, 2.2693315258948132e-05, 2.2693315258948132e-05, 2.2693315258948132e-05, 2.2693315258948132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2693315258948132e-05

Optimization complete. Final v2v error: 3.6580896377563477 mm

Highest mean error: 28.866621017456055 mm for frame 136

Lowest mean error: 3.4260971546173096 mm for frame 118

Saving results

Total time: 137.95927262306213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850706
Iteration 2/25 | Loss: 0.00098836
Iteration 3/25 | Loss: 0.00084419
Iteration 4/25 | Loss: 0.00082351
Iteration 5/25 | Loss: 0.00081800
Iteration 6/25 | Loss: 0.00081608
Iteration 7/25 | Loss: 0.00081585
Iteration 8/25 | Loss: 0.00081585
Iteration 9/25 | Loss: 0.00081585
Iteration 10/25 | Loss: 0.00081585
Iteration 11/25 | Loss: 0.00081585
Iteration 12/25 | Loss: 0.00081585
Iteration 13/25 | Loss: 0.00081585
Iteration 14/25 | Loss: 0.00081585
Iteration 15/25 | Loss: 0.00081585
Iteration 16/25 | Loss: 0.00081585
Iteration 17/25 | Loss: 0.00081585
Iteration 18/25 | Loss: 0.00081585
Iteration 19/25 | Loss: 0.00081585
Iteration 20/25 | Loss: 0.00081585
Iteration 21/25 | Loss: 0.00081585
Iteration 22/25 | Loss: 0.00081585
Iteration 23/25 | Loss: 0.00081585
Iteration 24/25 | Loss: 0.00081585
Iteration 25/25 | Loss: 0.00081585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50040615
Iteration 2/25 | Loss: 0.00052138
Iteration 3/25 | Loss: 0.00052136
Iteration 4/25 | Loss: 0.00052136
Iteration 5/25 | Loss: 0.00052136
Iteration 6/25 | Loss: 0.00052136
Iteration 7/25 | Loss: 0.00052136
Iteration 8/25 | Loss: 0.00052136
Iteration 9/25 | Loss: 0.00052136
Iteration 10/25 | Loss: 0.00052136
Iteration 11/25 | Loss: 0.00052136
Iteration 12/25 | Loss: 0.00052136
Iteration 13/25 | Loss: 0.00052136
Iteration 14/25 | Loss: 0.00052136
Iteration 15/25 | Loss: 0.00052136
Iteration 16/25 | Loss: 0.00052136
Iteration 17/25 | Loss: 0.00052136
Iteration 18/25 | Loss: 0.00052136
Iteration 19/25 | Loss: 0.00052136
Iteration 20/25 | Loss: 0.00052136
Iteration 21/25 | Loss: 0.00052136
Iteration 22/25 | Loss: 0.00052136
Iteration 23/25 | Loss: 0.00052136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005213574040681124, 0.0005213574040681124, 0.0005213574040681124, 0.0005213574040681124, 0.0005213574040681124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005213574040681124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052136
Iteration 2/1000 | Loss: 0.00002605
Iteration 3/1000 | Loss: 0.00001871
Iteration 4/1000 | Loss: 0.00001580
Iteration 5/1000 | Loss: 0.00001452
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001226
Iteration 167/1000 | Loss: 0.00001226
Iteration 168/1000 | Loss: 0.00001226
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001223
Iteration 187/1000 | Loss: 0.00001223
Iteration 188/1000 | Loss: 0.00001223
Iteration 189/1000 | Loss: 0.00001223
Iteration 190/1000 | Loss: 0.00001223
Iteration 191/1000 | Loss: 0.00001223
Iteration 192/1000 | Loss: 0.00001223
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Iteration 201/1000 | Loss: 0.00001222
Iteration 202/1000 | Loss: 0.00001222
Iteration 203/1000 | Loss: 0.00001222
Iteration 204/1000 | Loss: 0.00001222
Iteration 205/1000 | Loss: 0.00001222
Iteration 206/1000 | Loss: 0.00001222
Iteration 207/1000 | Loss: 0.00001222
Iteration 208/1000 | Loss: 0.00001222
Iteration 209/1000 | Loss: 0.00001222
Iteration 210/1000 | Loss: 0.00001222
Iteration 211/1000 | Loss: 0.00001222
Iteration 212/1000 | Loss: 0.00001222
Iteration 213/1000 | Loss: 0.00001222
Iteration 214/1000 | Loss: 0.00001222
Iteration 215/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.222016908286605e-05, 1.222016908286605e-05, 1.222016908286605e-05, 1.222016908286605e-05, 1.222016908286605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.222016908286605e-05

Optimization complete. Final v2v error: 2.970254898071289 mm

Highest mean error: 3.0762104988098145 mm for frame 130

Lowest mean error: 2.8624179363250732 mm for frame 118

Saving results

Total time: 38.71789050102234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00661298
Iteration 2/25 | Loss: 0.00129852
Iteration 3/25 | Loss: 0.00099564
Iteration 4/25 | Loss: 0.00093291
Iteration 5/25 | Loss: 0.00092142
Iteration 6/25 | Loss: 0.00091462
Iteration 7/25 | Loss: 0.00090981
Iteration 8/25 | Loss: 0.00090267
Iteration 9/25 | Loss: 0.00090204
Iteration 10/25 | Loss: 0.00089809
Iteration 11/25 | Loss: 0.00089490
Iteration 12/25 | Loss: 0.00089321
Iteration 13/25 | Loss: 0.00089031
Iteration 14/25 | Loss: 0.00088951
Iteration 15/25 | Loss: 0.00088939
Iteration 16/25 | Loss: 0.00088939
Iteration 17/25 | Loss: 0.00088938
Iteration 18/25 | Loss: 0.00088938
Iteration 19/25 | Loss: 0.00088938
Iteration 20/25 | Loss: 0.00088938
Iteration 21/25 | Loss: 0.00088938
Iteration 22/25 | Loss: 0.00088938
Iteration 23/25 | Loss: 0.00088938
Iteration 24/25 | Loss: 0.00088938
Iteration 25/25 | Loss: 0.00088938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16000676
Iteration 2/25 | Loss: 0.00067360
Iteration 3/25 | Loss: 0.00067329
Iteration 4/25 | Loss: 0.00067329
Iteration 5/25 | Loss: 0.00067329
Iteration 6/25 | Loss: 0.00067328
Iteration 7/25 | Loss: 0.00067328
Iteration 8/25 | Loss: 0.00067328
Iteration 9/25 | Loss: 0.00067328
Iteration 10/25 | Loss: 0.00067328
Iteration 11/25 | Loss: 0.00067328
Iteration 12/25 | Loss: 0.00067328
Iteration 13/25 | Loss: 0.00067328
Iteration 14/25 | Loss: 0.00067328
Iteration 15/25 | Loss: 0.00067328
Iteration 16/25 | Loss: 0.00067328
Iteration 17/25 | Loss: 0.00067328
Iteration 18/25 | Loss: 0.00067328
Iteration 19/25 | Loss: 0.00067328
Iteration 20/25 | Loss: 0.00067328
Iteration 21/25 | Loss: 0.00067328
Iteration 22/25 | Loss: 0.00067328
Iteration 23/25 | Loss: 0.00067328
Iteration 24/25 | Loss: 0.00067328
Iteration 25/25 | Loss: 0.00067328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067328
Iteration 2/1000 | Loss: 0.00005025
Iteration 3/1000 | Loss: 0.00003491
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002778
Iteration 7/1000 | Loss: 0.00002693
Iteration 8/1000 | Loss: 0.00002646
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002527
Iteration 12/1000 | Loss: 0.00002510
Iteration 13/1000 | Loss: 0.00002504
Iteration 14/1000 | Loss: 0.00002490
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002444
Iteration 19/1000 | Loss: 0.00002439
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002436
Iteration 23/1000 | Loss: 0.00002435
Iteration 24/1000 | Loss: 0.00002432
Iteration 25/1000 | Loss: 0.00002431
Iteration 26/1000 | Loss: 0.00002431
Iteration 27/1000 | Loss: 0.00002430
Iteration 28/1000 | Loss: 0.00002430
Iteration 29/1000 | Loss: 0.00002429
Iteration 30/1000 | Loss: 0.00002429
Iteration 31/1000 | Loss: 0.00002429
Iteration 32/1000 | Loss: 0.00002428
Iteration 33/1000 | Loss: 0.00002428
Iteration 34/1000 | Loss: 0.00002422
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002421
Iteration 37/1000 | Loss: 0.00002421
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002413
Iteration 41/1000 | Loss: 0.00002412
Iteration 42/1000 | Loss: 0.00002411
Iteration 43/1000 | Loss: 0.00002408
Iteration 44/1000 | Loss: 0.00002401
Iteration 45/1000 | Loss: 0.00002400
Iteration 46/1000 | Loss: 0.00002399
Iteration 47/1000 | Loss: 0.00002397
Iteration 48/1000 | Loss: 0.00002395
Iteration 49/1000 | Loss: 0.00002394
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00002392
Iteration 52/1000 | Loss: 0.00002391
Iteration 53/1000 | Loss: 0.00002391
Iteration 54/1000 | Loss: 0.00002391
Iteration 55/1000 | Loss: 0.00002390
Iteration 56/1000 | Loss: 0.00002390
Iteration 57/1000 | Loss: 0.00002389
Iteration 58/1000 | Loss: 0.00002389
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002388
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00002385
Iteration 70/1000 | Loss: 0.00002385
Iteration 71/1000 | Loss: 0.00002385
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002385
Iteration 74/1000 | Loss: 0.00002385
Iteration 75/1000 | Loss: 0.00002385
Iteration 76/1000 | Loss: 0.00002385
Iteration 77/1000 | Loss: 0.00002384
Iteration 78/1000 | Loss: 0.00002384
Iteration 79/1000 | Loss: 0.00002384
Iteration 80/1000 | Loss: 0.00002384
Iteration 81/1000 | Loss: 0.00002384
Iteration 82/1000 | Loss: 0.00002384
Iteration 83/1000 | Loss: 0.00002384
Iteration 84/1000 | Loss: 0.00002384
Iteration 85/1000 | Loss: 0.00002383
Iteration 86/1000 | Loss: 0.00002383
Iteration 87/1000 | Loss: 0.00002383
Iteration 88/1000 | Loss: 0.00002382
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002382
Iteration 92/1000 | Loss: 0.00002381
Iteration 93/1000 | Loss: 0.00002381
Iteration 94/1000 | Loss: 0.00002381
Iteration 95/1000 | Loss: 0.00002381
Iteration 96/1000 | Loss: 0.00002380
Iteration 97/1000 | Loss: 0.00002380
Iteration 98/1000 | Loss: 0.00002380
Iteration 99/1000 | Loss: 0.00002379
Iteration 100/1000 | Loss: 0.00002379
Iteration 101/1000 | Loss: 0.00002379
Iteration 102/1000 | Loss: 0.00002378
Iteration 103/1000 | Loss: 0.00002378
Iteration 104/1000 | Loss: 0.00002378
Iteration 105/1000 | Loss: 0.00002378
Iteration 106/1000 | Loss: 0.00002378
Iteration 107/1000 | Loss: 0.00002378
Iteration 108/1000 | Loss: 0.00002378
Iteration 109/1000 | Loss: 0.00002378
Iteration 110/1000 | Loss: 0.00002378
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002377
Iteration 113/1000 | Loss: 0.00002377
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002377
Iteration 117/1000 | Loss: 0.00002377
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002376
Iteration 121/1000 | Loss: 0.00002376
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002374
Iteration 126/1000 | Loss: 0.00002374
Iteration 127/1000 | Loss: 0.00002374
Iteration 128/1000 | Loss: 0.00002374
Iteration 129/1000 | Loss: 0.00002373
Iteration 130/1000 | Loss: 0.00002373
Iteration 131/1000 | Loss: 0.00002373
Iteration 132/1000 | Loss: 0.00002373
Iteration 133/1000 | Loss: 0.00002373
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002373
Iteration 136/1000 | Loss: 0.00002373
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00002372
Iteration 139/1000 | Loss: 0.00002372
Iteration 140/1000 | Loss: 0.00002372
Iteration 141/1000 | Loss: 0.00002371
Iteration 142/1000 | Loss: 0.00002371
Iteration 143/1000 | Loss: 0.00002371
Iteration 144/1000 | Loss: 0.00002371
Iteration 145/1000 | Loss: 0.00002371
Iteration 146/1000 | Loss: 0.00002370
Iteration 147/1000 | Loss: 0.00002370
Iteration 148/1000 | Loss: 0.00002370
Iteration 149/1000 | Loss: 0.00002370
Iteration 150/1000 | Loss: 0.00002370
Iteration 151/1000 | Loss: 0.00002370
Iteration 152/1000 | Loss: 0.00002370
Iteration 153/1000 | Loss: 0.00002370
Iteration 154/1000 | Loss: 0.00002370
Iteration 155/1000 | Loss: 0.00002370
Iteration 156/1000 | Loss: 0.00002370
Iteration 157/1000 | Loss: 0.00002370
Iteration 158/1000 | Loss: 0.00002370
Iteration 159/1000 | Loss: 0.00002370
Iteration 160/1000 | Loss: 0.00002369
Iteration 161/1000 | Loss: 0.00002369
Iteration 162/1000 | Loss: 0.00002369
Iteration 163/1000 | Loss: 0.00002369
Iteration 164/1000 | Loss: 0.00002369
Iteration 165/1000 | Loss: 0.00002369
Iteration 166/1000 | Loss: 0.00002369
Iteration 167/1000 | Loss: 0.00002369
Iteration 168/1000 | Loss: 0.00002368
Iteration 169/1000 | Loss: 0.00002368
Iteration 170/1000 | Loss: 0.00002368
Iteration 171/1000 | Loss: 0.00002368
Iteration 172/1000 | Loss: 0.00002368
Iteration 173/1000 | Loss: 0.00002368
Iteration 174/1000 | Loss: 0.00002368
Iteration 175/1000 | Loss: 0.00002367
Iteration 176/1000 | Loss: 0.00002367
Iteration 177/1000 | Loss: 0.00002367
Iteration 178/1000 | Loss: 0.00002367
Iteration 179/1000 | Loss: 0.00002367
Iteration 180/1000 | Loss: 0.00002367
Iteration 181/1000 | Loss: 0.00002367
Iteration 182/1000 | Loss: 0.00002367
Iteration 183/1000 | Loss: 0.00002367
Iteration 184/1000 | Loss: 0.00002367
Iteration 185/1000 | Loss: 0.00002367
Iteration 186/1000 | Loss: 0.00002367
Iteration 187/1000 | Loss: 0.00002367
Iteration 188/1000 | Loss: 0.00002367
Iteration 189/1000 | Loss: 0.00002367
Iteration 190/1000 | Loss: 0.00002367
Iteration 191/1000 | Loss: 0.00002367
Iteration 192/1000 | Loss: 0.00002367
Iteration 193/1000 | Loss: 0.00002367
Iteration 194/1000 | Loss: 0.00002367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.3671338567510247e-05, 2.3671338567510247e-05, 2.3671338567510247e-05, 2.3671338567510247e-05, 2.3671338567510247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3671338567510247e-05

Optimization complete. Final v2v error: 3.9464094638824463 mm

Highest mean error: 6.244045734405518 mm for frame 117

Lowest mean error: 3.073146343231201 mm for frame 163

Saving results

Total time: 74.56841945648193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964789
Iteration 2/25 | Loss: 0.00287146
Iteration 3/25 | Loss: 0.00171792
Iteration 4/25 | Loss: 0.00148131
Iteration 5/25 | Loss: 0.00146364
Iteration 6/25 | Loss: 0.00150928
Iteration 7/25 | Loss: 0.00142532
Iteration 8/25 | Loss: 0.00129107
Iteration 9/25 | Loss: 0.00120666
Iteration 10/25 | Loss: 0.00117863
Iteration 11/25 | Loss: 0.00114626
Iteration 12/25 | Loss: 0.00111520
Iteration 13/25 | Loss: 0.00110980
Iteration 14/25 | Loss: 0.00110715
Iteration 15/25 | Loss: 0.00110967
Iteration 16/25 | Loss: 0.00109311
Iteration 17/25 | Loss: 0.00109525
Iteration 18/25 | Loss: 0.00106630
Iteration 19/25 | Loss: 0.00104867
Iteration 20/25 | Loss: 0.00105462
Iteration 21/25 | Loss: 0.00104582
Iteration 22/25 | Loss: 0.00104352
Iteration 23/25 | Loss: 0.00103298
Iteration 24/25 | Loss: 0.00103143
Iteration 25/25 | Loss: 0.00103252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25789845
Iteration 2/25 | Loss: 0.00188715
Iteration 3/25 | Loss: 0.00188714
Iteration 4/25 | Loss: 0.00188714
Iteration 5/25 | Loss: 0.00188714
Iteration 6/25 | Loss: 0.00188714
Iteration 7/25 | Loss: 0.00188714
Iteration 8/25 | Loss: 0.00188714
Iteration 9/25 | Loss: 0.00188714
Iteration 10/25 | Loss: 0.00188714
Iteration 11/25 | Loss: 0.00188714
Iteration 12/25 | Loss: 0.00188714
Iteration 13/25 | Loss: 0.00188714
Iteration 14/25 | Loss: 0.00188714
Iteration 15/25 | Loss: 0.00188714
Iteration 16/25 | Loss: 0.00188714
Iteration 17/25 | Loss: 0.00188714
Iteration 18/25 | Loss: 0.00188714
Iteration 19/25 | Loss: 0.00188714
Iteration 20/25 | Loss: 0.00188714
Iteration 21/25 | Loss: 0.00188714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018871407955884933, 0.0018871407955884933, 0.0018871407955884933, 0.0018871407955884933, 0.0018871407955884933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018871407955884933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188714
Iteration 2/1000 | Loss: 0.00036903
Iteration 3/1000 | Loss: 0.00083648
Iteration 4/1000 | Loss: 0.00077619
Iteration 5/1000 | Loss: 0.00030615
Iteration 6/1000 | Loss: 0.00072713
Iteration 7/1000 | Loss: 0.00078014
Iteration 8/1000 | Loss: 0.00048248
Iteration 9/1000 | Loss: 0.00018526
Iteration 10/1000 | Loss: 0.00026037
Iteration 11/1000 | Loss: 0.00059470
Iteration 12/1000 | Loss: 0.00130475
Iteration 13/1000 | Loss: 0.00095747
Iteration 14/1000 | Loss: 0.00058764
Iteration 15/1000 | Loss: 0.00039467
Iteration 16/1000 | Loss: 0.00056761
Iteration 17/1000 | Loss: 0.00035292
Iteration 18/1000 | Loss: 0.00052188
Iteration 19/1000 | Loss: 0.00070521
Iteration 20/1000 | Loss: 0.00017018
Iteration 21/1000 | Loss: 0.00106793
Iteration 22/1000 | Loss: 0.00011055
Iteration 23/1000 | Loss: 0.00023519
Iteration 24/1000 | Loss: 0.00034001
Iteration 25/1000 | Loss: 0.00011777
Iteration 26/1000 | Loss: 0.00009523
Iteration 27/1000 | Loss: 0.00010979
Iteration 28/1000 | Loss: 0.00012272
Iteration 29/1000 | Loss: 0.00065650
Iteration 30/1000 | Loss: 0.00009566
Iteration 31/1000 | Loss: 0.00009584
Iteration 32/1000 | Loss: 0.00010523
Iteration 33/1000 | Loss: 0.00011031
Iteration 34/1000 | Loss: 0.00010611
Iteration 35/1000 | Loss: 0.00011020
Iteration 36/1000 | Loss: 0.00042064
Iteration 37/1000 | Loss: 0.00102987
Iteration 38/1000 | Loss: 0.00052209
Iteration 39/1000 | Loss: 0.00039440
Iteration 40/1000 | Loss: 0.00033534
Iteration 41/1000 | Loss: 0.00010677
Iteration 42/1000 | Loss: 0.00013486
Iteration 43/1000 | Loss: 0.00013906
Iteration 44/1000 | Loss: 0.00030537
Iteration 45/1000 | Loss: 0.00021337
Iteration 46/1000 | Loss: 0.00024542
Iteration 47/1000 | Loss: 0.00017810
Iteration 48/1000 | Loss: 0.00024433
Iteration 49/1000 | Loss: 0.00016302
Iteration 50/1000 | Loss: 0.00033412
Iteration 51/1000 | Loss: 0.00019249
Iteration 52/1000 | Loss: 0.00014985
Iteration 53/1000 | Loss: 0.00032191
Iteration 54/1000 | Loss: 0.00041750
Iteration 55/1000 | Loss: 0.00016670
Iteration 56/1000 | Loss: 0.00025769
Iteration 57/1000 | Loss: 0.00014336
Iteration 58/1000 | Loss: 0.00051678
Iteration 59/1000 | Loss: 0.00030911
Iteration 60/1000 | Loss: 0.00032883
Iteration 61/1000 | Loss: 0.00025331
Iteration 62/1000 | Loss: 0.00025871
Iteration 63/1000 | Loss: 0.00046960
Iteration 64/1000 | Loss: 0.00024613
Iteration 65/1000 | Loss: 0.00024225
Iteration 66/1000 | Loss: 0.00012463
Iteration 67/1000 | Loss: 0.00012200
Iteration 68/1000 | Loss: 0.00007664
Iteration 69/1000 | Loss: 0.00009412
Iteration 70/1000 | Loss: 0.00009902
Iteration 71/1000 | Loss: 0.00012141
Iteration 72/1000 | Loss: 0.00010347
Iteration 73/1000 | Loss: 0.00013724
Iteration 74/1000 | Loss: 0.00012966
Iteration 75/1000 | Loss: 0.00013973
Iteration 76/1000 | Loss: 0.00013566
Iteration 77/1000 | Loss: 0.00036179
Iteration 78/1000 | Loss: 0.00025961
Iteration 79/1000 | Loss: 0.00016211
Iteration 80/1000 | Loss: 0.00036047
Iteration 81/1000 | Loss: 0.00020579
Iteration 82/1000 | Loss: 0.00014743
Iteration 83/1000 | Loss: 0.00037521
Iteration 84/1000 | Loss: 0.00029571
Iteration 85/1000 | Loss: 0.00043898
Iteration 86/1000 | Loss: 0.00023623
Iteration 87/1000 | Loss: 0.00034527
Iteration 88/1000 | Loss: 0.00023573
Iteration 89/1000 | Loss: 0.00037302
Iteration 90/1000 | Loss: 0.00012899
Iteration 91/1000 | Loss: 0.00033809
Iteration 92/1000 | Loss: 0.00017417
Iteration 93/1000 | Loss: 0.00025510
Iteration 94/1000 | Loss: 0.00011559
Iteration 95/1000 | Loss: 0.00015952
Iteration 96/1000 | Loss: 0.00033910
Iteration 97/1000 | Loss: 0.00025841
Iteration 98/1000 | Loss: 0.00018317
Iteration 99/1000 | Loss: 0.00014564
Iteration 100/1000 | Loss: 0.00066750
Iteration 101/1000 | Loss: 0.00015902
Iteration 102/1000 | Loss: 0.00014729
Iteration 103/1000 | Loss: 0.00021170
Iteration 104/1000 | Loss: 0.00092793
Iteration 105/1000 | Loss: 0.00026563
Iteration 106/1000 | Loss: 0.00033639
Iteration 107/1000 | Loss: 0.00015873
Iteration 108/1000 | Loss: 0.00022630
Iteration 109/1000 | Loss: 0.00015491
Iteration 110/1000 | Loss: 0.00010643
Iteration 111/1000 | Loss: 0.00012568
Iteration 112/1000 | Loss: 0.00017989
Iteration 113/1000 | Loss: 0.00027230
Iteration 114/1000 | Loss: 0.00022639
Iteration 115/1000 | Loss: 0.00043589
Iteration 116/1000 | Loss: 0.00012970
Iteration 117/1000 | Loss: 0.00014422
Iteration 118/1000 | Loss: 0.00009788
Iteration 119/1000 | Loss: 0.00008092
Iteration 120/1000 | Loss: 0.00009837
Iteration 121/1000 | Loss: 0.00010156
Iteration 122/1000 | Loss: 0.00011260
Iteration 123/1000 | Loss: 0.00007788
Iteration 124/1000 | Loss: 0.00006936
Iteration 125/1000 | Loss: 0.00007734
Iteration 126/1000 | Loss: 0.00009255
Iteration 127/1000 | Loss: 0.00017121
Iteration 128/1000 | Loss: 0.00010418
Iteration 129/1000 | Loss: 0.00013030
Iteration 130/1000 | Loss: 0.00008434
Iteration 131/1000 | Loss: 0.00007349
Iteration 132/1000 | Loss: 0.00008790
Iteration 133/1000 | Loss: 0.00006525
Iteration 134/1000 | Loss: 0.00007633
Iteration 135/1000 | Loss: 0.00007040
Iteration 136/1000 | Loss: 0.00009616
Iteration 137/1000 | Loss: 0.00010010
Iteration 138/1000 | Loss: 0.00101647
Iteration 139/1000 | Loss: 0.00039793
Iteration 140/1000 | Loss: 0.00102008
Iteration 141/1000 | Loss: 0.00129713
Iteration 142/1000 | Loss: 0.00057167
Iteration 143/1000 | Loss: 0.00017222
Iteration 144/1000 | Loss: 0.00008942
Iteration 145/1000 | Loss: 0.00009995
Iteration 146/1000 | Loss: 0.00023735
Iteration 147/1000 | Loss: 0.00011000
Iteration 148/1000 | Loss: 0.00006776
Iteration 149/1000 | Loss: 0.00008771
Iteration 150/1000 | Loss: 0.00006910
Iteration 151/1000 | Loss: 0.00099258
Iteration 152/1000 | Loss: 0.00043155
Iteration 153/1000 | Loss: 0.00081731
Iteration 154/1000 | Loss: 0.00027386
Iteration 155/1000 | Loss: 0.00080805
Iteration 156/1000 | Loss: 0.00047150
Iteration 157/1000 | Loss: 0.00081789
Iteration 158/1000 | Loss: 0.00048529
Iteration 159/1000 | Loss: 0.00008605
Iteration 160/1000 | Loss: 0.00079384
Iteration 161/1000 | Loss: 0.00008242
Iteration 162/1000 | Loss: 0.00006606
Iteration 163/1000 | Loss: 0.00005137
Iteration 164/1000 | Loss: 0.00009032
Iteration 165/1000 | Loss: 0.00006529
Iteration 166/1000 | Loss: 0.00006147
Iteration 167/1000 | Loss: 0.00004636
Iteration 168/1000 | Loss: 0.00007069
Iteration 169/1000 | Loss: 0.00006451
Iteration 170/1000 | Loss: 0.00092931
Iteration 171/1000 | Loss: 0.00044784
Iteration 172/1000 | Loss: 0.00084354
Iteration 173/1000 | Loss: 0.00060325
Iteration 174/1000 | Loss: 0.00006822
Iteration 175/1000 | Loss: 0.00009213
Iteration 176/1000 | Loss: 0.00005943
Iteration 177/1000 | Loss: 0.00005390
Iteration 178/1000 | Loss: 0.00006221
Iteration 179/1000 | Loss: 0.00005760
Iteration 180/1000 | Loss: 0.00005844
Iteration 181/1000 | Loss: 0.00005660
Iteration 182/1000 | Loss: 0.00004357
Iteration 183/1000 | Loss: 0.00005896
Iteration 184/1000 | Loss: 0.00004631
Iteration 185/1000 | Loss: 0.00005485
Iteration 186/1000 | Loss: 0.00005541
Iteration 187/1000 | Loss: 0.00004852
Iteration 188/1000 | Loss: 0.00005842
Iteration 189/1000 | Loss: 0.00005453
Iteration 190/1000 | Loss: 0.00004785
Iteration 191/1000 | Loss: 0.00005663
Iteration 192/1000 | Loss: 0.00005331
Iteration 193/1000 | Loss: 0.00004348
Iteration 194/1000 | Loss: 0.00010763
Iteration 195/1000 | Loss: 0.00005910
Iteration 196/1000 | Loss: 0.00006010
Iteration 197/1000 | Loss: 0.00008258
Iteration 198/1000 | Loss: 0.00006841
Iteration 199/1000 | Loss: 0.00006113
Iteration 200/1000 | Loss: 0.00007828
Iteration 201/1000 | Loss: 0.00056684
Iteration 202/1000 | Loss: 0.00006483
Iteration 203/1000 | Loss: 0.00004882
Iteration 204/1000 | Loss: 0.00004396
Iteration 205/1000 | Loss: 0.00004358
Iteration 206/1000 | Loss: 0.00004778
Iteration 207/1000 | Loss: 0.00004652
Iteration 208/1000 | Loss: 0.00003913
Iteration 209/1000 | Loss: 0.00003835
Iteration 210/1000 | Loss: 0.00003803
Iteration 211/1000 | Loss: 0.00003784
Iteration 212/1000 | Loss: 0.00003774
Iteration 213/1000 | Loss: 0.00008207
Iteration 214/1000 | Loss: 0.00003764
Iteration 215/1000 | Loss: 0.00003743
Iteration 216/1000 | Loss: 0.00003732
Iteration 217/1000 | Loss: 0.00003724
Iteration 218/1000 | Loss: 0.00003723
Iteration 219/1000 | Loss: 0.00043441
Iteration 220/1000 | Loss: 0.00003947
Iteration 221/1000 | Loss: 0.00003719
Iteration 222/1000 | Loss: 0.00003638
Iteration 223/1000 | Loss: 0.00003556
Iteration 224/1000 | Loss: 0.00003492
Iteration 225/1000 | Loss: 0.00003467
Iteration 226/1000 | Loss: 0.00003451
Iteration 227/1000 | Loss: 0.00003447
Iteration 228/1000 | Loss: 0.00003445
Iteration 229/1000 | Loss: 0.00003444
Iteration 230/1000 | Loss: 0.00003433
Iteration 231/1000 | Loss: 0.00003430
Iteration 232/1000 | Loss: 0.00003429
Iteration 233/1000 | Loss: 0.00003428
Iteration 234/1000 | Loss: 0.00003428
Iteration 235/1000 | Loss: 0.00003428
Iteration 236/1000 | Loss: 0.00003428
Iteration 237/1000 | Loss: 0.00003428
Iteration 238/1000 | Loss: 0.00003428
Iteration 239/1000 | Loss: 0.00003428
Iteration 240/1000 | Loss: 0.00003428
Iteration 241/1000 | Loss: 0.00003427
Iteration 242/1000 | Loss: 0.00003427
Iteration 243/1000 | Loss: 0.00003425
Iteration 244/1000 | Loss: 0.00003424
Iteration 245/1000 | Loss: 0.00003423
Iteration 246/1000 | Loss: 0.00003423
Iteration 247/1000 | Loss: 0.00003423
Iteration 248/1000 | Loss: 0.00003423
Iteration 249/1000 | Loss: 0.00003423
Iteration 250/1000 | Loss: 0.00003423
Iteration 251/1000 | Loss: 0.00003423
Iteration 252/1000 | Loss: 0.00003423
Iteration 253/1000 | Loss: 0.00003423
Iteration 254/1000 | Loss: 0.00003423
Iteration 255/1000 | Loss: 0.00003422
Iteration 256/1000 | Loss: 0.00003422
Iteration 257/1000 | Loss: 0.00003422
Iteration 258/1000 | Loss: 0.00003421
Iteration 259/1000 | Loss: 0.00003420
Iteration 260/1000 | Loss: 0.00003420
Iteration 261/1000 | Loss: 0.00003419
Iteration 262/1000 | Loss: 0.00003419
Iteration 263/1000 | Loss: 0.00003419
Iteration 264/1000 | Loss: 0.00003418
Iteration 265/1000 | Loss: 0.00003418
Iteration 266/1000 | Loss: 0.00003418
Iteration 267/1000 | Loss: 0.00003417
Iteration 268/1000 | Loss: 0.00003417
Iteration 269/1000 | Loss: 0.00003416
Iteration 270/1000 | Loss: 0.00003416
Iteration 271/1000 | Loss: 0.00003416
Iteration 272/1000 | Loss: 0.00003415
Iteration 273/1000 | Loss: 0.00003415
Iteration 274/1000 | Loss: 0.00003414
Iteration 275/1000 | Loss: 0.00003414
Iteration 276/1000 | Loss: 0.00003414
Iteration 277/1000 | Loss: 0.00003413
Iteration 278/1000 | Loss: 0.00003413
Iteration 279/1000 | Loss: 0.00003412
Iteration 280/1000 | Loss: 0.00003411
Iteration 281/1000 | Loss: 0.00003411
Iteration 282/1000 | Loss: 0.00003411
Iteration 283/1000 | Loss: 0.00003410
Iteration 284/1000 | Loss: 0.00003410
Iteration 285/1000 | Loss: 0.00003410
Iteration 286/1000 | Loss: 0.00003410
Iteration 287/1000 | Loss: 0.00003410
Iteration 288/1000 | Loss: 0.00003410
Iteration 289/1000 | Loss: 0.00003409
Iteration 290/1000 | Loss: 0.00003409
Iteration 291/1000 | Loss: 0.00003409
Iteration 292/1000 | Loss: 0.00003409
Iteration 293/1000 | Loss: 0.00003409
Iteration 294/1000 | Loss: 0.00003409
Iteration 295/1000 | Loss: 0.00003408
Iteration 296/1000 | Loss: 0.00003408
Iteration 297/1000 | Loss: 0.00003408
Iteration 298/1000 | Loss: 0.00003408
Iteration 299/1000 | Loss: 0.00003408
Iteration 300/1000 | Loss: 0.00003408
Iteration 301/1000 | Loss: 0.00003408
Iteration 302/1000 | Loss: 0.00003408
Iteration 303/1000 | Loss: 0.00003407
Iteration 304/1000 | Loss: 0.00003407
Iteration 305/1000 | Loss: 0.00003407
Iteration 306/1000 | Loss: 0.00003407
Iteration 307/1000 | Loss: 0.00003407
Iteration 308/1000 | Loss: 0.00003407
Iteration 309/1000 | Loss: 0.00003407
Iteration 310/1000 | Loss: 0.00003407
Iteration 311/1000 | Loss: 0.00003407
Iteration 312/1000 | Loss: 0.00003407
Iteration 313/1000 | Loss: 0.00003407
Iteration 314/1000 | Loss: 0.00003407
Iteration 315/1000 | Loss: 0.00003407
Iteration 316/1000 | Loss: 0.00003407
Iteration 317/1000 | Loss: 0.00003407
Iteration 318/1000 | Loss: 0.00003407
Iteration 319/1000 | Loss: 0.00003407
Iteration 320/1000 | Loss: 0.00003407
Iteration 321/1000 | Loss: 0.00003407
Iteration 322/1000 | Loss: 0.00003407
Iteration 323/1000 | Loss: 0.00003407
Iteration 324/1000 | Loss: 0.00003407
Iteration 325/1000 | Loss: 0.00003407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [3.407173790037632e-05, 3.407173790037632e-05, 3.407173790037632e-05, 3.407173790037632e-05, 3.407173790037632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.407173790037632e-05

Optimization complete. Final v2v error: 4.25833797454834 mm

Highest mean error: 11.872090339660645 mm for frame 20

Lowest mean error: 3.1060798168182373 mm for frame 92

Saving results

Total time: 364.03354024887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860023
Iteration 2/25 | Loss: 0.00120019
Iteration 3/25 | Loss: 0.00099246
Iteration 4/25 | Loss: 0.00093740
Iteration 5/25 | Loss: 0.00092739
Iteration 6/25 | Loss: 0.00092620
Iteration 7/25 | Loss: 0.00092600
Iteration 8/25 | Loss: 0.00092600
Iteration 9/25 | Loss: 0.00092600
Iteration 10/25 | Loss: 0.00092600
Iteration 11/25 | Loss: 0.00092600
Iteration 12/25 | Loss: 0.00092600
Iteration 13/25 | Loss: 0.00092600
Iteration 14/25 | Loss: 0.00092600
Iteration 15/25 | Loss: 0.00092600
Iteration 16/25 | Loss: 0.00092600
Iteration 17/25 | Loss: 0.00092600
Iteration 18/25 | Loss: 0.00092600
Iteration 19/25 | Loss: 0.00092600
Iteration 20/25 | Loss: 0.00092600
Iteration 21/25 | Loss: 0.00092600
Iteration 22/25 | Loss: 0.00092600
Iteration 23/25 | Loss: 0.00092600
Iteration 24/25 | Loss: 0.00092600
Iteration 25/25 | Loss: 0.00092600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07148182
Iteration 2/25 | Loss: 0.00063853
Iteration 3/25 | Loss: 0.00063852
Iteration 4/25 | Loss: 0.00063852
Iteration 5/25 | Loss: 0.00063852
Iteration 6/25 | Loss: 0.00063852
Iteration 7/25 | Loss: 0.00063852
Iteration 8/25 | Loss: 0.00063852
Iteration 9/25 | Loss: 0.00063852
Iteration 10/25 | Loss: 0.00063852
Iteration 11/25 | Loss: 0.00063852
Iteration 12/25 | Loss: 0.00063852
Iteration 13/25 | Loss: 0.00063852
Iteration 14/25 | Loss: 0.00063852
Iteration 15/25 | Loss: 0.00063852
Iteration 16/25 | Loss: 0.00063852
Iteration 17/25 | Loss: 0.00063852
Iteration 18/25 | Loss: 0.00063852
Iteration 19/25 | Loss: 0.00063852
Iteration 20/25 | Loss: 0.00063852
Iteration 21/25 | Loss: 0.00063852
Iteration 22/25 | Loss: 0.00063852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006385177257470787, 0.0006385177257470787, 0.0006385177257470787, 0.0006385177257470787, 0.0006385177257470787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006385177257470787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063852
Iteration 2/1000 | Loss: 0.00005520
Iteration 3/1000 | Loss: 0.00004217
Iteration 4/1000 | Loss: 0.00003674
Iteration 5/1000 | Loss: 0.00003450
Iteration 6/1000 | Loss: 0.00003289
Iteration 7/1000 | Loss: 0.00003161
Iteration 8/1000 | Loss: 0.00003076
Iteration 9/1000 | Loss: 0.00003035
Iteration 10/1000 | Loss: 0.00003012
Iteration 11/1000 | Loss: 0.00002987
Iteration 12/1000 | Loss: 0.00002985
Iteration 13/1000 | Loss: 0.00002980
Iteration 14/1000 | Loss: 0.00002969
Iteration 15/1000 | Loss: 0.00002964
Iteration 16/1000 | Loss: 0.00002964
Iteration 17/1000 | Loss: 0.00002963
Iteration 18/1000 | Loss: 0.00002961
Iteration 19/1000 | Loss: 0.00002961
Iteration 20/1000 | Loss: 0.00002960
Iteration 21/1000 | Loss: 0.00002958
Iteration 22/1000 | Loss: 0.00002955
Iteration 23/1000 | Loss: 0.00002955
Iteration 24/1000 | Loss: 0.00002954
Iteration 25/1000 | Loss: 0.00002953
Iteration 26/1000 | Loss: 0.00002953
Iteration 27/1000 | Loss: 0.00002953
Iteration 28/1000 | Loss: 0.00002953
Iteration 29/1000 | Loss: 0.00002951
Iteration 30/1000 | Loss: 0.00002951
Iteration 31/1000 | Loss: 0.00002950
Iteration 32/1000 | Loss: 0.00002950
Iteration 33/1000 | Loss: 0.00002949
Iteration 34/1000 | Loss: 0.00002949
Iteration 35/1000 | Loss: 0.00002949
Iteration 36/1000 | Loss: 0.00002948
Iteration 37/1000 | Loss: 0.00002948
Iteration 38/1000 | Loss: 0.00002946
Iteration 39/1000 | Loss: 0.00002946
Iteration 40/1000 | Loss: 0.00002946
Iteration 41/1000 | Loss: 0.00002946
Iteration 42/1000 | Loss: 0.00002946
Iteration 43/1000 | Loss: 0.00002946
Iteration 44/1000 | Loss: 0.00002946
Iteration 45/1000 | Loss: 0.00002946
Iteration 46/1000 | Loss: 0.00002946
Iteration 47/1000 | Loss: 0.00002946
Iteration 48/1000 | Loss: 0.00002946
Iteration 49/1000 | Loss: 0.00002945
Iteration 50/1000 | Loss: 0.00002945
Iteration 51/1000 | Loss: 0.00002945
Iteration 52/1000 | Loss: 0.00002945
Iteration 53/1000 | Loss: 0.00002944
Iteration 54/1000 | Loss: 0.00002944
Iteration 55/1000 | Loss: 0.00002944
Iteration 56/1000 | Loss: 0.00002944
Iteration 57/1000 | Loss: 0.00002944
Iteration 58/1000 | Loss: 0.00002944
Iteration 59/1000 | Loss: 0.00002944
Iteration 60/1000 | Loss: 0.00002944
Iteration 61/1000 | Loss: 0.00002944
Iteration 62/1000 | Loss: 0.00002944
Iteration 63/1000 | Loss: 0.00002944
Iteration 64/1000 | Loss: 0.00002944
Iteration 65/1000 | Loss: 0.00002944
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002944
Iteration 68/1000 | Loss: 0.00002943
Iteration 69/1000 | Loss: 0.00002943
Iteration 70/1000 | Loss: 0.00002943
Iteration 71/1000 | Loss: 0.00002943
Iteration 72/1000 | Loss: 0.00002943
Iteration 73/1000 | Loss: 0.00002943
Iteration 74/1000 | Loss: 0.00002943
Iteration 75/1000 | Loss: 0.00002943
Iteration 76/1000 | Loss: 0.00002942
Iteration 77/1000 | Loss: 0.00002942
Iteration 78/1000 | Loss: 0.00002942
Iteration 79/1000 | Loss: 0.00002942
Iteration 80/1000 | Loss: 0.00002942
Iteration 81/1000 | Loss: 0.00002942
Iteration 82/1000 | Loss: 0.00002942
Iteration 83/1000 | Loss: 0.00002942
Iteration 84/1000 | Loss: 0.00002942
Iteration 85/1000 | Loss: 0.00002942
Iteration 86/1000 | Loss: 0.00002942
Iteration 87/1000 | Loss: 0.00002942
Iteration 88/1000 | Loss: 0.00002942
Iteration 89/1000 | Loss: 0.00002942
Iteration 90/1000 | Loss: 0.00002942
Iteration 91/1000 | Loss: 0.00002942
Iteration 92/1000 | Loss: 0.00002942
Iteration 93/1000 | Loss: 0.00002942
Iteration 94/1000 | Loss: 0.00002942
Iteration 95/1000 | Loss: 0.00002942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.9416396500892006e-05, 2.9416396500892006e-05, 2.9416396500892006e-05, 2.9416396500892006e-05, 2.9416396500892006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9416396500892006e-05

Optimization complete. Final v2v error: 4.54885196685791 mm

Highest mean error: 4.73306131362915 mm for frame 68

Lowest mean error: 4.474663257598877 mm for frame 146

Saving results

Total time: 33.73014545440674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866810
Iteration 2/25 | Loss: 0.00109669
Iteration 3/25 | Loss: 0.00087451
Iteration 4/25 | Loss: 0.00085273
Iteration 5/25 | Loss: 0.00084614
Iteration 6/25 | Loss: 0.00084513
Iteration 7/25 | Loss: 0.00084513
Iteration 8/25 | Loss: 0.00084513
Iteration 9/25 | Loss: 0.00084513
Iteration 10/25 | Loss: 0.00084513
Iteration 11/25 | Loss: 0.00084513
Iteration 12/25 | Loss: 0.00084513
Iteration 13/25 | Loss: 0.00084513
Iteration 14/25 | Loss: 0.00084513
Iteration 15/25 | Loss: 0.00084513
Iteration 16/25 | Loss: 0.00084513
Iteration 17/25 | Loss: 0.00084513
Iteration 18/25 | Loss: 0.00084513
Iteration 19/25 | Loss: 0.00084513
Iteration 20/25 | Loss: 0.00084513
Iteration 21/25 | Loss: 0.00084513
Iteration 22/25 | Loss: 0.00084513
Iteration 23/25 | Loss: 0.00084513
Iteration 24/25 | Loss: 0.00084513
Iteration 25/25 | Loss: 0.00084513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008451332105323672, 0.0008451332105323672, 0.0008451332105323672, 0.0008451332105323672, 0.0008451332105323672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008451332105323672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43774235
Iteration 2/25 | Loss: 0.00060482
Iteration 3/25 | Loss: 0.00060482
Iteration 4/25 | Loss: 0.00060482
Iteration 5/25 | Loss: 0.00060482
Iteration 6/25 | Loss: 0.00060481
Iteration 7/25 | Loss: 0.00060481
Iteration 8/25 | Loss: 0.00060481
Iteration 9/25 | Loss: 0.00060481
Iteration 10/25 | Loss: 0.00060481
Iteration 11/25 | Loss: 0.00060481
Iteration 12/25 | Loss: 0.00060481
Iteration 13/25 | Loss: 0.00060481
Iteration 14/25 | Loss: 0.00060481
Iteration 15/25 | Loss: 0.00060481
Iteration 16/25 | Loss: 0.00060481
Iteration 17/25 | Loss: 0.00060481
Iteration 18/25 | Loss: 0.00060481
Iteration 19/25 | Loss: 0.00060481
Iteration 20/25 | Loss: 0.00060481
Iteration 21/25 | Loss: 0.00060481
Iteration 22/25 | Loss: 0.00060481
Iteration 23/25 | Loss: 0.00060481
Iteration 24/25 | Loss: 0.00060481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006048135692253709, 0.0006048135692253709, 0.0006048135692253709, 0.0006048135692253709, 0.0006048135692253709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006048135692253709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060481
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001514
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001469
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001469
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001460
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001458
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001446
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001444
Iteration 82/1000 | Loss: 0.00001444
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001438
Iteration 129/1000 | Loss: 0.00001438
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001434
Iteration 151/1000 | Loss: 0.00001434
Iteration 152/1000 | Loss: 0.00001434
Iteration 153/1000 | Loss: 0.00001434
Iteration 154/1000 | Loss: 0.00001434
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001433
Iteration 157/1000 | Loss: 0.00001433
Iteration 158/1000 | Loss: 0.00001433
Iteration 159/1000 | Loss: 0.00001433
Iteration 160/1000 | Loss: 0.00001433
Iteration 161/1000 | Loss: 0.00001433
Iteration 162/1000 | Loss: 0.00001433
Iteration 163/1000 | Loss: 0.00001432
Iteration 164/1000 | Loss: 0.00001432
Iteration 165/1000 | Loss: 0.00001432
Iteration 166/1000 | Loss: 0.00001432
Iteration 167/1000 | Loss: 0.00001432
Iteration 168/1000 | Loss: 0.00001432
Iteration 169/1000 | Loss: 0.00001432
Iteration 170/1000 | Loss: 0.00001432
Iteration 171/1000 | Loss: 0.00001432
Iteration 172/1000 | Loss: 0.00001432
Iteration 173/1000 | Loss: 0.00001432
Iteration 174/1000 | Loss: 0.00001432
Iteration 175/1000 | Loss: 0.00001432
Iteration 176/1000 | Loss: 0.00001432
Iteration 177/1000 | Loss: 0.00001432
Iteration 178/1000 | Loss: 0.00001432
Iteration 179/1000 | Loss: 0.00001432
Iteration 180/1000 | Loss: 0.00001432
Iteration 181/1000 | Loss: 0.00001432
Iteration 182/1000 | Loss: 0.00001432
Iteration 183/1000 | Loss: 0.00001432
Iteration 184/1000 | Loss: 0.00001432
Iteration 185/1000 | Loss: 0.00001432
Iteration 186/1000 | Loss: 0.00001432
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001432
Iteration 191/1000 | Loss: 0.00001432
Iteration 192/1000 | Loss: 0.00001432
Iteration 193/1000 | Loss: 0.00001432
Iteration 194/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4315839507617056e-05, 1.4315839507617056e-05, 1.4315839507617056e-05, 1.4315839507617056e-05, 1.4315839507617056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4315839507617056e-05

Optimization complete. Final v2v error: 3.049522876739502 mm

Highest mean error: 3.990656614303589 mm for frame 222

Lowest mean error: 2.5781097412109375 mm for frame 157

Saving results

Total time: 45.019572257995605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478667
Iteration 2/25 | Loss: 0.00111537
Iteration 3/25 | Loss: 0.00092756
Iteration 4/25 | Loss: 0.00090511
Iteration 5/25 | Loss: 0.00089948
Iteration 6/25 | Loss: 0.00089850
Iteration 7/25 | Loss: 0.00089850
Iteration 8/25 | Loss: 0.00089850
Iteration 9/25 | Loss: 0.00089850
Iteration 10/25 | Loss: 0.00089850
Iteration 11/25 | Loss: 0.00089850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008985012536868453, 0.0008985012536868453, 0.0008985012536868453, 0.0008985012536868453, 0.0008985012536868453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008985012536868453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52696145
Iteration 2/25 | Loss: 0.00056960
Iteration 3/25 | Loss: 0.00056959
Iteration 4/25 | Loss: 0.00056959
Iteration 5/25 | Loss: 0.00056959
Iteration 6/25 | Loss: 0.00056959
Iteration 7/25 | Loss: 0.00056959
Iteration 8/25 | Loss: 0.00056959
Iteration 9/25 | Loss: 0.00056959
Iteration 10/25 | Loss: 0.00056959
Iteration 11/25 | Loss: 0.00056959
Iteration 12/25 | Loss: 0.00056959
Iteration 13/25 | Loss: 0.00056959
Iteration 14/25 | Loss: 0.00056959
Iteration 15/25 | Loss: 0.00056959
Iteration 16/25 | Loss: 0.00056959
Iteration 17/25 | Loss: 0.00056959
Iteration 18/25 | Loss: 0.00056959
Iteration 19/25 | Loss: 0.00056959
Iteration 20/25 | Loss: 0.00056959
Iteration 21/25 | Loss: 0.00056959
Iteration 22/25 | Loss: 0.00056959
Iteration 23/25 | Loss: 0.00056959
Iteration 24/25 | Loss: 0.00056959
Iteration 25/25 | Loss: 0.00056959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056959
Iteration 2/1000 | Loss: 0.00003744
Iteration 3/1000 | Loss: 0.00002788
Iteration 4/1000 | Loss: 0.00002619
Iteration 5/1000 | Loss: 0.00002492
Iteration 6/1000 | Loss: 0.00002421
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002317
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002265
Iteration 11/1000 | Loss: 0.00002256
Iteration 12/1000 | Loss: 0.00002234
Iteration 13/1000 | Loss: 0.00002223
Iteration 14/1000 | Loss: 0.00002222
Iteration 15/1000 | Loss: 0.00002221
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002219
Iteration 18/1000 | Loss: 0.00002219
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002218
Iteration 21/1000 | Loss: 0.00002217
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002215
Iteration 25/1000 | Loss: 0.00002213
Iteration 26/1000 | Loss: 0.00002213
Iteration 27/1000 | Loss: 0.00002210
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00002207
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002196
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002195
Iteration 34/1000 | Loss: 0.00002195
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002193
Iteration 39/1000 | Loss: 0.00002192
Iteration 40/1000 | Loss: 0.00002192
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002190
Iteration 47/1000 | Loss: 0.00002190
Iteration 48/1000 | Loss: 0.00002189
Iteration 49/1000 | Loss: 0.00002189
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002188
Iteration 53/1000 | Loss: 0.00002188
Iteration 54/1000 | Loss: 0.00002188
Iteration 55/1000 | Loss: 0.00002188
Iteration 56/1000 | Loss: 0.00002188
Iteration 57/1000 | Loss: 0.00002188
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002187
Iteration 62/1000 | Loss: 0.00002187
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002187
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.1871579519938678e-05, 2.1871579519938678e-05, 2.1871579519938678e-05, 2.1871579519938678e-05, 2.1871579519938678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1871579519938678e-05

Optimization complete. Final v2v error: 3.9340872764587402 mm

Highest mean error: 4.680060863494873 mm for frame 57

Lowest mean error: 3.4728145599365234 mm for frame 5

Saving results

Total time: 37.21327304840088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006922
Iteration 2/25 | Loss: 0.01006922
Iteration 3/25 | Loss: 0.01006921
Iteration 4/25 | Loss: 0.00313935
Iteration 5/25 | Loss: 0.00208305
Iteration 6/25 | Loss: 0.00194452
Iteration 7/25 | Loss: 0.00179327
Iteration 8/25 | Loss: 0.00180454
Iteration 9/25 | Loss: 0.00173189
Iteration 10/25 | Loss: 0.00157962
Iteration 11/25 | Loss: 0.00150616
Iteration 12/25 | Loss: 0.00145639
Iteration 13/25 | Loss: 0.00142179
Iteration 14/25 | Loss: 0.00139311
Iteration 15/25 | Loss: 0.00138179
Iteration 16/25 | Loss: 0.00137209
Iteration 17/25 | Loss: 0.00135703
Iteration 18/25 | Loss: 0.00132338
Iteration 19/25 | Loss: 0.00131846
Iteration 20/25 | Loss: 0.00131076
Iteration 21/25 | Loss: 0.00129961
Iteration 22/25 | Loss: 0.00129425
Iteration 23/25 | Loss: 0.00129166
Iteration 24/25 | Loss: 0.00128875
Iteration 25/25 | Loss: 0.00129306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56682134
Iteration 2/25 | Loss: 0.00553616
Iteration 3/25 | Loss: 0.00284909
Iteration 4/25 | Loss: 0.00284908
Iteration 5/25 | Loss: 0.00284908
Iteration 6/25 | Loss: 0.00284908
Iteration 7/25 | Loss: 0.00284908
Iteration 8/25 | Loss: 0.00284907
Iteration 9/25 | Loss: 0.00284907
Iteration 10/25 | Loss: 0.00284907
Iteration 11/25 | Loss: 0.00284907
Iteration 12/25 | Loss: 0.00284907
Iteration 13/25 | Loss: 0.00284907
Iteration 14/25 | Loss: 0.00284907
Iteration 15/25 | Loss: 0.00284907
Iteration 16/25 | Loss: 0.00284907
Iteration 17/25 | Loss: 0.00284907
Iteration 18/25 | Loss: 0.00284907
Iteration 19/25 | Loss: 0.00284907
Iteration 20/25 | Loss: 0.00284907
Iteration 21/25 | Loss: 0.00284907
Iteration 22/25 | Loss: 0.00284907
Iteration 23/25 | Loss: 0.00284907
Iteration 24/25 | Loss: 0.00284907
Iteration 25/25 | Loss: 0.00284907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284907
Iteration 2/1000 | Loss: 0.00383052
Iteration 3/1000 | Loss: 0.00348548
Iteration 4/1000 | Loss: 0.00215787
Iteration 5/1000 | Loss: 0.00145882
Iteration 6/1000 | Loss: 0.00158298
Iteration 7/1000 | Loss: 0.00122806
Iteration 8/1000 | Loss: 0.00176844
Iteration 9/1000 | Loss: 0.00126947
Iteration 10/1000 | Loss: 0.00069918
Iteration 11/1000 | Loss: 0.00067702
Iteration 12/1000 | Loss: 0.00033477
Iteration 13/1000 | Loss: 0.00050074
Iteration 14/1000 | Loss: 0.00062744
Iteration 15/1000 | Loss: 0.00090490
Iteration 16/1000 | Loss: 0.00232332
Iteration 17/1000 | Loss: 0.00049220
Iteration 18/1000 | Loss: 0.00030786
Iteration 19/1000 | Loss: 0.00055028
Iteration 20/1000 | Loss: 0.00151425
Iteration 21/1000 | Loss: 0.00160109
Iteration 22/1000 | Loss: 0.00090692
Iteration 23/1000 | Loss: 0.00064608
Iteration 24/1000 | Loss: 0.00028733
Iteration 25/1000 | Loss: 0.00109087
Iteration 26/1000 | Loss: 0.00061332
Iteration 27/1000 | Loss: 0.00071039
Iteration 28/1000 | Loss: 0.00033028
Iteration 29/1000 | Loss: 0.00090072
Iteration 30/1000 | Loss: 0.00068484
Iteration 31/1000 | Loss: 0.00173398
Iteration 32/1000 | Loss: 0.00045143
Iteration 33/1000 | Loss: 0.00096620
Iteration 34/1000 | Loss: 0.00074156
Iteration 35/1000 | Loss: 0.00045204
Iteration 36/1000 | Loss: 0.00058001
Iteration 37/1000 | Loss: 0.00032163
Iteration 38/1000 | Loss: 0.00021634
Iteration 39/1000 | Loss: 0.00035444
Iteration 40/1000 | Loss: 0.00052455
Iteration 41/1000 | Loss: 0.00021526
Iteration 42/1000 | Loss: 0.00031249
Iteration 43/1000 | Loss: 0.00036606
Iteration 44/1000 | Loss: 0.00020839
Iteration 45/1000 | Loss: 0.00021138
Iteration 46/1000 | Loss: 0.00039723
Iteration 47/1000 | Loss: 0.00069784
Iteration 48/1000 | Loss: 0.00062335
Iteration 49/1000 | Loss: 0.00071878
Iteration 50/1000 | Loss: 0.00049997
Iteration 51/1000 | Loss: 0.00022032
Iteration 52/1000 | Loss: 0.00042456
Iteration 53/1000 | Loss: 0.00106005
Iteration 54/1000 | Loss: 0.00074508
Iteration 55/1000 | Loss: 0.00094767
Iteration 56/1000 | Loss: 0.00060468
Iteration 57/1000 | Loss: 0.00113019
Iteration 58/1000 | Loss: 0.00025255
Iteration 59/1000 | Loss: 0.00060397
Iteration 60/1000 | Loss: 0.00043853
Iteration 61/1000 | Loss: 0.00049453
Iteration 62/1000 | Loss: 0.00043171
Iteration 63/1000 | Loss: 0.00040013
Iteration 64/1000 | Loss: 0.00062184
Iteration 65/1000 | Loss: 0.00118255
Iteration 66/1000 | Loss: 0.00122667
Iteration 67/1000 | Loss: 0.00027187
Iteration 68/1000 | Loss: 0.00027262
Iteration 69/1000 | Loss: 0.00090844
Iteration 70/1000 | Loss: 0.00051385
Iteration 71/1000 | Loss: 0.00018492
Iteration 72/1000 | Loss: 0.00043154
Iteration 73/1000 | Loss: 0.00048969
Iteration 74/1000 | Loss: 0.00044634
Iteration 75/1000 | Loss: 0.00021916
Iteration 76/1000 | Loss: 0.00018457
Iteration 77/1000 | Loss: 0.00018906
Iteration 78/1000 | Loss: 0.00030258
Iteration 79/1000 | Loss: 0.00022758
Iteration 80/1000 | Loss: 0.00049527
Iteration 81/1000 | Loss: 0.00041435
Iteration 82/1000 | Loss: 0.00020049
Iteration 83/1000 | Loss: 0.00017647
Iteration 84/1000 | Loss: 0.00044573
Iteration 85/1000 | Loss: 0.00060396
Iteration 86/1000 | Loss: 0.00062379
Iteration 87/1000 | Loss: 0.00038155
Iteration 88/1000 | Loss: 0.00019571
Iteration 89/1000 | Loss: 0.00022467
Iteration 90/1000 | Loss: 0.00017336
Iteration 91/1000 | Loss: 0.00018385
Iteration 92/1000 | Loss: 0.00046538
Iteration 93/1000 | Loss: 0.00021539
Iteration 94/1000 | Loss: 0.00017801
Iteration 95/1000 | Loss: 0.00016075
Iteration 96/1000 | Loss: 0.00024113
Iteration 97/1000 | Loss: 0.00015828
Iteration 98/1000 | Loss: 0.00019563
Iteration 99/1000 | Loss: 0.00016457
Iteration 100/1000 | Loss: 0.00016003
Iteration 101/1000 | Loss: 0.00016647
Iteration 102/1000 | Loss: 0.00015800
Iteration 103/1000 | Loss: 0.00015942
Iteration 104/1000 | Loss: 0.00035822
Iteration 105/1000 | Loss: 0.00016442
Iteration 106/1000 | Loss: 0.00052630
Iteration 107/1000 | Loss: 0.00050559
Iteration 108/1000 | Loss: 0.00142560
Iteration 109/1000 | Loss: 0.00020755
Iteration 110/1000 | Loss: 0.00017713
Iteration 111/1000 | Loss: 0.00015913
Iteration 112/1000 | Loss: 0.00015798
Iteration 113/1000 | Loss: 0.00015221
Iteration 114/1000 | Loss: 0.00016128
Iteration 115/1000 | Loss: 0.00016297
Iteration 116/1000 | Loss: 0.00015935
Iteration 117/1000 | Loss: 0.00016092
Iteration 118/1000 | Loss: 0.00016135
Iteration 119/1000 | Loss: 0.00016139
Iteration 120/1000 | Loss: 0.00015756
Iteration 121/1000 | Loss: 0.00015391
Iteration 122/1000 | Loss: 0.00028604
Iteration 123/1000 | Loss: 0.00016511
Iteration 124/1000 | Loss: 0.00021178
Iteration 125/1000 | Loss: 0.00015557
Iteration 126/1000 | Loss: 0.00017555
Iteration 127/1000 | Loss: 0.00016604
Iteration 128/1000 | Loss: 0.00017567
Iteration 129/1000 | Loss: 0.00018384
Iteration 130/1000 | Loss: 0.00017049
Iteration 131/1000 | Loss: 0.00017225
Iteration 132/1000 | Loss: 0.00018933
Iteration 133/1000 | Loss: 0.00016987
Iteration 134/1000 | Loss: 0.00018268
Iteration 135/1000 | Loss: 0.00016978
Iteration 136/1000 | Loss: 0.00016752
Iteration 137/1000 | Loss: 0.00015342
Iteration 138/1000 | Loss: 0.00014992
Iteration 139/1000 | Loss: 0.00014852
Iteration 140/1000 | Loss: 0.00024071
Iteration 141/1000 | Loss: 0.00020763
Iteration 142/1000 | Loss: 0.00014782
Iteration 143/1000 | Loss: 0.00014729
Iteration 144/1000 | Loss: 0.00024827
Iteration 145/1000 | Loss: 0.00018545
Iteration 146/1000 | Loss: 0.00014726
Iteration 147/1000 | Loss: 0.00028747
Iteration 148/1000 | Loss: 0.00015786
Iteration 149/1000 | Loss: 0.00014714
Iteration 150/1000 | Loss: 0.00054377
Iteration 151/1000 | Loss: 0.00171236
Iteration 152/1000 | Loss: 0.00153409
Iteration 153/1000 | Loss: 0.00029616
Iteration 154/1000 | Loss: 0.00016686
Iteration 155/1000 | Loss: 0.00032460
Iteration 156/1000 | Loss: 0.00022228
Iteration 157/1000 | Loss: 0.00014889
Iteration 158/1000 | Loss: 0.00026102
Iteration 159/1000 | Loss: 0.00054853
Iteration 160/1000 | Loss: 0.00044152
Iteration 161/1000 | Loss: 0.00021143
Iteration 162/1000 | Loss: 0.00024773
Iteration 163/1000 | Loss: 0.00014329
Iteration 164/1000 | Loss: 0.00014462
Iteration 165/1000 | Loss: 0.00015258
Iteration 166/1000 | Loss: 0.00035757
Iteration 167/1000 | Loss: 0.00018937
Iteration 168/1000 | Loss: 0.00017389
Iteration 169/1000 | Loss: 0.00052524
Iteration 170/1000 | Loss: 0.00018739
Iteration 171/1000 | Loss: 0.00015634
Iteration 172/1000 | Loss: 0.00015540
Iteration 173/1000 | Loss: 0.00031203
Iteration 174/1000 | Loss: 0.00051470
Iteration 175/1000 | Loss: 0.00016863
Iteration 176/1000 | Loss: 0.00014895
Iteration 177/1000 | Loss: 0.00015033
Iteration 178/1000 | Loss: 0.00029532
Iteration 179/1000 | Loss: 0.00018898
Iteration 180/1000 | Loss: 0.00015487
Iteration 181/1000 | Loss: 0.00015247
Iteration 182/1000 | Loss: 0.00014806
Iteration 183/1000 | Loss: 0.00015062
Iteration 184/1000 | Loss: 0.00015368
Iteration 185/1000 | Loss: 0.00014866
Iteration 186/1000 | Loss: 0.00022250
Iteration 187/1000 | Loss: 0.00021592
Iteration 188/1000 | Loss: 0.00015184
Iteration 189/1000 | Loss: 0.00023966
Iteration 190/1000 | Loss: 0.00017771
Iteration 191/1000 | Loss: 0.00029422
Iteration 192/1000 | Loss: 0.00016822
Iteration 193/1000 | Loss: 0.00018537
Iteration 194/1000 | Loss: 0.00015440
Iteration 195/1000 | Loss: 0.00019367
Iteration 196/1000 | Loss: 0.00016521
Iteration 197/1000 | Loss: 0.00040290
Iteration 198/1000 | Loss: 0.00016081
Iteration 199/1000 | Loss: 0.00014907
Iteration 200/1000 | Loss: 0.00017938
Iteration 201/1000 | Loss: 0.00015546
Iteration 202/1000 | Loss: 0.00018106
Iteration 203/1000 | Loss: 0.00014269
Iteration 204/1000 | Loss: 0.00014869
Iteration 205/1000 | Loss: 0.00015359
Iteration 206/1000 | Loss: 0.00017772
Iteration 207/1000 | Loss: 0.00014738
Iteration 208/1000 | Loss: 0.00015447
Iteration 209/1000 | Loss: 0.00014669
Iteration 210/1000 | Loss: 0.00023801
Iteration 211/1000 | Loss: 0.00015120
Iteration 212/1000 | Loss: 0.00014400
Iteration 213/1000 | Loss: 0.00015034
Iteration 214/1000 | Loss: 0.00015720
Iteration 215/1000 | Loss: 0.00014805
Iteration 216/1000 | Loss: 0.00015383
Iteration 217/1000 | Loss: 0.00014795
Iteration 218/1000 | Loss: 0.00016103
Iteration 219/1000 | Loss: 0.00093703
Iteration 220/1000 | Loss: 0.00042208
Iteration 221/1000 | Loss: 0.00022331
Iteration 222/1000 | Loss: 0.00014254
Iteration 223/1000 | Loss: 0.00014019
Iteration 224/1000 | Loss: 0.00013890
Iteration 225/1000 | Loss: 0.00037883
Iteration 226/1000 | Loss: 0.00020027
Iteration 227/1000 | Loss: 0.00028892
Iteration 228/1000 | Loss: 0.00075306
Iteration 229/1000 | Loss: 0.00016942
Iteration 230/1000 | Loss: 0.00013771
Iteration 231/1000 | Loss: 0.00013657
Iteration 232/1000 | Loss: 0.00056238
Iteration 233/1000 | Loss: 0.00013907
Iteration 234/1000 | Loss: 0.00013628
Iteration 235/1000 | Loss: 0.00013503
Iteration 236/1000 | Loss: 0.00013417
Iteration 237/1000 | Loss: 0.00041071
Iteration 238/1000 | Loss: 0.00020592
Iteration 239/1000 | Loss: 0.00013742
Iteration 240/1000 | Loss: 0.00027288
Iteration 241/1000 | Loss: 0.00017593
Iteration 242/1000 | Loss: 0.00013372
Iteration 243/1000 | Loss: 0.00013300
Iteration 244/1000 | Loss: 0.00013288
Iteration 245/1000 | Loss: 0.00013287
Iteration 246/1000 | Loss: 0.00028033
Iteration 247/1000 | Loss: 0.00013357
Iteration 248/1000 | Loss: 0.00013278
Iteration 249/1000 | Loss: 0.00014254
Iteration 250/1000 | Loss: 0.00014254
Iteration 251/1000 | Loss: 0.00044891
Iteration 252/1000 | Loss: 0.00018773
Iteration 253/1000 | Loss: 0.00013260
Iteration 254/1000 | Loss: 0.00013248
Iteration 255/1000 | Loss: 0.00013247
Iteration 256/1000 | Loss: 0.00013244
Iteration 257/1000 | Loss: 0.00013243
Iteration 258/1000 | Loss: 0.00013243
Iteration 259/1000 | Loss: 0.00013243
Iteration 260/1000 | Loss: 0.00013243
Iteration 261/1000 | Loss: 0.00013243
Iteration 262/1000 | Loss: 0.00013243
Iteration 263/1000 | Loss: 0.00013243
Iteration 264/1000 | Loss: 0.00013243
Iteration 265/1000 | Loss: 0.00013243
Iteration 266/1000 | Loss: 0.00013243
Iteration 267/1000 | Loss: 0.00013243
Iteration 268/1000 | Loss: 0.00013243
Iteration 269/1000 | Loss: 0.00013242
Iteration 270/1000 | Loss: 0.00013242
Iteration 271/1000 | Loss: 0.00013242
Iteration 272/1000 | Loss: 0.00013242
Iteration 273/1000 | Loss: 0.00013242
Iteration 274/1000 | Loss: 0.00013240
Iteration 275/1000 | Loss: 0.00013240
Iteration 276/1000 | Loss: 0.00013240
Iteration 277/1000 | Loss: 0.00013239
Iteration 278/1000 | Loss: 0.00013239
Iteration 279/1000 | Loss: 0.00013239
Iteration 280/1000 | Loss: 0.00013239
Iteration 281/1000 | Loss: 0.00013238
Iteration 282/1000 | Loss: 0.00013238
Iteration 283/1000 | Loss: 0.00013238
Iteration 284/1000 | Loss: 0.00013237
Iteration 285/1000 | Loss: 0.00013237
Iteration 286/1000 | Loss: 0.00013237
Iteration 287/1000 | Loss: 0.00013237
Iteration 288/1000 | Loss: 0.00013236
Iteration 289/1000 | Loss: 0.00013236
Iteration 290/1000 | Loss: 0.00013236
Iteration 291/1000 | Loss: 0.00013236
Iteration 292/1000 | Loss: 0.00013236
Iteration 293/1000 | Loss: 0.00013236
Iteration 294/1000 | Loss: 0.00013235
Iteration 295/1000 | Loss: 0.00013235
Iteration 296/1000 | Loss: 0.00013235
Iteration 297/1000 | Loss: 0.00013235
Iteration 298/1000 | Loss: 0.00013235
Iteration 299/1000 | Loss: 0.00013235
Iteration 300/1000 | Loss: 0.00013235
Iteration 301/1000 | Loss: 0.00013235
Iteration 302/1000 | Loss: 0.00013235
Iteration 303/1000 | Loss: 0.00013234
Iteration 304/1000 | Loss: 0.00013234
Iteration 305/1000 | Loss: 0.00013234
Iteration 306/1000 | Loss: 0.00013234
Iteration 307/1000 | Loss: 0.00013234
Iteration 308/1000 | Loss: 0.00013234
Iteration 309/1000 | Loss: 0.00013234
Iteration 310/1000 | Loss: 0.00013234
Iteration 311/1000 | Loss: 0.00013233
Iteration 312/1000 | Loss: 0.00013233
Iteration 313/1000 | Loss: 0.00013233
Iteration 314/1000 | Loss: 0.00013233
Iteration 315/1000 | Loss: 0.00013233
Iteration 316/1000 | Loss: 0.00013233
Iteration 317/1000 | Loss: 0.00013233
Iteration 318/1000 | Loss: 0.00013233
Iteration 319/1000 | Loss: 0.00013233
Iteration 320/1000 | Loss: 0.00013233
Iteration 321/1000 | Loss: 0.00013233
Iteration 322/1000 | Loss: 0.00013233
Iteration 323/1000 | Loss: 0.00013233
Iteration 324/1000 | Loss: 0.00013233
Iteration 325/1000 | Loss: 0.00013233
Iteration 326/1000 | Loss: 0.00013233
Iteration 327/1000 | Loss: 0.00013233
Iteration 328/1000 | Loss: 0.00013233
Iteration 329/1000 | Loss: 0.00013233
Iteration 330/1000 | Loss: 0.00013233
Iteration 331/1000 | Loss: 0.00013233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [0.00013233213394414634, 0.00013233213394414634, 0.00013233213394414634, 0.00013233213394414634, 0.00013233213394414634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013233213394414634

Optimization complete. Final v2v error: 6.38934326171875 mm

Highest mean error: 12.406667709350586 mm for frame 196

Lowest mean error: 3.6436829566955566 mm for frame 103

Saving results

Total time: 459.94696402549744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546566
Iteration 2/25 | Loss: 0.00116371
Iteration 3/25 | Loss: 0.00090319
Iteration 4/25 | Loss: 0.00088256
Iteration 5/25 | Loss: 0.00087656
Iteration 6/25 | Loss: 0.00087489
Iteration 7/25 | Loss: 0.00087448
Iteration 8/25 | Loss: 0.00087448
Iteration 9/25 | Loss: 0.00087448
Iteration 10/25 | Loss: 0.00087448
Iteration 11/25 | Loss: 0.00087448
Iteration 12/25 | Loss: 0.00087448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008744841325096786, 0.0008744841325096786, 0.0008744841325096786, 0.0008744841325096786, 0.0008744841325096786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008744841325096786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80058378
Iteration 2/25 | Loss: 0.00045745
Iteration 3/25 | Loss: 0.00045745
Iteration 4/25 | Loss: 0.00045745
Iteration 5/25 | Loss: 0.00045745
Iteration 6/25 | Loss: 0.00045744
Iteration 7/25 | Loss: 0.00045744
Iteration 8/25 | Loss: 0.00045744
Iteration 9/25 | Loss: 0.00045744
Iteration 10/25 | Loss: 0.00045744
Iteration 11/25 | Loss: 0.00045744
Iteration 12/25 | Loss: 0.00045744
Iteration 13/25 | Loss: 0.00045744
Iteration 14/25 | Loss: 0.00045744
Iteration 15/25 | Loss: 0.00045744
Iteration 16/25 | Loss: 0.00045744
Iteration 17/25 | Loss: 0.00045744
Iteration 18/25 | Loss: 0.00045744
Iteration 19/25 | Loss: 0.00045744
Iteration 20/25 | Loss: 0.00045744
Iteration 21/25 | Loss: 0.00045744
Iteration 22/25 | Loss: 0.00045744
Iteration 23/25 | Loss: 0.00045744
Iteration 24/25 | Loss: 0.00045744
Iteration 25/25 | Loss: 0.00045744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045744
Iteration 2/1000 | Loss: 0.00003740
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001878
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001696
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001633
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001548
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001531
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001531
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001530
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001529
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001520
Iteration 116/1000 | Loss: 0.00001520
Iteration 117/1000 | Loss: 0.00001520
Iteration 118/1000 | Loss: 0.00001520
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001519
Iteration 123/1000 | Loss: 0.00001519
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001516
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001516
Iteration 172/1000 | Loss: 0.00001516
Iteration 173/1000 | Loss: 0.00001516
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001516
Iteration 176/1000 | Loss: 0.00001516
Iteration 177/1000 | Loss: 0.00001516
Iteration 178/1000 | Loss: 0.00001516
Iteration 179/1000 | Loss: 0.00001516
Iteration 180/1000 | Loss: 0.00001516
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001516
Iteration 183/1000 | Loss: 0.00001516
Iteration 184/1000 | Loss: 0.00001516
Iteration 185/1000 | Loss: 0.00001516
Iteration 186/1000 | Loss: 0.00001516
Iteration 187/1000 | Loss: 0.00001516
Iteration 188/1000 | Loss: 0.00001516
Iteration 189/1000 | Loss: 0.00001516
Iteration 190/1000 | Loss: 0.00001516
Iteration 191/1000 | Loss: 0.00001516
Iteration 192/1000 | Loss: 0.00001516
Iteration 193/1000 | Loss: 0.00001516
Iteration 194/1000 | Loss: 0.00001516
Iteration 195/1000 | Loss: 0.00001516
Iteration 196/1000 | Loss: 0.00001516
Iteration 197/1000 | Loss: 0.00001516
Iteration 198/1000 | Loss: 0.00001516
Iteration 199/1000 | Loss: 0.00001516
Iteration 200/1000 | Loss: 0.00001516
Iteration 201/1000 | Loss: 0.00001516
Iteration 202/1000 | Loss: 0.00001516
Iteration 203/1000 | Loss: 0.00001516
Iteration 204/1000 | Loss: 0.00001516
Iteration 205/1000 | Loss: 0.00001516
Iteration 206/1000 | Loss: 0.00001516
Iteration 207/1000 | Loss: 0.00001516
Iteration 208/1000 | Loss: 0.00001516
Iteration 209/1000 | Loss: 0.00001516
Iteration 210/1000 | Loss: 0.00001516
Iteration 211/1000 | Loss: 0.00001516
Iteration 212/1000 | Loss: 0.00001516
Iteration 213/1000 | Loss: 0.00001516
Iteration 214/1000 | Loss: 0.00001516
Iteration 215/1000 | Loss: 0.00001516
Iteration 216/1000 | Loss: 0.00001516
Iteration 217/1000 | Loss: 0.00001516
Iteration 218/1000 | Loss: 0.00001516
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001516
Iteration 221/1000 | Loss: 0.00001516
Iteration 222/1000 | Loss: 0.00001516
Iteration 223/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.5158625501499046e-05, 1.5158625501499046e-05, 1.5158625501499046e-05, 1.5158625501499046e-05, 1.5158625501499046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5158625501499046e-05

Optimization complete. Final v2v error: 3.2581474781036377 mm

Highest mean error: 4.074258804321289 mm for frame 8

Lowest mean error: 2.945809841156006 mm for frame 48

Saving results

Total time: 44.54463791847229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053188
Iteration 2/25 | Loss: 0.00289716
Iteration 3/25 | Loss: 0.00161340
Iteration 4/25 | Loss: 0.00139153
Iteration 5/25 | Loss: 0.00127612
Iteration 6/25 | Loss: 0.00122976
Iteration 7/25 | Loss: 0.00118040
Iteration 8/25 | Loss: 0.00113717
Iteration 9/25 | Loss: 0.00110917
Iteration 10/25 | Loss: 0.00105639
Iteration 11/25 | Loss: 0.00102762
Iteration 12/25 | Loss: 0.00100221
Iteration 13/25 | Loss: 0.00098859
Iteration 14/25 | Loss: 0.00097726
Iteration 15/25 | Loss: 0.00096583
Iteration 16/25 | Loss: 0.00095696
Iteration 17/25 | Loss: 0.00096631
Iteration 18/25 | Loss: 0.00098208
Iteration 19/25 | Loss: 0.00098102
Iteration 20/25 | Loss: 0.00098370
Iteration 21/25 | Loss: 0.00097382
Iteration 22/25 | Loss: 0.00096998
Iteration 23/25 | Loss: 0.00097682
Iteration 24/25 | Loss: 0.00096933
Iteration 25/25 | Loss: 0.00097551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58394241
Iteration 2/25 | Loss: 0.00256820
Iteration 3/25 | Loss: 0.00205269
Iteration 4/25 | Loss: 0.00205269
Iteration 5/25 | Loss: 0.00205269
Iteration 6/25 | Loss: 0.00205269
Iteration 7/25 | Loss: 0.00205269
Iteration 8/25 | Loss: 0.00205269
Iteration 9/25 | Loss: 0.00205269
Iteration 10/25 | Loss: 0.00205269
Iteration 11/25 | Loss: 0.00205269
Iteration 12/25 | Loss: 0.00205269
Iteration 13/25 | Loss: 0.00205269
Iteration 14/25 | Loss: 0.00205269
Iteration 15/25 | Loss: 0.00205269
Iteration 16/25 | Loss: 0.00205269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020526873413473368, 0.0020526873413473368, 0.0020526873413473368, 0.0020526873413473368, 0.0020526873413473368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020526873413473368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205269
Iteration 2/1000 | Loss: 0.00249461
Iteration 3/1000 | Loss: 0.00204458
Iteration 4/1000 | Loss: 0.00271379
Iteration 5/1000 | Loss: 0.00175159
Iteration 6/1000 | Loss: 0.00149447
Iteration 7/1000 | Loss: 0.00137028
Iteration 8/1000 | Loss: 0.00128837
Iteration 9/1000 | Loss: 0.00130670
Iteration 10/1000 | Loss: 0.00152484
Iteration 11/1000 | Loss: 0.00129000
Iteration 12/1000 | Loss: 0.00219135
Iteration 13/1000 | Loss: 0.00175463
Iteration 14/1000 | Loss: 0.00146684
Iteration 15/1000 | Loss: 0.00366328
Iteration 16/1000 | Loss: 0.00442340
Iteration 17/1000 | Loss: 0.00371628
Iteration 18/1000 | Loss: 0.00169108
Iteration 19/1000 | Loss: 0.00134127
Iteration 20/1000 | Loss: 0.00140506
Iteration 21/1000 | Loss: 0.00193563
Iteration 22/1000 | Loss: 0.00125887
Iteration 23/1000 | Loss: 0.00133830
Iteration 24/1000 | Loss: 0.00173328
Iteration 25/1000 | Loss: 0.00141747
Iteration 26/1000 | Loss: 0.00146753
Iteration 27/1000 | Loss: 0.00162295
Iteration 28/1000 | Loss: 0.00197262
Iteration 29/1000 | Loss: 0.00134135
Iteration 30/1000 | Loss: 0.00120138
Iteration 31/1000 | Loss: 0.00108758
Iteration 32/1000 | Loss: 0.00122758
Iteration 33/1000 | Loss: 0.00106959
Iteration 34/1000 | Loss: 0.00084401
Iteration 35/1000 | Loss: 0.00112561
Iteration 36/1000 | Loss: 0.00131976
Iteration 37/1000 | Loss: 0.00148554
Iteration 38/1000 | Loss: 0.00060834
Iteration 39/1000 | Loss: 0.00071024
Iteration 40/1000 | Loss: 0.00094741
Iteration 41/1000 | Loss: 0.00058446
Iteration 42/1000 | Loss: 0.00050825
Iteration 43/1000 | Loss: 0.00058855
Iteration 44/1000 | Loss: 0.00090078
Iteration 45/1000 | Loss: 0.00062307
Iteration 46/1000 | Loss: 0.00076082
Iteration 47/1000 | Loss: 0.00060322
Iteration 48/1000 | Loss: 0.00101018
Iteration 49/1000 | Loss: 0.00074693
Iteration 50/1000 | Loss: 0.00033764
Iteration 51/1000 | Loss: 0.00033004
Iteration 52/1000 | Loss: 0.00027411
Iteration 53/1000 | Loss: 0.00111054
Iteration 54/1000 | Loss: 0.00122959
Iteration 55/1000 | Loss: 0.00059317
Iteration 56/1000 | Loss: 0.00048725
Iteration 57/1000 | Loss: 0.00067914
Iteration 58/1000 | Loss: 0.00059966
Iteration 59/1000 | Loss: 0.00070173
Iteration 60/1000 | Loss: 0.00058981
Iteration 61/1000 | Loss: 0.00125383
Iteration 62/1000 | Loss: 0.00081289
Iteration 63/1000 | Loss: 0.00089386
Iteration 64/1000 | Loss: 0.00063488
Iteration 65/1000 | Loss: 0.00091496
Iteration 66/1000 | Loss: 0.00060609
Iteration 67/1000 | Loss: 0.00147664
Iteration 68/1000 | Loss: 0.00060108
Iteration 69/1000 | Loss: 0.00086835
Iteration 70/1000 | Loss: 0.00137957
Iteration 71/1000 | Loss: 0.00106196
Iteration 72/1000 | Loss: 0.00083281
Iteration 73/1000 | Loss: 0.00086976
Iteration 74/1000 | Loss: 0.00071884
Iteration 75/1000 | Loss: 0.00058557
Iteration 76/1000 | Loss: 0.00052608
Iteration 77/1000 | Loss: 0.00052085
Iteration 78/1000 | Loss: 0.00063915
Iteration 79/1000 | Loss: 0.00228390
Iteration 80/1000 | Loss: 0.00075229
Iteration 81/1000 | Loss: 0.00086167
Iteration 82/1000 | Loss: 0.00045336
Iteration 83/1000 | Loss: 0.00023372
Iteration 84/1000 | Loss: 0.00047097
Iteration 85/1000 | Loss: 0.00055666
Iteration 86/1000 | Loss: 0.00064313
Iteration 87/1000 | Loss: 0.00083140
Iteration 88/1000 | Loss: 0.00129039
Iteration 89/1000 | Loss: 0.00046154
Iteration 90/1000 | Loss: 0.00042711
Iteration 91/1000 | Loss: 0.00031871
Iteration 92/1000 | Loss: 0.00042173
Iteration 93/1000 | Loss: 0.00085329
Iteration 94/1000 | Loss: 0.00076173
Iteration 95/1000 | Loss: 0.00054443
Iteration 96/1000 | Loss: 0.00136510
Iteration 97/1000 | Loss: 0.00104941
Iteration 98/1000 | Loss: 0.00062876
Iteration 99/1000 | Loss: 0.00064208
Iteration 100/1000 | Loss: 0.00073030
Iteration 101/1000 | Loss: 0.00079908
Iteration 102/1000 | Loss: 0.00064192
Iteration 103/1000 | Loss: 0.00056347
Iteration 104/1000 | Loss: 0.00033829
Iteration 105/1000 | Loss: 0.00024217
Iteration 106/1000 | Loss: 0.00033644
Iteration 107/1000 | Loss: 0.00086001
Iteration 108/1000 | Loss: 0.00061731
Iteration 109/1000 | Loss: 0.00008913
Iteration 110/1000 | Loss: 0.00018549
Iteration 111/1000 | Loss: 0.00007311
Iteration 112/1000 | Loss: 0.00051325
Iteration 113/1000 | Loss: 0.00042341
Iteration 114/1000 | Loss: 0.00019078
Iteration 115/1000 | Loss: 0.00066719
Iteration 116/1000 | Loss: 0.00085403
Iteration 117/1000 | Loss: 0.00087669
Iteration 118/1000 | Loss: 0.00095375
Iteration 119/1000 | Loss: 0.00085879
Iteration 120/1000 | Loss: 0.00056819
Iteration 121/1000 | Loss: 0.00113845
Iteration 122/1000 | Loss: 0.00164316
Iteration 123/1000 | Loss: 0.00065291
Iteration 124/1000 | Loss: 0.00032186
Iteration 125/1000 | Loss: 0.00025303
Iteration 126/1000 | Loss: 0.00102910
Iteration 127/1000 | Loss: 0.00069936
Iteration 128/1000 | Loss: 0.00040508
Iteration 129/1000 | Loss: 0.00013580
Iteration 130/1000 | Loss: 0.00031684
Iteration 131/1000 | Loss: 0.00031496
Iteration 132/1000 | Loss: 0.00029000
Iteration 133/1000 | Loss: 0.00027418
Iteration 134/1000 | Loss: 0.00018404
Iteration 135/1000 | Loss: 0.00030015
Iteration 136/1000 | Loss: 0.00047250
Iteration 137/1000 | Loss: 0.00039296
Iteration 138/1000 | Loss: 0.00049654
Iteration 139/1000 | Loss: 0.00044212
Iteration 140/1000 | Loss: 0.00015350
Iteration 141/1000 | Loss: 0.00012016
Iteration 142/1000 | Loss: 0.00049552
Iteration 143/1000 | Loss: 0.00020548
Iteration 144/1000 | Loss: 0.00005840
Iteration 145/1000 | Loss: 0.00057937
Iteration 146/1000 | Loss: 0.00102684
Iteration 147/1000 | Loss: 0.00046525
Iteration 148/1000 | Loss: 0.00040050
Iteration 149/1000 | Loss: 0.00029655
Iteration 150/1000 | Loss: 0.00020104
Iteration 151/1000 | Loss: 0.00044688
Iteration 152/1000 | Loss: 0.00027744
Iteration 153/1000 | Loss: 0.00055852
Iteration 154/1000 | Loss: 0.00054621
Iteration 155/1000 | Loss: 0.00033847
Iteration 156/1000 | Loss: 0.00014549
Iteration 157/1000 | Loss: 0.00078687
Iteration 158/1000 | Loss: 0.00030167
Iteration 159/1000 | Loss: 0.00026639
Iteration 160/1000 | Loss: 0.00018286
Iteration 161/1000 | Loss: 0.00058660
Iteration 162/1000 | Loss: 0.00043149
Iteration 163/1000 | Loss: 0.00021605
Iteration 164/1000 | Loss: 0.00024600
Iteration 165/1000 | Loss: 0.00021677
Iteration 166/1000 | Loss: 0.00022590
Iteration 167/1000 | Loss: 0.00024476
Iteration 168/1000 | Loss: 0.00027584
Iteration 169/1000 | Loss: 0.00030052
Iteration 170/1000 | Loss: 0.00014432
Iteration 171/1000 | Loss: 0.00014654
Iteration 172/1000 | Loss: 0.00012590
Iteration 173/1000 | Loss: 0.00028863
Iteration 174/1000 | Loss: 0.00030287
Iteration 175/1000 | Loss: 0.00030870
Iteration 176/1000 | Loss: 0.00069617
Iteration 177/1000 | Loss: 0.00151404
Iteration 178/1000 | Loss: 0.00072469
Iteration 179/1000 | Loss: 0.00096069
Iteration 180/1000 | Loss: 0.00054162
Iteration 181/1000 | Loss: 0.00048890
Iteration 182/1000 | Loss: 0.00038489
Iteration 183/1000 | Loss: 0.00055585
Iteration 184/1000 | Loss: 0.00084769
Iteration 185/1000 | Loss: 0.00043546
Iteration 186/1000 | Loss: 0.00053291
Iteration 187/1000 | Loss: 0.00024261
Iteration 188/1000 | Loss: 0.00041932
Iteration 189/1000 | Loss: 0.00028024
Iteration 190/1000 | Loss: 0.00015698
Iteration 191/1000 | Loss: 0.00018429
Iteration 192/1000 | Loss: 0.00042784
Iteration 193/1000 | Loss: 0.00030265
Iteration 194/1000 | Loss: 0.00078451
Iteration 195/1000 | Loss: 0.00068246
Iteration 196/1000 | Loss: 0.00019597
Iteration 197/1000 | Loss: 0.00084939
Iteration 198/1000 | Loss: 0.00051696
Iteration 199/1000 | Loss: 0.00041460
Iteration 200/1000 | Loss: 0.00041860
Iteration 201/1000 | Loss: 0.00098693
Iteration 202/1000 | Loss: 0.00190030
Iteration 203/1000 | Loss: 0.00099161
Iteration 204/1000 | Loss: 0.00064036
Iteration 205/1000 | Loss: 0.00344591
Iteration 206/1000 | Loss: 0.00061892
Iteration 207/1000 | Loss: 0.00027857
Iteration 208/1000 | Loss: 0.00050784
Iteration 209/1000 | Loss: 0.00037659
Iteration 210/1000 | Loss: 0.00080055
Iteration 211/1000 | Loss: 0.00058504
Iteration 212/1000 | Loss: 0.00072508
Iteration 213/1000 | Loss: 0.00155679
Iteration 214/1000 | Loss: 0.00053710
Iteration 215/1000 | Loss: 0.00071126
Iteration 216/1000 | Loss: 0.00045753
Iteration 217/1000 | Loss: 0.00067397
Iteration 218/1000 | Loss: 0.00020226
Iteration 219/1000 | Loss: 0.00040773
Iteration 220/1000 | Loss: 0.00095215
Iteration 221/1000 | Loss: 0.00086149
Iteration 222/1000 | Loss: 0.00041323
Iteration 223/1000 | Loss: 0.00018688
Iteration 224/1000 | Loss: 0.00012843
Iteration 225/1000 | Loss: 0.00034662
Iteration 226/1000 | Loss: 0.00018677
Iteration 227/1000 | Loss: 0.00019052
Iteration 228/1000 | Loss: 0.00017160
Iteration 229/1000 | Loss: 0.00021352
Iteration 230/1000 | Loss: 0.00070993
Iteration 231/1000 | Loss: 0.00028200
Iteration 232/1000 | Loss: 0.00069586
Iteration 233/1000 | Loss: 0.00038682
Iteration 234/1000 | Loss: 0.00068197
Iteration 235/1000 | Loss: 0.00026775
Iteration 236/1000 | Loss: 0.00026590
Iteration 237/1000 | Loss: 0.00023486
Iteration 238/1000 | Loss: 0.00004572
Iteration 239/1000 | Loss: 0.00021170
Iteration 240/1000 | Loss: 0.00033722
Iteration 241/1000 | Loss: 0.00019736
Iteration 242/1000 | Loss: 0.00003656
Iteration 243/1000 | Loss: 0.00020607
Iteration 244/1000 | Loss: 0.00039442
Iteration 245/1000 | Loss: 0.00035983
Iteration 246/1000 | Loss: 0.00045642
Iteration 247/1000 | Loss: 0.00033626
Iteration 248/1000 | Loss: 0.00033581
Iteration 249/1000 | Loss: 0.00029962
Iteration 250/1000 | Loss: 0.00031577
Iteration 251/1000 | Loss: 0.00026815
Iteration 252/1000 | Loss: 0.00027759
Iteration 253/1000 | Loss: 0.00020909
Iteration 254/1000 | Loss: 0.00020175
Iteration 255/1000 | Loss: 0.00018590
Iteration 256/1000 | Loss: 0.00030373
Iteration 257/1000 | Loss: 0.00015804
Iteration 258/1000 | Loss: 0.00015637
Iteration 259/1000 | Loss: 0.00011362
Iteration 260/1000 | Loss: 0.00010973
Iteration 261/1000 | Loss: 0.00015140
Iteration 262/1000 | Loss: 0.00015642
Iteration 263/1000 | Loss: 0.00006745
Iteration 264/1000 | Loss: 0.00023878
Iteration 265/1000 | Loss: 0.00019534
Iteration 266/1000 | Loss: 0.00014051
Iteration 267/1000 | Loss: 0.00015153
Iteration 268/1000 | Loss: 0.00011825
Iteration 269/1000 | Loss: 0.00023547
Iteration 270/1000 | Loss: 0.00013322
Iteration 271/1000 | Loss: 0.00032049
Iteration 272/1000 | Loss: 0.00004146
Iteration 273/1000 | Loss: 0.00018034
Iteration 274/1000 | Loss: 0.00004083
Iteration 275/1000 | Loss: 0.00006620
Iteration 276/1000 | Loss: 0.00003632
Iteration 277/1000 | Loss: 0.00024480
Iteration 278/1000 | Loss: 0.00008679
Iteration 279/1000 | Loss: 0.00012739
Iteration 280/1000 | Loss: 0.00015497
Iteration 281/1000 | Loss: 0.00014639
Iteration 282/1000 | Loss: 0.00021071
Iteration 283/1000 | Loss: 0.00009709
Iteration 284/1000 | Loss: 0.00016343
Iteration 285/1000 | Loss: 0.00009619
Iteration 286/1000 | Loss: 0.00010885
Iteration 287/1000 | Loss: 0.00009322
Iteration 288/1000 | Loss: 0.00033637
Iteration 289/1000 | Loss: 0.00020432
Iteration 290/1000 | Loss: 0.00076445
Iteration 291/1000 | Loss: 0.00035217
Iteration 292/1000 | Loss: 0.00038940
Iteration 293/1000 | Loss: 0.00038483
Iteration 294/1000 | Loss: 0.00040006
Iteration 295/1000 | Loss: 0.00034993
Iteration 296/1000 | Loss: 0.00018058
Iteration 297/1000 | Loss: 0.00032791
Iteration 298/1000 | Loss: 0.00042659
Iteration 299/1000 | Loss: 0.00060175
Iteration 300/1000 | Loss: 0.00025548
Iteration 301/1000 | Loss: 0.00014806
Iteration 302/1000 | Loss: 0.00033583
Iteration 303/1000 | Loss: 0.00034799
Iteration 304/1000 | Loss: 0.00024636
Iteration 305/1000 | Loss: 0.00081419
Iteration 306/1000 | Loss: 0.00005152
Iteration 307/1000 | Loss: 0.00003739
Iteration 308/1000 | Loss: 0.00005647
Iteration 309/1000 | Loss: 0.00010425
Iteration 310/1000 | Loss: 0.00033186
Iteration 311/1000 | Loss: 0.00048973
Iteration 312/1000 | Loss: 0.00011672
Iteration 313/1000 | Loss: 0.00011116
Iteration 314/1000 | Loss: 0.00061266
Iteration 315/1000 | Loss: 0.00037910
Iteration 316/1000 | Loss: 0.00021273
Iteration 317/1000 | Loss: 0.00014315
Iteration 318/1000 | Loss: 0.00041865
Iteration 319/1000 | Loss: 0.00049867
Iteration 320/1000 | Loss: 0.00061122
Iteration 321/1000 | Loss: 0.00080750
Iteration 322/1000 | Loss: 0.00043216
Iteration 323/1000 | Loss: 0.00041489
Iteration 324/1000 | Loss: 0.00047562
Iteration 325/1000 | Loss: 0.00032923
Iteration 326/1000 | Loss: 0.00083151
Iteration 327/1000 | Loss: 0.00041794
Iteration 328/1000 | Loss: 0.00071327
Iteration 329/1000 | Loss: 0.00027085
Iteration 330/1000 | Loss: 0.00073149
Iteration 331/1000 | Loss: 0.00046428
Iteration 332/1000 | Loss: 0.00076499
Iteration 333/1000 | Loss: 0.00082098
Iteration 334/1000 | Loss: 0.00068217
Iteration 335/1000 | Loss: 0.00049594
Iteration 336/1000 | Loss: 0.00025661
Iteration 337/1000 | Loss: 0.00023631
Iteration 338/1000 | Loss: 0.00076329
Iteration 339/1000 | Loss: 0.00048175
Iteration 340/1000 | Loss: 0.00054224
Iteration 341/1000 | Loss: 0.00069243
Iteration 342/1000 | Loss: 0.00046214
Iteration 343/1000 | Loss: 0.00026372
Iteration 344/1000 | Loss: 0.00026057
Iteration 345/1000 | Loss: 0.00019936
Iteration 346/1000 | Loss: 0.00027597
Iteration 347/1000 | Loss: 0.00023492
Iteration 348/1000 | Loss: 0.00025646
Iteration 349/1000 | Loss: 0.00047187
Iteration 350/1000 | Loss: 0.00059906
Iteration 351/1000 | Loss: 0.00020396
Iteration 352/1000 | Loss: 0.00019814
Iteration 353/1000 | Loss: 0.00030876
Iteration 354/1000 | Loss: 0.00017553
Iteration 355/1000 | Loss: 0.00034969
Iteration 356/1000 | Loss: 0.00037420
Iteration 357/1000 | Loss: 0.00039156
Iteration 358/1000 | Loss: 0.00018162
Iteration 359/1000 | Loss: 0.00031303
Iteration 360/1000 | Loss: 0.00017467
Iteration 361/1000 | Loss: 0.00046041
Iteration 362/1000 | Loss: 0.00086825
Iteration 363/1000 | Loss: 0.00052737
Iteration 364/1000 | Loss: 0.00044129
Iteration 365/1000 | Loss: 0.00015201
Iteration 366/1000 | Loss: 0.00024533
Iteration 367/1000 | Loss: 0.00033991
Iteration 368/1000 | Loss: 0.00007914
Iteration 369/1000 | Loss: 0.00041973
Iteration 370/1000 | Loss: 0.00031713
Iteration 371/1000 | Loss: 0.00010040
Iteration 372/1000 | Loss: 0.00021759
Iteration 373/1000 | Loss: 0.00003996
Iteration 374/1000 | Loss: 0.00018856
Iteration 375/1000 | Loss: 0.00033669
Iteration 376/1000 | Loss: 0.00024003
Iteration 377/1000 | Loss: 0.00034852
Iteration 378/1000 | Loss: 0.00023711
Iteration 379/1000 | Loss: 0.00027250
Iteration 380/1000 | Loss: 0.00062085
Iteration 381/1000 | Loss: 0.00027894
Iteration 382/1000 | Loss: 0.00031210
Iteration 383/1000 | Loss: 0.00017772
Iteration 384/1000 | Loss: 0.00030446
Iteration 385/1000 | Loss: 0.00014829
Iteration 386/1000 | Loss: 0.00018002
Iteration 387/1000 | Loss: 0.00014728
Iteration 388/1000 | Loss: 0.00016027
Iteration 389/1000 | Loss: 0.00023011
Iteration 390/1000 | Loss: 0.00013387
Iteration 391/1000 | Loss: 0.00075324
Iteration 392/1000 | Loss: 0.00053526
Iteration 393/1000 | Loss: 0.00028048
Iteration 394/1000 | Loss: 0.00036917
Iteration 395/1000 | Loss: 0.00025870
Iteration 396/1000 | Loss: 0.00025327
Iteration 397/1000 | Loss: 0.00017736
Iteration 398/1000 | Loss: 0.00011845
Iteration 399/1000 | Loss: 0.00050973
Iteration 400/1000 | Loss: 0.00015660
Iteration 401/1000 | Loss: 0.00011830
Iteration 402/1000 | Loss: 0.00008627
Iteration 403/1000 | Loss: 0.00024051
Iteration 404/1000 | Loss: 0.00003356
Iteration 405/1000 | Loss: 0.00010979
Iteration 406/1000 | Loss: 0.00012659
Iteration 407/1000 | Loss: 0.00013281
Iteration 408/1000 | Loss: 0.00017694
Iteration 409/1000 | Loss: 0.00018825
Iteration 410/1000 | Loss: 0.00011197
Iteration 411/1000 | Loss: 0.00016561
Iteration 412/1000 | Loss: 0.00012658
Iteration 413/1000 | Loss: 0.00008534
Iteration 414/1000 | Loss: 0.00006969
Iteration 415/1000 | Loss: 0.00002563
Iteration 416/1000 | Loss: 0.00006108
Iteration 417/1000 | Loss: 0.00002375
Iteration 418/1000 | Loss: 0.00002312
Iteration 419/1000 | Loss: 0.00018954
Iteration 420/1000 | Loss: 0.00007647
Iteration 421/1000 | Loss: 0.00013100
Iteration 422/1000 | Loss: 0.00008214
Iteration 423/1000 | Loss: 0.00014838
Iteration 424/1000 | Loss: 0.00002446
Iteration 425/1000 | Loss: 0.00002312
Iteration 426/1000 | Loss: 0.00002226
Iteration 427/1000 | Loss: 0.00002158
Iteration 428/1000 | Loss: 0.00002071
Iteration 429/1000 | Loss: 0.00001997
Iteration 430/1000 | Loss: 0.00001940
Iteration 431/1000 | Loss: 0.00019306
Iteration 432/1000 | Loss: 0.00018339
Iteration 433/1000 | Loss: 0.00024470
Iteration 434/1000 | Loss: 0.00014940
Iteration 435/1000 | Loss: 0.00029932
Iteration 436/1000 | Loss: 0.00029810
Iteration 437/1000 | Loss: 0.00009683
Iteration 438/1000 | Loss: 0.00005557
Iteration 439/1000 | Loss: 0.00002371
Iteration 440/1000 | Loss: 0.00002877
Iteration 441/1000 | Loss: 0.00016933
Iteration 442/1000 | Loss: 0.00098296
Iteration 443/1000 | Loss: 0.00011029
Iteration 444/1000 | Loss: 0.00002459
Iteration 445/1000 | Loss: 0.00002146
Iteration 446/1000 | Loss: 0.00002025
Iteration 447/1000 | Loss: 0.00001906
Iteration 448/1000 | Loss: 0.00022181
Iteration 449/1000 | Loss: 0.00019935
Iteration 450/1000 | Loss: 0.00033116
Iteration 451/1000 | Loss: 0.00012818
Iteration 452/1000 | Loss: 0.00008813
Iteration 453/1000 | Loss: 0.00008095
Iteration 454/1000 | Loss: 0.00008598
Iteration 455/1000 | Loss: 0.00011989
Iteration 456/1000 | Loss: 0.00012766
Iteration 457/1000 | Loss: 0.00012793
Iteration 458/1000 | Loss: 0.00002318
Iteration 459/1000 | Loss: 0.00005909
Iteration 460/1000 | Loss: 0.00015577
Iteration 461/1000 | Loss: 0.00005684
Iteration 462/1000 | Loss: 0.00016037
Iteration 463/1000 | Loss: 0.00008258
Iteration 464/1000 | Loss: 0.00014741
Iteration 465/1000 | Loss: 0.00008885
Iteration 466/1000 | Loss: 0.00011807
Iteration 467/1000 | Loss: 0.00013983
Iteration 468/1000 | Loss: 0.00002854
Iteration 469/1000 | Loss: 0.00002130
Iteration 470/1000 | Loss: 0.00002003
Iteration 471/1000 | Loss: 0.00010845
Iteration 472/1000 | Loss: 0.00001846
Iteration 473/1000 | Loss: 0.00001799
Iteration 474/1000 | Loss: 0.00002080
Iteration 475/1000 | Loss: 0.00001738
Iteration 476/1000 | Loss: 0.00001726
Iteration 477/1000 | Loss: 0.00001726
Iteration 478/1000 | Loss: 0.00001726
Iteration 479/1000 | Loss: 0.00001726
Iteration 480/1000 | Loss: 0.00001725
Iteration 481/1000 | Loss: 0.00001725
Iteration 482/1000 | Loss: 0.00001725
Iteration 483/1000 | Loss: 0.00001725
Iteration 484/1000 | Loss: 0.00001725
Iteration 485/1000 | Loss: 0.00001725
Iteration 486/1000 | Loss: 0.00001725
Iteration 487/1000 | Loss: 0.00001725
Iteration 488/1000 | Loss: 0.00001725
Iteration 489/1000 | Loss: 0.00001725
Iteration 490/1000 | Loss: 0.00001724
Iteration 491/1000 | Loss: 0.00001724
Iteration 492/1000 | Loss: 0.00001722
Iteration 493/1000 | Loss: 0.00001722
Iteration 494/1000 | Loss: 0.00001721
Iteration 495/1000 | Loss: 0.00001720
Iteration 496/1000 | Loss: 0.00001720
Iteration 497/1000 | Loss: 0.00001719
Iteration 498/1000 | Loss: 0.00001719
Iteration 499/1000 | Loss: 0.00001708
Iteration 500/1000 | Loss: 0.00001708
Iteration 501/1000 | Loss: 0.00001708
Iteration 502/1000 | Loss: 0.00001708
Iteration 503/1000 | Loss: 0.00001708
Iteration 504/1000 | Loss: 0.00001708
Iteration 505/1000 | Loss: 0.00001708
Iteration 506/1000 | Loss: 0.00001707
Iteration 507/1000 | Loss: 0.00001707
Iteration 508/1000 | Loss: 0.00001707
Iteration 509/1000 | Loss: 0.00001707
Iteration 510/1000 | Loss: 0.00001707
Iteration 511/1000 | Loss: 0.00001706
Iteration 512/1000 | Loss: 0.00001705
Iteration 513/1000 | Loss: 0.00001705
Iteration 514/1000 | Loss: 0.00001703
Iteration 515/1000 | Loss: 0.00001703
Iteration 516/1000 | Loss: 0.00001703
Iteration 517/1000 | Loss: 0.00001702
Iteration 518/1000 | Loss: 0.00001702
Iteration 519/1000 | Loss: 0.00001701
Iteration 520/1000 | Loss: 0.00001701
Iteration 521/1000 | Loss: 0.00001700
Iteration 522/1000 | Loss: 0.00001699
Iteration 523/1000 | Loss: 0.00001698
Iteration 524/1000 | Loss: 0.00001697
Iteration 525/1000 | Loss: 0.00001697
Iteration 526/1000 | Loss: 0.00001696
Iteration 527/1000 | Loss: 0.00001696
Iteration 528/1000 | Loss: 0.00001695
Iteration 529/1000 | Loss: 0.00001695
Iteration 530/1000 | Loss: 0.00001694
Iteration 531/1000 | Loss: 0.00001693
Iteration 532/1000 | Loss: 0.00001692
Iteration 533/1000 | Loss: 0.00001691
Iteration 534/1000 | Loss: 0.00001690
Iteration 535/1000 | Loss: 0.00001690
Iteration 536/1000 | Loss: 0.00001687
Iteration 537/1000 | Loss: 0.00001687
Iteration 538/1000 | Loss: 0.00001687
Iteration 539/1000 | Loss: 0.00001687
Iteration 540/1000 | Loss: 0.00001687
Iteration 541/1000 | Loss: 0.00001686
Iteration 542/1000 | Loss: 0.00001686
Iteration 543/1000 | Loss: 0.00001686
Iteration 544/1000 | Loss: 0.00001685
Iteration 545/1000 | Loss: 0.00001685
Iteration 546/1000 | Loss: 0.00001684
Iteration 547/1000 | Loss: 0.00001684
Iteration 548/1000 | Loss: 0.00001684
Iteration 549/1000 | Loss: 0.00001683
Iteration 550/1000 | Loss: 0.00001683
Iteration 551/1000 | Loss: 0.00001683
Iteration 552/1000 | Loss: 0.00001682
Iteration 553/1000 | Loss: 0.00001682
Iteration 554/1000 | Loss: 0.00001682
Iteration 555/1000 | Loss: 0.00001681
Iteration 556/1000 | Loss: 0.00001680
Iteration 557/1000 | Loss: 0.00001680
Iteration 558/1000 | Loss: 0.00001680
Iteration 559/1000 | Loss: 0.00001679
Iteration 560/1000 | Loss: 0.00001679
Iteration 561/1000 | Loss: 0.00001679
Iteration 562/1000 | Loss: 0.00001679
Iteration 563/1000 | Loss: 0.00001679
Iteration 564/1000 | Loss: 0.00001679
Iteration 565/1000 | Loss: 0.00001679
Iteration 566/1000 | Loss: 0.00001679
Iteration 567/1000 | Loss: 0.00001678
Iteration 568/1000 | Loss: 0.00001678
Iteration 569/1000 | Loss: 0.00001678
Iteration 570/1000 | Loss: 0.00001678
Iteration 571/1000 | Loss: 0.00001678
Iteration 572/1000 | Loss: 0.00001678
Iteration 573/1000 | Loss: 0.00001677
Iteration 574/1000 | Loss: 0.00001677
Iteration 575/1000 | Loss: 0.00001677
Iteration 576/1000 | Loss: 0.00001677
Iteration 577/1000 | Loss: 0.00001677
Iteration 578/1000 | Loss: 0.00001677
Iteration 579/1000 | Loss: 0.00001677
Iteration 580/1000 | Loss: 0.00001677
Iteration 581/1000 | Loss: 0.00001677
Iteration 582/1000 | Loss: 0.00001676
Iteration 583/1000 | Loss: 0.00001676
Iteration 584/1000 | Loss: 0.00001676
Iteration 585/1000 | Loss: 0.00001676
Iteration 586/1000 | Loss: 0.00001676
Iteration 587/1000 | Loss: 0.00001676
Iteration 588/1000 | Loss: 0.00001676
Iteration 589/1000 | Loss: 0.00001675
Iteration 590/1000 | Loss: 0.00001675
Iteration 591/1000 | Loss: 0.00001674
Iteration 592/1000 | Loss: 0.00001674
Iteration 593/1000 | Loss: 0.00001673
Iteration 594/1000 | Loss: 0.00001673
Iteration 595/1000 | Loss: 0.00001672
Iteration 596/1000 | Loss: 0.00001672
Iteration 597/1000 | Loss: 0.00001672
Iteration 598/1000 | Loss: 0.00001672
Iteration 599/1000 | Loss: 0.00001671
Iteration 600/1000 | Loss: 0.00001671
Iteration 601/1000 | Loss: 0.00001671
Iteration 602/1000 | Loss: 0.00001671
Iteration 603/1000 | Loss: 0.00001670
Iteration 604/1000 | Loss: 0.00001670
Iteration 605/1000 | Loss: 0.00001670
Iteration 606/1000 | Loss: 0.00001670
Iteration 607/1000 | Loss: 0.00001670
Iteration 608/1000 | Loss: 0.00001670
Iteration 609/1000 | Loss: 0.00001669
Iteration 610/1000 | Loss: 0.00001669
Iteration 611/1000 | Loss: 0.00001669
Iteration 612/1000 | Loss: 0.00001669
Iteration 613/1000 | Loss: 0.00001668
Iteration 614/1000 | Loss: 0.00001668
Iteration 615/1000 | Loss: 0.00001668
Iteration 616/1000 | Loss: 0.00001668
Iteration 617/1000 | Loss: 0.00001668
Iteration 618/1000 | Loss: 0.00001667
Iteration 619/1000 | Loss: 0.00001667
Iteration 620/1000 | Loss: 0.00001667
Iteration 621/1000 | Loss: 0.00001667
Iteration 622/1000 | Loss: 0.00001667
Iteration 623/1000 | Loss: 0.00001667
Iteration 624/1000 | Loss: 0.00001667
Iteration 625/1000 | Loss: 0.00001667
Iteration 626/1000 | Loss: 0.00001667
Iteration 627/1000 | Loss: 0.00001667
Iteration 628/1000 | Loss: 0.00001667
Iteration 629/1000 | Loss: 0.00001667
Iteration 630/1000 | Loss: 0.00001667
Iteration 631/1000 | Loss: 0.00001667
Iteration 632/1000 | Loss: 0.00001667
Iteration 633/1000 | Loss: 0.00001667
Iteration 634/1000 | Loss: 0.00001667
Iteration 635/1000 | Loss: 0.00001667
Iteration 636/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 636. Stopping optimization.
Last 5 losses: [1.6667441741446964e-05, 1.6667441741446964e-05, 1.6667441741446964e-05, 1.6667441741446964e-05, 1.6667441741446964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6667441741446964e-05

Optimization complete. Final v2v error: 3.395841121673584 mm

Highest mean error: 7.754641532897949 mm for frame 148

Lowest mean error: 2.7535400390625 mm for frame 132

Saving results

Total time: 816.0969159603119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498489
Iteration 2/25 | Loss: 0.00109687
Iteration 3/25 | Loss: 0.00096510
Iteration 4/25 | Loss: 0.00092908
Iteration 5/25 | Loss: 0.00092227
Iteration 6/25 | Loss: 0.00092092
Iteration 7/25 | Loss: 0.00092090
Iteration 8/25 | Loss: 0.00092090
Iteration 9/25 | Loss: 0.00092090
Iteration 10/25 | Loss: 0.00092090
Iteration 11/25 | Loss: 0.00092090
Iteration 12/25 | Loss: 0.00092090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009209047420881689, 0.0009209047420881689, 0.0009209047420881689, 0.0009209047420881689, 0.0009209047420881689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009209047420881689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47811091
Iteration 2/25 | Loss: 0.00055944
Iteration 3/25 | Loss: 0.00055944
Iteration 4/25 | Loss: 0.00055944
Iteration 5/25 | Loss: 0.00055944
Iteration 6/25 | Loss: 0.00055943
Iteration 7/25 | Loss: 0.00055943
Iteration 8/25 | Loss: 0.00055943
Iteration 9/25 | Loss: 0.00055943
Iteration 10/25 | Loss: 0.00055943
Iteration 11/25 | Loss: 0.00055943
Iteration 12/25 | Loss: 0.00055943
Iteration 13/25 | Loss: 0.00055943
Iteration 14/25 | Loss: 0.00055943
Iteration 15/25 | Loss: 0.00055943
Iteration 16/25 | Loss: 0.00055943
Iteration 17/25 | Loss: 0.00055943
Iteration 18/25 | Loss: 0.00055943
Iteration 19/25 | Loss: 0.00055943
Iteration 20/25 | Loss: 0.00055943
Iteration 21/25 | Loss: 0.00055943
Iteration 22/25 | Loss: 0.00055943
Iteration 23/25 | Loss: 0.00055943
Iteration 24/25 | Loss: 0.00055943
Iteration 25/25 | Loss: 0.00055943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055943
Iteration 2/1000 | Loss: 0.00005796
Iteration 3/1000 | Loss: 0.00004106
Iteration 4/1000 | Loss: 0.00003763
Iteration 5/1000 | Loss: 0.00003558
Iteration 6/1000 | Loss: 0.00003454
Iteration 7/1000 | Loss: 0.00003349
Iteration 8/1000 | Loss: 0.00003298
Iteration 9/1000 | Loss: 0.00003244
Iteration 10/1000 | Loss: 0.00003213
Iteration 11/1000 | Loss: 0.00003187
Iteration 12/1000 | Loss: 0.00003171
Iteration 13/1000 | Loss: 0.00003162
Iteration 14/1000 | Loss: 0.00003146
Iteration 15/1000 | Loss: 0.00003143
Iteration 16/1000 | Loss: 0.00003127
Iteration 17/1000 | Loss: 0.00003110
Iteration 18/1000 | Loss: 0.00003109
Iteration 19/1000 | Loss: 0.00003108
Iteration 20/1000 | Loss: 0.00003107
Iteration 21/1000 | Loss: 0.00003105
Iteration 22/1000 | Loss: 0.00003104
Iteration 23/1000 | Loss: 0.00003104
Iteration 24/1000 | Loss: 0.00003100
Iteration 25/1000 | Loss: 0.00003099
Iteration 26/1000 | Loss: 0.00003099
Iteration 27/1000 | Loss: 0.00003098
Iteration 28/1000 | Loss: 0.00003098
Iteration 29/1000 | Loss: 0.00003098
Iteration 30/1000 | Loss: 0.00003097
Iteration 31/1000 | Loss: 0.00003097
Iteration 32/1000 | Loss: 0.00003097
Iteration 33/1000 | Loss: 0.00003096
Iteration 34/1000 | Loss: 0.00003096
Iteration 35/1000 | Loss: 0.00003095
Iteration 36/1000 | Loss: 0.00003095
Iteration 37/1000 | Loss: 0.00003095
Iteration 38/1000 | Loss: 0.00003095
Iteration 39/1000 | Loss: 0.00003094
Iteration 40/1000 | Loss: 0.00003094
Iteration 41/1000 | Loss: 0.00003094
Iteration 42/1000 | Loss: 0.00003094
Iteration 43/1000 | Loss: 0.00003094
Iteration 44/1000 | Loss: 0.00003093
Iteration 45/1000 | Loss: 0.00003093
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00003093
Iteration 48/1000 | Loss: 0.00003093
Iteration 49/1000 | Loss: 0.00003093
Iteration 50/1000 | Loss: 0.00003093
Iteration 51/1000 | Loss: 0.00003093
Iteration 52/1000 | Loss: 0.00003093
Iteration 53/1000 | Loss: 0.00003092
Iteration 54/1000 | Loss: 0.00003092
Iteration 55/1000 | Loss: 0.00003092
Iteration 56/1000 | Loss: 0.00003091
Iteration 57/1000 | Loss: 0.00003091
Iteration 58/1000 | Loss: 0.00003091
Iteration 59/1000 | Loss: 0.00003091
Iteration 60/1000 | Loss: 0.00003091
Iteration 61/1000 | Loss: 0.00003091
Iteration 62/1000 | Loss: 0.00003091
Iteration 63/1000 | Loss: 0.00003091
Iteration 64/1000 | Loss: 0.00003091
Iteration 65/1000 | Loss: 0.00003091
Iteration 66/1000 | Loss: 0.00003091
Iteration 67/1000 | Loss: 0.00003090
Iteration 68/1000 | Loss: 0.00003090
Iteration 69/1000 | Loss: 0.00003090
Iteration 70/1000 | Loss: 0.00003090
Iteration 71/1000 | Loss: 0.00003090
Iteration 72/1000 | Loss: 0.00003090
Iteration 73/1000 | Loss: 0.00003090
Iteration 74/1000 | Loss: 0.00003090
Iteration 75/1000 | Loss: 0.00003090
Iteration 76/1000 | Loss: 0.00003090
Iteration 77/1000 | Loss: 0.00003090
Iteration 78/1000 | Loss: 0.00003090
Iteration 79/1000 | Loss: 0.00003090
Iteration 80/1000 | Loss: 0.00003090
Iteration 81/1000 | Loss: 0.00003090
Iteration 82/1000 | Loss: 0.00003090
Iteration 83/1000 | Loss: 0.00003090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [3.0901548598194495e-05, 3.0901548598194495e-05, 3.0901548598194495e-05, 3.0901548598194495e-05, 3.0901548598194495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0901548598194495e-05

Optimization complete. Final v2v error: 4.34559965133667 mm

Highest mean error: 5.36502742767334 mm for frame 96

Lowest mean error: 3.6783218383789062 mm for frame 134

Saving results

Total time: 36.94764852523804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416974
Iteration 2/25 | Loss: 0.00090596
Iteration 3/25 | Loss: 0.00082725
Iteration 4/25 | Loss: 0.00081226
Iteration 5/25 | Loss: 0.00080756
Iteration 6/25 | Loss: 0.00080625
Iteration 7/25 | Loss: 0.00080611
Iteration 8/25 | Loss: 0.00080611
Iteration 9/25 | Loss: 0.00080611
Iteration 10/25 | Loss: 0.00080611
Iteration 11/25 | Loss: 0.00080611
Iteration 12/25 | Loss: 0.00080611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008061077678576112, 0.0008061077678576112, 0.0008061077678576112, 0.0008061077678576112, 0.0008061077678576112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008061077678576112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90431142
Iteration 2/25 | Loss: 0.00051688
Iteration 3/25 | Loss: 0.00051688
Iteration 4/25 | Loss: 0.00051688
Iteration 5/25 | Loss: 0.00051688
Iteration 6/25 | Loss: 0.00051688
Iteration 7/25 | Loss: 0.00051688
Iteration 8/25 | Loss: 0.00051688
Iteration 9/25 | Loss: 0.00051688
Iteration 10/25 | Loss: 0.00051688
Iteration 11/25 | Loss: 0.00051688
Iteration 12/25 | Loss: 0.00051688
Iteration 13/25 | Loss: 0.00051688
Iteration 14/25 | Loss: 0.00051688
Iteration 15/25 | Loss: 0.00051688
Iteration 16/25 | Loss: 0.00051688
Iteration 17/25 | Loss: 0.00051688
Iteration 18/25 | Loss: 0.00051688
Iteration 19/25 | Loss: 0.00051688
Iteration 20/25 | Loss: 0.00051688
Iteration 21/25 | Loss: 0.00051688
Iteration 22/25 | Loss: 0.00051688
Iteration 23/25 | Loss: 0.00051688
Iteration 24/25 | Loss: 0.00051688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005168779753148556, 0.0005168779753148556, 0.0005168779753148556, 0.0005168779753148556, 0.0005168779753148556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005168779753148556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051688
Iteration 2/1000 | Loss: 0.00002745
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001383
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001261
Iteration 108/1000 | Loss: 0.00001261
Iteration 109/1000 | Loss: 0.00001261
Iteration 110/1000 | Loss: 0.00001261
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001261
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001261
Iteration 119/1000 | Loss: 0.00001261
Iteration 120/1000 | Loss: 0.00001261
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2606264135683887e-05, 1.2606264135683887e-05, 1.2606264135683887e-05, 1.2606264135683887e-05, 1.2606264135683887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2606264135683887e-05

Optimization complete. Final v2v error: 3.0652272701263428 mm

Highest mean error: 3.2838001251220703 mm for frame 121

Lowest mean error: 2.8887295722961426 mm for frame 157

Saving results

Total time: 34.65662336349487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488861
Iteration 2/25 | Loss: 0.00105473
Iteration 3/25 | Loss: 0.00090323
Iteration 4/25 | Loss: 0.00088493
Iteration 5/25 | Loss: 0.00087875
Iteration 6/25 | Loss: 0.00087713
Iteration 7/25 | Loss: 0.00087705
Iteration 8/25 | Loss: 0.00087705
Iteration 9/25 | Loss: 0.00087705
Iteration 10/25 | Loss: 0.00087705
Iteration 11/25 | Loss: 0.00087705
Iteration 12/25 | Loss: 0.00087705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008770485874265432, 0.0008770485874265432, 0.0008770485874265432, 0.0008770485874265432, 0.0008770485874265432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008770485874265432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51838863
Iteration 2/25 | Loss: 0.00060194
Iteration 3/25 | Loss: 0.00060192
Iteration 4/25 | Loss: 0.00060192
Iteration 5/25 | Loss: 0.00060192
Iteration 6/25 | Loss: 0.00060191
Iteration 7/25 | Loss: 0.00060191
Iteration 8/25 | Loss: 0.00060191
Iteration 9/25 | Loss: 0.00060191
Iteration 10/25 | Loss: 0.00060191
Iteration 11/25 | Loss: 0.00060191
Iteration 12/25 | Loss: 0.00060191
Iteration 13/25 | Loss: 0.00060191
Iteration 14/25 | Loss: 0.00060191
Iteration 15/25 | Loss: 0.00060191
Iteration 16/25 | Loss: 0.00060191
Iteration 17/25 | Loss: 0.00060191
Iteration 18/25 | Loss: 0.00060191
Iteration 19/25 | Loss: 0.00060191
Iteration 20/25 | Loss: 0.00060191
Iteration 21/25 | Loss: 0.00060191
Iteration 22/25 | Loss: 0.00060191
Iteration 23/25 | Loss: 0.00060191
Iteration 24/25 | Loss: 0.00060191
Iteration 25/25 | Loss: 0.00060191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060191
Iteration 2/1000 | Loss: 0.00002872
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001919
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001714
Iteration 8/1000 | Loss: 0.00001700
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001647
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001630
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001624
Iteration 29/1000 | Loss: 0.00001624
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001618
Iteration 34/1000 | Loss: 0.00001617
Iteration 35/1000 | Loss: 0.00001617
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001615
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001615
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001609
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001607
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001605
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001605
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001604
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001604
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001603
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00001601
Iteration 119/1000 | Loss: 0.00001601
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001600
Iteration 123/1000 | Loss: 0.00001600
Iteration 124/1000 | Loss: 0.00001600
Iteration 125/1000 | Loss: 0.00001600
Iteration 126/1000 | Loss: 0.00001600
Iteration 127/1000 | Loss: 0.00001599
Iteration 128/1000 | Loss: 0.00001599
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001599
Iteration 132/1000 | Loss: 0.00001599
Iteration 133/1000 | Loss: 0.00001599
Iteration 134/1000 | Loss: 0.00001599
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.599139977770392e-05, 1.599139977770392e-05, 1.599139977770392e-05, 1.599139977770392e-05, 1.599139977770392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599139977770392e-05

Optimization complete. Final v2v error: 3.352330207824707 mm

Highest mean error: 3.8183019161224365 mm for frame 58

Lowest mean error: 2.9519503116607666 mm for frame 12

Saving results

Total time: 39.96232533454895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505298
Iteration 2/25 | Loss: 0.00117159
Iteration 3/25 | Loss: 0.00088881
Iteration 4/25 | Loss: 0.00086492
Iteration 5/25 | Loss: 0.00085671
Iteration 6/25 | Loss: 0.00085452
Iteration 7/25 | Loss: 0.00085366
Iteration 8/25 | Loss: 0.00085358
Iteration 9/25 | Loss: 0.00085358
Iteration 10/25 | Loss: 0.00085358
Iteration 11/25 | Loss: 0.00085358
Iteration 12/25 | Loss: 0.00085358
Iteration 13/25 | Loss: 0.00085358
Iteration 14/25 | Loss: 0.00085358
Iteration 15/25 | Loss: 0.00085358
Iteration 16/25 | Loss: 0.00085358
Iteration 17/25 | Loss: 0.00085358
Iteration 18/25 | Loss: 0.00085358
Iteration 19/25 | Loss: 0.00085358
Iteration 20/25 | Loss: 0.00085358
Iteration 21/25 | Loss: 0.00085358
Iteration 22/25 | Loss: 0.00085358
Iteration 23/25 | Loss: 0.00085358
Iteration 24/25 | Loss: 0.00085358
Iteration 25/25 | Loss: 0.00085358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52991068
Iteration 2/25 | Loss: 0.00053442
Iteration 3/25 | Loss: 0.00053441
Iteration 4/25 | Loss: 0.00053441
Iteration 5/25 | Loss: 0.00053441
Iteration 6/25 | Loss: 0.00053441
Iteration 7/25 | Loss: 0.00053441
Iteration 8/25 | Loss: 0.00053441
Iteration 9/25 | Loss: 0.00053441
Iteration 10/25 | Loss: 0.00053441
Iteration 11/25 | Loss: 0.00053441
Iteration 12/25 | Loss: 0.00053441
Iteration 13/25 | Loss: 0.00053441
Iteration 14/25 | Loss: 0.00053441
Iteration 15/25 | Loss: 0.00053441
Iteration 16/25 | Loss: 0.00053441
Iteration 17/25 | Loss: 0.00053441
Iteration 18/25 | Loss: 0.00053441
Iteration 19/25 | Loss: 0.00053441
Iteration 20/25 | Loss: 0.00053441
Iteration 21/25 | Loss: 0.00053441
Iteration 22/25 | Loss: 0.00053441
Iteration 23/25 | Loss: 0.00053441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005344104720279574, 0.0005344104720279574, 0.0005344104720279574, 0.0005344104720279574, 0.0005344104720279574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005344104720279574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053441
Iteration 2/1000 | Loss: 0.00003482
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00001940
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001748
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001652
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001634
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001588
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001586
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001585
Iteration 30/1000 | Loss: 0.00001584
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001579
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001566
Iteration 116/1000 | Loss: 0.00001566
Iteration 117/1000 | Loss: 0.00001565
Iteration 118/1000 | Loss: 0.00001565
Iteration 119/1000 | Loss: 0.00001562
Iteration 120/1000 | Loss: 0.00001562
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001561
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001557
Iteration 148/1000 | Loss: 0.00001557
Iteration 149/1000 | Loss: 0.00001557
Iteration 150/1000 | Loss: 0.00001557
Iteration 151/1000 | Loss: 0.00001557
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001556
Iteration 154/1000 | Loss: 0.00001556
Iteration 155/1000 | Loss: 0.00001556
Iteration 156/1000 | Loss: 0.00001556
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.556356619403232e-05, 1.556356619403232e-05, 1.556356619403232e-05, 1.556356619403232e-05, 1.556356619403232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556356619403232e-05

Optimization complete. Final v2v error: 3.1269237995147705 mm

Highest mean error: 4.653467178344727 mm for frame 61

Lowest mean error: 2.6196937561035156 mm for frame 120

Saving results

Total time: 38.864617586135864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021559
Iteration 2/25 | Loss: 0.00230486
Iteration 3/25 | Loss: 0.00191920
Iteration 4/25 | Loss: 0.00149593
Iteration 5/25 | Loss: 0.00177796
Iteration 6/25 | Loss: 0.00143979
Iteration 7/25 | Loss: 0.00109526
Iteration 8/25 | Loss: 0.00099482
Iteration 9/25 | Loss: 0.00097309
Iteration 10/25 | Loss: 0.00096635
Iteration 11/25 | Loss: 0.00095872
Iteration 12/25 | Loss: 0.00095741
Iteration 13/25 | Loss: 0.00095700
Iteration 14/25 | Loss: 0.00095650
Iteration 15/25 | Loss: 0.00095622
Iteration 16/25 | Loss: 0.00095593
Iteration 17/25 | Loss: 0.00095559
Iteration 18/25 | Loss: 0.00095528
Iteration 19/25 | Loss: 0.00096478
Iteration 20/25 | Loss: 0.00095018
Iteration 21/25 | Loss: 0.00094595
Iteration 22/25 | Loss: 0.00094466
Iteration 23/25 | Loss: 0.00094445
Iteration 24/25 | Loss: 0.00094428
Iteration 25/25 | Loss: 0.00095129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50102365
Iteration 2/25 | Loss: 0.00092910
Iteration 3/25 | Loss: 0.00092909
Iteration 4/25 | Loss: 0.00092909
Iteration 5/25 | Loss: 0.00092909
Iteration 6/25 | Loss: 0.00092909
Iteration 7/25 | Loss: 0.00092909
Iteration 8/25 | Loss: 0.00092909
Iteration 9/25 | Loss: 0.00092909
Iteration 10/25 | Loss: 0.00092909
Iteration 11/25 | Loss: 0.00092909
Iteration 12/25 | Loss: 0.00092909
Iteration 13/25 | Loss: 0.00092909
Iteration 14/25 | Loss: 0.00092909
Iteration 15/25 | Loss: 0.00092909
Iteration 16/25 | Loss: 0.00092909
Iteration 17/25 | Loss: 0.00092909
Iteration 18/25 | Loss: 0.00092909
Iteration 19/25 | Loss: 0.00092909
Iteration 20/25 | Loss: 0.00092909
Iteration 21/25 | Loss: 0.00092909
Iteration 22/25 | Loss: 0.00092909
Iteration 23/25 | Loss: 0.00092909
Iteration 24/25 | Loss: 0.00092909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009290894959121943, 0.0009290894959121943, 0.0009290894959121943, 0.0009290894959121943, 0.0009290894959121943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009290894959121943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092909
Iteration 2/1000 | Loss: 0.00075865
Iteration 3/1000 | Loss: 0.00025736
Iteration 4/1000 | Loss: 0.00009577
Iteration 5/1000 | Loss: 0.00006899
Iteration 6/1000 | Loss: 0.00004042
Iteration 7/1000 | Loss: 0.00003188
Iteration 8/1000 | Loss: 0.00002745
Iteration 9/1000 | Loss: 0.00002517
Iteration 10/1000 | Loss: 0.00002346
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002099
Iteration 14/1000 | Loss: 0.00002061
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002051
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002013
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001994
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001991
Iteration 24/1000 | Loss: 0.00001987
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001986
Iteration 27/1000 | Loss: 0.00001986
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001982
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001980
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001979
Iteration 43/1000 | Loss: 0.00001979
Iteration 44/1000 | Loss: 0.00001979
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001978
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001977
Iteration 56/1000 | Loss: 0.00001977
Iteration 57/1000 | Loss: 0.00001977
Iteration 58/1000 | Loss: 0.00001977
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00001975
Iteration 65/1000 | Loss: 0.00001975
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001972
Iteration 84/1000 | Loss: 0.00001972
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001971
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001969
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001968
Iteration 130/1000 | Loss: 0.00001967
Iteration 131/1000 | Loss: 0.00001967
Iteration 132/1000 | Loss: 0.00001967
Iteration 133/1000 | Loss: 0.00001967
Iteration 134/1000 | Loss: 0.00001967
Iteration 135/1000 | Loss: 0.00001967
Iteration 136/1000 | Loss: 0.00001967
Iteration 137/1000 | Loss: 0.00001967
Iteration 138/1000 | Loss: 0.00001967
Iteration 139/1000 | Loss: 0.00001967
Iteration 140/1000 | Loss: 0.00001967
Iteration 141/1000 | Loss: 0.00001967
Iteration 142/1000 | Loss: 0.00001967
Iteration 143/1000 | Loss: 0.00001967
Iteration 144/1000 | Loss: 0.00001967
Iteration 145/1000 | Loss: 0.00001967
Iteration 146/1000 | Loss: 0.00001967
Iteration 147/1000 | Loss: 0.00001967
Iteration 148/1000 | Loss: 0.00001967
Iteration 149/1000 | Loss: 0.00001967
Iteration 150/1000 | Loss: 0.00001966
Iteration 151/1000 | Loss: 0.00001966
Iteration 152/1000 | Loss: 0.00001966
Iteration 153/1000 | Loss: 0.00001966
Iteration 154/1000 | Loss: 0.00001966
Iteration 155/1000 | Loss: 0.00001966
Iteration 156/1000 | Loss: 0.00001966
Iteration 157/1000 | Loss: 0.00001966
Iteration 158/1000 | Loss: 0.00001966
Iteration 159/1000 | Loss: 0.00001966
Iteration 160/1000 | Loss: 0.00001966
Iteration 161/1000 | Loss: 0.00001966
Iteration 162/1000 | Loss: 0.00001966
Iteration 163/1000 | Loss: 0.00001966
Iteration 164/1000 | Loss: 0.00001966
Iteration 165/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.9660768884932622e-05, 1.9660768884932622e-05, 1.9660768884932622e-05, 1.9660768884932622e-05, 1.9660768884932622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9660768884932622e-05

Optimization complete. Final v2v error: 3.7059905529022217 mm

Highest mean error: 3.846630811691284 mm for frame 0

Lowest mean error: 3.6426334381103516 mm for frame 50

Saving results

Total time: 76.34109330177307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907903
Iteration 2/25 | Loss: 0.00104775
Iteration 3/25 | Loss: 0.00087794
Iteration 4/25 | Loss: 0.00084548
Iteration 5/25 | Loss: 0.00083252
Iteration 6/25 | Loss: 0.00082975
Iteration 7/25 | Loss: 0.00082915
Iteration 8/25 | Loss: 0.00082915
Iteration 9/25 | Loss: 0.00082915
Iteration 10/25 | Loss: 0.00082915
Iteration 11/25 | Loss: 0.00082915
Iteration 12/25 | Loss: 0.00082915
Iteration 13/25 | Loss: 0.00082915
Iteration 14/25 | Loss: 0.00082915
Iteration 15/25 | Loss: 0.00082915
Iteration 16/25 | Loss: 0.00082915
Iteration 17/25 | Loss: 0.00082915
Iteration 18/25 | Loss: 0.00082915
Iteration 19/25 | Loss: 0.00082915
Iteration 20/25 | Loss: 0.00082915
Iteration 21/25 | Loss: 0.00082915
Iteration 22/25 | Loss: 0.00082915
Iteration 23/25 | Loss: 0.00082915
Iteration 24/25 | Loss: 0.00082915
Iteration 25/25 | Loss: 0.00082915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50664961
Iteration 2/25 | Loss: 0.00053533
Iteration 3/25 | Loss: 0.00053532
Iteration 4/25 | Loss: 0.00053532
Iteration 5/25 | Loss: 0.00053532
Iteration 6/25 | Loss: 0.00053532
Iteration 7/25 | Loss: 0.00053532
Iteration 8/25 | Loss: 0.00053532
Iteration 9/25 | Loss: 0.00053532
Iteration 10/25 | Loss: 0.00053532
Iteration 11/25 | Loss: 0.00053532
Iteration 12/25 | Loss: 0.00053532
Iteration 13/25 | Loss: 0.00053532
Iteration 14/25 | Loss: 0.00053532
Iteration 15/25 | Loss: 0.00053532
Iteration 16/25 | Loss: 0.00053532
Iteration 17/25 | Loss: 0.00053532
Iteration 18/25 | Loss: 0.00053532
Iteration 19/25 | Loss: 0.00053532
Iteration 20/25 | Loss: 0.00053532
Iteration 21/25 | Loss: 0.00053532
Iteration 22/25 | Loss: 0.00053532
Iteration 23/25 | Loss: 0.00053532
Iteration 24/25 | Loss: 0.00053532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005353194428607821, 0.0005353194428607821, 0.0005353194428607821, 0.0005353194428607821, 0.0005353194428607821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005353194428607821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053532
Iteration 2/1000 | Loss: 0.00003201
Iteration 3/1000 | Loss: 0.00001955
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001590
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001518
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001443
Iteration 26/1000 | Loss: 0.00001442
Iteration 27/1000 | Loss: 0.00001442
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001427
Iteration 99/1000 | Loss: 0.00001427
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001426
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001423
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001421
Iteration 145/1000 | Loss: 0.00001421
Iteration 146/1000 | Loss: 0.00001421
Iteration 147/1000 | Loss: 0.00001421
Iteration 148/1000 | Loss: 0.00001421
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001420
Iteration 155/1000 | Loss: 0.00001420
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001419
Iteration 163/1000 | Loss: 0.00001419
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.4178665878716856e-05, 1.4178665878716856e-05, 1.4178665878716856e-05, 1.4178665878716856e-05, 1.4178665878716856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4178665878716856e-05

Optimization complete. Final v2v error: 3.176631450653076 mm

Highest mean error: 4.038161277770996 mm for frame 175

Lowest mean error: 2.8861982822418213 mm for frame 78

Saving results

Total time: 44.20102310180664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937192
Iteration 2/25 | Loss: 0.00308199
Iteration 3/25 | Loss: 0.00231821
Iteration 4/25 | Loss: 0.00192515
Iteration 5/25 | Loss: 0.00168946
Iteration 6/25 | Loss: 0.00174818
Iteration 7/25 | Loss: 0.00178594
Iteration 8/25 | Loss: 0.00171469
Iteration 9/25 | Loss: 0.00156598
Iteration 10/25 | Loss: 0.00150929
Iteration 11/25 | Loss: 0.00148459
Iteration 12/25 | Loss: 0.00147542
Iteration 13/25 | Loss: 0.00143738
Iteration 14/25 | Loss: 0.00136781
Iteration 15/25 | Loss: 0.00136954
Iteration 16/25 | Loss: 0.00133043
Iteration 17/25 | Loss: 0.00131120
Iteration 18/25 | Loss: 0.00130745
Iteration 19/25 | Loss: 0.00135370
Iteration 20/25 | Loss: 0.00129626
Iteration 21/25 | Loss: 0.00127975
Iteration 22/25 | Loss: 0.00127128
Iteration 23/25 | Loss: 0.00129294
Iteration 24/25 | Loss: 0.00127086
Iteration 25/25 | Loss: 0.00126746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.48921824
Iteration 2/25 | Loss: 0.00671599
Iteration 3/25 | Loss: 0.00485940
Iteration 4/25 | Loss: 0.00485940
Iteration 5/25 | Loss: 0.00485940
Iteration 6/25 | Loss: 0.00485940
Iteration 7/25 | Loss: 0.00485940
Iteration 8/25 | Loss: 0.00485940
Iteration 9/25 | Loss: 0.00485940
Iteration 10/25 | Loss: 0.00485940
Iteration 11/25 | Loss: 0.00485940
Iteration 12/25 | Loss: 0.00485940
Iteration 13/25 | Loss: 0.00485940
Iteration 14/25 | Loss: 0.00485940
Iteration 15/25 | Loss: 0.00485940
Iteration 16/25 | Loss: 0.00485940
Iteration 17/25 | Loss: 0.00485940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004859397653490305, 0.004859397653490305, 0.004859397653490305, 0.004859397653490305, 0.004859397653490305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004859397653490305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00485940
Iteration 2/1000 | Loss: 0.00112461
Iteration 3/1000 | Loss: 0.00376458
Iteration 4/1000 | Loss: 0.00092132
Iteration 5/1000 | Loss: 0.00264501
Iteration 6/1000 | Loss: 0.00778331
Iteration 7/1000 | Loss: 0.00133574
Iteration 8/1000 | Loss: 0.00351504
Iteration 9/1000 | Loss: 0.01116873
Iteration 10/1000 | Loss: 0.00060639
Iteration 11/1000 | Loss: 0.00809794
Iteration 12/1000 | Loss: 0.00209244
Iteration 13/1000 | Loss: 0.00343641
Iteration 14/1000 | Loss: 0.00364084
Iteration 15/1000 | Loss: 0.00415590
Iteration 16/1000 | Loss: 0.00292494
Iteration 17/1000 | Loss: 0.00194921
Iteration 18/1000 | Loss: 0.00086998
Iteration 19/1000 | Loss: 0.00368578
Iteration 20/1000 | Loss: 0.00216769
Iteration 21/1000 | Loss: 0.00239032
Iteration 22/1000 | Loss: 0.00367067
Iteration 23/1000 | Loss: 0.00233497
Iteration 24/1000 | Loss: 0.00206703
Iteration 25/1000 | Loss: 0.00179258
Iteration 26/1000 | Loss: 0.00076015
Iteration 27/1000 | Loss: 0.00054205
Iteration 28/1000 | Loss: 0.00114009
Iteration 29/1000 | Loss: 0.00103993
Iteration 30/1000 | Loss: 0.00182954
Iteration 31/1000 | Loss: 0.00156828
Iteration 32/1000 | Loss: 0.00156654
Iteration 33/1000 | Loss: 0.00126472
Iteration 34/1000 | Loss: 0.00173325
Iteration 35/1000 | Loss: 0.00186914
Iteration 36/1000 | Loss: 0.00151607
Iteration 37/1000 | Loss: 0.00212087
Iteration 38/1000 | Loss: 0.00234940
Iteration 39/1000 | Loss: 0.00204475
Iteration 40/1000 | Loss: 0.00250944
Iteration 41/1000 | Loss: 0.00528208
Iteration 42/1000 | Loss: 0.00292734
Iteration 43/1000 | Loss: 0.00302497
Iteration 44/1000 | Loss: 0.00296746
Iteration 45/1000 | Loss: 0.00175818
Iteration 46/1000 | Loss: 0.00110092
Iteration 47/1000 | Loss: 0.00051247
Iteration 48/1000 | Loss: 0.00114720
Iteration 49/1000 | Loss: 0.00130199
Iteration 50/1000 | Loss: 0.00160981
Iteration 51/1000 | Loss: 0.00165214
Iteration 52/1000 | Loss: 0.00157696
Iteration 53/1000 | Loss: 0.00175111
Iteration 54/1000 | Loss: 0.00118550
Iteration 55/1000 | Loss: 0.00113490
Iteration 56/1000 | Loss: 0.00162964
Iteration 57/1000 | Loss: 0.00216065
Iteration 58/1000 | Loss: 0.00203947
Iteration 59/1000 | Loss: 0.00084182
Iteration 60/1000 | Loss: 0.00161358
Iteration 61/1000 | Loss: 0.00067303
Iteration 62/1000 | Loss: 0.00189397
Iteration 63/1000 | Loss: 0.00112716
Iteration 64/1000 | Loss: 0.00050983
Iteration 65/1000 | Loss: 0.00053094
Iteration 66/1000 | Loss: 0.00154762
Iteration 67/1000 | Loss: 0.00074774
Iteration 68/1000 | Loss: 0.00088789
Iteration 69/1000 | Loss: 0.00064752
Iteration 70/1000 | Loss: 0.00098655
Iteration 71/1000 | Loss: 0.00107658
Iteration 72/1000 | Loss: 0.00132154
Iteration 73/1000 | Loss: 0.00044359
Iteration 74/1000 | Loss: 0.00087877
Iteration 75/1000 | Loss: 0.00037293
Iteration 76/1000 | Loss: 0.00061177
Iteration 77/1000 | Loss: 0.00140289
Iteration 78/1000 | Loss: 0.00096121
Iteration 79/1000 | Loss: 0.00551065
Iteration 80/1000 | Loss: 0.00231973
Iteration 81/1000 | Loss: 0.00348641
Iteration 82/1000 | Loss: 0.00085283
Iteration 83/1000 | Loss: 0.00269874
Iteration 84/1000 | Loss: 0.00118699
Iteration 85/1000 | Loss: 0.00298336
Iteration 86/1000 | Loss: 0.01131122
Iteration 87/1000 | Loss: 0.01098468
Iteration 88/1000 | Loss: 0.00318638
Iteration 89/1000 | Loss: 0.00215296
Iteration 90/1000 | Loss: 0.00256118
Iteration 91/1000 | Loss: 0.00177996
Iteration 92/1000 | Loss: 0.00270574
Iteration 93/1000 | Loss: 0.00043117
Iteration 94/1000 | Loss: 0.00129158
Iteration 95/1000 | Loss: 0.00286881
Iteration 96/1000 | Loss: 0.00098783
Iteration 97/1000 | Loss: 0.00065268
Iteration 98/1000 | Loss: 0.00050881
Iteration 99/1000 | Loss: 0.00074286
Iteration 100/1000 | Loss: 0.00060298
Iteration 101/1000 | Loss: 0.00078510
Iteration 102/1000 | Loss: 0.00055265
Iteration 103/1000 | Loss: 0.00053069
Iteration 104/1000 | Loss: 0.00014897
Iteration 105/1000 | Loss: 0.00014861
Iteration 106/1000 | Loss: 0.00052228
Iteration 107/1000 | Loss: 0.00089301
Iteration 108/1000 | Loss: 0.00266160
Iteration 109/1000 | Loss: 0.00052192
Iteration 110/1000 | Loss: 0.00271387
Iteration 111/1000 | Loss: 0.00037901
Iteration 112/1000 | Loss: 0.00039547
Iteration 113/1000 | Loss: 0.00039624
Iteration 114/1000 | Loss: 0.00014804
Iteration 115/1000 | Loss: 0.00046443
Iteration 116/1000 | Loss: 0.00007451
Iteration 117/1000 | Loss: 0.00006524
Iteration 118/1000 | Loss: 0.00073000
Iteration 119/1000 | Loss: 0.00013594
Iteration 120/1000 | Loss: 0.00008278
Iteration 121/1000 | Loss: 0.00005251
Iteration 122/1000 | Loss: 0.00004966
Iteration 123/1000 | Loss: 0.00004871
Iteration 124/1000 | Loss: 0.00004437
Iteration 125/1000 | Loss: 0.00115102
Iteration 126/1000 | Loss: 0.00102995
Iteration 127/1000 | Loss: 0.00061241
Iteration 128/1000 | Loss: 0.00067647
Iteration 129/1000 | Loss: 0.00017877
Iteration 130/1000 | Loss: 0.00005276
Iteration 131/1000 | Loss: 0.00004115
Iteration 132/1000 | Loss: 0.00130394
Iteration 133/1000 | Loss: 0.00050774
Iteration 134/1000 | Loss: 0.00003617
Iteration 135/1000 | Loss: 0.00003383
Iteration 136/1000 | Loss: 0.00003212
Iteration 137/1000 | Loss: 0.00004328
Iteration 138/1000 | Loss: 0.00002989
Iteration 139/1000 | Loss: 0.00002902
Iteration 140/1000 | Loss: 0.00002841
Iteration 141/1000 | Loss: 0.00129839
Iteration 142/1000 | Loss: 0.00044002
Iteration 143/1000 | Loss: 0.00042081
Iteration 144/1000 | Loss: 0.00091023
Iteration 145/1000 | Loss: 0.00041152
Iteration 146/1000 | Loss: 0.00007898
Iteration 147/1000 | Loss: 0.00003724
Iteration 148/1000 | Loss: 0.00003289
Iteration 149/1000 | Loss: 0.00002993
Iteration 150/1000 | Loss: 0.00002794
Iteration 151/1000 | Loss: 0.00002708
Iteration 152/1000 | Loss: 0.00059744
Iteration 153/1000 | Loss: 0.00005246
Iteration 154/1000 | Loss: 0.00002975
Iteration 155/1000 | Loss: 0.00002398
Iteration 156/1000 | Loss: 0.00002357
Iteration 157/1000 | Loss: 0.00003815
Iteration 158/1000 | Loss: 0.00002474
Iteration 159/1000 | Loss: 0.00002283
Iteration 160/1000 | Loss: 0.00002270
Iteration 161/1000 | Loss: 0.00002259
Iteration 162/1000 | Loss: 0.00002252
Iteration 163/1000 | Loss: 0.00002240
Iteration 164/1000 | Loss: 0.00002229
Iteration 165/1000 | Loss: 0.00002228
Iteration 166/1000 | Loss: 0.00002228
Iteration 167/1000 | Loss: 0.00002227
Iteration 168/1000 | Loss: 0.00002226
Iteration 169/1000 | Loss: 0.00002226
Iteration 170/1000 | Loss: 0.00002225
Iteration 171/1000 | Loss: 0.00002225
Iteration 172/1000 | Loss: 0.00002224
Iteration 173/1000 | Loss: 0.00002223
Iteration 174/1000 | Loss: 0.00002222
Iteration 175/1000 | Loss: 0.00002221
Iteration 176/1000 | Loss: 0.00002221
Iteration 177/1000 | Loss: 0.00002220
Iteration 178/1000 | Loss: 0.00002220
Iteration 179/1000 | Loss: 0.00002219
Iteration 180/1000 | Loss: 0.00002219
Iteration 181/1000 | Loss: 0.00002218
Iteration 182/1000 | Loss: 0.00002217
Iteration 183/1000 | Loss: 0.00002217
Iteration 184/1000 | Loss: 0.00002217
Iteration 185/1000 | Loss: 0.00002217
Iteration 186/1000 | Loss: 0.00002216
Iteration 187/1000 | Loss: 0.00002216
Iteration 188/1000 | Loss: 0.00002216
Iteration 189/1000 | Loss: 0.00002216
Iteration 190/1000 | Loss: 0.00002216
Iteration 191/1000 | Loss: 0.00002216
Iteration 192/1000 | Loss: 0.00002216
Iteration 193/1000 | Loss: 0.00002215
Iteration 194/1000 | Loss: 0.00002215
Iteration 195/1000 | Loss: 0.00002215
Iteration 196/1000 | Loss: 0.00002215
Iteration 197/1000 | Loss: 0.00002215
Iteration 198/1000 | Loss: 0.00002215
Iteration 199/1000 | Loss: 0.00002214
Iteration 200/1000 | Loss: 0.00002214
Iteration 201/1000 | Loss: 0.00002214
Iteration 202/1000 | Loss: 0.00002214
Iteration 203/1000 | Loss: 0.00002214
Iteration 204/1000 | Loss: 0.00002213
Iteration 205/1000 | Loss: 0.00002213
Iteration 206/1000 | Loss: 0.00002213
Iteration 207/1000 | Loss: 0.00002213
Iteration 208/1000 | Loss: 0.00002212
Iteration 209/1000 | Loss: 0.00002212
Iteration 210/1000 | Loss: 0.00002212
Iteration 211/1000 | Loss: 0.00002212
Iteration 212/1000 | Loss: 0.00002212
Iteration 213/1000 | Loss: 0.00002212
Iteration 214/1000 | Loss: 0.00002212
Iteration 215/1000 | Loss: 0.00002212
Iteration 216/1000 | Loss: 0.00002211
Iteration 217/1000 | Loss: 0.00002211
Iteration 218/1000 | Loss: 0.00002211
Iteration 219/1000 | Loss: 0.00002211
Iteration 220/1000 | Loss: 0.00002211
Iteration 221/1000 | Loss: 0.00002211
Iteration 222/1000 | Loss: 0.00002211
Iteration 223/1000 | Loss: 0.00002211
Iteration 224/1000 | Loss: 0.00002211
Iteration 225/1000 | Loss: 0.00002211
Iteration 226/1000 | Loss: 0.00002211
Iteration 227/1000 | Loss: 0.00002211
Iteration 228/1000 | Loss: 0.00002211
Iteration 229/1000 | Loss: 0.00002210
Iteration 230/1000 | Loss: 0.00002210
Iteration 231/1000 | Loss: 0.00002210
Iteration 232/1000 | Loss: 0.00002210
Iteration 233/1000 | Loss: 0.00002210
Iteration 234/1000 | Loss: 0.00002209
Iteration 235/1000 | Loss: 0.00002209
Iteration 236/1000 | Loss: 0.00002209
Iteration 237/1000 | Loss: 0.00002209
Iteration 238/1000 | Loss: 0.00002209
Iteration 239/1000 | Loss: 0.00002209
Iteration 240/1000 | Loss: 0.00002209
Iteration 241/1000 | Loss: 0.00002209
Iteration 242/1000 | Loss: 0.00002209
Iteration 243/1000 | Loss: 0.00002209
Iteration 244/1000 | Loss: 0.00002208
Iteration 245/1000 | Loss: 0.00002208
Iteration 246/1000 | Loss: 0.00002208
Iteration 247/1000 | Loss: 0.00002208
Iteration 248/1000 | Loss: 0.00002208
Iteration 249/1000 | Loss: 0.00002208
Iteration 250/1000 | Loss: 0.00002208
Iteration 251/1000 | Loss: 0.00002208
Iteration 252/1000 | Loss: 0.00002208
Iteration 253/1000 | Loss: 0.00002208
Iteration 254/1000 | Loss: 0.00002208
Iteration 255/1000 | Loss: 0.00002208
Iteration 256/1000 | Loss: 0.00002208
Iteration 257/1000 | Loss: 0.00002207
Iteration 258/1000 | Loss: 0.00002207
Iteration 259/1000 | Loss: 0.00002207
Iteration 260/1000 | Loss: 0.00002207
Iteration 261/1000 | Loss: 0.00002207
Iteration 262/1000 | Loss: 0.00002207
Iteration 263/1000 | Loss: 0.00002207
Iteration 264/1000 | Loss: 0.00002207
Iteration 265/1000 | Loss: 0.00002206
Iteration 266/1000 | Loss: 0.00002206
Iteration 267/1000 | Loss: 0.00002206
Iteration 268/1000 | Loss: 0.00002206
Iteration 269/1000 | Loss: 0.00002206
Iteration 270/1000 | Loss: 0.00002206
Iteration 271/1000 | Loss: 0.00002205
Iteration 272/1000 | Loss: 0.00002205
Iteration 273/1000 | Loss: 0.00002205
Iteration 274/1000 | Loss: 0.00002205
Iteration 275/1000 | Loss: 0.00002205
Iteration 276/1000 | Loss: 0.00002205
Iteration 277/1000 | Loss: 0.00002205
Iteration 278/1000 | Loss: 0.00002205
Iteration 279/1000 | Loss: 0.00002205
Iteration 280/1000 | Loss: 0.00002205
Iteration 281/1000 | Loss: 0.00002204
Iteration 282/1000 | Loss: 0.00002204
Iteration 283/1000 | Loss: 0.00002204
Iteration 284/1000 | Loss: 0.00002203
Iteration 285/1000 | Loss: 0.00002203
Iteration 286/1000 | Loss: 0.00002203
Iteration 287/1000 | Loss: 0.00002202
Iteration 288/1000 | Loss: 0.00002202
Iteration 289/1000 | Loss: 0.00002202
Iteration 290/1000 | Loss: 0.00002202
Iteration 291/1000 | Loss: 0.00002202
Iteration 292/1000 | Loss: 0.00002202
Iteration 293/1000 | Loss: 0.00002201
Iteration 294/1000 | Loss: 0.00002201
Iteration 295/1000 | Loss: 0.00002201
Iteration 296/1000 | Loss: 0.00002200
Iteration 297/1000 | Loss: 0.00002200
Iteration 298/1000 | Loss: 0.00002199
Iteration 299/1000 | Loss: 0.00002199
Iteration 300/1000 | Loss: 0.00002199
Iteration 301/1000 | Loss: 0.00002198
Iteration 302/1000 | Loss: 0.00002198
Iteration 303/1000 | Loss: 0.00002198
Iteration 304/1000 | Loss: 0.00002197
Iteration 305/1000 | Loss: 0.00002197
Iteration 306/1000 | Loss: 0.00002197
Iteration 307/1000 | Loss: 0.00002197
Iteration 308/1000 | Loss: 0.00002197
Iteration 309/1000 | Loss: 0.00002197
Iteration 310/1000 | Loss: 0.00002197
Iteration 311/1000 | Loss: 0.00002197
Iteration 312/1000 | Loss: 0.00002196
Iteration 313/1000 | Loss: 0.00002196
Iteration 314/1000 | Loss: 0.00002196
Iteration 315/1000 | Loss: 0.00002196
Iteration 316/1000 | Loss: 0.00002196
Iteration 317/1000 | Loss: 0.00002196
Iteration 318/1000 | Loss: 0.00002196
Iteration 319/1000 | Loss: 0.00002195
Iteration 320/1000 | Loss: 0.00002195
Iteration 321/1000 | Loss: 0.00002195
Iteration 322/1000 | Loss: 0.00002195
Iteration 323/1000 | Loss: 0.00002195
Iteration 324/1000 | Loss: 0.00002195
Iteration 325/1000 | Loss: 0.00002195
Iteration 326/1000 | Loss: 0.00002194
Iteration 327/1000 | Loss: 0.00002194
Iteration 328/1000 | Loss: 0.00002194
Iteration 329/1000 | Loss: 0.00002194
Iteration 330/1000 | Loss: 0.00002194
Iteration 331/1000 | Loss: 0.00002194
Iteration 332/1000 | Loss: 0.00002194
Iteration 333/1000 | Loss: 0.00002194
Iteration 334/1000 | Loss: 0.00002194
Iteration 335/1000 | Loss: 0.00002194
Iteration 336/1000 | Loss: 0.00002194
Iteration 337/1000 | Loss: 0.00002194
Iteration 338/1000 | Loss: 0.00002194
Iteration 339/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 339. Stopping optimization.
Last 5 losses: [2.1937221390544437e-05, 2.1937221390544437e-05, 2.1937221390544437e-05, 2.1937221390544437e-05, 2.1937221390544437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1937221390544437e-05

Optimization complete. Final v2v error: 3.8335442543029785 mm

Highest mean error: 5.649113655090332 mm for frame 70

Lowest mean error: 2.879781484603882 mm for frame 21

Saving results

Total time: 300.72826862335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777063
Iteration 2/25 | Loss: 0.00155464
Iteration 3/25 | Loss: 0.00114515
Iteration 4/25 | Loss: 0.00103688
Iteration 5/25 | Loss: 0.00101275
Iteration 6/25 | Loss: 0.00100633
Iteration 7/25 | Loss: 0.00100476
Iteration 8/25 | Loss: 0.00100474
Iteration 9/25 | Loss: 0.00100474
Iteration 10/25 | Loss: 0.00100474
Iteration 11/25 | Loss: 0.00100474
Iteration 12/25 | Loss: 0.00100474
Iteration 13/25 | Loss: 0.00100474
Iteration 14/25 | Loss: 0.00100474
Iteration 15/25 | Loss: 0.00100474
Iteration 16/25 | Loss: 0.00100474
Iteration 17/25 | Loss: 0.00100474
Iteration 18/25 | Loss: 0.00100474
Iteration 19/25 | Loss: 0.00100474
Iteration 20/25 | Loss: 0.00100474
Iteration 21/25 | Loss: 0.00100474
Iteration 22/25 | Loss: 0.00100474
Iteration 23/25 | Loss: 0.00100474
Iteration 24/25 | Loss: 0.00100474
Iteration 25/25 | Loss: 0.00100474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36572087
Iteration 2/25 | Loss: 0.00058860
Iteration 3/25 | Loss: 0.00058860
Iteration 4/25 | Loss: 0.00058860
Iteration 5/25 | Loss: 0.00058860
Iteration 6/25 | Loss: 0.00058859
Iteration 7/25 | Loss: 0.00058859
Iteration 8/25 | Loss: 0.00058859
Iteration 9/25 | Loss: 0.00058859
Iteration 10/25 | Loss: 0.00058859
Iteration 11/25 | Loss: 0.00058859
Iteration 12/25 | Loss: 0.00058859
Iteration 13/25 | Loss: 0.00058859
Iteration 14/25 | Loss: 0.00058859
Iteration 15/25 | Loss: 0.00058859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005885940627194941, 0.0005885940627194941, 0.0005885940627194941, 0.0005885940627194941, 0.0005885940627194941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005885940627194941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058859
Iteration 2/1000 | Loss: 0.00006908
Iteration 3/1000 | Loss: 0.00004730
Iteration 4/1000 | Loss: 0.00004108
Iteration 5/1000 | Loss: 0.00003869
Iteration 6/1000 | Loss: 0.00003695
Iteration 7/1000 | Loss: 0.00003554
Iteration 8/1000 | Loss: 0.00003474
Iteration 9/1000 | Loss: 0.00003404
Iteration 10/1000 | Loss: 0.00003362
Iteration 11/1000 | Loss: 0.00003325
Iteration 12/1000 | Loss: 0.00003302
Iteration 13/1000 | Loss: 0.00003279
Iteration 14/1000 | Loss: 0.00003263
Iteration 15/1000 | Loss: 0.00003248
Iteration 16/1000 | Loss: 0.00003247
Iteration 17/1000 | Loss: 0.00003236
Iteration 18/1000 | Loss: 0.00003235
Iteration 19/1000 | Loss: 0.00003232
Iteration 20/1000 | Loss: 0.00003228
Iteration 21/1000 | Loss: 0.00003222
Iteration 22/1000 | Loss: 0.00003222
Iteration 23/1000 | Loss: 0.00003221
Iteration 24/1000 | Loss: 0.00003220
Iteration 25/1000 | Loss: 0.00003219
Iteration 26/1000 | Loss: 0.00003212
Iteration 27/1000 | Loss: 0.00003206
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003206
Iteration 30/1000 | Loss: 0.00003206
Iteration 31/1000 | Loss: 0.00003205
Iteration 32/1000 | Loss: 0.00003205
Iteration 33/1000 | Loss: 0.00003204
Iteration 34/1000 | Loss: 0.00003204
Iteration 35/1000 | Loss: 0.00003203
Iteration 36/1000 | Loss: 0.00003203
Iteration 37/1000 | Loss: 0.00003203
Iteration 38/1000 | Loss: 0.00003203
Iteration 39/1000 | Loss: 0.00003202
Iteration 40/1000 | Loss: 0.00003202
Iteration 41/1000 | Loss: 0.00003202
Iteration 42/1000 | Loss: 0.00003202
Iteration 43/1000 | Loss: 0.00003202
Iteration 44/1000 | Loss: 0.00003201
Iteration 45/1000 | Loss: 0.00003201
Iteration 46/1000 | Loss: 0.00003201
Iteration 47/1000 | Loss: 0.00003201
Iteration 48/1000 | Loss: 0.00003200
Iteration 49/1000 | Loss: 0.00003200
Iteration 50/1000 | Loss: 0.00003199
Iteration 51/1000 | Loss: 0.00003199
Iteration 52/1000 | Loss: 0.00003199
Iteration 53/1000 | Loss: 0.00003199
Iteration 54/1000 | Loss: 0.00003199
Iteration 55/1000 | Loss: 0.00003199
Iteration 56/1000 | Loss: 0.00003199
Iteration 57/1000 | Loss: 0.00003198
Iteration 58/1000 | Loss: 0.00003198
Iteration 59/1000 | Loss: 0.00003198
Iteration 60/1000 | Loss: 0.00003197
Iteration 61/1000 | Loss: 0.00003197
Iteration 62/1000 | Loss: 0.00003196
Iteration 63/1000 | Loss: 0.00003196
Iteration 64/1000 | Loss: 0.00003195
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003194
Iteration 67/1000 | Loss: 0.00003194
Iteration 68/1000 | Loss: 0.00003194
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003194
Iteration 71/1000 | Loss: 0.00003193
Iteration 72/1000 | Loss: 0.00003193
Iteration 73/1000 | Loss: 0.00003193
Iteration 74/1000 | Loss: 0.00003193
Iteration 75/1000 | Loss: 0.00003193
Iteration 76/1000 | Loss: 0.00003193
Iteration 77/1000 | Loss: 0.00003193
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003193
Iteration 80/1000 | Loss: 0.00003192
Iteration 81/1000 | Loss: 0.00003192
Iteration 82/1000 | Loss: 0.00003192
Iteration 83/1000 | Loss: 0.00003191
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00003191
Iteration 86/1000 | Loss: 0.00003191
Iteration 87/1000 | Loss: 0.00003191
Iteration 88/1000 | Loss: 0.00003191
Iteration 89/1000 | Loss: 0.00003191
Iteration 90/1000 | Loss: 0.00003191
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003191
Iteration 93/1000 | Loss: 0.00003191
Iteration 94/1000 | Loss: 0.00003190
Iteration 95/1000 | Loss: 0.00003190
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003190
Iteration 98/1000 | Loss: 0.00003190
Iteration 99/1000 | Loss: 0.00003190
Iteration 100/1000 | Loss: 0.00003189
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003189
Iteration 104/1000 | Loss: 0.00003189
Iteration 105/1000 | Loss: 0.00003189
Iteration 106/1000 | Loss: 0.00003188
Iteration 107/1000 | Loss: 0.00003188
Iteration 108/1000 | Loss: 0.00003188
Iteration 109/1000 | Loss: 0.00003188
Iteration 110/1000 | Loss: 0.00003188
Iteration 111/1000 | Loss: 0.00003188
Iteration 112/1000 | Loss: 0.00003188
Iteration 113/1000 | Loss: 0.00003188
Iteration 114/1000 | Loss: 0.00003188
Iteration 115/1000 | Loss: 0.00003188
Iteration 116/1000 | Loss: 0.00003187
Iteration 117/1000 | Loss: 0.00003187
Iteration 118/1000 | Loss: 0.00003187
Iteration 119/1000 | Loss: 0.00003187
Iteration 120/1000 | Loss: 0.00003186
Iteration 121/1000 | Loss: 0.00003186
Iteration 122/1000 | Loss: 0.00003186
Iteration 123/1000 | Loss: 0.00003186
Iteration 124/1000 | Loss: 0.00003186
Iteration 125/1000 | Loss: 0.00003186
Iteration 126/1000 | Loss: 0.00003186
Iteration 127/1000 | Loss: 0.00003185
Iteration 128/1000 | Loss: 0.00003185
Iteration 129/1000 | Loss: 0.00003185
Iteration 130/1000 | Loss: 0.00003185
Iteration 131/1000 | Loss: 0.00003185
Iteration 132/1000 | Loss: 0.00003185
Iteration 133/1000 | Loss: 0.00003185
Iteration 134/1000 | Loss: 0.00003184
Iteration 135/1000 | Loss: 0.00003184
Iteration 136/1000 | Loss: 0.00003184
Iteration 137/1000 | Loss: 0.00003184
Iteration 138/1000 | Loss: 0.00003184
Iteration 139/1000 | Loss: 0.00003184
Iteration 140/1000 | Loss: 0.00003184
Iteration 141/1000 | Loss: 0.00003184
Iteration 142/1000 | Loss: 0.00003184
Iteration 143/1000 | Loss: 0.00003184
Iteration 144/1000 | Loss: 0.00003183
Iteration 145/1000 | Loss: 0.00003183
Iteration 146/1000 | Loss: 0.00003183
Iteration 147/1000 | Loss: 0.00003183
Iteration 148/1000 | Loss: 0.00003183
Iteration 149/1000 | Loss: 0.00003183
Iteration 150/1000 | Loss: 0.00003183
Iteration 151/1000 | Loss: 0.00003183
Iteration 152/1000 | Loss: 0.00003183
Iteration 153/1000 | Loss: 0.00003183
Iteration 154/1000 | Loss: 0.00003183
Iteration 155/1000 | Loss: 0.00003183
Iteration 156/1000 | Loss: 0.00003183
Iteration 157/1000 | Loss: 0.00003183
Iteration 158/1000 | Loss: 0.00003183
Iteration 159/1000 | Loss: 0.00003183
Iteration 160/1000 | Loss: 0.00003183
Iteration 161/1000 | Loss: 0.00003182
Iteration 162/1000 | Loss: 0.00003182
Iteration 163/1000 | Loss: 0.00003182
Iteration 164/1000 | Loss: 0.00003182
Iteration 165/1000 | Loss: 0.00003182
Iteration 166/1000 | Loss: 0.00003182
Iteration 167/1000 | Loss: 0.00003182
Iteration 168/1000 | Loss: 0.00003182
Iteration 169/1000 | Loss: 0.00003182
Iteration 170/1000 | Loss: 0.00003181
Iteration 171/1000 | Loss: 0.00003181
Iteration 172/1000 | Loss: 0.00003181
Iteration 173/1000 | Loss: 0.00003181
Iteration 174/1000 | Loss: 0.00003181
Iteration 175/1000 | Loss: 0.00003180
Iteration 176/1000 | Loss: 0.00003180
Iteration 177/1000 | Loss: 0.00003180
Iteration 178/1000 | Loss: 0.00003180
Iteration 179/1000 | Loss: 0.00003180
Iteration 180/1000 | Loss: 0.00003180
Iteration 181/1000 | Loss: 0.00003180
Iteration 182/1000 | Loss: 0.00003180
Iteration 183/1000 | Loss: 0.00003180
Iteration 184/1000 | Loss: 0.00003180
Iteration 185/1000 | Loss: 0.00003180
Iteration 186/1000 | Loss: 0.00003180
Iteration 187/1000 | Loss: 0.00003179
Iteration 188/1000 | Loss: 0.00003179
Iteration 189/1000 | Loss: 0.00003179
Iteration 190/1000 | Loss: 0.00003179
Iteration 191/1000 | Loss: 0.00003179
Iteration 192/1000 | Loss: 0.00003179
Iteration 193/1000 | Loss: 0.00003179
Iteration 194/1000 | Loss: 0.00003179
Iteration 195/1000 | Loss: 0.00003179
Iteration 196/1000 | Loss: 0.00003179
Iteration 197/1000 | Loss: 0.00003179
Iteration 198/1000 | Loss: 0.00003179
Iteration 199/1000 | Loss: 0.00003179
Iteration 200/1000 | Loss: 0.00003178
Iteration 201/1000 | Loss: 0.00003178
Iteration 202/1000 | Loss: 0.00003178
Iteration 203/1000 | Loss: 0.00003178
Iteration 204/1000 | Loss: 0.00003178
Iteration 205/1000 | Loss: 0.00003178
Iteration 206/1000 | Loss: 0.00003178
Iteration 207/1000 | Loss: 0.00003178
Iteration 208/1000 | Loss: 0.00003178
Iteration 209/1000 | Loss: 0.00003178
Iteration 210/1000 | Loss: 0.00003178
Iteration 211/1000 | Loss: 0.00003178
Iteration 212/1000 | Loss: 0.00003178
Iteration 213/1000 | Loss: 0.00003178
Iteration 214/1000 | Loss: 0.00003178
Iteration 215/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [3.177822145516984e-05, 3.177822145516984e-05, 3.177822145516984e-05, 3.177822145516984e-05, 3.177822145516984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.177822145516984e-05

Optimization complete. Final v2v error: 4.674739360809326 mm

Highest mean error: 6.127645492553711 mm for frame 53

Lowest mean error: 3.77066707611084 mm for frame 239

Saving results

Total time: 57.80184292793274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481264
Iteration 2/25 | Loss: 0.00090858
Iteration 3/25 | Loss: 0.00083339
Iteration 4/25 | Loss: 0.00082236
Iteration 5/25 | Loss: 0.00081867
Iteration 6/25 | Loss: 0.00081759
Iteration 7/25 | Loss: 0.00081742
Iteration 8/25 | Loss: 0.00081742
Iteration 9/25 | Loss: 0.00081742
Iteration 10/25 | Loss: 0.00081742
Iteration 11/25 | Loss: 0.00081742
Iteration 12/25 | Loss: 0.00081742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00081742083420977, 0.00081742083420977, 0.00081742083420977, 0.00081742083420977, 0.00081742083420977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00081742083420977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.22100687
Iteration 2/25 | Loss: 0.00051333
Iteration 3/25 | Loss: 0.00051333
Iteration 4/25 | Loss: 0.00051332
Iteration 5/25 | Loss: 0.00051332
Iteration 6/25 | Loss: 0.00051332
Iteration 7/25 | Loss: 0.00051332
Iteration 8/25 | Loss: 0.00051332
Iteration 9/25 | Loss: 0.00051332
Iteration 10/25 | Loss: 0.00051332
Iteration 11/25 | Loss: 0.00051332
Iteration 12/25 | Loss: 0.00051332
Iteration 13/25 | Loss: 0.00051332
Iteration 14/25 | Loss: 0.00051332
Iteration 15/25 | Loss: 0.00051332
Iteration 16/25 | Loss: 0.00051332
Iteration 17/25 | Loss: 0.00051332
Iteration 18/25 | Loss: 0.00051332
Iteration 19/25 | Loss: 0.00051332
Iteration 20/25 | Loss: 0.00051332
Iteration 21/25 | Loss: 0.00051332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005133214872330427, 0.0005133214872330427, 0.0005133214872330427, 0.0005133214872330427, 0.0005133214872330427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005133214872330427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051332
Iteration 2/1000 | Loss: 0.00002839
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001292
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.2890991456515621e-05, 1.2890991456515621e-05, 1.2890991456515621e-05, 1.2890991456515621e-05, 1.2890991456515621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2890991456515621e-05

Optimization complete. Final v2v error: 3.091202974319458 mm

Highest mean error: 3.4890494346618652 mm for frame 66

Lowest mean error: 2.8631627559661865 mm for frame 1

Saving results

Total time: 34.40444254875183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023046
Iteration 2/25 | Loss: 0.00386371
Iteration 3/25 | Loss: 0.00212720
Iteration 4/25 | Loss: 0.00184495
Iteration 5/25 | Loss: 0.00165079
Iteration 6/25 | Loss: 0.00165422
Iteration 7/25 | Loss: 0.00162046
Iteration 8/25 | Loss: 0.00159178
Iteration 9/25 | Loss: 0.00151446
Iteration 10/25 | Loss: 0.00144089
Iteration 11/25 | Loss: 0.00140582
Iteration 12/25 | Loss: 0.00137193
Iteration 13/25 | Loss: 0.00135874
Iteration 14/25 | Loss: 0.00131100
Iteration 15/25 | Loss: 0.00128914
Iteration 16/25 | Loss: 0.00125012
Iteration 17/25 | Loss: 0.00122800
Iteration 18/25 | Loss: 0.00121453
Iteration 19/25 | Loss: 0.00120732
Iteration 20/25 | Loss: 0.00120480
Iteration 21/25 | Loss: 0.00118285
Iteration 22/25 | Loss: 0.00118475
Iteration 23/25 | Loss: 0.00117694
Iteration 24/25 | Loss: 0.00116532
Iteration 25/25 | Loss: 0.00116260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51916361
Iteration 2/25 | Loss: 0.00691208
Iteration 3/25 | Loss: 0.00414975
Iteration 4/25 | Loss: 0.00414974
Iteration 5/25 | Loss: 0.00414974
Iteration 6/25 | Loss: 0.00414974
Iteration 7/25 | Loss: 0.00414974
Iteration 8/25 | Loss: 0.00414974
Iteration 9/25 | Loss: 0.00414974
Iteration 10/25 | Loss: 0.00414974
Iteration 11/25 | Loss: 0.00414974
Iteration 12/25 | Loss: 0.00414974
Iteration 13/25 | Loss: 0.00414974
Iteration 14/25 | Loss: 0.00414974
Iteration 15/25 | Loss: 0.00414974
Iteration 16/25 | Loss: 0.00414974
Iteration 17/25 | Loss: 0.00414974
Iteration 18/25 | Loss: 0.00414974
Iteration 19/25 | Loss: 0.00414974
Iteration 20/25 | Loss: 0.00414974
Iteration 21/25 | Loss: 0.00414974
Iteration 22/25 | Loss: 0.00414974
Iteration 23/25 | Loss: 0.00414974
Iteration 24/25 | Loss: 0.00414974
Iteration 25/25 | Loss: 0.00414974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00414974
Iteration 2/1000 | Loss: 0.00459293
Iteration 3/1000 | Loss: 0.00116331
Iteration 4/1000 | Loss: 0.00144145
Iteration 5/1000 | Loss: 0.00070394
Iteration 6/1000 | Loss: 0.00060402
Iteration 7/1000 | Loss: 0.00052313
Iteration 8/1000 | Loss: 0.00125580
Iteration 9/1000 | Loss: 0.00101066
Iteration 10/1000 | Loss: 0.00037111
Iteration 11/1000 | Loss: 0.00067178
Iteration 12/1000 | Loss: 0.00248172
Iteration 13/1000 | Loss: 0.00126141
Iteration 14/1000 | Loss: 0.00252424
Iteration 15/1000 | Loss: 0.00384796
Iteration 16/1000 | Loss: 0.00157999
Iteration 17/1000 | Loss: 0.00080762
Iteration 18/1000 | Loss: 0.00060568
Iteration 19/1000 | Loss: 0.00024320
Iteration 20/1000 | Loss: 0.00087440
Iteration 21/1000 | Loss: 0.00050587
Iteration 22/1000 | Loss: 0.00068270
Iteration 23/1000 | Loss: 0.00085541
Iteration 24/1000 | Loss: 0.00037633
Iteration 25/1000 | Loss: 0.00015946
Iteration 26/1000 | Loss: 0.00024364
Iteration 27/1000 | Loss: 0.00146504
Iteration 28/1000 | Loss: 0.00057251
Iteration 29/1000 | Loss: 0.00061714
Iteration 30/1000 | Loss: 0.00067139
Iteration 31/1000 | Loss: 0.00092080
Iteration 32/1000 | Loss: 0.00029715
Iteration 33/1000 | Loss: 0.00014756
Iteration 34/1000 | Loss: 0.00087214
Iteration 35/1000 | Loss: 0.00141251
Iteration 36/1000 | Loss: 0.00095143
Iteration 37/1000 | Loss: 0.00121173
Iteration 38/1000 | Loss: 0.00040395
Iteration 39/1000 | Loss: 0.00097328
Iteration 40/1000 | Loss: 0.00075439
Iteration 41/1000 | Loss: 0.00042634
Iteration 42/1000 | Loss: 0.00039121
Iteration 43/1000 | Loss: 0.00015976
Iteration 44/1000 | Loss: 0.00010374
Iteration 45/1000 | Loss: 0.00064359
Iteration 46/1000 | Loss: 0.00086114
Iteration 47/1000 | Loss: 0.00085800
Iteration 48/1000 | Loss: 0.00094692
Iteration 49/1000 | Loss: 0.00051737
Iteration 50/1000 | Loss: 0.00057369
Iteration 51/1000 | Loss: 0.00082709
Iteration 52/1000 | Loss: 0.00049735
Iteration 53/1000 | Loss: 0.00064483
Iteration 54/1000 | Loss: 0.00011755
Iteration 55/1000 | Loss: 0.00008668
Iteration 56/1000 | Loss: 0.00008281
Iteration 57/1000 | Loss: 0.00050901
Iteration 58/1000 | Loss: 0.00042398
Iteration 59/1000 | Loss: 0.00059497
Iteration 60/1000 | Loss: 0.00044280
Iteration 61/1000 | Loss: 0.00027941
Iteration 62/1000 | Loss: 0.00009774
Iteration 63/1000 | Loss: 0.00067297
Iteration 64/1000 | Loss: 0.00022888
Iteration 65/1000 | Loss: 0.00029991
Iteration 66/1000 | Loss: 0.00027854
Iteration 67/1000 | Loss: 0.00018269
Iteration 68/1000 | Loss: 0.00024326
Iteration 69/1000 | Loss: 0.00027830
Iteration 70/1000 | Loss: 0.00042383
Iteration 71/1000 | Loss: 0.00116591
Iteration 72/1000 | Loss: 0.00040651
Iteration 73/1000 | Loss: 0.00037444
Iteration 74/1000 | Loss: 0.00007353
Iteration 75/1000 | Loss: 0.00006286
Iteration 76/1000 | Loss: 0.00006992
Iteration 77/1000 | Loss: 0.00032062
Iteration 78/1000 | Loss: 0.00006781
Iteration 79/1000 | Loss: 0.00005557
Iteration 80/1000 | Loss: 0.00031176
Iteration 81/1000 | Loss: 0.00030419
Iteration 82/1000 | Loss: 0.00008627
Iteration 83/1000 | Loss: 0.00005942
Iteration 84/1000 | Loss: 0.00005080
Iteration 85/1000 | Loss: 0.00014329
Iteration 86/1000 | Loss: 0.00004338
Iteration 87/1000 | Loss: 0.00070608
Iteration 88/1000 | Loss: 0.00005910
Iteration 89/1000 | Loss: 0.00022824
Iteration 90/1000 | Loss: 0.00007298
Iteration 91/1000 | Loss: 0.00004426
Iteration 92/1000 | Loss: 0.00019642
Iteration 93/1000 | Loss: 0.00054481
Iteration 94/1000 | Loss: 0.00036281
Iteration 95/1000 | Loss: 0.00069101
Iteration 96/1000 | Loss: 0.00041731
Iteration 97/1000 | Loss: 0.00004748
Iteration 98/1000 | Loss: 0.00003812
Iteration 99/1000 | Loss: 0.00023749
Iteration 100/1000 | Loss: 0.00032880
Iteration 101/1000 | Loss: 0.00021915
Iteration 102/1000 | Loss: 0.00003761
Iteration 103/1000 | Loss: 0.00023699
Iteration 104/1000 | Loss: 0.00013961
Iteration 105/1000 | Loss: 0.00018391
Iteration 106/1000 | Loss: 0.00030796
Iteration 107/1000 | Loss: 0.00020797
Iteration 108/1000 | Loss: 0.00003790
Iteration 109/1000 | Loss: 0.00003282
Iteration 110/1000 | Loss: 0.00002986
Iteration 111/1000 | Loss: 0.00002832
Iteration 112/1000 | Loss: 0.00002712
Iteration 113/1000 | Loss: 0.00004160
Iteration 114/1000 | Loss: 0.00002785
Iteration 115/1000 | Loss: 0.00002665
Iteration 116/1000 | Loss: 0.00002557
Iteration 117/1000 | Loss: 0.00002407
Iteration 118/1000 | Loss: 0.00002323
Iteration 119/1000 | Loss: 0.00002281
Iteration 120/1000 | Loss: 0.00002248
Iteration 121/1000 | Loss: 0.00002231
Iteration 122/1000 | Loss: 0.00002226
Iteration 123/1000 | Loss: 0.00002225
Iteration 124/1000 | Loss: 0.00002224
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002222
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002218
Iteration 130/1000 | Loss: 0.00002217
Iteration 131/1000 | Loss: 0.00002217
Iteration 132/1000 | Loss: 0.00002216
Iteration 133/1000 | Loss: 0.00002215
Iteration 134/1000 | Loss: 0.00002215
Iteration 135/1000 | Loss: 0.00002215
Iteration 136/1000 | Loss: 0.00002215
Iteration 137/1000 | Loss: 0.00002214
Iteration 138/1000 | Loss: 0.00002214
Iteration 139/1000 | Loss: 0.00002213
Iteration 140/1000 | Loss: 0.00002211
Iteration 141/1000 | Loss: 0.00002211
Iteration 142/1000 | Loss: 0.00002210
Iteration 143/1000 | Loss: 0.00002210
Iteration 144/1000 | Loss: 0.00002209
Iteration 145/1000 | Loss: 0.00002206
Iteration 146/1000 | Loss: 0.00002204
Iteration 147/1000 | Loss: 0.00002202
Iteration 148/1000 | Loss: 0.00002201
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002199
Iteration 151/1000 | Loss: 0.00002198
Iteration 152/1000 | Loss: 0.00002197
Iteration 153/1000 | Loss: 0.00002197
Iteration 154/1000 | Loss: 0.00002196
Iteration 155/1000 | Loss: 0.00002188
Iteration 156/1000 | Loss: 0.00002186
Iteration 157/1000 | Loss: 0.00044648
Iteration 158/1000 | Loss: 0.00002831
Iteration 159/1000 | Loss: 0.00002328
Iteration 160/1000 | Loss: 0.00002232
Iteration 161/1000 | Loss: 0.00002113
Iteration 162/1000 | Loss: 0.00002032
Iteration 163/1000 | Loss: 0.00001990
Iteration 164/1000 | Loss: 0.00001983
Iteration 165/1000 | Loss: 0.00001983
Iteration 166/1000 | Loss: 0.00001980
Iteration 167/1000 | Loss: 0.00001978
Iteration 168/1000 | Loss: 0.00001974
Iteration 169/1000 | Loss: 0.00001974
Iteration 170/1000 | Loss: 0.00001973
Iteration 171/1000 | Loss: 0.00001973
Iteration 172/1000 | Loss: 0.00001972
Iteration 173/1000 | Loss: 0.00001972
Iteration 174/1000 | Loss: 0.00001971
Iteration 175/1000 | Loss: 0.00001971
Iteration 176/1000 | Loss: 0.00001970
Iteration 177/1000 | Loss: 0.00001969
Iteration 178/1000 | Loss: 0.00001966
Iteration 179/1000 | Loss: 0.00001966
Iteration 180/1000 | Loss: 0.00001964
Iteration 181/1000 | Loss: 0.00001964
Iteration 182/1000 | Loss: 0.00001964
Iteration 183/1000 | Loss: 0.00001964
Iteration 184/1000 | Loss: 0.00001963
Iteration 185/1000 | Loss: 0.00001963
Iteration 186/1000 | Loss: 0.00001963
Iteration 187/1000 | Loss: 0.00001962
Iteration 188/1000 | Loss: 0.00001962
Iteration 189/1000 | Loss: 0.00001962
Iteration 190/1000 | Loss: 0.00001962
Iteration 191/1000 | Loss: 0.00001962
Iteration 192/1000 | Loss: 0.00001962
Iteration 193/1000 | Loss: 0.00001962
Iteration 194/1000 | Loss: 0.00001962
Iteration 195/1000 | Loss: 0.00001962
Iteration 196/1000 | Loss: 0.00001961
Iteration 197/1000 | Loss: 0.00001961
Iteration 198/1000 | Loss: 0.00001961
Iteration 199/1000 | Loss: 0.00001961
Iteration 200/1000 | Loss: 0.00001961
Iteration 201/1000 | Loss: 0.00001961
Iteration 202/1000 | Loss: 0.00001961
Iteration 203/1000 | Loss: 0.00001960
Iteration 204/1000 | Loss: 0.00001960
Iteration 205/1000 | Loss: 0.00001960
Iteration 206/1000 | Loss: 0.00001960
Iteration 207/1000 | Loss: 0.00001960
Iteration 208/1000 | Loss: 0.00001960
Iteration 209/1000 | Loss: 0.00001960
Iteration 210/1000 | Loss: 0.00001960
Iteration 211/1000 | Loss: 0.00001959
Iteration 212/1000 | Loss: 0.00001959
Iteration 213/1000 | Loss: 0.00001959
Iteration 214/1000 | Loss: 0.00001959
Iteration 215/1000 | Loss: 0.00001958
Iteration 216/1000 | Loss: 0.00001958
Iteration 217/1000 | Loss: 0.00001958
Iteration 218/1000 | Loss: 0.00001958
Iteration 219/1000 | Loss: 0.00001957
Iteration 220/1000 | Loss: 0.00001957
Iteration 221/1000 | Loss: 0.00001957
Iteration 222/1000 | Loss: 0.00001957
Iteration 223/1000 | Loss: 0.00001957
Iteration 224/1000 | Loss: 0.00001956
Iteration 225/1000 | Loss: 0.00001956
Iteration 226/1000 | Loss: 0.00001955
Iteration 227/1000 | Loss: 0.00001955
Iteration 228/1000 | Loss: 0.00001955
Iteration 229/1000 | Loss: 0.00001955
Iteration 230/1000 | Loss: 0.00001954
Iteration 231/1000 | Loss: 0.00001954
Iteration 232/1000 | Loss: 0.00001954
Iteration 233/1000 | Loss: 0.00001954
Iteration 234/1000 | Loss: 0.00001953
Iteration 235/1000 | Loss: 0.00001953
Iteration 236/1000 | Loss: 0.00001953
Iteration 237/1000 | Loss: 0.00001953
Iteration 238/1000 | Loss: 0.00001953
Iteration 239/1000 | Loss: 0.00001953
Iteration 240/1000 | Loss: 0.00001953
Iteration 241/1000 | Loss: 0.00001952
Iteration 242/1000 | Loss: 0.00001952
Iteration 243/1000 | Loss: 0.00001952
Iteration 244/1000 | Loss: 0.00001952
Iteration 245/1000 | Loss: 0.00001952
Iteration 246/1000 | Loss: 0.00001952
Iteration 247/1000 | Loss: 0.00001952
Iteration 248/1000 | Loss: 0.00001952
Iteration 249/1000 | Loss: 0.00001952
Iteration 250/1000 | Loss: 0.00001952
Iteration 251/1000 | Loss: 0.00001952
Iteration 252/1000 | Loss: 0.00001952
Iteration 253/1000 | Loss: 0.00001951
Iteration 254/1000 | Loss: 0.00001951
Iteration 255/1000 | Loss: 0.00001951
Iteration 256/1000 | Loss: 0.00001951
Iteration 257/1000 | Loss: 0.00001951
Iteration 258/1000 | Loss: 0.00001951
Iteration 259/1000 | Loss: 0.00001950
Iteration 260/1000 | Loss: 0.00001950
Iteration 261/1000 | Loss: 0.00001950
Iteration 262/1000 | Loss: 0.00001950
Iteration 263/1000 | Loss: 0.00001950
Iteration 264/1000 | Loss: 0.00001950
Iteration 265/1000 | Loss: 0.00001950
Iteration 266/1000 | Loss: 0.00001950
Iteration 267/1000 | Loss: 0.00001950
Iteration 268/1000 | Loss: 0.00001949
Iteration 269/1000 | Loss: 0.00001949
Iteration 270/1000 | Loss: 0.00001949
Iteration 271/1000 | Loss: 0.00001949
Iteration 272/1000 | Loss: 0.00001949
Iteration 273/1000 | Loss: 0.00001949
Iteration 274/1000 | Loss: 0.00001949
Iteration 275/1000 | Loss: 0.00001949
Iteration 276/1000 | Loss: 0.00001948
Iteration 277/1000 | Loss: 0.00001948
Iteration 278/1000 | Loss: 0.00001948
Iteration 279/1000 | Loss: 0.00001948
Iteration 280/1000 | Loss: 0.00001948
Iteration 281/1000 | Loss: 0.00001948
Iteration 282/1000 | Loss: 0.00001948
Iteration 283/1000 | Loss: 0.00001948
Iteration 284/1000 | Loss: 0.00001948
Iteration 285/1000 | Loss: 0.00001948
Iteration 286/1000 | Loss: 0.00001948
Iteration 287/1000 | Loss: 0.00001948
Iteration 288/1000 | Loss: 0.00001948
Iteration 289/1000 | Loss: 0.00001948
Iteration 290/1000 | Loss: 0.00001948
Iteration 291/1000 | Loss: 0.00001948
Iteration 292/1000 | Loss: 0.00001948
Iteration 293/1000 | Loss: 0.00001948
Iteration 294/1000 | Loss: 0.00001948
Iteration 295/1000 | Loss: 0.00001948
Iteration 296/1000 | Loss: 0.00001948
Iteration 297/1000 | Loss: 0.00001948
Iteration 298/1000 | Loss: 0.00001948
Iteration 299/1000 | Loss: 0.00001948
Iteration 300/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.9479393813526258e-05, 1.9479393813526258e-05, 1.9479393813526258e-05, 1.9479393813526258e-05, 1.9479393813526258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9479393813526258e-05

Optimization complete. Final v2v error: 3.264078140258789 mm

Highest mean error: 11.240591049194336 mm for frame 201

Lowest mean error: 2.8744795322418213 mm for frame 71

Saving results

Total time: 268.76823687553406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463880
Iteration 2/25 | Loss: 0.00106525
Iteration 3/25 | Loss: 0.00095086
Iteration 4/25 | Loss: 0.00091989
Iteration 5/25 | Loss: 0.00091292
Iteration 6/25 | Loss: 0.00091189
Iteration 7/25 | Loss: 0.00091189
Iteration 8/25 | Loss: 0.00091189
Iteration 9/25 | Loss: 0.00091189
Iteration 10/25 | Loss: 0.00091189
Iteration 11/25 | Loss: 0.00091189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009118941379711032, 0.0009118941379711032, 0.0009118941379711032, 0.0009118941379711032, 0.0009118941379711032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009118941379711032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54346299
Iteration 2/25 | Loss: 0.00060072
Iteration 3/25 | Loss: 0.00060072
Iteration 4/25 | Loss: 0.00060071
Iteration 5/25 | Loss: 0.00060071
Iteration 6/25 | Loss: 0.00060071
Iteration 7/25 | Loss: 0.00060071
Iteration 8/25 | Loss: 0.00060071
Iteration 9/25 | Loss: 0.00060071
Iteration 10/25 | Loss: 0.00060071
Iteration 11/25 | Loss: 0.00060071
Iteration 12/25 | Loss: 0.00060071
Iteration 13/25 | Loss: 0.00060071
Iteration 14/25 | Loss: 0.00060071
Iteration 15/25 | Loss: 0.00060071
Iteration 16/25 | Loss: 0.00060071
Iteration 17/25 | Loss: 0.00060071
Iteration 18/25 | Loss: 0.00060071
Iteration 19/25 | Loss: 0.00060071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006007125484757125, 0.0006007125484757125, 0.0006007125484757125, 0.0006007125484757125, 0.0006007125484757125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006007125484757125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060071
Iteration 2/1000 | Loss: 0.00005551
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00003272
Iteration 5/1000 | Loss: 0.00003097
Iteration 6/1000 | Loss: 0.00002994
Iteration 7/1000 | Loss: 0.00002915
Iteration 8/1000 | Loss: 0.00002863
Iteration 9/1000 | Loss: 0.00002816
Iteration 10/1000 | Loss: 0.00002784
Iteration 11/1000 | Loss: 0.00002753
Iteration 12/1000 | Loss: 0.00002725
Iteration 13/1000 | Loss: 0.00002717
Iteration 14/1000 | Loss: 0.00002711
Iteration 15/1000 | Loss: 0.00002710
Iteration 16/1000 | Loss: 0.00002710
Iteration 17/1000 | Loss: 0.00002692
Iteration 18/1000 | Loss: 0.00002678
Iteration 19/1000 | Loss: 0.00002678
Iteration 20/1000 | Loss: 0.00002676
Iteration 21/1000 | Loss: 0.00002675
Iteration 22/1000 | Loss: 0.00002675
Iteration 23/1000 | Loss: 0.00002674
Iteration 24/1000 | Loss: 0.00002674
Iteration 25/1000 | Loss: 0.00002674
Iteration 26/1000 | Loss: 0.00002673
Iteration 27/1000 | Loss: 0.00002673
Iteration 28/1000 | Loss: 0.00002673
Iteration 29/1000 | Loss: 0.00002673
Iteration 30/1000 | Loss: 0.00002673
Iteration 31/1000 | Loss: 0.00002672
Iteration 32/1000 | Loss: 0.00002671
Iteration 33/1000 | Loss: 0.00002671
Iteration 34/1000 | Loss: 0.00002670
Iteration 35/1000 | Loss: 0.00002670
Iteration 36/1000 | Loss: 0.00002669
Iteration 37/1000 | Loss: 0.00002669
Iteration 38/1000 | Loss: 0.00002669
Iteration 39/1000 | Loss: 0.00002669
Iteration 40/1000 | Loss: 0.00002669
Iteration 41/1000 | Loss: 0.00002669
Iteration 42/1000 | Loss: 0.00002668
Iteration 43/1000 | Loss: 0.00002668
Iteration 44/1000 | Loss: 0.00002668
Iteration 45/1000 | Loss: 0.00002668
Iteration 46/1000 | Loss: 0.00002668
Iteration 47/1000 | Loss: 0.00002668
Iteration 48/1000 | Loss: 0.00002668
Iteration 49/1000 | Loss: 0.00002668
Iteration 50/1000 | Loss: 0.00002668
Iteration 51/1000 | Loss: 0.00002667
Iteration 52/1000 | Loss: 0.00002667
Iteration 53/1000 | Loss: 0.00002666
Iteration 54/1000 | Loss: 0.00002666
Iteration 55/1000 | Loss: 0.00002665
Iteration 56/1000 | Loss: 0.00002665
Iteration 57/1000 | Loss: 0.00002665
Iteration 58/1000 | Loss: 0.00002665
Iteration 59/1000 | Loss: 0.00002665
Iteration 60/1000 | Loss: 0.00002665
Iteration 61/1000 | Loss: 0.00002664
Iteration 62/1000 | Loss: 0.00002664
Iteration 63/1000 | Loss: 0.00002663
Iteration 64/1000 | Loss: 0.00002663
Iteration 65/1000 | Loss: 0.00002662
Iteration 66/1000 | Loss: 0.00002662
Iteration 67/1000 | Loss: 0.00002662
Iteration 68/1000 | Loss: 0.00002662
Iteration 69/1000 | Loss: 0.00002662
Iteration 70/1000 | Loss: 0.00002661
Iteration 71/1000 | Loss: 0.00002661
Iteration 72/1000 | Loss: 0.00002661
Iteration 73/1000 | Loss: 0.00002661
Iteration 74/1000 | Loss: 0.00002660
Iteration 75/1000 | Loss: 0.00002660
Iteration 76/1000 | Loss: 0.00002660
Iteration 77/1000 | Loss: 0.00002660
Iteration 78/1000 | Loss: 0.00002659
Iteration 79/1000 | Loss: 0.00002659
Iteration 80/1000 | Loss: 0.00002658
Iteration 81/1000 | Loss: 0.00002658
Iteration 82/1000 | Loss: 0.00002658
Iteration 83/1000 | Loss: 0.00002657
Iteration 84/1000 | Loss: 0.00002657
Iteration 85/1000 | Loss: 0.00002657
Iteration 86/1000 | Loss: 0.00002657
Iteration 87/1000 | Loss: 0.00002657
Iteration 88/1000 | Loss: 0.00002657
Iteration 89/1000 | Loss: 0.00002657
Iteration 90/1000 | Loss: 0.00002657
Iteration 91/1000 | Loss: 0.00002657
Iteration 92/1000 | Loss: 0.00002657
Iteration 93/1000 | Loss: 0.00002657
Iteration 94/1000 | Loss: 0.00002656
Iteration 95/1000 | Loss: 0.00002656
Iteration 96/1000 | Loss: 0.00002656
Iteration 97/1000 | Loss: 0.00002656
Iteration 98/1000 | Loss: 0.00002656
Iteration 99/1000 | Loss: 0.00002656
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00002655
Iteration 102/1000 | Loss: 0.00002655
Iteration 103/1000 | Loss: 0.00002655
Iteration 104/1000 | Loss: 0.00002655
Iteration 105/1000 | Loss: 0.00002654
Iteration 106/1000 | Loss: 0.00002654
Iteration 107/1000 | Loss: 0.00002654
Iteration 108/1000 | Loss: 0.00002654
Iteration 109/1000 | Loss: 0.00002654
Iteration 110/1000 | Loss: 0.00002654
Iteration 111/1000 | Loss: 0.00002654
Iteration 112/1000 | Loss: 0.00002654
Iteration 113/1000 | Loss: 0.00002654
Iteration 114/1000 | Loss: 0.00002654
Iteration 115/1000 | Loss: 0.00002654
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002653
Iteration 119/1000 | Loss: 0.00002653
Iteration 120/1000 | Loss: 0.00002653
Iteration 121/1000 | Loss: 0.00002653
Iteration 122/1000 | Loss: 0.00002653
Iteration 123/1000 | Loss: 0.00002653
Iteration 124/1000 | Loss: 0.00002653
Iteration 125/1000 | Loss: 0.00002653
Iteration 126/1000 | Loss: 0.00002653
Iteration 127/1000 | Loss: 0.00002653
Iteration 128/1000 | Loss: 0.00002652
Iteration 129/1000 | Loss: 0.00002652
Iteration 130/1000 | Loss: 0.00002652
Iteration 131/1000 | Loss: 0.00002652
Iteration 132/1000 | Loss: 0.00002652
Iteration 133/1000 | Loss: 0.00002652
Iteration 134/1000 | Loss: 0.00002652
Iteration 135/1000 | Loss: 0.00002652
Iteration 136/1000 | Loss: 0.00002652
Iteration 137/1000 | Loss: 0.00002652
Iteration 138/1000 | Loss: 0.00002652
Iteration 139/1000 | Loss: 0.00002652
Iteration 140/1000 | Loss: 0.00002652
Iteration 141/1000 | Loss: 0.00002652
Iteration 142/1000 | Loss: 0.00002652
Iteration 143/1000 | Loss: 0.00002652
Iteration 144/1000 | Loss: 0.00002652
Iteration 145/1000 | Loss: 0.00002652
Iteration 146/1000 | Loss: 0.00002652
Iteration 147/1000 | Loss: 0.00002652
Iteration 148/1000 | Loss: 0.00002652
Iteration 149/1000 | Loss: 0.00002652
Iteration 150/1000 | Loss: 0.00002652
Iteration 151/1000 | Loss: 0.00002652
Iteration 152/1000 | Loss: 0.00002652
Iteration 153/1000 | Loss: 0.00002652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.6519448510953225e-05, 2.6519448510953225e-05, 2.6519448510953225e-05, 2.6519448510953225e-05, 2.6519448510953225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6519448510953225e-05

Optimization complete. Final v2v error: 4.29283332824707 mm

Highest mean error: 4.708878993988037 mm for frame 31

Lowest mean error: 4.032570838928223 mm for frame 72

Saving results

Total time: 38.31613755226135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516464
Iteration 2/25 | Loss: 0.00118269
Iteration 3/25 | Loss: 0.00094712
Iteration 4/25 | Loss: 0.00092934
Iteration 5/25 | Loss: 0.00092538
Iteration 6/25 | Loss: 0.00092531
Iteration 7/25 | Loss: 0.00092531
Iteration 8/25 | Loss: 0.00092531
Iteration 9/25 | Loss: 0.00092531
Iteration 10/25 | Loss: 0.00092531
Iteration 11/25 | Loss: 0.00092531
Iteration 12/25 | Loss: 0.00092531
Iteration 13/25 | Loss: 0.00092531
Iteration 14/25 | Loss: 0.00092531
Iteration 15/25 | Loss: 0.00092531
Iteration 16/25 | Loss: 0.00092531
Iteration 17/25 | Loss: 0.00092531
Iteration 18/25 | Loss: 0.00092531
Iteration 19/25 | Loss: 0.00092531
Iteration 20/25 | Loss: 0.00092531
Iteration 21/25 | Loss: 0.00092531
Iteration 22/25 | Loss: 0.00092531
Iteration 23/25 | Loss: 0.00092531
Iteration 24/25 | Loss: 0.00092531
Iteration 25/25 | Loss: 0.00092531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53472674
Iteration 2/25 | Loss: 0.00058165
Iteration 3/25 | Loss: 0.00058163
Iteration 4/25 | Loss: 0.00058163
Iteration 5/25 | Loss: 0.00058163
Iteration 6/25 | Loss: 0.00058163
Iteration 7/25 | Loss: 0.00058163
Iteration 8/25 | Loss: 0.00058163
Iteration 9/25 | Loss: 0.00058163
Iteration 10/25 | Loss: 0.00058163
Iteration 11/25 | Loss: 0.00058163
Iteration 12/25 | Loss: 0.00058163
Iteration 13/25 | Loss: 0.00058163
Iteration 14/25 | Loss: 0.00058163
Iteration 15/25 | Loss: 0.00058163
Iteration 16/25 | Loss: 0.00058163
Iteration 17/25 | Loss: 0.00058163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005816317861899734, 0.0005816317861899734, 0.0005816317861899734, 0.0005816317861899734, 0.0005816317861899734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005816317861899734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058163
Iteration 2/1000 | Loss: 0.00004136
Iteration 3/1000 | Loss: 0.00002747
Iteration 4/1000 | Loss: 0.00002469
Iteration 5/1000 | Loss: 0.00002304
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002089
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001936
Iteration 33/1000 | Loss: 0.00001935
Iteration 34/1000 | Loss: 0.00001935
Iteration 35/1000 | Loss: 0.00001934
Iteration 36/1000 | Loss: 0.00001934
Iteration 37/1000 | Loss: 0.00001934
Iteration 38/1000 | Loss: 0.00001933
Iteration 39/1000 | Loss: 0.00001933
Iteration 40/1000 | Loss: 0.00001933
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001932
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001931
Iteration 50/1000 | Loss: 0.00001931
Iteration 51/1000 | Loss: 0.00001931
Iteration 52/1000 | Loss: 0.00001930
Iteration 53/1000 | Loss: 0.00001930
Iteration 54/1000 | Loss: 0.00001930
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001928
Iteration 61/1000 | Loss: 0.00001928
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001927
Iteration 65/1000 | Loss: 0.00001927
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001927
Iteration 68/1000 | Loss: 0.00001927
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001925
Iteration 79/1000 | Loss: 0.00001925
Iteration 80/1000 | Loss: 0.00001925
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001925
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001923
Iteration 97/1000 | Loss: 0.00001923
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001921
Iteration 109/1000 | Loss: 0.00001921
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001914
Iteration 149/1000 | Loss: 0.00001914
Iteration 150/1000 | Loss: 0.00001914
Iteration 151/1000 | Loss: 0.00001914
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001914
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001914
Iteration 159/1000 | Loss: 0.00001914
Iteration 160/1000 | Loss: 0.00001914
Iteration 161/1000 | Loss: 0.00001914
Iteration 162/1000 | Loss: 0.00001914
Iteration 163/1000 | Loss: 0.00001914
Iteration 164/1000 | Loss: 0.00001914
Iteration 165/1000 | Loss: 0.00001914
Iteration 166/1000 | Loss: 0.00001914
Iteration 167/1000 | Loss: 0.00001914
Iteration 168/1000 | Loss: 0.00001914
Iteration 169/1000 | Loss: 0.00001914
Iteration 170/1000 | Loss: 0.00001914
Iteration 171/1000 | Loss: 0.00001914
Iteration 172/1000 | Loss: 0.00001914
Iteration 173/1000 | Loss: 0.00001914
Iteration 174/1000 | Loss: 0.00001914
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.913970845635049e-05, 1.913970845635049e-05, 1.913970845635049e-05, 1.913970845635049e-05, 1.913970845635049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.913970845635049e-05

Optimization complete. Final v2v error: 3.6350257396698 mm

Highest mean error: 4.250979900360107 mm for frame 206

Lowest mean error: 3.115272283554077 mm for frame 59

Saving results

Total time: 44.27672290802002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600472
Iteration 2/25 | Loss: 0.00123613
Iteration 3/25 | Loss: 0.00096674
Iteration 4/25 | Loss: 0.00090798
Iteration 5/25 | Loss: 0.00084219
Iteration 6/25 | Loss: 0.00083748
Iteration 7/25 | Loss: 0.00084353
Iteration 8/25 | Loss: 0.00083258
Iteration 9/25 | Loss: 0.00082718
Iteration 10/25 | Loss: 0.00082269
Iteration 11/25 | Loss: 0.00082113
Iteration 12/25 | Loss: 0.00082029
Iteration 13/25 | Loss: 0.00082001
Iteration 14/25 | Loss: 0.00081985
Iteration 15/25 | Loss: 0.00081976
Iteration 16/25 | Loss: 0.00081973
Iteration 17/25 | Loss: 0.00081973
Iteration 18/25 | Loss: 0.00081973
Iteration 19/25 | Loss: 0.00081973
Iteration 20/25 | Loss: 0.00081973
Iteration 21/25 | Loss: 0.00081973
Iteration 22/25 | Loss: 0.00081973
Iteration 23/25 | Loss: 0.00081972
Iteration 24/25 | Loss: 0.00081972
Iteration 25/25 | Loss: 0.00081972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33162069
Iteration 2/25 | Loss: 0.00053007
Iteration 3/25 | Loss: 0.00053007
Iteration 4/25 | Loss: 0.00053007
Iteration 5/25 | Loss: 0.00053007
Iteration 6/25 | Loss: 0.00053007
Iteration 7/25 | Loss: 0.00053007
Iteration 8/25 | Loss: 0.00053007
Iteration 9/25 | Loss: 0.00053007
Iteration 10/25 | Loss: 0.00053007
Iteration 11/25 | Loss: 0.00053007
Iteration 12/25 | Loss: 0.00053007
Iteration 13/25 | Loss: 0.00053007
Iteration 14/25 | Loss: 0.00053007
Iteration 15/25 | Loss: 0.00053007
Iteration 16/25 | Loss: 0.00053007
Iteration 17/25 | Loss: 0.00053007
Iteration 18/25 | Loss: 0.00053007
Iteration 19/25 | Loss: 0.00053007
Iteration 20/25 | Loss: 0.00053007
Iteration 21/25 | Loss: 0.00053007
Iteration 22/25 | Loss: 0.00053007
Iteration 23/25 | Loss: 0.00053007
Iteration 24/25 | Loss: 0.00053007
Iteration 25/25 | Loss: 0.00053007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053007
Iteration 2/1000 | Loss: 0.00002141
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001473
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00042210
Iteration 8/1000 | Loss: 0.00019139
Iteration 9/1000 | Loss: 0.00002563
Iteration 10/1000 | Loss: 0.00002044
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001209
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.1699755305016879e-05, 1.1699755305016879e-05, 1.1699755305016879e-05, 1.1699755305016879e-05, 1.1699755305016879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1699755305016879e-05

Optimization complete. Final v2v error: 2.9058029651641846 mm

Highest mean error: 4.341555118560791 mm for frame 195

Lowest mean error: 2.6492693424224854 mm for frame 112

Saving results

Total time: 68.92775654792786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439179
Iteration 2/25 | Loss: 0.00104862
Iteration 3/25 | Loss: 0.00090539
Iteration 4/25 | Loss: 0.00088110
Iteration 5/25 | Loss: 0.00087686
Iteration 6/25 | Loss: 0.00087604
Iteration 7/25 | Loss: 0.00087599
Iteration 8/25 | Loss: 0.00087599
Iteration 9/25 | Loss: 0.00087599
Iteration 10/25 | Loss: 0.00087599
Iteration 11/25 | Loss: 0.00087599
Iteration 12/25 | Loss: 0.00087599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008759925840422511, 0.0008759925840422511, 0.0008759925840422511, 0.0008759925840422511, 0.0008759925840422511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008759925840422511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45733070
Iteration 2/25 | Loss: 0.00054400
Iteration 3/25 | Loss: 0.00054399
Iteration 4/25 | Loss: 0.00054399
Iteration 5/25 | Loss: 0.00054399
Iteration 6/25 | Loss: 0.00054399
Iteration 7/25 | Loss: 0.00054399
Iteration 8/25 | Loss: 0.00054399
Iteration 9/25 | Loss: 0.00054399
Iteration 10/25 | Loss: 0.00054399
Iteration 11/25 | Loss: 0.00054399
Iteration 12/25 | Loss: 0.00054399
Iteration 13/25 | Loss: 0.00054399
Iteration 14/25 | Loss: 0.00054399
Iteration 15/25 | Loss: 0.00054399
Iteration 16/25 | Loss: 0.00054399
Iteration 17/25 | Loss: 0.00054399
Iteration 18/25 | Loss: 0.00054399
Iteration 19/25 | Loss: 0.00054399
Iteration 20/25 | Loss: 0.00054399
Iteration 21/25 | Loss: 0.00054399
Iteration 22/25 | Loss: 0.00054399
Iteration 23/25 | Loss: 0.00054399
Iteration 24/25 | Loss: 0.00054399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005439923843368888, 0.0005439923843368888, 0.0005439923843368888, 0.0005439923843368888, 0.0005439923843368888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005439923843368888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054399
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00003316
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002615
Iteration 6/1000 | Loss: 0.00002506
Iteration 7/1000 | Loss: 0.00002413
Iteration 8/1000 | Loss: 0.00002356
Iteration 9/1000 | Loss: 0.00002315
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002229
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002189
Iteration 27/1000 | Loss: 0.00002189
Iteration 28/1000 | Loss: 0.00002188
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002188
Iteration 31/1000 | Loss: 0.00002186
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002180
Iteration 45/1000 | Loss: 0.00002180
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002177
Iteration 48/1000 | Loss: 0.00002175
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002175
Iteration 51/1000 | Loss: 0.00002174
Iteration 52/1000 | Loss: 0.00002174
Iteration 53/1000 | Loss: 0.00002174
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002173
Iteration 56/1000 | Loss: 0.00002173
Iteration 57/1000 | Loss: 0.00002172
Iteration 58/1000 | Loss: 0.00002171
Iteration 59/1000 | Loss: 0.00002171
Iteration 60/1000 | Loss: 0.00002170
Iteration 61/1000 | Loss: 0.00002170
Iteration 62/1000 | Loss: 0.00002170
Iteration 63/1000 | Loss: 0.00002170
Iteration 64/1000 | Loss: 0.00002170
Iteration 65/1000 | Loss: 0.00002170
Iteration 66/1000 | Loss: 0.00002170
Iteration 67/1000 | Loss: 0.00002170
Iteration 68/1000 | Loss: 0.00002170
Iteration 69/1000 | Loss: 0.00002170
Iteration 70/1000 | Loss: 0.00002170
Iteration 71/1000 | Loss: 0.00002169
Iteration 72/1000 | Loss: 0.00002169
Iteration 73/1000 | Loss: 0.00002169
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002168
Iteration 76/1000 | Loss: 0.00002168
Iteration 77/1000 | Loss: 0.00002168
Iteration 78/1000 | Loss: 0.00002168
Iteration 79/1000 | Loss: 0.00002168
Iteration 80/1000 | Loss: 0.00002168
Iteration 81/1000 | Loss: 0.00002168
Iteration 82/1000 | Loss: 0.00002168
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00002168
Iteration 85/1000 | Loss: 0.00002168
Iteration 86/1000 | Loss: 0.00002167
Iteration 87/1000 | Loss: 0.00002167
Iteration 88/1000 | Loss: 0.00002167
Iteration 89/1000 | Loss: 0.00002166
Iteration 90/1000 | Loss: 0.00002166
Iteration 91/1000 | Loss: 0.00002166
Iteration 92/1000 | Loss: 0.00002166
Iteration 93/1000 | Loss: 0.00002166
Iteration 94/1000 | Loss: 0.00002166
Iteration 95/1000 | Loss: 0.00002166
Iteration 96/1000 | Loss: 0.00002165
Iteration 97/1000 | Loss: 0.00002165
Iteration 98/1000 | Loss: 0.00002165
Iteration 99/1000 | Loss: 0.00002165
Iteration 100/1000 | Loss: 0.00002165
Iteration 101/1000 | Loss: 0.00002165
Iteration 102/1000 | Loss: 0.00002165
Iteration 103/1000 | Loss: 0.00002164
Iteration 104/1000 | Loss: 0.00002164
Iteration 105/1000 | Loss: 0.00002164
Iteration 106/1000 | Loss: 0.00002164
Iteration 107/1000 | Loss: 0.00002164
Iteration 108/1000 | Loss: 0.00002164
Iteration 109/1000 | Loss: 0.00002164
Iteration 110/1000 | Loss: 0.00002164
Iteration 111/1000 | Loss: 0.00002164
Iteration 112/1000 | Loss: 0.00002164
Iteration 113/1000 | Loss: 0.00002163
Iteration 114/1000 | Loss: 0.00002163
Iteration 115/1000 | Loss: 0.00002163
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00002163
Iteration 118/1000 | Loss: 0.00002163
Iteration 119/1000 | Loss: 0.00002163
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002162
Iteration 123/1000 | Loss: 0.00002162
Iteration 124/1000 | Loss: 0.00002162
Iteration 125/1000 | Loss: 0.00002162
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002162
Iteration 132/1000 | Loss: 0.00002162
Iteration 133/1000 | Loss: 0.00002162
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002161
Iteration 138/1000 | Loss: 0.00002161
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002159
Iteration 151/1000 | Loss: 0.00002159
Iteration 152/1000 | Loss: 0.00002159
Iteration 153/1000 | Loss: 0.00002159
Iteration 154/1000 | Loss: 0.00002159
Iteration 155/1000 | Loss: 0.00002159
Iteration 156/1000 | Loss: 0.00002159
Iteration 157/1000 | Loss: 0.00002159
Iteration 158/1000 | Loss: 0.00002159
Iteration 159/1000 | Loss: 0.00002158
Iteration 160/1000 | Loss: 0.00002158
Iteration 161/1000 | Loss: 0.00002158
Iteration 162/1000 | Loss: 0.00002158
Iteration 163/1000 | Loss: 0.00002158
Iteration 164/1000 | Loss: 0.00002158
Iteration 165/1000 | Loss: 0.00002158
Iteration 166/1000 | Loss: 0.00002158
Iteration 167/1000 | Loss: 0.00002158
Iteration 168/1000 | Loss: 0.00002158
Iteration 169/1000 | Loss: 0.00002158
Iteration 170/1000 | Loss: 0.00002158
Iteration 171/1000 | Loss: 0.00002158
Iteration 172/1000 | Loss: 0.00002157
Iteration 173/1000 | Loss: 0.00002157
Iteration 174/1000 | Loss: 0.00002157
Iteration 175/1000 | Loss: 0.00002157
Iteration 176/1000 | Loss: 0.00002157
Iteration 177/1000 | Loss: 0.00002157
Iteration 178/1000 | Loss: 0.00002157
Iteration 179/1000 | Loss: 0.00002157
Iteration 180/1000 | Loss: 0.00002157
Iteration 181/1000 | Loss: 0.00002157
Iteration 182/1000 | Loss: 0.00002157
Iteration 183/1000 | Loss: 0.00002157
Iteration 184/1000 | Loss: 0.00002157
Iteration 185/1000 | Loss: 0.00002157
Iteration 186/1000 | Loss: 0.00002157
Iteration 187/1000 | Loss: 0.00002157
Iteration 188/1000 | Loss: 0.00002157
Iteration 189/1000 | Loss: 0.00002157
Iteration 190/1000 | Loss: 0.00002157
Iteration 191/1000 | Loss: 0.00002157
Iteration 192/1000 | Loss: 0.00002157
Iteration 193/1000 | Loss: 0.00002157
Iteration 194/1000 | Loss: 0.00002157
Iteration 195/1000 | Loss: 0.00002157
Iteration 196/1000 | Loss: 0.00002157
Iteration 197/1000 | Loss: 0.00002157
Iteration 198/1000 | Loss: 0.00002157
Iteration 199/1000 | Loss: 0.00002157
Iteration 200/1000 | Loss: 0.00002157
Iteration 201/1000 | Loss: 0.00002157
Iteration 202/1000 | Loss: 0.00002157
Iteration 203/1000 | Loss: 0.00002157
Iteration 204/1000 | Loss: 0.00002157
Iteration 205/1000 | Loss: 0.00002157
Iteration 206/1000 | Loss: 0.00002157
Iteration 207/1000 | Loss: 0.00002157
Iteration 208/1000 | Loss: 0.00002157
Iteration 209/1000 | Loss: 0.00002157
Iteration 210/1000 | Loss: 0.00002157
Iteration 211/1000 | Loss: 0.00002157
Iteration 212/1000 | Loss: 0.00002157
Iteration 213/1000 | Loss: 0.00002157
Iteration 214/1000 | Loss: 0.00002157
Iteration 215/1000 | Loss: 0.00002157
Iteration 216/1000 | Loss: 0.00002157
Iteration 217/1000 | Loss: 0.00002157
Iteration 218/1000 | Loss: 0.00002157
Iteration 219/1000 | Loss: 0.00002157
Iteration 220/1000 | Loss: 0.00002157
Iteration 221/1000 | Loss: 0.00002157
Iteration 222/1000 | Loss: 0.00002157
Iteration 223/1000 | Loss: 0.00002157
Iteration 224/1000 | Loss: 0.00002157
Iteration 225/1000 | Loss: 0.00002157
Iteration 226/1000 | Loss: 0.00002157
Iteration 227/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.1574047423200682e-05, 2.1574047423200682e-05, 2.1574047423200682e-05, 2.1574047423200682e-05, 2.1574047423200682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1574047423200682e-05

Optimization complete. Final v2v error: 3.8792195320129395 mm

Highest mean error: 4.110659599304199 mm for frame 101

Lowest mean error: 3.4975180625915527 mm for frame 25

Saving results

Total time: 41.47502899169922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494250
Iteration 2/25 | Loss: 0.00105313
Iteration 3/25 | Loss: 0.00094915
Iteration 4/25 | Loss: 0.00092630
Iteration 5/25 | Loss: 0.00092211
Iteration 6/25 | Loss: 0.00092094
Iteration 7/25 | Loss: 0.00092094
Iteration 8/25 | Loss: 0.00092094
Iteration 9/25 | Loss: 0.00092094
Iteration 10/25 | Loss: 0.00092094
Iteration 11/25 | Loss: 0.00092094
Iteration 12/25 | Loss: 0.00092094
Iteration 13/25 | Loss: 0.00092094
Iteration 14/25 | Loss: 0.00092094
Iteration 15/25 | Loss: 0.00092094
Iteration 16/25 | Loss: 0.00092094
Iteration 17/25 | Loss: 0.00092094
Iteration 18/25 | Loss: 0.00092094
Iteration 19/25 | Loss: 0.00092094
Iteration 20/25 | Loss: 0.00092094
Iteration 21/25 | Loss: 0.00092094
Iteration 22/25 | Loss: 0.00092094
Iteration 23/25 | Loss: 0.00092094
Iteration 24/25 | Loss: 0.00092094
Iteration 25/25 | Loss: 0.00092094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46951210
Iteration 2/25 | Loss: 0.00057520
Iteration 3/25 | Loss: 0.00057519
Iteration 4/25 | Loss: 0.00057519
Iteration 5/25 | Loss: 0.00057519
Iteration 6/25 | Loss: 0.00057519
Iteration 7/25 | Loss: 0.00057519
Iteration 8/25 | Loss: 0.00057519
Iteration 9/25 | Loss: 0.00057519
Iteration 10/25 | Loss: 0.00057519
Iteration 11/25 | Loss: 0.00057519
Iteration 12/25 | Loss: 0.00057519
Iteration 13/25 | Loss: 0.00057519
Iteration 14/25 | Loss: 0.00057519
Iteration 15/25 | Loss: 0.00057519
Iteration 16/25 | Loss: 0.00057519
Iteration 17/25 | Loss: 0.00057519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000575187848880887, 0.000575187848880887, 0.000575187848880887, 0.000575187848880887, 0.000575187848880887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000575187848880887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057519
Iteration 2/1000 | Loss: 0.00006600
Iteration 3/1000 | Loss: 0.00004149
Iteration 4/1000 | Loss: 0.00003663
Iteration 5/1000 | Loss: 0.00003491
Iteration 6/1000 | Loss: 0.00003371
Iteration 7/1000 | Loss: 0.00003294
Iteration 8/1000 | Loss: 0.00003230
Iteration 9/1000 | Loss: 0.00003188
Iteration 10/1000 | Loss: 0.00003154
Iteration 11/1000 | Loss: 0.00003129
Iteration 12/1000 | Loss: 0.00003105
Iteration 13/1000 | Loss: 0.00003096
Iteration 14/1000 | Loss: 0.00003082
Iteration 15/1000 | Loss: 0.00003076
Iteration 16/1000 | Loss: 0.00003061
Iteration 17/1000 | Loss: 0.00003053
Iteration 18/1000 | Loss: 0.00003053
Iteration 19/1000 | Loss: 0.00003048
Iteration 20/1000 | Loss: 0.00003047
Iteration 21/1000 | Loss: 0.00003046
Iteration 22/1000 | Loss: 0.00003045
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00003045
Iteration 25/1000 | Loss: 0.00003045
Iteration 26/1000 | Loss: 0.00003045
Iteration 27/1000 | Loss: 0.00003045
Iteration 28/1000 | Loss: 0.00003044
Iteration 29/1000 | Loss: 0.00003044
Iteration 30/1000 | Loss: 0.00003043
Iteration 31/1000 | Loss: 0.00003043
Iteration 32/1000 | Loss: 0.00003043
Iteration 33/1000 | Loss: 0.00003043
Iteration 34/1000 | Loss: 0.00003043
Iteration 35/1000 | Loss: 0.00003043
Iteration 36/1000 | Loss: 0.00003043
Iteration 37/1000 | Loss: 0.00003043
Iteration 38/1000 | Loss: 0.00003043
Iteration 39/1000 | Loss: 0.00003042
Iteration 40/1000 | Loss: 0.00003042
Iteration 41/1000 | Loss: 0.00003042
Iteration 42/1000 | Loss: 0.00003042
Iteration 43/1000 | Loss: 0.00003041
Iteration 44/1000 | Loss: 0.00003041
Iteration 45/1000 | Loss: 0.00003041
Iteration 46/1000 | Loss: 0.00003040
Iteration 47/1000 | Loss: 0.00003040
Iteration 48/1000 | Loss: 0.00003039
Iteration 49/1000 | Loss: 0.00003039
Iteration 50/1000 | Loss: 0.00003039
Iteration 51/1000 | Loss: 0.00003039
Iteration 52/1000 | Loss: 0.00003039
Iteration 53/1000 | Loss: 0.00003038
Iteration 54/1000 | Loss: 0.00003038
Iteration 55/1000 | Loss: 0.00003038
Iteration 56/1000 | Loss: 0.00003037
Iteration 57/1000 | Loss: 0.00003037
Iteration 58/1000 | Loss: 0.00003037
Iteration 59/1000 | Loss: 0.00003036
Iteration 60/1000 | Loss: 0.00003036
Iteration 61/1000 | Loss: 0.00003036
Iteration 62/1000 | Loss: 0.00003035
Iteration 63/1000 | Loss: 0.00003035
Iteration 64/1000 | Loss: 0.00003035
Iteration 65/1000 | Loss: 0.00003035
Iteration 66/1000 | Loss: 0.00003035
Iteration 67/1000 | Loss: 0.00003035
Iteration 68/1000 | Loss: 0.00003034
Iteration 69/1000 | Loss: 0.00003034
Iteration 70/1000 | Loss: 0.00003034
Iteration 71/1000 | Loss: 0.00003034
Iteration 72/1000 | Loss: 0.00003034
Iteration 73/1000 | Loss: 0.00003034
Iteration 74/1000 | Loss: 0.00003033
Iteration 75/1000 | Loss: 0.00003033
Iteration 76/1000 | Loss: 0.00003033
Iteration 77/1000 | Loss: 0.00003033
Iteration 78/1000 | Loss: 0.00003033
Iteration 79/1000 | Loss: 0.00003032
Iteration 80/1000 | Loss: 0.00003032
Iteration 81/1000 | Loss: 0.00003032
Iteration 82/1000 | Loss: 0.00003031
Iteration 83/1000 | Loss: 0.00003031
Iteration 84/1000 | Loss: 0.00003031
Iteration 85/1000 | Loss: 0.00003031
Iteration 86/1000 | Loss: 0.00003031
Iteration 87/1000 | Loss: 0.00003031
Iteration 88/1000 | Loss: 0.00003031
Iteration 89/1000 | Loss: 0.00003031
Iteration 90/1000 | Loss: 0.00003031
Iteration 91/1000 | Loss: 0.00003031
Iteration 92/1000 | Loss: 0.00003031
Iteration 93/1000 | Loss: 0.00003031
Iteration 94/1000 | Loss: 0.00003031
Iteration 95/1000 | Loss: 0.00003031
Iteration 96/1000 | Loss: 0.00003031
Iteration 97/1000 | Loss: 0.00003030
Iteration 98/1000 | Loss: 0.00003030
Iteration 99/1000 | Loss: 0.00003030
Iteration 100/1000 | Loss: 0.00003029
Iteration 101/1000 | Loss: 0.00003029
Iteration 102/1000 | Loss: 0.00003029
Iteration 103/1000 | Loss: 0.00003029
Iteration 104/1000 | Loss: 0.00003029
Iteration 105/1000 | Loss: 0.00003029
Iteration 106/1000 | Loss: 0.00003028
Iteration 107/1000 | Loss: 0.00003028
Iteration 108/1000 | Loss: 0.00003028
Iteration 109/1000 | Loss: 0.00003028
Iteration 110/1000 | Loss: 0.00003028
Iteration 111/1000 | Loss: 0.00003028
Iteration 112/1000 | Loss: 0.00003028
Iteration 113/1000 | Loss: 0.00003028
Iteration 114/1000 | Loss: 0.00003028
Iteration 115/1000 | Loss: 0.00003028
Iteration 116/1000 | Loss: 0.00003028
Iteration 117/1000 | Loss: 0.00003028
Iteration 118/1000 | Loss: 0.00003028
Iteration 119/1000 | Loss: 0.00003028
Iteration 120/1000 | Loss: 0.00003028
Iteration 121/1000 | Loss: 0.00003027
Iteration 122/1000 | Loss: 0.00003027
Iteration 123/1000 | Loss: 0.00003027
Iteration 124/1000 | Loss: 0.00003027
Iteration 125/1000 | Loss: 0.00003027
Iteration 126/1000 | Loss: 0.00003027
Iteration 127/1000 | Loss: 0.00003027
Iteration 128/1000 | Loss: 0.00003027
Iteration 129/1000 | Loss: 0.00003027
Iteration 130/1000 | Loss: 0.00003027
Iteration 131/1000 | Loss: 0.00003027
Iteration 132/1000 | Loss: 0.00003026
Iteration 133/1000 | Loss: 0.00003026
Iteration 134/1000 | Loss: 0.00003026
Iteration 135/1000 | Loss: 0.00003026
Iteration 136/1000 | Loss: 0.00003026
Iteration 137/1000 | Loss: 0.00003026
Iteration 138/1000 | Loss: 0.00003026
Iteration 139/1000 | Loss: 0.00003026
Iteration 140/1000 | Loss: 0.00003026
Iteration 141/1000 | Loss: 0.00003026
Iteration 142/1000 | Loss: 0.00003026
Iteration 143/1000 | Loss: 0.00003025
Iteration 144/1000 | Loss: 0.00003025
Iteration 145/1000 | Loss: 0.00003025
Iteration 146/1000 | Loss: 0.00003025
Iteration 147/1000 | Loss: 0.00003025
Iteration 148/1000 | Loss: 0.00003025
Iteration 149/1000 | Loss: 0.00003025
Iteration 150/1000 | Loss: 0.00003025
Iteration 151/1000 | Loss: 0.00003025
Iteration 152/1000 | Loss: 0.00003025
Iteration 153/1000 | Loss: 0.00003025
Iteration 154/1000 | Loss: 0.00003025
Iteration 155/1000 | Loss: 0.00003025
Iteration 156/1000 | Loss: 0.00003025
Iteration 157/1000 | Loss: 0.00003025
Iteration 158/1000 | Loss: 0.00003025
Iteration 159/1000 | Loss: 0.00003025
Iteration 160/1000 | Loss: 0.00003025
Iteration 161/1000 | Loss: 0.00003025
Iteration 162/1000 | Loss: 0.00003025
Iteration 163/1000 | Loss: 0.00003025
Iteration 164/1000 | Loss: 0.00003025
Iteration 165/1000 | Loss: 0.00003025
Iteration 166/1000 | Loss: 0.00003025
Iteration 167/1000 | Loss: 0.00003025
Iteration 168/1000 | Loss: 0.00003025
Iteration 169/1000 | Loss: 0.00003025
Iteration 170/1000 | Loss: 0.00003025
Iteration 171/1000 | Loss: 0.00003025
Iteration 172/1000 | Loss: 0.00003025
Iteration 173/1000 | Loss: 0.00003025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.025099795195274e-05, 3.025099795195274e-05, 3.025099795195274e-05, 3.025099795195274e-05, 3.025099795195274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.025099795195274e-05

Optimization complete. Final v2v error: 4.33518648147583 mm

Highest mean error: 5.46563196182251 mm for frame 80

Lowest mean error: 3.7028207778930664 mm for frame 113

Saving results

Total time: 41.70975685119629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824536
Iteration 2/25 | Loss: 0.00101263
Iteration 3/25 | Loss: 0.00090667
Iteration 4/25 | Loss: 0.00086244
Iteration 5/25 | Loss: 0.00085110
Iteration 6/25 | Loss: 0.00084911
Iteration 7/25 | Loss: 0.00084849
Iteration 8/25 | Loss: 0.00084849
Iteration 9/25 | Loss: 0.00084849
Iteration 10/25 | Loss: 0.00084849
Iteration 11/25 | Loss: 0.00084849
Iteration 12/25 | Loss: 0.00084849
Iteration 13/25 | Loss: 0.00084849
Iteration 14/25 | Loss: 0.00084849
Iteration 15/25 | Loss: 0.00084849
Iteration 16/25 | Loss: 0.00084849
Iteration 17/25 | Loss: 0.00084849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008484892314299941, 0.0008484892314299941, 0.0008484892314299941, 0.0008484892314299941, 0.0008484892314299941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008484892314299941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49896765
Iteration 2/25 | Loss: 0.00057110
Iteration 3/25 | Loss: 0.00057104
Iteration 4/25 | Loss: 0.00057104
Iteration 5/25 | Loss: 0.00057104
Iteration 6/25 | Loss: 0.00057104
Iteration 7/25 | Loss: 0.00057104
Iteration 8/25 | Loss: 0.00057104
Iteration 9/25 | Loss: 0.00057104
Iteration 10/25 | Loss: 0.00057104
Iteration 11/25 | Loss: 0.00057104
Iteration 12/25 | Loss: 0.00057104
Iteration 13/25 | Loss: 0.00057104
Iteration 14/25 | Loss: 0.00057104
Iteration 15/25 | Loss: 0.00057104
Iteration 16/25 | Loss: 0.00057104
Iteration 17/25 | Loss: 0.00057104
Iteration 18/25 | Loss: 0.00057104
Iteration 19/25 | Loss: 0.00057104
Iteration 20/25 | Loss: 0.00057104
Iteration 21/25 | Loss: 0.00057104
Iteration 22/25 | Loss: 0.00057104
Iteration 23/25 | Loss: 0.00057104
Iteration 24/25 | Loss: 0.00057104
Iteration 25/25 | Loss: 0.00057104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057104
Iteration 2/1000 | Loss: 0.00004166
Iteration 3/1000 | Loss: 0.00002456
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001945
Iteration 7/1000 | Loss: 0.00001896
Iteration 8/1000 | Loss: 0.00001876
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001827
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001799
Iteration 14/1000 | Loss: 0.00001797
Iteration 15/1000 | Loss: 0.00001794
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001790
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001789
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001776
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001775
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001774
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001774
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001768
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001768
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001765
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001763
Iteration 131/1000 | Loss: 0.00001763
Iteration 132/1000 | Loss: 0.00001763
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001762
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001762
Iteration 150/1000 | Loss: 0.00001762
Iteration 151/1000 | Loss: 0.00001762
Iteration 152/1000 | Loss: 0.00001762
Iteration 153/1000 | Loss: 0.00001762
Iteration 154/1000 | Loss: 0.00001762
Iteration 155/1000 | Loss: 0.00001762
Iteration 156/1000 | Loss: 0.00001762
Iteration 157/1000 | Loss: 0.00001762
Iteration 158/1000 | Loss: 0.00001762
Iteration 159/1000 | Loss: 0.00001762
Iteration 160/1000 | Loss: 0.00001762
Iteration 161/1000 | Loss: 0.00001762
Iteration 162/1000 | Loss: 0.00001762
Iteration 163/1000 | Loss: 0.00001762
Iteration 164/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7616719560464844e-05, 1.7616719560464844e-05, 1.7616719560464844e-05, 1.7616719560464844e-05, 1.7616719560464844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7616719560464844e-05

Optimization complete. Final v2v error: 3.5840835571289062 mm

Highest mean error: 4.007802486419678 mm for frame 167

Lowest mean error: 3.302741527557373 mm for frame 53

Saving results

Total time: 42.24182629585266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417172
Iteration 2/25 | Loss: 0.00097006
Iteration 3/25 | Loss: 0.00087183
Iteration 4/25 | Loss: 0.00084611
Iteration 5/25 | Loss: 0.00084240
Iteration 6/25 | Loss: 0.00084128
Iteration 7/25 | Loss: 0.00084128
Iteration 8/25 | Loss: 0.00084128
Iteration 9/25 | Loss: 0.00084128
Iteration 10/25 | Loss: 0.00084128
Iteration 11/25 | Loss: 0.00084128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008412767783738673, 0.0008412767783738673, 0.0008412767783738673, 0.0008412767783738673, 0.0008412767783738673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008412767783738673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56149220
Iteration 2/25 | Loss: 0.00053483
Iteration 3/25 | Loss: 0.00053483
Iteration 4/25 | Loss: 0.00053483
Iteration 5/25 | Loss: 0.00053483
Iteration 6/25 | Loss: 0.00053483
Iteration 7/25 | Loss: 0.00053483
Iteration 8/25 | Loss: 0.00053483
Iteration 9/25 | Loss: 0.00053483
Iteration 10/25 | Loss: 0.00053483
Iteration 11/25 | Loss: 0.00053483
Iteration 12/25 | Loss: 0.00053483
Iteration 13/25 | Loss: 0.00053483
Iteration 14/25 | Loss: 0.00053483
Iteration 15/25 | Loss: 0.00053483
Iteration 16/25 | Loss: 0.00053483
Iteration 17/25 | Loss: 0.00053483
Iteration 18/25 | Loss: 0.00053483
Iteration 19/25 | Loss: 0.00053483
Iteration 20/25 | Loss: 0.00053483
Iteration 21/25 | Loss: 0.00053483
Iteration 22/25 | Loss: 0.00053483
Iteration 23/25 | Loss: 0.00053483
Iteration 24/25 | Loss: 0.00053483
Iteration 25/25 | Loss: 0.00053483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053483
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002067
Iteration 4/1000 | Loss: 0.00001930
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001693
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001646
Iteration 15/1000 | Loss: 0.00001645
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001625
Iteration 29/1000 | Loss: 0.00001624
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001624
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001621
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001619
Iteration 45/1000 | Loss: 0.00001619
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001619
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001618
Iteration 52/1000 | Loss: 0.00001618
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001611
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001610
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001598
Iteration 94/1000 | Loss: 0.00001598
Iteration 95/1000 | Loss: 0.00001598
Iteration 96/1000 | Loss: 0.00001597
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001594
Iteration 102/1000 | Loss: 0.00001594
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001593
Iteration 111/1000 | Loss: 0.00001593
Iteration 112/1000 | Loss: 0.00001593
Iteration 113/1000 | Loss: 0.00001593
Iteration 114/1000 | Loss: 0.00001593
Iteration 115/1000 | Loss: 0.00001593
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001592
Iteration 121/1000 | Loss: 0.00001592
Iteration 122/1000 | Loss: 0.00001592
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001591
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001591
Iteration 141/1000 | Loss: 0.00001591
Iteration 142/1000 | Loss: 0.00001591
Iteration 143/1000 | Loss: 0.00001591
Iteration 144/1000 | Loss: 0.00001591
Iteration 145/1000 | Loss: 0.00001591
Iteration 146/1000 | Loss: 0.00001591
Iteration 147/1000 | Loss: 0.00001591
Iteration 148/1000 | Loss: 0.00001591
Iteration 149/1000 | Loss: 0.00001591
Iteration 150/1000 | Loss: 0.00001591
Iteration 151/1000 | Loss: 0.00001591
Iteration 152/1000 | Loss: 0.00001591
Iteration 153/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5912355593172833e-05, 1.5912355593172833e-05, 1.5912355593172833e-05, 1.5912355593172833e-05, 1.5912355593172833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5912355593172833e-05

Optimization complete. Final v2v error: 3.390511989593506 mm

Highest mean error: 3.6081721782684326 mm for frame 114

Lowest mean error: 3.2869014739990234 mm for frame 93

Saving results

Total time: 34.828248023986816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500780
Iteration 2/25 | Loss: 0.00142435
Iteration 3/25 | Loss: 0.00098550
Iteration 4/25 | Loss: 0.00088294
Iteration 5/25 | Loss: 0.00085636
Iteration 6/25 | Loss: 0.00084949
Iteration 7/25 | Loss: 0.00084805
Iteration 8/25 | Loss: 0.00084787
Iteration 9/25 | Loss: 0.00084787
Iteration 10/25 | Loss: 0.00084787
Iteration 11/25 | Loss: 0.00084787
Iteration 12/25 | Loss: 0.00084787
Iteration 13/25 | Loss: 0.00084787
Iteration 14/25 | Loss: 0.00084787
Iteration 15/25 | Loss: 0.00084787
Iteration 16/25 | Loss: 0.00084787
Iteration 17/25 | Loss: 0.00084787
Iteration 18/25 | Loss: 0.00084787
Iteration 19/25 | Loss: 0.00084787
Iteration 20/25 | Loss: 0.00084787
Iteration 21/25 | Loss: 0.00084787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008478706004098058, 0.0008478706004098058, 0.0008478706004098058, 0.0008478706004098058, 0.0008478706004098058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008478706004098058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52369750
Iteration 2/25 | Loss: 0.00054095
Iteration 3/25 | Loss: 0.00054095
Iteration 4/25 | Loss: 0.00054095
Iteration 5/25 | Loss: 0.00054095
Iteration 6/25 | Loss: 0.00054094
Iteration 7/25 | Loss: 0.00054094
Iteration 8/25 | Loss: 0.00054094
Iteration 9/25 | Loss: 0.00054094
Iteration 10/25 | Loss: 0.00054094
Iteration 11/25 | Loss: 0.00054094
Iteration 12/25 | Loss: 0.00054094
Iteration 13/25 | Loss: 0.00054094
Iteration 14/25 | Loss: 0.00054094
Iteration 15/25 | Loss: 0.00054094
Iteration 16/25 | Loss: 0.00054094
Iteration 17/25 | Loss: 0.00054094
Iteration 18/25 | Loss: 0.00054094
Iteration 19/25 | Loss: 0.00054094
Iteration 20/25 | Loss: 0.00054094
Iteration 21/25 | Loss: 0.00054094
Iteration 22/25 | Loss: 0.00054094
Iteration 23/25 | Loss: 0.00054094
Iteration 24/25 | Loss: 0.00054094
Iteration 25/25 | Loss: 0.00054094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054094
Iteration 2/1000 | Loss: 0.00003047
Iteration 3/1000 | Loss: 0.00002080
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001490
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001480
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001475
Iteration 115/1000 | Loss: 0.00001475
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001474
Iteration 121/1000 | Loss: 0.00001474
Iteration 122/1000 | Loss: 0.00001474
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Iteration 143/1000 | Loss: 0.00001470
Iteration 144/1000 | Loss: 0.00001470
Iteration 145/1000 | Loss: 0.00001470
Iteration 146/1000 | Loss: 0.00001470
Iteration 147/1000 | Loss: 0.00001470
Iteration 148/1000 | Loss: 0.00001470
Iteration 149/1000 | Loss: 0.00001469
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001469
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.4693918274133466e-05, 1.4693918274133466e-05, 1.4693918274133466e-05, 1.4693918274133466e-05, 1.4693918274133466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4693918274133466e-05

Optimization complete. Final v2v error: 3.204289436340332 mm

Highest mean error: 4.370734691619873 mm for frame 90

Lowest mean error: 2.9147250652313232 mm for frame 18

Saving results

Total time: 40.8103129863739
