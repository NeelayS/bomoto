Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=103, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5768-5823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942019
Iteration 2/25 | Loss: 0.00161076
Iteration 3/25 | Loss: 0.00114975
Iteration 4/25 | Loss: 0.00102758
Iteration 5/25 | Loss: 0.00098107
Iteration 6/25 | Loss: 0.00095397
Iteration 7/25 | Loss: 0.00093499
Iteration 8/25 | Loss: 0.00087449
Iteration 9/25 | Loss: 0.00085345
Iteration 10/25 | Loss: 0.00083601
Iteration 11/25 | Loss: 0.00082526
Iteration 12/25 | Loss: 0.00081969
Iteration 13/25 | Loss: 0.00082189
Iteration 14/25 | Loss: 0.00081825
Iteration 15/25 | Loss: 0.00081360
Iteration 16/25 | Loss: 0.00082117
Iteration 17/25 | Loss: 0.00081714
Iteration 18/25 | Loss: 0.00081237
Iteration 19/25 | Loss: 0.00080999
Iteration 20/25 | Loss: 0.00080894
Iteration 21/25 | Loss: 0.00080855
Iteration 22/25 | Loss: 0.00081316
Iteration 23/25 | Loss: 0.00081065
Iteration 24/25 | Loss: 0.00080131
Iteration 25/25 | Loss: 0.00079921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36082888
Iteration 2/25 | Loss: 0.00034152
Iteration 3/25 | Loss: 0.00034147
Iteration 4/25 | Loss: 0.00034147
Iteration 5/25 | Loss: 0.00034147
Iteration 6/25 | Loss: 0.00034147
Iteration 7/25 | Loss: 0.00034147
Iteration 8/25 | Loss: 0.00034147
Iteration 9/25 | Loss: 0.00034147
Iteration 10/25 | Loss: 0.00034147
Iteration 11/25 | Loss: 0.00034147
Iteration 12/25 | Loss: 0.00034147
Iteration 13/25 | Loss: 0.00034147
Iteration 14/25 | Loss: 0.00034147
Iteration 15/25 | Loss: 0.00034147
Iteration 16/25 | Loss: 0.00034147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000341466860845685, 0.000341466860845685, 0.000341466860845685, 0.000341466860845685, 0.000341466860845685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000341466860845685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034147
Iteration 2/1000 | Loss: 0.00061995
Iteration 3/1000 | Loss: 0.00034479
Iteration 4/1000 | Loss: 0.00032985
Iteration 5/1000 | Loss: 0.00022585
Iteration 6/1000 | Loss: 0.00013556
Iteration 7/1000 | Loss: 0.00010667
Iteration 8/1000 | Loss: 0.00022837
Iteration 9/1000 | Loss: 0.00036219
Iteration 10/1000 | Loss: 0.00015285
Iteration 11/1000 | Loss: 0.00012629
Iteration 12/1000 | Loss: 0.00020507
Iteration 13/1000 | Loss: 0.00012533
Iteration 14/1000 | Loss: 0.00011006
Iteration 15/1000 | Loss: 0.00009887
Iteration 16/1000 | Loss: 0.00017486
Iteration 17/1000 | Loss: 0.00010131
Iteration 18/1000 | Loss: 0.00009423
Iteration 19/1000 | Loss: 0.00012012
Iteration 20/1000 | Loss: 0.00011726
Iteration 21/1000 | Loss: 0.00008740
Iteration 22/1000 | Loss: 0.00008193
Iteration 23/1000 | Loss: 0.00014276
Iteration 24/1000 | Loss: 0.00009464
Iteration 25/1000 | Loss: 0.00007485
Iteration 26/1000 | Loss: 0.00006763
Iteration 27/1000 | Loss: 0.00006234
Iteration 28/1000 | Loss: 0.00007435
Iteration 29/1000 | Loss: 0.00006920
Iteration 30/1000 | Loss: 0.00005708
Iteration 31/1000 | Loss: 0.00021010
Iteration 32/1000 | Loss: 0.00006581
Iteration 33/1000 | Loss: 0.00012638
Iteration 34/1000 | Loss: 0.00006065
Iteration 35/1000 | Loss: 0.00005599
Iteration 36/1000 | Loss: 0.00007237
Iteration 37/1000 | Loss: 0.00006386
Iteration 38/1000 | Loss: 0.00023974
Iteration 39/1000 | Loss: 0.00006129
Iteration 40/1000 | Loss: 0.00005538
Iteration 41/1000 | Loss: 0.00005277
Iteration 42/1000 | Loss: 0.00005049
Iteration 43/1000 | Loss: 0.00005636
Iteration 44/1000 | Loss: 0.00004953
Iteration 45/1000 | Loss: 0.00004600
Iteration 46/1000 | Loss: 0.00005048
Iteration 47/1000 | Loss: 0.00004495
Iteration 48/1000 | Loss: 0.00007042
Iteration 49/1000 | Loss: 0.00007446
Iteration 50/1000 | Loss: 0.00007407
Iteration 51/1000 | Loss: 0.00005992
Iteration 52/1000 | Loss: 0.00006747
Iteration 53/1000 | Loss: 0.00004838
Iteration 54/1000 | Loss: 0.00004237
Iteration 55/1000 | Loss: 0.00004163
Iteration 56/1000 | Loss: 0.00006166
Iteration 57/1000 | Loss: 0.00017615
Iteration 58/1000 | Loss: 0.00009027
Iteration 59/1000 | Loss: 0.00005137
Iteration 60/1000 | Loss: 0.00004548
Iteration 61/1000 | Loss: 0.00004295
Iteration 62/1000 | Loss: 0.00003976
Iteration 63/1000 | Loss: 0.00003791
Iteration 64/1000 | Loss: 0.00003648
Iteration 65/1000 | Loss: 0.00003563
Iteration 66/1000 | Loss: 0.00003496
Iteration 67/1000 | Loss: 0.00003434
Iteration 68/1000 | Loss: 0.00003381
Iteration 69/1000 | Loss: 0.00003349
Iteration 70/1000 | Loss: 0.00003335
Iteration 71/1000 | Loss: 0.00003333
Iteration 72/1000 | Loss: 0.00003330
Iteration 73/1000 | Loss: 0.00003327
Iteration 74/1000 | Loss: 0.00003322
Iteration 75/1000 | Loss: 0.00003321
Iteration 76/1000 | Loss: 0.00003320
Iteration 77/1000 | Loss: 0.00003319
Iteration 78/1000 | Loss: 0.00003319
Iteration 79/1000 | Loss: 0.00003319
Iteration 80/1000 | Loss: 0.00003318
Iteration 81/1000 | Loss: 0.00003318
Iteration 82/1000 | Loss: 0.00003316
Iteration 83/1000 | Loss: 0.00003316
Iteration 84/1000 | Loss: 0.00003316
Iteration 85/1000 | Loss: 0.00003315
Iteration 86/1000 | Loss: 0.00003315
Iteration 87/1000 | Loss: 0.00003315
Iteration 88/1000 | Loss: 0.00003315
Iteration 89/1000 | Loss: 0.00003314
Iteration 90/1000 | Loss: 0.00003314
Iteration 91/1000 | Loss: 0.00003314
Iteration 92/1000 | Loss: 0.00003314
Iteration 93/1000 | Loss: 0.00003314
Iteration 94/1000 | Loss: 0.00003314
Iteration 95/1000 | Loss: 0.00003313
Iteration 96/1000 | Loss: 0.00003312
Iteration 97/1000 | Loss: 0.00003312
Iteration 98/1000 | Loss: 0.00003311
Iteration 99/1000 | Loss: 0.00003311
Iteration 100/1000 | Loss: 0.00003311
Iteration 101/1000 | Loss: 0.00003310
Iteration 102/1000 | Loss: 0.00003310
Iteration 103/1000 | Loss: 0.00003310
Iteration 104/1000 | Loss: 0.00003309
Iteration 105/1000 | Loss: 0.00003309
Iteration 106/1000 | Loss: 0.00003309
Iteration 107/1000 | Loss: 0.00003308
Iteration 108/1000 | Loss: 0.00003304
Iteration 109/1000 | Loss: 0.00003298
Iteration 110/1000 | Loss: 0.00003297
Iteration 111/1000 | Loss: 0.00003295
Iteration 112/1000 | Loss: 0.00003292
Iteration 113/1000 | Loss: 0.00004132
Iteration 114/1000 | Loss: 0.00003746
Iteration 115/1000 | Loss: 0.00004507
Iteration 116/1000 | Loss: 0.00004033
Iteration 117/1000 | Loss: 0.00004387
Iteration 118/1000 | Loss: 0.00003655
Iteration 119/1000 | Loss: 0.00004213
Iteration 120/1000 | Loss: 0.00004571
Iteration 121/1000 | Loss: 0.00004107
Iteration 122/1000 | Loss: 0.00004765
Iteration 123/1000 | Loss: 0.00003717
Iteration 124/1000 | Loss: 0.00003595
Iteration 125/1000 | Loss: 0.00003503
Iteration 126/1000 | Loss: 0.00005917
Iteration 127/1000 | Loss: 0.00004965
Iteration 128/1000 | Loss: 0.00003743
Iteration 129/1000 | Loss: 0.00005968
Iteration 130/1000 | Loss: 0.00004692
Iteration 131/1000 | Loss: 0.00004876
Iteration 132/1000 | Loss: 0.00004024
Iteration 133/1000 | Loss: 0.00003831
Iteration 134/1000 | Loss: 0.00003758
Iteration 135/1000 | Loss: 0.00003669
Iteration 136/1000 | Loss: 0.00003598
Iteration 137/1000 | Loss: 0.00003527
Iteration 138/1000 | Loss: 0.00003470
Iteration 139/1000 | Loss: 0.00004452
Iteration 140/1000 | Loss: 0.00004256
Iteration 141/1000 | Loss: 0.00004079
Iteration 142/1000 | Loss: 0.00003867
Iteration 143/1000 | Loss: 0.00003687
Iteration 144/1000 | Loss: 0.00004335
Iteration 145/1000 | Loss: 0.00003587
Iteration 146/1000 | Loss: 0.00003430
Iteration 147/1000 | Loss: 0.00003341
Iteration 148/1000 | Loss: 0.00003297
Iteration 149/1000 | Loss: 0.00003245
Iteration 150/1000 | Loss: 0.00003213
Iteration 151/1000 | Loss: 0.00003192
Iteration 152/1000 | Loss: 0.00003184
Iteration 153/1000 | Loss: 0.00003172
Iteration 154/1000 | Loss: 0.00003166
Iteration 155/1000 | Loss: 0.00003159
Iteration 156/1000 | Loss: 0.00003159
Iteration 157/1000 | Loss: 0.00003158
Iteration 158/1000 | Loss: 0.00003158
Iteration 159/1000 | Loss: 0.00003157
Iteration 160/1000 | Loss: 0.00003157
Iteration 161/1000 | Loss: 0.00003157
Iteration 162/1000 | Loss: 0.00003157
Iteration 163/1000 | Loss: 0.00003156
Iteration 164/1000 | Loss: 0.00003156
Iteration 165/1000 | Loss: 0.00003156
Iteration 166/1000 | Loss: 0.00003156
Iteration 167/1000 | Loss: 0.00003156
Iteration 168/1000 | Loss: 0.00003156
Iteration 169/1000 | Loss: 0.00003156
Iteration 170/1000 | Loss: 0.00003156
Iteration 171/1000 | Loss: 0.00003155
Iteration 172/1000 | Loss: 0.00003155
Iteration 173/1000 | Loss: 0.00003155
Iteration 174/1000 | Loss: 0.00003155
Iteration 175/1000 | Loss: 0.00003154
Iteration 176/1000 | Loss: 0.00003154
Iteration 177/1000 | Loss: 0.00003154
Iteration 178/1000 | Loss: 0.00003154
Iteration 179/1000 | Loss: 0.00003154
Iteration 180/1000 | Loss: 0.00003154
Iteration 181/1000 | Loss: 0.00003153
Iteration 182/1000 | Loss: 0.00003153
Iteration 183/1000 | Loss: 0.00003153
Iteration 184/1000 | Loss: 0.00003153
Iteration 185/1000 | Loss: 0.00003153
Iteration 186/1000 | Loss: 0.00003153
Iteration 187/1000 | Loss: 0.00003153
Iteration 188/1000 | Loss: 0.00003153
Iteration 189/1000 | Loss: 0.00003153
Iteration 190/1000 | Loss: 0.00003153
Iteration 191/1000 | Loss: 0.00003153
Iteration 192/1000 | Loss: 0.00003153
Iteration 193/1000 | Loss: 0.00003153
Iteration 194/1000 | Loss: 0.00003152
Iteration 195/1000 | Loss: 0.00003152
Iteration 196/1000 | Loss: 0.00003152
Iteration 197/1000 | Loss: 0.00003152
Iteration 198/1000 | Loss: 0.00003152
Iteration 199/1000 | Loss: 0.00003152
Iteration 200/1000 | Loss: 0.00003151
Iteration 201/1000 | Loss: 0.00003151
Iteration 202/1000 | Loss: 0.00003151
Iteration 203/1000 | Loss: 0.00003151
Iteration 204/1000 | Loss: 0.00003151
Iteration 205/1000 | Loss: 0.00003151
Iteration 206/1000 | Loss: 0.00003151
Iteration 207/1000 | Loss: 0.00003151
Iteration 208/1000 | Loss: 0.00003151
Iteration 209/1000 | Loss: 0.00003151
Iteration 210/1000 | Loss: 0.00003150
Iteration 211/1000 | Loss: 0.00003150
Iteration 212/1000 | Loss: 0.00003150
Iteration 213/1000 | Loss: 0.00003150
Iteration 214/1000 | Loss: 0.00003150
Iteration 215/1000 | Loss: 0.00003150
Iteration 216/1000 | Loss: 0.00003150
Iteration 217/1000 | Loss: 0.00003150
Iteration 218/1000 | Loss: 0.00003150
Iteration 219/1000 | Loss: 0.00003150
Iteration 220/1000 | Loss: 0.00003150
Iteration 221/1000 | Loss: 0.00003149
Iteration 222/1000 | Loss: 0.00003149
Iteration 223/1000 | Loss: 0.00003149
Iteration 224/1000 | Loss: 0.00003149
Iteration 225/1000 | Loss: 0.00003149
Iteration 226/1000 | Loss: 0.00003149
Iteration 227/1000 | Loss: 0.00003149
Iteration 228/1000 | Loss: 0.00003149
Iteration 229/1000 | Loss: 0.00003148
Iteration 230/1000 | Loss: 0.00003148
Iteration 231/1000 | Loss: 0.00003148
Iteration 232/1000 | Loss: 0.00003148
Iteration 233/1000 | Loss: 0.00003147
Iteration 234/1000 | Loss: 0.00003147
Iteration 235/1000 | Loss: 0.00003147
Iteration 236/1000 | Loss: 0.00003147
Iteration 237/1000 | Loss: 0.00003147
Iteration 238/1000 | Loss: 0.00003147
Iteration 239/1000 | Loss: 0.00003147
Iteration 240/1000 | Loss: 0.00003147
Iteration 241/1000 | Loss: 0.00003146
Iteration 242/1000 | Loss: 0.00003146
Iteration 243/1000 | Loss: 0.00003146
Iteration 244/1000 | Loss: 0.00003146
Iteration 245/1000 | Loss: 0.00003146
Iteration 246/1000 | Loss: 0.00003146
Iteration 247/1000 | Loss: 0.00003146
Iteration 248/1000 | Loss: 0.00003146
Iteration 249/1000 | Loss: 0.00003146
Iteration 250/1000 | Loss: 0.00003145
Iteration 251/1000 | Loss: 0.00003145
Iteration 252/1000 | Loss: 0.00003145
Iteration 253/1000 | Loss: 0.00003145
Iteration 254/1000 | Loss: 0.00003145
Iteration 255/1000 | Loss: 0.00003145
Iteration 256/1000 | Loss: 0.00003145
Iteration 257/1000 | Loss: 0.00003145
Iteration 258/1000 | Loss: 0.00003145
Iteration 259/1000 | Loss: 0.00003145
Iteration 260/1000 | Loss: 0.00003145
Iteration 261/1000 | Loss: 0.00003145
Iteration 262/1000 | Loss: 0.00003145
Iteration 263/1000 | Loss: 0.00003145
Iteration 264/1000 | Loss: 0.00003145
Iteration 265/1000 | Loss: 0.00003145
Iteration 266/1000 | Loss: 0.00003145
Iteration 267/1000 | Loss: 0.00003145
Iteration 268/1000 | Loss: 0.00003145
Iteration 269/1000 | Loss: 0.00003144
Iteration 270/1000 | Loss: 0.00003144
Iteration 271/1000 | Loss: 0.00003144
Iteration 272/1000 | Loss: 0.00003144
Iteration 273/1000 | Loss: 0.00003144
Iteration 274/1000 | Loss: 0.00003144
Iteration 275/1000 | Loss: 0.00003144
Iteration 276/1000 | Loss: 0.00003144
Iteration 277/1000 | Loss: 0.00003144
Iteration 278/1000 | Loss: 0.00003144
Iteration 279/1000 | Loss: 0.00003144
Iteration 280/1000 | Loss: 0.00003144
Iteration 281/1000 | Loss: 0.00003144
Iteration 282/1000 | Loss: 0.00003144
Iteration 283/1000 | Loss: 0.00003144
Iteration 284/1000 | Loss: 0.00003144
Iteration 285/1000 | Loss: 0.00003144
Iteration 286/1000 | Loss: 0.00003144
Iteration 287/1000 | Loss: 0.00003144
Iteration 288/1000 | Loss: 0.00003144
Iteration 289/1000 | Loss: 0.00003144
Iteration 290/1000 | Loss: 0.00003144
Iteration 291/1000 | Loss: 0.00003144
Iteration 292/1000 | Loss: 0.00003143
Iteration 293/1000 | Loss: 0.00003143
Iteration 294/1000 | Loss: 0.00003143
Iteration 295/1000 | Loss: 0.00003143
Iteration 296/1000 | Loss: 0.00003143
Iteration 297/1000 | Loss: 0.00003143
Iteration 298/1000 | Loss: 0.00003143
Iteration 299/1000 | Loss: 0.00003143
Iteration 300/1000 | Loss: 0.00003143
Iteration 301/1000 | Loss: 0.00003143
Iteration 302/1000 | Loss: 0.00003143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [3.143414141959511e-05, 3.143414141959511e-05, 3.143414141959511e-05, 3.143414141959511e-05, 3.143414141959511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.143414141959511e-05

Optimization complete. Final v2v error: 4.361837387084961 mm

Highest mean error: 8.223457336425781 mm for frame 118

Lowest mean error: 3.200317859649658 mm for frame 74

Saving results

Total time: 230.63198232650757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853075
Iteration 2/25 | Loss: 0.00174508
Iteration 3/25 | Loss: 0.00098420
Iteration 4/25 | Loss: 0.00084418
Iteration 5/25 | Loss: 0.00083428
Iteration 6/25 | Loss: 0.00083299
Iteration 7/25 | Loss: 0.00083299
Iteration 8/25 | Loss: 0.00083299
Iteration 9/25 | Loss: 0.00083299
Iteration 10/25 | Loss: 0.00083299
Iteration 11/25 | Loss: 0.00083299
Iteration 12/25 | Loss: 0.00083299
Iteration 13/25 | Loss: 0.00083299
Iteration 14/25 | Loss: 0.00083299
Iteration 15/25 | Loss: 0.00083299
Iteration 16/25 | Loss: 0.00083299
Iteration 17/25 | Loss: 0.00083299
Iteration 18/25 | Loss: 0.00083299
Iteration 19/25 | Loss: 0.00083299
Iteration 20/25 | Loss: 0.00083299
Iteration 21/25 | Loss: 0.00083299
Iteration 22/25 | Loss: 0.00083299
Iteration 23/25 | Loss: 0.00083299
Iteration 24/25 | Loss: 0.00083299
Iteration 25/25 | Loss: 0.00083299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40322495
Iteration 2/25 | Loss: 0.00049274
Iteration 3/25 | Loss: 0.00049271
Iteration 4/25 | Loss: 0.00049271
Iteration 5/25 | Loss: 0.00049271
Iteration 6/25 | Loss: 0.00049271
Iteration 7/25 | Loss: 0.00049271
Iteration 8/25 | Loss: 0.00049271
Iteration 9/25 | Loss: 0.00049271
Iteration 10/25 | Loss: 0.00049271
Iteration 11/25 | Loss: 0.00049271
Iteration 12/25 | Loss: 0.00049271
Iteration 13/25 | Loss: 0.00049271
Iteration 14/25 | Loss: 0.00049271
Iteration 15/25 | Loss: 0.00049271
Iteration 16/25 | Loss: 0.00049271
Iteration 17/25 | Loss: 0.00049271
Iteration 18/25 | Loss: 0.00049271
Iteration 19/25 | Loss: 0.00049271
Iteration 20/25 | Loss: 0.00049271
Iteration 21/25 | Loss: 0.00049271
Iteration 22/25 | Loss: 0.00049271
Iteration 23/25 | Loss: 0.00049271
Iteration 24/25 | Loss: 0.00049271
Iteration 25/25 | Loss: 0.00049271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049271
Iteration 2/1000 | Loss: 0.00004149
Iteration 3/1000 | Loss: 0.00002710
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002203
Iteration 8/1000 | Loss: 0.00002174
Iteration 9/1000 | Loss: 0.00002170
Iteration 10/1000 | Loss: 0.00002165
Iteration 11/1000 | Loss: 0.00002165
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002162
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002144
Iteration 23/1000 | Loss: 0.00002143
Iteration 24/1000 | Loss: 0.00002141
Iteration 25/1000 | Loss: 0.00002140
Iteration 26/1000 | Loss: 0.00002140
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002139
Iteration 30/1000 | Loss: 0.00002139
Iteration 31/1000 | Loss: 0.00002139
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002137
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002136
Iteration 40/1000 | Loss: 0.00002136
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002135
Iteration 43/1000 | Loss: 0.00002135
Iteration 44/1000 | Loss: 0.00002134
Iteration 45/1000 | Loss: 0.00002134
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002133
Iteration 49/1000 | Loss: 0.00002133
Iteration 50/1000 | Loss: 0.00002133
Iteration 51/1000 | Loss: 0.00002133
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002131
Iteration 57/1000 | Loss: 0.00002130
Iteration 58/1000 | Loss: 0.00002130
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002130
Iteration 62/1000 | Loss: 0.00002129
Iteration 63/1000 | Loss: 0.00002129
Iteration 64/1000 | Loss: 0.00002129
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002127
Iteration 68/1000 | Loss: 0.00002127
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002127
Iteration 72/1000 | Loss: 0.00002127
Iteration 73/1000 | Loss: 0.00002127
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002125
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002124
Iteration 128/1000 | Loss: 0.00002124
Iteration 129/1000 | Loss: 0.00002124
Iteration 130/1000 | Loss: 0.00002124
Iteration 131/1000 | Loss: 0.00002124
Iteration 132/1000 | Loss: 0.00002124
Iteration 133/1000 | Loss: 0.00002124
Iteration 134/1000 | Loss: 0.00002124
Iteration 135/1000 | Loss: 0.00002124
Iteration 136/1000 | Loss: 0.00002124
Iteration 137/1000 | Loss: 0.00002124
Iteration 138/1000 | Loss: 0.00002124
Iteration 139/1000 | Loss: 0.00002124
Iteration 140/1000 | Loss: 0.00002124
Iteration 141/1000 | Loss: 0.00002124
Iteration 142/1000 | Loss: 0.00002124
Iteration 143/1000 | Loss: 0.00002124
Iteration 144/1000 | Loss: 0.00002124
Iteration 145/1000 | Loss: 0.00002124
Iteration 146/1000 | Loss: 0.00002124
Iteration 147/1000 | Loss: 0.00002124
Iteration 148/1000 | Loss: 0.00002124
Iteration 149/1000 | Loss: 0.00002124
Iteration 150/1000 | Loss: 0.00002124
Iteration 151/1000 | Loss: 0.00002124
Iteration 152/1000 | Loss: 0.00002124
Iteration 153/1000 | Loss: 0.00002124
Iteration 154/1000 | Loss: 0.00002124
Iteration 155/1000 | Loss: 0.00002124
Iteration 156/1000 | Loss: 0.00002124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.1241934518911876e-05, 2.1241934518911876e-05, 2.1241934518911876e-05, 2.1241934518911876e-05, 2.1241934518911876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1241934518911876e-05

Optimization complete. Final v2v error: 3.877422571182251 mm

Highest mean error: 4.111416339874268 mm for frame 77

Lowest mean error: 3.725281000137329 mm for frame 158

Saving results

Total time: 37.020829916000366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913674
Iteration 2/25 | Loss: 0.00078072
Iteration 3/25 | Loss: 0.00062722
Iteration 4/25 | Loss: 0.00059412
Iteration 5/25 | Loss: 0.00058913
Iteration 6/25 | Loss: 0.00058785
Iteration 7/25 | Loss: 0.00058764
Iteration 8/25 | Loss: 0.00058764
Iteration 9/25 | Loss: 0.00058764
Iteration 10/25 | Loss: 0.00058764
Iteration 11/25 | Loss: 0.00058764
Iteration 12/25 | Loss: 0.00058764
Iteration 13/25 | Loss: 0.00058764
Iteration 14/25 | Loss: 0.00058764
Iteration 15/25 | Loss: 0.00058764
Iteration 16/25 | Loss: 0.00058764
Iteration 17/25 | Loss: 0.00058764
Iteration 18/25 | Loss: 0.00058764
Iteration 19/25 | Loss: 0.00058764
Iteration 20/25 | Loss: 0.00058764
Iteration 21/25 | Loss: 0.00058764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005876383511349559, 0.0005876383511349559, 0.0005876383511349559, 0.0005876383511349559, 0.0005876383511349559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005876383511349559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02000618
Iteration 2/25 | Loss: 0.00023068
Iteration 3/25 | Loss: 0.00023067
Iteration 4/25 | Loss: 0.00023067
Iteration 5/25 | Loss: 0.00023067
Iteration 6/25 | Loss: 0.00023067
Iteration 7/25 | Loss: 0.00023067
Iteration 8/25 | Loss: 0.00023067
Iteration 9/25 | Loss: 0.00023067
Iteration 10/25 | Loss: 0.00023067
Iteration 11/25 | Loss: 0.00023067
Iteration 12/25 | Loss: 0.00023067
Iteration 13/25 | Loss: 0.00023067
Iteration 14/25 | Loss: 0.00023067
Iteration 15/25 | Loss: 0.00023067
Iteration 16/25 | Loss: 0.00023067
Iteration 17/25 | Loss: 0.00023067
Iteration 18/25 | Loss: 0.00023067
Iteration 19/25 | Loss: 0.00023067
Iteration 20/25 | Loss: 0.00023067
Iteration 21/25 | Loss: 0.00023067
Iteration 22/25 | Loss: 0.00023067
Iteration 23/25 | Loss: 0.00023067
Iteration 24/25 | Loss: 0.00023067
Iteration 25/25 | Loss: 0.00023067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023067
Iteration 2/1000 | Loss: 0.00002594
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001340
Iteration 5/1000 | Loss: 0.00001245
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001114
Iteration 11/1000 | Loss: 0.00001109
Iteration 12/1000 | Loss: 0.00001108
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001105
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001094
Iteration 43/1000 | Loss: 0.00001094
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001093
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001088
Iteration 61/1000 | Loss: 0.00001088
Iteration 62/1000 | Loss: 0.00001088
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001083
Iteration 95/1000 | Loss: 0.00001083
Iteration 96/1000 | Loss: 0.00001083
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001082
Iteration 111/1000 | Loss: 0.00001082
Iteration 112/1000 | Loss: 0.00001082
Iteration 113/1000 | Loss: 0.00001082
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001082
Iteration 116/1000 | Loss: 0.00001082
Iteration 117/1000 | Loss: 0.00001082
Iteration 118/1000 | Loss: 0.00001082
Iteration 119/1000 | Loss: 0.00001082
Iteration 120/1000 | Loss: 0.00001082
Iteration 121/1000 | Loss: 0.00001082
Iteration 122/1000 | Loss: 0.00001082
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.0822089279827196e-05, 1.0822089279827196e-05, 1.0822089279827196e-05, 1.0822089279827196e-05, 1.0822089279827196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0822089279827196e-05

Optimization complete. Final v2v error: 2.8349449634552 mm

Highest mean error: 3.0821588039398193 mm for frame 63

Lowest mean error: 2.683882713317871 mm for frame 30

Saving results

Total time: 32.26889896392822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049526
Iteration 2/25 | Loss: 0.00157110
Iteration 3/25 | Loss: 0.00096096
Iteration 4/25 | Loss: 0.00090920
Iteration 5/25 | Loss: 0.00089400
Iteration 6/25 | Loss: 0.00088929
Iteration 7/25 | Loss: 0.00088840
Iteration 8/25 | Loss: 0.00088823
Iteration 9/25 | Loss: 0.00088823
Iteration 10/25 | Loss: 0.00088823
Iteration 11/25 | Loss: 0.00088823
Iteration 12/25 | Loss: 0.00088823
Iteration 13/25 | Loss: 0.00088823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008882253896445036, 0.0008882253896445036, 0.0008882253896445036, 0.0008882253896445036, 0.0008882253896445036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008882253896445036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58851582
Iteration 2/25 | Loss: 0.00020429
Iteration 3/25 | Loss: 0.00020429
Iteration 4/25 | Loss: 0.00020429
Iteration 5/25 | Loss: 0.00020429
Iteration 6/25 | Loss: 0.00020429
Iteration 7/25 | Loss: 0.00020429
Iteration 8/25 | Loss: 0.00020429
Iteration 9/25 | Loss: 0.00020429
Iteration 10/25 | Loss: 0.00020429
Iteration 11/25 | Loss: 0.00020429
Iteration 12/25 | Loss: 0.00020429
Iteration 13/25 | Loss: 0.00020429
Iteration 14/25 | Loss: 0.00020429
Iteration 15/25 | Loss: 0.00020429
Iteration 16/25 | Loss: 0.00020429
Iteration 17/25 | Loss: 0.00020429
Iteration 18/25 | Loss: 0.00020429
Iteration 19/25 | Loss: 0.00020429
Iteration 20/25 | Loss: 0.00020429
Iteration 21/25 | Loss: 0.00020429
Iteration 22/25 | Loss: 0.00020429
Iteration 23/25 | Loss: 0.00020429
Iteration 24/25 | Loss: 0.00020429
Iteration 25/25 | Loss: 0.00020429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020429
Iteration 2/1000 | Loss: 0.00007152
Iteration 3/1000 | Loss: 0.00004662
Iteration 4/1000 | Loss: 0.00004161
Iteration 5/1000 | Loss: 0.00003961
Iteration 6/1000 | Loss: 0.00003865
Iteration 7/1000 | Loss: 0.00003787
Iteration 8/1000 | Loss: 0.00003719
Iteration 9/1000 | Loss: 0.00003675
Iteration 10/1000 | Loss: 0.00003639
Iteration 11/1000 | Loss: 0.00003614
Iteration 12/1000 | Loss: 0.00003587
Iteration 13/1000 | Loss: 0.00003565
Iteration 14/1000 | Loss: 0.00003551
Iteration 15/1000 | Loss: 0.00003547
Iteration 16/1000 | Loss: 0.00003541
Iteration 17/1000 | Loss: 0.00003539
Iteration 18/1000 | Loss: 0.00003533
Iteration 19/1000 | Loss: 0.00003518
Iteration 20/1000 | Loss: 0.00003516
Iteration 21/1000 | Loss: 0.00003513
Iteration 22/1000 | Loss: 0.00003513
Iteration 23/1000 | Loss: 0.00003512
Iteration 24/1000 | Loss: 0.00003510
Iteration 25/1000 | Loss: 0.00003510
Iteration 26/1000 | Loss: 0.00003510
Iteration 27/1000 | Loss: 0.00003510
Iteration 28/1000 | Loss: 0.00003510
Iteration 29/1000 | Loss: 0.00003510
Iteration 30/1000 | Loss: 0.00003510
Iteration 31/1000 | Loss: 0.00003509
Iteration 32/1000 | Loss: 0.00003509
Iteration 33/1000 | Loss: 0.00003508
Iteration 34/1000 | Loss: 0.00003507
Iteration 35/1000 | Loss: 0.00003507
Iteration 36/1000 | Loss: 0.00003506
Iteration 37/1000 | Loss: 0.00003506
Iteration 38/1000 | Loss: 0.00003506
Iteration 39/1000 | Loss: 0.00003506
Iteration 40/1000 | Loss: 0.00003506
Iteration 41/1000 | Loss: 0.00003505
Iteration 42/1000 | Loss: 0.00003505
Iteration 43/1000 | Loss: 0.00003505
Iteration 44/1000 | Loss: 0.00003505
Iteration 45/1000 | Loss: 0.00003504
Iteration 46/1000 | Loss: 0.00003503
Iteration 47/1000 | Loss: 0.00003503
Iteration 48/1000 | Loss: 0.00003503
Iteration 49/1000 | Loss: 0.00003503
Iteration 50/1000 | Loss: 0.00003503
Iteration 51/1000 | Loss: 0.00003502
Iteration 52/1000 | Loss: 0.00003502
Iteration 53/1000 | Loss: 0.00003502
Iteration 54/1000 | Loss: 0.00003502
Iteration 55/1000 | Loss: 0.00003501
Iteration 56/1000 | Loss: 0.00003501
Iteration 57/1000 | Loss: 0.00003500
Iteration 58/1000 | Loss: 0.00003500
Iteration 59/1000 | Loss: 0.00003500
Iteration 60/1000 | Loss: 0.00003499
Iteration 61/1000 | Loss: 0.00003499
Iteration 62/1000 | Loss: 0.00003498
Iteration 63/1000 | Loss: 0.00003498
Iteration 64/1000 | Loss: 0.00003497
Iteration 65/1000 | Loss: 0.00003497
Iteration 66/1000 | Loss: 0.00003497
Iteration 67/1000 | Loss: 0.00003496
Iteration 68/1000 | Loss: 0.00003495
Iteration 69/1000 | Loss: 0.00003495
Iteration 70/1000 | Loss: 0.00003495
Iteration 71/1000 | Loss: 0.00003494
Iteration 72/1000 | Loss: 0.00003494
Iteration 73/1000 | Loss: 0.00003493
Iteration 74/1000 | Loss: 0.00003493
Iteration 75/1000 | Loss: 0.00003493
Iteration 76/1000 | Loss: 0.00003493
Iteration 77/1000 | Loss: 0.00003493
Iteration 78/1000 | Loss: 0.00003492
Iteration 79/1000 | Loss: 0.00003492
Iteration 80/1000 | Loss: 0.00003492
Iteration 81/1000 | Loss: 0.00003492
Iteration 82/1000 | Loss: 0.00003492
Iteration 83/1000 | Loss: 0.00003491
Iteration 84/1000 | Loss: 0.00003491
Iteration 85/1000 | Loss: 0.00003491
Iteration 86/1000 | Loss: 0.00003491
Iteration 87/1000 | Loss: 0.00003490
Iteration 88/1000 | Loss: 0.00003490
Iteration 89/1000 | Loss: 0.00003490
Iteration 90/1000 | Loss: 0.00003490
Iteration 91/1000 | Loss: 0.00003489
Iteration 92/1000 | Loss: 0.00003489
Iteration 93/1000 | Loss: 0.00003489
Iteration 94/1000 | Loss: 0.00003489
Iteration 95/1000 | Loss: 0.00003489
Iteration 96/1000 | Loss: 0.00003489
Iteration 97/1000 | Loss: 0.00003489
Iteration 98/1000 | Loss: 0.00003489
Iteration 99/1000 | Loss: 0.00003489
Iteration 100/1000 | Loss: 0.00003489
Iteration 101/1000 | Loss: 0.00003489
Iteration 102/1000 | Loss: 0.00003488
Iteration 103/1000 | Loss: 0.00003488
Iteration 104/1000 | Loss: 0.00003488
Iteration 105/1000 | Loss: 0.00003488
Iteration 106/1000 | Loss: 0.00003488
Iteration 107/1000 | Loss: 0.00003488
Iteration 108/1000 | Loss: 0.00003488
Iteration 109/1000 | Loss: 0.00003488
Iteration 110/1000 | Loss: 0.00003487
Iteration 111/1000 | Loss: 0.00003487
Iteration 112/1000 | Loss: 0.00003487
Iteration 113/1000 | Loss: 0.00003487
Iteration 114/1000 | Loss: 0.00003487
Iteration 115/1000 | Loss: 0.00003487
Iteration 116/1000 | Loss: 0.00003487
Iteration 117/1000 | Loss: 0.00003487
Iteration 118/1000 | Loss: 0.00003487
Iteration 119/1000 | Loss: 0.00003487
Iteration 120/1000 | Loss: 0.00003486
Iteration 121/1000 | Loss: 0.00003486
Iteration 122/1000 | Loss: 0.00003486
Iteration 123/1000 | Loss: 0.00003486
Iteration 124/1000 | Loss: 0.00003486
Iteration 125/1000 | Loss: 0.00003486
Iteration 126/1000 | Loss: 0.00003486
Iteration 127/1000 | Loss: 0.00003486
Iteration 128/1000 | Loss: 0.00003486
Iteration 129/1000 | Loss: 0.00003486
Iteration 130/1000 | Loss: 0.00003485
Iteration 131/1000 | Loss: 0.00003485
Iteration 132/1000 | Loss: 0.00003485
Iteration 133/1000 | Loss: 0.00003485
Iteration 134/1000 | Loss: 0.00003485
Iteration 135/1000 | Loss: 0.00003485
Iteration 136/1000 | Loss: 0.00003485
Iteration 137/1000 | Loss: 0.00003484
Iteration 138/1000 | Loss: 0.00003484
Iteration 139/1000 | Loss: 0.00003484
Iteration 140/1000 | Loss: 0.00003484
Iteration 141/1000 | Loss: 0.00003484
Iteration 142/1000 | Loss: 0.00003484
Iteration 143/1000 | Loss: 0.00003484
Iteration 144/1000 | Loss: 0.00003484
Iteration 145/1000 | Loss: 0.00003484
Iteration 146/1000 | Loss: 0.00003484
Iteration 147/1000 | Loss: 0.00003484
Iteration 148/1000 | Loss: 0.00003484
Iteration 149/1000 | Loss: 0.00003483
Iteration 150/1000 | Loss: 0.00003483
Iteration 151/1000 | Loss: 0.00003483
Iteration 152/1000 | Loss: 0.00003483
Iteration 153/1000 | Loss: 0.00003483
Iteration 154/1000 | Loss: 0.00003483
Iteration 155/1000 | Loss: 0.00003483
Iteration 156/1000 | Loss: 0.00003483
Iteration 157/1000 | Loss: 0.00003483
Iteration 158/1000 | Loss: 0.00003483
Iteration 159/1000 | Loss: 0.00003483
Iteration 160/1000 | Loss: 0.00003482
Iteration 161/1000 | Loss: 0.00003482
Iteration 162/1000 | Loss: 0.00003482
Iteration 163/1000 | Loss: 0.00003482
Iteration 164/1000 | Loss: 0.00003482
Iteration 165/1000 | Loss: 0.00003482
Iteration 166/1000 | Loss: 0.00003482
Iteration 167/1000 | Loss: 0.00003482
Iteration 168/1000 | Loss: 0.00003482
Iteration 169/1000 | Loss: 0.00003482
Iteration 170/1000 | Loss: 0.00003482
Iteration 171/1000 | Loss: 0.00003482
Iteration 172/1000 | Loss: 0.00003482
Iteration 173/1000 | Loss: 0.00003482
Iteration 174/1000 | Loss: 0.00003482
Iteration 175/1000 | Loss: 0.00003482
Iteration 176/1000 | Loss: 0.00003482
Iteration 177/1000 | Loss: 0.00003482
Iteration 178/1000 | Loss: 0.00003482
Iteration 179/1000 | Loss: 0.00003481
Iteration 180/1000 | Loss: 0.00003481
Iteration 181/1000 | Loss: 0.00003481
Iteration 182/1000 | Loss: 0.00003481
Iteration 183/1000 | Loss: 0.00003481
Iteration 184/1000 | Loss: 0.00003481
Iteration 185/1000 | Loss: 0.00003481
Iteration 186/1000 | Loss: 0.00003481
Iteration 187/1000 | Loss: 0.00003481
Iteration 188/1000 | Loss: 0.00003481
Iteration 189/1000 | Loss: 0.00003481
Iteration 190/1000 | Loss: 0.00003480
Iteration 191/1000 | Loss: 0.00003480
Iteration 192/1000 | Loss: 0.00003480
Iteration 193/1000 | Loss: 0.00003480
Iteration 194/1000 | Loss: 0.00003480
Iteration 195/1000 | Loss: 0.00003480
Iteration 196/1000 | Loss: 0.00003480
Iteration 197/1000 | Loss: 0.00003480
Iteration 198/1000 | Loss: 0.00003480
Iteration 199/1000 | Loss: 0.00003480
Iteration 200/1000 | Loss: 0.00003479
Iteration 201/1000 | Loss: 0.00003479
Iteration 202/1000 | Loss: 0.00003479
Iteration 203/1000 | Loss: 0.00003479
Iteration 204/1000 | Loss: 0.00003479
Iteration 205/1000 | Loss: 0.00003479
Iteration 206/1000 | Loss: 0.00003479
Iteration 207/1000 | Loss: 0.00003479
Iteration 208/1000 | Loss: 0.00003479
Iteration 209/1000 | Loss: 0.00003479
Iteration 210/1000 | Loss: 0.00003479
Iteration 211/1000 | Loss: 0.00003479
Iteration 212/1000 | Loss: 0.00003479
Iteration 213/1000 | Loss: 0.00003479
Iteration 214/1000 | Loss: 0.00003479
Iteration 215/1000 | Loss: 0.00003479
Iteration 216/1000 | Loss: 0.00003479
Iteration 217/1000 | Loss: 0.00003479
Iteration 218/1000 | Loss: 0.00003479
Iteration 219/1000 | Loss: 0.00003479
Iteration 220/1000 | Loss: 0.00003479
Iteration 221/1000 | Loss: 0.00003479
Iteration 222/1000 | Loss: 0.00003478
Iteration 223/1000 | Loss: 0.00003478
Iteration 224/1000 | Loss: 0.00003478
Iteration 225/1000 | Loss: 0.00003478
Iteration 226/1000 | Loss: 0.00003478
Iteration 227/1000 | Loss: 0.00003478
Iteration 228/1000 | Loss: 0.00003478
Iteration 229/1000 | Loss: 0.00003478
Iteration 230/1000 | Loss: 0.00003478
Iteration 231/1000 | Loss: 0.00003478
Iteration 232/1000 | Loss: 0.00003478
Iteration 233/1000 | Loss: 0.00003478
Iteration 234/1000 | Loss: 0.00003478
Iteration 235/1000 | Loss: 0.00003478
Iteration 236/1000 | Loss: 0.00003478
Iteration 237/1000 | Loss: 0.00003478
Iteration 238/1000 | Loss: 0.00003478
Iteration 239/1000 | Loss: 0.00003478
Iteration 240/1000 | Loss: 0.00003478
Iteration 241/1000 | Loss: 0.00003478
Iteration 242/1000 | Loss: 0.00003478
Iteration 243/1000 | Loss: 0.00003478
Iteration 244/1000 | Loss: 0.00003477
Iteration 245/1000 | Loss: 0.00003477
Iteration 246/1000 | Loss: 0.00003477
Iteration 247/1000 | Loss: 0.00003477
Iteration 248/1000 | Loss: 0.00003477
Iteration 249/1000 | Loss: 0.00003477
Iteration 250/1000 | Loss: 0.00003477
Iteration 251/1000 | Loss: 0.00003477
Iteration 252/1000 | Loss: 0.00003477
Iteration 253/1000 | Loss: 0.00003477
Iteration 254/1000 | Loss: 0.00003477
Iteration 255/1000 | Loss: 0.00003477
Iteration 256/1000 | Loss: 0.00003477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [3.477237987681292e-05, 3.477237987681292e-05, 3.477237987681292e-05, 3.477237987681292e-05, 3.477237987681292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.477237987681292e-05

Optimization complete. Final v2v error: 4.840543746948242 mm

Highest mean error: 5.5538649559021 mm for frame 0

Lowest mean error: 4.14698600769043 mm for frame 30

Saving results

Total time: 51.761661767959595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449346
Iteration 2/25 | Loss: 0.00083337
Iteration 3/25 | Loss: 0.00072997
Iteration 4/25 | Loss: 0.00070325
Iteration 5/25 | Loss: 0.00069721
Iteration 6/25 | Loss: 0.00069610
Iteration 7/25 | Loss: 0.00069608
Iteration 8/25 | Loss: 0.00069608
Iteration 9/25 | Loss: 0.00069608
Iteration 10/25 | Loss: 0.00069608
Iteration 11/25 | Loss: 0.00069608
Iteration 12/25 | Loss: 0.00069608
Iteration 13/25 | Loss: 0.00069608
Iteration 14/25 | Loss: 0.00069608
Iteration 15/25 | Loss: 0.00069608
Iteration 16/25 | Loss: 0.00069608
Iteration 17/25 | Loss: 0.00069608
Iteration 18/25 | Loss: 0.00069608
Iteration 19/25 | Loss: 0.00069608
Iteration 20/25 | Loss: 0.00069608
Iteration 21/25 | Loss: 0.00069608
Iteration 22/25 | Loss: 0.00069608
Iteration 23/25 | Loss: 0.00069608
Iteration 24/25 | Loss: 0.00069608
Iteration 25/25 | Loss: 0.00069608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42674470
Iteration 2/25 | Loss: 0.00026898
Iteration 3/25 | Loss: 0.00026897
Iteration 4/25 | Loss: 0.00026897
Iteration 5/25 | Loss: 0.00026897
Iteration 6/25 | Loss: 0.00026897
Iteration 7/25 | Loss: 0.00026897
Iteration 8/25 | Loss: 0.00026897
Iteration 9/25 | Loss: 0.00026897
Iteration 10/25 | Loss: 0.00026897
Iteration 11/25 | Loss: 0.00026897
Iteration 12/25 | Loss: 0.00026897
Iteration 13/25 | Loss: 0.00026897
Iteration 14/25 | Loss: 0.00026897
Iteration 15/25 | Loss: 0.00026897
Iteration 16/25 | Loss: 0.00026897
Iteration 17/25 | Loss: 0.00026897
Iteration 18/25 | Loss: 0.00026897
Iteration 19/25 | Loss: 0.00026897
Iteration 20/25 | Loss: 0.00026897
Iteration 21/25 | Loss: 0.00026897
Iteration 22/25 | Loss: 0.00026897
Iteration 23/25 | Loss: 0.00026897
Iteration 24/25 | Loss: 0.00026897
Iteration 25/25 | Loss: 0.00026897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026897
Iteration 2/1000 | Loss: 0.00004764
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00003113
Iteration 5/1000 | Loss: 0.00002919
Iteration 6/1000 | Loss: 0.00002814
Iteration 7/1000 | Loss: 0.00002715
Iteration 8/1000 | Loss: 0.00002652
Iteration 9/1000 | Loss: 0.00002623
Iteration 10/1000 | Loss: 0.00002603
Iteration 11/1000 | Loss: 0.00002583
Iteration 12/1000 | Loss: 0.00002576
Iteration 13/1000 | Loss: 0.00002573
Iteration 14/1000 | Loss: 0.00002561
Iteration 15/1000 | Loss: 0.00002557
Iteration 16/1000 | Loss: 0.00002556
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002556
Iteration 19/1000 | Loss: 0.00002555
Iteration 20/1000 | Loss: 0.00002554
Iteration 21/1000 | Loss: 0.00002553
Iteration 22/1000 | Loss: 0.00002552
Iteration 23/1000 | Loss: 0.00002552
Iteration 24/1000 | Loss: 0.00002551
Iteration 25/1000 | Loss: 0.00002551
Iteration 26/1000 | Loss: 0.00002551
Iteration 27/1000 | Loss: 0.00002550
Iteration 28/1000 | Loss: 0.00002550
Iteration 29/1000 | Loss: 0.00002548
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002546
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002544
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002540
Iteration 36/1000 | Loss: 0.00002539
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002533
Iteration 39/1000 | Loss: 0.00002532
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002529
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002529
Iteration 44/1000 | Loss: 0.00002529
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00002529
Iteration 47/1000 | Loss: 0.00002529
Iteration 48/1000 | Loss: 0.00002529
Iteration 49/1000 | Loss: 0.00002529
Iteration 50/1000 | Loss: 0.00002529
Iteration 51/1000 | Loss: 0.00002528
Iteration 52/1000 | Loss: 0.00002528
Iteration 53/1000 | Loss: 0.00002528
Iteration 54/1000 | Loss: 0.00002528
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002527
Iteration 57/1000 | Loss: 0.00002527
Iteration 58/1000 | Loss: 0.00002527
Iteration 59/1000 | Loss: 0.00002527
Iteration 60/1000 | Loss: 0.00002527
Iteration 61/1000 | Loss: 0.00002526
Iteration 62/1000 | Loss: 0.00002526
Iteration 63/1000 | Loss: 0.00002526
Iteration 64/1000 | Loss: 0.00002526
Iteration 65/1000 | Loss: 0.00002526
Iteration 66/1000 | Loss: 0.00002526
Iteration 67/1000 | Loss: 0.00002526
Iteration 68/1000 | Loss: 0.00002525
Iteration 69/1000 | Loss: 0.00002525
Iteration 70/1000 | Loss: 0.00002525
Iteration 71/1000 | Loss: 0.00002525
Iteration 72/1000 | Loss: 0.00002525
Iteration 73/1000 | Loss: 0.00002525
Iteration 74/1000 | Loss: 0.00002524
Iteration 75/1000 | Loss: 0.00002524
Iteration 76/1000 | Loss: 0.00002524
Iteration 77/1000 | Loss: 0.00002524
Iteration 78/1000 | Loss: 0.00002524
Iteration 79/1000 | Loss: 0.00002524
Iteration 80/1000 | Loss: 0.00002524
Iteration 81/1000 | Loss: 0.00002524
Iteration 82/1000 | Loss: 0.00002524
Iteration 83/1000 | Loss: 0.00002524
Iteration 84/1000 | Loss: 0.00002524
Iteration 85/1000 | Loss: 0.00002523
Iteration 86/1000 | Loss: 0.00002523
Iteration 87/1000 | Loss: 0.00002523
Iteration 88/1000 | Loss: 0.00002523
Iteration 89/1000 | Loss: 0.00002522
Iteration 90/1000 | Loss: 0.00002522
Iteration 91/1000 | Loss: 0.00002522
Iteration 92/1000 | Loss: 0.00002522
Iteration 93/1000 | Loss: 0.00002522
Iteration 94/1000 | Loss: 0.00002522
Iteration 95/1000 | Loss: 0.00002522
Iteration 96/1000 | Loss: 0.00002521
Iteration 97/1000 | Loss: 0.00002521
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002520
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002519
Iteration 103/1000 | Loss: 0.00002519
Iteration 104/1000 | Loss: 0.00002518
Iteration 105/1000 | Loss: 0.00002518
Iteration 106/1000 | Loss: 0.00002518
Iteration 107/1000 | Loss: 0.00002518
Iteration 108/1000 | Loss: 0.00002517
Iteration 109/1000 | Loss: 0.00002517
Iteration 110/1000 | Loss: 0.00002517
Iteration 111/1000 | Loss: 0.00002517
Iteration 112/1000 | Loss: 0.00002516
Iteration 113/1000 | Loss: 0.00002516
Iteration 114/1000 | Loss: 0.00002516
Iteration 115/1000 | Loss: 0.00002516
Iteration 116/1000 | Loss: 0.00002516
Iteration 117/1000 | Loss: 0.00002516
Iteration 118/1000 | Loss: 0.00002516
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002516
Iteration 121/1000 | Loss: 0.00002516
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002516
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002516
Iteration 127/1000 | Loss: 0.00002516
Iteration 128/1000 | Loss: 0.00002516
Iteration 129/1000 | Loss: 0.00002516
Iteration 130/1000 | Loss: 0.00002516
Iteration 131/1000 | Loss: 0.00002516
Iteration 132/1000 | Loss: 0.00002515
Iteration 133/1000 | Loss: 0.00002515
Iteration 134/1000 | Loss: 0.00002515
Iteration 135/1000 | Loss: 0.00002515
Iteration 136/1000 | Loss: 0.00002515
Iteration 137/1000 | Loss: 0.00002515
Iteration 138/1000 | Loss: 0.00002515
Iteration 139/1000 | Loss: 0.00002515
Iteration 140/1000 | Loss: 0.00002515
Iteration 141/1000 | Loss: 0.00002515
Iteration 142/1000 | Loss: 0.00002515
Iteration 143/1000 | Loss: 0.00002515
Iteration 144/1000 | Loss: 0.00002515
Iteration 145/1000 | Loss: 0.00002515
Iteration 146/1000 | Loss: 0.00002514
Iteration 147/1000 | Loss: 0.00002514
Iteration 148/1000 | Loss: 0.00002514
Iteration 149/1000 | Loss: 0.00002514
Iteration 150/1000 | Loss: 0.00002514
Iteration 151/1000 | Loss: 0.00002514
Iteration 152/1000 | Loss: 0.00002514
Iteration 153/1000 | Loss: 0.00002514
Iteration 154/1000 | Loss: 0.00002514
Iteration 155/1000 | Loss: 0.00002514
Iteration 156/1000 | Loss: 0.00002514
Iteration 157/1000 | Loss: 0.00002514
Iteration 158/1000 | Loss: 0.00002514
Iteration 159/1000 | Loss: 0.00002514
Iteration 160/1000 | Loss: 0.00002514
Iteration 161/1000 | Loss: 0.00002514
Iteration 162/1000 | Loss: 0.00002514
Iteration 163/1000 | Loss: 0.00002514
Iteration 164/1000 | Loss: 0.00002514
Iteration 165/1000 | Loss: 0.00002514
Iteration 166/1000 | Loss: 0.00002514
Iteration 167/1000 | Loss: 0.00002514
Iteration 168/1000 | Loss: 0.00002514
Iteration 169/1000 | Loss: 0.00002514
Iteration 170/1000 | Loss: 0.00002514
Iteration 171/1000 | Loss: 0.00002514
Iteration 172/1000 | Loss: 0.00002514
Iteration 173/1000 | Loss: 0.00002514
Iteration 174/1000 | Loss: 0.00002514
Iteration 175/1000 | Loss: 0.00002514
Iteration 176/1000 | Loss: 0.00002514
Iteration 177/1000 | Loss: 0.00002514
Iteration 178/1000 | Loss: 0.00002514
Iteration 179/1000 | Loss: 0.00002514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.5135750547633506e-05, 2.5135750547633506e-05, 2.5135750547633506e-05, 2.5135750547633506e-05, 2.5135750547633506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5135750547633506e-05

Optimization complete. Final v2v error: 4.170558929443359 mm

Highest mean error: 4.377919673919678 mm for frame 114

Lowest mean error: 3.8812825679779053 mm for frame 65

Saving results

Total time: 40.06318807601929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886331
Iteration 2/25 | Loss: 0.00104638
Iteration 3/25 | Loss: 0.00072384
Iteration 4/25 | Loss: 0.00067202
Iteration 5/25 | Loss: 0.00065532
Iteration 6/25 | Loss: 0.00064692
Iteration 7/25 | Loss: 0.00064887
Iteration 8/25 | Loss: 0.00064260
Iteration 9/25 | Loss: 0.00064091
Iteration 10/25 | Loss: 0.00065565
Iteration 11/25 | Loss: 0.00063153
Iteration 12/25 | Loss: 0.00062926
Iteration 13/25 | Loss: 0.00062872
Iteration 14/25 | Loss: 0.00062857
Iteration 15/25 | Loss: 0.00062834
Iteration 16/25 | Loss: 0.00062803
Iteration 17/25 | Loss: 0.00062789
Iteration 18/25 | Loss: 0.00062780
Iteration 19/25 | Loss: 0.00062778
Iteration 20/25 | Loss: 0.00062778
Iteration 21/25 | Loss: 0.00062777
Iteration 22/25 | Loss: 0.00062776
Iteration 23/25 | Loss: 0.00062776
Iteration 24/25 | Loss: 0.00062776
Iteration 25/25 | Loss: 0.00062776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58651841
Iteration 2/25 | Loss: 0.00031158
Iteration 3/25 | Loss: 0.00031157
Iteration 4/25 | Loss: 0.00031157
Iteration 5/25 | Loss: 0.00031157
Iteration 6/25 | Loss: 0.00031157
Iteration 7/25 | Loss: 0.00031157
Iteration 8/25 | Loss: 0.00031157
Iteration 9/25 | Loss: 0.00031157
Iteration 10/25 | Loss: 0.00031157
Iteration 11/25 | Loss: 0.00031156
Iteration 12/25 | Loss: 0.00031156
Iteration 13/25 | Loss: 0.00031156
Iteration 14/25 | Loss: 0.00031156
Iteration 15/25 | Loss: 0.00031156
Iteration 16/25 | Loss: 0.00031156
Iteration 17/25 | Loss: 0.00031156
Iteration 18/25 | Loss: 0.00031156
Iteration 19/25 | Loss: 0.00031156
Iteration 20/25 | Loss: 0.00031156
Iteration 21/25 | Loss: 0.00031156
Iteration 22/25 | Loss: 0.00031156
Iteration 23/25 | Loss: 0.00031156
Iteration 24/25 | Loss: 0.00031156
Iteration 25/25 | Loss: 0.00031156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031156
Iteration 2/1000 | Loss: 0.00002771
Iteration 3/1000 | Loss: 0.00002062
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001633
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001625
Iteration 29/1000 | Loss: 0.00001625
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001624
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00032845
Iteration 39/1000 | Loss: 0.00002147
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001409
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001381
Iteration 64/1000 | Loss: 0.00001381
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001380
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001378
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001376
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001372
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001371
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001371
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3711891369894147e-05, 1.3711891369894147e-05, 1.3711891369894147e-05, 1.3711891369894147e-05, 1.3711891369894147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3711891369894147e-05

Optimization complete. Final v2v error: 3.0709798336029053 mm

Highest mean error: 4.5768513679504395 mm for frame 85

Lowest mean error: 2.615053415298462 mm for frame 130

Saving results

Total time: 69.94721698760986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028849
Iteration 2/25 | Loss: 0.00416794
Iteration 3/25 | Loss: 0.00261393
Iteration 4/25 | Loss: 0.00242169
Iteration 5/25 | Loss: 0.00192927
Iteration 6/25 | Loss: 0.00160178
Iteration 7/25 | Loss: 0.00152445
Iteration 8/25 | Loss: 0.00150402
Iteration 9/25 | Loss: 0.00148090
Iteration 10/25 | Loss: 0.00144466
Iteration 11/25 | Loss: 0.00142415
Iteration 12/25 | Loss: 0.00140264
Iteration 13/25 | Loss: 0.00141285
Iteration 14/25 | Loss: 0.00137023
Iteration 15/25 | Loss: 0.00137824
Iteration 16/25 | Loss: 0.00136145
Iteration 17/25 | Loss: 0.00135402
Iteration 18/25 | Loss: 0.00135191
Iteration 19/25 | Loss: 0.00135951
Iteration 20/25 | Loss: 0.00138380
Iteration 21/25 | Loss: 0.00136859
Iteration 22/25 | Loss: 0.00133183
Iteration 23/25 | Loss: 0.00127168
Iteration 24/25 | Loss: 0.00119060
Iteration 25/25 | Loss: 0.00115987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44849098
Iteration 2/25 | Loss: 0.01013821
Iteration 3/25 | Loss: 0.00473471
Iteration 4/25 | Loss: 0.00473466
Iteration 5/25 | Loss: 0.00473466
Iteration 6/25 | Loss: 0.00473466
Iteration 7/25 | Loss: 0.00473466
Iteration 8/25 | Loss: 0.00473466
Iteration 9/25 | Loss: 0.00473466
Iteration 10/25 | Loss: 0.00473466
Iteration 11/25 | Loss: 0.00473466
Iteration 12/25 | Loss: 0.00473466
Iteration 13/25 | Loss: 0.00473466
Iteration 14/25 | Loss: 0.00473466
Iteration 15/25 | Loss: 0.00473466
Iteration 16/25 | Loss: 0.00473466
Iteration 17/25 | Loss: 0.00473466
Iteration 18/25 | Loss: 0.00473466
Iteration 19/25 | Loss: 0.00473466
Iteration 20/25 | Loss: 0.00473466
Iteration 21/25 | Loss: 0.00473466
Iteration 22/25 | Loss: 0.00473466
Iteration 23/25 | Loss: 0.00473466
Iteration 24/25 | Loss: 0.00473466
Iteration 25/25 | Loss: 0.00473466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00473466
Iteration 2/1000 | Loss: 0.01100793
Iteration 3/1000 | Loss: 0.01716766
Iteration 4/1000 | Loss: 0.00377826
Iteration 5/1000 | Loss: 0.00473336
Iteration 6/1000 | Loss: 0.00840639
Iteration 7/1000 | Loss: 0.00294411
Iteration 8/1000 | Loss: 0.00164500
Iteration 9/1000 | Loss: 0.00143706
Iteration 10/1000 | Loss: 0.00055411
Iteration 11/1000 | Loss: 0.00144463
Iteration 12/1000 | Loss: 0.00338383
Iteration 13/1000 | Loss: 0.00185072
Iteration 14/1000 | Loss: 0.00399676
Iteration 15/1000 | Loss: 0.00190831
Iteration 16/1000 | Loss: 0.00107431
Iteration 17/1000 | Loss: 0.00126286
Iteration 18/1000 | Loss: 0.00239872
Iteration 19/1000 | Loss: 0.00160736
Iteration 20/1000 | Loss: 0.00221052
Iteration 21/1000 | Loss: 0.00061014
Iteration 22/1000 | Loss: 0.00187284
Iteration 23/1000 | Loss: 0.00042639
Iteration 24/1000 | Loss: 0.00041471
Iteration 25/1000 | Loss: 0.00028386
Iteration 26/1000 | Loss: 0.00017256
Iteration 27/1000 | Loss: 0.00049015
Iteration 28/1000 | Loss: 0.00126109
Iteration 29/1000 | Loss: 0.00158602
Iteration 30/1000 | Loss: 0.00042102
Iteration 31/1000 | Loss: 0.00068795
Iteration 32/1000 | Loss: 0.00010862
Iteration 33/1000 | Loss: 0.00007661
Iteration 34/1000 | Loss: 0.00013582
Iteration 35/1000 | Loss: 0.00026762
Iteration 36/1000 | Loss: 0.00025796
Iteration 37/1000 | Loss: 0.00015364
Iteration 38/1000 | Loss: 0.00007031
Iteration 39/1000 | Loss: 0.00005847
Iteration 40/1000 | Loss: 0.00047974
Iteration 41/1000 | Loss: 0.00006622
Iteration 42/1000 | Loss: 0.00006689
Iteration 43/1000 | Loss: 0.00005899
Iteration 44/1000 | Loss: 0.00006034
Iteration 45/1000 | Loss: 0.00035939
Iteration 46/1000 | Loss: 0.00004723
Iteration 47/1000 | Loss: 0.00004298
Iteration 48/1000 | Loss: 0.00006024
Iteration 49/1000 | Loss: 0.00004910
Iteration 50/1000 | Loss: 0.00018457
Iteration 51/1000 | Loss: 0.00014028
Iteration 52/1000 | Loss: 0.00007617
Iteration 53/1000 | Loss: 0.00009870
Iteration 54/1000 | Loss: 0.00037295
Iteration 55/1000 | Loss: 0.00065778
Iteration 56/1000 | Loss: 0.00004560
Iteration 57/1000 | Loss: 0.00005718
Iteration 58/1000 | Loss: 0.00112441
Iteration 59/1000 | Loss: 0.00110900
Iteration 60/1000 | Loss: 0.00004482
Iteration 61/1000 | Loss: 0.00011142
Iteration 62/1000 | Loss: 0.00008978
Iteration 63/1000 | Loss: 0.00005414
Iteration 64/1000 | Loss: 0.00005933
Iteration 65/1000 | Loss: 0.00005075
Iteration 66/1000 | Loss: 0.00013851
Iteration 67/1000 | Loss: 0.00014853
Iteration 68/1000 | Loss: 0.00023622
Iteration 69/1000 | Loss: 0.00004253
Iteration 70/1000 | Loss: 0.00014013
Iteration 71/1000 | Loss: 0.00005224
Iteration 72/1000 | Loss: 0.00182704
Iteration 73/1000 | Loss: 0.00103972
Iteration 74/1000 | Loss: 0.00146113
Iteration 75/1000 | Loss: 0.00069150
Iteration 76/1000 | Loss: 0.00013629
Iteration 77/1000 | Loss: 0.00006069
Iteration 78/1000 | Loss: 0.00016218
Iteration 79/1000 | Loss: 0.00009673
Iteration 80/1000 | Loss: 0.00005923
Iteration 81/1000 | Loss: 0.00004894
Iteration 82/1000 | Loss: 0.00015012
Iteration 83/1000 | Loss: 0.00005045
Iteration 84/1000 | Loss: 0.00005563
Iteration 85/1000 | Loss: 0.00005602
Iteration 86/1000 | Loss: 0.00006006
Iteration 87/1000 | Loss: 0.00007466
Iteration 88/1000 | Loss: 0.00005956
Iteration 89/1000 | Loss: 0.00005818
Iteration 90/1000 | Loss: 0.00005628
Iteration 91/1000 | Loss: 0.00005313
Iteration 92/1000 | Loss: 0.00005588
Iteration 93/1000 | Loss: 0.00024974
Iteration 94/1000 | Loss: 0.00005174
Iteration 95/1000 | Loss: 0.00005099
Iteration 96/1000 | Loss: 0.00004504
Iteration 97/1000 | Loss: 0.00005380
Iteration 98/1000 | Loss: 0.00098871
Iteration 99/1000 | Loss: 0.00018909
Iteration 100/1000 | Loss: 0.00005707
Iteration 101/1000 | Loss: 0.00014655
Iteration 102/1000 | Loss: 0.00014500
Iteration 103/1000 | Loss: 0.00003465
Iteration 104/1000 | Loss: 0.00007441
Iteration 105/1000 | Loss: 0.00002830
Iteration 106/1000 | Loss: 0.00007335
Iteration 107/1000 | Loss: 0.00002689
Iteration 108/1000 | Loss: 0.00002495
Iteration 109/1000 | Loss: 0.00004991
Iteration 110/1000 | Loss: 0.00002376
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002214
Iteration 114/1000 | Loss: 0.00002191
Iteration 115/1000 | Loss: 0.00002187
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002166
Iteration 119/1000 | Loss: 0.00002165
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002158
Iteration 124/1000 | Loss: 0.00002157
Iteration 125/1000 | Loss: 0.00002157
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00028313
Iteration 129/1000 | Loss: 0.00004245
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00006014
Iteration 133/1000 | Loss: 0.00002163
Iteration 134/1000 | Loss: 0.00004646
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002138
Iteration 138/1000 | Loss: 0.00002138
Iteration 139/1000 | Loss: 0.00002138
Iteration 140/1000 | Loss: 0.00002138
Iteration 141/1000 | Loss: 0.00002138
Iteration 142/1000 | Loss: 0.00002137
Iteration 143/1000 | Loss: 0.00002137
Iteration 144/1000 | Loss: 0.00002137
Iteration 145/1000 | Loss: 0.00002137
Iteration 146/1000 | Loss: 0.00002137
Iteration 147/1000 | Loss: 0.00002137
Iteration 148/1000 | Loss: 0.00002137
Iteration 149/1000 | Loss: 0.00002137
Iteration 150/1000 | Loss: 0.00002137
Iteration 151/1000 | Loss: 0.00002137
Iteration 152/1000 | Loss: 0.00002137
Iteration 153/1000 | Loss: 0.00002136
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002135
Iteration 156/1000 | Loss: 0.00002135
Iteration 157/1000 | Loss: 0.00002135
Iteration 158/1000 | Loss: 0.00002135
Iteration 159/1000 | Loss: 0.00002135
Iteration 160/1000 | Loss: 0.00002135
Iteration 161/1000 | Loss: 0.00002135
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002135
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002134
Iteration 169/1000 | Loss: 0.00002134
Iteration 170/1000 | Loss: 0.00002134
Iteration 171/1000 | Loss: 0.00002134
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002134
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002134
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Iteration 178/1000 | Loss: 0.00002134
Iteration 179/1000 | Loss: 0.00002134
Iteration 180/1000 | Loss: 0.00002133
Iteration 181/1000 | Loss: 0.00002133
Iteration 182/1000 | Loss: 0.00002133
Iteration 183/1000 | Loss: 0.00002133
Iteration 184/1000 | Loss: 0.00002133
Iteration 185/1000 | Loss: 0.00002133
Iteration 186/1000 | Loss: 0.00002133
Iteration 187/1000 | Loss: 0.00002133
Iteration 188/1000 | Loss: 0.00002133
Iteration 189/1000 | Loss: 0.00002133
Iteration 190/1000 | Loss: 0.00002133
Iteration 191/1000 | Loss: 0.00002133
Iteration 192/1000 | Loss: 0.00002133
Iteration 193/1000 | Loss: 0.00002133
Iteration 194/1000 | Loss: 0.00002133
Iteration 195/1000 | Loss: 0.00002133
Iteration 196/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.1333904442144558e-05, 2.1333904442144558e-05, 2.1333904442144558e-05, 2.1333904442144558e-05, 2.1333904442144558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1333904442144558e-05

Optimization complete. Final v2v error: 3.869239330291748 mm

Highest mean error: 4.868832111358643 mm for frame 120

Lowest mean error: 3.346832036972046 mm for frame 98

Saving results

Total time: 225.3282709121704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942753
Iteration 2/25 | Loss: 0.00193645
Iteration 3/25 | Loss: 0.00145386
Iteration 4/25 | Loss: 0.00141539
Iteration 5/25 | Loss: 0.00140963
Iteration 6/25 | Loss: 0.00140828
Iteration 7/25 | Loss: 0.00140828
Iteration 8/25 | Loss: 0.00140828
Iteration 9/25 | Loss: 0.00140828
Iteration 10/25 | Loss: 0.00140828
Iteration 11/25 | Loss: 0.00140828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014082766138017178, 0.0014082766138017178, 0.0014082766138017178, 0.0014082766138017178, 0.0014082766138017178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014082766138017178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33878744
Iteration 2/25 | Loss: 0.00142249
Iteration 3/25 | Loss: 0.00142249
Iteration 4/25 | Loss: 0.00142249
Iteration 5/25 | Loss: 0.00142249
Iteration 6/25 | Loss: 0.00142249
Iteration 7/25 | Loss: 0.00142249
Iteration 8/25 | Loss: 0.00142249
Iteration 9/25 | Loss: 0.00142249
Iteration 10/25 | Loss: 0.00142249
Iteration 11/25 | Loss: 0.00142249
Iteration 12/25 | Loss: 0.00142249
Iteration 13/25 | Loss: 0.00142249
Iteration 14/25 | Loss: 0.00142249
Iteration 15/25 | Loss: 0.00142249
Iteration 16/25 | Loss: 0.00142249
Iteration 17/25 | Loss: 0.00142249
Iteration 18/25 | Loss: 0.00142249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014224867336452007, 0.0014224867336452007, 0.0014224867336452007, 0.0014224867336452007, 0.0014224867336452007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014224867336452007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142249
Iteration 2/1000 | Loss: 0.00004173
Iteration 3/1000 | Loss: 0.00003304
Iteration 4/1000 | Loss: 0.00003027
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002760
Iteration 9/1000 | Loss: 0.00002726
Iteration 10/1000 | Loss: 0.00002708
Iteration 11/1000 | Loss: 0.00002695
Iteration 12/1000 | Loss: 0.00002687
Iteration 13/1000 | Loss: 0.00002686
Iteration 14/1000 | Loss: 0.00002684
Iteration 15/1000 | Loss: 0.00002684
Iteration 16/1000 | Loss: 0.00002680
Iteration 17/1000 | Loss: 0.00002680
Iteration 18/1000 | Loss: 0.00002679
Iteration 19/1000 | Loss: 0.00002678
Iteration 20/1000 | Loss: 0.00002677
Iteration 21/1000 | Loss: 0.00002677
Iteration 22/1000 | Loss: 0.00002676
Iteration 23/1000 | Loss: 0.00002676
Iteration 24/1000 | Loss: 0.00002675
Iteration 25/1000 | Loss: 0.00002675
Iteration 26/1000 | Loss: 0.00002674
Iteration 27/1000 | Loss: 0.00002673
Iteration 28/1000 | Loss: 0.00002673
Iteration 29/1000 | Loss: 0.00002672
Iteration 30/1000 | Loss: 0.00002672
Iteration 31/1000 | Loss: 0.00002672
Iteration 32/1000 | Loss: 0.00002672
Iteration 33/1000 | Loss: 0.00002672
Iteration 34/1000 | Loss: 0.00002672
Iteration 35/1000 | Loss: 0.00002672
Iteration 36/1000 | Loss: 0.00002672
Iteration 37/1000 | Loss: 0.00002672
Iteration 38/1000 | Loss: 0.00002671
Iteration 39/1000 | Loss: 0.00002671
Iteration 40/1000 | Loss: 0.00002671
Iteration 41/1000 | Loss: 0.00002671
Iteration 42/1000 | Loss: 0.00002671
Iteration 43/1000 | Loss: 0.00002670
Iteration 44/1000 | Loss: 0.00002670
Iteration 45/1000 | Loss: 0.00002670
Iteration 46/1000 | Loss: 0.00002670
Iteration 47/1000 | Loss: 0.00002669
Iteration 48/1000 | Loss: 0.00002669
Iteration 49/1000 | Loss: 0.00002669
Iteration 50/1000 | Loss: 0.00002669
Iteration 51/1000 | Loss: 0.00002669
Iteration 52/1000 | Loss: 0.00002669
Iteration 53/1000 | Loss: 0.00002669
Iteration 54/1000 | Loss: 0.00002669
Iteration 55/1000 | Loss: 0.00002668
Iteration 56/1000 | Loss: 0.00002668
Iteration 57/1000 | Loss: 0.00002668
Iteration 58/1000 | Loss: 0.00002668
Iteration 59/1000 | Loss: 0.00002668
Iteration 60/1000 | Loss: 0.00002667
Iteration 61/1000 | Loss: 0.00002667
Iteration 62/1000 | Loss: 0.00002667
Iteration 63/1000 | Loss: 0.00002667
Iteration 64/1000 | Loss: 0.00002667
Iteration 65/1000 | Loss: 0.00002667
Iteration 66/1000 | Loss: 0.00002667
Iteration 67/1000 | Loss: 0.00002667
Iteration 68/1000 | Loss: 0.00002667
Iteration 69/1000 | Loss: 0.00002667
Iteration 70/1000 | Loss: 0.00002667
Iteration 71/1000 | Loss: 0.00002667
Iteration 72/1000 | Loss: 0.00002667
Iteration 73/1000 | Loss: 0.00002666
Iteration 74/1000 | Loss: 0.00002666
Iteration 75/1000 | Loss: 0.00002666
Iteration 76/1000 | Loss: 0.00002666
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002666
Iteration 79/1000 | Loss: 0.00002666
Iteration 80/1000 | Loss: 0.00002665
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002665
Iteration 84/1000 | Loss: 0.00002665
Iteration 85/1000 | Loss: 0.00002665
Iteration 86/1000 | Loss: 0.00002665
Iteration 87/1000 | Loss: 0.00002665
Iteration 88/1000 | Loss: 0.00002665
Iteration 89/1000 | Loss: 0.00002665
Iteration 90/1000 | Loss: 0.00002665
Iteration 91/1000 | Loss: 0.00002665
Iteration 92/1000 | Loss: 0.00002665
Iteration 93/1000 | Loss: 0.00002665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.6654195607989095e-05, 2.6654195607989095e-05, 2.6654195607989095e-05, 2.6654195607989095e-05, 2.6654195607989095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6654195607989095e-05

Optimization complete. Final v2v error: 4.542638778686523 mm

Highest mean error: 5.07934045791626 mm for frame 195

Lowest mean error: 4.117191791534424 mm for frame 167

Saving results

Total time: 35.22144341468811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569881
Iteration 2/25 | Loss: 0.00159696
Iteration 3/25 | Loss: 0.00151487
Iteration 4/25 | Loss: 0.00149515
Iteration 5/25 | Loss: 0.00148714
Iteration 6/25 | Loss: 0.00148418
Iteration 7/25 | Loss: 0.00148317
Iteration 8/25 | Loss: 0.00148317
Iteration 9/25 | Loss: 0.00148317
Iteration 10/25 | Loss: 0.00148317
Iteration 11/25 | Loss: 0.00148317
Iteration 12/25 | Loss: 0.00148317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014831662410870194, 0.0014831662410870194, 0.0014831662410870194, 0.0014831662410870194, 0.0014831662410870194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014831662410870194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.16588402
Iteration 2/25 | Loss: 0.00158094
Iteration 3/25 | Loss: 0.00158094
Iteration 4/25 | Loss: 0.00158094
Iteration 5/25 | Loss: 0.00158094
Iteration 6/25 | Loss: 0.00158094
Iteration 7/25 | Loss: 0.00158094
Iteration 8/25 | Loss: 0.00158094
Iteration 9/25 | Loss: 0.00158094
Iteration 10/25 | Loss: 0.00158094
Iteration 11/25 | Loss: 0.00158094
Iteration 12/25 | Loss: 0.00158094
Iteration 13/25 | Loss: 0.00158094
Iteration 14/25 | Loss: 0.00158094
Iteration 15/25 | Loss: 0.00158094
Iteration 16/25 | Loss: 0.00158094
Iteration 17/25 | Loss: 0.00158094
Iteration 18/25 | Loss: 0.00158094
Iteration 19/25 | Loss: 0.00158094
Iteration 20/25 | Loss: 0.00158094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015809376491233706, 0.0015809376491233706, 0.0015809376491233706, 0.0015809376491233706, 0.0015809376491233706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015809376491233706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158094
Iteration 2/1000 | Loss: 0.00005941
Iteration 3/1000 | Loss: 0.00004478
Iteration 4/1000 | Loss: 0.00003945
Iteration 5/1000 | Loss: 0.00003744
Iteration 6/1000 | Loss: 0.00003666
Iteration 7/1000 | Loss: 0.00003609
Iteration 8/1000 | Loss: 0.00003540
Iteration 9/1000 | Loss: 0.00003483
Iteration 10/1000 | Loss: 0.00003442
Iteration 11/1000 | Loss: 0.00003417
Iteration 12/1000 | Loss: 0.00003411
Iteration 13/1000 | Loss: 0.00003400
Iteration 14/1000 | Loss: 0.00003400
Iteration 15/1000 | Loss: 0.00003399
Iteration 16/1000 | Loss: 0.00003396
Iteration 17/1000 | Loss: 0.00003394
Iteration 18/1000 | Loss: 0.00003393
Iteration 19/1000 | Loss: 0.00003391
Iteration 20/1000 | Loss: 0.00003389
Iteration 21/1000 | Loss: 0.00003387
Iteration 22/1000 | Loss: 0.00003386
Iteration 23/1000 | Loss: 0.00003385
Iteration 24/1000 | Loss: 0.00003383
Iteration 25/1000 | Loss: 0.00003378
Iteration 26/1000 | Loss: 0.00003375
Iteration 27/1000 | Loss: 0.00003375
Iteration 28/1000 | Loss: 0.00003375
Iteration 29/1000 | Loss: 0.00003374
Iteration 30/1000 | Loss: 0.00003374
Iteration 31/1000 | Loss: 0.00003373
Iteration 32/1000 | Loss: 0.00003373
Iteration 33/1000 | Loss: 0.00003372
Iteration 34/1000 | Loss: 0.00003372
Iteration 35/1000 | Loss: 0.00003372
Iteration 36/1000 | Loss: 0.00003371
Iteration 37/1000 | Loss: 0.00003371
Iteration 38/1000 | Loss: 0.00003371
Iteration 39/1000 | Loss: 0.00003370
Iteration 40/1000 | Loss: 0.00003370
Iteration 41/1000 | Loss: 0.00003370
Iteration 42/1000 | Loss: 0.00003370
Iteration 43/1000 | Loss: 0.00003370
Iteration 44/1000 | Loss: 0.00003370
Iteration 45/1000 | Loss: 0.00003370
Iteration 46/1000 | Loss: 0.00003370
Iteration 47/1000 | Loss: 0.00003370
Iteration 48/1000 | Loss: 0.00003370
Iteration 49/1000 | Loss: 0.00003370
Iteration 50/1000 | Loss: 0.00003369
Iteration 51/1000 | Loss: 0.00003369
Iteration 52/1000 | Loss: 0.00003369
Iteration 53/1000 | Loss: 0.00003369
Iteration 54/1000 | Loss: 0.00003369
Iteration 55/1000 | Loss: 0.00003369
Iteration 56/1000 | Loss: 0.00003369
Iteration 57/1000 | Loss: 0.00003369
Iteration 58/1000 | Loss: 0.00003369
Iteration 59/1000 | Loss: 0.00003369
Iteration 60/1000 | Loss: 0.00003368
Iteration 61/1000 | Loss: 0.00003368
Iteration 62/1000 | Loss: 0.00003368
Iteration 63/1000 | Loss: 0.00003367
Iteration 64/1000 | Loss: 0.00003367
Iteration 65/1000 | Loss: 0.00003367
Iteration 66/1000 | Loss: 0.00003367
Iteration 67/1000 | Loss: 0.00003366
Iteration 68/1000 | Loss: 0.00003366
Iteration 69/1000 | Loss: 0.00003366
Iteration 70/1000 | Loss: 0.00003366
Iteration 71/1000 | Loss: 0.00003365
Iteration 72/1000 | Loss: 0.00003365
Iteration 73/1000 | Loss: 0.00003365
Iteration 74/1000 | Loss: 0.00003364
Iteration 75/1000 | Loss: 0.00003364
Iteration 76/1000 | Loss: 0.00003364
Iteration 77/1000 | Loss: 0.00003363
Iteration 78/1000 | Loss: 0.00003363
Iteration 79/1000 | Loss: 0.00003363
Iteration 80/1000 | Loss: 0.00003363
Iteration 81/1000 | Loss: 0.00003362
Iteration 82/1000 | Loss: 0.00003362
Iteration 83/1000 | Loss: 0.00003362
Iteration 84/1000 | Loss: 0.00003361
Iteration 85/1000 | Loss: 0.00003361
Iteration 86/1000 | Loss: 0.00003361
Iteration 87/1000 | Loss: 0.00003361
Iteration 88/1000 | Loss: 0.00003361
Iteration 89/1000 | Loss: 0.00003361
Iteration 90/1000 | Loss: 0.00003360
Iteration 91/1000 | Loss: 0.00003360
Iteration 92/1000 | Loss: 0.00003360
Iteration 93/1000 | Loss: 0.00003360
Iteration 94/1000 | Loss: 0.00003360
Iteration 95/1000 | Loss: 0.00003360
Iteration 96/1000 | Loss: 0.00003359
Iteration 97/1000 | Loss: 0.00003359
Iteration 98/1000 | Loss: 0.00003359
Iteration 99/1000 | Loss: 0.00003358
Iteration 100/1000 | Loss: 0.00003358
Iteration 101/1000 | Loss: 0.00003357
Iteration 102/1000 | Loss: 0.00003357
Iteration 103/1000 | Loss: 0.00003357
Iteration 104/1000 | Loss: 0.00003357
Iteration 105/1000 | Loss: 0.00003356
Iteration 106/1000 | Loss: 0.00003356
Iteration 107/1000 | Loss: 0.00003356
Iteration 108/1000 | Loss: 0.00003356
Iteration 109/1000 | Loss: 0.00003356
Iteration 110/1000 | Loss: 0.00003356
Iteration 111/1000 | Loss: 0.00003356
Iteration 112/1000 | Loss: 0.00003356
Iteration 113/1000 | Loss: 0.00003355
Iteration 114/1000 | Loss: 0.00003355
Iteration 115/1000 | Loss: 0.00003355
Iteration 116/1000 | Loss: 0.00003355
Iteration 117/1000 | Loss: 0.00003354
Iteration 118/1000 | Loss: 0.00003354
Iteration 119/1000 | Loss: 0.00003354
Iteration 120/1000 | Loss: 0.00003354
Iteration 121/1000 | Loss: 0.00003354
Iteration 122/1000 | Loss: 0.00003353
Iteration 123/1000 | Loss: 0.00003353
Iteration 124/1000 | Loss: 0.00003353
Iteration 125/1000 | Loss: 0.00003353
Iteration 126/1000 | Loss: 0.00003353
Iteration 127/1000 | Loss: 0.00003353
Iteration 128/1000 | Loss: 0.00003353
Iteration 129/1000 | Loss: 0.00003353
Iteration 130/1000 | Loss: 0.00003353
Iteration 131/1000 | Loss: 0.00003353
Iteration 132/1000 | Loss: 0.00003353
Iteration 133/1000 | Loss: 0.00003353
Iteration 134/1000 | Loss: 0.00003353
Iteration 135/1000 | Loss: 0.00003353
Iteration 136/1000 | Loss: 0.00003353
Iteration 137/1000 | Loss: 0.00003352
Iteration 138/1000 | Loss: 0.00003352
Iteration 139/1000 | Loss: 0.00003352
Iteration 140/1000 | Loss: 0.00003352
Iteration 141/1000 | Loss: 0.00003352
Iteration 142/1000 | Loss: 0.00003352
Iteration 143/1000 | Loss: 0.00003352
Iteration 144/1000 | Loss: 0.00003352
Iteration 145/1000 | Loss: 0.00003352
Iteration 146/1000 | Loss: 0.00003352
Iteration 147/1000 | Loss: 0.00003352
Iteration 148/1000 | Loss: 0.00003351
Iteration 149/1000 | Loss: 0.00003351
Iteration 150/1000 | Loss: 0.00003351
Iteration 151/1000 | Loss: 0.00003351
Iteration 152/1000 | Loss: 0.00003351
Iteration 153/1000 | Loss: 0.00003351
Iteration 154/1000 | Loss: 0.00003351
Iteration 155/1000 | Loss: 0.00003351
Iteration 156/1000 | Loss: 0.00003351
Iteration 157/1000 | Loss: 0.00003351
Iteration 158/1000 | Loss: 0.00003351
Iteration 159/1000 | Loss: 0.00003351
Iteration 160/1000 | Loss: 0.00003351
Iteration 161/1000 | Loss: 0.00003351
Iteration 162/1000 | Loss: 0.00003351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.351293707964942e-05, 3.351293707964942e-05, 3.351293707964942e-05, 3.351293707964942e-05, 3.351293707964942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.351293707964942e-05

Optimization complete. Final v2v error: 5.034554958343506 mm

Highest mean error: 5.800013542175293 mm for frame 68

Lowest mean error: 4.491734027862549 mm for frame 10

Saving results

Total time: 39.000816345214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01189655
Iteration 2/25 | Loss: 0.00437977
Iteration 3/25 | Loss: 0.00194037
Iteration 4/25 | Loss: 0.00172096
Iteration 5/25 | Loss: 0.00161063
Iteration 6/25 | Loss: 0.00161451
Iteration 7/25 | Loss: 0.00152015
Iteration 8/25 | Loss: 0.00147062
Iteration 9/25 | Loss: 0.00144612
Iteration 10/25 | Loss: 0.00145065
Iteration 11/25 | Loss: 0.00141828
Iteration 12/25 | Loss: 0.00140066
Iteration 13/25 | Loss: 0.00140300
Iteration 14/25 | Loss: 0.00137613
Iteration 15/25 | Loss: 0.00137564
Iteration 16/25 | Loss: 0.00136490
Iteration 17/25 | Loss: 0.00135227
Iteration 18/25 | Loss: 0.00134487
Iteration 19/25 | Loss: 0.00134115
Iteration 20/25 | Loss: 0.00134209
Iteration 21/25 | Loss: 0.00132755
Iteration 22/25 | Loss: 0.00136023
Iteration 23/25 | Loss: 0.00132477
Iteration 24/25 | Loss: 0.00131402
Iteration 25/25 | Loss: 0.00138249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46383727
Iteration 2/25 | Loss: 0.00271985
Iteration 3/25 | Loss: 0.00271985
Iteration 4/25 | Loss: 0.00271985
Iteration 5/25 | Loss: 0.00271985
Iteration 6/25 | Loss: 0.00271985
Iteration 7/25 | Loss: 0.00271984
Iteration 8/25 | Loss: 0.00271984
Iteration 9/25 | Loss: 0.00271984
Iteration 10/25 | Loss: 0.00271984
Iteration 11/25 | Loss: 0.00271984
Iteration 12/25 | Loss: 0.00271984
Iteration 13/25 | Loss: 0.00271984
Iteration 14/25 | Loss: 0.00271984
Iteration 15/25 | Loss: 0.00271984
Iteration 16/25 | Loss: 0.00271984
Iteration 17/25 | Loss: 0.00271984
Iteration 18/25 | Loss: 0.00271984
Iteration 19/25 | Loss: 0.00271984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0027198437601327896, 0.0027198437601327896, 0.0027198437601327896, 0.0027198437601327896, 0.0027198437601327896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027198437601327896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271984
Iteration 2/1000 | Loss: 0.00216870
Iteration 3/1000 | Loss: 0.00324594
Iteration 4/1000 | Loss: 0.00329653
Iteration 5/1000 | Loss: 0.00270046
Iteration 6/1000 | Loss: 0.00360127
Iteration 7/1000 | Loss: 0.00215894
Iteration 8/1000 | Loss: 0.00162932
Iteration 9/1000 | Loss: 0.00186016
Iteration 10/1000 | Loss: 0.00171705
Iteration 11/1000 | Loss: 0.00156351
Iteration 12/1000 | Loss: 0.00121154
Iteration 13/1000 | Loss: 0.00157682
Iteration 14/1000 | Loss: 0.00143408
Iteration 15/1000 | Loss: 0.00072984
Iteration 16/1000 | Loss: 0.00157342
Iteration 17/1000 | Loss: 0.00168377
Iteration 18/1000 | Loss: 0.00159074
Iteration 19/1000 | Loss: 0.00121754
Iteration 20/1000 | Loss: 0.00046673
Iteration 21/1000 | Loss: 0.00046153
Iteration 22/1000 | Loss: 0.00268636
Iteration 23/1000 | Loss: 0.00370056
Iteration 24/1000 | Loss: 0.00150771
Iteration 25/1000 | Loss: 0.00173041
Iteration 26/1000 | Loss: 0.00113646
Iteration 27/1000 | Loss: 0.00081480
Iteration 28/1000 | Loss: 0.00219333
Iteration 29/1000 | Loss: 0.00227056
Iteration 30/1000 | Loss: 0.00257640
Iteration 31/1000 | Loss: 0.00152317
Iteration 32/1000 | Loss: 0.00195233
Iteration 33/1000 | Loss: 0.00174606
Iteration 34/1000 | Loss: 0.00158182
Iteration 35/1000 | Loss: 0.00014259
Iteration 36/1000 | Loss: 0.00014349
Iteration 37/1000 | Loss: 0.00219553
Iteration 38/1000 | Loss: 0.00226458
Iteration 39/1000 | Loss: 0.00144376
Iteration 40/1000 | Loss: 0.00195006
Iteration 41/1000 | Loss: 0.00369410
Iteration 42/1000 | Loss: 0.00070785
Iteration 43/1000 | Loss: 0.00051519
Iteration 44/1000 | Loss: 0.00069350
Iteration 45/1000 | Loss: 0.00022813
Iteration 46/1000 | Loss: 0.00029291
Iteration 47/1000 | Loss: 0.00016021
Iteration 48/1000 | Loss: 0.00020981
Iteration 49/1000 | Loss: 0.00028596
Iteration 50/1000 | Loss: 0.00012815
Iteration 51/1000 | Loss: 0.00025338
Iteration 52/1000 | Loss: 0.00065019
Iteration 53/1000 | Loss: 0.00052955
Iteration 54/1000 | Loss: 0.00018694
Iteration 55/1000 | Loss: 0.00029539
Iteration 56/1000 | Loss: 0.00013371
Iteration 57/1000 | Loss: 0.00030582
Iteration 58/1000 | Loss: 0.00022755
Iteration 59/1000 | Loss: 0.00029386
Iteration 60/1000 | Loss: 0.00020027
Iteration 61/1000 | Loss: 0.00019922
Iteration 62/1000 | Loss: 0.00045562
Iteration 63/1000 | Loss: 0.00121385
Iteration 64/1000 | Loss: 0.00109372
Iteration 65/1000 | Loss: 0.00051435
Iteration 66/1000 | Loss: 0.00028765
Iteration 67/1000 | Loss: 0.00027398
Iteration 68/1000 | Loss: 0.00018326
Iteration 69/1000 | Loss: 0.00023529
Iteration 70/1000 | Loss: 0.00087281
Iteration 71/1000 | Loss: 0.00052311
Iteration 72/1000 | Loss: 0.00033552
Iteration 73/1000 | Loss: 0.00040300
Iteration 74/1000 | Loss: 0.00040276
Iteration 75/1000 | Loss: 0.00041445
Iteration 76/1000 | Loss: 0.00032199
Iteration 77/1000 | Loss: 0.00063913
Iteration 78/1000 | Loss: 0.00057605
Iteration 79/1000 | Loss: 0.00064933
Iteration 80/1000 | Loss: 0.00033645
Iteration 81/1000 | Loss: 0.00013756
Iteration 82/1000 | Loss: 0.00022085
Iteration 83/1000 | Loss: 0.00033648
Iteration 84/1000 | Loss: 0.00022250
Iteration 85/1000 | Loss: 0.00032756
Iteration 86/1000 | Loss: 0.00038201
Iteration 87/1000 | Loss: 0.00032087
Iteration 88/1000 | Loss: 0.00033856
Iteration 89/1000 | Loss: 0.00013625
Iteration 90/1000 | Loss: 0.00025708
Iteration 91/1000 | Loss: 0.00034250
Iteration 92/1000 | Loss: 0.00022701
Iteration 93/1000 | Loss: 0.00020227
Iteration 94/1000 | Loss: 0.00033929
Iteration 95/1000 | Loss: 0.00017977
Iteration 96/1000 | Loss: 0.00023989
Iteration 97/1000 | Loss: 0.00020548
Iteration 98/1000 | Loss: 0.00022617
Iteration 99/1000 | Loss: 0.00013955
Iteration 100/1000 | Loss: 0.00013984
Iteration 101/1000 | Loss: 0.00025810
Iteration 102/1000 | Loss: 0.00011143
Iteration 103/1000 | Loss: 0.00010497
Iteration 104/1000 | Loss: 0.00012848
Iteration 105/1000 | Loss: 0.00013829
Iteration 106/1000 | Loss: 0.00014011
Iteration 107/1000 | Loss: 0.00013289
Iteration 108/1000 | Loss: 0.00014485
Iteration 109/1000 | Loss: 0.00014133
Iteration 110/1000 | Loss: 0.00013232
Iteration 111/1000 | Loss: 0.00013906
Iteration 112/1000 | Loss: 0.00034220
Iteration 113/1000 | Loss: 0.00014835
Iteration 114/1000 | Loss: 0.00010304
Iteration 115/1000 | Loss: 0.00023617
Iteration 116/1000 | Loss: 0.00011767
Iteration 117/1000 | Loss: 0.00017589
Iteration 118/1000 | Loss: 0.00022947
Iteration 119/1000 | Loss: 0.00010797
Iteration 120/1000 | Loss: 0.00017650
Iteration 121/1000 | Loss: 0.00027237
Iteration 122/1000 | Loss: 0.00013986
Iteration 123/1000 | Loss: 0.00011936
Iteration 124/1000 | Loss: 0.00019290
Iteration 125/1000 | Loss: 0.00020759
Iteration 126/1000 | Loss: 0.00027074
Iteration 127/1000 | Loss: 0.00033697
Iteration 128/1000 | Loss: 0.00028267
Iteration 129/1000 | Loss: 0.00032719
Iteration 130/1000 | Loss: 0.00030165
Iteration 131/1000 | Loss: 0.00032505
Iteration 132/1000 | Loss: 0.00023843
Iteration 133/1000 | Loss: 0.00006472
Iteration 134/1000 | Loss: 0.00017828
Iteration 135/1000 | Loss: 0.00026065
Iteration 136/1000 | Loss: 0.00031284
Iteration 137/1000 | Loss: 0.00006744
Iteration 138/1000 | Loss: 0.00006008
Iteration 139/1000 | Loss: 0.00006707
Iteration 140/1000 | Loss: 0.00006446
Iteration 141/1000 | Loss: 0.00006145
Iteration 142/1000 | Loss: 0.00006135
Iteration 143/1000 | Loss: 0.00005748
Iteration 144/1000 | Loss: 0.00005648
Iteration 145/1000 | Loss: 0.00006105
Iteration 146/1000 | Loss: 0.00006294
Iteration 147/1000 | Loss: 0.00004722
Iteration 148/1000 | Loss: 0.00004875
Iteration 149/1000 | Loss: 0.00004717
Iteration 150/1000 | Loss: 0.00005187
Iteration 151/1000 | Loss: 0.00005570
Iteration 152/1000 | Loss: 0.00004745
Iteration 153/1000 | Loss: 0.00006399
Iteration 154/1000 | Loss: 0.00005543
Iteration 155/1000 | Loss: 0.00005413
Iteration 156/1000 | Loss: 0.00005884
Iteration 157/1000 | Loss: 0.00005116
Iteration 158/1000 | Loss: 0.00005728
Iteration 159/1000 | Loss: 0.00006111
Iteration 160/1000 | Loss: 0.00005756
Iteration 161/1000 | Loss: 0.00005804
Iteration 162/1000 | Loss: 0.00005731
Iteration 163/1000 | Loss: 0.00005779
Iteration 164/1000 | Loss: 0.00005343
Iteration 165/1000 | Loss: 0.00005733
Iteration 166/1000 | Loss: 0.00005732
Iteration 167/1000 | Loss: 0.00006934
Iteration 168/1000 | Loss: 0.00005676
Iteration 169/1000 | Loss: 0.00004899
Iteration 170/1000 | Loss: 0.00005669
Iteration 171/1000 | Loss: 0.00005737
Iteration 172/1000 | Loss: 0.00005782
Iteration 173/1000 | Loss: 0.00005780
Iteration 174/1000 | Loss: 0.00005696
Iteration 175/1000 | Loss: 0.00005756
Iteration 176/1000 | Loss: 0.00005751
Iteration 177/1000 | Loss: 0.00005328
Iteration 178/1000 | Loss: 0.00005742
Iteration 179/1000 | Loss: 0.00005918
Iteration 180/1000 | Loss: 0.00006017
Iteration 181/1000 | Loss: 0.00005637
Iteration 182/1000 | Loss: 0.00005727
Iteration 183/1000 | Loss: 0.00005763
Iteration 184/1000 | Loss: 0.00005893
Iteration 185/1000 | Loss: 0.00005852
Iteration 186/1000 | Loss: 0.00005887
Iteration 187/1000 | Loss: 0.00005470
Iteration 188/1000 | Loss: 0.00005888
Iteration 189/1000 | Loss: 0.00005758
Iteration 190/1000 | Loss: 0.00005791
Iteration 191/1000 | Loss: 0.00005833
Iteration 192/1000 | Loss: 0.00005688
Iteration 193/1000 | Loss: 0.00005665
Iteration 194/1000 | Loss: 0.00005856
Iteration 195/1000 | Loss: 0.00008381
Iteration 196/1000 | Loss: 0.00006036
Iteration 197/1000 | Loss: 0.00008418
Iteration 198/1000 | Loss: 0.00005912
Iteration 199/1000 | Loss: 0.00008066
Iteration 200/1000 | Loss: 0.00005282
Iteration 201/1000 | Loss: 0.00005762
Iteration 202/1000 | Loss: 0.00005239
Iteration 203/1000 | Loss: 0.00005715
Iteration 204/1000 | Loss: 0.00005243
Iteration 205/1000 | Loss: 0.00004460
Iteration 206/1000 | Loss: 0.00006976
Iteration 207/1000 | Loss: 0.00006148
Iteration 208/1000 | Loss: 0.00006030
Iteration 209/1000 | Loss: 0.00005901
Iteration 210/1000 | Loss: 0.00005862
Iteration 211/1000 | Loss: 0.00006753
Iteration 212/1000 | Loss: 0.00004237
Iteration 213/1000 | Loss: 0.00003897
Iteration 214/1000 | Loss: 0.00003714
Iteration 215/1000 | Loss: 0.00003637
Iteration 216/1000 | Loss: 0.00003615
Iteration 217/1000 | Loss: 0.00003600
Iteration 218/1000 | Loss: 0.00003599
Iteration 219/1000 | Loss: 0.00003599
Iteration 220/1000 | Loss: 0.00003598
Iteration 221/1000 | Loss: 0.00003597
Iteration 222/1000 | Loss: 0.00003597
Iteration 223/1000 | Loss: 0.00003596
Iteration 224/1000 | Loss: 0.00003595
Iteration 225/1000 | Loss: 0.00003595
Iteration 226/1000 | Loss: 0.00003595
Iteration 227/1000 | Loss: 0.00003594
Iteration 228/1000 | Loss: 0.00003594
Iteration 229/1000 | Loss: 0.00003594
Iteration 230/1000 | Loss: 0.00003593
Iteration 231/1000 | Loss: 0.00003593
Iteration 232/1000 | Loss: 0.00003593
Iteration 233/1000 | Loss: 0.00003593
Iteration 234/1000 | Loss: 0.00003593
Iteration 235/1000 | Loss: 0.00003592
Iteration 236/1000 | Loss: 0.00003592
Iteration 237/1000 | Loss: 0.00003592
Iteration 238/1000 | Loss: 0.00003592
Iteration 239/1000 | Loss: 0.00003592
Iteration 240/1000 | Loss: 0.00003592
Iteration 241/1000 | Loss: 0.00003591
Iteration 242/1000 | Loss: 0.00003591
Iteration 243/1000 | Loss: 0.00003591
Iteration 244/1000 | Loss: 0.00003591
Iteration 245/1000 | Loss: 0.00003591
Iteration 246/1000 | Loss: 0.00003591
Iteration 247/1000 | Loss: 0.00003591
Iteration 248/1000 | Loss: 0.00003590
Iteration 249/1000 | Loss: 0.00003590
Iteration 250/1000 | Loss: 0.00003590
Iteration 251/1000 | Loss: 0.00003590
Iteration 252/1000 | Loss: 0.00003590
Iteration 253/1000 | Loss: 0.00003590
Iteration 254/1000 | Loss: 0.00003590
Iteration 255/1000 | Loss: 0.00003589
Iteration 256/1000 | Loss: 0.00003589
Iteration 257/1000 | Loss: 0.00003589
Iteration 258/1000 | Loss: 0.00003589
Iteration 259/1000 | Loss: 0.00003589
Iteration 260/1000 | Loss: 0.00003589
Iteration 261/1000 | Loss: 0.00003589
Iteration 262/1000 | Loss: 0.00003588
Iteration 263/1000 | Loss: 0.00003588
Iteration 264/1000 | Loss: 0.00003588
Iteration 265/1000 | Loss: 0.00003588
Iteration 266/1000 | Loss: 0.00003588
Iteration 267/1000 | Loss: 0.00003588
Iteration 268/1000 | Loss: 0.00003587
Iteration 269/1000 | Loss: 0.00003587
Iteration 270/1000 | Loss: 0.00003587
Iteration 271/1000 | Loss: 0.00003587
Iteration 272/1000 | Loss: 0.00003587
Iteration 273/1000 | Loss: 0.00003587
Iteration 274/1000 | Loss: 0.00003587
Iteration 275/1000 | Loss: 0.00003586
Iteration 276/1000 | Loss: 0.00003586
Iteration 277/1000 | Loss: 0.00003586
Iteration 278/1000 | Loss: 0.00003586
Iteration 279/1000 | Loss: 0.00003586
Iteration 280/1000 | Loss: 0.00003586
Iteration 281/1000 | Loss: 0.00003586
Iteration 282/1000 | Loss: 0.00003586
Iteration 283/1000 | Loss: 0.00003586
Iteration 284/1000 | Loss: 0.00003586
Iteration 285/1000 | Loss: 0.00003586
Iteration 286/1000 | Loss: 0.00003586
Iteration 287/1000 | Loss: 0.00003586
Iteration 288/1000 | Loss: 0.00003586
Iteration 289/1000 | Loss: 0.00003586
Iteration 290/1000 | Loss: 0.00003586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [3.5855930036632344e-05, 3.5855930036632344e-05, 3.5855930036632344e-05, 3.5855930036632344e-05, 3.5855930036632344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5855930036632344e-05

Optimization complete. Final v2v error: 4.6556830406188965 mm

Highest mean error: 22.708454132080078 mm for frame 131

Lowest mean error: 4.106242656707764 mm for frame 58

Saving results

Total time: 363.881698846817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515919
Iteration 2/25 | Loss: 0.00147423
Iteration 3/25 | Loss: 0.00139056
Iteration 4/25 | Loss: 0.00137886
Iteration 5/25 | Loss: 0.00137607
Iteration 6/25 | Loss: 0.00137515
Iteration 7/25 | Loss: 0.00137506
Iteration 8/25 | Loss: 0.00137506
Iteration 9/25 | Loss: 0.00137506
Iteration 10/25 | Loss: 0.00137506
Iteration 11/25 | Loss: 0.00137506
Iteration 12/25 | Loss: 0.00137506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013750585494562984, 0.0013750585494562984, 0.0013750585494562984, 0.0013750585494562984, 0.0013750585494562984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013750585494562984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48808706
Iteration 2/25 | Loss: 0.00112486
Iteration 3/25 | Loss: 0.00112485
Iteration 4/25 | Loss: 0.00112484
Iteration 5/25 | Loss: 0.00112484
Iteration 6/25 | Loss: 0.00112484
Iteration 7/25 | Loss: 0.00112484
Iteration 8/25 | Loss: 0.00112484
Iteration 9/25 | Loss: 0.00112484
Iteration 10/25 | Loss: 0.00112484
Iteration 11/25 | Loss: 0.00112484
Iteration 12/25 | Loss: 0.00112484
Iteration 13/25 | Loss: 0.00112484
Iteration 14/25 | Loss: 0.00112484
Iteration 15/25 | Loss: 0.00112484
Iteration 16/25 | Loss: 0.00112484
Iteration 17/25 | Loss: 0.00112484
Iteration 18/25 | Loss: 0.00112484
Iteration 19/25 | Loss: 0.00112484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011248431401327252, 0.0011248431401327252, 0.0011248431401327252, 0.0011248431401327252, 0.0011248431401327252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011248431401327252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112484
Iteration 2/1000 | Loss: 0.00004031
Iteration 3/1000 | Loss: 0.00003079
Iteration 4/1000 | Loss: 0.00002849
Iteration 5/1000 | Loss: 0.00002741
Iteration 6/1000 | Loss: 0.00002675
Iteration 7/1000 | Loss: 0.00002624
Iteration 8/1000 | Loss: 0.00002602
Iteration 9/1000 | Loss: 0.00002601
Iteration 10/1000 | Loss: 0.00002585
Iteration 11/1000 | Loss: 0.00002584
Iteration 12/1000 | Loss: 0.00002583
Iteration 13/1000 | Loss: 0.00002579
Iteration 14/1000 | Loss: 0.00002578
Iteration 15/1000 | Loss: 0.00002577
Iteration 16/1000 | Loss: 0.00002577
Iteration 17/1000 | Loss: 0.00002577
Iteration 18/1000 | Loss: 0.00002576
Iteration 19/1000 | Loss: 0.00002571
Iteration 20/1000 | Loss: 0.00002569
Iteration 21/1000 | Loss: 0.00002567
Iteration 22/1000 | Loss: 0.00002566
Iteration 23/1000 | Loss: 0.00002566
Iteration 24/1000 | Loss: 0.00002560
Iteration 25/1000 | Loss: 0.00002549
Iteration 26/1000 | Loss: 0.00002545
Iteration 27/1000 | Loss: 0.00002542
Iteration 28/1000 | Loss: 0.00002542
Iteration 29/1000 | Loss: 0.00002541
Iteration 30/1000 | Loss: 0.00002539
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002538
Iteration 35/1000 | Loss: 0.00002538
Iteration 36/1000 | Loss: 0.00002538
Iteration 37/1000 | Loss: 0.00002538
Iteration 38/1000 | Loss: 0.00002537
Iteration 39/1000 | Loss: 0.00002536
Iteration 40/1000 | Loss: 0.00002535
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002534
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002533
Iteration 46/1000 | Loss: 0.00002532
Iteration 47/1000 | Loss: 0.00002532
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002532
Iteration 50/1000 | Loss: 0.00002532
Iteration 51/1000 | Loss: 0.00002531
Iteration 52/1000 | Loss: 0.00002531
Iteration 53/1000 | Loss: 0.00002531
Iteration 54/1000 | Loss: 0.00002531
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002530
Iteration 57/1000 | Loss: 0.00002530
Iteration 58/1000 | Loss: 0.00002529
Iteration 59/1000 | Loss: 0.00002529
Iteration 60/1000 | Loss: 0.00002529
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002528
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002528
Iteration 65/1000 | Loss: 0.00002528
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002528
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002527
Iteration 79/1000 | Loss: 0.00002527
Iteration 80/1000 | Loss: 0.00002527
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002527
Iteration 91/1000 | Loss: 0.00002527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.527219294279348e-05, 2.527219294279348e-05, 2.527219294279348e-05, 2.527219294279348e-05, 2.527219294279348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.527219294279348e-05

Optimization complete. Final v2v error: 4.2510786056518555 mm

Highest mean error: 5.151543617248535 mm for frame 71

Lowest mean error: 3.751776933670044 mm for frame 36

Saving results

Total time: 30.796711444854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603496
Iteration 2/25 | Loss: 0.00173964
Iteration 3/25 | Loss: 0.00152393
Iteration 4/25 | Loss: 0.00149166
Iteration 5/25 | Loss: 0.00147878
Iteration 6/25 | Loss: 0.00147580
Iteration 7/25 | Loss: 0.00147489
Iteration 8/25 | Loss: 0.00147489
Iteration 9/25 | Loss: 0.00147489
Iteration 10/25 | Loss: 0.00147489
Iteration 11/25 | Loss: 0.00147489
Iteration 12/25 | Loss: 0.00147489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014748917892575264, 0.0014748917892575264, 0.0014748917892575264, 0.0014748917892575264, 0.0014748917892575264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014748917892575264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77561539
Iteration 2/25 | Loss: 0.00137716
Iteration 3/25 | Loss: 0.00137715
Iteration 4/25 | Loss: 0.00137715
Iteration 5/25 | Loss: 0.00137715
Iteration 6/25 | Loss: 0.00137715
Iteration 7/25 | Loss: 0.00137715
Iteration 8/25 | Loss: 0.00137715
Iteration 9/25 | Loss: 0.00137715
Iteration 10/25 | Loss: 0.00137715
Iteration 11/25 | Loss: 0.00137715
Iteration 12/25 | Loss: 0.00137715
Iteration 13/25 | Loss: 0.00137715
Iteration 14/25 | Loss: 0.00137715
Iteration 15/25 | Loss: 0.00137715
Iteration 16/25 | Loss: 0.00137715
Iteration 17/25 | Loss: 0.00137715
Iteration 18/25 | Loss: 0.00137715
Iteration 19/25 | Loss: 0.00137715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013771492522209883, 0.0013771492522209883, 0.0013771492522209883, 0.0013771492522209883, 0.0013771492522209883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013771492522209883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137715
Iteration 2/1000 | Loss: 0.00005466
Iteration 3/1000 | Loss: 0.00004208
Iteration 4/1000 | Loss: 0.00003905
Iteration 5/1000 | Loss: 0.00003797
Iteration 6/1000 | Loss: 0.00003703
Iteration 7/1000 | Loss: 0.00003600
Iteration 8/1000 | Loss: 0.00003551
Iteration 9/1000 | Loss: 0.00003530
Iteration 10/1000 | Loss: 0.00003529
Iteration 11/1000 | Loss: 0.00003512
Iteration 12/1000 | Loss: 0.00003509
Iteration 13/1000 | Loss: 0.00003489
Iteration 14/1000 | Loss: 0.00003482
Iteration 15/1000 | Loss: 0.00003480
Iteration 16/1000 | Loss: 0.00003480
Iteration 17/1000 | Loss: 0.00003480
Iteration 18/1000 | Loss: 0.00003479
Iteration 19/1000 | Loss: 0.00003479
Iteration 20/1000 | Loss: 0.00003478
Iteration 21/1000 | Loss: 0.00003473
Iteration 22/1000 | Loss: 0.00003473
Iteration 23/1000 | Loss: 0.00003473
Iteration 24/1000 | Loss: 0.00003472
Iteration 25/1000 | Loss: 0.00003472
Iteration 26/1000 | Loss: 0.00003472
Iteration 27/1000 | Loss: 0.00003456
Iteration 28/1000 | Loss: 0.00003446
Iteration 29/1000 | Loss: 0.00003445
Iteration 30/1000 | Loss: 0.00003445
Iteration 31/1000 | Loss: 0.00003445
Iteration 32/1000 | Loss: 0.00003445
Iteration 33/1000 | Loss: 0.00003445
Iteration 34/1000 | Loss: 0.00003445
Iteration 35/1000 | Loss: 0.00003445
Iteration 36/1000 | Loss: 0.00003445
Iteration 37/1000 | Loss: 0.00003444
Iteration 38/1000 | Loss: 0.00003443
Iteration 39/1000 | Loss: 0.00003443
Iteration 40/1000 | Loss: 0.00003442
Iteration 41/1000 | Loss: 0.00003441
Iteration 42/1000 | Loss: 0.00003440
Iteration 43/1000 | Loss: 0.00003440
Iteration 44/1000 | Loss: 0.00003439
Iteration 45/1000 | Loss: 0.00003438
Iteration 46/1000 | Loss: 0.00003437
Iteration 47/1000 | Loss: 0.00003437
Iteration 48/1000 | Loss: 0.00003437
Iteration 49/1000 | Loss: 0.00003437
Iteration 50/1000 | Loss: 0.00003436
Iteration 51/1000 | Loss: 0.00003435
Iteration 52/1000 | Loss: 0.00003432
Iteration 53/1000 | Loss: 0.00003432
Iteration 54/1000 | Loss: 0.00003432
Iteration 55/1000 | Loss: 0.00003431
Iteration 56/1000 | Loss: 0.00003431
Iteration 57/1000 | Loss: 0.00003431
Iteration 58/1000 | Loss: 0.00003431
Iteration 59/1000 | Loss: 0.00003431
Iteration 60/1000 | Loss: 0.00003431
Iteration 61/1000 | Loss: 0.00003431
Iteration 62/1000 | Loss: 0.00003431
Iteration 63/1000 | Loss: 0.00003431
Iteration 64/1000 | Loss: 0.00003431
Iteration 65/1000 | Loss: 0.00003430
Iteration 66/1000 | Loss: 0.00003430
Iteration 67/1000 | Loss: 0.00003430
Iteration 68/1000 | Loss: 0.00003430
Iteration 69/1000 | Loss: 0.00003429
Iteration 70/1000 | Loss: 0.00003429
Iteration 71/1000 | Loss: 0.00003429
Iteration 72/1000 | Loss: 0.00003428
Iteration 73/1000 | Loss: 0.00003428
Iteration 74/1000 | Loss: 0.00003428
Iteration 75/1000 | Loss: 0.00003428
Iteration 76/1000 | Loss: 0.00003428
Iteration 77/1000 | Loss: 0.00003428
Iteration 78/1000 | Loss: 0.00003427
Iteration 79/1000 | Loss: 0.00003427
Iteration 80/1000 | Loss: 0.00003427
Iteration 81/1000 | Loss: 0.00003427
Iteration 82/1000 | Loss: 0.00003427
Iteration 83/1000 | Loss: 0.00003427
Iteration 84/1000 | Loss: 0.00003427
Iteration 85/1000 | Loss: 0.00003427
Iteration 86/1000 | Loss: 0.00003427
Iteration 87/1000 | Loss: 0.00003427
Iteration 88/1000 | Loss: 0.00003426
Iteration 89/1000 | Loss: 0.00003426
Iteration 90/1000 | Loss: 0.00003426
Iteration 91/1000 | Loss: 0.00003426
Iteration 92/1000 | Loss: 0.00003426
Iteration 93/1000 | Loss: 0.00003426
Iteration 94/1000 | Loss: 0.00003426
Iteration 95/1000 | Loss: 0.00003426
Iteration 96/1000 | Loss: 0.00003426
Iteration 97/1000 | Loss: 0.00003426
Iteration 98/1000 | Loss: 0.00003426
Iteration 99/1000 | Loss: 0.00003426
Iteration 100/1000 | Loss: 0.00003426
Iteration 101/1000 | Loss: 0.00003425
Iteration 102/1000 | Loss: 0.00003425
Iteration 103/1000 | Loss: 0.00003425
Iteration 104/1000 | Loss: 0.00003425
Iteration 105/1000 | Loss: 0.00003425
Iteration 106/1000 | Loss: 0.00003425
Iteration 107/1000 | Loss: 0.00003425
Iteration 108/1000 | Loss: 0.00003425
Iteration 109/1000 | Loss: 0.00003425
Iteration 110/1000 | Loss: 0.00003424
Iteration 111/1000 | Loss: 0.00003424
Iteration 112/1000 | Loss: 0.00003424
Iteration 113/1000 | Loss: 0.00003424
Iteration 114/1000 | Loss: 0.00003424
Iteration 115/1000 | Loss: 0.00003424
Iteration 116/1000 | Loss: 0.00003424
Iteration 117/1000 | Loss: 0.00003424
Iteration 118/1000 | Loss: 0.00003424
Iteration 119/1000 | Loss: 0.00003424
Iteration 120/1000 | Loss: 0.00003424
Iteration 121/1000 | Loss: 0.00003424
Iteration 122/1000 | Loss: 0.00003424
Iteration 123/1000 | Loss: 0.00003424
Iteration 124/1000 | Loss: 0.00003424
Iteration 125/1000 | Loss: 0.00003424
Iteration 126/1000 | Loss: 0.00003424
Iteration 127/1000 | Loss: 0.00003424
Iteration 128/1000 | Loss: 0.00003424
Iteration 129/1000 | Loss: 0.00003424
Iteration 130/1000 | Loss: 0.00003424
Iteration 131/1000 | Loss: 0.00003424
Iteration 132/1000 | Loss: 0.00003424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.424253736739047e-05, 3.424253736739047e-05, 3.424253736739047e-05, 3.424253736739047e-05, 3.424253736739047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.424253736739047e-05

Optimization complete. Final v2v error: 5.008545875549316 mm

Highest mean error: 5.35013484954834 mm for frame 155

Lowest mean error: 4.730628967285156 mm for frame 77

Saving results

Total time: 39.86369252204895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881296
Iteration 2/25 | Loss: 0.00147884
Iteration 3/25 | Loss: 0.00139550
Iteration 4/25 | Loss: 0.00138758
Iteration 5/25 | Loss: 0.00138486
Iteration 6/25 | Loss: 0.00138477
Iteration 7/25 | Loss: 0.00138477
Iteration 8/25 | Loss: 0.00138477
Iteration 9/25 | Loss: 0.00138477
Iteration 10/25 | Loss: 0.00138477
Iteration 11/25 | Loss: 0.00138477
Iteration 12/25 | Loss: 0.00138477
Iteration 13/25 | Loss: 0.00138477
Iteration 14/25 | Loss: 0.00138477
Iteration 15/25 | Loss: 0.00138477
Iteration 16/25 | Loss: 0.00138477
Iteration 17/25 | Loss: 0.00138477
Iteration 18/25 | Loss: 0.00138477
Iteration 19/25 | Loss: 0.00138477
Iteration 20/25 | Loss: 0.00138477
Iteration 21/25 | Loss: 0.00138477
Iteration 22/25 | Loss: 0.00138477
Iteration 23/25 | Loss: 0.00138477
Iteration 24/25 | Loss: 0.00138477
Iteration 25/25 | Loss: 0.00138477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48760521
Iteration 2/25 | Loss: 0.00123103
Iteration 3/25 | Loss: 0.00123103
Iteration 4/25 | Loss: 0.00123103
Iteration 5/25 | Loss: 0.00123103
Iteration 6/25 | Loss: 0.00123103
Iteration 7/25 | Loss: 0.00123103
Iteration 8/25 | Loss: 0.00123103
Iteration 9/25 | Loss: 0.00123103
Iteration 10/25 | Loss: 0.00123103
Iteration 11/25 | Loss: 0.00123103
Iteration 12/25 | Loss: 0.00123103
Iteration 13/25 | Loss: 0.00123103
Iteration 14/25 | Loss: 0.00123103
Iteration 15/25 | Loss: 0.00123103
Iteration 16/25 | Loss: 0.00123103
Iteration 17/25 | Loss: 0.00123103
Iteration 18/25 | Loss: 0.00123103
Iteration 19/25 | Loss: 0.00123103
Iteration 20/25 | Loss: 0.00123103
Iteration 21/25 | Loss: 0.00123103
Iteration 22/25 | Loss: 0.00123103
Iteration 23/25 | Loss: 0.00123103
Iteration 24/25 | Loss: 0.00123103
Iteration 25/25 | Loss: 0.00123103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123103
Iteration 2/1000 | Loss: 0.00003320
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002673
Iteration 5/1000 | Loss: 0.00002602
Iteration 6/1000 | Loss: 0.00002545
Iteration 7/1000 | Loss: 0.00002505
Iteration 8/1000 | Loss: 0.00002486
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00002461
Iteration 11/1000 | Loss: 0.00002455
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002438
Iteration 14/1000 | Loss: 0.00002426
Iteration 15/1000 | Loss: 0.00002420
Iteration 16/1000 | Loss: 0.00002420
Iteration 17/1000 | Loss: 0.00002419
Iteration 18/1000 | Loss: 0.00002417
Iteration 19/1000 | Loss: 0.00002413
Iteration 20/1000 | Loss: 0.00002410
Iteration 21/1000 | Loss: 0.00002404
Iteration 22/1000 | Loss: 0.00002401
Iteration 23/1000 | Loss: 0.00002400
Iteration 24/1000 | Loss: 0.00002400
Iteration 25/1000 | Loss: 0.00002400
Iteration 26/1000 | Loss: 0.00002399
Iteration 27/1000 | Loss: 0.00002397
Iteration 28/1000 | Loss: 0.00002397
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002394
Iteration 31/1000 | Loss: 0.00002394
Iteration 32/1000 | Loss: 0.00002393
Iteration 33/1000 | Loss: 0.00002393
Iteration 34/1000 | Loss: 0.00002393
Iteration 35/1000 | Loss: 0.00002393
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002392
Iteration 41/1000 | Loss: 0.00002392
Iteration 42/1000 | Loss: 0.00002391
Iteration 43/1000 | Loss: 0.00002391
Iteration 44/1000 | Loss: 0.00002390
Iteration 45/1000 | Loss: 0.00002390
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00002390
Iteration 48/1000 | Loss: 0.00002389
Iteration 49/1000 | Loss: 0.00002389
Iteration 50/1000 | Loss: 0.00002389
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002387
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002386
Iteration 60/1000 | Loss: 0.00002386
Iteration 61/1000 | Loss: 0.00002385
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00002384
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002384
Iteration 70/1000 | Loss: 0.00002384
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002383
Iteration 73/1000 | Loss: 0.00002383
Iteration 74/1000 | Loss: 0.00002383
Iteration 75/1000 | Loss: 0.00002383
Iteration 76/1000 | Loss: 0.00002383
Iteration 77/1000 | Loss: 0.00002383
Iteration 78/1000 | Loss: 0.00002383
Iteration 79/1000 | Loss: 0.00002383
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00002382
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002382
Iteration 86/1000 | Loss: 0.00002382
Iteration 87/1000 | Loss: 0.00002382
Iteration 88/1000 | Loss: 0.00002382
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002381
Iteration 91/1000 | Loss: 0.00002381
Iteration 92/1000 | Loss: 0.00002381
Iteration 93/1000 | Loss: 0.00002381
Iteration 94/1000 | Loss: 0.00002381
Iteration 95/1000 | Loss: 0.00002381
Iteration 96/1000 | Loss: 0.00002381
Iteration 97/1000 | Loss: 0.00002381
Iteration 98/1000 | Loss: 0.00002381
Iteration 99/1000 | Loss: 0.00002381
Iteration 100/1000 | Loss: 0.00002381
Iteration 101/1000 | Loss: 0.00002381
Iteration 102/1000 | Loss: 0.00002381
Iteration 103/1000 | Loss: 0.00002380
Iteration 104/1000 | Loss: 0.00002380
Iteration 105/1000 | Loss: 0.00002380
Iteration 106/1000 | Loss: 0.00002380
Iteration 107/1000 | Loss: 0.00002380
Iteration 108/1000 | Loss: 0.00002380
Iteration 109/1000 | Loss: 0.00002380
Iteration 110/1000 | Loss: 0.00002380
Iteration 111/1000 | Loss: 0.00002380
Iteration 112/1000 | Loss: 0.00002380
Iteration 113/1000 | Loss: 0.00002380
Iteration 114/1000 | Loss: 0.00002380
Iteration 115/1000 | Loss: 0.00002380
Iteration 116/1000 | Loss: 0.00002380
Iteration 117/1000 | Loss: 0.00002380
Iteration 118/1000 | Loss: 0.00002380
Iteration 119/1000 | Loss: 0.00002380
Iteration 120/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.3796388632035814e-05, 2.3796388632035814e-05, 2.3796388632035814e-05, 2.3796388632035814e-05, 2.3796388632035814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3796388632035814e-05

Optimization complete. Final v2v error: 4.240969181060791 mm

Highest mean error: 4.511252403259277 mm for frame 208

Lowest mean error: 3.9308228492736816 mm for frame 264

Saving results

Total time: 38.57993674278259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990527
Iteration 2/25 | Loss: 0.00159996
Iteration 3/25 | Loss: 0.00144489
Iteration 4/25 | Loss: 0.00143116
Iteration 5/25 | Loss: 0.00142749
Iteration 6/25 | Loss: 0.00142675
Iteration 7/25 | Loss: 0.00142675
Iteration 8/25 | Loss: 0.00142675
Iteration 9/25 | Loss: 0.00142675
Iteration 10/25 | Loss: 0.00142675
Iteration 11/25 | Loss: 0.00142675
Iteration 12/25 | Loss: 0.00142675
Iteration 13/25 | Loss: 0.00142675
Iteration 14/25 | Loss: 0.00142675
Iteration 15/25 | Loss: 0.00142675
Iteration 16/25 | Loss: 0.00142675
Iteration 17/25 | Loss: 0.00142675
Iteration 18/25 | Loss: 0.00142675
Iteration 19/25 | Loss: 0.00142675
Iteration 20/25 | Loss: 0.00142675
Iteration 21/25 | Loss: 0.00142675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014267537044361234, 0.0014267537044361234, 0.0014267537044361234, 0.0014267537044361234, 0.0014267537044361234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014267537044361234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52139282
Iteration 2/25 | Loss: 0.00132147
Iteration 3/25 | Loss: 0.00132147
Iteration 4/25 | Loss: 0.00132147
Iteration 5/25 | Loss: 0.00132147
Iteration 6/25 | Loss: 0.00132147
Iteration 7/25 | Loss: 0.00132147
Iteration 8/25 | Loss: 0.00132147
Iteration 9/25 | Loss: 0.00132147
Iteration 10/25 | Loss: 0.00132147
Iteration 11/25 | Loss: 0.00132147
Iteration 12/25 | Loss: 0.00132147
Iteration 13/25 | Loss: 0.00132147
Iteration 14/25 | Loss: 0.00132147
Iteration 15/25 | Loss: 0.00132146
Iteration 16/25 | Loss: 0.00132147
Iteration 17/25 | Loss: 0.00132147
Iteration 18/25 | Loss: 0.00132147
Iteration 19/25 | Loss: 0.00132147
Iteration 20/25 | Loss: 0.00132147
Iteration 21/25 | Loss: 0.00132147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013214650098234415, 0.0013214650098234415, 0.0013214650098234415, 0.0013214650098234415, 0.0013214650098234415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013214650098234415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132147
Iteration 2/1000 | Loss: 0.00005700
Iteration 3/1000 | Loss: 0.00004086
Iteration 4/1000 | Loss: 0.00003818
Iteration 5/1000 | Loss: 0.00003645
Iteration 6/1000 | Loss: 0.00003542
Iteration 7/1000 | Loss: 0.00003453
Iteration 8/1000 | Loss: 0.00003398
Iteration 9/1000 | Loss: 0.00003367
Iteration 10/1000 | Loss: 0.00003342
Iteration 11/1000 | Loss: 0.00003338
Iteration 12/1000 | Loss: 0.00003327
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003321
Iteration 15/1000 | Loss: 0.00003320
Iteration 16/1000 | Loss: 0.00003314
Iteration 17/1000 | Loss: 0.00003311
Iteration 18/1000 | Loss: 0.00003311
Iteration 19/1000 | Loss: 0.00003310
Iteration 20/1000 | Loss: 0.00003309
Iteration 21/1000 | Loss: 0.00003307
Iteration 22/1000 | Loss: 0.00003307
Iteration 23/1000 | Loss: 0.00003306
Iteration 24/1000 | Loss: 0.00003305
Iteration 25/1000 | Loss: 0.00003305
Iteration 26/1000 | Loss: 0.00003304
Iteration 27/1000 | Loss: 0.00003303
Iteration 28/1000 | Loss: 0.00003303
Iteration 29/1000 | Loss: 0.00003303
Iteration 30/1000 | Loss: 0.00003302
Iteration 31/1000 | Loss: 0.00003302
Iteration 32/1000 | Loss: 0.00003300
Iteration 33/1000 | Loss: 0.00003299
Iteration 34/1000 | Loss: 0.00003299
Iteration 35/1000 | Loss: 0.00003299
Iteration 36/1000 | Loss: 0.00003299
Iteration 37/1000 | Loss: 0.00003299
Iteration 38/1000 | Loss: 0.00003299
Iteration 39/1000 | Loss: 0.00003298
Iteration 40/1000 | Loss: 0.00003297
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003296
Iteration 44/1000 | Loss: 0.00003296
Iteration 45/1000 | Loss: 0.00003295
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003292
Iteration 48/1000 | Loss: 0.00003291
Iteration 49/1000 | Loss: 0.00003289
Iteration 50/1000 | Loss: 0.00003289
Iteration 51/1000 | Loss: 0.00003288
Iteration 52/1000 | Loss: 0.00003288
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00003288
Iteration 55/1000 | Loss: 0.00003287
Iteration 56/1000 | Loss: 0.00003287
Iteration 57/1000 | Loss: 0.00003287
Iteration 58/1000 | Loss: 0.00003286
Iteration 59/1000 | Loss: 0.00003286
Iteration 60/1000 | Loss: 0.00003286
Iteration 61/1000 | Loss: 0.00003286
Iteration 62/1000 | Loss: 0.00003286
Iteration 63/1000 | Loss: 0.00003286
Iteration 64/1000 | Loss: 0.00003286
Iteration 65/1000 | Loss: 0.00003286
Iteration 66/1000 | Loss: 0.00003286
Iteration 67/1000 | Loss: 0.00003286
Iteration 68/1000 | Loss: 0.00003286
Iteration 69/1000 | Loss: 0.00003285
Iteration 70/1000 | Loss: 0.00003285
Iteration 71/1000 | Loss: 0.00003285
Iteration 72/1000 | Loss: 0.00003285
Iteration 73/1000 | Loss: 0.00003285
Iteration 74/1000 | Loss: 0.00003285
Iteration 75/1000 | Loss: 0.00003285
Iteration 76/1000 | Loss: 0.00003285
Iteration 77/1000 | Loss: 0.00003285
Iteration 78/1000 | Loss: 0.00003285
Iteration 79/1000 | Loss: 0.00003285
Iteration 80/1000 | Loss: 0.00003285
Iteration 81/1000 | Loss: 0.00003284
Iteration 82/1000 | Loss: 0.00003284
Iteration 83/1000 | Loss: 0.00003284
Iteration 84/1000 | Loss: 0.00003284
Iteration 85/1000 | Loss: 0.00003284
Iteration 86/1000 | Loss: 0.00003284
Iteration 87/1000 | Loss: 0.00003284
Iteration 88/1000 | Loss: 0.00003284
Iteration 89/1000 | Loss: 0.00003283
Iteration 90/1000 | Loss: 0.00003283
Iteration 91/1000 | Loss: 0.00003283
Iteration 92/1000 | Loss: 0.00003283
Iteration 93/1000 | Loss: 0.00003283
Iteration 94/1000 | Loss: 0.00003283
Iteration 95/1000 | Loss: 0.00003283
Iteration 96/1000 | Loss: 0.00003283
Iteration 97/1000 | Loss: 0.00003283
Iteration 98/1000 | Loss: 0.00003283
Iteration 99/1000 | Loss: 0.00003283
Iteration 100/1000 | Loss: 0.00003283
Iteration 101/1000 | Loss: 0.00003283
Iteration 102/1000 | Loss: 0.00003283
Iteration 103/1000 | Loss: 0.00003282
Iteration 104/1000 | Loss: 0.00003282
Iteration 105/1000 | Loss: 0.00003282
Iteration 106/1000 | Loss: 0.00003282
Iteration 107/1000 | Loss: 0.00003282
Iteration 108/1000 | Loss: 0.00003282
Iteration 109/1000 | Loss: 0.00003282
Iteration 110/1000 | Loss: 0.00003282
Iteration 111/1000 | Loss: 0.00003282
Iteration 112/1000 | Loss: 0.00003282
Iteration 113/1000 | Loss: 0.00003282
Iteration 114/1000 | Loss: 0.00003282
Iteration 115/1000 | Loss: 0.00003282
Iteration 116/1000 | Loss: 0.00003282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [3.2819316402310506e-05, 3.2819316402310506e-05, 3.2819316402310506e-05, 3.2819316402310506e-05, 3.2819316402310506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2819316402310506e-05

Optimization complete. Final v2v error: 4.877561092376709 mm

Highest mean error: 6.444433212280273 mm for frame 59

Lowest mean error: 4.372007369995117 mm for frame 32

Saving results

Total time: 35.00441884994507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101887
Iteration 2/25 | Loss: 0.00266871
Iteration 3/25 | Loss: 0.00205997
Iteration 4/25 | Loss: 0.00207102
Iteration 5/25 | Loss: 0.00199690
Iteration 6/25 | Loss: 0.00186861
Iteration 7/25 | Loss: 0.00173573
Iteration 8/25 | Loss: 0.00163238
Iteration 9/25 | Loss: 0.00156080
Iteration 10/25 | Loss: 0.00151020
Iteration 11/25 | Loss: 0.00148545
Iteration 12/25 | Loss: 0.00148242
Iteration 13/25 | Loss: 0.00149221
Iteration 14/25 | Loss: 0.00147274
Iteration 15/25 | Loss: 0.00147103
Iteration 16/25 | Loss: 0.00146377
Iteration 17/25 | Loss: 0.00145769
Iteration 18/25 | Loss: 0.00145368
Iteration 19/25 | Loss: 0.00145719
Iteration 20/25 | Loss: 0.00145565
Iteration 21/25 | Loss: 0.00145321
Iteration 22/25 | Loss: 0.00144723
Iteration 23/25 | Loss: 0.00145264
Iteration 24/25 | Loss: 0.00144577
Iteration 25/25 | Loss: 0.00144485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49723601
Iteration 2/25 | Loss: 0.00218335
Iteration 3/25 | Loss: 0.00218334
Iteration 4/25 | Loss: 0.00218334
Iteration 5/25 | Loss: 0.00218334
Iteration 6/25 | Loss: 0.00218334
Iteration 7/25 | Loss: 0.00218334
Iteration 8/25 | Loss: 0.00218334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0021833425853401423, 0.0021833425853401423, 0.0021833425853401423, 0.0021833425853401423, 0.0021833425853401423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021833425853401423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218334
Iteration 2/1000 | Loss: 0.00034108
Iteration 3/1000 | Loss: 0.00023981
Iteration 4/1000 | Loss: 0.00111449
Iteration 5/1000 | Loss: 0.00106164
Iteration 6/1000 | Loss: 0.00089555
Iteration 7/1000 | Loss: 0.00060581
Iteration 8/1000 | Loss: 0.00056283
Iteration 9/1000 | Loss: 0.00012603
Iteration 10/1000 | Loss: 0.00108665
Iteration 11/1000 | Loss: 0.00078460
Iteration 12/1000 | Loss: 0.00064742
Iteration 13/1000 | Loss: 0.00079579
Iteration 14/1000 | Loss: 0.00047960
Iteration 15/1000 | Loss: 0.00060104
Iteration 16/1000 | Loss: 0.00029522
Iteration 17/1000 | Loss: 0.00046722
Iteration 18/1000 | Loss: 0.00028364
Iteration 19/1000 | Loss: 0.00042036
Iteration 20/1000 | Loss: 0.00047796
Iteration 21/1000 | Loss: 0.00053873
Iteration 22/1000 | Loss: 0.00053804
Iteration 23/1000 | Loss: 0.00044887
Iteration 24/1000 | Loss: 0.00046705
Iteration 25/1000 | Loss: 0.00055297
Iteration 26/1000 | Loss: 0.00049636
Iteration 27/1000 | Loss: 0.00076382
Iteration 28/1000 | Loss: 0.00056145
Iteration 29/1000 | Loss: 0.00050714
Iteration 30/1000 | Loss: 0.00047915
Iteration 31/1000 | Loss: 0.00027430
Iteration 32/1000 | Loss: 0.00015035
Iteration 33/1000 | Loss: 0.00013873
Iteration 34/1000 | Loss: 0.00014103
Iteration 35/1000 | Loss: 0.00087355
Iteration 36/1000 | Loss: 0.00057647
Iteration 37/1000 | Loss: 0.00040740
Iteration 38/1000 | Loss: 0.00052786
Iteration 39/1000 | Loss: 0.00019919
Iteration 40/1000 | Loss: 0.00006876
Iteration 41/1000 | Loss: 0.00089004
Iteration 42/1000 | Loss: 0.00047962
Iteration 43/1000 | Loss: 0.00021367
Iteration 44/1000 | Loss: 0.00067548
Iteration 45/1000 | Loss: 0.00060635
Iteration 46/1000 | Loss: 0.00005442
Iteration 47/1000 | Loss: 0.00056706
Iteration 48/1000 | Loss: 0.00105853
Iteration 49/1000 | Loss: 0.00049250
Iteration 50/1000 | Loss: 0.00061427
Iteration 51/1000 | Loss: 0.00011108
Iteration 52/1000 | Loss: 0.00009502
Iteration 53/1000 | Loss: 0.00028172
Iteration 54/1000 | Loss: 0.00011566
Iteration 55/1000 | Loss: 0.00004093
Iteration 56/1000 | Loss: 0.00031019
Iteration 57/1000 | Loss: 0.00022181
Iteration 58/1000 | Loss: 0.00012407
Iteration 59/1000 | Loss: 0.00019481
Iteration 60/1000 | Loss: 0.00027097
Iteration 61/1000 | Loss: 0.00022257
Iteration 62/1000 | Loss: 0.00022732
Iteration 63/1000 | Loss: 0.00018025
Iteration 64/1000 | Loss: 0.00010584
Iteration 65/1000 | Loss: 0.00078608
Iteration 66/1000 | Loss: 0.00084704
Iteration 67/1000 | Loss: 0.00019527
Iteration 68/1000 | Loss: 0.00027776
Iteration 69/1000 | Loss: 0.00037274
Iteration 70/1000 | Loss: 0.00047228
Iteration 71/1000 | Loss: 0.00026512
Iteration 72/1000 | Loss: 0.00011824
Iteration 73/1000 | Loss: 0.00004917
Iteration 74/1000 | Loss: 0.00006157
Iteration 75/1000 | Loss: 0.00011673
Iteration 76/1000 | Loss: 0.00005646
Iteration 77/1000 | Loss: 0.00013121
Iteration 78/1000 | Loss: 0.00005250
Iteration 79/1000 | Loss: 0.00015692
Iteration 80/1000 | Loss: 0.00018567
Iteration 81/1000 | Loss: 0.00009025
Iteration 82/1000 | Loss: 0.00004420
Iteration 83/1000 | Loss: 0.00004243
Iteration 84/1000 | Loss: 0.00003888
Iteration 85/1000 | Loss: 0.00005879
Iteration 86/1000 | Loss: 0.00034621
Iteration 87/1000 | Loss: 0.00013720
Iteration 88/1000 | Loss: 0.00004270
Iteration 89/1000 | Loss: 0.00006403
Iteration 90/1000 | Loss: 0.00034436
Iteration 91/1000 | Loss: 0.00030423
Iteration 92/1000 | Loss: 0.00023425
Iteration 93/1000 | Loss: 0.00026138
Iteration 94/1000 | Loss: 0.00021758
Iteration 95/1000 | Loss: 0.00020574
Iteration 96/1000 | Loss: 0.00003900
Iteration 97/1000 | Loss: 0.00003651
Iteration 98/1000 | Loss: 0.00028528
Iteration 99/1000 | Loss: 0.00026581
Iteration 100/1000 | Loss: 0.00005049
Iteration 101/1000 | Loss: 0.00004040
Iteration 102/1000 | Loss: 0.00003558
Iteration 103/1000 | Loss: 0.00003281
Iteration 104/1000 | Loss: 0.00003203
Iteration 105/1000 | Loss: 0.00003159
Iteration 106/1000 | Loss: 0.00003106
Iteration 107/1000 | Loss: 0.00033599
Iteration 108/1000 | Loss: 0.00020237
Iteration 109/1000 | Loss: 0.00018317
Iteration 110/1000 | Loss: 0.00017147
Iteration 111/1000 | Loss: 0.00018482
Iteration 112/1000 | Loss: 0.00022977
Iteration 113/1000 | Loss: 0.00022656
Iteration 114/1000 | Loss: 0.00015545
Iteration 115/1000 | Loss: 0.00019896
Iteration 116/1000 | Loss: 0.00014031
Iteration 117/1000 | Loss: 0.00026381
Iteration 118/1000 | Loss: 0.00004794
Iteration 119/1000 | Loss: 0.00003829
Iteration 120/1000 | Loss: 0.00003324
Iteration 121/1000 | Loss: 0.00016642
Iteration 122/1000 | Loss: 0.00018225
Iteration 123/1000 | Loss: 0.00003587
Iteration 124/1000 | Loss: 0.00003437
Iteration 125/1000 | Loss: 0.00003356
Iteration 126/1000 | Loss: 0.00006948
Iteration 127/1000 | Loss: 0.00003519
Iteration 128/1000 | Loss: 0.00003191
Iteration 129/1000 | Loss: 0.00003101
Iteration 130/1000 | Loss: 0.00003016
Iteration 131/1000 | Loss: 0.00002977
Iteration 132/1000 | Loss: 0.00002918
Iteration 133/1000 | Loss: 0.00002897
Iteration 134/1000 | Loss: 0.00002880
Iteration 135/1000 | Loss: 0.00029638
Iteration 136/1000 | Loss: 0.00020500
Iteration 137/1000 | Loss: 0.00022054
Iteration 138/1000 | Loss: 0.00015078
Iteration 139/1000 | Loss: 0.00006542
Iteration 140/1000 | Loss: 0.00024191
Iteration 141/1000 | Loss: 0.00020804
Iteration 142/1000 | Loss: 0.00014540
Iteration 143/1000 | Loss: 0.00003366
Iteration 144/1000 | Loss: 0.00002995
Iteration 145/1000 | Loss: 0.00002907
Iteration 146/1000 | Loss: 0.00002870
Iteration 147/1000 | Loss: 0.00002851
Iteration 148/1000 | Loss: 0.00002839
Iteration 149/1000 | Loss: 0.00002836
Iteration 150/1000 | Loss: 0.00002828
Iteration 151/1000 | Loss: 0.00002821
Iteration 152/1000 | Loss: 0.00002821
Iteration 153/1000 | Loss: 0.00002820
Iteration 154/1000 | Loss: 0.00002820
Iteration 155/1000 | Loss: 0.00002819
Iteration 156/1000 | Loss: 0.00002818
Iteration 157/1000 | Loss: 0.00002818
Iteration 158/1000 | Loss: 0.00002818
Iteration 159/1000 | Loss: 0.00002818
Iteration 160/1000 | Loss: 0.00002818
Iteration 161/1000 | Loss: 0.00002817
Iteration 162/1000 | Loss: 0.00002817
Iteration 163/1000 | Loss: 0.00002817
Iteration 164/1000 | Loss: 0.00002816
Iteration 165/1000 | Loss: 0.00002816
Iteration 166/1000 | Loss: 0.00002816
Iteration 167/1000 | Loss: 0.00002815
Iteration 168/1000 | Loss: 0.00002815
Iteration 169/1000 | Loss: 0.00002815
Iteration 170/1000 | Loss: 0.00002815
Iteration 171/1000 | Loss: 0.00002814
Iteration 172/1000 | Loss: 0.00002814
Iteration 173/1000 | Loss: 0.00002814
Iteration 174/1000 | Loss: 0.00002813
Iteration 175/1000 | Loss: 0.00002813
Iteration 176/1000 | Loss: 0.00002813
Iteration 177/1000 | Loss: 0.00002813
Iteration 178/1000 | Loss: 0.00002813
Iteration 179/1000 | Loss: 0.00002812
Iteration 180/1000 | Loss: 0.00002812
Iteration 181/1000 | Loss: 0.00002812
Iteration 182/1000 | Loss: 0.00002812
Iteration 183/1000 | Loss: 0.00002812
Iteration 184/1000 | Loss: 0.00002811
Iteration 185/1000 | Loss: 0.00002811
Iteration 186/1000 | Loss: 0.00002811
Iteration 187/1000 | Loss: 0.00002811
Iteration 188/1000 | Loss: 0.00002810
Iteration 189/1000 | Loss: 0.00002810
Iteration 190/1000 | Loss: 0.00002810
Iteration 191/1000 | Loss: 0.00002810
Iteration 192/1000 | Loss: 0.00002810
Iteration 193/1000 | Loss: 0.00002809
Iteration 194/1000 | Loss: 0.00002809
Iteration 195/1000 | Loss: 0.00002809
Iteration 196/1000 | Loss: 0.00002809
Iteration 197/1000 | Loss: 0.00002809
Iteration 198/1000 | Loss: 0.00002809
Iteration 199/1000 | Loss: 0.00002809
Iteration 200/1000 | Loss: 0.00002809
Iteration 201/1000 | Loss: 0.00002808
Iteration 202/1000 | Loss: 0.00002808
Iteration 203/1000 | Loss: 0.00002808
Iteration 204/1000 | Loss: 0.00002808
Iteration 205/1000 | Loss: 0.00002808
Iteration 206/1000 | Loss: 0.00002807
Iteration 207/1000 | Loss: 0.00002807
Iteration 208/1000 | Loss: 0.00002807
Iteration 209/1000 | Loss: 0.00002807
Iteration 210/1000 | Loss: 0.00002807
Iteration 211/1000 | Loss: 0.00002807
Iteration 212/1000 | Loss: 0.00002807
Iteration 213/1000 | Loss: 0.00002807
Iteration 214/1000 | Loss: 0.00002807
Iteration 215/1000 | Loss: 0.00002807
Iteration 216/1000 | Loss: 0.00002807
Iteration 217/1000 | Loss: 0.00002807
Iteration 218/1000 | Loss: 0.00002807
Iteration 219/1000 | Loss: 0.00002807
Iteration 220/1000 | Loss: 0.00002807
Iteration 221/1000 | Loss: 0.00002806
Iteration 222/1000 | Loss: 0.00002806
Iteration 223/1000 | Loss: 0.00002806
Iteration 224/1000 | Loss: 0.00002806
Iteration 225/1000 | Loss: 0.00002806
Iteration 226/1000 | Loss: 0.00002806
Iteration 227/1000 | Loss: 0.00002806
Iteration 228/1000 | Loss: 0.00002806
Iteration 229/1000 | Loss: 0.00002806
Iteration 230/1000 | Loss: 0.00002806
Iteration 231/1000 | Loss: 0.00002806
Iteration 232/1000 | Loss: 0.00002806
Iteration 233/1000 | Loss: 0.00002806
Iteration 234/1000 | Loss: 0.00002806
Iteration 235/1000 | Loss: 0.00002806
Iteration 236/1000 | Loss: 0.00002806
Iteration 237/1000 | Loss: 0.00002806
Iteration 238/1000 | Loss: 0.00002806
Iteration 239/1000 | Loss: 0.00002806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.8059384931111708e-05, 2.8059384931111708e-05, 2.8059384931111708e-05, 2.8059384931111708e-05, 2.8059384931111708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8059384931111708e-05

Optimization complete. Final v2v error: 4.548831462860107 mm

Highest mean error: 10.119625091552734 mm for frame 157

Lowest mean error: 4.196192264556885 mm for frame 141

Saving results

Total time: 270.3101849555969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01163882
Iteration 2/25 | Loss: 0.00312911
Iteration 3/25 | Loss: 0.00257110
Iteration 4/25 | Loss: 0.00240504
Iteration 5/25 | Loss: 0.00213098
Iteration 6/25 | Loss: 0.00198461
Iteration 7/25 | Loss: 0.00193215
Iteration 8/25 | Loss: 0.00185424
Iteration 9/25 | Loss: 0.00182913
Iteration 10/25 | Loss: 0.00185114
Iteration 11/25 | Loss: 0.00177218
Iteration 12/25 | Loss: 0.00173786
Iteration 13/25 | Loss: 0.00174273
Iteration 14/25 | Loss: 0.00173340
Iteration 15/25 | Loss: 0.00173650
Iteration 16/25 | Loss: 0.00173120
Iteration 17/25 | Loss: 0.00173068
Iteration 18/25 | Loss: 0.00171899
Iteration 19/25 | Loss: 0.00171163
Iteration 20/25 | Loss: 0.00171040
Iteration 21/25 | Loss: 0.00171008
Iteration 22/25 | Loss: 0.00171001
Iteration 23/25 | Loss: 0.00171001
Iteration 24/25 | Loss: 0.00171001
Iteration 25/25 | Loss: 0.00171001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59953916
Iteration 2/25 | Loss: 0.00452999
Iteration 3/25 | Loss: 0.00318125
Iteration 4/25 | Loss: 0.00318125
Iteration 5/25 | Loss: 0.00318125
Iteration 6/25 | Loss: 0.00318125
Iteration 7/25 | Loss: 0.00318125
Iteration 8/25 | Loss: 0.00318125
Iteration 9/25 | Loss: 0.00318125
Iteration 10/25 | Loss: 0.00318124
Iteration 11/25 | Loss: 0.00318124
Iteration 12/25 | Loss: 0.00318124
Iteration 13/25 | Loss: 0.00318124
Iteration 14/25 | Loss: 0.00318124
Iteration 15/25 | Loss: 0.00318124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003181244945153594, 0.003181244945153594, 0.003181244945153594, 0.003181244945153594, 0.003181244945153594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003181244945153594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00318124
Iteration 2/1000 | Loss: 0.00136360
Iteration 3/1000 | Loss: 0.00502654
Iteration 4/1000 | Loss: 0.00101625
Iteration 5/1000 | Loss: 0.00040725
Iteration 6/1000 | Loss: 0.00049518
Iteration 7/1000 | Loss: 0.00081796
Iteration 8/1000 | Loss: 0.00043031
Iteration 9/1000 | Loss: 0.00069818
Iteration 10/1000 | Loss: 0.00023071
Iteration 11/1000 | Loss: 0.00103282
Iteration 12/1000 | Loss: 0.00055278
Iteration 13/1000 | Loss: 0.00036501
Iteration 14/1000 | Loss: 0.00020386
Iteration 15/1000 | Loss: 0.00154436
Iteration 16/1000 | Loss: 0.00301855
Iteration 17/1000 | Loss: 0.00100579
Iteration 18/1000 | Loss: 0.00057806
Iteration 19/1000 | Loss: 0.00088452
Iteration 20/1000 | Loss: 0.00092507
Iteration 21/1000 | Loss: 0.00050355
Iteration 22/1000 | Loss: 0.00059623
Iteration 23/1000 | Loss: 0.00040121
Iteration 24/1000 | Loss: 0.00033766
Iteration 25/1000 | Loss: 0.00033221
Iteration 26/1000 | Loss: 0.00014568
Iteration 27/1000 | Loss: 0.00075013
Iteration 28/1000 | Loss: 0.00014690
Iteration 29/1000 | Loss: 0.00065035
Iteration 30/1000 | Loss: 0.00010755
Iteration 31/1000 | Loss: 0.00069372
Iteration 32/1000 | Loss: 0.00014109
Iteration 33/1000 | Loss: 0.00008979
Iteration 34/1000 | Loss: 0.00049123
Iteration 35/1000 | Loss: 0.00007549
Iteration 36/1000 | Loss: 0.00063557
Iteration 37/1000 | Loss: 0.00035497
Iteration 38/1000 | Loss: 0.00158805
Iteration 39/1000 | Loss: 0.00138604
Iteration 40/1000 | Loss: 0.00127844
Iteration 41/1000 | Loss: 0.00081777
Iteration 42/1000 | Loss: 0.00006969
Iteration 43/1000 | Loss: 0.00006647
Iteration 44/1000 | Loss: 0.00006485
Iteration 45/1000 | Loss: 0.00075443
Iteration 46/1000 | Loss: 0.00136851
Iteration 47/1000 | Loss: 0.00011646
Iteration 48/1000 | Loss: 0.00072192
Iteration 49/1000 | Loss: 0.00053038
Iteration 50/1000 | Loss: 0.00008037
Iteration 51/1000 | Loss: 0.00017532
Iteration 52/1000 | Loss: 0.00007203
Iteration 53/1000 | Loss: 0.00032796
Iteration 54/1000 | Loss: 0.00006338
Iteration 55/1000 | Loss: 0.00038938
Iteration 56/1000 | Loss: 0.00007133
Iteration 57/1000 | Loss: 0.00005889
Iteration 58/1000 | Loss: 0.00005759
Iteration 59/1000 | Loss: 0.00005705
Iteration 60/1000 | Loss: 0.00005662
Iteration 61/1000 | Loss: 0.00005639
Iteration 62/1000 | Loss: 0.00005616
Iteration 63/1000 | Loss: 0.00005598
Iteration 64/1000 | Loss: 0.00005585
Iteration 65/1000 | Loss: 0.00005585
Iteration 66/1000 | Loss: 0.00005585
Iteration 67/1000 | Loss: 0.00005583
Iteration 68/1000 | Loss: 0.00005583
Iteration 69/1000 | Loss: 0.00005580
Iteration 70/1000 | Loss: 0.00005578
Iteration 71/1000 | Loss: 0.00005577
Iteration 72/1000 | Loss: 0.00005576
Iteration 73/1000 | Loss: 0.00005576
Iteration 74/1000 | Loss: 0.00005576
Iteration 75/1000 | Loss: 0.00005576
Iteration 76/1000 | Loss: 0.00005576
Iteration 77/1000 | Loss: 0.00005575
Iteration 78/1000 | Loss: 0.00005573
Iteration 79/1000 | Loss: 0.00005570
Iteration 80/1000 | Loss: 0.00005569
Iteration 81/1000 | Loss: 0.00005569
Iteration 82/1000 | Loss: 0.00005568
Iteration 83/1000 | Loss: 0.00005568
Iteration 84/1000 | Loss: 0.00005568
Iteration 85/1000 | Loss: 0.00005568
Iteration 86/1000 | Loss: 0.00005568
Iteration 87/1000 | Loss: 0.00005567
Iteration 88/1000 | Loss: 0.00005567
Iteration 89/1000 | Loss: 0.00005567
Iteration 90/1000 | Loss: 0.00005566
Iteration 91/1000 | Loss: 0.00005566
Iteration 92/1000 | Loss: 0.00005566
Iteration 93/1000 | Loss: 0.00005565
Iteration 94/1000 | Loss: 0.00005565
Iteration 95/1000 | Loss: 0.00005565
Iteration 96/1000 | Loss: 0.00005564
Iteration 97/1000 | Loss: 0.00005564
Iteration 98/1000 | Loss: 0.00005563
Iteration 99/1000 | Loss: 0.00005562
Iteration 100/1000 | Loss: 0.00005561
Iteration 101/1000 | Loss: 0.00005561
Iteration 102/1000 | Loss: 0.00005561
Iteration 103/1000 | Loss: 0.00005561
Iteration 104/1000 | Loss: 0.00005561
Iteration 105/1000 | Loss: 0.00005561
Iteration 106/1000 | Loss: 0.00005561
Iteration 107/1000 | Loss: 0.00005561
Iteration 108/1000 | Loss: 0.00005561
Iteration 109/1000 | Loss: 0.00005561
Iteration 110/1000 | Loss: 0.00005559
Iteration 111/1000 | Loss: 0.00005559
Iteration 112/1000 | Loss: 0.00005559
Iteration 113/1000 | Loss: 0.00005559
Iteration 114/1000 | Loss: 0.00005559
Iteration 115/1000 | Loss: 0.00005559
Iteration 116/1000 | Loss: 0.00005558
Iteration 117/1000 | Loss: 0.00005558
Iteration 118/1000 | Loss: 0.00005558
Iteration 119/1000 | Loss: 0.00005558
Iteration 120/1000 | Loss: 0.00005558
Iteration 121/1000 | Loss: 0.00005558
Iteration 122/1000 | Loss: 0.00005558
Iteration 123/1000 | Loss: 0.00005557
Iteration 124/1000 | Loss: 0.00005557
Iteration 125/1000 | Loss: 0.00005557
Iteration 126/1000 | Loss: 0.00005556
Iteration 127/1000 | Loss: 0.00005555
Iteration 128/1000 | Loss: 0.00005555
Iteration 129/1000 | Loss: 0.00005555
Iteration 130/1000 | Loss: 0.00005555
Iteration 131/1000 | Loss: 0.00005555
Iteration 132/1000 | Loss: 0.00005555
Iteration 133/1000 | Loss: 0.00005555
Iteration 134/1000 | Loss: 0.00005555
Iteration 135/1000 | Loss: 0.00005555
Iteration 136/1000 | Loss: 0.00005555
Iteration 137/1000 | Loss: 0.00005555
Iteration 138/1000 | Loss: 0.00005555
Iteration 139/1000 | Loss: 0.00005555
Iteration 140/1000 | Loss: 0.00005555
Iteration 141/1000 | Loss: 0.00005555
Iteration 142/1000 | Loss: 0.00005554
Iteration 143/1000 | Loss: 0.00005554
Iteration 144/1000 | Loss: 0.00005554
Iteration 145/1000 | Loss: 0.00005554
Iteration 146/1000 | Loss: 0.00005554
Iteration 147/1000 | Loss: 0.00005554
Iteration 148/1000 | Loss: 0.00005554
Iteration 149/1000 | Loss: 0.00005554
Iteration 150/1000 | Loss: 0.00005554
Iteration 151/1000 | Loss: 0.00005553
Iteration 152/1000 | Loss: 0.00005553
Iteration 153/1000 | Loss: 0.00005553
Iteration 154/1000 | Loss: 0.00005553
Iteration 155/1000 | Loss: 0.00005553
Iteration 156/1000 | Loss: 0.00005552
Iteration 157/1000 | Loss: 0.00005552
Iteration 158/1000 | Loss: 0.00005552
Iteration 159/1000 | Loss: 0.00005552
Iteration 160/1000 | Loss: 0.00005552
Iteration 161/1000 | Loss: 0.00005552
Iteration 162/1000 | Loss: 0.00005551
Iteration 163/1000 | Loss: 0.00005551
Iteration 164/1000 | Loss: 0.00005551
Iteration 165/1000 | Loss: 0.00005551
Iteration 166/1000 | Loss: 0.00005550
Iteration 167/1000 | Loss: 0.00005550
Iteration 168/1000 | Loss: 0.00005550
Iteration 169/1000 | Loss: 0.00005550
Iteration 170/1000 | Loss: 0.00005550
Iteration 171/1000 | Loss: 0.00005550
Iteration 172/1000 | Loss: 0.00005550
Iteration 173/1000 | Loss: 0.00005550
Iteration 174/1000 | Loss: 0.00005550
Iteration 175/1000 | Loss: 0.00005550
Iteration 176/1000 | Loss: 0.00005550
Iteration 177/1000 | Loss: 0.00005550
Iteration 178/1000 | Loss: 0.00005550
Iteration 179/1000 | Loss: 0.00005550
Iteration 180/1000 | Loss: 0.00005550
Iteration 181/1000 | Loss: 0.00005550
Iteration 182/1000 | Loss: 0.00005550
Iteration 183/1000 | Loss: 0.00005550
Iteration 184/1000 | Loss: 0.00005550
Iteration 185/1000 | Loss: 0.00005550
Iteration 186/1000 | Loss: 0.00005550
Iteration 187/1000 | Loss: 0.00005550
Iteration 188/1000 | Loss: 0.00005550
Iteration 189/1000 | Loss: 0.00005550
Iteration 190/1000 | Loss: 0.00005550
Iteration 191/1000 | Loss: 0.00005550
Iteration 192/1000 | Loss: 0.00005550
Iteration 193/1000 | Loss: 0.00005550
Iteration 194/1000 | Loss: 0.00005550
Iteration 195/1000 | Loss: 0.00005550
Iteration 196/1000 | Loss: 0.00005550
Iteration 197/1000 | Loss: 0.00005550
Iteration 198/1000 | Loss: 0.00005550
Iteration 199/1000 | Loss: 0.00005550
Iteration 200/1000 | Loss: 0.00005550
Iteration 201/1000 | Loss: 0.00005550
Iteration 202/1000 | Loss: 0.00005550
Iteration 203/1000 | Loss: 0.00005550
Iteration 204/1000 | Loss: 0.00005550
Iteration 205/1000 | Loss: 0.00005550
Iteration 206/1000 | Loss: 0.00005550
Iteration 207/1000 | Loss: 0.00005550
Iteration 208/1000 | Loss: 0.00005550
Iteration 209/1000 | Loss: 0.00005550
Iteration 210/1000 | Loss: 0.00005550
Iteration 211/1000 | Loss: 0.00005550
Iteration 212/1000 | Loss: 0.00005550
Iteration 213/1000 | Loss: 0.00005550
Iteration 214/1000 | Loss: 0.00005550
Iteration 215/1000 | Loss: 0.00005550
Iteration 216/1000 | Loss: 0.00005550
Iteration 217/1000 | Loss: 0.00005550
Iteration 218/1000 | Loss: 0.00005550
Iteration 219/1000 | Loss: 0.00005550
Iteration 220/1000 | Loss: 0.00005550
Iteration 221/1000 | Loss: 0.00005550
Iteration 222/1000 | Loss: 0.00005550
Iteration 223/1000 | Loss: 0.00005550
Iteration 224/1000 | Loss: 0.00005550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [5.549511479330249e-05, 5.549511479330249e-05, 5.549511479330249e-05, 5.549511479330249e-05, 5.549511479330249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.549511479330249e-05

Optimization complete. Final v2v error: 5.081502914428711 mm

Highest mean error: 13.357805252075195 mm for frame 27

Lowest mean error: 4.383761405944824 mm for frame 11

Saving results

Total time: 136.06090211868286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440087
Iteration 2/25 | Loss: 0.00163794
Iteration 3/25 | Loss: 0.00146667
Iteration 4/25 | Loss: 0.00144590
Iteration 5/25 | Loss: 0.00144061
Iteration 6/25 | Loss: 0.00143934
Iteration 7/25 | Loss: 0.00143925
Iteration 8/25 | Loss: 0.00143925
Iteration 9/25 | Loss: 0.00143925
Iteration 10/25 | Loss: 0.00143925
Iteration 11/25 | Loss: 0.00143925
Iteration 12/25 | Loss: 0.00143925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014392525190487504, 0.0014392525190487504, 0.0014392525190487504, 0.0014392525190487504, 0.0014392525190487504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014392525190487504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48261571
Iteration 2/25 | Loss: 0.00149655
Iteration 3/25 | Loss: 0.00149654
Iteration 4/25 | Loss: 0.00149654
Iteration 5/25 | Loss: 0.00149654
Iteration 6/25 | Loss: 0.00149654
Iteration 7/25 | Loss: 0.00149654
Iteration 8/25 | Loss: 0.00149654
Iteration 9/25 | Loss: 0.00149654
Iteration 10/25 | Loss: 0.00149654
Iteration 11/25 | Loss: 0.00149654
Iteration 12/25 | Loss: 0.00149654
Iteration 13/25 | Loss: 0.00149654
Iteration 14/25 | Loss: 0.00149654
Iteration 15/25 | Loss: 0.00149654
Iteration 16/25 | Loss: 0.00149654
Iteration 17/25 | Loss: 0.00149654
Iteration 18/25 | Loss: 0.00149654
Iteration 19/25 | Loss: 0.00149654
Iteration 20/25 | Loss: 0.00149654
Iteration 21/25 | Loss: 0.00149654
Iteration 22/25 | Loss: 0.00149654
Iteration 23/25 | Loss: 0.00149654
Iteration 24/25 | Loss: 0.00149654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014965416630730033, 0.0014965416630730033, 0.0014965416630730033, 0.0014965416630730033, 0.0014965416630730033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014965416630730033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149654
Iteration 2/1000 | Loss: 0.00004543
Iteration 3/1000 | Loss: 0.00003553
Iteration 4/1000 | Loss: 0.00003132
Iteration 5/1000 | Loss: 0.00003000
Iteration 6/1000 | Loss: 0.00002904
Iteration 7/1000 | Loss: 0.00002828
Iteration 8/1000 | Loss: 0.00002766
Iteration 9/1000 | Loss: 0.00002735
Iteration 10/1000 | Loss: 0.00002712
Iteration 11/1000 | Loss: 0.00002699
Iteration 12/1000 | Loss: 0.00002696
Iteration 13/1000 | Loss: 0.00002696
Iteration 14/1000 | Loss: 0.00002695
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002688
Iteration 17/1000 | Loss: 0.00002684
Iteration 18/1000 | Loss: 0.00002684
Iteration 19/1000 | Loss: 0.00002683
Iteration 20/1000 | Loss: 0.00002683
Iteration 21/1000 | Loss: 0.00002683
Iteration 22/1000 | Loss: 0.00002681
Iteration 23/1000 | Loss: 0.00002681
Iteration 24/1000 | Loss: 0.00002680
Iteration 25/1000 | Loss: 0.00002680
Iteration 26/1000 | Loss: 0.00002680
Iteration 27/1000 | Loss: 0.00002679
Iteration 28/1000 | Loss: 0.00002679
Iteration 29/1000 | Loss: 0.00002679
Iteration 30/1000 | Loss: 0.00002679
Iteration 31/1000 | Loss: 0.00002679
Iteration 32/1000 | Loss: 0.00002679
Iteration 33/1000 | Loss: 0.00002679
Iteration 34/1000 | Loss: 0.00002678
Iteration 35/1000 | Loss: 0.00002678
Iteration 36/1000 | Loss: 0.00002678
Iteration 37/1000 | Loss: 0.00002677
Iteration 38/1000 | Loss: 0.00002677
Iteration 39/1000 | Loss: 0.00002677
Iteration 40/1000 | Loss: 0.00002677
Iteration 41/1000 | Loss: 0.00002677
Iteration 42/1000 | Loss: 0.00002677
Iteration 43/1000 | Loss: 0.00002677
Iteration 44/1000 | Loss: 0.00002676
Iteration 45/1000 | Loss: 0.00002676
Iteration 46/1000 | Loss: 0.00002676
Iteration 47/1000 | Loss: 0.00002675
Iteration 48/1000 | Loss: 0.00002675
Iteration 49/1000 | Loss: 0.00002675
Iteration 50/1000 | Loss: 0.00002674
Iteration 51/1000 | Loss: 0.00002674
Iteration 52/1000 | Loss: 0.00002674
Iteration 53/1000 | Loss: 0.00002674
Iteration 54/1000 | Loss: 0.00002673
Iteration 55/1000 | Loss: 0.00002673
Iteration 56/1000 | Loss: 0.00002673
Iteration 57/1000 | Loss: 0.00002673
Iteration 58/1000 | Loss: 0.00002672
Iteration 59/1000 | Loss: 0.00002672
Iteration 60/1000 | Loss: 0.00002672
Iteration 61/1000 | Loss: 0.00002672
Iteration 62/1000 | Loss: 0.00002671
Iteration 63/1000 | Loss: 0.00002671
Iteration 64/1000 | Loss: 0.00002671
Iteration 65/1000 | Loss: 0.00002671
Iteration 66/1000 | Loss: 0.00002671
Iteration 67/1000 | Loss: 0.00002671
Iteration 68/1000 | Loss: 0.00002670
Iteration 69/1000 | Loss: 0.00002670
Iteration 70/1000 | Loss: 0.00002669
Iteration 71/1000 | Loss: 0.00002669
Iteration 72/1000 | Loss: 0.00002669
Iteration 73/1000 | Loss: 0.00002668
Iteration 74/1000 | Loss: 0.00002668
Iteration 75/1000 | Loss: 0.00002668
Iteration 76/1000 | Loss: 0.00002667
Iteration 77/1000 | Loss: 0.00002667
Iteration 78/1000 | Loss: 0.00002667
Iteration 79/1000 | Loss: 0.00002667
Iteration 80/1000 | Loss: 0.00002666
Iteration 81/1000 | Loss: 0.00002666
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002665
Iteration 84/1000 | Loss: 0.00002665
Iteration 85/1000 | Loss: 0.00002665
Iteration 86/1000 | Loss: 0.00002665
Iteration 87/1000 | Loss: 0.00002665
Iteration 88/1000 | Loss: 0.00002664
Iteration 89/1000 | Loss: 0.00002664
Iteration 90/1000 | Loss: 0.00002664
Iteration 91/1000 | Loss: 0.00002664
Iteration 92/1000 | Loss: 0.00002664
Iteration 93/1000 | Loss: 0.00002664
Iteration 94/1000 | Loss: 0.00002664
Iteration 95/1000 | Loss: 0.00002663
Iteration 96/1000 | Loss: 0.00002663
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00002663
Iteration 99/1000 | Loss: 0.00002663
Iteration 100/1000 | Loss: 0.00002663
Iteration 101/1000 | Loss: 0.00002663
Iteration 102/1000 | Loss: 0.00002662
Iteration 103/1000 | Loss: 0.00002662
Iteration 104/1000 | Loss: 0.00002662
Iteration 105/1000 | Loss: 0.00002662
Iteration 106/1000 | Loss: 0.00002662
Iteration 107/1000 | Loss: 0.00002662
Iteration 108/1000 | Loss: 0.00002662
Iteration 109/1000 | Loss: 0.00002662
Iteration 110/1000 | Loss: 0.00002662
Iteration 111/1000 | Loss: 0.00002662
Iteration 112/1000 | Loss: 0.00002662
Iteration 113/1000 | Loss: 0.00002662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.6621302822604775e-05, 2.6621302822604775e-05, 2.6621302822604775e-05, 2.6621302822604775e-05, 2.6621302822604775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6621302822604775e-05

Optimization complete. Final v2v error: 4.451234340667725 mm

Highest mean error: 4.722859859466553 mm for frame 52

Lowest mean error: 4.145389556884766 mm for frame 15

Saving results

Total time: 34.365668296813965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141977
Iteration 2/25 | Loss: 0.01141977
Iteration 3/25 | Loss: 0.00337550
Iteration 4/25 | Loss: 0.00296510
Iteration 5/25 | Loss: 0.00277387
Iteration 6/25 | Loss: 0.00227039
Iteration 7/25 | Loss: 0.00197472
Iteration 8/25 | Loss: 0.00174000
Iteration 9/25 | Loss: 0.00165802
Iteration 10/25 | Loss: 0.00160577
Iteration 11/25 | Loss: 0.00167167
Iteration 12/25 | Loss: 0.00166451
Iteration 13/25 | Loss: 0.00156967
Iteration 14/25 | Loss: 0.00155228
Iteration 15/25 | Loss: 0.00153605
Iteration 16/25 | Loss: 0.00152773
Iteration 17/25 | Loss: 0.00153195
Iteration 18/25 | Loss: 0.00151827
Iteration 19/25 | Loss: 0.00150988
Iteration 20/25 | Loss: 0.00151505
Iteration 21/25 | Loss: 0.00151143
Iteration 22/25 | Loss: 0.00151040
Iteration 23/25 | Loss: 0.00150702
Iteration 24/25 | Loss: 0.00150767
Iteration 25/25 | Loss: 0.00150899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52895045
Iteration 2/25 | Loss: 0.00221628
Iteration 3/25 | Loss: 0.00215802
Iteration 4/25 | Loss: 0.00215802
Iteration 5/25 | Loss: 0.00215802
Iteration 6/25 | Loss: 0.00215802
Iteration 7/25 | Loss: 0.00215802
Iteration 8/25 | Loss: 0.00215802
Iteration 9/25 | Loss: 0.00215802
Iteration 10/25 | Loss: 0.00215802
Iteration 11/25 | Loss: 0.00215802
Iteration 12/25 | Loss: 0.00215802
Iteration 13/25 | Loss: 0.00215802
Iteration 14/25 | Loss: 0.00215802
Iteration 15/25 | Loss: 0.00215802
Iteration 16/25 | Loss: 0.00215802
Iteration 17/25 | Loss: 0.00215802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00215801945887506, 0.00215801945887506, 0.00215801945887506, 0.00215801945887506, 0.00215801945887506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00215801945887506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215802
Iteration 2/1000 | Loss: 0.00574487
Iteration 3/1000 | Loss: 0.00229279
Iteration 4/1000 | Loss: 0.00453909
Iteration 5/1000 | Loss: 0.00164701
Iteration 6/1000 | Loss: 0.00232991
Iteration 7/1000 | Loss: 0.00266457
Iteration 8/1000 | Loss: 0.00333541
Iteration 9/1000 | Loss: 0.00225203
Iteration 10/1000 | Loss: 0.00088290
Iteration 11/1000 | Loss: 0.00174275
Iteration 12/1000 | Loss: 0.00141313
Iteration 13/1000 | Loss: 0.00114659
Iteration 14/1000 | Loss: 0.00149085
Iteration 15/1000 | Loss: 0.00161910
Iteration 16/1000 | Loss: 0.00100034
Iteration 17/1000 | Loss: 0.00093787
Iteration 18/1000 | Loss: 0.00173069
Iteration 19/1000 | Loss: 0.00095933
Iteration 20/1000 | Loss: 0.00126132
Iteration 21/1000 | Loss: 0.00083349
Iteration 22/1000 | Loss: 0.00127716
Iteration 23/1000 | Loss: 0.00111063
Iteration 24/1000 | Loss: 0.00143388
Iteration 25/1000 | Loss: 0.00087957
Iteration 26/1000 | Loss: 0.00113904
Iteration 27/1000 | Loss: 0.00124759
Iteration 28/1000 | Loss: 0.00027736
Iteration 29/1000 | Loss: 0.00235887
Iteration 30/1000 | Loss: 0.00387471
Iteration 31/1000 | Loss: 0.00184853
Iteration 32/1000 | Loss: 0.00314705
Iteration 33/1000 | Loss: 0.00084651
Iteration 34/1000 | Loss: 0.00038471
Iteration 35/1000 | Loss: 0.00022825
Iteration 36/1000 | Loss: 0.00038105
Iteration 37/1000 | Loss: 0.00030359
Iteration 38/1000 | Loss: 0.00042913
Iteration 39/1000 | Loss: 0.00030680
Iteration 40/1000 | Loss: 0.00025073
Iteration 41/1000 | Loss: 0.00065152
Iteration 42/1000 | Loss: 0.00207329
Iteration 43/1000 | Loss: 0.00143297
Iteration 44/1000 | Loss: 0.00162155
Iteration 45/1000 | Loss: 0.00028748
Iteration 46/1000 | Loss: 0.00025694
Iteration 47/1000 | Loss: 0.00124945
Iteration 48/1000 | Loss: 0.00091858
Iteration 49/1000 | Loss: 0.00062904
Iteration 50/1000 | Loss: 0.00032790
Iteration 51/1000 | Loss: 0.00026843
Iteration 52/1000 | Loss: 0.00113006
Iteration 53/1000 | Loss: 0.00068373
Iteration 54/1000 | Loss: 0.00134508
Iteration 55/1000 | Loss: 0.00278519
Iteration 56/1000 | Loss: 0.00152380
Iteration 57/1000 | Loss: 0.00200153
Iteration 58/1000 | Loss: 0.00241063
Iteration 59/1000 | Loss: 0.00210718
Iteration 60/1000 | Loss: 0.00113299
Iteration 61/1000 | Loss: 0.00371685
Iteration 62/1000 | Loss: 0.00153886
Iteration 63/1000 | Loss: 0.00571993
Iteration 64/1000 | Loss: 0.00348250
Iteration 65/1000 | Loss: 0.00167286
Iteration 66/1000 | Loss: 0.00199145
Iteration 67/1000 | Loss: 0.00287051
Iteration 68/1000 | Loss: 0.00104611
Iteration 69/1000 | Loss: 0.00097438
Iteration 70/1000 | Loss: 0.00185484
Iteration 71/1000 | Loss: 0.00140943
Iteration 72/1000 | Loss: 0.00204179
Iteration 73/1000 | Loss: 0.00207855
Iteration 74/1000 | Loss: 0.00161949
Iteration 75/1000 | Loss: 0.00146800
Iteration 76/1000 | Loss: 0.00181242
Iteration 77/1000 | Loss: 0.00221331
Iteration 78/1000 | Loss: 0.00234455
Iteration 79/1000 | Loss: 0.00186349
Iteration 80/1000 | Loss: 0.00229107
Iteration 81/1000 | Loss: 0.00273661
Iteration 82/1000 | Loss: 0.00158609
Iteration 83/1000 | Loss: 0.00143738
Iteration 84/1000 | Loss: 0.00214241
Iteration 85/1000 | Loss: 0.00079489
Iteration 86/1000 | Loss: 0.00094850
Iteration 87/1000 | Loss: 0.00197505
Iteration 88/1000 | Loss: 0.00111063
Iteration 89/1000 | Loss: 0.00194595
Iteration 90/1000 | Loss: 0.00108119
Iteration 91/1000 | Loss: 0.00099136
Iteration 92/1000 | Loss: 0.00067182
Iteration 93/1000 | Loss: 0.00066191
Iteration 94/1000 | Loss: 0.00101469
Iteration 95/1000 | Loss: 0.00109962
Iteration 96/1000 | Loss: 0.00034707
Iteration 97/1000 | Loss: 0.00251734
Iteration 98/1000 | Loss: 0.00015600
Iteration 99/1000 | Loss: 0.00154889
Iteration 100/1000 | Loss: 0.00008067
Iteration 101/1000 | Loss: 0.00005496
Iteration 102/1000 | Loss: 0.00004855
Iteration 103/1000 | Loss: 0.00005287
Iteration 104/1000 | Loss: 0.00004962
Iteration 105/1000 | Loss: 0.00004186
Iteration 106/1000 | Loss: 0.00003884
Iteration 107/1000 | Loss: 0.00009318
Iteration 108/1000 | Loss: 0.00004838
Iteration 109/1000 | Loss: 0.00027395
Iteration 110/1000 | Loss: 0.00003376
Iteration 111/1000 | Loss: 0.00003181
Iteration 112/1000 | Loss: 0.00003100
Iteration 113/1000 | Loss: 0.00003022
Iteration 114/1000 | Loss: 0.00002971
Iteration 115/1000 | Loss: 0.00002933
Iteration 116/1000 | Loss: 0.00002893
Iteration 117/1000 | Loss: 0.00002871
Iteration 118/1000 | Loss: 0.00010027
Iteration 119/1000 | Loss: 0.00003791
Iteration 120/1000 | Loss: 0.00003920
Iteration 121/1000 | Loss: 0.00003001
Iteration 122/1000 | Loss: 0.00004149
Iteration 123/1000 | Loss: 0.00002963
Iteration 124/1000 | Loss: 0.00002836
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002836
Iteration 130/1000 | Loss: 0.00002835
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Iteration 134/1000 | Loss: 0.00002835
Iteration 135/1000 | Loss: 0.00002835
Iteration 136/1000 | Loss: 0.00002835
Iteration 137/1000 | Loss: 0.00002835
Iteration 138/1000 | Loss: 0.00002834
Iteration 139/1000 | Loss: 0.00002834
Iteration 140/1000 | Loss: 0.00002834
Iteration 141/1000 | Loss: 0.00002833
Iteration 142/1000 | Loss: 0.00002833
Iteration 143/1000 | Loss: 0.00002833
Iteration 144/1000 | Loss: 0.00002823
Iteration 145/1000 | Loss: 0.00002822
Iteration 146/1000 | Loss: 0.00002822
Iteration 147/1000 | Loss: 0.00002822
Iteration 148/1000 | Loss: 0.00002822
Iteration 149/1000 | Loss: 0.00002821
Iteration 150/1000 | Loss: 0.00002821
Iteration 151/1000 | Loss: 0.00002821
Iteration 152/1000 | Loss: 0.00002820
Iteration 153/1000 | Loss: 0.00002820
Iteration 154/1000 | Loss: 0.00002820
Iteration 155/1000 | Loss: 0.00002820
Iteration 156/1000 | Loss: 0.00002820
Iteration 157/1000 | Loss: 0.00002819
Iteration 158/1000 | Loss: 0.00002819
Iteration 159/1000 | Loss: 0.00002819
Iteration 160/1000 | Loss: 0.00002819
Iteration 161/1000 | Loss: 0.00002818
Iteration 162/1000 | Loss: 0.00002818
Iteration 163/1000 | Loss: 0.00002818
Iteration 164/1000 | Loss: 0.00002817
Iteration 165/1000 | Loss: 0.00002817
Iteration 166/1000 | Loss: 0.00002816
Iteration 167/1000 | Loss: 0.00011994
Iteration 168/1000 | Loss: 0.00004242
Iteration 169/1000 | Loss: 0.00007096
Iteration 170/1000 | Loss: 0.00003412
Iteration 171/1000 | Loss: 0.00004661
Iteration 172/1000 | Loss: 0.00003239
Iteration 173/1000 | Loss: 0.00003674
Iteration 174/1000 | Loss: 0.00002890
Iteration 175/1000 | Loss: 0.00003703
Iteration 176/1000 | Loss: 0.00002878
Iteration 177/1000 | Loss: 0.00003299
Iteration 178/1000 | Loss: 0.00002850
Iteration 179/1000 | Loss: 0.00002829
Iteration 180/1000 | Loss: 0.00002815
Iteration 181/1000 | Loss: 0.00002815
Iteration 182/1000 | Loss: 0.00002809
Iteration 183/1000 | Loss: 0.00002808
Iteration 184/1000 | Loss: 0.00002808
Iteration 185/1000 | Loss: 0.00002804
Iteration 186/1000 | Loss: 0.00002804
Iteration 187/1000 | Loss: 0.00002804
Iteration 188/1000 | Loss: 0.00002804
Iteration 189/1000 | Loss: 0.00002803
Iteration 190/1000 | Loss: 0.00002803
Iteration 191/1000 | Loss: 0.00002803
Iteration 192/1000 | Loss: 0.00002802
Iteration 193/1000 | Loss: 0.00002802
Iteration 194/1000 | Loss: 0.00002801
Iteration 195/1000 | Loss: 0.00003493
Iteration 196/1000 | Loss: 0.00002870
Iteration 197/1000 | Loss: 0.00002803
Iteration 198/1000 | Loss: 0.00002800
Iteration 199/1000 | Loss: 0.00002800
Iteration 200/1000 | Loss: 0.00002800
Iteration 201/1000 | Loss: 0.00002800
Iteration 202/1000 | Loss: 0.00002800
Iteration 203/1000 | Loss: 0.00002800
Iteration 204/1000 | Loss: 0.00002799
Iteration 205/1000 | Loss: 0.00002799
Iteration 206/1000 | Loss: 0.00002799
Iteration 207/1000 | Loss: 0.00002799
Iteration 208/1000 | Loss: 0.00002799
Iteration 209/1000 | Loss: 0.00002799
Iteration 210/1000 | Loss: 0.00002799
Iteration 211/1000 | Loss: 0.00002799
Iteration 212/1000 | Loss: 0.00002799
Iteration 213/1000 | Loss: 0.00002799
Iteration 214/1000 | Loss: 0.00002799
Iteration 215/1000 | Loss: 0.00002799
Iteration 216/1000 | Loss: 0.00002799
Iteration 217/1000 | Loss: 0.00002798
Iteration 218/1000 | Loss: 0.00002798
Iteration 219/1000 | Loss: 0.00002798
Iteration 220/1000 | Loss: 0.00002798
Iteration 221/1000 | Loss: 0.00002798
Iteration 222/1000 | Loss: 0.00002798
Iteration 223/1000 | Loss: 0.00002798
Iteration 224/1000 | Loss: 0.00002798
Iteration 225/1000 | Loss: 0.00002798
Iteration 226/1000 | Loss: 0.00002798
Iteration 227/1000 | Loss: 0.00002798
Iteration 228/1000 | Loss: 0.00002798
Iteration 229/1000 | Loss: 0.00002798
Iteration 230/1000 | Loss: 0.00002798
Iteration 231/1000 | Loss: 0.00002798
Iteration 232/1000 | Loss: 0.00002798
Iteration 233/1000 | Loss: 0.00002798
Iteration 234/1000 | Loss: 0.00002798
Iteration 235/1000 | Loss: 0.00002798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.798018977046013e-05, 2.798018977046013e-05, 2.798018977046013e-05, 2.798018977046013e-05, 2.798018977046013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.798018977046013e-05

Optimization complete. Final v2v error: 4.344890117645264 mm

Highest mean error: 16.008878707885742 mm for frame 155

Lowest mean error: 3.6310272216796875 mm for frame 21

Saving results

Total time: 283.31631445884705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125355
Iteration 2/25 | Loss: 0.00260359
Iteration 3/25 | Loss: 0.00202372
Iteration 4/25 | Loss: 0.00210664
Iteration 5/25 | Loss: 0.00194030
Iteration 6/25 | Loss: 0.00175766
Iteration 7/25 | Loss: 0.00163270
Iteration 8/25 | Loss: 0.00154677
Iteration 9/25 | Loss: 0.00147545
Iteration 10/25 | Loss: 0.00145595
Iteration 11/25 | Loss: 0.00144273
Iteration 12/25 | Loss: 0.00143118
Iteration 13/25 | Loss: 0.00143223
Iteration 14/25 | Loss: 0.00142998
Iteration 15/25 | Loss: 0.00142577
Iteration 16/25 | Loss: 0.00142433
Iteration 17/25 | Loss: 0.00141160
Iteration 18/25 | Loss: 0.00141923
Iteration 19/25 | Loss: 0.00141677
Iteration 20/25 | Loss: 0.00140882
Iteration 21/25 | Loss: 0.00141526
Iteration 22/25 | Loss: 0.00140625
Iteration 23/25 | Loss: 0.00140175
Iteration 24/25 | Loss: 0.00140631
Iteration 25/25 | Loss: 0.00140755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52868247
Iteration 2/25 | Loss: 0.00179520
Iteration 3/25 | Loss: 0.00175959
Iteration 4/25 | Loss: 0.00175959
Iteration 5/25 | Loss: 0.00175959
Iteration 6/25 | Loss: 0.00175959
Iteration 7/25 | Loss: 0.00175959
Iteration 8/25 | Loss: 0.00175959
Iteration 9/25 | Loss: 0.00175959
Iteration 10/25 | Loss: 0.00175959
Iteration 11/25 | Loss: 0.00175959
Iteration 12/25 | Loss: 0.00175959
Iteration 13/25 | Loss: 0.00175959
Iteration 14/25 | Loss: 0.00175959
Iteration 15/25 | Loss: 0.00175959
Iteration 16/25 | Loss: 0.00175959
Iteration 17/25 | Loss: 0.00175959
Iteration 18/25 | Loss: 0.00175959
Iteration 19/25 | Loss: 0.00175959
Iteration 20/25 | Loss: 0.00175959
Iteration 21/25 | Loss: 0.00175959
Iteration 22/25 | Loss: 0.00175959
Iteration 23/25 | Loss: 0.00175959
Iteration 24/25 | Loss: 0.00175959
Iteration 25/25 | Loss: 0.00175959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175959
Iteration 2/1000 | Loss: 0.00039331
Iteration 3/1000 | Loss: 0.00023479
Iteration 4/1000 | Loss: 0.00008289
Iteration 5/1000 | Loss: 0.00022466
Iteration 6/1000 | Loss: 0.00030280
Iteration 7/1000 | Loss: 0.00008769
Iteration 8/1000 | Loss: 0.00020600
Iteration 9/1000 | Loss: 0.00021542
Iteration 10/1000 | Loss: 0.00022627
Iteration 11/1000 | Loss: 0.00030501
Iteration 12/1000 | Loss: 0.00030096
Iteration 13/1000 | Loss: 0.00091204
Iteration 14/1000 | Loss: 0.00052666
Iteration 15/1000 | Loss: 0.00039739
Iteration 16/1000 | Loss: 0.00039076
Iteration 17/1000 | Loss: 0.00072692
Iteration 18/1000 | Loss: 0.00029666
Iteration 19/1000 | Loss: 0.00047703
Iteration 20/1000 | Loss: 0.00015547
Iteration 21/1000 | Loss: 0.00128273
Iteration 22/1000 | Loss: 0.00105038
Iteration 23/1000 | Loss: 0.00026680
Iteration 24/1000 | Loss: 0.00039427
Iteration 25/1000 | Loss: 0.00019615
Iteration 26/1000 | Loss: 0.00036924
Iteration 27/1000 | Loss: 0.00131389
Iteration 28/1000 | Loss: 0.00048652
Iteration 29/1000 | Loss: 0.00015758
Iteration 30/1000 | Loss: 0.00024603
Iteration 31/1000 | Loss: 0.00043510
Iteration 32/1000 | Loss: 0.00035233
Iteration 33/1000 | Loss: 0.00040060
Iteration 34/1000 | Loss: 0.00056997
Iteration 35/1000 | Loss: 0.00023996
Iteration 36/1000 | Loss: 0.00036661
Iteration 37/1000 | Loss: 0.00043988
Iteration 38/1000 | Loss: 0.00029434
Iteration 39/1000 | Loss: 0.00123103
Iteration 40/1000 | Loss: 0.00049454
Iteration 41/1000 | Loss: 0.00048350
Iteration 42/1000 | Loss: 0.00133150
Iteration 43/1000 | Loss: 0.00020147
Iteration 44/1000 | Loss: 0.00014035
Iteration 45/1000 | Loss: 0.00012436
Iteration 46/1000 | Loss: 0.00011706
Iteration 47/1000 | Loss: 0.00014214
Iteration 48/1000 | Loss: 0.00019303
Iteration 49/1000 | Loss: 0.00014146
Iteration 50/1000 | Loss: 0.00017382
Iteration 51/1000 | Loss: 0.00009301
Iteration 52/1000 | Loss: 0.00009279
Iteration 53/1000 | Loss: 0.00129169
Iteration 54/1000 | Loss: 0.00037901
Iteration 55/1000 | Loss: 0.00068839
Iteration 56/1000 | Loss: 0.00009037
Iteration 57/1000 | Loss: 0.00009375
Iteration 58/1000 | Loss: 0.00011547
Iteration 59/1000 | Loss: 0.00008613
Iteration 60/1000 | Loss: 0.00009745
Iteration 61/1000 | Loss: 0.00009125
Iteration 62/1000 | Loss: 0.00009221
Iteration 63/1000 | Loss: 0.00007275
Iteration 64/1000 | Loss: 0.00018843
Iteration 65/1000 | Loss: 0.00017565
Iteration 66/1000 | Loss: 0.00017312
Iteration 67/1000 | Loss: 0.00008418
Iteration 68/1000 | Loss: 0.00008280
Iteration 69/1000 | Loss: 0.00038792
Iteration 70/1000 | Loss: 0.00010177
Iteration 71/1000 | Loss: 0.00008878
Iteration 72/1000 | Loss: 0.00009099
Iteration 73/1000 | Loss: 0.00018710
Iteration 74/1000 | Loss: 0.00007292
Iteration 75/1000 | Loss: 0.00006485
Iteration 76/1000 | Loss: 0.00024477
Iteration 77/1000 | Loss: 0.00019864
Iteration 78/1000 | Loss: 0.00019473
Iteration 79/1000 | Loss: 0.00004710
Iteration 80/1000 | Loss: 0.00004288
Iteration 81/1000 | Loss: 0.00004380
Iteration 82/1000 | Loss: 0.00004197
Iteration 83/1000 | Loss: 0.00004006
Iteration 84/1000 | Loss: 0.00003903
Iteration 85/1000 | Loss: 0.00003832
Iteration 86/1000 | Loss: 0.00003776
Iteration 87/1000 | Loss: 0.00023441
Iteration 88/1000 | Loss: 0.00025066
Iteration 89/1000 | Loss: 0.00039012
Iteration 90/1000 | Loss: 0.00007681
Iteration 91/1000 | Loss: 0.00004662
Iteration 92/1000 | Loss: 0.00020869
Iteration 93/1000 | Loss: 0.00018337
Iteration 94/1000 | Loss: 0.00004380
Iteration 95/1000 | Loss: 0.00006418
Iteration 96/1000 | Loss: 0.00019716
Iteration 97/1000 | Loss: 0.00007809
Iteration 98/1000 | Loss: 0.00004659
Iteration 99/1000 | Loss: 0.00004164
Iteration 100/1000 | Loss: 0.00003889
Iteration 101/1000 | Loss: 0.00009264
Iteration 102/1000 | Loss: 0.00005834
Iteration 103/1000 | Loss: 0.00009802
Iteration 104/1000 | Loss: 0.00005285
Iteration 105/1000 | Loss: 0.00009874
Iteration 106/1000 | Loss: 0.00005266
Iteration 107/1000 | Loss: 0.00009766
Iteration 108/1000 | Loss: 0.00005460
Iteration 109/1000 | Loss: 0.00003682
Iteration 110/1000 | Loss: 0.00003556
Iteration 111/1000 | Loss: 0.00003519
Iteration 112/1000 | Loss: 0.00003488
Iteration 113/1000 | Loss: 0.00003466
Iteration 114/1000 | Loss: 0.00003463
Iteration 115/1000 | Loss: 0.00003443
Iteration 116/1000 | Loss: 0.00003443
Iteration 117/1000 | Loss: 0.00003443
Iteration 118/1000 | Loss: 0.00003442
Iteration 119/1000 | Loss: 0.00003442
Iteration 120/1000 | Loss: 0.00003441
Iteration 121/1000 | Loss: 0.00003441
Iteration 122/1000 | Loss: 0.00003440
Iteration 123/1000 | Loss: 0.00003439
Iteration 124/1000 | Loss: 0.00003439
Iteration 125/1000 | Loss: 0.00003438
Iteration 126/1000 | Loss: 0.00003438
Iteration 127/1000 | Loss: 0.00003437
Iteration 128/1000 | Loss: 0.00003437
Iteration 129/1000 | Loss: 0.00003436
Iteration 130/1000 | Loss: 0.00003433
Iteration 131/1000 | Loss: 0.00003432
Iteration 132/1000 | Loss: 0.00003430
Iteration 133/1000 | Loss: 0.00003430
Iteration 134/1000 | Loss: 0.00003429
Iteration 135/1000 | Loss: 0.00003429
Iteration 136/1000 | Loss: 0.00003428
Iteration 137/1000 | Loss: 0.00003428
Iteration 138/1000 | Loss: 0.00003426
Iteration 139/1000 | Loss: 0.00003425
Iteration 140/1000 | Loss: 0.00003424
Iteration 141/1000 | Loss: 0.00003424
Iteration 142/1000 | Loss: 0.00003423
Iteration 143/1000 | Loss: 0.00003423
Iteration 144/1000 | Loss: 0.00003421
Iteration 145/1000 | Loss: 0.00003421
Iteration 146/1000 | Loss: 0.00003421
Iteration 147/1000 | Loss: 0.00003421
Iteration 148/1000 | Loss: 0.00003421
Iteration 149/1000 | Loss: 0.00003421
Iteration 150/1000 | Loss: 0.00003421
Iteration 151/1000 | Loss: 0.00003421
Iteration 152/1000 | Loss: 0.00003421
Iteration 153/1000 | Loss: 0.00003421
Iteration 154/1000 | Loss: 0.00003421
Iteration 155/1000 | Loss: 0.00003421
Iteration 156/1000 | Loss: 0.00003421
Iteration 157/1000 | Loss: 0.00003420
Iteration 158/1000 | Loss: 0.00003420
Iteration 159/1000 | Loss: 0.00003420
Iteration 160/1000 | Loss: 0.00003420
Iteration 161/1000 | Loss: 0.00003420
Iteration 162/1000 | Loss: 0.00003420
Iteration 163/1000 | Loss: 0.00003420
Iteration 164/1000 | Loss: 0.00003420
Iteration 165/1000 | Loss: 0.00003420
Iteration 166/1000 | Loss: 0.00003419
Iteration 167/1000 | Loss: 0.00003419
Iteration 168/1000 | Loss: 0.00003419
Iteration 169/1000 | Loss: 0.00003418
Iteration 170/1000 | Loss: 0.00003418
Iteration 171/1000 | Loss: 0.00003418
Iteration 172/1000 | Loss: 0.00003418
Iteration 173/1000 | Loss: 0.00003417
Iteration 174/1000 | Loss: 0.00003417
Iteration 175/1000 | Loss: 0.00003417
Iteration 176/1000 | Loss: 0.00003417
Iteration 177/1000 | Loss: 0.00003417
Iteration 178/1000 | Loss: 0.00003417
Iteration 179/1000 | Loss: 0.00003417
Iteration 180/1000 | Loss: 0.00003417
Iteration 181/1000 | Loss: 0.00003416
Iteration 182/1000 | Loss: 0.00003416
Iteration 183/1000 | Loss: 0.00003416
Iteration 184/1000 | Loss: 0.00003416
Iteration 185/1000 | Loss: 0.00003416
Iteration 186/1000 | Loss: 0.00003415
Iteration 187/1000 | Loss: 0.00003415
Iteration 188/1000 | Loss: 0.00003415
Iteration 189/1000 | Loss: 0.00003415
Iteration 190/1000 | Loss: 0.00003415
Iteration 191/1000 | Loss: 0.00003415
Iteration 192/1000 | Loss: 0.00003415
Iteration 193/1000 | Loss: 0.00003414
Iteration 194/1000 | Loss: 0.00003414
Iteration 195/1000 | Loss: 0.00003414
Iteration 196/1000 | Loss: 0.00003414
Iteration 197/1000 | Loss: 0.00003414
Iteration 198/1000 | Loss: 0.00003414
Iteration 199/1000 | Loss: 0.00003414
Iteration 200/1000 | Loss: 0.00003414
Iteration 201/1000 | Loss: 0.00003414
Iteration 202/1000 | Loss: 0.00003413
Iteration 203/1000 | Loss: 0.00003413
Iteration 204/1000 | Loss: 0.00003413
Iteration 205/1000 | Loss: 0.00003413
Iteration 206/1000 | Loss: 0.00003413
Iteration 207/1000 | Loss: 0.00003413
Iteration 208/1000 | Loss: 0.00003413
Iteration 209/1000 | Loss: 0.00003412
Iteration 210/1000 | Loss: 0.00003412
Iteration 211/1000 | Loss: 0.00003412
Iteration 212/1000 | Loss: 0.00003412
Iteration 213/1000 | Loss: 0.00003412
Iteration 214/1000 | Loss: 0.00003412
Iteration 215/1000 | Loss: 0.00003412
Iteration 216/1000 | Loss: 0.00003412
Iteration 217/1000 | Loss: 0.00003412
Iteration 218/1000 | Loss: 0.00003412
Iteration 219/1000 | Loss: 0.00003412
Iteration 220/1000 | Loss: 0.00003412
Iteration 221/1000 | Loss: 0.00003412
Iteration 222/1000 | Loss: 0.00003412
Iteration 223/1000 | Loss: 0.00003412
Iteration 224/1000 | Loss: 0.00003412
Iteration 225/1000 | Loss: 0.00003412
Iteration 226/1000 | Loss: 0.00003412
Iteration 227/1000 | Loss: 0.00003411
Iteration 228/1000 | Loss: 0.00003411
Iteration 229/1000 | Loss: 0.00003411
Iteration 230/1000 | Loss: 0.00003411
Iteration 231/1000 | Loss: 0.00003411
Iteration 232/1000 | Loss: 0.00003411
Iteration 233/1000 | Loss: 0.00003411
Iteration 234/1000 | Loss: 0.00003411
Iteration 235/1000 | Loss: 0.00003411
Iteration 236/1000 | Loss: 0.00003411
Iteration 237/1000 | Loss: 0.00003411
Iteration 238/1000 | Loss: 0.00003411
Iteration 239/1000 | Loss: 0.00003411
Iteration 240/1000 | Loss: 0.00003411
Iteration 241/1000 | Loss: 0.00003411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [3.41133818437811e-05, 3.41133818437811e-05, 3.41133818437811e-05, 3.41133818437811e-05, 3.41133818437811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.41133818437811e-05

Optimization complete. Final v2v error: 4.421596527099609 mm

Highest mean error: 14.179543495178223 mm for frame 97

Lowest mean error: 3.759376287460327 mm for frame 3

Saving results

Total time: 214.3524079322815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460369
Iteration 2/25 | Loss: 0.00155874
Iteration 3/25 | Loss: 0.00144484
Iteration 4/25 | Loss: 0.00143269
Iteration 5/25 | Loss: 0.00142462
Iteration 6/25 | Loss: 0.00142442
Iteration 7/25 | Loss: 0.00142442
Iteration 8/25 | Loss: 0.00142442
Iteration 9/25 | Loss: 0.00142442
Iteration 10/25 | Loss: 0.00142442
Iteration 11/25 | Loss: 0.00142442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014244163176044822, 0.0014244163176044822, 0.0014244163176044822, 0.0014244163176044822, 0.0014244163176044822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014244163176044822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76788116
Iteration 2/25 | Loss: 0.00162412
Iteration 3/25 | Loss: 0.00162412
Iteration 4/25 | Loss: 0.00162412
Iteration 5/25 | Loss: 0.00162412
Iteration 6/25 | Loss: 0.00162412
Iteration 7/25 | Loss: 0.00162412
Iteration 8/25 | Loss: 0.00162412
Iteration 9/25 | Loss: 0.00162412
Iteration 10/25 | Loss: 0.00162412
Iteration 11/25 | Loss: 0.00162412
Iteration 12/25 | Loss: 0.00162412
Iteration 13/25 | Loss: 0.00162412
Iteration 14/25 | Loss: 0.00162412
Iteration 15/25 | Loss: 0.00162412
Iteration 16/25 | Loss: 0.00162412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016241221455857158, 0.0016241221455857158, 0.0016241221455857158, 0.0016241221455857158, 0.0016241221455857158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016241221455857158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162412
Iteration 2/1000 | Loss: 0.00004468
Iteration 3/1000 | Loss: 0.00003307
Iteration 4/1000 | Loss: 0.00002997
Iteration 5/1000 | Loss: 0.00002855
Iteration 6/1000 | Loss: 0.00002704
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002597
Iteration 9/1000 | Loss: 0.00002565
Iteration 10/1000 | Loss: 0.00002537
Iteration 11/1000 | Loss: 0.00002511
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002488
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00002483
Iteration 16/1000 | Loss: 0.00002482
Iteration 17/1000 | Loss: 0.00002478
Iteration 18/1000 | Loss: 0.00002478
Iteration 19/1000 | Loss: 0.00002478
Iteration 20/1000 | Loss: 0.00002478
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00002478
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002477
Iteration 28/1000 | Loss: 0.00002474
Iteration 29/1000 | Loss: 0.00002474
Iteration 30/1000 | Loss: 0.00002474
Iteration 31/1000 | Loss: 0.00002474
Iteration 32/1000 | Loss: 0.00002474
Iteration 33/1000 | Loss: 0.00002474
Iteration 34/1000 | Loss: 0.00002474
Iteration 35/1000 | Loss: 0.00002473
Iteration 36/1000 | Loss: 0.00002473
Iteration 37/1000 | Loss: 0.00002473
Iteration 38/1000 | Loss: 0.00002472
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002472
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002471
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002470
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00002470
Iteration 51/1000 | Loss: 0.00002469
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002468
Iteration 54/1000 | Loss: 0.00002468
Iteration 55/1000 | Loss: 0.00002468
Iteration 56/1000 | Loss: 0.00002468
Iteration 57/1000 | Loss: 0.00002468
Iteration 58/1000 | Loss: 0.00002468
Iteration 59/1000 | Loss: 0.00002468
Iteration 60/1000 | Loss: 0.00002467
Iteration 61/1000 | Loss: 0.00002467
Iteration 62/1000 | Loss: 0.00002467
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00002467
Iteration 65/1000 | Loss: 0.00002467
Iteration 66/1000 | Loss: 0.00002467
Iteration 67/1000 | Loss: 0.00002466
Iteration 68/1000 | Loss: 0.00002466
Iteration 69/1000 | Loss: 0.00002466
Iteration 70/1000 | Loss: 0.00002465
Iteration 71/1000 | Loss: 0.00002465
Iteration 72/1000 | Loss: 0.00002465
Iteration 73/1000 | Loss: 0.00002465
Iteration 74/1000 | Loss: 0.00002465
Iteration 75/1000 | Loss: 0.00002465
Iteration 76/1000 | Loss: 0.00002465
Iteration 77/1000 | Loss: 0.00002464
Iteration 78/1000 | Loss: 0.00002464
Iteration 79/1000 | Loss: 0.00002464
Iteration 80/1000 | Loss: 0.00002464
Iteration 81/1000 | Loss: 0.00002464
Iteration 82/1000 | Loss: 0.00002464
Iteration 83/1000 | Loss: 0.00002464
Iteration 84/1000 | Loss: 0.00002463
Iteration 85/1000 | Loss: 0.00002463
Iteration 86/1000 | Loss: 0.00002463
Iteration 87/1000 | Loss: 0.00002463
Iteration 88/1000 | Loss: 0.00002463
Iteration 89/1000 | Loss: 0.00002463
Iteration 90/1000 | Loss: 0.00002463
Iteration 91/1000 | Loss: 0.00002463
Iteration 92/1000 | Loss: 0.00002463
Iteration 93/1000 | Loss: 0.00002463
Iteration 94/1000 | Loss: 0.00002463
Iteration 95/1000 | Loss: 0.00002462
Iteration 96/1000 | Loss: 0.00002462
Iteration 97/1000 | Loss: 0.00002462
Iteration 98/1000 | Loss: 0.00002462
Iteration 99/1000 | Loss: 0.00002462
Iteration 100/1000 | Loss: 0.00002462
Iteration 101/1000 | Loss: 0.00002462
Iteration 102/1000 | Loss: 0.00002462
Iteration 103/1000 | Loss: 0.00002462
Iteration 104/1000 | Loss: 0.00002462
Iteration 105/1000 | Loss: 0.00002462
Iteration 106/1000 | Loss: 0.00002462
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002462
Iteration 110/1000 | Loss: 0.00002462
Iteration 111/1000 | Loss: 0.00002462
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002462
Iteration 118/1000 | Loss: 0.00002462
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002462
Iteration 123/1000 | Loss: 0.00002462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.4620898329885677e-05, 2.4620898329885677e-05, 2.4620898329885677e-05, 2.4620898329885677e-05, 2.4620898329885677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4620898329885677e-05

Optimization complete. Final v2v error: 4.3878936767578125 mm

Highest mean error: 4.63522481918335 mm for frame 8

Lowest mean error: 4.150557994842529 mm for frame 90

Saving results

Total time: 38.40140962600708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131548
Iteration 2/25 | Loss: 0.00367750
Iteration 3/25 | Loss: 0.00247722
Iteration 4/25 | Loss: 0.00223549
Iteration 5/25 | Loss: 0.00222315
Iteration 6/25 | Loss: 0.00207242
Iteration 7/25 | Loss: 0.00205097
Iteration 8/25 | Loss: 0.00199649
Iteration 9/25 | Loss: 0.00194666
Iteration 10/25 | Loss: 0.00192597
Iteration 11/25 | Loss: 0.00192681
Iteration 12/25 | Loss: 0.00191951
Iteration 13/25 | Loss: 0.00190833
Iteration 14/25 | Loss: 0.00190262
Iteration 15/25 | Loss: 0.00190285
Iteration 16/25 | Loss: 0.00189640
Iteration 17/25 | Loss: 0.00187779
Iteration 18/25 | Loss: 0.00187759
Iteration 19/25 | Loss: 0.00188623
Iteration 20/25 | Loss: 0.00188719
Iteration 21/25 | Loss: 0.00187481
Iteration 22/25 | Loss: 0.00182761
Iteration 23/25 | Loss: 0.00182875
Iteration 24/25 | Loss: 0.00181699
Iteration 25/25 | Loss: 0.00180099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62625825
Iteration 2/25 | Loss: 0.01409198
Iteration 3/25 | Loss: 0.00541747
Iteration 4/25 | Loss: 0.00541739
Iteration 5/25 | Loss: 0.00541739
Iteration 6/25 | Loss: 0.00541739
Iteration 7/25 | Loss: 0.00541738
Iteration 8/25 | Loss: 0.00541738
Iteration 9/25 | Loss: 0.00541738
Iteration 10/25 | Loss: 0.00541738
Iteration 11/25 | Loss: 0.00541738
Iteration 12/25 | Loss: 0.00541738
Iteration 13/25 | Loss: 0.00541738
Iteration 14/25 | Loss: 0.00541738
Iteration 15/25 | Loss: 0.00541738
Iteration 16/25 | Loss: 0.00541738
Iteration 17/25 | Loss: 0.00541738
Iteration 18/25 | Loss: 0.00541738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005417381878942251, 0.005417381878942251, 0.005417381878942251, 0.005417381878942251, 0.005417381878942251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005417381878942251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00541738
Iteration 2/1000 | Loss: 0.00833188
Iteration 3/1000 | Loss: 0.00278105
Iteration 4/1000 | Loss: 0.00100044
Iteration 5/1000 | Loss: 0.00112209
Iteration 6/1000 | Loss: 0.00100210
Iteration 7/1000 | Loss: 0.00038458
Iteration 8/1000 | Loss: 0.00085541
Iteration 9/1000 | Loss: 0.00081324
Iteration 10/1000 | Loss: 0.00146412
Iteration 11/1000 | Loss: 0.00021785
Iteration 12/1000 | Loss: 0.00039685
Iteration 13/1000 | Loss: 0.00031797
Iteration 14/1000 | Loss: 0.00089806
Iteration 15/1000 | Loss: 0.00057409
Iteration 16/1000 | Loss: 0.00016396
Iteration 17/1000 | Loss: 0.00016175
Iteration 18/1000 | Loss: 0.00584918
Iteration 19/1000 | Loss: 0.00313282
Iteration 20/1000 | Loss: 0.00389095
Iteration 21/1000 | Loss: 0.00038361
Iteration 22/1000 | Loss: 0.00857810
Iteration 23/1000 | Loss: 0.00224946
Iteration 24/1000 | Loss: 0.00823331
Iteration 25/1000 | Loss: 0.00475950
Iteration 26/1000 | Loss: 0.00039816
Iteration 27/1000 | Loss: 0.00261022
Iteration 28/1000 | Loss: 0.00017791
Iteration 29/1000 | Loss: 0.00069735
Iteration 30/1000 | Loss: 0.00161905
Iteration 31/1000 | Loss: 0.00078759
Iteration 32/1000 | Loss: 0.00155362
Iteration 33/1000 | Loss: 0.00192172
Iteration 34/1000 | Loss: 0.00099123
Iteration 35/1000 | Loss: 0.00028735
Iteration 36/1000 | Loss: 0.00083571
Iteration 37/1000 | Loss: 0.00053975
Iteration 38/1000 | Loss: 0.00046735
Iteration 39/1000 | Loss: 0.00135556
Iteration 40/1000 | Loss: 0.00121523
Iteration 41/1000 | Loss: 0.00097968
Iteration 42/1000 | Loss: 0.00022188
Iteration 43/1000 | Loss: 0.00021678
Iteration 44/1000 | Loss: 0.00125815
Iteration 45/1000 | Loss: 0.00049244
Iteration 46/1000 | Loss: 0.00123392
Iteration 47/1000 | Loss: 0.00070266
Iteration 48/1000 | Loss: 0.00058080
Iteration 49/1000 | Loss: 0.00019464
Iteration 50/1000 | Loss: 0.00010275
Iteration 51/1000 | Loss: 0.00044057
Iteration 52/1000 | Loss: 0.00170568
Iteration 53/1000 | Loss: 0.00113439
Iteration 54/1000 | Loss: 0.00038651
Iteration 55/1000 | Loss: 0.00035384
Iteration 56/1000 | Loss: 0.00011313
Iteration 57/1000 | Loss: 0.00051267
Iteration 58/1000 | Loss: 0.00015971
Iteration 59/1000 | Loss: 0.00120323
Iteration 60/1000 | Loss: 0.00117442
Iteration 61/1000 | Loss: 0.00040899
Iteration 62/1000 | Loss: 0.00017615
Iteration 63/1000 | Loss: 0.00008746
Iteration 64/1000 | Loss: 0.00029351
Iteration 65/1000 | Loss: 0.00008325
Iteration 66/1000 | Loss: 0.00053922
Iteration 67/1000 | Loss: 0.00007948
Iteration 68/1000 | Loss: 0.00102900
Iteration 69/1000 | Loss: 0.00192790
Iteration 70/1000 | Loss: 0.00049064
Iteration 71/1000 | Loss: 0.00029497
Iteration 72/1000 | Loss: 0.00102143
Iteration 73/1000 | Loss: 0.00095510
Iteration 74/1000 | Loss: 0.00010489
Iteration 75/1000 | Loss: 0.00007879
Iteration 76/1000 | Loss: 0.00013291
Iteration 77/1000 | Loss: 0.00022484
Iteration 78/1000 | Loss: 0.00006529
Iteration 79/1000 | Loss: 0.00080630
Iteration 80/1000 | Loss: 0.00015756
Iteration 81/1000 | Loss: 0.00007227
Iteration 82/1000 | Loss: 0.00006853
Iteration 83/1000 | Loss: 0.00023642
Iteration 84/1000 | Loss: 0.00070726
Iteration 85/1000 | Loss: 0.00008737
Iteration 86/1000 | Loss: 0.00006698
Iteration 87/1000 | Loss: 0.00006394
Iteration 88/1000 | Loss: 0.00067971
Iteration 89/1000 | Loss: 0.00064548
Iteration 90/1000 | Loss: 0.00025446
Iteration 91/1000 | Loss: 0.00005901
Iteration 92/1000 | Loss: 0.00005538
Iteration 93/1000 | Loss: 0.00005300
Iteration 94/1000 | Loss: 0.00019165
Iteration 95/1000 | Loss: 0.00006484
Iteration 96/1000 | Loss: 0.00025444
Iteration 97/1000 | Loss: 0.00035500
Iteration 98/1000 | Loss: 0.00048423
Iteration 99/1000 | Loss: 0.00029730
Iteration 100/1000 | Loss: 0.00021676
Iteration 101/1000 | Loss: 0.00030917
Iteration 102/1000 | Loss: 0.00007665
Iteration 103/1000 | Loss: 0.00006378
Iteration 104/1000 | Loss: 0.00005064
Iteration 105/1000 | Loss: 0.00025474
Iteration 106/1000 | Loss: 0.00043202
Iteration 107/1000 | Loss: 0.00018599
Iteration 108/1000 | Loss: 0.00020251
Iteration 109/1000 | Loss: 0.00036087
Iteration 110/1000 | Loss: 0.00044966
Iteration 111/1000 | Loss: 0.00009386
Iteration 112/1000 | Loss: 0.00006217
Iteration 113/1000 | Loss: 0.00005219
Iteration 114/1000 | Loss: 0.00004982
Iteration 115/1000 | Loss: 0.00004805
Iteration 116/1000 | Loss: 0.00005364
Iteration 117/1000 | Loss: 0.00004909
Iteration 118/1000 | Loss: 0.00004665
Iteration 119/1000 | Loss: 0.00004636
Iteration 120/1000 | Loss: 0.00004612
Iteration 121/1000 | Loss: 0.00004594
Iteration 122/1000 | Loss: 0.00004591
Iteration 123/1000 | Loss: 0.00004588
Iteration 124/1000 | Loss: 0.00004587
Iteration 125/1000 | Loss: 0.00004587
Iteration 126/1000 | Loss: 0.00004586
Iteration 127/1000 | Loss: 0.00004586
Iteration 128/1000 | Loss: 0.00004585
Iteration 129/1000 | Loss: 0.00004582
Iteration 130/1000 | Loss: 0.00004582
Iteration 131/1000 | Loss: 0.00004581
Iteration 132/1000 | Loss: 0.00004578
Iteration 133/1000 | Loss: 0.00004577
Iteration 134/1000 | Loss: 0.00004574
Iteration 135/1000 | Loss: 0.00004569
Iteration 136/1000 | Loss: 0.00004569
Iteration 137/1000 | Loss: 0.00004569
Iteration 138/1000 | Loss: 0.00004569
Iteration 139/1000 | Loss: 0.00004568
Iteration 140/1000 | Loss: 0.00004568
Iteration 141/1000 | Loss: 0.00004564
Iteration 142/1000 | Loss: 0.00004564
Iteration 143/1000 | Loss: 0.00004564
Iteration 144/1000 | Loss: 0.00004564
Iteration 145/1000 | Loss: 0.00004563
Iteration 146/1000 | Loss: 0.00004563
Iteration 147/1000 | Loss: 0.00004563
Iteration 148/1000 | Loss: 0.00004563
Iteration 149/1000 | Loss: 0.00004563
Iteration 150/1000 | Loss: 0.00004563
Iteration 151/1000 | Loss: 0.00004562
Iteration 152/1000 | Loss: 0.00004562
Iteration 153/1000 | Loss: 0.00004561
Iteration 154/1000 | Loss: 0.00004561
Iteration 155/1000 | Loss: 0.00004561
Iteration 156/1000 | Loss: 0.00004561
Iteration 157/1000 | Loss: 0.00004561
Iteration 158/1000 | Loss: 0.00004561
Iteration 159/1000 | Loss: 0.00004561
Iteration 160/1000 | Loss: 0.00004561
Iteration 161/1000 | Loss: 0.00004561
Iteration 162/1000 | Loss: 0.00004560
Iteration 163/1000 | Loss: 0.00004560
Iteration 164/1000 | Loss: 0.00004560
Iteration 165/1000 | Loss: 0.00004559
Iteration 166/1000 | Loss: 0.00004559
Iteration 167/1000 | Loss: 0.00004559
Iteration 168/1000 | Loss: 0.00004558
Iteration 169/1000 | Loss: 0.00004558
Iteration 170/1000 | Loss: 0.00004557
Iteration 171/1000 | Loss: 0.00004557
Iteration 172/1000 | Loss: 0.00004557
Iteration 173/1000 | Loss: 0.00004556
Iteration 174/1000 | Loss: 0.00004556
Iteration 175/1000 | Loss: 0.00004554
Iteration 176/1000 | Loss: 0.00004554
Iteration 177/1000 | Loss: 0.00004554
Iteration 178/1000 | Loss: 0.00004553
Iteration 179/1000 | Loss: 0.00004553
Iteration 180/1000 | Loss: 0.00004553
Iteration 181/1000 | Loss: 0.00004553
Iteration 182/1000 | Loss: 0.00004552
Iteration 183/1000 | Loss: 0.00004552
Iteration 184/1000 | Loss: 0.00004552
Iteration 185/1000 | Loss: 0.00004552
Iteration 186/1000 | Loss: 0.00004551
Iteration 187/1000 | Loss: 0.00004551
Iteration 188/1000 | Loss: 0.00004551
Iteration 189/1000 | Loss: 0.00004551
Iteration 190/1000 | Loss: 0.00004551
Iteration 191/1000 | Loss: 0.00004550
Iteration 192/1000 | Loss: 0.00004550
Iteration 193/1000 | Loss: 0.00004550
Iteration 194/1000 | Loss: 0.00004550
Iteration 195/1000 | Loss: 0.00004549
Iteration 196/1000 | Loss: 0.00004549
Iteration 197/1000 | Loss: 0.00004549
Iteration 198/1000 | Loss: 0.00004549
Iteration 199/1000 | Loss: 0.00004549
Iteration 200/1000 | Loss: 0.00004549
Iteration 201/1000 | Loss: 0.00004549
Iteration 202/1000 | Loss: 0.00004549
Iteration 203/1000 | Loss: 0.00004549
Iteration 204/1000 | Loss: 0.00004549
Iteration 205/1000 | Loss: 0.00004549
Iteration 206/1000 | Loss: 0.00004549
Iteration 207/1000 | Loss: 0.00004549
Iteration 208/1000 | Loss: 0.00004548
Iteration 209/1000 | Loss: 0.00004548
Iteration 210/1000 | Loss: 0.00004548
Iteration 211/1000 | Loss: 0.00004548
Iteration 212/1000 | Loss: 0.00004548
Iteration 213/1000 | Loss: 0.00004548
Iteration 214/1000 | Loss: 0.00004548
Iteration 215/1000 | Loss: 0.00004548
Iteration 216/1000 | Loss: 0.00004548
Iteration 217/1000 | Loss: 0.00004547
Iteration 218/1000 | Loss: 0.00004547
Iteration 219/1000 | Loss: 0.00004547
Iteration 220/1000 | Loss: 0.00004547
Iteration 221/1000 | Loss: 0.00004547
Iteration 222/1000 | Loss: 0.00004547
Iteration 223/1000 | Loss: 0.00004547
Iteration 224/1000 | Loss: 0.00004547
Iteration 225/1000 | Loss: 0.00004547
Iteration 226/1000 | Loss: 0.00004547
Iteration 227/1000 | Loss: 0.00004547
Iteration 228/1000 | Loss: 0.00004547
Iteration 229/1000 | Loss: 0.00004547
Iteration 230/1000 | Loss: 0.00004546
Iteration 231/1000 | Loss: 0.00004546
Iteration 232/1000 | Loss: 0.00004546
Iteration 233/1000 | Loss: 0.00004546
Iteration 234/1000 | Loss: 0.00004546
Iteration 235/1000 | Loss: 0.00004546
Iteration 236/1000 | Loss: 0.00004546
Iteration 237/1000 | Loss: 0.00004546
Iteration 238/1000 | Loss: 0.00004546
Iteration 239/1000 | Loss: 0.00004546
Iteration 240/1000 | Loss: 0.00004546
Iteration 241/1000 | Loss: 0.00004546
Iteration 242/1000 | Loss: 0.00004546
Iteration 243/1000 | Loss: 0.00004545
Iteration 244/1000 | Loss: 0.00004545
Iteration 245/1000 | Loss: 0.00004545
Iteration 246/1000 | Loss: 0.00004545
Iteration 247/1000 | Loss: 0.00004545
Iteration 248/1000 | Loss: 0.00004545
Iteration 249/1000 | Loss: 0.00004545
Iteration 250/1000 | Loss: 0.00004545
Iteration 251/1000 | Loss: 0.00004545
Iteration 252/1000 | Loss: 0.00004545
Iteration 253/1000 | Loss: 0.00004545
Iteration 254/1000 | Loss: 0.00004545
Iteration 255/1000 | Loss: 0.00004545
Iteration 256/1000 | Loss: 0.00004545
Iteration 257/1000 | Loss: 0.00004545
Iteration 258/1000 | Loss: 0.00004545
Iteration 259/1000 | Loss: 0.00004545
Iteration 260/1000 | Loss: 0.00004545
Iteration 261/1000 | Loss: 0.00004545
Iteration 262/1000 | Loss: 0.00004545
Iteration 263/1000 | Loss: 0.00004545
Iteration 264/1000 | Loss: 0.00004545
Iteration 265/1000 | Loss: 0.00004545
Iteration 266/1000 | Loss: 0.00004544
Iteration 267/1000 | Loss: 0.00004544
Iteration 268/1000 | Loss: 0.00004544
Iteration 269/1000 | Loss: 0.00004544
Iteration 270/1000 | Loss: 0.00004544
Iteration 271/1000 | Loss: 0.00004544
Iteration 272/1000 | Loss: 0.00004544
Iteration 273/1000 | Loss: 0.00004544
Iteration 274/1000 | Loss: 0.00004544
Iteration 275/1000 | Loss: 0.00004544
Iteration 276/1000 | Loss: 0.00004544
Iteration 277/1000 | Loss: 0.00004544
Iteration 278/1000 | Loss: 0.00004544
Iteration 279/1000 | Loss: 0.00004544
Iteration 280/1000 | Loss: 0.00004544
Iteration 281/1000 | Loss: 0.00004544
Iteration 282/1000 | Loss: 0.00004544
Iteration 283/1000 | Loss: 0.00004544
Iteration 284/1000 | Loss: 0.00004544
Iteration 285/1000 | Loss: 0.00004544
Iteration 286/1000 | Loss: 0.00004544
Iteration 287/1000 | Loss: 0.00004544
Iteration 288/1000 | Loss: 0.00004544
Iteration 289/1000 | Loss: 0.00004544
Iteration 290/1000 | Loss: 0.00004544
Iteration 291/1000 | Loss: 0.00004544
Iteration 292/1000 | Loss: 0.00004544
Iteration 293/1000 | Loss: 0.00004544
Iteration 294/1000 | Loss: 0.00004544
Iteration 295/1000 | Loss: 0.00004544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [4.543629620457068e-05, 4.543629620457068e-05, 4.543629620457068e-05, 4.543629620457068e-05, 4.543629620457068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.543629620457068e-05

Optimization complete. Final v2v error: 5.47587251663208 mm

Highest mean error: 14.523005485534668 mm for frame 48

Lowest mean error: 4.1727705001831055 mm for frame 108

Saving results

Total time: 263.9526584148407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450247
Iteration 2/25 | Loss: 0.00148839
Iteration 3/25 | Loss: 0.00138821
Iteration 4/25 | Loss: 0.00137770
Iteration 5/25 | Loss: 0.00137515
Iteration 6/25 | Loss: 0.00137507
Iteration 7/25 | Loss: 0.00137507
Iteration 8/25 | Loss: 0.00137507
Iteration 9/25 | Loss: 0.00137507
Iteration 10/25 | Loss: 0.00137507
Iteration 11/25 | Loss: 0.00137507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001375070190988481, 0.001375070190988481, 0.001375070190988481, 0.001375070190988481, 0.001375070190988481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001375070190988481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49112427
Iteration 2/25 | Loss: 0.00125630
Iteration 3/25 | Loss: 0.00125630
Iteration 4/25 | Loss: 0.00125630
Iteration 5/25 | Loss: 0.00125630
Iteration 6/25 | Loss: 0.00125630
Iteration 7/25 | Loss: 0.00125630
Iteration 8/25 | Loss: 0.00125630
Iteration 9/25 | Loss: 0.00125630
Iteration 10/25 | Loss: 0.00125630
Iteration 11/25 | Loss: 0.00125630
Iteration 12/25 | Loss: 0.00125630
Iteration 13/25 | Loss: 0.00125630
Iteration 14/25 | Loss: 0.00125630
Iteration 15/25 | Loss: 0.00125630
Iteration 16/25 | Loss: 0.00125630
Iteration 17/25 | Loss: 0.00125630
Iteration 18/25 | Loss: 0.00125630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012562981573864818, 0.0012562981573864818, 0.0012562981573864818, 0.0012562981573864818, 0.0012562981573864818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012562981573864818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125630
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002804
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002476
Iteration 6/1000 | Loss: 0.00002413
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002352
Iteration 9/1000 | Loss: 0.00002352
Iteration 10/1000 | Loss: 0.00002342
Iteration 11/1000 | Loss: 0.00002342
Iteration 12/1000 | Loss: 0.00002333
Iteration 13/1000 | Loss: 0.00002330
Iteration 14/1000 | Loss: 0.00002329
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002322
Iteration 18/1000 | Loss: 0.00002322
Iteration 19/1000 | Loss: 0.00002322
Iteration 20/1000 | Loss: 0.00002319
Iteration 21/1000 | Loss: 0.00002319
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002316
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002314
Iteration 26/1000 | Loss: 0.00002313
Iteration 27/1000 | Loss: 0.00002313
Iteration 28/1000 | Loss: 0.00002313
Iteration 29/1000 | Loss: 0.00002313
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002312
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002311
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002308
Iteration 41/1000 | Loss: 0.00002308
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00002307
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002306
Iteration 48/1000 | Loss: 0.00002306
Iteration 49/1000 | Loss: 0.00002306
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002305
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002305
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002304
Iteration 61/1000 | Loss: 0.00002304
Iteration 62/1000 | Loss: 0.00002304
Iteration 63/1000 | Loss: 0.00002304
Iteration 64/1000 | Loss: 0.00002304
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002303
Iteration 67/1000 | Loss: 0.00002303
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002303
Iteration 70/1000 | Loss: 0.00002303
Iteration 71/1000 | Loss: 0.00002303
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002302
Iteration 76/1000 | Loss: 0.00002302
Iteration 77/1000 | Loss: 0.00002302
Iteration 78/1000 | Loss: 0.00002302
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002301
Iteration 82/1000 | Loss: 0.00002301
Iteration 83/1000 | Loss: 0.00002301
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002301
Iteration 86/1000 | Loss: 0.00002301
Iteration 87/1000 | Loss: 0.00002300
Iteration 88/1000 | Loss: 0.00002300
Iteration 89/1000 | Loss: 0.00002300
Iteration 90/1000 | Loss: 0.00002300
Iteration 91/1000 | Loss: 0.00002300
Iteration 92/1000 | Loss: 0.00002300
Iteration 93/1000 | Loss: 0.00002300
Iteration 94/1000 | Loss: 0.00002300
Iteration 95/1000 | Loss: 0.00002300
Iteration 96/1000 | Loss: 0.00002300
Iteration 97/1000 | Loss: 0.00002300
Iteration 98/1000 | Loss: 0.00002300
Iteration 99/1000 | Loss: 0.00002300
Iteration 100/1000 | Loss: 0.00002300
Iteration 101/1000 | Loss: 0.00002300
Iteration 102/1000 | Loss: 0.00002300
Iteration 103/1000 | Loss: 0.00002300
Iteration 104/1000 | Loss: 0.00002300
Iteration 105/1000 | Loss: 0.00002300
Iteration 106/1000 | Loss: 0.00002300
Iteration 107/1000 | Loss: 0.00002300
Iteration 108/1000 | Loss: 0.00002300
Iteration 109/1000 | Loss: 0.00002300
Iteration 110/1000 | Loss: 0.00002300
Iteration 111/1000 | Loss: 0.00002300
Iteration 112/1000 | Loss: 0.00002300
Iteration 113/1000 | Loss: 0.00002300
Iteration 114/1000 | Loss: 0.00002300
Iteration 115/1000 | Loss: 0.00002300
Iteration 116/1000 | Loss: 0.00002300
Iteration 117/1000 | Loss: 0.00002300
Iteration 118/1000 | Loss: 0.00002300
Iteration 119/1000 | Loss: 0.00002300
Iteration 120/1000 | Loss: 0.00002300
Iteration 121/1000 | Loss: 0.00002300
Iteration 122/1000 | Loss: 0.00002300
Iteration 123/1000 | Loss: 0.00002300
Iteration 124/1000 | Loss: 0.00002300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.2997042833594605e-05, 2.2997042833594605e-05, 2.2997042833594605e-05, 2.2997042833594605e-05, 2.2997042833594605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2997042833594605e-05

Optimization complete. Final v2v error: 4.15878438949585 mm

Highest mean error: 4.517463684082031 mm for frame 113

Lowest mean error: 3.8226404190063477 mm for frame 0

Saving results

Total time: 31.360960960388184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890298
Iteration 2/25 | Loss: 0.00205538
Iteration 3/25 | Loss: 0.00165452
Iteration 4/25 | Loss: 0.00155987
Iteration 5/25 | Loss: 0.00155583
Iteration 6/25 | Loss: 0.00153250
Iteration 7/25 | Loss: 0.00154015
Iteration 8/25 | Loss: 0.00147610
Iteration 9/25 | Loss: 0.00145723
Iteration 10/25 | Loss: 0.00145010
Iteration 11/25 | Loss: 0.00143236
Iteration 12/25 | Loss: 0.00143048
Iteration 13/25 | Loss: 0.00142844
Iteration 14/25 | Loss: 0.00142466
Iteration 15/25 | Loss: 0.00141547
Iteration 16/25 | Loss: 0.00141478
Iteration 17/25 | Loss: 0.00141500
Iteration 18/25 | Loss: 0.00142252
Iteration 19/25 | Loss: 0.00141457
Iteration 20/25 | Loss: 0.00141161
Iteration 21/25 | Loss: 0.00141841
Iteration 22/25 | Loss: 0.00141552
Iteration 23/25 | Loss: 0.00141368
Iteration 24/25 | Loss: 0.00141459
Iteration 25/25 | Loss: 0.00141610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16004109
Iteration 2/25 | Loss: 0.00134330
Iteration 3/25 | Loss: 0.00134329
Iteration 4/25 | Loss: 0.00134329
Iteration 5/25 | Loss: 0.00134329
Iteration 6/25 | Loss: 0.00134329
Iteration 7/25 | Loss: 0.00134329
Iteration 8/25 | Loss: 0.00134329
Iteration 9/25 | Loss: 0.00134329
Iteration 10/25 | Loss: 0.00134329
Iteration 11/25 | Loss: 0.00134329
Iteration 12/25 | Loss: 0.00134329
Iteration 13/25 | Loss: 0.00134329
Iteration 14/25 | Loss: 0.00134329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013432912528514862, 0.0013432912528514862, 0.0013432912528514862, 0.0013432912528514862, 0.0013432912528514862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013432912528514862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134329
Iteration 2/1000 | Loss: 0.00005450
Iteration 3/1000 | Loss: 0.00015471
Iteration 4/1000 | Loss: 0.00018864
Iteration 5/1000 | Loss: 0.00030744
Iteration 6/1000 | Loss: 0.00027548
Iteration 7/1000 | Loss: 0.00004423
Iteration 8/1000 | Loss: 0.00027489
Iteration 9/1000 | Loss: 0.00004087
Iteration 10/1000 | Loss: 0.00003395
Iteration 11/1000 | Loss: 0.00004422
Iteration 12/1000 | Loss: 0.00006913
Iteration 13/1000 | Loss: 0.00003076
Iteration 14/1000 | Loss: 0.00002984
Iteration 15/1000 | Loss: 0.00016589
Iteration 16/1000 | Loss: 0.00002900
Iteration 17/1000 | Loss: 0.00002819
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002763
Iteration 20/1000 | Loss: 0.00002737
Iteration 21/1000 | Loss: 0.00002716
Iteration 22/1000 | Loss: 0.00002716
Iteration 23/1000 | Loss: 0.00002715
Iteration 24/1000 | Loss: 0.00002714
Iteration 25/1000 | Loss: 0.00002713
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002709
Iteration 28/1000 | Loss: 0.00002708
Iteration 29/1000 | Loss: 0.00002707
Iteration 30/1000 | Loss: 0.00002707
Iteration 31/1000 | Loss: 0.00002707
Iteration 32/1000 | Loss: 0.00002705
Iteration 33/1000 | Loss: 0.00002704
Iteration 34/1000 | Loss: 0.00002704
Iteration 35/1000 | Loss: 0.00002704
Iteration 36/1000 | Loss: 0.00002703
Iteration 37/1000 | Loss: 0.00002703
Iteration 38/1000 | Loss: 0.00002703
Iteration 39/1000 | Loss: 0.00002702
Iteration 40/1000 | Loss: 0.00002702
Iteration 41/1000 | Loss: 0.00002702
Iteration 42/1000 | Loss: 0.00002702
Iteration 43/1000 | Loss: 0.00002702
Iteration 44/1000 | Loss: 0.00002701
Iteration 45/1000 | Loss: 0.00002701
Iteration 46/1000 | Loss: 0.00002701
Iteration 47/1000 | Loss: 0.00002701
Iteration 48/1000 | Loss: 0.00002701
Iteration 49/1000 | Loss: 0.00002701
Iteration 50/1000 | Loss: 0.00002701
Iteration 51/1000 | Loss: 0.00002700
Iteration 52/1000 | Loss: 0.00002700
Iteration 53/1000 | Loss: 0.00002700
Iteration 54/1000 | Loss: 0.00002700
Iteration 55/1000 | Loss: 0.00002699
Iteration 56/1000 | Loss: 0.00002699
Iteration 57/1000 | Loss: 0.00002699
Iteration 58/1000 | Loss: 0.00002698
Iteration 59/1000 | Loss: 0.00002698
Iteration 60/1000 | Loss: 0.00002698
Iteration 61/1000 | Loss: 0.00002697
Iteration 62/1000 | Loss: 0.00002697
Iteration 63/1000 | Loss: 0.00002697
Iteration 64/1000 | Loss: 0.00002696
Iteration 65/1000 | Loss: 0.00002696
Iteration 66/1000 | Loss: 0.00002696
Iteration 67/1000 | Loss: 0.00002696
Iteration 68/1000 | Loss: 0.00002696
Iteration 69/1000 | Loss: 0.00002696
Iteration 70/1000 | Loss: 0.00002696
Iteration 71/1000 | Loss: 0.00002696
Iteration 72/1000 | Loss: 0.00002696
Iteration 73/1000 | Loss: 0.00002695
Iteration 74/1000 | Loss: 0.00002695
Iteration 75/1000 | Loss: 0.00002695
Iteration 76/1000 | Loss: 0.00002695
Iteration 77/1000 | Loss: 0.00002695
Iteration 78/1000 | Loss: 0.00002695
Iteration 79/1000 | Loss: 0.00002695
Iteration 80/1000 | Loss: 0.00002695
Iteration 81/1000 | Loss: 0.00002695
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002695
Iteration 85/1000 | Loss: 0.00002695
Iteration 86/1000 | Loss: 0.00002695
Iteration 87/1000 | Loss: 0.00002694
Iteration 88/1000 | Loss: 0.00002694
Iteration 89/1000 | Loss: 0.00002694
Iteration 90/1000 | Loss: 0.00009042
Iteration 91/1000 | Loss: 0.00009042
Iteration 92/1000 | Loss: 0.00002799
Iteration 93/1000 | Loss: 0.00002705
Iteration 94/1000 | Loss: 0.00002694
Iteration 95/1000 | Loss: 0.00002692
Iteration 96/1000 | Loss: 0.00002692
Iteration 97/1000 | Loss: 0.00002692
Iteration 98/1000 | Loss: 0.00002692
Iteration 99/1000 | Loss: 0.00002692
Iteration 100/1000 | Loss: 0.00002692
Iteration 101/1000 | Loss: 0.00002691
Iteration 102/1000 | Loss: 0.00002691
Iteration 103/1000 | Loss: 0.00002691
Iteration 104/1000 | Loss: 0.00002691
Iteration 105/1000 | Loss: 0.00002691
Iteration 106/1000 | Loss: 0.00002691
Iteration 107/1000 | Loss: 0.00002690
Iteration 108/1000 | Loss: 0.00002690
Iteration 109/1000 | Loss: 0.00002690
Iteration 110/1000 | Loss: 0.00002690
Iteration 111/1000 | Loss: 0.00002690
Iteration 112/1000 | Loss: 0.00002690
Iteration 113/1000 | Loss: 0.00002690
Iteration 114/1000 | Loss: 0.00002690
Iteration 115/1000 | Loss: 0.00002690
Iteration 116/1000 | Loss: 0.00002690
Iteration 117/1000 | Loss: 0.00002689
Iteration 118/1000 | Loss: 0.00002689
Iteration 119/1000 | Loss: 0.00002689
Iteration 120/1000 | Loss: 0.00002689
Iteration 121/1000 | Loss: 0.00002689
Iteration 122/1000 | Loss: 0.00002689
Iteration 123/1000 | Loss: 0.00002689
Iteration 124/1000 | Loss: 0.00002689
Iteration 125/1000 | Loss: 0.00002689
Iteration 126/1000 | Loss: 0.00002689
Iteration 127/1000 | Loss: 0.00002689
Iteration 128/1000 | Loss: 0.00002689
Iteration 129/1000 | Loss: 0.00002689
Iteration 130/1000 | Loss: 0.00002689
Iteration 131/1000 | Loss: 0.00002689
Iteration 132/1000 | Loss: 0.00002689
Iteration 133/1000 | Loss: 0.00002689
Iteration 134/1000 | Loss: 0.00002689
Iteration 135/1000 | Loss: 0.00002689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.6887426429311745e-05, 2.6887426429311745e-05, 2.6887426429311745e-05, 2.6887426429311745e-05, 2.6887426429311745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6887426429311745e-05

Optimization complete. Final v2v error: 4.494216442108154 mm

Highest mean error: 5.023863315582275 mm for frame 162

Lowest mean error: 4.2171220779418945 mm for frame 58

Saving results

Total time: 88.90358233451843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041070
Iteration 2/25 | Loss: 0.00208422
Iteration 3/25 | Loss: 0.00159203
Iteration 4/25 | Loss: 0.00154949
Iteration 5/25 | Loss: 0.00153256
Iteration 6/25 | Loss: 0.00152844
Iteration 7/25 | Loss: 0.00152824
Iteration 8/25 | Loss: 0.00152824
Iteration 9/25 | Loss: 0.00152824
Iteration 10/25 | Loss: 0.00152824
Iteration 11/25 | Loss: 0.00152824
Iteration 12/25 | Loss: 0.00152824
Iteration 13/25 | Loss: 0.00152824
Iteration 14/25 | Loss: 0.00152824
Iteration 15/25 | Loss: 0.00152824
Iteration 16/25 | Loss: 0.00152824
Iteration 17/25 | Loss: 0.00152824
Iteration 18/25 | Loss: 0.00152824
Iteration 19/25 | Loss: 0.00152824
Iteration 20/25 | Loss: 0.00152824
Iteration 21/25 | Loss: 0.00152824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015282367821782827, 0.0015282367821782827, 0.0015282367821782827, 0.0015282367821782827, 0.0015282367821782827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015282367821782827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21172023
Iteration 2/25 | Loss: 0.00132303
Iteration 3/25 | Loss: 0.00132302
Iteration 4/25 | Loss: 0.00132302
Iteration 5/25 | Loss: 0.00132302
Iteration 6/25 | Loss: 0.00132302
Iteration 7/25 | Loss: 0.00132302
Iteration 8/25 | Loss: 0.00132302
Iteration 9/25 | Loss: 0.00132302
Iteration 10/25 | Loss: 0.00132302
Iteration 11/25 | Loss: 0.00132302
Iteration 12/25 | Loss: 0.00132302
Iteration 13/25 | Loss: 0.00132302
Iteration 14/25 | Loss: 0.00132302
Iteration 15/25 | Loss: 0.00132302
Iteration 16/25 | Loss: 0.00132302
Iteration 17/25 | Loss: 0.00132302
Iteration 18/25 | Loss: 0.00132302
Iteration 19/25 | Loss: 0.00132302
Iteration 20/25 | Loss: 0.00132302
Iteration 21/25 | Loss: 0.00132302
Iteration 22/25 | Loss: 0.00132302
Iteration 23/25 | Loss: 0.00132302
Iteration 24/25 | Loss: 0.00132302
Iteration 25/25 | Loss: 0.00132302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132302
Iteration 2/1000 | Loss: 0.00007481
Iteration 3/1000 | Loss: 0.00005785
Iteration 4/1000 | Loss: 0.00005309
Iteration 5/1000 | Loss: 0.00005021
Iteration 6/1000 | Loss: 0.00004872
Iteration 7/1000 | Loss: 0.00004732
Iteration 8/1000 | Loss: 0.00004635
Iteration 9/1000 | Loss: 0.00004574
Iteration 10/1000 | Loss: 0.00004529
Iteration 11/1000 | Loss: 0.00004497
Iteration 12/1000 | Loss: 0.00004482
Iteration 13/1000 | Loss: 0.00004469
Iteration 14/1000 | Loss: 0.00004464
Iteration 15/1000 | Loss: 0.00004461
Iteration 16/1000 | Loss: 0.00004458
Iteration 17/1000 | Loss: 0.00004457
Iteration 18/1000 | Loss: 0.00004452
Iteration 19/1000 | Loss: 0.00004449
Iteration 20/1000 | Loss: 0.00004448
Iteration 21/1000 | Loss: 0.00004448
Iteration 22/1000 | Loss: 0.00004445
Iteration 23/1000 | Loss: 0.00004445
Iteration 24/1000 | Loss: 0.00004445
Iteration 25/1000 | Loss: 0.00004442
Iteration 26/1000 | Loss: 0.00004442
Iteration 27/1000 | Loss: 0.00004442
Iteration 28/1000 | Loss: 0.00004442
Iteration 29/1000 | Loss: 0.00004442
Iteration 30/1000 | Loss: 0.00004442
Iteration 31/1000 | Loss: 0.00004441
Iteration 32/1000 | Loss: 0.00004441
Iteration 33/1000 | Loss: 0.00004440
Iteration 34/1000 | Loss: 0.00004440
Iteration 35/1000 | Loss: 0.00004440
Iteration 36/1000 | Loss: 0.00004440
Iteration 37/1000 | Loss: 0.00004440
Iteration 38/1000 | Loss: 0.00004440
Iteration 39/1000 | Loss: 0.00004439
Iteration 40/1000 | Loss: 0.00004438
Iteration 41/1000 | Loss: 0.00004438
Iteration 42/1000 | Loss: 0.00004437
Iteration 43/1000 | Loss: 0.00004436
Iteration 44/1000 | Loss: 0.00004435
Iteration 45/1000 | Loss: 0.00004435
Iteration 46/1000 | Loss: 0.00004435
Iteration 47/1000 | Loss: 0.00004433
Iteration 48/1000 | Loss: 0.00004432
Iteration 49/1000 | Loss: 0.00004432
Iteration 50/1000 | Loss: 0.00004432
Iteration 51/1000 | Loss: 0.00004432
Iteration 52/1000 | Loss: 0.00004432
Iteration 53/1000 | Loss: 0.00004432
Iteration 54/1000 | Loss: 0.00004432
Iteration 55/1000 | Loss: 0.00004432
Iteration 56/1000 | Loss: 0.00004430
Iteration 57/1000 | Loss: 0.00004430
Iteration 58/1000 | Loss: 0.00004428
Iteration 59/1000 | Loss: 0.00004428
Iteration 60/1000 | Loss: 0.00004428
Iteration 61/1000 | Loss: 0.00004428
Iteration 62/1000 | Loss: 0.00004428
Iteration 63/1000 | Loss: 0.00004426
Iteration 64/1000 | Loss: 0.00004426
Iteration 65/1000 | Loss: 0.00004426
Iteration 66/1000 | Loss: 0.00004426
Iteration 67/1000 | Loss: 0.00004426
Iteration 68/1000 | Loss: 0.00004426
Iteration 69/1000 | Loss: 0.00004425
Iteration 70/1000 | Loss: 0.00004425
Iteration 71/1000 | Loss: 0.00004425
Iteration 72/1000 | Loss: 0.00004425
Iteration 73/1000 | Loss: 0.00004425
Iteration 74/1000 | Loss: 0.00004425
Iteration 75/1000 | Loss: 0.00004424
Iteration 76/1000 | Loss: 0.00004424
Iteration 77/1000 | Loss: 0.00004424
Iteration 78/1000 | Loss: 0.00004423
Iteration 79/1000 | Loss: 0.00004423
Iteration 80/1000 | Loss: 0.00004423
Iteration 81/1000 | Loss: 0.00004423
Iteration 82/1000 | Loss: 0.00004423
Iteration 83/1000 | Loss: 0.00004423
Iteration 84/1000 | Loss: 0.00004422
Iteration 85/1000 | Loss: 0.00004422
Iteration 86/1000 | Loss: 0.00004422
Iteration 87/1000 | Loss: 0.00004422
Iteration 88/1000 | Loss: 0.00004422
Iteration 89/1000 | Loss: 0.00004422
Iteration 90/1000 | Loss: 0.00004422
Iteration 91/1000 | Loss: 0.00004421
Iteration 92/1000 | Loss: 0.00004421
Iteration 93/1000 | Loss: 0.00004421
Iteration 94/1000 | Loss: 0.00004421
Iteration 95/1000 | Loss: 0.00004421
Iteration 96/1000 | Loss: 0.00004421
Iteration 97/1000 | Loss: 0.00004421
Iteration 98/1000 | Loss: 0.00004421
Iteration 99/1000 | Loss: 0.00004420
Iteration 100/1000 | Loss: 0.00004420
Iteration 101/1000 | Loss: 0.00004420
Iteration 102/1000 | Loss: 0.00004420
Iteration 103/1000 | Loss: 0.00004420
Iteration 104/1000 | Loss: 0.00004420
Iteration 105/1000 | Loss: 0.00004420
Iteration 106/1000 | Loss: 0.00004419
Iteration 107/1000 | Loss: 0.00004419
Iteration 108/1000 | Loss: 0.00004419
Iteration 109/1000 | Loss: 0.00004419
Iteration 110/1000 | Loss: 0.00004419
Iteration 111/1000 | Loss: 0.00004418
Iteration 112/1000 | Loss: 0.00004418
Iteration 113/1000 | Loss: 0.00004418
Iteration 114/1000 | Loss: 0.00004418
Iteration 115/1000 | Loss: 0.00004418
Iteration 116/1000 | Loss: 0.00004417
Iteration 117/1000 | Loss: 0.00004417
Iteration 118/1000 | Loss: 0.00004417
Iteration 119/1000 | Loss: 0.00004417
Iteration 120/1000 | Loss: 0.00004416
Iteration 121/1000 | Loss: 0.00004416
Iteration 122/1000 | Loss: 0.00004416
Iteration 123/1000 | Loss: 0.00004416
Iteration 124/1000 | Loss: 0.00004416
Iteration 125/1000 | Loss: 0.00004416
Iteration 126/1000 | Loss: 0.00004415
Iteration 127/1000 | Loss: 0.00004415
Iteration 128/1000 | Loss: 0.00004415
Iteration 129/1000 | Loss: 0.00004415
Iteration 130/1000 | Loss: 0.00004415
Iteration 131/1000 | Loss: 0.00004415
Iteration 132/1000 | Loss: 0.00004415
Iteration 133/1000 | Loss: 0.00004415
Iteration 134/1000 | Loss: 0.00004415
Iteration 135/1000 | Loss: 0.00004414
Iteration 136/1000 | Loss: 0.00004414
Iteration 137/1000 | Loss: 0.00004414
Iteration 138/1000 | Loss: 0.00004414
Iteration 139/1000 | Loss: 0.00004414
Iteration 140/1000 | Loss: 0.00004414
Iteration 141/1000 | Loss: 0.00004414
Iteration 142/1000 | Loss: 0.00004414
Iteration 143/1000 | Loss: 0.00004413
Iteration 144/1000 | Loss: 0.00004413
Iteration 145/1000 | Loss: 0.00004413
Iteration 146/1000 | Loss: 0.00004413
Iteration 147/1000 | Loss: 0.00004413
Iteration 148/1000 | Loss: 0.00004413
Iteration 149/1000 | Loss: 0.00004413
Iteration 150/1000 | Loss: 0.00004413
Iteration 151/1000 | Loss: 0.00004413
Iteration 152/1000 | Loss: 0.00004413
Iteration 153/1000 | Loss: 0.00004412
Iteration 154/1000 | Loss: 0.00004412
Iteration 155/1000 | Loss: 0.00004412
Iteration 156/1000 | Loss: 0.00004412
Iteration 157/1000 | Loss: 0.00004412
Iteration 158/1000 | Loss: 0.00004412
Iteration 159/1000 | Loss: 0.00004412
Iteration 160/1000 | Loss: 0.00004412
Iteration 161/1000 | Loss: 0.00004412
Iteration 162/1000 | Loss: 0.00004412
Iteration 163/1000 | Loss: 0.00004412
Iteration 164/1000 | Loss: 0.00004412
Iteration 165/1000 | Loss: 0.00004412
Iteration 166/1000 | Loss: 0.00004412
Iteration 167/1000 | Loss: 0.00004412
Iteration 168/1000 | Loss: 0.00004411
Iteration 169/1000 | Loss: 0.00004411
Iteration 170/1000 | Loss: 0.00004411
Iteration 171/1000 | Loss: 0.00004411
Iteration 172/1000 | Loss: 0.00004411
Iteration 173/1000 | Loss: 0.00004411
Iteration 174/1000 | Loss: 0.00004411
Iteration 175/1000 | Loss: 0.00004411
Iteration 176/1000 | Loss: 0.00004411
Iteration 177/1000 | Loss: 0.00004411
Iteration 178/1000 | Loss: 0.00004411
Iteration 179/1000 | Loss: 0.00004410
Iteration 180/1000 | Loss: 0.00004410
Iteration 181/1000 | Loss: 0.00004410
Iteration 182/1000 | Loss: 0.00004410
Iteration 183/1000 | Loss: 0.00004410
Iteration 184/1000 | Loss: 0.00004410
Iteration 185/1000 | Loss: 0.00004410
Iteration 186/1000 | Loss: 0.00004410
Iteration 187/1000 | Loss: 0.00004410
Iteration 188/1000 | Loss: 0.00004410
Iteration 189/1000 | Loss: 0.00004410
Iteration 190/1000 | Loss: 0.00004410
Iteration 191/1000 | Loss: 0.00004410
Iteration 192/1000 | Loss: 0.00004410
Iteration 193/1000 | Loss: 0.00004410
Iteration 194/1000 | Loss: 0.00004410
Iteration 195/1000 | Loss: 0.00004410
Iteration 196/1000 | Loss: 0.00004410
Iteration 197/1000 | Loss: 0.00004410
Iteration 198/1000 | Loss: 0.00004410
Iteration 199/1000 | Loss: 0.00004410
Iteration 200/1000 | Loss: 0.00004410
Iteration 201/1000 | Loss: 0.00004410
Iteration 202/1000 | Loss: 0.00004410
Iteration 203/1000 | Loss: 0.00004410
Iteration 204/1000 | Loss: 0.00004410
Iteration 205/1000 | Loss: 0.00004410
Iteration 206/1000 | Loss: 0.00004410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [4.4103857362642884e-05, 4.4103857362642884e-05, 4.4103857362642884e-05, 4.4103857362642884e-05, 4.4103857362642884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4103857362642884e-05

Optimization complete. Final v2v error: 5.540010452270508 mm

Highest mean error: 6.753605842590332 mm for frame 66

Lowest mean error: 4.656048774719238 mm for frame 41

Saving results

Total time: 43.75044131278992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872668
Iteration 2/25 | Loss: 0.00174966
Iteration 3/25 | Loss: 0.00152184
Iteration 4/25 | Loss: 0.00148827
Iteration 5/25 | Loss: 0.00147882
Iteration 6/25 | Loss: 0.00147637
Iteration 7/25 | Loss: 0.00147637
Iteration 8/25 | Loss: 0.00147637
Iteration 9/25 | Loss: 0.00147637
Iteration 10/25 | Loss: 0.00147637
Iteration 11/25 | Loss: 0.00147637
Iteration 12/25 | Loss: 0.00147637
Iteration 13/25 | Loss: 0.00147637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014763673534616828, 0.0014763673534616828, 0.0014763673534616828, 0.0014763673534616828, 0.0014763673534616828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014763673534616828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26443934
Iteration 2/25 | Loss: 0.00113580
Iteration 3/25 | Loss: 0.00113580
Iteration 4/25 | Loss: 0.00113580
Iteration 5/25 | Loss: 0.00113580
Iteration 6/25 | Loss: 0.00113580
Iteration 7/25 | Loss: 0.00113580
Iteration 8/25 | Loss: 0.00113580
Iteration 9/25 | Loss: 0.00113580
Iteration 10/25 | Loss: 0.00113580
Iteration 11/25 | Loss: 0.00113580
Iteration 12/25 | Loss: 0.00113580
Iteration 13/25 | Loss: 0.00113580
Iteration 14/25 | Loss: 0.00113580
Iteration 15/25 | Loss: 0.00113580
Iteration 16/25 | Loss: 0.00113580
Iteration 17/25 | Loss: 0.00113580
Iteration 18/25 | Loss: 0.00113580
Iteration 19/25 | Loss: 0.00113580
Iteration 20/25 | Loss: 0.00113580
Iteration 21/25 | Loss: 0.00113580
Iteration 22/25 | Loss: 0.00113580
Iteration 23/25 | Loss: 0.00113580
Iteration 24/25 | Loss: 0.00113580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011357980547472835, 0.0011357980547472835, 0.0011357980547472835, 0.0011357980547472835, 0.0011357980547472835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011357980547472835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113580
Iteration 2/1000 | Loss: 0.00006017
Iteration 3/1000 | Loss: 0.00004659
Iteration 4/1000 | Loss: 0.00004042
Iteration 5/1000 | Loss: 0.00003787
Iteration 6/1000 | Loss: 0.00003640
Iteration 7/1000 | Loss: 0.00003546
Iteration 8/1000 | Loss: 0.00003469
Iteration 9/1000 | Loss: 0.00003418
Iteration 10/1000 | Loss: 0.00003385
Iteration 11/1000 | Loss: 0.00003358
Iteration 12/1000 | Loss: 0.00003337
Iteration 13/1000 | Loss: 0.00003319
Iteration 14/1000 | Loss: 0.00003316
Iteration 15/1000 | Loss: 0.00003302
Iteration 16/1000 | Loss: 0.00003297
Iteration 17/1000 | Loss: 0.00003294
Iteration 18/1000 | Loss: 0.00003289
Iteration 19/1000 | Loss: 0.00003287
Iteration 20/1000 | Loss: 0.00003284
Iteration 21/1000 | Loss: 0.00003284
Iteration 22/1000 | Loss: 0.00003284
Iteration 23/1000 | Loss: 0.00003283
Iteration 24/1000 | Loss: 0.00003283
Iteration 25/1000 | Loss: 0.00003283
Iteration 26/1000 | Loss: 0.00003283
Iteration 27/1000 | Loss: 0.00003282
Iteration 28/1000 | Loss: 0.00003282
Iteration 29/1000 | Loss: 0.00003282
Iteration 30/1000 | Loss: 0.00003281
Iteration 31/1000 | Loss: 0.00003280
Iteration 32/1000 | Loss: 0.00003279
Iteration 33/1000 | Loss: 0.00003279
Iteration 34/1000 | Loss: 0.00003279
Iteration 35/1000 | Loss: 0.00003278
Iteration 36/1000 | Loss: 0.00003274
Iteration 37/1000 | Loss: 0.00003274
Iteration 38/1000 | Loss: 0.00003274
Iteration 39/1000 | Loss: 0.00003274
Iteration 40/1000 | Loss: 0.00003273
Iteration 41/1000 | Loss: 0.00003273
Iteration 42/1000 | Loss: 0.00003272
Iteration 43/1000 | Loss: 0.00003271
Iteration 44/1000 | Loss: 0.00003271
Iteration 45/1000 | Loss: 0.00003271
Iteration 46/1000 | Loss: 0.00003270
Iteration 47/1000 | Loss: 0.00003270
Iteration 48/1000 | Loss: 0.00003268
Iteration 49/1000 | Loss: 0.00003268
Iteration 50/1000 | Loss: 0.00003267
Iteration 51/1000 | Loss: 0.00003267
Iteration 52/1000 | Loss: 0.00003267
Iteration 53/1000 | Loss: 0.00003267
Iteration 54/1000 | Loss: 0.00003267
Iteration 55/1000 | Loss: 0.00003266
Iteration 56/1000 | Loss: 0.00003266
Iteration 57/1000 | Loss: 0.00003266
Iteration 58/1000 | Loss: 0.00003265
Iteration 59/1000 | Loss: 0.00003264
Iteration 60/1000 | Loss: 0.00003264
Iteration 61/1000 | Loss: 0.00003263
Iteration 62/1000 | Loss: 0.00003263
Iteration 63/1000 | Loss: 0.00003263
Iteration 64/1000 | Loss: 0.00003263
Iteration 65/1000 | Loss: 0.00003263
Iteration 66/1000 | Loss: 0.00003262
Iteration 67/1000 | Loss: 0.00003262
Iteration 68/1000 | Loss: 0.00003262
Iteration 69/1000 | Loss: 0.00003262
Iteration 70/1000 | Loss: 0.00003261
Iteration 71/1000 | Loss: 0.00003261
Iteration 72/1000 | Loss: 0.00003261
Iteration 73/1000 | Loss: 0.00003261
Iteration 74/1000 | Loss: 0.00003261
Iteration 75/1000 | Loss: 0.00003261
Iteration 76/1000 | Loss: 0.00003261
Iteration 77/1000 | Loss: 0.00003261
Iteration 78/1000 | Loss: 0.00003260
Iteration 79/1000 | Loss: 0.00003260
Iteration 80/1000 | Loss: 0.00003260
Iteration 81/1000 | Loss: 0.00003260
Iteration 82/1000 | Loss: 0.00003260
Iteration 83/1000 | Loss: 0.00003260
Iteration 84/1000 | Loss: 0.00003260
Iteration 85/1000 | Loss: 0.00003260
Iteration 86/1000 | Loss: 0.00003260
Iteration 87/1000 | Loss: 0.00003259
Iteration 88/1000 | Loss: 0.00003259
Iteration 89/1000 | Loss: 0.00003259
Iteration 90/1000 | Loss: 0.00003259
Iteration 91/1000 | Loss: 0.00003259
Iteration 92/1000 | Loss: 0.00003259
Iteration 93/1000 | Loss: 0.00003259
Iteration 94/1000 | Loss: 0.00003259
Iteration 95/1000 | Loss: 0.00003259
Iteration 96/1000 | Loss: 0.00003259
Iteration 97/1000 | Loss: 0.00003258
Iteration 98/1000 | Loss: 0.00003258
Iteration 99/1000 | Loss: 0.00003258
Iteration 100/1000 | Loss: 0.00003258
Iteration 101/1000 | Loss: 0.00003258
Iteration 102/1000 | Loss: 0.00003258
Iteration 103/1000 | Loss: 0.00003258
Iteration 104/1000 | Loss: 0.00003257
Iteration 105/1000 | Loss: 0.00003257
Iteration 106/1000 | Loss: 0.00003257
Iteration 107/1000 | Loss: 0.00003257
Iteration 108/1000 | Loss: 0.00003257
Iteration 109/1000 | Loss: 0.00003257
Iteration 110/1000 | Loss: 0.00003257
Iteration 111/1000 | Loss: 0.00003257
Iteration 112/1000 | Loss: 0.00003256
Iteration 113/1000 | Loss: 0.00003256
Iteration 114/1000 | Loss: 0.00003256
Iteration 115/1000 | Loss: 0.00003256
Iteration 116/1000 | Loss: 0.00003256
Iteration 117/1000 | Loss: 0.00003255
Iteration 118/1000 | Loss: 0.00003255
Iteration 119/1000 | Loss: 0.00003255
Iteration 120/1000 | Loss: 0.00003255
Iteration 121/1000 | Loss: 0.00003255
Iteration 122/1000 | Loss: 0.00003255
Iteration 123/1000 | Loss: 0.00003255
Iteration 124/1000 | Loss: 0.00003255
Iteration 125/1000 | Loss: 0.00003255
Iteration 126/1000 | Loss: 0.00003255
Iteration 127/1000 | Loss: 0.00003255
Iteration 128/1000 | Loss: 0.00003255
Iteration 129/1000 | Loss: 0.00003255
Iteration 130/1000 | Loss: 0.00003255
Iteration 131/1000 | Loss: 0.00003255
Iteration 132/1000 | Loss: 0.00003255
Iteration 133/1000 | Loss: 0.00003255
Iteration 134/1000 | Loss: 0.00003255
Iteration 135/1000 | Loss: 0.00003255
Iteration 136/1000 | Loss: 0.00003255
Iteration 137/1000 | Loss: 0.00003255
Iteration 138/1000 | Loss: 0.00003255
Iteration 139/1000 | Loss: 0.00003255
Iteration 140/1000 | Loss: 0.00003255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.254586044931784e-05, 3.254586044931784e-05, 3.254586044931784e-05, 3.254586044931784e-05, 3.254586044931784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.254586044931784e-05

Optimization complete. Final v2v error: 4.837608814239502 mm

Highest mean error: 6.102969169616699 mm for frame 211

Lowest mean error: 3.943380355834961 mm for frame 127

Saving results

Total time: 45.1585807800293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870507
Iteration 2/25 | Loss: 0.00202746
Iteration 3/25 | Loss: 0.00175754
Iteration 4/25 | Loss: 0.00168250
Iteration 5/25 | Loss: 0.00165640
Iteration 6/25 | Loss: 0.00163100
Iteration 7/25 | Loss: 0.00161701
Iteration 8/25 | Loss: 0.00160863
Iteration 9/25 | Loss: 0.00160363
Iteration 10/25 | Loss: 0.00160022
Iteration 11/25 | Loss: 0.00159989
Iteration 12/25 | Loss: 0.00159975
Iteration 13/25 | Loss: 0.00159948
Iteration 14/25 | Loss: 0.00159910
Iteration 15/25 | Loss: 0.00159886
Iteration 16/25 | Loss: 0.00159851
Iteration 17/25 | Loss: 0.00160112
Iteration 18/25 | Loss: 0.00159494
Iteration 19/25 | Loss: 0.00159795
Iteration 20/25 | Loss: 0.00159689
Iteration 21/25 | Loss: 0.00159781
Iteration 22/25 | Loss: 0.00159647
Iteration 23/25 | Loss: 0.00159576
Iteration 24/25 | Loss: 0.00159309
Iteration 25/25 | Loss: 0.00159289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06089854
Iteration 2/25 | Loss: 0.00215452
Iteration 3/25 | Loss: 0.00215447
Iteration 4/25 | Loss: 0.00215447
Iteration 5/25 | Loss: 0.00215447
Iteration 6/25 | Loss: 0.00215447
Iteration 7/25 | Loss: 0.00215447
Iteration 8/25 | Loss: 0.00215447
Iteration 9/25 | Loss: 0.00215447
Iteration 10/25 | Loss: 0.00215447
Iteration 11/25 | Loss: 0.00215447
Iteration 12/25 | Loss: 0.00215447
Iteration 13/25 | Loss: 0.00215447
Iteration 14/25 | Loss: 0.00215447
Iteration 15/25 | Loss: 0.00215447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002154470421373844, 0.002154470421373844, 0.002154470421373844, 0.002154470421373844, 0.002154470421373844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002154470421373844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215447
Iteration 2/1000 | Loss: 0.00010714
Iteration 3/1000 | Loss: 0.00057696
Iteration 4/1000 | Loss: 0.00008690
Iteration 5/1000 | Loss: 0.00005964
Iteration 6/1000 | Loss: 0.00038367
Iteration 7/1000 | Loss: 0.00011688
Iteration 8/1000 | Loss: 0.00026368
Iteration 9/1000 | Loss: 0.00003888
Iteration 10/1000 | Loss: 0.00003708
Iteration 11/1000 | Loss: 0.00003625
Iteration 12/1000 | Loss: 0.00003564
Iteration 13/1000 | Loss: 0.00003513
Iteration 14/1000 | Loss: 0.00003472
Iteration 15/1000 | Loss: 0.00003439
Iteration 16/1000 | Loss: 0.00003409
Iteration 17/1000 | Loss: 0.00003389
Iteration 18/1000 | Loss: 0.00003387
Iteration 19/1000 | Loss: 0.00003379
Iteration 20/1000 | Loss: 0.00003378
Iteration 21/1000 | Loss: 0.00003377
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00003373
Iteration 25/1000 | Loss: 0.00003372
Iteration 26/1000 | Loss: 0.00003372
Iteration 27/1000 | Loss: 0.00003372
Iteration 28/1000 | Loss: 0.00003372
Iteration 29/1000 | Loss: 0.00003372
Iteration 30/1000 | Loss: 0.00003371
Iteration 31/1000 | Loss: 0.00003367
Iteration 32/1000 | Loss: 0.00003367
Iteration 33/1000 | Loss: 0.00003365
Iteration 34/1000 | Loss: 0.00003362
Iteration 35/1000 | Loss: 0.00003359
Iteration 36/1000 | Loss: 0.00003359
Iteration 37/1000 | Loss: 0.00003359
Iteration 38/1000 | Loss: 0.00003358
Iteration 39/1000 | Loss: 0.00003358
Iteration 40/1000 | Loss: 0.00003358
Iteration 41/1000 | Loss: 0.00017508
Iteration 42/1000 | Loss: 0.00014655
Iteration 43/1000 | Loss: 0.00007820
Iteration 44/1000 | Loss: 0.00003942
Iteration 45/1000 | Loss: 0.00003768
Iteration 46/1000 | Loss: 0.00003616
Iteration 47/1000 | Loss: 0.00007433
Iteration 48/1000 | Loss: 0.00003677
Iteration 49/1000 | Loss: 0.00006063
Iteration 50/1000 | Loss: 0.00017311
Iteration 51/1000 | Loss: 0.00004515
Iteration 52/1000 | Loss: 0.00005469
Iteration 53/1000 | Loss: 0.00003884
Iteration 54/1000 | Loss: 0.00014865
Iteration 55/1000 | Loss: 0.00006061
Iteration 56/1000 | Loss: 0.00003871
Iteration 57/1000 | Loss: 0.00018971
Iteration 58/1000 | Loss: 0.00007743
Iteration 59/1000 | Loss: 0.00013806
Iteration 60/1000 | Loss: 0.00007339
Iteration 61/1000 | Loss: 0.00003898
Iteration 62/1000 | Loss: 0.00010902
Iteration 63/1000 | Loss: 0.00005933
Iteration 64/1000 | Loss: 0.00028373
Iteration 65/1000 | Loss: 0.00007253
Iteration 66/1000 | Loss: 0.00015621
Iteration 67/1000 | Loss: 0.00005885
Iteration 68/1000 | Loss: 0.00029072
Iteration 69/1000 | Loss: 0.00008923
Iteration 70/1000 | Loss: 0.00003688
Iteration 71/1000 | Loss: 0.00024471
Iteration 72/1000 | Loss: 0.00020233
Iteration 73/1000 | Loss: 0.00018577
Iteration 74/1000 | Loss: 0.00004710
Iteration 75/1000 | Loss: 0.00004012
Iteration 76/1000 | Loss: 0.00003913
Iteration 77/1000 | Loss: 0.00004043
Iteration 78/1000 | Loss: 0.00003983
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00004003
Iteration 81/1000 | Loss: 0.00004537
Iteration 82/1000 | Loss: 0.00003965
Iteration 83/1000 | Loss: 0.00004299
Iteration 84/1000 | Loss: 0.00003593
Iteration 85/1000 | Loss: 0.00003508
Iteration 86/1000 | Loss: 0.00003419
Iteration 87/1000 | Loss: 0.00003370
Iteration 88/1000 | Loss: 0.00003345
Iteration 89/1000 | Loss: 0.00003343
Iteration 90/1000 | Loss: 0.00003341
Iteration 91/1000 | Loss: 0.00003339
Iteration 92/1000 | Loss: 0.00003336
Iteration 93/1000 | Loss: 0.00003310
Iteration 94/1000 | Loss: 0.00003287
Iteration 95/1000 | Loss: 0.00003287
Iteration 96/1000 | Loss: 0.00003287
Iteration 97/1000 | Loss: 0.00003280
Iteration 98/1000 | Loss: 0.00003279
Iteration 99/1000 | Loss: 0.00003279
Iteration 100/1000 | Loss: 0.00003278
Iteration 101/1000 | Loss: 0.00003278
Iteration 102/1000 | Loss: 0.00003277
Iteration 103/1000 | Loss: 0.00003276
Iteration 104/1000 | Loss: 0.00003275
Iteration 105/1000 | Loss: 0.00003274
Iteration 106/1000 | Loss: 0.00003274
Iteration 107/1000 | Loss: 0.00003273
Iteration 108/1000 | Loss: 0.00003273
Iteration 109/1000 | Loss: 0.00003268
Iteration 110/1000 | Loss: 0.00003268
Iteration 111/1000 | Loss: 0.00003268
Iteration 112/1000 | Loss: 0.00003268
Iteration 113/1000 | Loss: 0.00003268
Iteration 114/1000 | Loss: 0.00003268
Iteration 115/1000 | Loss: 0.00003267
Iteration 116/1000 | Loss: 0.00003267
Iteration 117/1000 | Loss: 0.00003267
Iteration 118/1000 | Loss: 0.00003267
Iteration 119/1000 | Loss: 0.00003267
Iteration 120/1000 | Loss: 0.00003266
Iteration 121/1000 | Loss: 0.00003266
Iteration 122/1000 | Loss: 0.00003266
Iteration 123/1000 | Loss: 0.00003265
Iteration 124/1000 | Loss: 0.00003264
Iteration 125/1000 | Loss: 0.00003264
Iteration 126/1000 | Loss: 0.00003264
Iteration 127/1000 | Loss: 0.00003264
Iteration 128/1000 | Loss: 0.00003263
Iteration 129/1000 | Loss: 0.00003263
Iteration 130/1000 | Loss: 0.00003263
Iteration 131/1000 | Loss: 0.00003263
Iteration 132/1000 | Loss: 0.00003263
Iteration 133/1000 | Loss: 0.00003263
Iteration 134/1000 | Loss: 0.00003263
Iteration 135/1000 | Loss: 0.00003263
Iteration 136/1000 | Loss: 0.00003262
Iteration 137/1000 | Loss: 0.00003262
Iteration 138/1000 | Loss: 0.00003262
Iteration 139/1000 | Loss: 0.00003262
Iteration 140/1000 | Loss: 0.00003261
Iteration 141/1000 | Loss: 0.00003261
Iteration 142/1000 | Loss: 0.00003261
Iteration 143/1000 | Loss: 0.00003261
Iteration 144/1000 | Loss: 0.00003261
Iteration 145/1000 | Loss: 0.00003260
Iteration 146/1000 | Loss: 0.00003260
Iteration 147/1000 | Loss: 0.00003260
Iteration 148/1000 | Loss: 0.00003260
Iteration 149/1000 | Loss: 0.00003259
Iteration 150/1000 | Loss: 0.00003259
Iteration 151/1000 | Loss: 0.00003259
Iteration 152/1000 | Loss: 0.00003259
Iteration 153/1000 | Loss: 0.00003259
Iteration 154/1000 | Loss: 0.00003259
Iteration 155/1000 | Loss: 0.00003259
Iteration 156/1000 | Loss: 0.00003259
Iteration 157/1000 | Loss: 0.00003258
Iteration 158/1000 | Loss: 0.00003258
Iteration 159/1000 | Loss: 0.00003258
Iteration 160/1000 | Loss: 0.00003258
Iteration 161/1000 | Loss: 0.00003258
Iteration 162/1000 | Loss: 0.00003258
Iteration 163/1000 | Loss: 0.00003258
Iteration 164/1000 | Loss: 0.00003258
Iteration 165/1000 | Loss: 0.00003258
Iteration 166/1000 | Loss: 0.00003258
Iteration 167/1000 | Loss: 0.00003258
Iteration 168/1000 | Loss: 0.00003258
Iteration 169/1000 | Loss: 0.00003258
Iteration 170/1000 | Loss: 0.00003258
Iteration 171/1000 | Loss: 0.00003258
Iteration 172/1000 | Loss: 0.00003258
Iteration 173/1000 | Loss: 0.00003258
Iteration 174/1000 | Loss: 0.00003258
Iteration 175/1000 | Loss: 0.00003258
Iteration 176/1000 | Loss: 0.00003258
Iteration 177/1000 | Loss: 0.00003258
Iteration 178/1000 | Loss: 0.00003258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.257969001424499e-05, 3.257969001424499e-05, 3.257969001424499e-05, 3.257969001424499e-05, 3.257969001424499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.257969001424499e-05

Optimization complete. Final v2v error: 4.862849235534668 mm

Highest mean error: 6.2680888175964355 mm for frame 148

Lowest mean error: 4.530276775360107 mm for frame 128

Saving results

Total time: 171.0660834312439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436744
Iteration 2/25 | Loss: 0.00150510
Iteration 3/25 | Loss: 0.00142367
Iteration 4/25 | Loss: 0.00141182
Iteration 5/25 | Loss: 0.00140520
Iteration 6/25 | Loss: 0.00140321
Iteration 7/25 | Loss: 0.00140306
Iteration 8/25 | Loss: 0.00140306
Iteration 9/25 | Loss: 0.00140306
Iteration 10/25 | Loss: 0.00140306
Iteration 11/25 | Loss: 0.00140306
Iteration 12/25 | Loss: 0.00140306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014030571328476071, 0.0014030571328476071, 0.0014030571328476071, 0.0014030571328476071, 0.0014030571328476071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014030571328476071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66290033
Iteration 2/25 | Loss: 0.00140881
Iteration 3/25 | Loss: 0.00140881
Iteration 4/25 | Loss: 0.00140881
Iteration 5/25 | Loss: 0.00140881
Iteration 6/25 | Loss: 0.00140881
Iteration 7/25 | Loss: 0.00140881
Iteration 8/25 | Loss: 0.00140881
Iteration 9/25 | Loss: 0.00140881
Iteration 10/25 | Loss: 0.00140881
Iteration 11/25 | Loss: 0.00140881
Iteration 12/25 | Loss: 0.00140881
Iteration 13/25 | Loss: 0.00140881
Iteration 14/25 | Loss: 0.00140881
Iteration 15/25 | Loss: 0.00140881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014088067691773176, 0.0014088067691773176, 0.0014088067691773176, 0.0014088067691773176, 0.0014088067691773176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014088067691773176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140881
Iteration 2/1000 | Loss: 0.00004482
Iteration 3/1000 | Loss: 0.00003586
Iteration 4/1000 | Loss: 0.00003135
Iteration 5/1000 | Loss: 0.00003016
Iteration 6/1000 | Loss: 0.00002913
Iteration 7/1000 | Loss: 0.00002827
Iteration 8/1000 | Loss: 0.00002771
Iteration 9/1000 | Loss: 0.00002726
Iteration 10/1000 | Loss: 0.00002692
Iteration 11/1000 | Loss: 0.00002668
Iteration 12/1000 | Loss: 0.00002645
Iteration 13/1000 | Loss: 0.00002624
Iteration 14/1000 | Loss: 0.00002615
Iteration 15/1000 | Loss: 0.00002614
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002612
Iteration 18/1000 | Loss: 0.00002612
Iteration 19/1000 | Loss: 0.00002608
Iteration 20/1000 | Loss: 0.00002607
Iteration 21/1000 | Loss: 0.00002607
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002606
Iteration 24/1000 | Loss: 0.00002605
Iteration 25/1000 | Loss: 0.00002604
Iteration 26/1000 | Loss: 0.00002604
Iteration 27/1000 | Loss: 0.00002604
Iteration 28/1000 | Loss: 0.00002604
Iteration 29/1000 | Loss: 0.00002603
Iteration 30/1000 | Loss: 0.00002603
Iteration 31/1000 | Loss: 0.00002603
Iteration 32/1000 | Loss: 0.00002603
Iteration 33/1000 | Loss: 0.00002603
Iteration 34/1000 | Loss: 0.00002603
Iteration 35/1000 | Loss: 0.00002603
Iteration 36/1000 | Loss: 0.00002603
Iteration 37/1000 | Loss: 0.00002603
Iteration 38/1000 | Loss: 0.00002602
Iteration 39/1000 | Loss: 0.00002602
Iteration 40/1000 | Loss: 0.00002602
Iteration 41/1000 | Loss: 0.00002601
Iteration 42/1000 | Loss: 0.00002601
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002601
Iteration 45/1000 | Loss: 0.00002601
Iteration 46/1000 | Loss: 0.00002601
Iteration 47/1000 | Loss: 0.00002601
Iteration 48/1000 | Loss: 0.00002601
Iteration 49/1000 | Loss: 0.00002601
Iteration 50/1000 | Loss: 0.00002601
Iteration 51/1000 | Loss: 0.00002601
Iteration 52/1000 | Loss: 0.00002600
Iteration 53/1000 | Loss: 0.00002600
Iteration 54/1000 | Loss: 0.00002600
Iteration 55/1000 | Loss: 0.00002600
Iteration 56/1000 | Loss: 0.00002600
Iteration 57/1000 | Loss: 0.00002600
Iteration 58/1000 | Loss: 0.00002599
Iteration 59/1000 | Loss: 0.00002599
Iteration 60/1000 | Loss: 0.00002599
Iteration 61/1000 | Loss: 0.00002599
Iteration 62/1000 | Loss: 0.00002599
Iteration 63/1000 | Loss: 0.00002599
Iteration 64/1000 | Loss: 0.00002599
Iteration 65/1000 | Loss: 0.00002598
Iteration 66/1000 | Loss: 0.00002598
Iteration 67/1000 | Loss: 0.00002598
Iteration 68/1000 | Loss: 0.00002598
Iteration 69/1000 | Loss: 0.00002598
Iteration 70/1000 | Loss: 0.00002598
Iteration 71/1000 | Loss: 0.00002598
Iteration 72/1000 | Loss: 0.00002598
Iteration 73/1000 | Loss: 0.00002598
Iteration 74/1000 | Loss: 0.00002597
Iteration 75/1000 | Loss: 0.00002597
Iteration 76/1000 | Loss: 0.00002597
Iteration 77/1000 | Loss: 0.00002597
Iteration 78/1000 | Loss: 0.00002597
Iteration 79/1000 | Loss: 0.00002596
Iteration 80/1000 | Loss: 0.00002596
Iteration 81/1000 | Loss: 0.00002596
Iteration 82/1000 | Loss: 0.00002596
Iteration 83/1000 | Loss: 0.00002595
Iteration 84/1000 | Loss: 0.00002595
Iteration 85/1000 | Loss: 0.00002595
Iteration 86/1000 | Loss: 0.00002595
Iteration 87/1000 | Loss: 0.00002595
Iteration 88/1000 | Loss: 0.00002595
Iteration 89/1000 | Loss: 0.00002595
Iteration 90/1000 | Loss: 0.00002595
Iteration 91/1000 | Loss: 0.00002595
Iteration 92/1000 | Loss: 0.00002595
Iteration 93/1000 | Loss: 0.00002595
Iteration 94/1000 | Loss: 0.00002595
Iteration 95/1000 | Loss: 0.00002595
Iteration 96/1000 | Loss: 0.00002595
Iteration 97/1000 | Loss: 0.00002594
Iteration 98/1000 | Loss: 0.00002594
Iteration 99/1000 | Loss: 0.00002594
Iteration 100/1000 | Loss: 0.00002594
Iteration 101/1000 | Loss: 0.00002594
Iteration 102/1000 | Loss: 0.00002594
Iteration 103/1000 | Loss: 0.00002594
Iteration 104/1000 | Loss: 0.00002593
Iteration 105/1000 | Loss: 0.00002593
Iteration 106/1000 | Loss: 0.00002593
Iteration 107/1000 | Loss: 0.00002593
Iteration 108/1000 | Loss: 0.00002593
Iteration 109/1000 | Loss: 0.00002593
Iteration 110/1000 | Loss: 0.00002592
Iteration 111/1000 | Loss: 0.00002592
Iteration 112/1000 | Loss: 0.00002592
Iteration 113/1000 | Loss: 0.00002592
Iteration 114/1000 | Loss: 0.00002592
Iteration 115/1000 | Loss: 0.00002592
Iteration 116/1000 | Loss: 0.00002592
Iteration 117/1000 | Loss: 0.00002592
Iteration 118/1000 | Loss: 0.00002592
Iteration 119/1000 | Loss: 0.00002592
Iteration 120/1000 | Loss: 0.00002592
Iteration 121/1000 | Loss: 0.00002592
Iteration 122/1000 | Loss: 0.00002592
Iteration 123/1000 | Loss: 0.00002592
Iteration 124/1000 | Loss: 0.00002592
Iteration 125/1000 | Loss: 0.00002592
Iteration 126/1000 | Loss: 0.00002592
Iteration 127/1000 | Loss: 0.00002592
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002592
Iteration 130/1000 | Loss: 0.00002592
Iteration 131/1000 | Loss: 0.00002592
Iteration 132/1000 | Loss: 0.00002592
Iteration 133/1000 | Loss: 0.00002592
Iteration 134/1000 | Loss: 0.00002592
Iteration 135/1000 | Loss: 0.00002592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.591664087958634e-05, 2.591664087958634e-05, 2.591664087958634e-05, 2.591664087958634e-05, 2.591664087958634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.591664087958634e-05

Optimization complete. Final v2v error: 4.510392665863037 mm

Highest mean error: 4.994473934173584 mm for frame 130

Lowest mean error: 4.176210880279541 mm for frame 257

Saving results

Total time: 41.60809659957886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938416
Iteration 2/25 | Loss: 0.00155310
Iteration 3/25 | Loss: 0.00140713
Iteration 4/25 | Loss: 0.00138911
Iteration 5/25 | Loss: 0.00138484
Iteration 6/25 | Loss: 0.00138426
Iteration 7/25 | Loss: 0.00138426
Iteration 8/25 | Loss: 0.00138426
Iteration 9/25 | Loss: 0.00138426
Iteration 10/25 | Loss: 0.00138426
Iteration 11/25 | Loss: 0.00138426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013842637417837977, 0.0013842637417837977, 0.0013842637417837977, 0.0013842637417837977, 0.0013842637417837977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013842637417837977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47930157
Iteration 2/25 | Loss: 0.00128345
Iteration 3/25 | Loss: 0.00128345
Iteration 4/25 | Loss: 0.00128345
Iteration 5/25 | Loss: 0.00128345
Iteration 6/25 | Loss: 0.00128345
Iteration 7/25 | Loss: 0.00128345
Iteration 8/25 | Loss: 0.00128345
Iteration 9/25 | Loss: 0.00128345
Iteration 10/25 | Loss: 0.00128345
Iteration 11/25 | Loss: 0.00128345
Iteration 12/25 | Loss: 0.00128345
Iteration 13/25 | Loss: 0.00128345
Iteration 14/25 | Loss: 0.00128345
Iteration 15/25 | Loss: 0.00128345
Iteration 16/25 | Loss: 0.00128345
Iteration 17/25 | Loss: 0.00128345
Iteration 18/25 | Loss: 0.00128345
Iteration 19/25 | Loss: 0.00128345
Iteration 20/25 | Loss: 0.00128345
Iteration 21/25 | Loss: 0.00128345
Iteration 22/25 | Loss: 0.00128345
Iteration 23/25 | Loss: 0.00128345
Iteration 24/25 | Loss: 0.00128345
Iteration 25/25 | Loss: 0.00128345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128345
Iteration 2/1000 | Loss: 0.00003668
Iteration 3/1000 | Loss: 0.00003017
Iteration 4/1000 | Loss: 0.00002679
Iteration 5/1000 | Loss: 0.00002549
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00002368
Iteration 8/1000 | Loss: 0.00002327
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002295
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00002290
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002279
Iteration 15/1000 | Loss: 0.00002279
Iteration 16/1000 | Loss: 0.00002274
Iteration 17/1000 | Loss: 0.00002274
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002272
Iteration 23/1000 | Loss: 0.00002271
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002268
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002268
Iteration 33/1000 | Loss: 0.00002268
Iteration 34/1000 | Loss: 0.00002267
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00002267
Iteration 37/1000 | Loss: 0.00002267
Iteration 38/1000 | Loss: 0.00002266
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002264
Iteration 48/1000 | Loss: 0.00002264
Iteration 49/1000 | Loss: 0.00002264
Iteration 50/1000 | Loss: 0.00002264
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002264
Iteration 53/1000 | Loss: 0.00002264
Iteration 54/1000 | Loss: 0.00002263
Iteration 55/1000 | Loss: 0.00002263
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002262
Iteration 58/1000 | Loss: 0.00002262
Iteration 59/1000 | Loss: 0.00002261
Iteration 60/1000 | Loss: 0.00002261
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002260
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002257
Iteration 72/1000 | Loss: 0.00002257
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002257
Iteration 76/1000 | Loss: 0.00002257
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002256
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002254
Iteration 86/1000 | Loss: 0.00002254
Iteration 87/1000 | Loss: 0.00002254
Iteration 88/1000 | Loss: 0.00002254
Iteration 89/1000 | Loss: 0.00002254
Iteration 90/1000 | Loss: 0.00002253
Iteration 91/1000 | Loss: 0.00002253
Iteration 92/1000 | Loss: 0.00002252
Iteration 93/1000 | Loss: 0.00002252
Iteration 94/1000 | Loss: 0.00002252
Iteration 95/1000 | Loss: 0.00002252
Iteration 96/1000 | Loss: 0.00002252
Iteration 97/1000 | Loss: 0.00002252
Iteration 98/1000 | Loss: 0.00002252
Iteration 99/1000 | Loss: 0.00002252
Iteration 100/1000 | Loss: 0.00002251
Iteration 101/1000 | Loss: 0.00002251
Iteration 102/1000 | Loss: 0.00002251
Iteration 103/1000 | Loss: 0.00002251
Iteration 104/1000 | Loss: 0.00002251
Iteration 105/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.2514816009788774e-05, 2.2514816009788774e-05, 2.2514816009788774e-05, 2.2514816009788774e-05, 2.2514816009788774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2514816009788774e-05

Optimization complete. Final v2v error: 4.089544296264648 mm

Highest mean error: 4.313518047332764 mm for frame 157

Lowest mean error: 3.8697521686553955 mm for frame 17

Saving results

Total time: 35.90805268287659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456223
Iteration 2/25 | Loss: 0.00155967
Iteration 3/25 | Loss: 0.00147997
Iteration 4/25 | Loss: 0.00146379
Iteration 5/25 | Loss: 0.00146021
Iteration 6/25 | Loss: 0.00146006
Iteration 7/25 | Loss: 0.00146006
Iteration 8/25 | Loss: 0.00146006
Iteration 9/25 | Loss: 0.00146006
Iteration 10/25 | Loss: 0.00146006
Iteration 11/25 | Loss: 0.00146006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014600602444261312, 0.0014600602444261312, 0.0014600602444261312, 0.0014600602444261312, 0.0014600602444261312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014600602444261312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78860080
Iteration 2/25 | Loss: 0.00164952
Iteration 3/25 | Loss: 0.00164952
Iteration 4/25 | Loss: 0.00164952
Iteration 5/25 | Loss: 0.00164952
Iteration 6/25 | Loss: 0.00164952
Iteration 7/25 | Loss: 0.00164952
Iteration 8/25 | Loss: 0.00164952
Iteration 9/25 | Loss: 0.00164952
Iteration 10/25 | Loss: 0.00164952
Iteration 11/25 | Loss: 0.00164952
Iteration 12/25 | Loss: 0.00164952
Iteration 13/25 | Loss: 0.00164952
Iteration 14/25 | Loss: 0.00164952
Iteration 15/25 | Loss: 0.00164952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016495203599333763, 0.0016495203599333763, 0.0016495203599333763, 0.0016495203599333763, 0.0016495203599333763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016495203599333763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164952
Iteration 2/1000 | Loss: 0.00005052
Iteration 3/1000 | Loss: 0.00003663
Iteration 4/1000 | Loss: 0.00003191
Iteration 5/1000 | Loss: 0.00003070
Iteration 6/1000 | Loss: 0.00002959
Iteration 7/1000 | Loss: 0.00002874
Iteration 8/1000 | Loss: 0.00002832
Iteration 9/1000 | Loss: 0.00002794
Iteration 10/1000 | Loss: 0.00002765
Iteration 11/1000 | Loss: 0.00002764
Iteration 12/1000 | Loss: 0.00002741
Iteration 13/1000 | Loss: 0.00002716
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002698
Iteration 16/1000 | Loss: 0.00002698
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002694
Iteration 19/1000 | Loss: 0.00002693
Iteration 20/1000 | Loss: 0.00002691
Iteration 21/1000 | Loss: 0.00002691
Iteration 22/1000 | Loss: 0.00002691
Iteration 23/1000 | Loss: 0.00002691
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002691
Iteration 26/1000 | Loss: 0.00002691
Iteration 27/1000 | Loss: 0.00002691
Iteration 28/1000 | Loss: 0.00002691
Iteration 29/1000 | Loss: 0.00002691
Iteration 30/1000 | Loss: 0.00002690
Iteration 31/1000 | Loss: 0.00002690
Iteration 32/1000 | Loss: 0.00002689
Iteration 33/1000 | Loss: 0.00002689
Iteration 34/1000 | Loss: 0.00002688
Iteration 35/1000 | Loss: 0.00002688
Iteration 36/1000 | Loss: 0.00002688
Iteration 37/1000 | Loss: 0.00002687
Iteration 38/1000 | Loss: 0.00002687
Iteration 39/1000 | Loss: 0.00002687
Iteration 40/1000 | Loss: 0.00002687
Iteration 41/1000 | Loss: 0.00002686
Iteration 42/1000 | Loss: 0.00002686
Iteration 43/1000 | Loss: 0.00002686
Iteration 44/1000 | Loss: 0.00002686
Iteration 45/1000 | Loss: 0.00002686
Iteration 46/1000 | Loss: 0.00002685
Iteration 47/1000 | Loss: 0.00002685
Iteration 48/1000 | Loss: 0.00002685
Iteration 49/1000 | Loss: 0.00002685
Iteration 50/1000 | Loss: 0.00002684
Iteration 51/1000 | Loss: 0.00002684
Iteration 52/1000 | Loss: 0.00002684
Iteration 53/1000 | Loss: 0.00002684
Iteration 54/1000 | Loss: 0.00002684
Iteration 55/1000 | Loss: 0.00002684
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00002684
Iteration 58/1000 | Loss: 0.00002684
Iteration 59/1000 | Loss: 0.00002683
Iteration 60/1000 | Loss: 0.00002683
Iteration 61/1000 | Loss: 0.00002683
Iteration 62/1000 | Loss: 0.00002682
Iteration 63/1000 | Loss: 0.00002682
Iteration 64/1000 | Loss: 0.00002681
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002681
Iteration 67/1000 | Loss: 0.00002681
Iteration 68/1000 | Loss: 0.00002681
Iteration 69/1000 | Loss: 0.00002680
Iteration 70/1000 | Loss: 0.00002680
Iteration 71/1000 | Loss: 0.00002680
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002680
Iteration 74/1000 | Loss: 0.00002679
Iteration 75/1000 | Loss: 0.00002679
Iteration 76/1000 | Loss: 0.00002679
Iteration 77/1000 | Loss: 0.00002679
Iteration 78/1000 | Loss: 0.00002679
Iteration 79/1000 | Loss: 0.00002678
Iteration 80/1000 | Loss: 0.00002678
Iteration 81/1000 | Loss: 0.00002678
Iteration 82/1000 | Loss: 0.00002678
Iteration 83/1000 | Loss: 0.00002678
Iteration 84/1000 | Loss: 0.00002678
Iteration 85/1000 | Loss: 0.00002677
Iteration 86/1000 | Loss: 0.00002677
Iteration 87/1000 | Loss: 0.00002677
Iteration 88/1000 | Loss: 0.00002677
Iteration 89/1000 | Loss: 0.00002677
Iteration 90/1000 | Loss: 0.00002677
Iteration 91/1000 | Loss: 0.00002677
Iteration 92/1000 | Loss: 0.00002677
Iteration 93/1000 | Loss: 0.00002677
Iteration 94/1000 | Loss: 0.00002677
Iteration 95/1000 | Loss: 0.00002677
Iteration 96/1000 | Loss: 0.00002676
Iteration 97/1000 | Loss: 0.00002676
Iteration 98/1000 | Loss: 0.00002676
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002676
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002676
Iteration 103/1000 | Loss: 0.00002676
Iteration 104/1000 | Loss: 0.00002676
Iteration 105/1000 | Loss: 0.00002675
Iteration 106/1000 | Loss: 0.00002675
Iteration 107/1000 | Loss: 0.00002675
Iteration 108/1000 | Loss: 0.00002675
Iteration 109/1000 | Loss: 0.00002675
Iteration 110/1000 | Loss: 0.00002674
Iteration 111/1000 | Loss: 0.00002674
Iteration 112/1000 | Loss: 0.00002674
Iteration 113/1000 | Loss: 0.00002674
Iteration 114/1000 | Loss: 0.00002674
Iteration 115/1000 | Loss: 0.00002674
Iteration 116/1000 | Loss: 0.00002673
Iteration 117/1000 | Loss: 0.00002673
Iteration 118/1000 | Loss: 0.00002673
Iteration 119/1000 | Loss: 0.00002673
Iteration 120/1000 | Loss: 0.00002673
Iteration 121/1000 | Loss: 0.00002673
Iteration 122/1000 | Loss: 0.00002673
Iteration 123/1000 | Loss: 0.00002672
Iteration 124/1000 | Loss: 0.00002672
Iteration 125/1000 | Loss: 0.00002672
Iteration 126/1000 | Loss: 0.00002672
Iteration 127/1000 | Loss: 0.00002672
Iteration 128/1000 | Loss: 0.00002672
Iteration 129/1000 | Loss: 0.00002672
Iteration 130/1000 | Loss: 0.00002672
Iteration 131/1000 | Loss: 0.00002672
Iteration 132/1000 | Loss: 0.00002672
Iteration 133/1000 | Loss: 0.00002672
Iteration 134/1000 | Loss: 0.00002672
Iteration 135/1000 | Loss: 0.00002672
Iteration 136/1000 | Loss: 0.00002672
Iteration 137/1000 | Loss: 0.00002672
Iteration 138/1000 | Loss: 0.00002672
Iteration 139/1000 | Loss: 0.00002672
Iteration 140/1000 | Loss: 0.00002672
Iteration 141/1000 | Loss: 0.00002672
Iteration 142/1000 | Loss: 0.00002672
Iteration 143/1000 | Loss: 0.00002672
Iteration 144/1000 | Loss: 0.00002672
Iteration 145/1000 | Loss: 0.00002672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.6716355932876468e-05, 2.6716355932876468e-05, 2.6716355932876468e-05, 2.6716355932876468e-05, 2.6716355932876468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6716355932876468e-05

Optimization complete. Final v2v error: 4.561121463775635 mm

Highest mean error: 4.882718563079834 mm for frame 36

Lowest mean error: 4.291460037231445 mm for frame 266

Saving results

Total time: 40.40775465965271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488578
Iteration 2/25 | Loss: 0.00154774
Iteration 3/25 | Loss: 0.00144355
Iteration 4/25 | Loss: 0.00142491
Iteration 5/25 | Loss: 0.00141809
Iteration 6/25 | Loss: 0.00141646
Iteration 7/25 | Loss: 0.00141599
Iteration 8/25 | Loss: 0.00141599
Iteration 9/25 | Loss: 0.00141599
Iteration 10/25 | Loss: 0.00141599
Iteration 11/25 | Loss: 0.00141599
Iteration 12/25 | Loss: 0.00141599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001415987266227603, 0.001415987266227603, 0.001415987266227603, 0.001415987266227603, 0.001415987266227603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001415987266227603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60112691
Iteration 2/25 | Loss: 0.00124824
Iteration 3/25 | Loss: 0.00124824
Iteration 4/25 | Loss: 0.00124824
Iteration 5/25 | Loss: 0.00124824
Iteration 6/25 | Loss: 0.00124824
Iteration 7/25 | Loss: 0.00124824
Iteration 8/25 | Loss: 0.00124824
Iteration 9/25 | Loss: 0.00124824
Iteration 10/25 | Loss: 0.00124824
Iteration 11/25 | Loss: 0.00124824
Iteration 12/25 | Loss: 0.00124824
Iteration 13/25 | Loss: 0.00124824
Iteration 14/25 | Loss: 0.00124824
Iteration 15/25 | Loss: 0.00124824
Iteration 16/25 | Loss: 0.00124824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012482400052249432, 0.0012482400052249432, 0.0012482400052249432, 0.0012482400052249432, 0.0012482400052249432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012482400052249432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124824
Iteration 2/1000 | Loss: 0.00004684
Iteration 3/1000 | Loss: 0.00003636
Iteration 4/1000 | Loss: 0.00003231
Iteration 5/1000 | Loss: 0.00003042
Iteration 6/1000 | Loss: 0.00002957
Iteration 7/1000 | Loss: 0.00002898
Iteration 8/1000 | Loss: 0.00002836
Iteration 9/1000 | Loss: 0.00002793
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00002749
Iteration 12/1000 | Loss: 0.00002732
Iteration 13/1000 | Loss: 0.00002731
Iteration 14/1000 | Loss: 0.00002726
Iteration 15/1000 | Loss: 0.00002725
Iteration 16/1000 | Loss: 0.00002725
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002724
Iteration 19/1000 | Loss: 0.00002723
Iteration 20/1000 | Loss: 0.00002722
Iteration 21/1000 | Loss: 0.00002717
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00002717
Iteration 24/1000 | Loss: 0.00002717
Iteration 25/1000 | Loss: 0.00002717
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002717
Iteration 28/1000 | Loss: 0.00002717
Iteration 29/1000 | Loss: 0.00002716
Iteration 30/1000 | Loss: 0.00002716
Iteration 31/1000 | Loss: 0.00002716
Iteration 32/1000 | Loss: 0.00002712
Iteration 33/1000 | Loss: 0.00002712
Iteration 34/1000 | Loss: 0.00002711
Iteration 35/1000 | Loss: 0.00002711
Iteration 36/1000 | Loss: 0.00002711
Iteration 37/1000 | Loss: 0.00002709
Iteration 38/1000 | Loss: 0.00002708
Iteration 39/1000 | Loss: 0.00002708
Iteration 40/1000 | Loss: 0.00002707
Iteration 41/1000 | Loss: 0.00002707
Iteration 42/1000 | Loss: 0.00002707
Iteration 43/1000 | Loss: 0.00002706
Iteration 44/1000 | Loss: 0.00002706
Iteration 45/1000 | Loss: 0.00002706
Iteration 46/1000 | Loss: 0.00002706
Iteration 47/1000 | Loss: 0.00002706
Iteration 48/1000 | Loss: 0.00002705
Iteration 49/1000 | Loss: 0.00002705
Iteration 50/1000 | Loss: 0.00002705
Iteration 51/1000 | Loss: 0.00002704
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00002704
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002704
Iteration 56/1000 | Loss: 0.00002704
Iteration 57/1000 | Loss: 0.00002704
Iteration 58/1000 | Loss: 0.00002703
Iteration 59/1000 | Loss: 0.00002703
Iteration 60/1000 | Loss: 0.00002703
Iteration 61/1000 | Loss: 0.00002703
Iteration 62/1000 | Loss: 0.00002703
Iteration 63/1000 | Loss: 0.00002703
Iteration 64/1000 | Loss: 0.00002703
Iteration 65/1000 | Loss: 0.00002702
Iteration 66/1000 | Loss: 0.00002702
Iteration 67/1000 | Loss: 0.00002702
Iteration 68/1000 | Loss: 0.00002702
Iteration 69/1000 | Loss: 0.00002702
Iteration 70/1000 | Loss: 0.00002702
Iteration 71/1000 | Loss: 0.00002702
Iteration 72/1000 | Loss: 0.00002702
Iteration 73/1000 | Loss: 0.00002702
Iteration 74/1000 | Loss: 0.00002701
Iteration 75/1000 | Loss: 0.00002701
Iteration 76/1000 | Loss: 0.00002701
Iteration 77/1000 | Loss: 0.00002701
Iteration 78/1000 | Loss: 0.00002701
Iteration 79/1000 | Loss: 0.00002701
Iteration 80/1000 | Loss: 0.00002701
Iteration 81/1000 | Loss: 0.00002701
Iteration 82/1000 | Loss: 0.00002701
Iteration 83/1000 | Loss: 0.00002701
Iteration 84/1000 | Loss: 0.00002701
Iteration 85/1000 | Loss: 0.00002700
Iteration 86/1000 | Loss: 0.00002700
Iteration 87/1000 | Loss: 0.00002700
Iteration 88/1000 | Loss: 0.00002700
Iteration 89/1000 | Loss: 0.00002700
Iteration 90/1000 | Loss: 0.00002700
Iteration 91/1000 | Loss: 0.00002700
Iteration 92/1000 | Loss: 0.00002700
Iteration 93/1000 | Loss: 0.00002700
Iteration 94/1000 | Loss: 0.00002700
Iteration 95/1000 | Loss: 0.00002700
Iteration 96/1000 | Loss: 0.00002700
Iteration 97/1000 | Loss: 0.00002700
Iteration 98/1000 | Loss: 0.00002699
Iteration 99/1000 | Loss: 0.00002699
Iteration 100/1000 | Loss: 0.00002699
Iteration 101/1000 | Loss: 0.00002699
Iteration 102/1000 | Loss: 0.00002699
Iteration 103/1000 | Loss: 0.00002699
Iteration 104/1000 | Loss: 0.00002699
Iteration 105/1000 | Loss: 0.00002699
Iteration 106/1000 | Loss: 0.00002699
Iteration 107/1000 | Loss: 0.00002699
Iteration 108/1000 | Loss: 0.00002699
Iteration 109/1000 | Loss: 0.00002699
Iteration 110/1000 | Loss: 0.00002698
Iteration 111/1000 | Loss: 0.00002698
Iteration 112/1000 | Loss: 0.00002698
Iteration 113/1000 | Loss: 0.00002698
Iteration 114/1000 | Loss: 0.00002698
Iteration 115/1000 | Loss: 0.00002698
Iteration 116/1000 | Loss: 0.00002698
Iteration 117/1000 | Loss: 0.00002698
Iteration 118/1000 | Loss: 0.00002698
Iteration 119/1000 | Loss: 0.00002698
Iteration 120/1000 | Loss: 0.00002697
Iteration 121/1000 | Loss: 0.00002697
Iteration 122/1000 | Loss: 0.00002697
Iteration 123/1000 | Loss: 0.00002697
Iteration 124/1000 | Loss: 0.00002697
Iteration 125/1000 | Loss: 0.00002697
Iteration 126/1000 | Loss: 0.00002697
Iteration 127/1000 | Loss: 0.00002697
Iteration 128/1000 | Loss: 0.00002697
Iteration 129/1000 | Loss: 0.00002697
Iteration 130/1000 | Loss: 0.00002697
Iteration 131/1000 | Loss: 0.00002697
Iteration 132/1000 | Loss: 0.00002696
Iteration 133/1000 | Loss: 0.00002696
Iteration 134/1000 | Loss: 0.00002696
Iteration 135/1000 | Loss: 0.00002696
Iteration 136/1000 | Loss: 0.00002696
Iteration 137/1000 | Loss: 0.00002696
Iteration 138/1000 | Loss: 0.00002696
Iteration 139/1000 | Loss: 0.00002696
Iteration 140/1000 | Loss: 0.00002696
Iteration 141/1000 | Loss: 0.00002696
Iteration 142/1000 | Loss: 0.00002696
Iteration 143/1000 | Loss: 0.00002696
Iteration 144/1000 | Loss: 0.00002696
Iteration 145/1000 | Loss: 0.00002695
Iteration 146/1000 | Loss: 0.00002695
Iteration 147/1000 | Loss: 0.00002695
Iteration 148/1000 | Loss: 0.00002695
Iteration 149/1000 | Loss: 0.00002695
Iteration 150/1000 | Loss: 0.00002695
Iteration 151/1000 | Loss: 0.00002695
Iteration 152/1000 | Loss: 0.00002695
Iteration 153/1000 | Loss: 0.00002694
Iteration 154/1000 | Loss: 0.00002694
Iteration 155/1000 | Loss: 0.00002694
Iteration 156/1000 | Loss: 0.00002694
Iteration 157/1000 | Loss: 0.00002694
Iteration 158/1000 | Loss: 0.00002694
Iteration 159/1000 | Loss: 0.00002694
Iteration 160/1000 | Loss: 0.00002694
Iteration 161/1000 | Loss: 0.00002694
Iteration 162/1000 | Loss: 0.00002694
Iteration 163/1000 | Loss: 0.00002694
Iteration 164/1000 | Loss: 0.00002694
Iteration 165/1000 | Loss: 0.00002693
Iteration 166/1000 | Loss: 0.00002693
Iteration 167/1000 | Loss: 0.00002693
Iteration 168/1000 | Loss: 0.00002693
Iteration 169/1000 | Loss: 0.00002693
Iteration 170/1000 | Loss: 0.00002693
Iteration 171/1000 | Loss: 0.00002693
Iteration 172/1000 | Loss: 0.00002693
Iteration 173/1000 | Loss: 0.00002693
Iteration 174/1000 | Loss: 0.00002693
Iteration 175/1000 | Loss: 0.00002693
Iteration 176/1000 | Loss: 0.00002693
Iteration 177/1000 | Loss: 0.00002693
Iteration 178/1000 | Loss: 0.00002693
Iteration 179/1000 | Loss: 0.00002693
Iteration 180/1000 | Loss: 0.00002693
Iteration 181/1000 | Loss: 0.00002693
Iteration 182/1000 | Loss: 0.00002693
Iteration 183/1000 | Loss: 0.00002692
Iteration 184/1000 | Loss: 0.00002692
Iteration 185/1000 | Loss: 0.00002692
Iteration 186/1000 | Loss: 0.00002692
Iteration 187/1000 | Loss: 0.00002692
Iteration 188/1000 | Loss: 0.00002692
Iteration 189/1000 | Loss: 0.00002692
Iteration 190/1000 | Loss: 0.00002692
Iteration 191/1000 | Loss: 0.00002692
Iteration 192/1000 | Loss: 0.00002692
Iteration 193/1000 | Loss: 0.00002692
Iteration 194/1000 | Loss: 0.00002692
Iteration 195/1000 | Loss: 0.00002692
Iteration 196/1000 | Loss: 0.00002692
Iteration 197/1000 | Loss: 0.00002692
Iteration 198/1000 | Loss: 0.00002692
Iteration 199/1000 | Loss: 0.00002692
Iteration 200/1000 | Loss: 0.00002692
Iteration 201/1000 | Loss: 0.00002692
Iteration 202/1000 | Loss: 0.00002692
Iteration 203/1000 | Loss: 0.00002692
Iteration 204/1000 | Loss: 0.00002692
Iteration 205/1000 | Loss: 0.00002692
Iteration 206/1000 | Loss: 0.00002692
Iteration 207/1000 | Loss: 0.00002692
Iteration 208/1000 | Loss: 0.00002692
Iteration 209/1000 | Loss: 0.00002692
Iteration 210/1000 | Loss: 0.00002692
Iteration 211/1000 | Loss: 0.00002692
Iteration 212/1000 | Loss: 0.00002692
Iteration 213/1000 | Loss: 0.00002692
Iteration 214/1000 | Loss: 0.00002692
Iteration 215/1000 | Loss: 0.00002692
Iteration 216/1000 | Loss: 0.00002692
Iteration 217/1000 | Loss: 0.00002692
Iteration 218/1000 | Loss: 0.00002692
Iteration 219/1000 | Loss: 0.00002692
Iteration 220/1000 | Loss: 0.00002692
Iteration 221/1000 | Loss: 0.00002692
Iteration 222/1000 | Loss: 0.00002692
Iteration 223/1000 | Loss: 0.00002692
Iteration 224/1000 | Loss: 0.00002692
Iteration 225/1000 | Loss: 0.00002692
Iteration 226/1000 | Loss: 0.00002692
Iteration 227/1000 | Loss: 0.00002692
Iteration 228/1000 | Loss: 0.00002692
Iteration 229/1000 | Loss: 0.00002692
Iteration 230/1000 | Loss: 0.00002692
Iteration 231/1000 | Loss: 0.00002692
Iteration 232/1000 | Loss: 0.00002692
Iteration 233/1000 | Loss: 0.00002692
Iteration 234/1000 | Loss: 0.00002692
Iteration 235/1000 | Loss: 0.00002692
Iteration 236/1000 | Loss: 0.00002692
Iteration 237/1000 | Loss: 0.00002692
Iteration 238/1000 | Loss: 0.00002692
Iteration 239/1000 | Loss: 0.00002692
Iteration 240/1000 | Loss: 0.00002692
Iteration 241/1000 | Loss: 0.00002692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.691911686270032e-05, 2.691911686270032e-05, 2.691911686270032e-05, 2.691911686270032e-05, 2.691911686270032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.691911686270032e-05

Optimization complete. Final v2v error: 4.4660515785217285 mm

Highest mean error: 5.188648223876953 mm for frame 47

Lowest mean error: 4.175846576690674 mm for frame 20

Saving results

Total time: 39.92873287200928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081767
Iteration 2/25 | Loss: 0.00254469
Iteration 3/25 | Loss: 0.00190906
Iteration 4/25 | Loss: 0.00178588
Iteration 5/25 | Loss: 0.00201299
Iteration 6/25 | Loss: 0.00170010
Iteration 7/25 | Loss: 0.00154697
Iteration 8/25 | Loss: 0.00150531
Iteration 9/25 | Loss: 0.00144090
Iteration 10/25 | Loss: 0.00140552
Iteration 11/25 | Loss: 0.00138695
Iteration 12/25 | Loss: 0.00136657
Iteration 13/25 | Loss: 0.00135596
Iteration 14/25 | Loss: 0.00135292
Iteration 15/25 | Loss: 0.00135557
Iteration 16/25 | Loss: 0.00135574
Iteration 17/25 | Loss: 0.00136050
Iteration 18/25 | Loss: 0.00136137
Iteration 19/25 | Loss: 0.00135476
Iteration 20/25 | Loss: 0.00135503
Iteration 21/25 | Loss: 0.00134922
Iteration 22/25 | Loss: 0.00134431
Iteration 23/25 | Loss: 0.00134378
Iteration 24/25 | Loss: 0.00134346
Iteration 25/25 | Loss: 0.00133869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49732351
Iteration 2/25 | Loss: 0.00147055
Iteration 3/25 | Loss: 0.00143523
Iteration 4/25 | Loss: 0.00143522
Iteration 5/25 | Loss: 0.00143522
Iteration 6/25 | Loss: 0.00143522
Iteration 7/25 | Loss: 0.00143522
Iteration 8/25 | Loss: 0.00143522
Iteration 9/25 | Loss: 0.00143522
Iteration 10/25 | Loss: 0.00143522
Iteration 11/25 | Loss: 0.00143522
Iteration 12/25 | Loss: 0.00143522
Iteration 13/25 | Loss: 0.00143522
Iteration 14/25 | Loss: 0.00143522
Iteration 15/25 | Loss: 0.00143522
Iteration 16/25 | Loss: 0.00143522
Iteration 17/25 | Loss: 0.00143522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001435222802683711, 0.001435222802683711, 0.001435222802683711, 0.001435222802683711, 0.001435222802683711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001435222802683711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143522
Iteration 2/1000 | Loss: 0.00006772
Iteration 3/1000 | Loss: 0.00004165
Iteration 4/1000 | Loss: 0.00003409
Iteration 5/1000 | Loss: 0.00003065
Iteration 6/1000 | Loss: 0.00040457
Iteration 7/1000 | Loss: 0.00022015
Iteration 8/1000 | Loss: 0.00011633
Iteration 9/1000 | Loss: 0.00010728
Iteration 10/1000 | Loss: 0.00002972
Iteration 11/1000 | Loss: 0.00036913
Iteration 12/1000 | Loss: 0.00016745
Iteration 13/1000 | Loss: 0.00010702
Iteration 14/1000 | Loss: 0.00018875
Iteration 15/1000 | Loss: 0.00070810
Iteration 16/1000 | Loss: 0.00016519
Iteration 17/1000 | Loss: 0.00003941
Iteration 18/1000 | Loss: 0.00003265
Iteration 19/1000 | Loss: 0.00003018
Iteration 20/1000 | Loss: 0.00011716
Iteration 21/1000 | Loss: 0.00002703
Iteration 22/1000 | Loss: 0.00002549
Iteration 23/1000 | Loss: 0.00002419
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00002278
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002237
Iteration 28/1000 | Loss: 0.00002221
Iteration 29/1000 | Loss: 0.00002219
Iteration 30/1000 | Loss: 0.00012646
Iteration 31/1000 | Loss: 0.00004619
Iteration 32/1000 | Loss: 0.00003297
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002213
Iteration 35/1000 | Loss: 0.00002211
Iteration 36/1000 | Loss: 0.00002210
Iteration 37/1000 | Loss: 0.00002209
Iteration 38/1000 | Loss: 0.00002209
Iteration 39/1000 | Loss: 0.00002209
Iteration 40/1000 | Loss: 0.00002209
Iteration 41/1000 | Loss: 0.00002209
Iteration 42/1000 | Loss: 0.00002209
Iteration 43/1000 | Loss: 0.00002208
Iteration 44/1000 | Loss: 0.00002208
Iteration 45/1000 | Loss: 0.00002208
Iteration 46/1000 | Loss: 0.00002208
Iteration 47/1000 | Loss: 0.00002208
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002205
Iteration 59/1000 | Loss: 0.00002205
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002203
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002203
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002203
Iteration 73/1000 | Loss: 0.00002202
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002202
Iteration 76/1000 | Loss: 0.00002201
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002199
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002199
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002198
Iteration 84/1000 | Loss: 0.00002198
Iteration 85/1000 | Loss: 0.00002198
Iteration 86/1000 | Loss: 0.00002198
Iteration 87/1000 | Loss: 0.00002198
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002198
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002198
Iteration 96/1000 | Loss: 0.00002198
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002197
Iteration 99/1000 | Loss: 0.00002197
Iteration 100/1000 | Loss: 0.00002197
Iteration 101/1000 | Loss: 0.00002197
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00002197
Iteration 104/1000 | Loss: 0.00002197
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002197
Iteration 108/1000 | Loss: 0.00002197
Iteration 109/1000 | Loss: 0.00002197
Iteration 110/1000 | Loss: 0.00002197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.197379944846034e-05, 2.197379944846034e-05, 2.197379944846034e-05, 2.197379944846034e-05, 2.197379944846034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.197379944846034e-05

Optimization complete. Final v2v error: 3.936955690383911 mm

Highest mean error: 10.76933765411377 mm for frame 79

Lowest mean error: 3.578047037124634 mm for frame 28

Saving results

Total time: 94.78883504867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0082/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0082/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971914
Iteration 2/25 | Loss: 0.00205523
Iteration 3/25 | Loss: 0.00180111
Iteration 4/25 | Loss: 0.00177025
Iteration 5/25 | Loss: 0.00176231
Iteration 6/25 | Loss: 0.00175927
Iteration 7/25 | Loss: 0.00175312
Iteration 8/25 | Loss: 0.00174328
Iteration 9/25 | Loss: 0.00174625
Iteration 10/25 | Loss: 0.00174080
Iteration 11/25 | Loss: 0.00173788
Iteration 12/25 | Loss: 0.00173698
Iteration 13/25 | Loss: 0.00173677
Iteration 14/25 | Loss: 0.00173677
Iteration 15/25 | Loss: 0.00173677
Iteration 16/25 | Loss: 0.00173676
Iteration 17/25 | Loss: 0.00173676
Iteration 18/25 | Loss: 0.00173676
Iteration 19/25 | Loss: 0.00173676
Iteration 20/25 | Loss: 0.00173676
Iteration 21/25 | Loss: 0.00173676
Iteration 22/25 | Loss: 0.00173676
Iteration 23/25 | Loss: 0.00173676
Iteration 24/25 | Loss: 0.00173676
Iteration 25/25 | Loss: 0.00173676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44310844
Iteration 2/25 | Loss: 0.00385047
Iteration 3/25 | Loss: 0.00385041
Iteration 4/25 | Loss: 0.00385040
Iteration 5/25 | Loss: 0.00385040
Iteration 6/25 | Loss: 0.00385040
Iteration 7/25 | Loss: 0.00385040
Iteration 8/25 | Loss: 0.00385040
Iteration 9/25 | Loss: 0.00385040
Iteration 10/25 | Loss: 0.00385040
Iteration 11/25 | Loss: 0.00385040
Iteration 12/25 | Loss: 0.00385040
Iteration 13/25 | Loss: 0.00385040
Iteration 14/25 | Loss: 0.00385040
Iteration 15/25 | Loss: 0.00385040
Iteration 16/25 | Loss: 0.00385040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003850401844829321, 0.003850401844829321, 0.003850401844829321, 0.003850401844829321, 0.003850401844829321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003850401844829321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385040
Iteration 2/1000 | Loss: 0.00055806
Iteration 3/1000 | Loss: 0.00035753
Iteration 4/1000 | Loss: 0.00055025
Iteration 5/1000 | Loss: 0.00146033
Iteration 6/1000 | Loss: 0.00088400
Iteration 7/1000 | Loss: 0.00061374
Iteration 8/1000 | Loss: 0.00187774
Iteration 9/1000 | Loss: 0.00016378
Iteration 10/1000 | Loss: 0.00076810
Iteration 11/1000 | Loss: 0.00026879
Iteration 12/1000 | Loss: 0.00008830
Iteration 13/1000 | Loss: 0.00007781
Iteration 14/1000 | Loss: 0.00061895
Iteration 15/1000 | Loss: 0.00008276
Iteration 16/1000 | Loss: 0.00035734
Iteration 17/1000 | Loss: 0.00007382
Iteration 18/1000 | Loss: 0.00006178
Iteration 19/1000 | Loss: 0.00005436
Iteration 20/1000 | Loss: 0.00004815
Iteration 21/1000 | Loss: 0.00004477
Iteration 22/1000 | Loss: 0.00004238
Iteration 23/1000 | Loss: 0.00122973
Iteration 24/1000 | Loss: 0.00004612
Iteration 25/1000 | Loss: 0.00004097
Iteration 26/1000 | Loss: 0.00003862
Iteration 27/1000 | Loss: 0.00003644
Iteration 28/1000 | Loss: 0.00003508
Iteration 29/1000 | Loss: 0.00003412
Iteration 30/1000 | Loss: 0.00003366
Iteration 31/1000 | Loss: 0.00003330
Iteration 32/1000 | Loss: 0.00003307
Iteration 33/1000 | Loss: 0.00003285
Iteration 34/1000 | Loss: 0.00003275
Iteration 35/1000 | Loss: 0.00003273
Iteration 36/1000 | Loss: 0.00003266
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00003262
Iteration 39/1000 | Loss: 0.00003260
Iteration 40/1000 | Loss: 0.00003260
Iteration 41/1000 | Loss: 0.00003259
Iteration 42/1000 | Loss: 0.00003259
Iteration 43/1000 | Loss: 0.00003259
Iteration 44/1000 | Loss: 0.00003259
Iteration 45/1000 | Loss: 0.00003255
Iteration 46/1000 | Loss: 0.00003254
Iteration 47/1000 | Loss: 0.00003249
Iteration 48/1000 | Loss: 0.00003249
Iteration 49/1000 | Loss: 0.00003246
Iteration 50/1000 | Loss: 0.00003243
Iteration 51/1000 | Loss: 0.00003242
Iteration 52/1000 | Loss: 0.00003242
Iteration 53/1000 | Loss: 0.00003241
Iteration 54/1000 | Loss: 0.00003241
Iteration 55/1000 | Loss: 0.00003236
Iteration 56/1000 | Loss: 0.00003236
Iteration 57/1000 | Loss: 0.00003236
Iteration 58/1000 | Loss: 0.00003235
Iteration 59/1000 | Loss: 0.00003235
Iteration 60/1000 | Loss: 0.00003235
Iteration 61/1000 | Loss: 0.00003231
Iteration 62/1000 | Loss: 0.00003229
Iteration 63/1000 | Loss: 0.00003229
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003229
Iteration 66/1000 | Loss: 0.00003229
Iteration 67/1000 | Loss: 0.00003228
Iteration 68/1000 | Loss: 0.00003228
Iteration 69/1000 | Loss: 0.00003228
Iteration 70/1000 | Loss: 0.00003228
Iteration 71/1000 | Loss: 0.00003227
Iteration 72/1000 | Loss: 0.00003227
Iteration 73/1000 | Loss: 0.00003227
Iteration 74/1000 | Loss: 0.00003227
Iteration 75/1000 | Loss: 0.00003227
Iteration 76/1000 | Loss: 0.00003227
Iteration 77/1000 | Loss: 0.00003226
Iteration 78/1000 | Loss: 0.00003226
Iteration 79/1000 | Loss: 0.00003226
Iteration 80/1000 | Loss: 0.00003226
Iteration 81/1000 | Loss: 0.00003226
Iteration 82/1000 | Loss: 0.00003226
Iteration 83/1000 | Loss: 0.00003225
Iteration 84/1000 | Loss: 0.00003225
Iteration 85/1000 | Loss: 0.00003225
Iteration 86/1000 | Loss: 0.00003225
Iteration 87/1000 | Loss: 0.00003225
Iteration 88/1000 | Loss: 0.00003225
Iteration 89/1000 | Loss: 0.00003225
Iteration 90/1000 | Loss: 0.00003224
Iteration 91/1000 | Loss: 0.00003224
Iteration 92/1000 | Loss: 0.00003224
Iteration 93/1000 | Loss: 0.00003224
Iteration 94/1000 | Loss: 0.00003224
Iteration 95/1000 | Loss: 0.00003224
Iteration 96/1000 | Loss: 0.00003224
Iteration 97/1000 | Loss: 0.00003224
Iteration 98/1000 | Loss: 0.00003223
Iteration 99/1000 | Loss: 0.00003223
Iteration 100/1000 | Loss: 0.00003223
Iteration 101/1000 | Loss: 0.00003223
Iteration 102/1000 | Loss: 0.00003222
Iteration 103/1000 | Loss: 0.00003222
Iteration 104/1000 | Loss: 0.00003222
Iteration 105/1000 | Loss: 0.00003222
Iteration 106/1000 | Loss: 0.00003222
Iteration 107/1000 | Loss: 0.00003221
Iteration 108/1000 | Loss: 0.00003221
Iteration 109/1000 | Loss: 0.00003221
Iteration 110/1000 | Loss: 0.00003221
Iteration 111/1000 | Loss: 0.00003221
Iteration 112/1000 | Loss: 0.00003221
Iteration 113/1000 | Loss: 0.00003221
Iteration 114/1000 | Loss: 0.00003221
Iteration 115/1000 | Loss: 0.00003221
Iteration 116/1000 | Loss: 0.00003221
Iteration 117/1000 | Loss: 0.00003221
Iteration 118/1000 | Loss: 0.00003221
Iteration 119/1000 | Loss: 0.00003221
Iteration 120/1000 | Loss: 0.00003221
Iteration 121/1000 | Loss: 0.00003221
Iteration 122/1000 | Loss: 0.00003221
Iteration 123/1000 | Loss: 0.00003221
Iteration 124/1000 | Loss: 0.00003221
Iteration 125/1000 | Loss: 0.00003221
Iteration 126/1000 | Loss: 0.00003221
Iteration 127/1000 | Loss: 0.00003221
Iteration 128/1000 | Loss: 0.00003221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [3.220880898879841e-05, 3.220880898879841e-05, 3.220880898879841e-05, 3.220880898879841e-05, 3.220880898879841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.220880898879841e-05

Optimization complete. Final v2v error: 4.846072196960449 mm

Highest mean error: 5.504737377166748 mm for frame 104

Lowest mean error: 4.063724517822266 mm for frame 8

Saving results

Total time: 79.65902662277222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758756
Iteration 2/25 | Loss: 0.00167891
Iteration 3/25 | Loss: 0.00110465
Iteration 4/25 | Loss: 0.00095782
Iteration 5/25 | Loss: 0.00094136
Iteration 6/25 | Loss: 0.00091342
Iteration 7/25 | Loss: 0.00090355
Iteration 8/25 | Loss: 0.00090714
Iteration 9/25 | Loss: 0.00089999
Iteration 10/25 | Loss: 0.00089778
Iteration 11/25 | Loss: 0.00089728
Iteration 12/25 | Loss: 0.00089702
Iteration 13/25 | Loss: 0.00089732
Iteration 14/25 | Loss: 0.00089692
Iteration 15/25 | Loss: 0.00089724
Iteration 16/25 | Loss: 0.00089687
Iteration 17/25 | Loss: 0.00089734
Iteration 18/25 | Loss: 0.00089695
Iteration 19/25 | Loss: 0.00089741
Iteration 20/25 | Loss: 0.00089695
Iteration 21/25 | Loss: 0.00089733
Iteration 22/25 | Loss: 0.00089693
Iteration 23/25 | Loss: 0.00089732
Iteration 24/25 | Loss: 0.00089695
Iteration 25/25 | Loss: 0.00089728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.48333740
Iteration 2/25 | Loss: 0.00045258
Iteration 3/25 | Loss: 0.00045251
Iteration 4/25 | Loss: 0.00045251
Iteration 5/25 | Loss: 0.00045251
Iteration 6/25 | Loss: 0.00045251
Iteration 7/25 | Loss: 0.00045251
Iteration 8/25 | Loss: 0.00045251
Iteration 9/25 | Loss: 0.00045251
Iteration 10/25 | Loss: 0.00045251
Iteration 11/25 | Loss: 0.00045250
Iteration 12/25 | Loss: 0.00045250
Iteration 13/25 | Loss: 0.00045250
Iteration 14/25 | Loss: 0.00045250
Iteration 15/25 | Loss: 0.00045250
Iteration 16/25 | Loss: 0.00045250
Iteration 17/25 | Loss: 0.00045250
Iteration 18/25 | Loss: 0.00045250
Iteration 19/25 | Loss: 0.00045250
Iteration 20/25 | Loss: 0.00045250
Iteration 21/25 | Loss: 0.00045250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00045250498806126416, 0.00045250498806126416, 0.00045250498806126416, 0.00045250498806126416, 0.00045250498806126416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045250498806126416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045250
Iteration 2/1000 | Loss: 0.00004343
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002219
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001951
Iteration 8/1000 | Loss: 0.00001911
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001902
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001853
Iteration 16/1000 | Loss: 0.00001851
Iteration 17/1000 | Loss: 0.00001846
Iteration 18/1000 | Loss: 0.00001845
Iteration 19/1000 | Loss: 0.00001844
Iteration 20/1000 | Loss: 0.00001843
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001842
Iteration 23/1000 | Loss: 0.00001841
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001837
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001827
Iteration 31/1000 | Loss: 0.00001826
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001820
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001816
Iteration 47/1000 | Loss: 0.00001816
Iteration 48/1000 | Loss: 0.00001816
Iteration 49/1000 | Loss: 0.00001816
Iteration 50/1000 | Loss: 0.00001816
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001815
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001814
Iteration 57/1000 | Loss: 0.00001814
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001811
Iteration 67/1000 | Loss: 0.00001811
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001809
Iteration 74/1000 | Loss: 0.00001837
Iteration 75/1000 | Loss: 0.00001837
Iteration 76/1000 | Loss: 0.00001837
Iteration 77/1000 | Loss: 0.00001810
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8061173250316642e-05, 1.8061173250316642e-05, 1.8061173250316642e-05, 1.8061173250316642e-05, 1.8061173250316642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8061173250316642e-05

Optimization complete. Final v2v error: 3.5529510974884033 mm

Highest mean error: 10.547374725341797 mm for frame 56

Lowest mean error: 3.022651433944702 mm for frame 194

Saving results

Total time: 83.39455270767212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398354
Iteration 2/25 | Loss: 0.00098194
Iteration 3/25 | Loss: 0.00090014
Iteration 4/25 | Loss: 0.00088002
Iteration 5/25 | Loss: 0.00087137
Iteration 6/25 | Loss: 0.00086926
Iteration 7/25 | Loss: 0.00086913
Iteration 8/25 | Loss: 0.00086913
Iteration 9/25 | Loss: 0.00086913
Iteration 10/25 | Loss: 0.00086913
Iteration 11/25 | Loss: 0.00086913
Iteration 12/25 | Loss: 0.00086913
Iteration 13/25 | Loss: 0.00086913
Iteration 14/25 | Loss: 0.00086913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000869131472427398, 0.000869131472427398, 0.000869131472427398, 0.000869131472427398, 0.000869131472427398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000869131472427398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84317899
Iteration 2/25 | Loss: 0.00037883
Iteration 3/25 | Loss: 0.00037883
Iteration 4/25 | Loss: 0.00037883
Iteration 5/25 | Loss: 0.00037883
Iteration 6/25 | Loss: 0.00037883
Iteration 7/25 | Loss: 0.00037883
Iteration 8/25 | Loss: 0.00037883
Iteration 9/25 | Loss: 0.00037883
Iteration 10/25 | Loss: 0.00037883
Iteration 11/25 | Loss: 0.00037883
Iteration 12/25 | Loss: 0.00037883
Iteration 13/25 | Loss: 0.00037883
Iteration 14/25 | Loss: 0.00037883
Iteration 15/25 | Loss: 0.00037883
Iteration 16/25 | Loss: 0.00037883
Iteration 17/25 | Loss: 0.00037883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00037883230834268034, 0.00037883230834268034, 0.00037883230834268034, 0.00037883230834268034, 0.00037883230834268034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037883230834268034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037883
Iteration 2/1000 | Loss: 0.00002673
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001661
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001648
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001643
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001640
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001640
Iteration 59/1000 | Loss: 0.00001640
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001636
Iteration 114/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.6361918824259192e-05, 1.6361918824259192e-05, 1.6361918824259192e-05, 1.6361918824259192e-05, 1.6361918824259192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6361918824259192e-05

Optimization complete. Final v2v error: 3.443000078201294 mm

Highest mean error: 3.67187237739563 mm for frame 227

Lowest mean error: 3.2576651573181152 mm for frame 140

Saving results

Total time: 31.286611795425415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560361
Iteration 2/25 | Loss: 0.00106927
Iteration 3/25 | Loss: 0.00095148
Iteration 4/25 | Loss: 0.00092450
Iteration 5/25 | Loss: 0.00091348
Iteration 6/25 | Loss: 0.00091108
Iteration 7/25 | Loss: 0.00091086
Iteration 8/25 | Loss: 0.00091086
Iteration 9/25 | Loss: 0.00091086
Iteration 10/25 | Loss: 0.00091086
Iteration 11/25 | Loss: 0.00091086
Iteration 12/25 | Loss: 0.00091086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009108645608648658, 0.0009108645608648658, 0.0009108645608648658, 0.0009108645608648658, 0.0009108645608648658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009108645608648658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36387634
Iteration 2/25 | Loss: 0.00044850
Iteration 3/25 | Loss: 0.00044846
Iteration 4/25 | Loss: 0.00044846
Iteration 5/25 | Loss: 0.00044846
Iteration 6/25 | Loss: 0.00044846
Iteration 7/25 | Loss: 0.00044846
Iteration 8/25 | Loss: 0.00044846
Iteration 9/25 | Loss: 0.00044846
Iteration 10/25 | Loss: 0.00044846
Iteration 11/25 | Loss: 0.00044846
Iteration 12/25 | Loss: 0.00044846
Iteration 13/25 | Loss: 0.00044846
Iteration 14/25 | Loss: 0.00044846
Iteration 15/25 | Loss: 0.00044846
Iteration 16/25 | Loss: 0.00044846
Iteration 17/25 | Loss: 0.00044846
Iteration 18/25 | Loss: 0.00044846
Iteration 19/25 | Loss: 0.00044846
Iteration 20/25 | Loss: 0.00044846
Iteration 21/25 | Loss: 0.00044846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00044845789670944214, 0.00044845789670944214, 0.00044845789670944214, 0.00044845789670944214, 0.00044845789670944214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044845789670944214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044846
Iteration 2/1000 | Loss: 0.00003703
Iteration 3/1000 | Loss: 0.00002468
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001888
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001851
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001844
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001840
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001839
Iteration 30/1000 | Loss: 0.00001839
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001835
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001830
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001830
Iteration 51/1000 | Loss: 0.00001830
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001829
Iteration 54/1000 | Loss: 0.00001829
Iteration 55/1000 | Loss: 0.00001829
Iteration 56/1000 | Loss: 0.00001829
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001826
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001825
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001825
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001824
Iteration 76/1000 | Loss: 0.00001824
Iteration 77/1000 | Loss: 0.00001824
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001817
Iteration 126/1000 | Loss: 0.00001817
Iteration 127/1000 | Loss: 0.00001817
Iteration 128/1000 | Loss: 0.00001817
Iteration 129/1000 | Loss: 0.00001817
Iteration 130/1000 | Loss: 0.00001817
Iteration 131/1000 | Loss: 0.00001817
Iteration 132/1000 | Loss: 0.00001817
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001817
Iteration 136/1000 | Loss: 0.00001817
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001817
Iteration 139/1000 | Loss: 0.00001817
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001817
Iteration 142/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.8172104319091886e-05, 1.8172104319091886e-05, 1.8172104319091886e-05, 1.8172104319091886e-05, 1.8172104319091886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8172104319091886e-05

Optimization complete. Final v2v error: 3.6415152549743652 mm

Highest mean error: 4.1271162033081055 mm for frame 21

Lowest mean error: 3.205571174621582 mm for frame 163

Saving results

Total time: 40.8558304309845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861257
Iteration 2/25 | Loss: 0.00106686
Iteration 3/25 | Loss: 0.00094730
Iteration 4/25 | Loss: 0.00091730
Iteration 5/25 | Loss: 0.00091112
Iteration 6/25 | Loss: 0.00090993
Iteration 7/25 | Loss: 0.00090993
Iteration 8/25 | Loss: 0.00090993
Iteration 9/25 | Loss: 0.00090993
Iteration 10/25 | Loss: 0.00090993
Iteration 11/25 | Loss: 0.00090993
Iteration 12/25 | Loss: 0.00090993
Iteration 13/25 | Loss: 0.00090993
Iteration 14/25 | Loss: 0.00090993
Iteration 15/25 | Loss: 0.00090993
Iteration 16/25 | Loss: 0.00090993
Iteration 17/25 | Loss: 0.00090993
Iteration 18/25 | Loss: 0.00090993
Iteration 19/25 | Loss: 0.00090993
Iteration 20/25 | Loss: 0.00090993
Iteration 21/25 | Loss: 0.00090993
Iteration 22/25 | Loss: 0.00090993
Iteration 23/25 | Loss: 0.00090993
Iteration 24/25 | Loss: 0.00090993
Iteration 25/25 | Loss: 0.00090993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36967099
Iteration 2/25 | Loss: 0.00050612
Iteration 3/25 | Loss: 0.00050612
Iteration 4/25 | Loss: 0.00050612
Iteration 5/25 | Loss: 0.00050612
Iteration 6/25 | Loss: 0.00050612
Iteration 7/25 | Loss: 0.00050612
Iteration 8/25 | Loss: 0.00050612
Iteration 9/25 | Loss: 0.00050612
Iteration 10/25 | Loss: 0.00050612
Iteration 11/25 | Loss: 0.00050612
Iteration 12/25 | Loss: 0.00050612
Iteration 13/25 | Loss: 0.00050612
Iteration 14/25 | Loss: 0.00050612
Iteration 15/25 | Loss: 0.00050612
Iteration 16/25 | Loss: 0.00050612
Iteration 17/25 | Loss: 0.00050612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005061184638179839, 0.0005061184638179839, 0.0005061184638179839, 0.0005061184638179839, 0.0005061184638179839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005061184638179839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050612
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00002566
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001821
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001719
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001699
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001690
Iteration 38/1000 | Loss: 0.00001690
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001689
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001679
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001675
Iteration 96/1000 | Loss: 0.00001675
Iteration 97/1000 | Loss: 0.00001675
Iteration 98/1000 | Loss: 0.00001675
Iteration 99/1000 | Loss: 0.00001675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6751988368923776e-05, 1.6751988368923776e-05, 1.6751988368923776e-05, 1.6751988368923776e-05, 1.6751988368923776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6751988368923776e-05

Optimization complete. Final v2v error: 3.4030556678771973 mm

Highest mean error: 4.464468002319336 mm for frame 31

Lowest mean error: 2.8821263313293457 mm for frame 104

Saving results

Total time: 40.7546124458313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375817
Iteration 2/25 | Loss: 0.00112856
Iteration 3/25 | Loss: 0.00096513
Iteration 4/25 | Loss: 0.00092071
Iteration 5/25 | Loss: 0.00091020
Iteration 6/25 | Loss: 0.00090717
Iteration 7/25 | Loss: 0.00090664
Iteration 8/25 | Loss: 0.00090664
Iteration 9/25 | Loss: 0.00090664
Iteration 10/25 | Loss: 0.00090664
Iteration 11/25 | Loss: 0.00090664
Iteration 12/25 | Loss: 0.00090664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009066396160051227, 0.0009066396160051227, 0.0009066396160051227, 0.0009066396160051227, 0.0009066396160051227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009066396160051227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36172247
Iteration 2/25 | Loss: 0.00050903
Iteration 3/25 | Loss: 0.00050902
Iteration 4/25 | Loss: 0.00050902
Iteration 5/25 | Loss: 0.00050902
Iteration 6/25 | Loss: 0.00050902
Iteration 7/25 | Loss: 0.00050902
Iteration 8/25 | Loss: 0.00050902
Iteration 9/25 | Loss: 0.00050902
Iteration 10/25 | Loss: 0.00050902
Iteration 11/25 | Loss: 0.00050902
Iteration 12/25 | Loss: 0.00050902
Iteration 13/25 | Loss: 0.00050902
Iteration 14/25 | Loss: 0.00050902
Iteration 15/25 | Loss: 0.00050902
Iteration 16/25 | Loss: 0.00050902
Iteration 17/25 | Loss: 0.00050902
Iteration 18/25 | Loss: 0.00050902
Iteration 19/25 | Loss: 0.00050902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005090211634524167, 0.0005090211634524167, 0.0005090211634524167, 0.0005090211634524167, 0.0005090211634524167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005090211634524167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050902
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00002360
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001928
Iteration 10/1000 | Loss: 0.00001927
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001926
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001910
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001903
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001901
Iteration 25/1000 | Loss: 0.00001901
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001899
Iteration 29/1000 | Loss: 0.00001898
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001898
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001896
Iteration 43/1000 | Loss: 0.00001896
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001892
Iteration 75/1000 | Loss: 0.00001892
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001892
Iteration 80/1000 | Loss: 0.00001892
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001891
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.8902417650679126e-05, 1.8902417650679126e-05, 1.8902417650679126e-05, 1.8902417650679126e-05, 1.8902417650679126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8902417650679126e-05

Optimization complete. Final v2v error: 3.6522789001464844 mm

Highest mean error: 3.811426877975464 mm for frame 76

Lowest mean error: 3.419875144958496 mm for frame 0

Saving results

Total time: 35.93329071998596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035991
Iteration 2/25 | Loss: 0.00241051
Iteration 3/25 | Loss: 0.00169653
Iteration 4/25 | Loss: 0.00152259
Iteration 5/25 | Loss: 0.00136412
Iteration 6/25 | Loss: 0.00127595
Iteration 7/25 | Loss: 0.00126056
Iteration 8/25 | Loss: 0.00125728
Iteration 9/25 | Loss: 0.00128839
Iteration 10/25 | Loss: 0.00128017
Iteration 11/25 | Loss: 0.00126355
Iteration 12/25 | Loss: 0.00126510
Iteration 13/25 | Loss: 0.00126329
Iteration 14/25 | Loss: 0.00126168
Iteration 15/25 | Loss: 0.00128298
Iteration 16/25 | Loss: 0.00128537
Iteration 17/25 | Loss: 0.00127163
Iteration 18/25 | Loss: 0.00125415
Iteration 19/25 | Loss: 0.00126140
Iteration 20/25 | Loss: 0.00125086
Iteration 21/25 | Loss: 0.00124613
Iteration 22/25 | Loss: 0.00124245
Iteration 23/25 | Loss: 0.00123970
Iteration 24/25 | Loss: 0.00123298
Iteration 25/25 | Loss: 0.00123179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40508389
Iteration 2/25 | Loss: 0.00292173
Iteration 3/25 | Loss: 0.00289591
Iteration 4/25 | Loss: 0.00289591
Iteration 5/25 | Loss: 0.00289591
Iteration 6/25 | Loss: 0.00289591
Iteration 7/25 | Loss: 0.00289591
Iteration 8/25 | Loss: 0.00289591
Iteration 9/25 | Loss: 0.00289591
Iteration 10/25 | Loss: 0.00289591
Iteration 11/25 | Loss: 0.00289591
Iteration 12/25 | Loss: 0.00289591
Iteration 13/25 | Loss: 0.00289591
Iteration 14/25 | Loss: 0.00289591
Iteration 15/25 | Loss: 0.00289591
Iteration 16/25 | Loss: 0.00289591
Iteration 17/25 | Loss: 0.00289591
Iteration 18/25 | Loss: 0.00289591
Iteration 19/25 | Loss: 0.00289591
Iteration 20/25 | Loss: 0.00289591
Iteration 21/25 | Loss: 0.00289591
Iteration 22/25 | Loss: 0.00289591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002895909361541271, 0.002895909361541271, 0.002895909361541271, 0.002895909361541271, 0.002895909361541271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002895909361541271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289591
Iteration 2/1000 | Loss: 0.00053455
Iteration 3/1000 | Loss: 0.00048426
Iteration 4/1000 | Loss: 0.00045790
Iteration 5/1000 | Loss: 0.00028457
Iteration 6/1000 | Loss: 0.00102630
Iteration 7/1000 | Loss: 0.00128740
Iteration 8/1000 | Loss: 0.00068977
Iteration 9/1000 | Loss: 0.00110767
Iteration 10/1000 | Loss: 0.00028996
Iteration 11/1000 | Loss: 0.00023239
Iteration 12/1000 | Loss: 0.00020358
Iteration 13/1000 | Loss: 0.00022229
Iteration 14/1000 | Loss: 0.00017599
Iteration 15/1000 | Loss: 0.00023019
Iteration 16/1000 | Loss: 0.00020980
Iteration 17/1000 | Loss: 0.00020001
Iteration 18/1000 | Loss: 0.00022228
Iteration 19/1000 | Loss: 0.00022671
Iteration 20/1000 | Loss: 0.00018170
Iteration 21/1000 | Loss: 0.00020201
Iteration 22/1000 | Loss: 0.00019361
Iteration 23/1000 | Loss: 0.00019070
Iteration 24/1000 | Loss: 0.00017762
Iteration 25/1000 | Loss: 0.00017625
Iteration 26/1000 | Loss: 0.00020074
Iteration 27/1000 | Loss: 0.00020375
Iteration 28/1000 | Loss: 0.00017975
Iteration 29/1000 | Loss: 0.00020646
Iteration 30/1000 | Loss: 0.00093481
Iteration 31/1000 | Loss: 0.00056523
Iteration 32/1000 | Loss: 0.00070808
Iteration 33/1000 | Loss: 0.00091183
Iteration 34/1000 | Loss: 0.00061609
Iteration 35/1000 | Loss: 0.00056402
Iteration 36/1000 | Loss: 0.00056736
Iteration 37/1000 | Loss: 0.00023055
Iteration 38/1000 | Loss: 0.00017898
Iteration 39/1000 | Loss: 0.00019222
Iteration 40/1000 | Loss: 0.00018010
Iteration 41/1000 | Loss: 0.00017459
Iteration 42/1000 | Loss: 0.00017222
Iteration 43/1000 | Loss: 0.00018502
Iteration 44/1000 | Loss: 0.00020742
Iteration 45/1000 | Loss: 0.00017144
Iteration 46/1000 | Loss: 0.00016839
Iteration 47/1000 | Loss: 0.00018700
Iteration 48/1000 | Loss: 0.00018555
Iteration 49/1000 | Loss: 0.00089758
Iteration 50/1000 | Loss: 0.00065233
Iteration 51/1000 | Loss: 0.00021630
Iteration 52/1000 | Loss: 0.00018318
Iteration 53/1000 | Loss: 0.00017368
Iteration 54/1000 | Loss: 0.00016883
Iteration 55/1000 | Loss: 0.00017533
Iteration 56/1000 | Loss: 0.00017425
Iteration 57/1000 | Loss: 0.00015792
Iteration 58/1000 | Loss: 0.00018869
Iteration 59/1000 | Loss: 0.00015762
Iteration 60/1000 | Loss: 0.00016493
Iteration 61/1000 | Loss: 0.00015204
Iteration 62/1000 | Loss: 0.00015734
Iteration 63/1000 | Loss: 0.00017787
Iteration 64/1000 | Loss: 0.00016102
Iteration 65/1000 | Loss: 0.00019463
Iteration 66/1000 | Loss: 0.00016797
Iteration 67/1000 | Loss: 0.00018603
Iteration 68/1000 | Loss: 0.00017130
Iteration 69/1000 | Loss: 0.00019413
Iteration 70/1000 | Loss: 0.00018854
Iteration 71/1000 | Loss: 0.00019446
Iteration 72/1000 | Loss: 0.00017325
Iteration 73/1000 | Loss: 0.00017829
Iteration 74/1000 | Loss: 0.00019592
Iteration 75/1000 | Loss: 0.00017501
Iteration 76/1000 | Loss: 0.00018375
Iteration 77/1000 | Loss: 0.00021936
Iteration 78/1000 | Loss: 0.00020170
Iteration 79/1000 | Loss: 0.00018008
Iteration 80/1000 | Loss: 0.00021120
Iteration 81/1000 | Loss: 0.00023510
Iteration 82/1000 | Loss: 0.00016412
Iteration 83/1000 | Loss: 0.00019712
Iteration 84/1000 | Loss: 0.00017130
Iteration 85/1000 | Loss: 0.00020566
Iteration 86/1000 | Loss: 0.00020731
Iteration 87/1000 | Loss: 0.00021335
Iteration 88/1000 | Loss: 0.00024998
Iteration 89/1000 | Loss: 0.00025987
Iteration 90/1000 | Loss: 0.00024481
Iteration 91/1000 | Loss: 0.00016921
Iteration 92/1000 | Loss: 0.00016213
Iteration 93/1000 | Loss: 0.00014330
Iteration 94/1000 | Loss: 0.00016674
Iteration 95/1000 | Loss: 0.00015131
Iteration 96/1000 | Loss: 0.00018692
Iteration 97/1000 | Loss: 0.00015152
Iteration 98/1000 | Loss: 0.00014192
Iteration 99/1000 | Loss: 0.00014199
Iteration 100/1000 | Loss: 0.00014219
Iteration 101/1000 | Loss: 0.00012855
Iteration 102/1000 | Loss: 0.00012731
Iteration 103/1000 | Loss: 0.00013506
Iteration 104/1000 | Loss: 0.00012942
Iteration 105/1000 | Loss: 0.00014753
Iteration 106/1000 | Loss: 0.00016133
Iteration 107/1000 | Loss: 0.00015352
Iteration 108/1000 | Loss: 0.00016107
Iteration 109/1000 | Loss: 0.00013921
Iteration 110/1000 | Loss: 0.00013458
Iteration 111/1000 | Loss: 0.00013101
Iteration 112/1000 | Loss: 0.00012899
Iteration 113/1000 | Loss: 0.00012791
Iteration 114/1000 | Loss: 0.00012722
Iteration 115/1000 | Loss: 0.00012636
Iteration 116/1000 | Loss: 0.00012568
Iteration 117/1000 | Loss: 0.00012530
Iteration 118/1000 | Loss: 0.00012505
Iteration 119/1000 | Loss: 0.00012472
Iteration 120/1000 | Loss: 0.00012451
Iteration 121/1000 | Loss: 0.00012432
Iteration 122/1000 | Loss: 0.00012419
Iteration 123/1000 | Loss: 0.00012409
Iteration 124/1000 | Loss: 0.00012401
Iteration 125/1000 | Loss: 0.00012400
Iteration 126/1000 | Loss: 0.00012400
Iteration 127/1000 | Loss: 0.00012399
Iteration 128/1000 | Loss: 0.00012399
Iteration 129/1000 | Loss: 0.00012398
Iteration 130/1000 | Loss: 0.00012398
Iteration 131/1000 | Loss: 0.00012398
Iteration 132/1000 | Loss: 0.00012397
Iteration 133/1000 | Loss: 0.00012397
Iteration 134/1000 | Loss: 0.00012396
Iteration 135/1000 | Loss: 0.00012396
Iteration 136/1000 | Loss: 0.00012396
Iteration 137/1000 | Loss: 0.00012396
Iteration 138/1000 | Loss: 0.00012395
Iteration 139/1000 | Loss: 0.00012395
Iteration 140/1000 | Loss: 0.00012395
Iteration 141/1000 | Loss: 0.00012395
Iteration 142/1000 | Loss: 0.00012395
Iteration 143/1000 | Loss: 0.00012395
Iteration 144/1000 | Loss: 0.00012395
Iteration 145/1000 | Loss: 0.00012395
Iteration 146/1000 | Loss: 0.00012395
Iteration 147/1000 | Loss: 0.00012394
Iteration 148/1000 | Loss: 0.00012394
Iteration 149/1000 | Loss: 0.00012394
Iteration 150/1000 | Loss: 0.00012394
Iteration 151/1000 | Loss: 0.00012394
Iteration 152/1000 | Loss: 0.00012394
Iteration 153/1000 | Loss: 0.00012394
Iteration 154/1000 | Loss: 0.00012394
Iteration 155/1000 | Loss: 0.00012394
Iteration 156/1000 | Loss: 0.00012394
Iteration 157/1000 | Loss: 0.00012394
Iteration 158/1000 | Loss: 0.00012394
Iteration 159/1000 | Loss: 0.00012394
Iteration 160/1000 | Loss: 0.00012394
Iteration 161/1000 | Loss: 0.00012393
Iteration 162/1000 | Loss: 0.00012393
Iteration 163/1000 | Loss: 0.00012393
Iteration 164/1000 | Loss: 0.00012393
Iteration 165/1000 | Loss: 0.00012393
Iteration 166/1000 | Loss: 0.00012393
Iteration 167/1000 | Loss: 0.00012393
Iteration 168/1000 | Loss: 0.00012393
Iteration 169/1000 | Loss: 0.00012393
Iteration 170/1000 | Loss: 0.00012393
Iteration 171/1000 | Loss: 0.00012393
Iteration 172/1000 | Loss: 0.00012393
Iteration 173/1000 | Loss: 0.00012393
Iteration 174/1000 | Loss: 0.00012393
Iteration 175/1000 | Loss: 0.00012393
Iteration 176/1000 | Loss: 0.00012393
Iteration 177/1000 | Loss: 0.00012392
Iteration 178/1000 | Loss: 0.00012392
Iteration 179/1000 | Loss: 0.00012392
Iteration 180/1000 | Loss: 0.00012392
Iteration 181/1000 | Loss: 0.00012392
Iteration 182/1000 | Loss: 0.00012392
Iteration 183/1000 | Loss: 0.00012392
Iteration 184/1000 | Loss: 0.00012392
Iteration 185/1000 | Loss: 0.00012392
Iteration 186/1000 | Loss: 0.00012392
Iteration 187/1000 | Loss: 0.00012392
Iteration 188/1000 | Loss: 0.00012392
Iteration 189/1000 | Loss: 0.00012392
Iteration 190/1000 | Loss: 0.00012392
Iteration 191/1000 | Loss: 0.00012392
Iteration 192/1000 | Loss: 0.00012392
Iteration 193/1000 | Loss: 0.00012392
Iteration 194/1000 | Loss: 0.00012392
Iteration 195/1000 | Loss: 0.00012392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [0.00012392211647238582, 0.00012392211647238582, 0.00012392211647238582, 0.00012392211647238582, 0.00012392211647238582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012392211647238582

Optimization complete. Final v2v error: 5.884210586547852 mm

Highest mean error: 12.765871047973633 mm for frame 15

Lowest mean error: 3.4296514987945557 mm for frame 20

Saving results

Total time: 220.52763986587524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108862
Iteration 2/25 | Loss: 0.00277109
Iteration 3/25 | Loss: 0.00248860
Iteration 4/25 | Loss: 0.00218455
Iteration 5/25 | Loss: 0.00197055
Iteration 6/25 | Loss: 0.00186369
Iteration 7/25 | Loss: 0.00176471
Iteration 8/25 | Loss: 0.00153462
Iteration 9/25 | Loss: 0.00141872
Iteration 10/25 | Loss: 0.00142524
Iteration 11/25 | Loss: 0.00139774
Iteration 12/25 | Loss: 0.00133082
Iteration 13/25 | Loss: 0.00131824
Iteration 14/25 | Loss: 0.00131382
Iteration 15/25 | Loss: 0.00131683
Iteration 16/25 | Loss: 0.00131132
Iteration 17/25 | Loss: 0.00130070
Iteration 18/25 | Loss: 0.00130060
Iteration 19/25 | Loss: 0.00129390
Iteration 20/25 | Loss: 0.00129345
Iteration 21/25 | Loss: 0.00129335
Iteration 22/25 | Loss: 0.00129329
Iteration 23/25 | Loss: 0.00129329
Iteration 24/25 | Loss: 0.00129329
Iteration 25/25 | Loss: 0.00129329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37535906
Iteration 2/25 | Loss: 0.00597199
Iteration 3/25 | Loss: 0.00275110
Iteration 4/25 | Loss: 0.00275109
Iteration 5/25 | Loss: 0.00275109
Iteration 6/25 | Loss: 0.00275109
Iteration 7/25 | Loss: 0.00275109
Iteration 8/25 | Loss: 0.00275109
Iteration 9/25 | Loss: 0.00275109
Iteration 10/25 | Loss: 0.00275109
Iteration 11/25 | Loss: 0.00275109
Iteration 12/25 | Loss: 0.00275109
Iteration 13/25 | Loss: 0.00275109
Iteration 14/25 | Loss: 0.00275109
Iteration 15/25 | Loss: 0.00275109
Iteration 16/25 | Loss: 0.00275109
Iteration 17/25 | Loss: 0.00275109
Iteration 18/25 | Loss: 0.00275109
Iteration 19/25 | Loss: 0.00275109
Iteration 20/25 | Loss: 0.00275109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002751086140051484, 0.002751086140051484, 0.002751086140051484, 0.002751086140051484, 0.002751086140051484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002751086140051484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275109
Iteration 2/1000 | Loss: 0.00344630
Iteration 3/1000 | Loss: 0.00095194
Iteration 4/1000 | Loss: 0.00117976
Iteration 5/1000 | Loss: 0.00169093
Iteration 6/1000 | Loss: 0.00069196
Iteration 7/1000 | Loss: 0.00338514
Iteration 8/1000 | Loss: 0.00887572
Iteration 9/1000 | Loss: 0.00122410
Iteration 10/1000 | Loss: 0.00087908
Iteration 11/1000 | Loss: 0.00257796
Iteration 12/1000 | Loss: 0.00084743
Iteration 13/1000 | Loss: 0.00391523
Iteration 14/1000 | Loss: 0.00029752
Iteration 15/1000 | Loss: 0.00033522
Iteration 16/1000 | Loss: 0.00036671
Iteration 17/1000 | Loss: 0.00165503
Iteration 18/1000 | Loss: 0.00064372
Iteration 19/1000 | Loss: 0.00019951
Iteration 20/1000 | Loss: 0.00051158
Iteration 21/1000 | Loss: 0.00611185
Iteration 22/1000 | Loss: 0.00953314
Iteration 23/1000 | Loss: 0.01174022
Iteration 24/1000 | Loss: 0.00455588
Iteration 25/1000 | Loss: 0.00259638
Iteration 26/1000 | Loss: 0.00507235
Iteration 27/1000 | Loss: 0.00242829
Iteration 28/1000 | Loss: 0.00347404
Iteration 29/1000 | Loss: 0.00202072
Iteration 30/1000 | Loss: 0.00171727
Iteration 31/1000 | Loss: 0.00345667
Iteration 32/1000 | Loss: 0.00137950
Iteration 33/1000 | Loss: 0.00157454
Iteration 34/1000 | Loss: 0.00132546
Iteration 35/1000 | Loss: 0.00202059
Iteration 36/1000 | Loss: 0.00270909
Iteration 37/1000 | Loss: 0.00104259
Iteration 38/1000 | Loss: 0.00133550
Iteration 39/1000 | Loss: 0.00226143
Iteration 40/1000 | Loss: 0.00138237
Iteration 41/1000 | Loss: 0.00277676
Iteration 42/1000 | Loss: 0.00136100
Iteration 43/1000 | Loss: 0.00110565
Iteration 44/1000 | Loss: 0.00185795
Iteration 45/1000 | Loss: 0.00184960
Iteration 46/1000 | Loss: 0.00147748
Iteration 47/1000 | Loss: 0.00109520
Iteration 48/1000 | Loss: 0.00109495
Iteration 49/1000 | Loss: 0.00353118
Iteration 50/1000 | Loss: 0.00452367
Iteration 51/1000 | Loss: 0.00335992
Iteration 52/1000 | Loss: 0.00285715
Iteration 53/1000 | Loss: 0.00259773
Iteration 54/1000 | Loss: 0.00207352
Iteration 55/1000 | Loss: 0.00268587
Iteration 56/1000 | Loss: 0.00085026
Iteration 57/1000 | Loss: 0.00156983
Iteration 58/1000 | Loss: 0.00133105
Iteration 59/1000 | Loss: 0.00161822
Iteration 60/1000 | Loss: 0.00112233
Iteration 61/1000 | Loss: 0.00211940
Iteration 62/1000 | Loss: 0.00096998
Iteration 63/1000 | Loss: 0.00092497
Iteration 64/1000 | Loss: 0.00151217
Iteration 65/1000 | Loss: 0.00090977
Iteration 66/1000 | Loss: 0.00076414
Iteration 67/1000 | Loss: 0.00119478
Iteration 68/1000 | Loss: 0.00083529
Iteration 69/1000 | Loss: 0.00114868
Iteration 70/1000 | Loss: 0.00092327
Iteration 71/1000 | Loss: 0.00217499
Iteration 72/1000 | Loss: 0.00201647
Iteration 73/1000 | Loss: 0.00159496
Iteration 74/1000 | Loss: 0.00526475
Iteration 75/1000 | Loss: 0.00409534
Iteration 76/1000 | Loss: 0.00381962
Iteration 77/1000 | Loss: 0.00106115
Iteration 78/1000 | Loss: 0.00074843
Iteration 79/1000 | Loss: 0.00086915
Iteration 80/1000 | Loss: 0.00050926
Iteration 81/1000 | Loss: 0.00103045
Iteration 82/1000 | Loss: 0.00077944
Iteration 83/1000 | Loss: 0.00077830
Iteration 84/1000 | Loss: 0.00059578
Iteration 85/1000 | Loss: 0.00109225
Iteration 86/1000 | Loss: 0.00078575
Iteration 87/1000 | Loss: 0.00072362
Iteration 88/1000 | Loss: 0.00128077
Iteration 89/1000 | Loss: 0.00064468
Iteration 90/1000 | Loss: 0.00083818
Iteration 91/1000 | Loss: 0.00069159
Iteration 92/1000 | Loss: 0.00084726
Iteration 93/1000 | Loss: 0.00131855
Iteration 94/1000 | Loss: 0.00510579
Iteration 95/1000 | Loss: 0.00068596
Iteration 96/1000 | Loss: 0.00111513
Iteration 97/1000 | Loss: 0.00060225
Iteration 98/1000 | Loss: 0.00240543
Iteration 99/1000 | Loss: 0.00144834
Iteration 100/1000 | Loss: 0.00042945
Iteration 101/1000 | Loss: 0.00040696
Iteration 102/1000 | Loss: 0.00164896
Iteration 103/1000 | Loss: 0.00160546
Iteration 104/1000 | Loss: 0.00060232
Iteration 105/1000 | Loss: 0.00071681
Iteration 106/1000 | Loss: 0.00056938
Iteration 107/1000 | Loss: 0.00097632
Iteration 108/1000 | Loss: 0.00080704
Iteration 109/1000 | Loss: 0.00059686
Iteration 110/1000 | Loss: 0.00102634
Iteration 111/1000 | Loss: 0.00078023
Iteration 112/1000 | Loss: 0.00058687
Iteration 113/1000 | Loss: 0.00142686
Iteration 114/1000 | Loss: 0.00043877
Iteration 115/1000 | Loss: 0.00040940
Iteration 116/1000 | Loss: 0.00033108
Iteration 117/1000 | Loss: 0.00030490
Iteration 118/1000 | Loss: 0.00036981
Iteration 119/1000 | Loss: 0.00048457
Iteration 120/1000 | Loss: 0.00203212
Iteration 121/1000 | Loss: 0.00047742
Iteration 122/1000 | Loss: 0.00072745
Iteration 123/1000 | Loss: 0.00229769
Iteration 124/1000 | Loss: 0.00356815
Iteration 125/1000 | Loss: 0.00354837
Iteration 126/1000 | Loss: 0.00284135
Iteration 127/1000 | Loss: 0.00038309
Iteration 128/1000 | Loss: 0.00018830
Iteration 129/1000 | Loss: 0.00022252
Iteration 130/1000 | Loss: 0.00045105
Iteration 131/1000 | Loss: 0.00022905
Iteration 132/1000 | Loss: 0.00045255
Iteration 133/1000 | Loss: 0.00021725
Iteration 134/1000 | Loss: 0.00033155
Iteration 135/1000 | Loss: 0.00028846
Iteration 136/1000 | Loss: 0.00028585
Iteration 137/1000 | Loss: 0.00030319
Iteration 138/1000 | Loss: 0.00021702
Iteration 139/1000 | Loss: 0.00013517
Iteration 140/1000 | Loss: 0.00031354
Iteration 141/1000 | Loss: 0.00026912
Iteration 142/1000 | Loss: 0.00029329
Iteration 143/1000 | Loss: 0.00022227
Iteration 144/1000 | Loss: 0.00039078
Iteration 145/1000 | Loss: 0.00020773
Iteration 146/1000 | Loss: 0.00049841
Iteration 147/1000 | Loss: 0.00142563
Iteration 148/1000 | Loss: 0.00025854
Iteration 149/1000 | Loss: 0.00018674
Iteration 150/1000 | Loss: 0.00095330
Iteration 151/1000 | Loss: 0.00101149
Iteration 152/1000 | Loss: 0.00027188
Iteration 153/1000 | Loss: 0.00012008
Iteration 154/1000 | Loss: 0.00013088
Iteration 155/1000 | Loss: 0.00013919
Iteration 156/1000 | Loss: 0.00015245
Iteration 157/1000 | Loss: 0.00043181
Iteration 158/1000 | Loss: 0.00016904
Iteration 159/1000 | Loss: 0.00019911
Iteration 160/1000 | Loss: 0.00033951
Iteration 161/1000 | Loss: 0.00016636
Iteration 162/1000 | Loss: 0.00039189
Iteration 163/1000 | Loss: 0.00018739
Iteration 164/1000 | Loss: 0.00020203
Iteration 165/1000 | Loss: 0.00018350
Iteration 166/1000 | Loss: 0.00015292
Iteration 167/1000 | Loss: 0.00020473
Iteration 168/1000 | Loss: 0.00021253
Iteration 169/1000 | Loss: 0.00010428
Iteration 170/1000 | Loss: 0.00018322
Iteration 171/1000 | Loss: 0.00012976
Iteration 172/1000 | Loss: 0.00013708
Iteration 173/1000 | Loss: 0.00077464
Iteration 174/1000 | Loss: 0.00058129
Iteration 175/1000 | Loss: 0.00031916
Iteration 176/1000 | Loss: 0.00028664
Iteration 177/1000 | Loss: 0.00015468
Iteration 178/1000 | Loss: 0.00060436
Iteration 179/1000 | Loss: 0.00027075
Iteration 180/1000 | Loss: 0.00033040
Iteration 181/1000 | Loss: 0.00031280
Iteration 182/1000 | Loss: 0.00035373
Iteration 183/1000 | Loss: 0.00024240
Iteration 184/1000 | Loss: 0.00049321
Iteration 185/1000 | Loss: 0.00007765
Iteration 186/1000 | Loss: 0.00016139
Iteration 187/1000 | Loss: 0.00012446
Iteration 188/1000 | Loss: 0.00063550
Iteration 189/1000 | Loss: 0.00114992
Iteration 190/1000 | Loss: 0.00052749
Iteration 191/1000 | Loss: 0.00013295
Iteration 192/1000 | Loss: 0.00016298
Iteration 193/1000 | Loss: 0.00013918
Iteration 194/1000 | Loss: 0.00020234
Iteration 195/1000 | Loss: 0.00016088
Iteration 196/1000 | Loss: 0.00020920
Iteration 197/1000 | Loss: 0.00171526
Iteration 198/1000 | Loss: 0.00025034
Iteration 199/1000 | Loss: 0.00038143
Iteration 200/1000 | Loss: 0.00020091
Iteration 201/1000 | Loss: 0.00014691
Iteration 202/1000 | Loss: 0.00014868
Iteration 203/1000 | Loss: 0.00095029
Iteration 204/1000 | Loss: 0.00049351
Iteration 205/1000 | Loss: 0.00032609
Iteration 206/1000 | Loss: 0.00029760
Iteration 207/1000 | Loss: 0.00040037
Iteration 208/1000 | Loss: 0.00010098
Iteration 209/1000 | Loss: 0.00014178
Iteration 210/1000 | Loss: 0.00054340
Iteration 211/1000 | Loss: 0.00009678
Iteration 212/1000 | Loss: 0.00010649
Iteration 213/1000 | Loss: 0.00013240
Iteration 214/1000 | Loss: 0.00012805
Iteration 215/1000 | Loss: 0.00014230
Iteration 216/1000 | Loss: 0.00012780
Iteration 217/1000 | Loss: 0.00015725
Iteration 218/1000 | Loss: 0.00014538
Iteration 219/1000 | Loss: 0.00015772
Iteration 220/1000 | Loss: 0.00140060
Iteration 221/1000 | Loss: 0.00014357
Iteration 222/1000 | Loss: 0.00015035
Iteration 223/1000 | Loss: 0.00018984
Iteration 224/1000 | Loss: 0.00012507
Iteration 225/1000 | Loss: 0.00007001
Iteration 226/1000 | Loss: 0.00011660
Iteration 227/1000 | Loss: 0.00011208
Iteration 228/1000 | Loss: 0.00014787
Iteration 229/1000 | Loss: 0.00014819
Iteration 230/1000 | Loss: 0.00051039
Iteration 231/1000 | Loss: 0.00013814
Iteration 232/1000 | Loss: 0.00010501
Iteration 233/1000 | Loss: 0.00011188
Iteration 234/1000 | Loss: 0.00012166
Iteration 235/1000 | Loss: 0.00012839
Iteration 236/1000 | Loss: 0.00012072
Iteration 237/1000 | Loss: 0.00009052
Iteration 238/1000 | Loss: 0.00060517
Iteration 239/1000 | Loss: 0.00026745
Iteration 240/1000 | Loss: 0.00059470
Iteration 241/1000 | Loss: 0.00028510
Iteration 242/1000 | Loss: 0.00069494
Iteration 243/1000 | Loss: 0.00026352
Iteration 244/1000 | Loss: 0.00039867
Iteration 245/1000 | Loss: 0.00026289
Iteration 246/1000 | Loss: 0.00025186
Iteration 247/1000 | Loss: 0.00025948
Iteration 248/1000 | Loss: 0.00081563
Iteration 249/1000 | Loss: 0.00025646
Iteration 250/1000 | Loss: 0.00031921
Iteration 251/1000 | Loss: 0.00026166
Iteration 252/1000 | Loss: 0.00024087
Iteration 253/1000 | Loss: 0.00012578
Iteration 254/1000 | Loss: 0.00038375
Iteration 255/1000 | Loss: 0.00015584
Iteration 256/1000 | Loss: 0.00019203
Iteration 257/1000 | Loss: 0.00015279
Iteration 258/1000 | Loss: 0.00020976
Iteration 259/1000 | Loss: 0.00016006
Iteration 260/1000 | Loss: 0.00033037
Iteration 261/1000 | Loss: 0.00082955
Iteration 262/1000 | Loss: 0.00021392
Iteration 263/1000 | Loss: 0.00017384
Iteration 264/1000 | Loss: 0.00020269
Iteration 265/1000 | Loss: 0.00056733
Iteration 266/1000 | Loss: 0.00023900
Iteration 267/1000 | Loss: 0.00024727
Iteration 268/1000 | Loss: 0.00015049
Iteration 269/1000 | Loss: 0.00007180
Iteration 270/1000 | Loss: 0.00023317
Iteration 271/1000 | Loss: 0.00053058
Iteration 272/1000 | Loss: 0.00051655
Iteration 273/1000 | Loss: 0.00009168
Iteration 274/1000 | Loss: 0.00009142
Iteration 275/1000 | Loss: 0.00036011
Iteration 276/1000 | Loss: 0.00009630
Iteration 277/1000 | Loss: 0.00011121
Iteration 278/1000 | Loss: 0.00024828
Iteration 279/1000 | Loss: 0.00014312
Iteration 280/1000 | Loss: 0.00022830
Iteration 281/1000 | Loss: 0.00010056
Iteration 282/1000 | Loss: 0.00008990
Iteration 283/1000 | Loss: 0.00007590
Iteration 284/1000 | Loss: 0.00012554
Iteration 285/1000 | Loss: 0.00014674
Iteration 286/1000 | Loss: 0.00028526
Iteration 287/1000 | Loss: 0.00007696
Iteration 288/1000 | Loss: 0.00023365
Iteration 289/1000 | Loss: 0.00006124
Iteration 290/1000 | Loss: 0.00015154
Iteration 291/1000 | Loss: 0.00020821
Iteration 292/1000 | Loss: 0.00012499
Iteration 293/1000 | Loss: 0.00018424
Iteration 294/1000 | Loss: 0.00013411
Iteration 295/1000 | Loss: 0.00021567
Iteration 296/1000 | Loss: 0.00030410
Iteration 297/1000 | Loss: 0.00014215
Iteration 298/1000 | Loss: 0.00014589
Iteration 299/1000 | Loss: 0.00060662
Iteration 300/1000 | Loss: 0.00009074
Iteration 301/1000 | Loss: 0.00030186
Iteration 302/1000 | Loss: 0.00017389
Iteration 303/1000 | Loss: 0.00026596
Iteration 304/1000 | Loss: 0.00048411
Iteration 305/1000 | Loss: 0.00012765
Iteration 306/1000 | Loss: 0.00038474
Iteration 307/1000 | Loss: 0.00009371
Iteration 308/1000 | Loss: 0.00017832
Iteration 309/1000 | Loss: 0.00079759
Iteration 310/1000 | Loss: 0.00026084
Iteration 311/1000 | Loss: 0.00027424
Iteration 312/1000 | Loss: 0.00039914
Iteration 313/1000 | Loss: 0.00065782
Iteration 314/1000 | Loss: 0.00012515
Iteration 315/1000 | Loss: 0.00005991
Iteration 316/1000 | Loss: 0.00009027
Iteration 317/1000 | Loss: 0.00009478
Iteration 318/1000 | Loss: 0.00037087
Iteration 319/1000 | Loss: 0.00043201
Iteration 320/1000 | Loss: 0.00013255
Iteration 321/1000 | Loss: 0.00011308
Iteration 322/1000 | Loss: 0.00047673
Iteration 323/1000 | Loss: 0.00013101
Iteration 324/1000 | Loss: 0.00027187
Iteration 325/1000 | Loss: 0.00006406
Iteration 326/1000 | Loss: 0.00008715
Iteration 327/1000 | Loss: 0.00009881
Iteration 328/1000 | Loss: 0.00029368
Iteration 329/1000 | Loss: 0.00008377
Iteration 330/1000 | Loss: 0.00012897
Iteration 331/1000 | Loss: 0.00017459
Iteration 332/1000 | Loss: 0.00006855
Iteration 333/1000 | Loss: 0.00011103
Iteration 334/1000 | Loss: 0.00008776
Iteration 335/1000 | Loss: 0.00079347
Iteration 336/1000 | Loss: 0.00012322
Iteration 337/1000 | Loss: 0.00003981
Iteration 338/1000 | Loss: 0.00007952
Iteration 339/1000 | Loss: 0.00007241
Iteration 340/1000 | Loss: 0.00006592
Iteration 341/1000 | Loss: 0.00003846
Iteration 342/1000 | Loss: 0.00003798
Iteration 343/1000 | Loss: 0.00003763
Iteration 344/1000 | Loss: 0.00003738
Iteration 345/1000 | Loss: 0.00003728
Iteration 346/1000 | Loss: 0.00003725
Iteration 347/1000 | Loss: 0.00003725
Iteration 348/1000 | Loss: 0.00003725
Iteration 349/1000 | Loss: 0.00003724
Iteration 350/1000 | Loss: 0.00003724
Iteration 351/1000 | Loss: 0.00003724
Iteration 352/1000 | Loss: 0.00003723
Iteration 353/1000 | Loss: 0.00003720
Iteration 354/1000 | Loss: 0.00003719
Iteration 355/1000 | Loss: 0.00003718
Iteration 356/1000 | Loss: 0.00003717
Iteration 357/1000 | Loss: 0.00003717
Iteration 358/1000 | Loss: 0.00003716
Iteration 359/1000 | Loss: 0.00003715
Iteration 360/1000 | Loss: 0.00003714
Iteration 361/1000 | Loss: 0.00003713
Iteration 362/1000 | Loss: 0.00003713
Iteration 363/1000 | Loss: 0.00003712
Iteration 364/1000 | Loss: 0.00003709
Iteration 365/1000 | Loss: 0.00003708
Iteration 366/1000 | Loss: 0.00003708
Iteration 367/1000 | Loss: 0.00003708
Iteration 368/1000 | Loss: 0.00003708
Iteration 369/1000 | Loss: 0.00003708
Iteration 370/1000 | Loss: 0.00003707
Iteration 371/1000 | Loss: 0.00003707
Iteration 372/1000 | Loss: 0.00003706
Iteration 373/1000 | Loss: 0.00003706
Iteration 374/1000 | Loss: 0.00003706
Iteration 375/1000 | Loss: 0.00003706
Iteration 376/1000 | Loss: 0.00003706
Iteration 377/1000 | Loss: 0.00003706
Iteration 378/1000 | Loss: 0.00003706
Iteration 379/1000 | Loss: 0.00003706
Iteration 380/1000 | Loss: 0.00003706
Iteration 381/1000 | Loss: 0.00003706
Iteration 382/1000 | Loss: 0.00003706
Iteration 383/1000 | Loss: 0.00003706
Iteration 384/1000 | Loss: 0.00003706
Iteration 385/1000 | Loss: 0.00003706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 385. Stopping optimization.
Last 5 losses: [3.705549897858873e-05, 3.705549897858873e-05, 3.705549897858873e-05, 3.705549897858873e-05, 3.705549897858873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.705549897858873e-05

Optimization complete. Final v2v error: 4.318700313568115 mm

Highest mean error: 13.432469367980957 mm for frame 83

Lowest mean error: 3.718496561050415 mm for frame 1

Saving results

Total time: 567.3130648136139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077952
Iteration 2/25 | Loss: 0.00181627
Iteration 3/25 | Loss: 0.00120214
Iteration 4/25 | Loss: 0.00115772
Iteration 5/25 | Loss: 0.00114299
Iteration 6/25 | Loss: 0.00113912
Iteration 7/25 | Loss: 0.00113860
Iteration 8/25 | Loss: 0.00113860
Iteration 9/25 | Loss: 0.00113860
Iteration 10/25 | Loss: 0.00113860
Iteration 11/25 | Loss: 0.00113860
Iteration 12/25 | Loss: 0.00113860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011385995894670486, 0.0011385995894670486, 0.0011385995894670486, 0.0011385995894670486, 0.0011385995894670486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011385995894670486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55958289
Iteration 2/25 | Loss: 0.00025942
Iteration 3/25 | Loss: 0.00025941
Iteration 4/25 | Loss: 0.00025941
Iteration 5/25 | Loss: 0.00025941
Iteration 6/25 | Loss: 0.00025941
Iteration 7/25 | Loss: 0.00025941
Iteration 8/25 | Loss: 0.00025941
Iteration 9/25 | Loss: 0.00025941
Iteration 10/25 | Loss: 0.00025941
Iteration 11/25 | Loss: 0.00025941
Iteration 12/25 | Loss: 0.00025941
Iteration 13/25 | Loss: 0.00025941
Iteration 14/25 | Loss: 0.00025941
Iteration 15/25 | Loss: 0.00025941
Iteration 16/25 | Loss: 0.00025941
Iteration 17/25 | Loss: 0.00025941
Iteration 18/25 | Loss: 0.00025941
Iteration 19/25 | Loss: 0.00025941
Iteration 20/25 | Loss: 0.00025941
Iteration 21/25 | Loss: 0.00025941
Iteration 22/25 | Loss: 0.00025941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002594095130916685, 0.0002594095130916685, 0.0002594095130916685, 0.0002594095130916685, 0.0002594095130916685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002594095130916685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025941
Iteration 2/1000 | Loss: 0.00007675
Iteration 3/1000 | Loss: 0.00005524
Iteration 4/1000 | Loss: 0.00005139
Iteration 5/1000 | Loss: 0.00004881
Iteration 6/1000 | Loss: 0.00004736
Iteration 7/1000 | Loss: 0.00004620
Iteration 8/1000 | Loss: 0.00004539
Iteration 9/1000 | Loss: 0.00004485
Iteration 10/1000 | Loss: 0.00004440
Iteration 11/1000 | Loss: 0.00004412
Iteration 12/1000 | Loss: 0.00004399
Iteration 13/1000 | Loss: 0.00004390
Iteration 14/1000 | Loss: 0.00004383
Iteration 15/1000 | Loss: 0.00004382
Iteration 16/1000 | Loss: 0.00004379
Iteration 17/1000 | Loss: 0.00004378
Iteration 18/1000 | Loss: 0.00004375
Iteration 19/1000 | Loss: 0.00004374
Iteration 20/1000 | Loss: 0.00004371
Iteration 21/1000 | Loss: 0.00004371
Iteration 22/1000 | Loss: 0.00004364
Iteration 23/1000 | Loss: 0.00004364
Iteration 24/1000 | Loss: 0.00004363
Iteration 25/1000 | Loss: 0.00004362
Iteration 26/1000 | Loss: 0.00004361
Iteration 27/1000 | Loss: 0.00004361
Iteration 28/1000 | Loss: 0.00004360
Iteration 29/1000 | Loss: 0.00004360
Iteration 30/1000 | Loss: 0.00004359
Iteration 31/1000 | Loss: 0.00004358
Iteration 32/1000 | Loss: 0.00004358
Iteration 33/1000 | Loss: 0.00004355
Iteration 34/1000 | Loss: 0.00004354
Iteration 35/1000 | Loss: 0.00004354
Iteration 36/1000 | Loss: 0.00004351
Iteration 37/1000 | Loss: 0.00004351
Iteration 38/1000 | Loss: 0.00004349
Iteration 39/1000 | Loss: 0.00004349
Iteration 40/1000 | Loss: 0.00004348
Iteration 41/1000 | Loss: 0.00004347
Iteration 42/1000 | Loss: 0.00004347
Iteration 43/1000 | Loss: 0.00004346
Iteration 44/1000 | Loss: 0.00004346
Iteration 45/1000 | Loss: 0.00004345
Iteration 46/1000 | Loss: 0.00004345
Iteration 47/1000 | Loss: 0.00004345
Iteration 48/1000 | Loss: 0.00004343
Iteration 49/1000 | Loss: 0.00004342
Iteration 50/1000 | Loss: 0.00004341
Iteration 51/1000 | Loss: 0.00004341
Iteration 52/1000 | Loss: 0.00004340
Iteration 53/1000 | Loss: 0.00004339
Iteration 54/1000 | Loss: 0.00004339
Iteration 55/1000 | Loss: 0.00004339
Iteration 56/1000 | Loss: 0.00004338
Iteration 57/1000 | Loss: 0.00004338
Iteration 58/1000 | Loss: 0.00004338
Iteration 59/1000 | Loss: 0.00004338
Iteration 60/1000 | Loss: 0.00004338
Iteration 61/1000 | Loss: 0.00004338
Iteration 62/1000 | Loss: 0.00004338
Iteration 63/1000 | Loss: 0.00004338
Iteration 64/1000 | Loss: 0.00004338
Iteration 65/1000 | Loss: 0.00004338
Iteration 66/1000 | Loss: 0.00004338
Iteration 67/1000 | Loss: 0.00004338
Iteration 68/1000 | Loss: 0.00004337
Iteration 69/1000 | Loss: 0.00004337
Iteration 70/1000 | Loss: 0.00004337
Iteration 71/1000 | Loss: 0.00004337
Iteration 72/1000 | Loss: 0.00004337
Iteration 73/1000 | Loss: 0.00004337
Iteration 74/1000 | Loss: 0.00004337
Iteration 75/1000 | Loss: 0.00004336
Iteration 76/1000 | Loss: 0.00004336
Iteration 77/1000 | Loss: 0.00004336
Iteration 78/1000 | Loss: 0.00004336
Iteration 79/1000 | Loss: 0.00004335
Iteration 80/1000 | Loss: 0.00004335
Iteration 81/1000 | Loss: 0.00004334
Iteration 82/1000 | Loss: 0.00004334
Iteration 83/1000 | Loss: 0.00004334
Iteration 84/1000 | Loss: 0.00004334
Iteration 85/1000 | Loss: 0.00004334
Iteration 86/1000 | Loss: 0.00004334
Iteration 87/1000 | Loss: 0.00004334
Iteration 88/1000 | Loss: 0.00004334
Iteration 89/1000 | Loss: 0.00004334
Iteration 90/1000 | Loss: 0.00004334
Iteration 91/1000 | Loss: 0.00004334
Iteration 92/1000 | Loss: 0.00004333
Iteration 93/1000 | Loss: 0.00004333
Iteration 94/1000 | Loss: 0.00004333
Iteration 95/1000 | Loss: 0.00004333
Iteration 96/1000 | Loss: 0.00004333
Iteration 97/1000 | Loss: 0.00004333
Iteration 98/1000 | Loss: 0.00004332
Iteration 99/1000 | Loss: 0.00004332
Iteration 100/1000 | Loss: 0.00004332
Iteration 101/1000 | Loss: 0.00004332
Iteration 102/1000 | Loss: 0.00004332
Iteration 103/1000 | Loss: 0.00004332
Iteration 104/1000 | Loss: 0.00004332
Iteration 105/1000 | Loss: 0.00004332
Iteration 106/1000 | Loss: 0.00004332
Iteration 107/1000 | Loss: 0.00004332
Iteration 108/1000 | Loss: 0.00004331
Iteration 109/1000 | Loss: 0.00004331
Iteration 110/1000 | Loss: 0.00004331
Iteration 111/1000 | Loss: 0.00004331
Iteration 112/1000 | Loss: 0.00004331
Iteration 113/1000 | Loss: 0.00004330
Iteration 114/1000 | Loss: 0.00004330
Iteration 115/1000 | Loss: 0.00004330
Iteration 116/1000 | Loss: 0.00004330
Iteration 117/1000 | Loss: 0.00004330
Iteration 118/1000 | Loss: 0.00004330
Iteration 119/1000 | Loss: 0.00004330
Iteration 120/1000 | Loss: 0.00004330
Iteration 121/1000 | Loss: 0.00004329
Iteration 122/1000 | Loss: 0.00004329
Iteration 123/1000 | Loss: 0.00004329
Iteration 124/1000 | Loss: 0.00004329
Iteration 125/1000 | Loss: 0.00004329
Iteration 126/1000 | Loss: 0.00004329
Iteration 127/1000 | Loss: 0.00004329
Iteration 128/1000 | Loss: 0.00004329
Iteration 129/1000 | Loss: 0.00004329
Iteration 130/1000 | Loss: 0.00004328
Iteration 131/1000 | Loss: 0.00004328
Iteration 132/1000 | Loss: 0.00004328
Iteration 133/1000 | Loss: 0.00004328
Iteration 134/1000 | Loss: 0.00004328
Iteration 135/1000 | Loss: 0.00004327
Iteration 136/1000 | Loss: 0.00004327
Iteration 137/1000 | Loss: 0.00004327
Iteration 138/1000 | Loss: 0.00004327
Iteration 139/1000 | Loss: 0.00004327
Iteration 140/1000 | Loss: 0.00004327
Iteration 141/1000 | Loss: 0.00004327
Iteration 142/1000 | Loss: 0.00004327
Iteration 143/1000 | Loss: 0.00004327
Iteration 144/1000 | Loss: 0.00004327
Iteration 145/1000 | Loss: 0.00004327
Iteration 146/1000 | Loss: 0.00004327
Iteration 147/1000 | Loss: 0.00004327
Iteration 148/1000 | Loss: 0.00004327
Iteration 149/1000 | Loss: 0.00004327
Iteration 150/1000 | Loss: 0.00004327
Iteration 151/1000 | Loss: 0.00004327
Iteration 152/1000 | Loss: 0.00004327
Iteration 153/1000 | Loss: 0.00004327
Iteration 154/1000 | Loss: 0.00004327
Iteration 155/1000 | Loss: 0.00004327
Iteration 156/1000 | Loss: 0.00004327
Iteration 157/1000 | Loss: 0.00004327
Iteration 158/1000 | Loss: 0.00004327
Iteration 159/1000 | Loss: 0.00004327
Iteration 160/1000 | Loss: 0.00004327
Iteration 161/1000 | Loss: 0.00004327
Iteration 162/1000 | Loss: 0.00004327
Iteration 163/1000 | Loss: 0.00004327
Iteration 164/1000 | Loss: 0.00004327
Iteration 165/1000 | Loss: 0.00004327
Iteration 166/1000 | Loss: 0.00004327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [4.326643465901725e-05, 4.326643465901725e-05, 4.326643465901725e-05, 4.326643465901725e-05, 4.326643465901725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.326643465901725e-05

Optimization complete. Final v2v error: 5.319016456604004 mm

Highest mean error: 6.518832206726074 mm for frame 39

Lowest mean error: 4.549720287322998 mm for frame 64

Saving results

Total time: 43.400893688201904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016441
Iteration 2/25 | Loss: 0.00179260
Iteration 3/25 | Loss: 0.00138271
Iteration 4/25 | Loss: 0.00131729
Iteration 5/25 | Loss: 0.00126737
Iteration 6/25 | Loss: 0.00124006
Iteration 7/25 | Loss: 0.00121969
Iteration 8/25 | Loss: 0.00120461
Iteration 9/25 | Loss: 0.00119721
Iteration 10/25 | Loss: 0.00119515
Iteration 11/25 | Loss: 0.00119029
Iteration 12/25 | Loss: 0.00118900
Iteration 13/25 | Loss: 0.00118860
Iteration 14/25 | Loss: 0.00121824
Iteration 15/25 | Loss: 0.00119248
Iteration 16/25 | Loss: 0.00118355
Iteration 17/25 | Loss: 0.00118000
Iteration 18/25 | Loss: 0.00117925
Iteration 19/25 | Loss: 0.00117902
Iteration 20/25 | Loss: 0.00117870
Iteration 21/25 | Loss: 0.00118104
Iteration 22/25 | Loss: 0.00118207
Iteration 23/25 | Loss: 0.00118265
Iteration 24/25 | Loss: 0.00118215
Iteration 25/25 | Loss: 0.00118098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87874269
Iteration 2/25 | Loss: 0.00181717
Iteration 3/25 | Loss: 0.00155264
Iteration 4/25 | Loss: 0.00155264
Iteration 5/25 | Loss: 0.00155264
Iteration 6/25 | Loss: 0.00155264
Iteration 7/25 | Loss: 0.00155264
Iteration 8/25 | Loss: 0.00155264
Iteration 9/25 | Loss: 0.00155264
Iteration 10/25 | Loss: 0.00155264
Iteration 11/25 | Loss: 0.00155264
Iteration 12/25 | Loss: 0.00155264
Iteration 13/25 | Loss: 0.00155264
Iteration 14/25 | Loss: 0.00155264
Iteration 15/25 | Loss: 0.00155264
Iteration 16/25 | Loss: 0.00155264
Iteration 17/25 | Loss: 0.00155264
Iteration 18/25 | Loss: 0.00155264
Iteration 19/25 | Loss: 0.00155264
Iteration 20/25 | Loss: 0.00155264
Iteration 21/25 | Loss: 0.00155264
Iteration 22/25 | Loss: 0.00155264
Iteration 23/25 | Loss: 0.00155264
Iteration 24/25 | Loss: 0.00155264
Iteration 25/25 | Loss: 0.00155264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015526422066614032, 0.0015526422066614032, 0.0015526422066614032, 0.0015526422066614032, 0.0015526422066614032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015526422066614032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155264
Iteration 2/1000 | Loss: 0.00111037
Iteration 3/1000 | Loss: 0.00056773
Iteration 4/1000 | Loss: 0.00150044
Iteration 5/1000 | Loss: 0.00079910
Iteration 6/1000 | Loss: 0.00089313
Iteration 7/1000 | Loss: 0.00081944
Iteration 8/1000 | Loss: 0.00025507
Iteration 9/1000 | Loss: 0.00019858
Iteration 10/1000 | Loss: 0.00140488
Iteration 11/1000 | Loss: 0.00076416
Iteration 12/1000 | Loss: 0.00132349
Iteration 13/1000 | Loss: 0.00043220
Iteration 14/1000 | Loss: 0.00035046
Iteration 15/1000 | Loss: 0.00031510
Iteration 16/1000 | Loss: 0.00009177
Iteration 17/1000 | Loss: 0.00008129
Iteration 18/1000 | Loss: 0.00052537
Iteration 19/1000 | Loss: 0.00046224
Iteration 20/1000 | Loss: 0.00074216
Iteration 21/1000 | Loss: 0.00153875
Iteration 22/1000 | Loss: 0.00137218
Iteration 23/1000 | Loss: 0.00030044
Iteration 24/1000 | Loss: 0.00092183
Iteration 25/1000 | Loss: 0.00062556
Iteration 26/1000 | Loss: 0.00007638
Iteration 27/1000 | Loss: 0.00018044
Iteration 28/1000 | Loss: 0.00018916
Iteration 29/1000 | Loss: 0.00015208
Iteration 30/1000 | Loss: 0.00012823
Iteration 31/1000 | Loss: 0.00061013
Iteration 32/1000 | Loss: 0.00008028
Iteration 33/1000 | Loss: 0.00073454
Iteration 34/1000 | Loss: 0.00076766
Iteration 35/1000 | Loss: 0.00070111
Iteration 36/1000 | Loss: 0.00020321
Iteration 37/1000 | Loss: 0.00007409
Iteration 38/1000 | Loss: 0.00005916
Iteration 39/1000 | Loss: 0.00005036
Iteration 40/1000 | Loss: 0.00004692
Iteration 41/1000 | Loss: 0.00004473
Iteration 42/1000 | Loss: 0.00004253
Iteration 43/1000 | Loss: 0.00004117
Iteration 44/1000 | Loss: 0.00003950
Iteration 45/1000 | Loss: 0.00010949
Iteration 46/1000 | Loss: 0.00004435
Iteration 47/1000 | Loss: 0.00003664
Iteration 48/1000 | Loss: 0.00003754
Iteration 49/1000 | Loss: 0.00003552
Iteration 50/1000 | Loss: 0.00003492
Iteration 51/1000 | Loss: 0.00003462
Iteration 52/1000 | Loss: 0.00003436
Iteration 53/1000 | Loss: 0.00014593
Iteration 54/1000 | Loss: 0.00003596
Iteration 55/1000 | Loss: 0.00003402
Iteration 56/1000 | Loss: 0.00003363
Iteration 57/1000 | Loss: 0.00003350
Iteration 58/1000 | Loss: 0.00012675
Iteration 59/1000 | Loss: 0.00004484
Iteration 60/1000 | Loss: 0.00005609
Iteration 61/1000 | Loss: 0.00003350
Iteration 62/1000 | Loss: 0.00003322
Iteration 63/1000 | Loss: 0.00003310
Iteration 64/1000 | Loss: 0.00003303
Iteration 65/1000 | Loss: 0.00003299
Iteration 66/1000 | Loss: 0.00003299
Iteration 67/1000 | Loss: 0.00003299
Iteration 68/1000 | Loss: 0.00003299
Iteration 69/1000 | Loss: 0.00003298
Iteration 70/1000 | Loss: 0.00003298
Iteration 71/1000 | Loss: 0.00003298
Iteration 72/1000 | Loss: 0.00003298
Iteration 73/1000 | Loss: 0.00003298
Iteration 74/1000 | Loss: 0.00003298
Iteration 75/1000 | Loss: 0.00003298
Iteration 76/1000 | Loss: 0.00003298
Iteration 77/1000 | Loss: 0.00003298
Iteration 78/1000 | Loss: 0.00003298
Iteration 79/1000 | Loss: 0.00003298
Iteration 80/1000 | Loss: 0.00003297
Iteration 81/1000 | Loss: 0.00003297
Iteration 82/1000 | Loss: 0.00003296
Iteration 83/1000 | Loss: 0.00003295
Iteration 84/1000 | Loss: 0.00003295
Iteration 85/1000 | Loss: 0.00003295
Iteration 86/1000 | Loss: 0.00003293
Iteration 87/1000 | Loss: 0.00003290
Iteration 88/1000 | Loss: 0.00003285
Iteration 89/1000 | Loss: 0.00003285
Iteration 90/1000 | Loss: 0.00003283
Iteration 91/1000 | Loss: 0.00003282
Iteration 92/1000 | Loss: 0.00003282
Iteration 93/1000 | Loss: 0.00003281
Iteration 94/1000 | Loss: 0.00003281
Iteration 95/1000 | Loss: 0.00003280
Iteration 96/1000 | Loss: 0.00003280
Iteration 97/1000 | Loss: 0.00003279
Iteration 98/1000 | Loss: 0.00003276
Iteration 99/1000 | Loss: 0.00003276
Iteration 100/1000 | Loss: 0.00003275
Iteration 101/1000 | Loss: 0.00003275
Iteration 102/1000 | Loss: 0.00003275
Iteration 103/1000 | Loss: 0.00003274
Iteration 104/1000 | Loss: 0.00003274
Iteration 105/1000 | Loss: 0.00003274
Iteration 106/1000 | Loss: 0.00003274
Iteration 107/1000 | Loss: 0.00003273
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00003272
Iteration 110/1000 | Loss: 0.00003272
Iteration 111/1000 | Loss: 0.00003272
Iteration 112/1000 | Loss: 0.00003272
Iteration 113/1000 | Loss: 0.00003272
Iteration 114/1000 | Loss: 0.00015288
Iteration 115/1000 | Loss: 0.00003744
Iteration 116/1000 | Loss: 0.00003289
Iteration 117/1000 | Loss: 0.00005960
Iteration 118/1000 | Loss: 0.00004475
Iteration 119/1000 | Loss: 0.00004237
Iteration 120/1000 | Loss: 0.00003897
Iteration 121/1000 | Loss: 0.00004218
Iteration 122/1000 | Loss: 0.00003954
Iteration 123/1000 | Loss: 0.00003268
Iteration 124/1000 | Loss: 0.00003268
Iteration 125/1000 | Loss: 0.00003268
Iteration 126/1000 | Loss: 0.00003268
Iteration 127/1000 | Loss: 0.00003268
Iteration 128/1000 | Loss: 0.00003268
Iteration 129/1000 | Loss: 0.00003268
Iteration 130/1000 | Loss: 0.00003268
Iteration 131/1000 | Loss: 0.00003268
Iteration 132/1000 | Loss: 0.00003268
Iteration 133/1000 | Loss: 0.00003268
Iteration 134/1000 | Loss: 0.00003267
Iteration 135/1000 | Loss: 0.00003267
Iteration 136/1000 | Loss: 0.00003267
Iteration 137/1000 | Loss: 0.00003267
Iteration 138/1000 | Loss: 0.00003267
Iteration 139/1000 | Loss: 0.00003267
Iteration 140/1000 | Loss: 0.00003267
Iteration 141/1000 | Loss: 0.00003267
Iteration 142/1000 | Loss: 0.00003267
Iteration 143/1000 | Loss: 0.00003266
Iteration 144/1000 | Loss: 0.00003266
Iteration 145/1000 | Loss: 0.00003266
Iteration 146/1000 | Loss: 0.00003265
Iteration 147/1000 | Loss: 0.00003265
Iteration 148/1000 | Loss: 0.00003265
Iteration 149/1000 | Loss: 0.00003265
Iteration 150/1000 | Loss: 0.00003265
Iteration 151/1000 | Loss: 0.00003265
Iteration 152/1000 | Loss: 0.00003265
Iteration 153/1000 | Loss: 0.00003265
Iteration 154/1000 | Loss: 0.00003264
Iteration 155/1000 | Loss: 0.00003264
Iteration 156/1000 | Loss: 0.00003264
Iteration 157/1000 | Loss: 0.00003264
Iteration 158/1000 | Loss: 0.00003264
Iteration 159/1000 | Loss: 0.00003264
Iteration 160/1000 | Loss: 0.00003263
Iteration 161/1000 | Loss: 0.00003263
Iteration 162/1000 | Loss: 0.00003263
Iteration 163/1000 | Loss: 0.00003263
Iteration 164/1000 | Loss: 0.00003262
Iteration 165/1000 | Loss: 0.00003262
Iteration 166/1000 | Loss: 0.00003262
Iteration 167/1000 | Loss: 0.00003262
Iteration 168/1000 | Loss: 0.00003261
Iteration 169/1000 | Loss: 0.00003261
Iteration 170/1000 | Loss: 0.00003261
Iteration 171/1000 | Loss: 0.00003260
Iteration 172/1000 | Loss: 0.00003260
Iteration 173/1000 | Loss: 0.00003260
Iteration 174/1000 | Loss: 0.00003259
Iteration 175/1000 | Loss: 0.00003259
Iteration 176/1000 | Loss: 0.00003259
Iteration 177/1000 | Loss: 0.00003259
Iteration 178/1000 | Loss: 0.00003259
Iteration 179/1000 | Loss: 0.00003259
Iteration 180/1000 | Loss: 0.00003259
Iteration 181/1000 | Loss: 0.00003259
Iteration 182/1000 | Loss: 0.00003258
Iteration 183/1000 | Loss: 0.00003258
Iteration 184/1000 | Loss: 0.00003258
Iteration 185/1000 | Loss: 0.00003258
Iteration 186/1000 | Loss: 0.00003258
Iteration 187/1000 | Loss: 0.00003258
Iteration 188/1000 | Loss: 0.00003258
Iteration 189/1000 | Loss: 0.00003258
Iteration 190/1000 | Loss: 0.00003258
Iteration 191/1000 | Loss: 0.00003258
Iteration 192/1000 | Loss: 0.00003258
Iteration 193/1000 | Loss: 0.00003258
Iteration 194/1000 | Loss: 0.00003258
Iteration 195/1000 | Loss: 0.00003258
Iteration 196/1000 | Loss: 0.00003258
Iteration 197/1000 | Loss: 0.00003258
Iteration 198/1000 | Loss: 0.00003258
Iteration 199/1000 | Loss: 0.00003258
Iteration 200/1000 | Loss: 0.00003258
Iteration 201/1000 | Loss: 0.00003258
Iteration 202/1000 | Loss: 0.00003258
Iteration 203/1000 | Loss: 0.00003258
Iteration 204/1000 | Loss: 0.00003258
Iteration 205/1000 | Loss: 0.00003258
Iteration 206/1000 | Loss: 0.00003258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.2578856917098165e-05, 3.2578856917098165e-05, 3.2578856917098165e-05, 3.2578856917098165e-05, 3.2578856917098165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2578856917098165e-05

Optimization complete. Final v2v error: 4.805582523345947 mm

Highest mean error: 5.933290958404541 mm for frame 0

Lowest mean error: 3.7137537002563477 mm for frame 206

Saving results

Total time: 179.0120587348938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00665174
Iteration 2/25 | Loss: 0.00121356
Iteration 3/25 | Loss: 0.00092890
Iteration 4/25 | Loss: 0.00090074
Iteration 5/25 | Loss: 0.00089476
Iteration 6/25 | Loss: 0.00089333
Iteration 7/25 | Loss: 0.00089331
Iteration 8/25 | Loss: 0.00089331
Iteration 9/25 | Loss: 0.00089331
Iteration 10/25 | Loss: 0.00089331
Iteration 11/25 | Loss: 0.00089331
Iteration 12/25 | Loss: 0.00089331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008933115750551224, 0.0008933115750551224, 0.0008933115750551224, 0.0008933115750551224, 0.0008933115750551224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008933115750551224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37289762
Iteration 2/25 | Loss: 0.00036487
Iteration 3/25 | Loss: 0.00036483
Iteration 4/25 | Loss: 0.00036483
Iteration 5/25 | Loss: 0.00036483
Iteration 6/25 | Loss: 0.00036483
Iteration 7/25 | Loss: 0.00036483
Iteration 8/25 | Loss: 0.00036483
Iteration 9/25 | Loss: 0.00036483
Iteration 10/25 | Loss: 0.00036483
Iteration 11/25 | Loss: 0.00036483
Iteration 12/25 | Loss: 0.00036483
Iteration 13/25 | Loss: 0.00036483
Iteration 14/25 | Loss: 0.00036483
Iteration 15/25 | Loss: 0.00036483
Iteration 16/25 | Loss: 0.00036483
Iteration 17/25 | Loss: 0.00036483
Iteration 18/25 | Loss: 0.00036483
Iteration 19/25 | Loss: 0.00036483
Iteration 20/25 | Loss: 0.00036483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003648281272035092, 0.0003648281272035092, 0.0003648281272035092, 0.0003648281272035092, 0.0003648281272035092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003648281272035092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036483
Iteration 2/1000 | Loss: 0.00002244
Iteration 3/1000 | Loss: 0.00001868
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001562
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001475
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001398
Iteration 25/1000 | Loss: 0.00001397
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001395
Iteration 36/1000 | Loss: 0.00001395
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001394
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001392
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001392
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001392
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001392
Iteration 89/1000 | Loss: 0.00001392
Iteration 90/1000 | Loss: 0.00001392
Iteration 91/1000 | Loss: 0.00001392
Iteration 92/1000 | Loss: 0.00001392
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.3920728633820545e-05, 1.3920728633820545e-05, 1.3920728633820545e-05, 1.3920728633820545e-05, 1.3920728633820545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3920728633820545e-05

Optimization complete. Final v2v error: 3.204233407974243 mm

Highest mean error: 3.5036211013793945 mm for frame 191

Lowest mean error: 3.0252761840820312 mm for frame 1

Saving results

Total time: 31.461134433746338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465566
Iteration 2/25 | Loss: 0.00131323
Iteration 3/25 | Loss: 0.00093965
Iteration 4/25 | Loss: 0.00088728
Iteration 5/25 | Loss: 0.00087796
Iteration 6/25 | Loss: 0.00087552
Iteration 7/25 | Loss: 0.00087504
Iteration 8/25 | Loss: 0.00087504
Iteration 9/25 | Loss: 0.00087504
Iteration 10/25 | Loss: 0.00087504
Iteration 11/25 | Loss: 0.00087504
Iteration 12/25 | Loss: 0.00087504
Iteration 13/25 | Loss: 0.00087504
Iteration 14/25 | Loss: 0.00087504
Iteration 15/25 | Loss: 0.00087504
Iteration 16/25 | Loss: 0.00087504
Iteration 17/25 | Loss: 0.00087504
Iteration 18/25 | Loss: 0.00087504
Iteration 19/25 | Loss: 0.00087504
Iteration 20/25 | Loss: 0.00087504
Iteration 21/25 | Loss: 0.00087504
Iteration 22/25 | Loss: 0.00087504
Iteration 23/25 | Loss: 0.00087504
Iteration 24/25 | Loss: 0.00087504
Iteration 25/25 | Loss: 0.00087504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46974170
Iteration 2/25 | Loss: 0.00033689
Iteration 3/25 | Loss: 0.00033688
Iteration 4/25 | Loss: 0.00033688
Iteration 5/25 | Loss: 0.00033688
Iteration 6/25 | Loss: 0.00033688
Iteration 7/25 | Loss: 0.00033688
Iteration 8/25 | Loss: 0.00033688
Iteration 9/25 | Loss: 0.00033688
Iteration 10/25 | Loss: 0.00033688
Iteration 11/25 | Loss: 0.00033688
Iteration 12/25 | Loss: 0.00033688
Iteration 13/25 | Loss: 0.00033688
Iteration 14/25 | Loss: 0.00033688
Iteration 15/25 | Loss: 0.00033688
Iteration 16/25 | Loss: 0.00033688
Iteration 17/25 | Loss: 0.00033688
Iteration 18/25 | Loss: 0.00033688
Iteration 19/25 | Loss: 0.00033688
Iteration 20/25 | Loss: 0.00033688
Iteration 21/25 | Loss: 0.00033688
Iteration 22/25 | Loss: 0.00033688
Iteration 23/25 | Loss: 0.00033688
Iteration 24/25 | Loss: 0.00033688
Iteration 25/25 | Loss: 0.00033688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033688
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00002208
Iteration 4/1000 | Loss: 0.00002006
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001803
Iteration 8/1000 | Loss: 0.00001765
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001737
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001720
Iteration 18/1000 | Loss: 0.00001716
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001707
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001706
Iteration 32/1000 | Loss: 0.00001705
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001704
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001701
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001697
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001696
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.696239269222133e-05, 1.696239269222133e-05, 1.696239269222133e-05, 1.696239269222133e-05, 1.696239269222133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.696239269222133e-05

Optimization complete. Final v2v error: 3.4935717582702637 mm

Highest mean error: 4.367568016052246 mm for frame 102

Lowest mean error: 3.0851759910583496 mm for frame 129

Saving results

Total time: 37.62415838241577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113398
Iteration 2/25 | Loss: 0.00426516
Iteration 3/25 | Loss: 0.00265530
Iteration 4/25 | Loss: 0.00196677
Iteration 5/25 | Loss: 0.00186238
Iteration 6/25 | Loss: 0.00162470
Iteration 7/25 | Loss: 0.00141187
Iteration 8/25 | Loss: 0.00132538
Iteration 9/25 | Loss: 0.00125380
Iteration 10/25 | Loss: 0.00121399
Iteration 11/25 | Loss: 0.00122555
Iteration 12/25 | Loss: 0.00117927
Iteration 13/25 | Loss: 0.00112312
Iteration 14/25 | Loss: 0.00108596
Iteration 15/25 | Loss: 0.00107379
Iteration 16/25 | Loss: 0.00106858
Iteration 17/25 | Loss: 0.00106210
Iteration 18/25 | Loss: 0.00105514
Iteration 19/25 | Loss: 0.00105100
Iteration 20/25 | Loss: 0.00105521
Iteration 21/25 | Loss: 0.00105470
Iteration 22/25 | Loss: 0.00105311
Iteration 23/25 | Loss: 0.00105238
Iteration 24/25 | Loss: 0.00105029
Iteration 25/25 | Loss: 0.00104537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36207545
Iteration 2/25 | Loss: 0.00137680
Iteration 3/25 | Loss: 0.00133741
Iteration 4/25 | Loss: 0.00133741
Iteration 5/25 | Loss: 0.00133741
Iteration 6/25 | Loss: 0.00133741
Iteration 7/25 | Loss: 0.00133741
Iteration 8/25 | Loss: 0.00133741
Iteration 9/25 | Loss: 0.00133741
Iteration 10/25 | Loss: 0.00133741
Iteration 11/25 | Loss: 0.00133741
Iteration 12/25 | Loss: 0.00133741
Iteration 13/25 | Loss: 0.00133741
Iteration 14/25 | Loss: 0.00133741
Iteration 15/25 | Loss: 0.00133741
Iteration 16/25 | Loss: 0.00133741
Iteration 17/25 | Loss: 0.00133741
Iteration 18/25 | Loss: 0.00133741
Iteration 19/25 | Loss: 0.00133741
Iteration 20/25 | Loss: 0.00133741
Iteration 21/25 | Loss: 0.00133741
Iteration 22/25 | Loss: 0.00133741
Iteration 23/25 | Loss: 0.00133741
Iteration 24/25 | Loss: 0.00133741
Iteration 25/25 | Loss: 0.00133741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133741
Iteration 2/1000 | Loss: 0.00042260
Iteration 3/1000 | Loss: 0.00058594
Iteration 4/1000 | Loss: 0.00085402
Iteration 5/1000 | Loss: 0.00140449
Iteration 6/1000 | Loss: 0.00014292
Iteration 7/1000 | Loss: 0.00009990
Iteration 8/1000 | Loss: 0.00036164
Iteration 9/1000 | Loss: 0.00090035
Iteration 10/1000 | Loss: 0.00188429
Iteration 11/1000 | Loss: 0.00070335
Iteration 12/1000 | Loss: 0.00021943
Iteration 13/1000 | Loss: 0.00088035
Iteration 14/1000 | Loss: 0.00019681
Iteration 15/1000 | Loss: 0.00068699
Iteration 16/1000 | Loss: 0.00017508
Iteration 17/1000 | Loss: 0.00009017
Iteration 18/1000 | Loss: 0.00034406
Iteration 19/1000 | Loss: 0.00193832
Iteration 20/1000 | Loss: 0.00137453
Iteration 21/1000 | Loss: 0.00083738
Iteration 22/1000 | Loss: 0.00086529
Iteration 23/1000 | Loss: 0.00099531
Iteration 24/1000 | Loss: 0.00056985
Iteration 25/1000 | Loss: 0.00060215
Iteration 26/1000 | Loss: 0.00038239
Iteration 27/1000 | Loss: 0.00030721
Iteration 28/1000 | Loss: 0.00008681
Iteration 29/1000 | Loss: 0.00074209
Iteration 30/1000 | Loss: 0.00090183
Iteration 31/1000 | Loss: 0.00056724
Iteration 32/1000 | Loss: 0.00040353
Iteration 33/1000 | Loss: 0.00036145
Iteration 34/1000 | Loss: 0.00008371
Iteration 35/1000 | Loss: 0.00007327
Iteration 36/1000 | Loss: 0.00039018
Iteration 37/1000 | Loss: 0.00043337
Iteration 38/1000 | Loss: 0.00028712
Iteration 39/1000 | Loss: 0.00017797
Iteration 40/1000 | Loss: 0.00030333
Iteration 41/1000 | Loss: 0.00009055
Iteration 42/1000 | Loss: 0.00006647
Iteration 43/1000 | Loss: 0.00034248
Iteration 44/1000 | Loss: 0.00028163
Iteration 45/1000 | Loss: 0.00026077
Iteration 46/1000 | Loss: 0.00007949
Iteration 47/1000 | Loss: 0.00007027
Iteration 48/1000 | Loss: 0.00072882
Iteration 49/1000 | Loss: 0.00078163
Iteration 50/1000 | Loss: 0.00023659
Iteration 51/1000 | Loss: 0.00039338
Iteration 52/1000 | Loss: 0.00023821
Iteration 53/1000 | Loss: 0.00006336
Iteration 54/1000 | Loss: 0.00006219
Iteration 55/1000 | Loss: 0.00081538
Iteration 56/1000 | Loss: 0.00137969
Iteration 57/1000 | Loss: 0.00399133
Iteration 58/1000 | Loss: 0.00102161
Iteration 59/1000 | Loss: 0.00049452
Iteration 60/1000 | Loss: 0.00054282
Iteration 61/1000 | Loss: 0.00113905
Iteration 62/1000 | Loss: 0.00009522
Iteration 63/1000 | Loss: 0.00036362
Iteration 64/1000 | Loss: 0.00033056
Iteration 65/1000 | Loss: 0.00021111
Iteration 66/1000 | Loss: 0.00036278
Iteration 67/1000 | Loss: 0.00021158
Iteration 68/1000 | Loss: 0.00083699
Iteration 69/1000 | Loss: 0.00144001
Iteration 70/1000 | Loss: 0.00101389
Iteration 71/1000 | Loss: 0.00139554
Iteration 72/1000 | Loss: 0.00110969
Iteration 73/1000 | Loss: 0.00046746
Iteration 74/1000 | Loss: 0.00106956
Iteration 75/1000 | Loss: 0.00039784
Iteration 76/1000 | Loss: 0.00024275
Iteration 77/1000 | Loss: 0.00085396
Iteration 78/1000 | Loss: 0.00061092
Iteration 79/1000 | Loss: 0.00080810
Iteration 80/1000 | Loss: 0.00033067
Iteration 81/1000 | Loss: 0.00021864
Iteration 82/1000 | Loss: 0.00016552
Iteration 83/1000 | Loss: 0.00031487
Iteration 84/1000 | Loss: 0.00031363
Iteration 85/1000 | Loss: 0.00044089
Iteration 86/1000 | Loss: 0.00017351
Iteration 87/1000 | Loss: 0.00031971
Iteration 88/1000 | Loss: 0.00057855
Iteration 89/1000 | Loss: 0.00077583
Iteration 90/1000 | Loss: 0.00032519
Iteration 91/1000 | Loss: 0.00007326
Iteration 92/1000 | Loss: 0.00005999
Iteration 93/1000 | Loss: 0.00005449
Iteration 94/1000 | Loss: 0.00059845
Iteration 95/1000 | Loss: 0.00069339
Iteration 96/1000 | Loss: 0.00027874
Iteration 97/1000 | Loss: 0.00053889
Iteration 98/1000 | Loss: 0.00014608
Iteration 99/1000 | Loss: 0.00068376
Iteration 100/1000 | Loss: 0.00068857
Iteration 101/1000 | Loss: 0.00007062
Iteration 102/1000 | Loss: 0.00046633
Iteration 103/1000 | Loss: 0.00051518
Iteration 104/1000 | Loss: 0.00080619
Iteration 105/1000 | Loss: 0.00043196
Iteration 106/1000 | Loss: 0.00054150
Iteration 107/1000 | Loss: 0.00007560
Iteration 108/1000 | Loss: 0.00006281
Iteration 109/1000 | Loss: 0.00005730
Iteration 110/1000 | Loss: 0.00074153
Iteration 111/1000 | Loss: 0.00062446
Iteration 112/1000 | Loss: 0.00034605
Iteration 113/1000 | Loss: 0.00018976
Iteration 114/1000 | Loss: 0.00018649
Iteration 115/1000 | Loss: 0.00007674
Iteration 116/1000 | Loss: 0.00015241
Iteration 117/1000 | Loss: 0.00007348
Iteration 118/1000 | Loss: 0.00012316
Iteration 119/1000 | Loss: 0.00070555
Iteration 120/1000 | Loss: 0.00021957
Iteration 121/1000 | Loss: 0.00040326
Iteration 122/1000 | Loss: 0.00006221
Iteration 123/1000 | Loss: 0.00017757
Iteration 124/1000 | Loss: 0.00019862
Iteration 125/1000 | Loss: 0.00015010
Iteration 126/1000 | Loss: 0.00041468
Iteration 127/1000 | Loss: 0.00087022
Iteration 128/1000 | Loss: 0.00107583
Iteration 129/1000 | Loss: 0.00072874
Iteration 130/1000 | Loss: 0.00086583
Iteration 131/1000 | Loss: 0.00060047
Iteration 132/1000 | Loss: 0.00011959
Iteration 133/1000 | Loss: 0.00055859
Iteration 134/1000 | Loss: 0.00060090
Iteration 135/1000 | Loss: 0.00058605
Iteration 136/1000 | Loss: 0.00008776
Iteration 137/1000 | Loss: 0.00059396
Iteration 138/1000 | Loss: 0.00011371
Iteration 139/1000 | Loss: 0.00030425
Iteration 140/1000 | Loss: 0.00024303
Iteration 141/1000 | Loss: 0.00019409
Iteration 142/1000 | Loss: 0.00029327
Iteration 143/1000 | Loss: 0.00019123
Iteration 144/1000 | Loss: 0.00026231
Iteration 145/1000 | Loss: 0.00019779
Iteration 146/1000 | Loss: 0.00018048
Iteration 147/1000 | Loss: 0.00029998
Iteration 148/1000 | Loss: 0.00018994
Iteration 149/1000 | Loss: 0.00016623
Iteration 150/1000 | Loss: 0.00008598
Iteration 151/1000 | Loss: 0.00015846
Iteration 152/1000 | Loss: 0.00018900
Iteration 153/1000 | Loss: 0.00045171
Iteration 154/1000 | Loss: 0.00028384
Iteration 155/1000 | Loss: 0.00012970
Iteration 156/1000 | Loss: 0.00013318
Iteration 157/1000 | Loss: 0.00017127
Iteration 158/1000 | Loss: 0.00013016
Iteration 159/1000 | Loss: 0.00017523
Iteration 160/1000 | Loss: 0.00018290
Iteration 161/1000 | Loss: 0.00016289
Iteration 162/1000 | Loss: 0.00020759
Iteration 163/1000 | Loss: 0.00053895
Iteration 164/1000 | Loss: 0.00015928
Iteration 165/1000 | Loss: 0.00031048
Iteration 166/1000 | Loss: 0.00023366
Iteration 167/1000 | Loss: 0.00019448
Iteration 168/1000 | Loss: 0.00056006
Iteration 169/1000 | Loss: 0.00019970
Iteration 170/1000 | Loss: 0.00032296
Iteration 171/1000 | Loss: 0.00015115
Iteration 172/1000 | Loss: 0.00045104
Iteration 173/1000 | Loss: 0.00027106
Iteration 174/1000 | Loss: 0.00006189
Iteration 175/1000 | Loss: 0.00005402
Iteration 176/1000 | Loss: 0.00019589
Iteration 177/1000 | Loss: 0.00006167
Iteration 178/1000 | Loss: 0.00005333
Iteration 179/1000 | Loss: 0.00004990
Iteration 180/1000 | Loss: 0.00004840
Iteration 181/1000 | Loss: 0.00004582
Iteration 182/1000 | Loss: 0.00004507
Iteration 183/1000 | Loss: 0.00004450
Iteration 184/1000 | Loss: 0.00004382
Iteration 185/1000 | Loss: 0.00004337
Iteration 186/1000 | Loss: 0.00004310
Iteration 187/1000 | Loss: 0.00004296
Iteration 188/1000 | Loss: 0.00052825
Iteration 189/1000 | Loss: 0.00023767
Iteration 190/1000 | Loss: 0.00038865
Iteration 191/1000 | Loss: 0.00013365
Iteration 192/1000 | Loss: 0.00032072
Iteration 193/1000 | Loss: 0.00014832
Iteration 194/1000 | Loss: 0.00004817
Iteration 195/1000 | Loss: 0.00004515
Iteration 196/1000 | Loss: 0.00033039
Iteration 197/1000 | Loss: 0.00005267
Iteration 198/1000 | Loss: 0.00023601
Iteration 199/1000 | Loss: 0.00004463
Iteration 200/1000 | Loss: 0.00033668
Iteration 201/1000 | Loss: 0.00004385
Iteration 202/1000 | Loss: 0.00004233
Iteration 203/1000 | Loss: 0.00004100
Iteration 204/1000 | Loss: 0.00003979
Iteration 205/1000 | Loss: 0.00003918
Iteration 206/1000 | Loss: 0.00003876
Iteration 207/1000 | Loss: 0.00003848
Iteration 208/1000 | Loss: 0.00003844
Iteration 209/1000 | Loss: 0.00003833
Iteration 210/1000 | Loss: 0.00003833
Iteration 211/1000 | Loss: 0.00003831
Iteration 212/1000 | Loss: 0.00003831
Iteration 213/1000 | Loss: 0.00003831
Iteration 214/1000 | Loss: 0.00003830
Iteration 215/1000 | Loss: 0.00003830
Iteration 216/1000 | Loss: 0.00003830
Iteration 217/1000 | Loss: 0.00003829
Iteration 218/1000 | Loss: 0.00003826
Iteration 219/1000 | Loss: 0.00003825
Iteration 220/1000 | Loss: 0.00003820
Iteration 221/1000 | Loss: 0.00003814
Iteration 222/1000 | Loss: 0.00003812
Iteration 223/1000 | Loss: 0.00066409
Iteration 224/1000 | Loss: 0.00004161
Iteration 225/1000 | Loss: 0.00003832
Iteration 226/1000 | Loss: 0.00003721
Iteration 227/1000 | Loss: 0.00003608
Iteration 228/1000 | Loss: 0.00003521
Iteration 229/1000 | Loss: 0.00003485
Iteration 230/1000 | Loss: 0.00003466
Iteration 231/1000 | Loss: 0.00003448
Iteration 232/1000 | Loss: 0.00003438
Iteration 233/1000 | Loss: 0.00003436
Iteration 234/1000 | Loss: 0.00003436
Iteration 235/1000 | Loss: 0.00003435
Iteration 236/1000 | Loss: 0.00003434
Iteration 237/1000 | Loss: 0.00003431
Iteration 238/1000 | Loss: 0.00003431
Iteration 239/1000 | Loss: 0.00003431
Iteration 240/1000 | Loss: 0.00003431
Iteration 241/1000 | Loss: 0.00003430
Iteration 242/1000 | Loss: 0.00003430
Iteration 243/1000 | Loss: 0.00003429
Iteration 244/1000 | Loss: 0.00003428
Iteration 245/1000 | Loss: 0.00003428
Iteration 246/1000 | Loss: 0.00003428
Iteration 247/1000 | Loss: 0.00003427
Iteration 248/1000 | Loss: 0.00003427
Iteration 249/1000 | Loss: 0.00003426
Iteration 250/1000 | Loss: 0.00003426
Iteration 251/1000 | Loss: 0.00003426
Iteration 252/1000 | Loss: 0.00003426
Iteration 253/1000 | Loss: 0.00003425
Iteration 254/1000 | Loss: 0.00003425
Iteration 255/1000 | Loss: 0.00003425
Iteration 256/1000 | Loss: 0.00003425
Iteration 257/1000 | Loss: 0.00003424
Iteration 258/1000 | Loss: 0.00003424
Iteration 259/1000 | Loss: 0.00003424
Iteration 260/1000 | Loss: 0.00003424
Iteration 261/1000 | Loss: 0.00003424
Iteration 262/1000 | Loss: 0.00003424
Iteration 263/1000 | Loss: 0.00003424
Iteration 264/1000 | Loss: 0.00003424
Iteration 265/1000 | Loss: 0.00003424
Iteration 266/1000 | Loss: 0.00003424
Iteration 267/1000 | Loss: 0.00003423
Iteration 268/1000 | Loss: 0.00003423
Iteration 269/1000 | Loss: 0.00003423
Iteration 270/1000 | Loss: 0.00003423
Iteration 271/1000 | Loss: 0.00003423
Iteration 272/1000 | Loss: 0.00003423
Iteration 273/1000 | Loss: 0.00003423
Iteration 274/1000 | Loss: 0.00003422
Iteration 275/1000 | Loss: 0.00003422
Iteration 276/1000 | Loss: 0.00003422
Iteration 277/1000 | Loss: 0.00003422
Iteration 278/1000 | Loss: 0.00003422
Iteration 279/1000 | Loss: 0.00003421
Iteration 280/1000 | Loss: 0.00003421
Iteration 281/1000 | Loss: 0.00003421
Iteration 282/1000 | Loss: 0.00003421
Iteration 283/1000 | Loss: 0.00003421
Iteration 284/1000 | Loss: 0.00003421
Iteration 285/1000 | Loss: 0.00003421
Iteration 286/1000 | Loss: 0.00003421
Iteration 287/1000 | Loss: 0.00003421
Iteration 288/1000 | Loss: 0.00003421
Iteration 289/1000 | Loss: 0.00003421
Iteration 290/1000 | Loss: 0.00003421
Iteration 291/1000 | Loss: 0.00003420
Iteration 292/1000 | Loss: 0.00003420
Iteration 293/1000 | Loss: 0.00003420
Iteration 294/1000 | Loss: 0.00003420
Iteration 295/1000 | Loss: 0.00003420
Iteration 296/1000 | Loss: 0.00003420
Iteration 297/1000 | Loss: 0.00003420
Iteration 298/1000 | Loss: 0.00003420
Iteration 299/1000 | Loss: 0.00003420
Iteration 300/1000 | Loss: 0.00003420
Iteration 301/1000 | Loss: 0.00003419
Iteration 302/1000 | Loss: 0.00003419
Iteration 303/1000 | Loss: 0.00003419
Iteration 304/1000 | Loss: 0.00003419
Iteration 305/1000 | Loss: 0.00003419
Iteration 306/1000 | Loss: 0.00003419
Iteration 307/1000 | Loss: 0.00003419
Iteration 308/1000 | Loss: 0.00003418
Iteration 309/1000 | Loss: 0.00003418
Iteration 310/1000 | Loss: 0.00003418
Iteration 311/1000 | Loss: 0.00003418
Iteration 312/1000 | Loss: 0.00003418
Iteration 313/1000 | Loss: 0.00003418
Iteration 314/1000 | Loss: 0.00003417
Iteration 315/1000 | Loss: 0.00003417
Iteration 316/1000 | Loss: 0.00003417
Iteration 317/1000 | Loss: 0.00003417
Iteration 318/1000 | Loss: 0.00003416
Iteration 319/1000 | Loss: 0.00003416
Iteration 320/1000 | Loss: 0.00003416
Iteration 321/1000 | Loss: 0.00003416
Iteration 322/1000 | Loss: 0.00003416
Iteration 323/1000 | Loss: 0.00003416
Iteration 324/1000 | Loss: 0.00003416
Iteration 325/1000 | Loss: 0.00003416
Iteration 326/1000 | Loss: 0.00003416
Iteration 327/1000 | Loss: 0.00003416
Iteration 328/1000 | Loss: 0.00003416
Iteration 329/1000 | Loss: 0.00003415
Iteration 330/1000 | Loss: 0.00003415
Iteration 331/1000 | Loss: 0.00003415
Iteration 332/1000 | Loss: 0.00003415
Iteration 333/1000 | Loss: 0.00003415
Iteration 334/1000 | Loss: 0.00003415
Iteration 335/1000 | Loss: 0.00003415
Iteration 336/1000 | Loss: 0.00003415
Iteration 337/1000 | Loss: 0.00003415
Iteration 338/1000 | Loss: 0.00003414
Iteration 339/1000 | Loss: 0.00003414
Iteration 340/1000 | Loss: 0.00003414
Iteration 341/1000 | Loss: 0.00003414
Iteration 342/1000 | Loss: 0.00003414
Iteration 343/1000 | Loss: 0.00003414
Iteration 344/1000 | Loss: 0.00003414
Iteration 345/1000 | Loss: 0.00003414
Iteration 346/1000 | Loss: 0.00003414
Iteration 347/1000 | Loss: 0.00003414
Iteration 348/1000 | Loss: 0.00003414
Iteration 349/1000 | Loss: 0.00003414
Iteration 350/1000 | Loss: 0.00003414
Iteration 351/1000 | Loss: 0.00003414
Iteration 352/1000 | Loss: 0.00003414
Iteration 353/1000 | Loss: 0.00003414
Iteration 354/1000 | Loss: 0.00003414
Iteration 355/1000 | Loss: 0.00003414
Iteration 356/1000 | Loss: 0.00003414
Iteration 357/1000 | Loss: 0.00003414
Iteration 358/1000 | Loss: 0.00003414
Iteration 359/1000 | Loss: 0.00003414
Iteration 360/1000 | Loss: 0.00003414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 360. Stopping optimization.
Last 5 losses: [3.4143122320529073e-05, 3.4143122320529073e-05, 3.4143122320529073e-05, 3.4143122320529073e-05, 3.4143122320529073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4143122320529073e-05

Optimization complete. Final v2v error: 4.311432838439941 mm

Highest mean error: 12.08438491821289 mm for frame 148

Lowest mean error: 3.436941146850586 mm for frame 143

Saving results

Total time: 366.3913185596466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864642
Iteration 2/25 | Loss: 0.00122702
Iteration 3/25 | Loss: 0.00100190
Iteration 4/25 | Loss: 0.00096424
Iteration 5/25 | Loss: 0.00095881
Iteration 6/25 | Loss: 0.00095816
Iteration 7/25 | Loss: 0.00095816
Iteration 8/25 | Loss: 0.00095816
Iteration 9/25 | Loss: 0.00095816
Iteration 10/25 | Loss: 0.00095816
Iteration 11/25 | Loss: 0.00095816
Iteration 12/25 | Loss: 0.00095816
Iteration 13/25 | Loss: 0.00095816
Iteration 14/25 | Loss: 0.00095816
Iteration 15/25 | Loss: 0.00095816
Iteration 16/25 | Loss: 0.00095816
Iteration 17/25 | Loss: 0.00095816
Iteration 18/25 | Loss: 0.00095816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000958158343564719, 0.000958158343564719, 0.000958158343564719, 0.000958158343564719, 0.000958158343564719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000958158343564719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24140310
Iteration 2/25 | Loss: 0.00042718
Iteration 3/25 | Loss: 0.00042713
Iteration 4/25 | Loss: 0.00042713
Iteration 5/25 | Loss: 0.00042713
Iteration 6/25 | Loss: 0.00042713
Iteration 7/25 | Loss: 0.00042713
Iteration 8/25 | Loss: 0.00042713
Iteration 9/25 | Loss: 0.00042713
Iteration 10/25 | Loss: 0.00042713
Iteration 11/25 | Loss: 0.00042713
Iteration 12/25 | Loss: 0.00042713
Iteration 13/25 | Loss: 0.00042713
Iteration 14/25 | Loss: 0.00042713
Iteration 15/25 | Loss: 0.00042713
Iteration 16/25 | Loss: 0.00042713
Iteration 17/25 | Loss: 0.00042713
Iteration 18/25 | Loss: 0.00042713
Iteration 19/25 | Loss: 0.00042713
Iteration 20/25 | Loss: 0.00042713
Iteration 21/25 | Loss: 0.00042713
Iteration 22/25 | Loss: 0.00042713
Iteration 23/25 | Loss: 0.00042713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004271264187991619, 0.0004271264187991619, 0.0004271264187991619, 0.0004271264187991619, 0.0004271264187991619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004271264187991619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042713
Iteration 2/1000 | Loss: 0.00004503
Iteration 3/1000 | Loss: 0.00003654
Iteration 4/1000 | Loss: 0.00003238
Iteration 5/1000 | Loss: 0.00003122
Iteration 6/1000 | Loss: 0.00003004
Iteration 7/1000 | Loss: 0.00002923
Iteration 8/1000 | Loss: 0.00002850
Iteration 9/1000 | Loss: 0.00002798
Iteration 10/1000 | Loss: 0.00002764
Iteration 11/1000 | Loss: 0.00002740
Iteration 12/1000 | Loss: 0.00002718
Iteration 13/1000 | Loss: 0.00002705
Iteration 14/1000 | Loss: 0.00002697
Iteration 15/1000 | Loss: 0.00002694
Iteration 16/1000 | Loss: 0.00002693
Iteration 17/1000 | Loss: 0.00002692
Iteration 18/1000 | Loss: 0.00002692
Iteration 19/1000 | Loss: 0.00002691
Iteration 20/1000 | Loss: 0.00002690
Iteration 21/1000 | Loss: 0.00002686
Iteration 22/1000 | Loss: 0.00002686
Iteration 23/1000 | Loss: 0.00002685
Iteration 24/1000 | Loss: 0.00002685
Iteration 25/1000 | Loss: 0.00002684
Iteration 26/1000 | Loss: 0.00002684
Iteration 27/1000 | Loss: 0.00002684
Iteration 28/1000 | Loss: 0.00002683
Iteration 29/1000 | Loss: 0.00002683
Iteration 30/1000 | Loss: 0.00002683
Iteration 31/1000 | Loss: 0.00002682
Iteration 32/1000 | Loss: 0.00002682
Iteration 33/1000 | Loss: 0.00002682
Iteration 34/1000 | Loss: 0.00002682
Iteration 35/1000 | Loss: 0.00002681
Iteration 36/1000 | Loss: 0.00002681
Iteration 37/1000 | Loss: 0.00002681
Iteration 38/1000 | Loss: 0.00002680
Iteration 39/1000 | Loss: 0.00002680
Iteration 40/1000 | Loss: 0.00002680
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002679
Iteration 43/1000 | Loss: 0.00002679
Iteration 44/1000 | Loss: 0.00002679
Iteration 45/1000 | Loss: 0.00002679
Iteration 46/1000 | Loss: 0.00002678
Iteration 47/1000 | Loss: 0.00002678
Iteration 48/1000 | Loss: 0.00002678
Iteration 49/1000 | Loss: 0.00002678
Iteration 50/1000 | Loss: 0.00002677
Iteration 51/1000 | Loss: 0.00002677
Iteration 52/1000 | Loss: 0.00002677
Iteration 53/1000 | Loss: 0.00002677
Iteration 54/1000 | Loss: 0.00002677
Iteration 55/1000 | Loss: 0.00002676
Iteration 56/1000 | Loss: 0.00002676
Iteration 57/1000 | Loss: 0.00002676
Iteration 58/1000 | Loss: 0.00002675
Iteration 59/1000 | Loss: 0.00002675
Iteration 60/1000 | Loss: 0.00002675
Iteration 61/1000 | Loss: 0.00002675
Iteration 62/1000 | Loss: 0.00002675
Iteration 63/1000 | Loss: 0.00002674
Iteration 64/1000 | Loss: 0.00002674
Iteration 65/1000 | Loss: 0.00002674
Iteration 66/1000 | Loss: 0.00002674
Iteration 67/1000 | Loss: 0.00002673
Iteration 68/1000 | Loss: 0.00002673
Iteration 69/1000 | Loss: 0.00002673
Iteration 70/1000 | Loss: 0.00002673
Iteration 71/1000 | Loss: 0.00002672
Iteration 72/1000 | Loss: 0.00002672
Iteration 73/1000 | Loss: 0.00002672
Iteration 74/1000 | Loss: 0.00002672
Iteration 75/1000 | Loss: 0.00002672
Iteration 76/1000 | Loss: 0.00002672
Iteration 77/1000 | Loss: 0.00002671
Iteration 78/1000 | Loss: 0.00002671
Iteration 79/1000 | Loss: 0.00002671
Iteration 80/1000 | Loss: 0.00002671
Iteration 81/1000 | Loss: 0.00002671
Iteration 82/1000 | Loss: 0.00002670
Iteration 83/1000 | Loss: 0.00002670
Iteration 84/1000 | Loss: 0.00002670
Iteration 85/1000 | Loss: 0.00002670
Iteration 86/1000 | Loss: 0.00002670
Iteration 87/1000 | Loss: 0.00002670
Iteration 88/1000 | Loss: 0.00002670
Iteration 89/1000 | Loss: 0.00002669
Iteration 90/1000 | Loss: 0.00002669
Iteration 91/1000 | Loss: 0.00002669
Iteration 92/1000 | Loss: 0.00002669
Iteration 93/1000 | Loss: 0.00002669
Iteration 94/1000 | Loss: 0.00002668
Iteration 95/1000 | Loss: 0.00002668
Iteration 96/1000 | Loss: 0.00002668
Iteration 97/1000 | Loss: 0.00002668
Iteration 98/1000 | Loss: 0.00002668
Iteration 99/1000 | Loss: 0.00002668
Iteration 100/1000 | Loss: 0.00002667
Iteration 101/1000 | Loss: 0.00002667
Iteration 102/1000 | Loss: 0.00002667
Iteration 103/1000 | Loss: 0.00002666
Iteration 104/1000 | Loss: 0.00002666
Iteration 105/1000 | Loss: 0.00002666
Iteration 106/1000 | Loss: 0.00002666
Iteration 107/1000 | Loss: 0.00002666
Iteration 108/1000 | Loss: 0.00002666
Iteration 109/1000 | Loss: 0.00002666
Iteration 110/1000 | Loss: 0.00002666
Iteration 111/1000 | Loss: 0.00002666
Iteration 112/1000 | Loss: 0.00002666
Iteration 113/1000 | Loss: 0.00002665
Iteration 114/1000 | Loss: 0.00002665
Iteration 115/1000 | Loss: 0.00002665
Iteration 116/1000 | Loss: 0.00002665
Iteration 117/1000 | Loss: 0.00002665
Iteration 118/1000 | Loss: 0.00002665
Iteration 119/1000 | Loss: 0.00002665
Iteration 120/1000 | Loss: 0.00002665
Iteration 121/1000 | Loss: 0.00002665
Iteration 122/1000 | Loss: 0.00002665
Iteration 123/1000 | Loss: 0.00002664
Iteration 124/1000 | Loss: 0.00002664
Iteration 125/1000 | Loss: 0.00002664
Iteration 126/1000 | Loss: 0.00002664
Iteration 127/1000 | Loss: 0.00002664
Iteration 128/1000 | Loss: 0.00002664
Iteration 129/1000 | Loss: 0.00002664
Iteration 130/1000 | Loss: 0.00002664
Iteration 131/1000 | Loss: 0.00002664
Iteration 132/1000 | Loss: 0.00002664
Iteration 133/1000 | Loss: 0.00002664
Iteration 134/1000 | Loss: 0.00002664
Iteration 135/1000 | Loss: 0.00002664
Iteration 136/1000 | Loss: 0.00002664
Iteration 137/1000 | Loss: 0.00002664
Iteration 138/1000 | Loss: 0.00002664
Iteration 139/1000 | Loss: 0.00002664
Iteration 140/1000 | Loss: 0.00002664
Iteration 141/1000 | Loss: 0.00002664
Iteration 142/1000 | Loss: 0.00002664
Iteration 143/1000 | Loss: 0.00002664
Iteration 144/1000 | Loss: 0.00002664
Iteration 145/1000 | Loss: 0.00002664
Iteration 146/1000 | Loss: 0.00002664
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002664
Iteration 149/1000 | Loss: 0.00002664
Iteration 150/1000 | Loss: 0.00002664
Iteration 151/1000 | Loss: 0.00002664
Iteration 152/1000 | Loss: 0.00002664
Iteration 153/1000 | Loss: 0.00002664
Iteration 154/1000 | Loss: 0.00002664
Iteration 155/1000 | Loss: 0.00002664
Iteration 156/1000 | Loss: 0.00002664
Iteration 157/1000 | Loss: 0.00002664
Iteration 158/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.663546365511138e-05, 2.663546365511138e-05, 2.663546365511138e-05, 2.663546365511138e-05, 2.663546365511138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.663546365511138e-05

Optimization complete. Final v2v error: 4.441431045532227 mm

Highest mean error: 5.452635765075684 mm for frame 55

Lowest mean error: 3.796741485595703 mm for frame 0

Saving results

Total time: 37.76260042190552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143664
Iteration 2/25 | Loss: 0.00157111
Iteration 3/25 | Loss: 0.00121603
Iteration 4/25 | Loss: 0.00113061
Iteration 5/25 | Loss: 0.00109986
Iteration 6/25 | Loss: 0.00107448
Iteration 7/25 | Loss: 0.00106565
Iteration 8/25 | Loss: 0.00106750
Iteration 9/25 | Loss: 0.00106001
Iteration 10/25 | Loss: 0.00105339
Iteration 11/25 | Loss: 0.00105135
Iteration 12/25 | Loss: 0.00105059
Iteration 13/25 | Loss: 0.00105036
Iteration 14/25 | Loss: 0.00105026
Iteration 15/25 | Loss: 0.00105026
Iteration 16/25 | Loss: 0.00105026
Iteration 17/25 | Loss: 0.00105026
Iteration 18/25 | Loss: 0.00105025
Iteration 19/25 | Loss: 0.00105025
Iteration 20/25 | Loss: 0.00105025
Iteration 21/25 | Loss: 0.00105025
Iteration 22/25 | Loss: 0.00105025
Iteration 23/25 | Loss: 0.00105025
Iteration 24/25 | Loss: 0.00105025
Iteration 25/25 | Loss: 0.00105025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.13817120
Iteration 2/25 | Loss: 0.00043151
Iteration 3/25 | Loss: 0.00043128
Iteration 4/25 | Loss: 0.00043128
Iteration 5/25 | Loss: 0.00043128
Iteration 6/25 | Loss: 0.00043128
Iteration 7/25 | Loss: 0.00043128
Iteration 8/25 | Loss: 0.00043128
Iteration 9/25 | Loss: 0.00043128
Iteration 10/25 | Loss: 0.00043128
Iteration 11/25 | Loss: 0.00043128
Iteration 12/25 | Loss: 0.00043128
Iteration 13/25 | Loss: 0.00043128
Iteration 14/25 | Loss: 0.00043128
Iteration 15/25 | Loss: 0.00043128
Iteration 16/25 | Loss: 0.00043128
Iteration 17/25 | Loss: 0.00043128
Iteration 18/25 | Loss: 0.00043128
Iteration 19/25 | Loss: 0.00043128
Iteration 20/25 | Loss: 0.00043128
Iteration 21/25 | Loss: 0.00043128
Iteration 22/25 | Loss: 0.00043128
Iteration 23/25 | Loss: 0.00043128
Iteration 24/25 | Loss: 0.00043128
Iteration 25/25 | Loss: 0.00043128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043128
Iteration 2/1000 | Loss: 0.00006854
Iteration 3/1000 | Loss: 0.00005453
Iteration 4/1000 | Loss: 0.00004810
Iteration 5/1000 | Loss: 0.00004569
Iteration 6/1000 | Loss: 0.00004396
Iteration 7/1000 | Loss: 0.00004287
Iteration 8/1000 | Loss: 0.00004223
Iteration 9/1000 | Loss: 0.00004167
Iteration 10/1000 | Loss: 0.00004120
Iteration 11/1000 | Loss: 0.00004076
Iteration 12/1000 | Loss: 0.00004035
Iteration 13/1000 | Loss: 0.00004006
Iteration 14/1000 | Loss: 0.00003976
Iteration 15/1000 | Loss: 0.00003949
Iteration 16/1000 | Loss: 0.00003935
Iteration 17/1000 | Loss: 0.00003924
Iteration 18/1000 | Loss: 0.00003914
Iteration 19/1000 | Loss: 0.00003908
Iteration 20/1000 | Loss: 0.00003901
Iteration 21/1000 | Loss: 0.00003896
Iteration 22/1000 | Loss: 0.00003895
Iteration 23/1000 | Loss: 0.00003886
Iteration 24/1000 | Loss: 0.00003884
Iteration 25/1000 | Loss: 0.00003881
Iteration 26/1000 | Loss: 0.00003881
Iteration 27/1000 | Loss: 0.00003877
Iteration 28/1000 | Loss: 0.00003877
Iteration 29/1000 | Loss: 0.00003875
Iteration 30/1000 | Loss: 0.00003874
Iteration 31/1000 | Loss: 0.00003873
Iteration 32/1000 | Loss: 0.00003873
Iteration 33/1000 | Loss: 0.00003872
Iteration 34/1000 | Loss: 0.00003872
Iteration 35/1000 | Loss: 0.00003870
Iteration 36/1000 | Loss: 0.00003869
Iteration 37/1000 | Loss: 0.00003869
Iteration 38/1000 | Loss: 0.00003868
Iteration 39/1000 | Loss: 0.00003867
Iteration 40/1000 | Loss: 0.00003867
Iteration 41/1000 | Loss: 0.00003864
Iteration 42/1000 | Loss: 0.00003862
Iteration 43/1000 | Loss: 0.00003861
Iteration 44/1000 | Loss: 0.00003861
Iteration 45/1000 | Loss: 0.00003861
Iteration 46/1000 | Loss: 0.00003861
Iteration 47/1000 | Loss: 0.00003861
Iteration 48/1000 | Loss: 0.00003861
Iteration 49/1000 | Loss: 0.00003861
Iteration 50/1000 | Loss: 0.00003861
Iteration 51/1000 | Loss: 0.00003861
Iteration 52/1000 | Loss: 0.00003860
Iteration 53/1000 | Loss: 0.00003860
Iteration 54/1000 | Loss: 0.00003859
Iteration 55/1000 | Loss: 0.00003859
Iteration 56/1000 | Loss: 0.00003858
Iteration 57/1000 | Loss: 0.00003858
Iteration 58/1000 | Loss: 0.00003858
Iteration 59/1000 | Loss: 0.00003858
Iteration 60/1000 | Loss: 0.00003857
Iteration 61/1000 | Loss: 0.00003857
Iteration 62/1000 | Loss: 0.00003857
Iteration 63/1000 | Loss: 0.00003857
Iteration 64/1000 | Loss: 0.00003857
Iteration 65/1000 | Loss: 0.00003857
Iteration 66/1000 | Loss: 0.00003857
Iteration 67/1000 | Loss: 0.00003856
Iteration 68/1000 | Loss: 0.00003856
Iteration 69/1000 | Loss: 0.00003856
Iteration 70/1000 | Loss: 0.00003855
Iteration 71/1000 | Loss: 0.00003855
Iteration 72/1000 | Loss: 0.00003854
Iteration 73/1000 | Loss: 0.00003854
Iteration 74/1000 | Loss: 0.00003854
Iteration 75/1000 | Loss: 0.00003854
Iteration 76/1000 | Loss: 0.00003854
Iteration 77/1000 | Loss: 0.00003854
Iteration 78/1000 | Loss: 0.00003854
Iteration 79/1000 | Loss: 0.00003853
Iteration 80/1000 | Loss: 0.00003853
Iteration 81/1000 | Loss: 0.00003853
Iteration 82/1000 | Loss: 0.00003853
Iteration 83/1000 | Loss: 0.00003852
Iteration 84/1000 | Loss: 0.00003852
Iteration 85/1000 | Loss: 0.00003852
Iteration 86/1000 | Loss: 0.00003851
Iteration 87/1000 | Loss: 0.00003851
Iteration 88/1000 | Loss: 0.00003851
Iteration 89/1000 | Loss: 0.00003851
Iteration 90/1000 | Loss: 0.00003850
Iteration 91/1000 | Loss: 0.00003850
Iteration 92/1000 | Loss: 0.00003850
Iteration 93/1000 | Loss: 0.00003850
Iteration 94/1000 | Loss: 0.00003850
Iteration 95/1000 | Loss: 0.00003850
Iteration 96/1000 | Loss: 0.00003849
Iteration 97/1000 | Loss: 0.00003849
Iteration 98/1000 | Loss: 0.00003848
Iteration 99/1000 | Loss: 0.00003848
Iteration 100/1000 | Loss: 0.00003848
Iteration 101/1000 | Loss: 0.00003848
Iteration 102/1000 | Loss: 0.00003847
Iteration 103/1000 | Loss: 0.00003847
Iteration 104/1000 | Loss: 0.00003847
Iteration 105/1000 | Loss: 0.00003847
Iteration 106/1000 | Loss: 0.00003846
Iteration 107/1000 | Loss: 0.00003846
Iteration 108/1000 | Loss: 0.00003846
Iteration 109/1000 | Loss: 0.00003846
Iteration 110/1000 | Loss: 0.00003845
Iteration 111/1000 | Loss: 0.00003845
Iteration 112/1000 | Loss: 0.00003845
Iteration 113/1000 | Loss: 0.00003845
Iteration 114/1000 | Loss: 0.00003845
Iteration 115/1000 | Loss: 0.00003844
Iteration 116/1000 | Loss: 0.00003844
Iteration 117/1000 | Loss: 0.00003844
Iteration 118/1000 | Loss: 0.00003844
Iteration 119/1000 | Loss: 0.00003844
Iteration 120/1000 | Loss: 0.00003844
Iteration 121/1000 | Loss: 0.00003844
Iteration 122/1000 | Loss: 0.00003844
Iteration 123/1000 | Loss: 0.00003844
Iteration 124/1000 | Loss: 0.00003843
Iteration 125/1000 | Loss: 0.00003843
Iteration 126/1000 | Loss: 0.00003843
Iteration 127/1000 | Loss: 0.00003843
Iteration 128/1000 | Loss: 0.00003843
Iteration 129/1000 | Loss: 0.00003843
Iteration 130/1000 | Loss: 0.00003842
Iteration 131/1000 | Loss: 0.00003842
Iteration 132/1000 | Loss: 0.00003842
Iteration 133/1000 | Loss: 0.00003842
Iteration 134/1000 | Loss: 0.00003842
Iteration 135/1000 | Loss: 0.00003842
Iteration 136/1000 | Loss: 0.00003842
Iteration 137/1000 | Loss: 0.00003841
Iteration 138/1000 | Loss: 0.00003841
Iteration 139/1000 | Loss: 0.00003841
Iteration 140/1000 | Loss: 0.00003841
Iteration 141/1000 | Loss: 0.00003840
Iteration 142/1000 | Loss: 0.00003840
Iteration 143/1000 | Loss: 0.00003840
Iteration 144/1000 | Loss: 0.00003840
Iteration 145/1000 | Loss: 0.00003840
Iteration 146/1000 | Loss: 0.00003839
Iteration 147/1000 | Loss: 0.00003839
Iteration 148/1000 | Loss: 0.00003839
Iteration 149/1000 | Loss: 0.00003839
Iteration 150/1000 | Loss: 0.00003839
Iteration 151/1000 | Loss: 0.00003838
Iteration 152/1000 | Loss: 0.00003838
Iteration 153/1000 | Loss: 0.00003838
Iteration 154/1000 | Loss: 0.00003838
Iteration 155/1000 | Loss: 0.00003838
Iteration 156/1000 | Loss: 0.00003838
Iteration 157/1000 | Loss: 0.00003837
Iteration 158/1000 | Loss: 0.00003837
Iteration 159/1000 | Loss: 0.00003837
Iteration 160/1000 | Loss: 0.00003837
Iteration 161/1000 | Loss: 0.00003837
Iteration 162/1000 | Loss: 0.00003837
Iteration 163/1000 | Loss: 0.00003836
Iteration 164/1000 | Loss: 0.00003836
Iteration 165/1000 | Loss: 0.00003836
Iteration 166/1000 | Loss: 0.00003836
Iteration 167/1000 | Loss: 0.00003836
Iteration 168/1000 | Loss: 0.00003836
Iteration 169/1000 | Loss: 0.00003835
Iteration 170/1000 | Loss: 0.00003835
Iteration 171/1000 | Loss: 0.00003835
Iteration 172/1000 | Loss: 0.00003835
Iteration 173/1000 | Loss: 0.00003835
Iteration 174/1000 | Loss: 0.00003835
Iteration 175/1000 | Loss: 0.00003834
Iteration 176/1000 | Loss: 0.00003834
Iteration 177/1000 | Loss: 0.00003834
Iteration 178/1000 | Loss: 0.00003834
Iteration 179/1000 | Loss: 0.00003833
Iteration 180/1000 | Loss: 0.00003833
Iteration 181/1000 | Loss: 0.00003833
Iteration 182/1000 | Loss: 0.00003833
Iteration 183/1000 | Loss: 0.00003833
Iteration 184/1000 | Loss: 0.00003833
Iteration 185/1000 | Loss: 0.00003833
Iteration 186/1000 | Loss: 0.00003833
Iteration 187/1000 | Loss: 0.00003833
Iteration 188/1000 | Loss: 0.00003832
Iteration 189/1000 | Loss: 0.00003832
Iteration 190/1000 | Loss: 0.00003832
Iteration 191/1000 | Loss: 0.00003832
Iteration 192/1000 | Loss: 0.00003832
Iteration 193/1000 | Loss: 0.00003832
Iteration 194/1000 | Loss: 0.00003832
Iteration 195/1000 | Loss: 0.00003832
Iteration 196/1000 | Loss: 0.00003832
Iteration 197/1000 | Loss: 0.00003832
Iteration 198/1000 | Loss: 0.00003832
Iteration 199/1000 | Loss: 0.00003832
Iteration 200/1000 | Loss: 0.00003832
Iteration 201/1000 | Loss: 0.00003832
Iteration 202/1000 | Loss: 0.00003832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.8315436540869996e-05, 3.8315436540869996e-05, 3.8315436540869996e-05, 3.8315436540869996e-05, 3.8315436540869996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8315436540869996e-05

Optimization complete. Final v2v error: 4.951922416687012 mm

Highest mean error: 6.988062381744385 mm for frame 99

Lowest mean error: 3.199631452560425 mm for frame 139

Saving results

Total time: 67.26479387283325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052992
Iteration 2/25 | Loss: 0.00166637
Iteration 3/25 | Loss: 0.00116955
Iteration 4/25 | Loss: 0.00109148
Iteration 5/25 | Loss: 0.00104560
Iteration 6/25 | Loss: 0.00104873
Iteration 7/25 | Loss: 0.00102686
Iteration 8/25 | Loss: 0.00100059
Iteration 9/25 | Loss: 0.00102747
Iteration 10/25 | Loss: 0.00096646
Iteration 11/25 | Loss: 0.00094714
Iteration 12/25 | Loss: 0.00094553
Iteration 13/25 | Loss: 0.00092530
Iteration 14/25 | Loss: 0.00091190
Iteration 15/25 | Loss: 0.00090436
Iteration 16/25 | Loss: 0.00089793
Iteration 17/25 | Loss: 0.00089625
Iteration 18/25 | Loss: 0.00089531
Iteration 19/25 | Loss: 0.00089467
Iteration 20/25 | Loss: 0.00089313
Iteration 21/25 | Loss: 0.00089831
Iteration 22/25 | Loss: 0.00089845
Iteration 23/25 | Loss: 0.00089602
Iteration 24/25 | Loss: 0.00089381
Iteration 25/25 | Loss: 0.00088935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47041118
Iteration 2/25 | Loss: 0.00056971
Iteration 3/25 | Loss: 0.00056971
Iteration 4/25 | Loss: 0.00055292
Iteration 5/25 | Loss: 0.00055292
Iteration 6/25 | Loss: 0.00055292
Iteration 7/25 | Loss: 0.00055291
Iteration 8/25 | Loss: 0.00055291
Iteration 9/25 | Loss: 0.00055291
Iteration 10/25 | Loss: 0.00055291
Iteration 11/25 | Loss: 0.00055291
Iteration 12/25 | Loss: 0.00055291
Iteration 13/25 | Loss: 0.00055291
Iteration 14/25 | Loss: 0.00055291
Iteration 15/25 | Loss: 0.00055291
Iteration 16/25 | Loss: 0.00055291
Iteration 17/25 | Loss: 0.00055291
Iteration 18/25 | Loss: 0.00055291
Iteration 19/25 | Loss: 0.00055291
Iteration 20/25 | Loss: 0.00055291
Iteration 21/25 | Loss: 0.00055291
Iteration 22/25 | Loss: 0.00055291
Iteration 23/25 | Loss: 0.00055291
Iteration 24/25 | Loss: 0.00055291
Iteration 25/25 | Loss: 0.00055291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055291
Iteration 2/1000 | Loss: 0.00006575
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00004074
Iteration 5/1000 | Loss: 0.00002837
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00006996
Iteration 9/1000 | Loss: 0.00002491
Iteration 10/1000 | Loss: 0.00002461
Iteration 11/1000 | Loss: 0.00003945
Iteration 12/1000 | Loss: 0.00002615
Iteration 13/1000 | Loss: 0.00002519
Iteration 14/1000 | Loss: 0.00002433
Iteration 15/1000 | Loss: 0.00002398
Iteration 16/1000 | Loss: 0.00002503
Iteration 17/1000 | Loss: 0.00002411
Iteration 18/1000 | Loss: 0.00002589
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002378
Iteration 21/1000 | Loss: 0.00002374
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002374
Iteration 24/1000 | Loss: 0.00002373
Iteration 25/1000 | Loss: 0.00002373
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002373
Iteration 28/1000 | Loss: 0.00002373
Iteration 29/1000 | Loss: 0.00002373
Iteration 30/1000 | Loss: 0.00002373
Iteration 31/1000 | Loss: 0.00002373
Iteration 32/1000 | Loss: 0.00002373
Iteration 33/1000 | Loss: 0.00002373
Iteration 34/1000 | Loss: 0.00002373
Iteration 35/1000 | Loss: 0.00002372
Iteration 36/1000 | Loss: 0.00002372
Iteration 37/1000 | Loss: 0.00002372
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002370
Iteration 40/1000 | Loss: 0.00002370
Iteration 41/1000 | Loss: 0.00002370
Iteration 42/1000 | Loss: 0.00002370
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00002370
Iteration 46/1000 | Loss: 0.00002370
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002369
Iteration 49/1000 | Loss: 0.00002369
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002368
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002367
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002552
Iteration 60/1000 | Loss: 0.00002404
Iteration 61/1000 | Loss: 0.00002363
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002363
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002540
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002364
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002362
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002362
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002539
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002533
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002393
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002359
Iteration 95/1000 | Loss: 0.00002359
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002450
Iteration 98/1000 | Loss: 0.00002371
Iteration 99/1000 | Loss: 0.00002519
Iteration 100/1000 | Loss: 0.00002518
Iteration 101/1000 | Loss: 0.00002420
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Iteration 108/1000 | Loss: 0.00002356
Iteration 109/1000 | Loss: 0.00002356
Iteration 110/1000 | Loss: 0.00002356
Iteration 111/1000 | Loss: 0.00002356
Iteration 112/1000 | Loss: 0.00002511
Iteration 113/1000 | Loss: 0.00002467
Iteration 114/1000 | Loss: 0.00002518
Iteration 115/1000 | Loss: 0.00002517
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002355
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002355
Iteration 122/1000 | Loss: 0.00002355
Iteration 123/1000 | Loss: 0.00002355
Iteration 124/1000 | Loss: 0.00002355
Iteration 125/1000 | Loss: 0.00002355
Iteration 126/1000 | Loss: 0.00002355
Iteration 127/1000 | Loss: 0.00002355
Iteration 128/1000 | Loss: 0.00002355
Iteration 129/1000 | Loss: 0.00002355
Iteration 130/1000 | Loss: 0.00002354
Iteration 131/1000 | Loss: 0.00002354
Iteration 132/1000 | Loss: 0.00002354
Iteration 133/1000 | Loss: 0.00002354
Iteration 134/1000 | Loss: 0.00002354
Iteration 135/1000 | Loss: 0.00002354
Iteration 136/1000 | Loss: 0.00002354
Iteration 137/1000 | Loss: 0.00002354
Iteration 138/1000 | Loss: 0.00002354
Iteration 139/1000 | Loss: 0.00002354
Iteration 140/1000 | Loss: 0.00002354
Iteration 141/1000 | Loss: 0.00002354
Iteration 142/1000 | Loss: 0.00002354
Iteration 143/1000 | Loss: 0.00002354
Iteration 144/1000 | Loss: 0.00002354
Iteration 145/1000 | Loss: 0.00002354
Iteration 146/1000 | Loss: 0.00002354
Iteration 147/1000 | Loss: 0.00002354
Iteration 148/1000 | Loss: 0.00002354
Iteration 149/1000 | Loss: 0.00002354
Iteration 150/1000 | Loss: 0.00002354
Iteration 151/1000 | Loss: 0.00002354
Iteration 152/1000 | Loss: 0.00002354
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.3539600078947842e-05, 2.3539600078947842e-05, 2.3539600078947842e-05, 2.3539600078947842e-05, 2.3539600078947842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3539600078947842e-05

Optimization complete. Final v2v error: 3.4107022285461426 mm

Highest mean error: 23.19146728515625 mm for frame 108

Lowest mean error: 2.8054182529449463 mm for frame 120

Saving results

Total time: 101.8694417476654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445191
Iteration 2/25 | Loss: 0.00117025
Iteration 3/25 | Loss: 0.00094363
Iteration 4/25 | Loss: 0.00091494
Iteration 5/25 | Loss: 0.00090685
Iteration 6/25 | Loss: 0.00090470
Iteration 7/25 | Loss: 0.00090408
Iteration 8/25 | Loss: 0.00090408
Iteration 9/25 | Loss: 0.00090408
Iteration 10/25 | Loss: 0.00090408
Iteration 11/25 | Loss: 0.00090408
Iteration 12/25 | Loss: 0.00090408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000904081913176924, 0.000904081913176924, 0.000904081913176924, 0.000904081913176924, 0.000904081913176924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000904081913176924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15187824
Iteration 2/25 | Loss: 0.00034438
Iteration 3/25 | Loss: 0.00034438
Iteration 4/25 | Loss: 0.00034438
Iteration 5/25 | Loss: 0.00034438
Iteration 6/25 | Loss: 0.00034438
Iteration 7/25 | Loss: 0.00034438
Iteration 8/25 | Loss: 0.00034438
Iteration 9/25 | Loss: 0.00034438
Iteration 10/25 | Loss: 0.00034438
Iteration 11/25 | Loss: 0.00034438
Iteration 12/25 | Loss: 0.00034438
Iteration 13/25 | Loss: 0.00034438
Iteration 14/25 | Loss: 0.00034438
Iteration 15/25 | Loss: 0.00034438
Iteration 16/25 | Loss: 0.00034438
Iteration 17/25 | Loss: 0.00034438
Iteration 18/25 | Loss: 0.00034438
Iteration 19/25 | Loss: 0.00034438
Iteration 20/25 | Loss: 0.00034438
Iteration 21/25 | Loss: 0.00034438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003443756140768528, 0.0003443756140768528, 0.0003443756140768528, 0.0003443756140768528, 0.0003443756140768528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003443756140768528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034438
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001849
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001773
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001729
Iteration 18/1000 | Loss: 0.00001728
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.7139695046353154e-05, 1.7139695046353154e-05, 1.7139695046353154e-05, 1.7139695046353154e-05, 1.7139695046353154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7139695046353154e-05

Optimization complete. Final v2v error: 3.4780311584472656 mm

Highest mean error: 3.761413097381592 mm for frame 90

Lowest mean error: 3.310004711151123 mm for frame 83

Saving results

Total time: 27.525866985321045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032128
Iteration 2/25 | Loss: 0.01032128
Iteration 3/25 | Loss: 0.01032128
Iteration 4/25 | Loss: 0.01032128
Iteration 5/25 | Loss: 0.01032127
Iteration 6/25 | Loss: 0.01032127
Iteration 7/25 | Loss: 0.01032127
Iteration 8/25 | Loss: 0.01032127
Iteration 9/25 | Loss: 0.01032127
Iteration 10/25 | Loss: 0.01032127
Iteration 11/25 | Loss: 0.01032127
Iteration 12/25 | Loss: 0.01032127
Iteration 13/25 | Loss: 0.01032126
Iteration 14/25 | Loss: 0.01032126
Iteration 15/25 | Loss: 0.01032126
Iteration 16/25 | Loss: 0.01032126
Iteration 17/25 | Loss: 0.01032126
Iteration 18/25 | Loss: 0.01032126
Iteration 19/25 | Loss: 0.01032125
Iteration 20/25 | Loss: 0.01032125
Iteration 21/25 | Loss: 0.01032125
Iteration 22/25 | Loss: 0.01032125
Iteration 23/25 | Loss: 0.01032125
Iteration 24/25 | Loss: 0.01032125
Iteration 25/25 | Loss: 0.01032125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79912436
Iteration 2/25 | Loss: 0.16904323
Iteration 3/25 | Loss: 0.16690634
Iteration 4/25 | Loss: 0.16576487
Iteration 5/25 | Loss: 0.16570711
Iteration 6/25 | Loss: 0.16570711
Iteration 7/25 | Loss: 0.16570711
Iteration 8/25 | Loss: 0.16570711
Iteration 9/25 | Loss: 0.16570711
Iteration 10/25 | Loss: 0.16570711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.16570711135864258, 0.16570711135864258, 0.16570711135864258, 0.16570711135864258, 0.16570711135864258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16570711135864258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16570710
Iteration 2/1000 | Loss: 0.01218734
Iteration 3/1000 | Loss: 0.00223702
Iteration 4/1000 | Loss: 0.00221463
Iteration 5/1000 | Loss: 0.00069750
Iteration 6/1000 | Loss: 0.00148701
Iteration 7/1000 | Loss: 0.00081358
Iteration 8/1000 | Loss: 0.00021669
Iteration 9/1000 | Loss: 0.00047276
Iteration 10/1000 | Loss: 0.00048446
Iteration 11/1000 | Loss: 0.00115947
Iteration 12/1000 | Loss: 0.00015280
Iteration 13/1000 | Loss: 0.00045239
Iteration 14/1000 | Loss: 0.00013645
Iteration 15/1000 | Loss: 0.00011809
Iteration 16/1000 | Loss: 0.00010379
Iteration 17/1000 | Loss: 0.00024603
Iteration 18/1000 | Loss: 0.00008733
Iteration 19/1000 | Loss: 0.00026158
Iteration 20/1000 | Loss: 0.00025237
Iteration 21/1000 | Loss: 0.00006941
Iteration 22/1000 | Loss: 0.00006370
Iteration 23/1000 | Loss: 0.00015085
Iteration 24/1000 | Loss: 0.00005998
Iteration 25/1000 | Loss: 0.00005650
Iteration 26/1000 | Loss: 0.00005471
Iteration 27/1000 | Loss: 0.00005259
Iteration 28/1000 | Loss: 0.00019902
Iteration 29/1000 | Loss: 0.00005867
Iteration 30/1000 | Loss: 0.00005413
Iteration 31/1000 | Loss: 0.00005022
Iteration 32/1000 | Loss: 0.00004888
Iteration 33/1000 | Loss: 0.00004775
Iteration 34/1000 | Loss: 0.00004703
Iteration 35/1000 | Loss: 0.00004668
Iteration 36/1000 | Loss: 0.00004641
Iteration 37/1000 | Loss: 0.00004609
Iteration 38/1000 | Loss: 0.00004577
Iteration 39/1000 | Loss: 0.00004589
Iteration 40/1000 | Loss: 0.00006317
Iteration 41/1000 | Loss: 0.00004998
Iteration 42/1000 | Loss: 0.00004734
Iteration 43/1000 | Loss: 0.00004627
Iteration 44/1000 | Loss: 0.00004578
Iteration 45/1000 | Loss: 0.00004542
Iteration 46/1000 | Loss: 0.00004494
Iteration 47/1000 | Loss: 0.00004450
Iteration 48/1000 | Loss: 0.00004454
Iteration 49/1000 | Loss: 0.00004412
Iteration 50/1000 | Loss: 0.00004392
Iteration 51/1000 | Loss: 0.00004384
Iteration 52/1000 | Loss: 0.00004407
Iteration 53/1000 | Loss: 0.00004419
Iteration 54/1000 | Loss: 0.00004419
Iteration 55/1000 | Loss: 0.00004419
Iteration 56/1000 | Loss: 0.00004448
Iteration 57/1000 | Loss: 0.00004386
Iteration 58/1000 | Loss: 0.00004364
Iteration 59/1000 | Loss: 0.00004377
Iteration 60/1000 | Loss: 0.00004389
Iteration 61/1000 | Loss: 0.00004365
Iteration 62/1000 | Loss: 0.00004364
Iteration 63/1000 | Loss: 0.00004360
Iteration 64/1000 | Loss: 0.00004345
Iteration 65/1000 | Loss: 0.00004328
Iteration 66/1000 | Loss: 0.00004328
Iteration 67/1000 | Loss: 0.00004320
Iteration 68/1000 | Loss: 0.00004320
Iteration 69/1000 | Loss: 0.00004320
Iteration 70/1000 | Loss: 0.00004320
Iteration 71/1000 | Loss: 0.00004320
Iteration 72/1000 | Loss: 0.00004320
Iteration 73/1000 | Loss: 0.00004320
Iteration 74/1000 | Loss: 0.00004319
Iteration 75/1000 | Loss: 0.00004319
Iteration 76/1000 | Loss: 0.00004319
Iteration 77/1000 | Loss: 0.00004319
Iteration 78/1000 | Loss: 0.00004319
Iteration 79/1000 | Loss: 0.00004319
Iteration 80/1000 | Loss: 0.00004319
Iteration 81/1000 | Loss: 0.00004319
Iteration 82/1000 | Loss: 0.00004318
Iteration 83/1000 | Loss: 0.00004318
Iteration 84/1000 | Loss: 0.00004318
Iteration 85/1000 | Loss: 0.00004318
Iteration 86/1000 | Loss: 0.00004317
Iteration 87/1000 | Loss: 0.00004317
Iteration 88/1000 | Loss: 0.00004317
Iteration 89/1000 | Loss: 0.00004317
Iteration 90/1000 | Loss: 0.00004317
Iteration 91/1000 | Loss: 0.00004317
Iteration 92/1000 | Loss: 0.00004317
Iteration 93/1000 | Loss: 0.00004317
Iteration 94/1000 | Loss: 0.00004317
Iteration 95/1000 | Loss: 0.00004317
Iteration 96/1000 | Loss: 0.00004317
Iteration 97/1000 | Loss: 0.00004317
Iteration 98/1000 | Loss: 0.00004317
Iteration 99/1000 | Loss: 0.00004317
Iteration 100/1000 | Loss: 0.00004317
Iteration 101/1000 | Loss: 0.00004317
Iteration 102/1000 | Loss: 0.00004317
Iteration 103/1000 | Loss: 0.00004317
Iteration 104/1000 | Loss: 0.00004317
Iteration 105/1000 | Loss: 0.00004317
Iteration 106/1000 | Loss: 0.00004317
Iteration 107/1000 | Loss: 0.00004317
Iteration 108/1000 | Loss: 0.00004317
Iteration 109/1000 | Loss: 0.00004317
Iteration 110/1000 | Loss: 0.00004317
Iteration 111/1000 | Loss: 0.00004317
Iteration 112/1000 | Loss: 0.00004317
Iteration 113/1000 | Loss: 0.00004317
Iteration 114/1000 | Loss: 0.00004317
Iteration 115/1000 | Loss: 0.00004317
Iteration 116/1000 | Loss: 0.00004317
Iteration 117/1000 | Loss: 0.00004317
Iteration 118/1000 | Loss: 0.00004317
Iteration 119/1000 | Loss: 0.00004317
Iteration 120/1000 | Loss: 0.00004317
Iteration 121/1000 | Loss: 0.00004317
Iteration 122/1000 | Loss: 0.00004317
Iteration 123/1000 | Loss: 0.00004317
Iteration 124/1000 | Loss: 0.00004317
Iteration 125/1000 | Loss: 0.00004317
Iteration 126/1000 | Loss: 0.00004317
Iteration 127/1000 | Loss: 0.00004317
Iteration 128/1000 | Loss: 0.00004317
Iteration 129/1000 | Loss: 0.00004317
Iteration 130/1000 | Loss: 0.00004317
Iteration 131/1000 | Loss: 0.00004317
Iteration 132/1000 | Loss: 0.00004317
Iteration 133/1000 | Loss: 0.00004317
Iteration 134/1000 | Loss: 0.00004317
Iteration 135/1000 | Loss: 0.00004317
Iteration 136/1000 | Loss: 0.00004317
Iteration 137/1000 | Loss: 0.00004317
Iteration 138/1000 | Loss: 0.00004317
Iteration 139/1000 | Loss: 0.00004317
Iteration 140/1000 | Loss: 0.00004317
Iteration 141/1000 | Loss: 0.00004317
Iteration 142/1000 | Loss: 0.00004317
Iteration 143/1000 | Loss: 0.00004317
Iteration 144/1000 | Loss: 0.00004317
Iteration 145/1000 | Loss: 0.00004317
Iteration 146/1000 | Loss: 0.00004317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [4.316976628615521e-05, 4.316976628615521e-05, 4.316976628615521e-05, 4.316976628615521e-05, 4.316976628615521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.316976628615521e-05

Optimization complete. Final v2v error: 4.469117164611816 mm

Highest mean error: 21.818653106689453 mm for frame 160

Lowest mean error: 3.7197682857513428 mm for frame 198

Saving results

Total time: 108.32748222351074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073821
Iteration 2/25 | Loss: 0.00241963
Iteration 3/25 | Loss: 0.00195671
Iteration 4/25 | Loss: 0.00182838
Iteration 5/25 | Loss: 0.00179257
Iteration 6/25 | Loss: 0.00176993
Iteration 7/25 | Loss: 0.00176858
Iteration 8/25 | Loss: 0.00176721
Iteration 9/25 | Loss: 0.00176473
Iteration 10/25 | Loss: 0.00175500
Iteration 11/25 | Loss: 0.00174579
Iteration 12/25 | Loss: 0.00174354
Iteration 13/25 | Loss: 0.00175018
Iteration 14/25 | Loss: 0.00174550
Iteration 15/25 | Loss: 0.00174052
Iteration 16/25 | Loss: 0.00173970
Iteration 17/25 | Loss: 0.00173954
Iteration 18/25 | Loss: 0.00173954
Iteration 19/25 | Loss: 0.00173954
Iteration 20/25 | Loss: 0.00173954
Iteration 21/25 | Loss: 0.00173954
Iteration 22/25 | Loss: 0.00173954
Iteration 23/25 | Loss: 0.00173953
Iteration 24/25 | Loss: 0.00173953
Iteration 25/25 | Loss: 0.00173953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34440339
Iteration 2/25 | Loss: 0.00822957
Iteration 3/25 | Loss: 0.00822956
Iteration 4/25 | Loss: 0.00822956
Iteration 5/25 | Loss: 0.00822956
Iteration 6/25 | Loss: 0.00822956
Iteration 7/25 | Loss: 0.00822956
Iteration 8/25 | Loss: 0.00822956
Iteration 9/25 | Loss: 0.00822956
Iteration 10/25 | Loss: 0.00822956
Iteration 11/25 | Loss: 0.00822956
Iteration 12/25 | Loss: 0.00822956
Iteration 13/25 | Loss: 0.00822956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00822956208139658, 0.00822956208139658, 0.00822956208139658, 0.00822956208139658, 0.00822956208139658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00822956208139658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00822956
Iteration 2/1000 | Loss: 0.00130720
Iteration 3/1000 | Loss: 0.00093235
Iteration 4/1000 | Loss: 0.00075853
Iteration 5/1000 | Loss: 0.00065211
Iteration 6/1000 | Loss: 0.00058142
Iteration 7/1000 | Loss: 0.00050541
Iteration 8/1000 | Loss: 0.00045597
Iteration 9/1000 | Loss: 0.00043076
Iteration 10/1000 | Loss: 0.04513446
Iteration 11/1000 | Loss: 0.00379257
Iteration 12/1000 | Loss: 0.00059794
Iteration 13/1000 | Loss: 0.00037331
Iteration 14/1000 | Loss: 0.00022282
Iteration 15/1000 | Loss: 0.00013795
Iteration 16/1000 | Loss: 0.00009664
Iteration 17/1000 | Loss: 0.00007380
Iteration 18/1000 | Loss: 0.00005346
Iteration 19/1000 | Loss: 0.00004075
Iteration 20/1000 | Loss: 0.00003383
Iteration 21/1000 | Loss: 0.00003041
Iteration 22/1000 | Loss: 0.00002828
Iteration 23/1000 | Loss: 0.00002669
Iteration 24/1000 | Loss: 0.00002457
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002211
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002177
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002156
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002154
Iteration 39/1000 | Loss: 0.00002153
Iteration 40/1000 | Loss: 0.00002149
Iteration 41/1000 | Loss: 0.00002149
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002148
Iteration 45/1000 | Loss: 0.00002148
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002146
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002145
Iteration 52/1000 | Loss: 0.00002145
Iteration 53/1000 | Loss: 0.00002144
Iteration 54/1000 | Loss: 0.00002144
Iteration 55/1000 | Loss: 0.00002144
Iteration 56/1000 | Loss: 0.00002144
Iteration 57/1000 | Loss: 0.00002144
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002143
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002141
Iteration 72/1000 | Loss: 0.00002141
Iteration 73/1000 | Loss: 0.00002141
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002140
Iteration 77/1000 | Loss: 0.00002140
Iteration 78/1000 | Loss: 0.00002140
Iteration 79/1000 | Loss: 0.00002140
Iteration 80/1000 | Loss: 0.00002140
Iteration 81/1000 | Loss: 0.00002140
Iteration 82/1000 | Loss: 0.00002140
Iteration 83/1000 | Loss: 0.00002140
Iteration 84/1000 | Loss: 0.00002140
Iteration 85/1000 | Loss: 0.00002139
Iteration 86/1000 | Loss: 0.00002139
Iteration 87/1000 | Loss: 0.00002139
Iteration 88/1000 | Loss: 0.00002139
Iteration 89/1000 | Loss: 0.00002139
Iteration 90/1000 | Loss: 0.00002139
Iteration 91/1000 | Loss: 0.00002139
Iteration 92/1000 | Loss: 0.00002139
Iteration 93/1000 | Loss: 0.00002139
Iteration 94/1000 | Loss: 0.00002139
Iteration 95/1000 | Loss: 0.00002139
Iteration 96/1000 | Loss: 0.00002139
Iteration 97/1000 | Loss: 0.00002139
Iteration 98/1000 | Loss: 0.00002139
Iteration 99/1000 | Loss: 0.00002139
Iteration 100/1000 | Loss: 0.00002139
Iteration 101/1000 | Loss: 0.00002139
Iteration 102/1000 | Loss: 0.00002139
Iteration 103/1000 | Loss: 0.00002139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.138805211870931e-05, 2.138805211870931e-05, 2.138805211870931e-05, 2.138805211870931e-05, 2.138805211870931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.138805211870931e-05

Optimization complete. Final v2v error: 3.970996856689453 mm

Highest mean error: 4.235200881958008 mm for frame 3

Lowest mean error: 3.6525747776031494 mm for frame 118

Saving results

Total time: 76.53945565223694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416183
Iteration 2/25 | Loss: 0.00110555
Iteration 3/25 | Loss: 0.00093111
Iteration 4/25 | Loss: 0.00090281
Iteration 5/25 | Loss: 0.00089235
Iteration 6/25 | Loss: 0.00088847
Iteration 7/25 | Loss: 0.00088761
Iteration 8/25 | Loss: 0.00088759
Iteration 9/25 | Loss: 0.00088753
Iteration 10/25 | Loss: 0.00088753
Iteration 11/25 | Loss: 0.00088753
Iteration 12/25 | Loss: 0.00088753
Iteration 13/25 | Loss: 0.00088753
Iteration 14/25 | Loss: 0.00088753
Iteration 15/25 | Loss: 0.00088753
Iteration 16/25 | Loss: 0.00088753
Iteration 17/25 | Loss: 0.00088753
Iteration 18/25 | Loss: 0.00088753
Iteration 19/25 | Loss: 0.00088753
Iteration 20/25 | Loss: 0.00088753
Iteration 21/25 | Loss: 0.00088753
Iteration 22/25 | Loss: 0.00088753
Iteration 23/25 | Loss: 0.00088753
Iteration 24/25 | Loss: 0.00088753
Iteration 25/25 | Loss: 0.00088753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000887534988578409, 0.000887534988578409, 0.000887534988578409, 0.000887534988578409, 0.000887534988578409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000887534988578409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32377219
Iteration 2/25 | Loss: 0.00041092
Iteration 3/25 | Loss: 0.00041090
Iteration 4/25 | Loss: 0.00041090
Iteration 5/25 | Loss: 0.00041090
Iteration 6/25 | Loss: 0.00041090
Iteration 7/25 | Loss: 0.00041090
Iteration 8/25 | Loss: 0.00041090
Iteration 9/25 | Loss: 0.00041090
Iteration 10/25 | Loss: 0.00041090
Iteration 11/25 | Loss: 0.00041090
Iteration 12/25 | Loss: 0.00041090
Iteration 13/25 | Loss: 0.00041090
Iteration 14/25 | Loss: 0.00041090
Iteration 15/25 | Loss: 0.00041090
Iteration 16/25 | Loss: 0.00041090
Iteration 17/25 | Loss: 0.00041090
Iteration 18/25 | Loss: 0.00041090
Iteration 19/25 | Loss: 0.00041090
Iteration 20/25 | Loss: 0.00041090
Iteration 21/25 | Loss: 0.00041090
Iteration 22/25 | Loss: 0.00041090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00041089655132964253, 0.00041089655132964253, 0.00041089655132964253, 0.00041089655132964253, 0.00041089655132964253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041089655132964253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041090
Iteration 2/1000 | Loss: 0.00003566
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00001976
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001718
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001642
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001531
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001530
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001529
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00001529
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001528
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001517
Iteration 96/1000 | Loss: 0.00001517
Iteration 97/1000 | Loss: 0.00001517
Iteration 98/1000 | Loss: 0.00001517
Iteration 99/1000 | Loss: 0.00001517
Iteration 100/1000 | Loss: 0.00001517
Iteration 101/1000 | Loss: 0.00001517
Iteration 102/1000 | Loss: 0.00001517
Iteration 103/1000 | Loss: 0.00001517
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001516
Iteration 112/1000 | Loss: 0.00001516
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001513
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001513
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001513
Iteration 173/1000 | Loss: 0.00001513
Iteration 174/1000 | Loss: 0.00001513
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001513
Iteration 177/1000 | Loss: 0.00001513
Iteration 178/1000 | Loss: 0.00001513
Iteration 179/1000 | Loss: 0.00001513
Iteration 180/1000 | Loss: 0.00001513
Iteration 181/1000 | Loss: 0.00001513
Iteration 182/1000 | Loss: 0.00001513
Iteration 183/1000 | Loss: 0.00001513
Iteration 184/1000 | Loss: 0.00001513
Iteration 185/1000 | Loss: 0.00001513
Iteration 186/1000 | Loss: 0.00001513
Iteration 187/1000 | Loss: 0.00001513
Iteration 188/1000 | Loss: 0.00001513
Iteration 189/1000 | Loss: 0.00001513
Iteration 190/1000 | Loss: 0.00001513
Iteration 191/1000 | Loss: 0.00001513
Iteration 192/1000 | Loss: 0.00001513
Iteration 193/1000 | Loss: 0.00001513
Iteration 194/1000 | Loss: 0.00001513
Iteration 195/1000 | Loss: 0.00001513
Iteration 196/1000 | Loss: 0.00001513
Iteration 197/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.51252579598804e-05, 1.51252579598804e-05, 1.51252579598804e-05, 1.51252579598804e-05, 1.51252579598804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.51252579598804e-05

Optimization complete. Final v2v error: 3.1720035076141357 mm

Highest mean error: 5.0178446769714355 mm for frame 75

Lowest mean error: 2.4640355110168457 mm for frame 160

Saving results

Total time: 43.23119592666626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277712
Iteration 2/25 | Loss: 0.00119101
Iteration 3/25 | Loss: 0.00102203
Iteration 4/25 | Loss: 0.00097066
Iteration 5/25 | Loss: 0.00096102
Iteration 6/25 | Loss: 0.00095968
Iteration 7/25 | Loss: 0.00095922
Iteration 8/25 | Loss: 0.00095922
Iteration 9/25 | Loss: 0.00095922
Iteration 10/25 | Loss: 0.00095922
Iteration 11/25 | Loss: 0.00095922
Iteration 12/25 | Loss: 0.00095922
Iteration 13/25 | Loss: 0.00095922
Iteration 14/25 | Loss: 0.00095922
Iteration 15/25 | Loss: 0.00095922
Iteration 16/25 | Loss: 0.00095922
Iteration 17/25 | Loss: 0.00095922
Iteration 18/25 | Loss: 0.00095922
Iteration 19/25 | Loss: 0.00095922
Iteration 20/25 | Loss: 0.00095922
Iteration 21/25 | Loss: 0.00095922
Iteration 22/25 | Loss: 0.00095922
Iteration 23/25 | Loss: 0.00095922
Iteration 24/25 | Loss: 0.00095922
Iteration 25/25 | Loss: 0.00095922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37033498
Iteration 2/25 | Loss: 0.00051953
Iteration 3/25 | Loss: 0.00051952
Iteration 4/25 | Loss: 0.00051952
Iteration 5/25 | Loss: 0.00051952
Iteration 6/25 | Loss: 0.00051952
Iteration 7/25 | Loss: 0.00051952
Iteration 8/25 | Loss: 0.00051952
Iteration 9/25 | Loss: 0.00051952
Iteration 10/25 | Loss: 0.00051952
Iteration 11/25 | Loss: 0.00051952
Iteration 12/25 | Loss: 0.00051952
Iteration 13/25 | Loss: 0.00051952
Iteration 14/25 | Loss: 0.00051952
Iteration 15/25 | Loss: 0.00051952
Iteration 16/25 | Loss: 0.00051952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005195221165195107, 0.0005195221165195107, 0.0005195221165195107, 0.0005195221165195107, 0.0005195221165195107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005195221165195107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051952
Iteration 2/1000 | Loss: 0.00005058
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002740
Iteration 6/1000 | Loss: 0.00002618
Iteration 7/1000 | Loss: 0.00002533
Iteration 8/1000 | Loss: 0.00002488
Iteration 9/1000 | Loss: 0.00002458
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002385
Iteration 14/1000 | Loss: 0.00002384
Iteration 15/1000 | Loss: 0.00002381
Iteration 16/1000 | Loss: 0.00002377
Iteration 17/1000 | Loss: 0.00002374
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002373
Iteration 20/1000 | Loss: 0.00002373
Iteration 21/1000 | Loss: 0.00002372
Iteration 22/1000 | Loss: 0.00002372
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002371
Iteration 25/1000 | Loss: 0.00002371
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002370
Iteration 28/1000 | Loss: 0.00002370
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002369
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00002368
Iteration 36/1000 | Loss: 0.00002368
Iteration 37/1000 | Loss: 0.00002368
Iteration 38/1000 | Loss: 0.00002368
Iteration 39/1000 | Loss: 0.00002368
Iteration 40/1000 | Loss: 0.00002368
Iteration 41/1000 | Loss: 0.00002368
Iteration 42/1000 | Loss: 0.00002368
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002367
Iteration 45/1000 | Loss: 0.00002367
Iteration 46/1000 | Loss: 0.00002367
Iteration 47/1000 | Loss: 0.00002367
Iteration 48/1000 | Loss: 0.00002367
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002367
Iteration 52/1000 | Loss: 0.00002367
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002367
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00002366
Iteration 62/1000 | Loss: 0.00002366
Iteration 63/1000 | Loss: 0.00002366
Iteration 64/1000 | Loss: 0.00002366
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002365
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002365
Iteration 71/1000 | Loss: 0.00002365
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002365
Iteration 78/1000 | Loss: 0.00002365
Iteration 79/1000 | Loss: 0.00002365
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00002365
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002364
Iteration 85/1000 | Loss: 0.00002364
Iteration 86/1000 | Loss: 0.00002364
Iteration 87/1000 | Loss: 0.00002364
Iteration 88/1000 | Loss: 0.00002364
Iteration 89/1000 | Loss: 0.00002364
Iteration 90/1000 | Loss: 0.00002364
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002363
Iteration 96/1000 | Loss: 0.00002363
Iteration 97/1000 | Loss: 0.00002363
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002363
Iteration 102/1000 | Loss: 0.00002363
Iteration 103/1000 | Loss: 0.00002363
Iteration 104/1000 | Loss: 0.00002363
Iteration 105/1000 | Loss: 0.00002363
Iteration 106/1000 | Loss: 0.00002363
Iteration 107/1000 | Loss: 0.00002363
Iteration 108/1000 | Loss: 0.00002363
Iteration 109/1000 | Loss: 0.00002363
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00002363
Iteration 112/1000 | Loss: 0.00002363
Iteration 113/1000 | Loss: 0.00002363
Iteration 114/1000 | Loss: 0.00002363
Iteration 115/1000 | Loss: 0.00002363
Iteration 116/1000 | Loss: 0.00002363
Iteration 117/1000 | Loss: 0.00002363
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002363
Iteration 120/1000 | Loss: 0.00002363
Iteration 121/1000 | Loss: 0.00002363
Iteration 122/1000 | Loss: 0.00002363
Iteration 123/1000 | Loss: 0.00002363
Iteration 124/1000 | Loss: 0.00002363
Iteration 125/1000 | Loss: 0.00002363
Iteration 126/1000 | Loss: 0.00002363
Iteration 127/1000 | Loss: 0.00002363
Iteration 128/1000 | Loss: 0.00002363
Iteration 129/1000 | Loss: 0.00002363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.36251453316072e-05, 2.36251453316072e-05, 2.36251453316072e-05, 2.36251453316072e-05, 2.36251453316072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.36251453316072e-05

Optimization complete. Final v2v error: 4.1226277351379395 mm

Highest mean error: 4.3357696533203125 mm for frame 112

Lowest mean error: 3.6227409839630127 mm for frame 0

Saving results

Total time: 34.512211322784424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407146
Iteration 2/25 | Loss: 0.00100296
Iteration 3/25 | Loss: 0.00090563
Iteration 4/25 | Loss: 0.00088806
Iteration 5/25 | Loss: 0.00088054
Iteration 6/25 | Loss: 0.00087883
Iteration 7/25 | Loss: 0.00087865
Iteration 8/25 | Loss: 0.00087865
Iteration 9/25 | Loss: 0.00087865
Iteration 10/25 | Loss: 0.00087865
Iteration 11/25 | Loss: 0.00087865
Iteration 12/25 | Loss: 0.00087865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008786508697085083, 0.0008786508697085083, 0.0008786508697085083, 0.0008786508697085083, 0.0008786508697085083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008786508697085083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28782320
Iteration 2/25 | Loss: 0.00038708
Iteration 3/25 | Loss: 0.00038708
Iteration 4/25 | Loss: 0.00038708
Iteration 5/25 | Loss: 0.00038708
Iteration 6/25 | Loss: 0.00038708
Iteration 7/25 | Loss: 0.00038708
Iteration 8/25 | Loss: 0.00038708
Iteration 9/25 | Loss: 0.00038708
Iteration 10/25 | Loss: 0.00038708
Iteration 11/25 | Loss: 0.00038708
Iteration 12/25 | Loss: 0.00038708
Iteration 13/25 | Loss: 0.00038708
Iteration 14/25 | Loss: 0.00038708
Iteration 15/25 | Loss: 0.00038708
Iteration 16/25 | Loss: 0.00038708
Iteration 17/25 | Loss: 0.00038708
Iteration 18/25 | Loss: 0.00038708
Iteration 19/25 | Loss: 0.00038708
Iteration 20/25 | Loss: 0.00038708
Iteration 21/25 | Loss: 0.00038708
Iteration 22/25 | Loss: 0.00038708
Iteration 23/25 | Loss: 0.00038708
Iteration 24/25 | Loss: 0.00038708
Iteration 25/25 | Loss: 0.00038708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038708
Iteration 2/1000 | Loss: 0.00002872
Iteration 3/1000 | Loss: 0.00002177
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001833
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001825
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001810
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001804
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001803
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001802
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001800
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00001799
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001796
Iteration 62/1000 | Loss: 0.00001796
Iteration 63/1000 | Loss: 0.00001796
Iteration 64/1000 | Loss: 0.00001796
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001796
Iteration 73/1000 | Loss: 0.00001796
Iteration 74/1000 | Loss: 0.00001796
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001795
Iteration 77/1000 | Loss: 0.00001795
Iteration 78/1000 | Loss: 0.00001795
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001795
Iteration 81/1000 | Loss: 0.00001795
Iteration 82/1000 | Loss: 0.00001795
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00001794
Iteration 85/1000 | Loss: 0.00001794
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001793
Iteration 88/1000 | Loss: 0.00001793
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00001793
Iteration 91/1000 | Loss: 0.00001793
Iteration 92/1000 | Loss: 0.00001793
Iteration 93/1000 | Loss: 0.00001793
Iteration 94/1000 | Loss: 0.00001793
Iteration 95/1000 | Loss: 0.00001793
Iteration 96/1000 | Loss: 0.00001793
Iteration 97/1000 | Loss: 0.00001793
Iteration 98/1000 | Loss: 0.00001793
Iteration 99/1000 | Loss: 0.00001792
Iteration 100/1000 | Loss: 0.00001792
Iteration 101/1000 | Loss: 0.00001792
Iteration 102/1000 | Loss: 0.00001792
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001792
Iteration 109/1000 | Loss: 0.00001792
Iteration 110/1000 | Loss: 0.00001792
Iteration 111/1000 | Loss: 0.00001792
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001791
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001791
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001791
Iteration 121/1000 | Loss: 0.00001791
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001790
Iteration 124/1000 | Loss: 0.00001790
Iteration 125/1000 | Loss: 0.00001790
Iteration 126/1000 | Loss: 0.00001790
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001789
Iteration 129/1000 | Loss: 0.00001789
Iteration 130/1000 | Loss: 0.00001789
Iteration 131/1000 | Loss: 0.00001789
Iteration 132/1000 | Loss: 0.00001789
Iteration 133/1000 | Loss: 0.00001789
Iteration 134/1000 | Loss: 0.00001789
Iteration 135/1000 | Loss: 0.00001789
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001788
Iteration 138/1000 | Loss: 0.00001788
Iteration 139/1000 | Loss: 0.00001788
Iteration 140/1000 | Loss: 0.00001788
Iteration 141/1000 | Loss: 0.00001788
Iteration 142/1000 | Loss: 0.00001788
Iteration 143/1000 | Loss: 0.00001788
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001788
Iteration 154/1000 | Loss: 0.00001788
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001788
Iteration 159/1000 | Loss: 0.00001788
Iteration 160/1000 | Loss: 0.00001788
Iteration 161/1000 | Loss: 0.00001788
Iteration 162/1000 | Loss: 0.00001788
Iteration 163/1000 | Loss: 0.00001788
Iteration 164/1000 | Loss: 0.00001788
Iteration 165/1000 | Loss: 0.00001788
Iteration 166/1000 | Loss: 0.00001788
Iteration 167/1000 | Loss: 0.00001788
Iteration 168/1000 | Loss: 0.00001788
Iteration 169/1000 | Loss: 0.00001788
Iteration 170/1000 | Loss: 0.00001788
Iteration 171/1000 | Loss: 0.00001788
Iteration 172/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.787553628673777e-05, 1.787553628673777e-05, 1.787553628673777e-05, 1.787553628673777e-05, 1.787553628673777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.787553628673777e-05

Optimization complete. Final v2v error: 3.6457324028015137 mm

Highest mean error: 3.8460988998413086 mm for frame 84

Lowest mean error: 3.2484023571014404 mm for frame 1

Saving results

Total time: 34.22063374519348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509035
Iteration 2/25 | Loss: 0.00107527
Iteration 3/25 | Loss: 0.00093190
Iteration 4/25 | Loss: 0.00090837
Iteration 5/25 | Loss: 0.00089995
Iteration 6/25 | Loss: 0.00089850
Iteration 7/25 | Loss: 0.00089843
Iteration 8/25 | Loss: 0.00089843
Iteration 9/25 | Loss: 0.00089843
Iteration 10/25 | Loss: 0.00089843
Iteration 11/25 | Loss: 0.00089843
Iteration 12/25 | Loss: 0.00089843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008984265150502324, 0.0008984265150502324, 0.0008984265150502324, 0.0008984265150502324, 0.0008984265150502324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008984265150502324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37971616
Iteration 2/25 | Loss: 0.00047999
Iteration 3/25 | Loss: 0.00047997
Iteration 4/25 | Loss: 0.00047997
Iteration 5/25 | Loss: 0.00047997
Iteration 6/25 | Loss: 0.00047997
Iteration 7/25 | Loss: 0.00047997
Iteration 8/25 | Loss: 0.00047997
Iteration 9/25 | Loss: 0.00047997
Iteration 10/25 | Loss: 0.00047997
Iteration 11/25 | Loss: 0.00047997
Iteration 12/25 | Loss: 0.00047997
Iteration 13/25 | Loss: 0.00047997
Iteration 14/25 | Loss: 0.00047997
Iteration 15/25 | Loss: 0.00047997
Iteration 16/25 | Loss: 0.00047997
Iteration 17/25 | Loss: 0.00047997
Iteration 18/25 | Loss: 0.00047997
Iteration 19/25 | Loss: 0.00047997
Iteration 20/25 | Loss: 0.00047997
Iteration 21/25 | Loss: 0.00047997
Iteration 22/25 | Loss: 0.00047997
Iteration 23/25 | Loss: 0.00047997
Iteration 24/25 | Loss: 0.00047997
Iteration 25/25 | Loss: 0.00047997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047997
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002294
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001791
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001774
Iteration 17/1000 | Loss: 0.00001773
Iteration 18/1000 | Loss: 0.00001772
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001764
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001759
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001755
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001752
Iteration 67/1000 | Loss: 0.00001752
Iteration 68/1000 | Loss: 0.00001752
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001752
Iteration 72/1000 | Loss: 0.00001751
Iteration 73/1000 | Loss: 0.00001751
Iteration 74/1000 | Loss: 0.00001751
Iteration 75/1000 | Loss: 0.00001751
Iteration 76/1000 | Loss: 0.00001751
Iteration 77/1000 | Loss: 0.00001751
Iteration 78/1000 | Loss: 0.00001751
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001750
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001749
Iteration 101/1000 | Loss: 0.00001749
Iteration 102/1000 | Loss: 0.00001749
Iteration 103/1000 | Loss: 0.00001749
Iteration 104/1000 | Loss: 0.00001749
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.7486596334492788e-05, 1.7486596334492788e-05, 1.7486596334492788e-05, 1.7486596334492788e-05, 1.7486596334492788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7486596334492788e-05

Optimization complete. Final v2v error: 3.5504915714263916 mm

Highest mean error: 4.249025344848633 mm for frame 61

Lowest mean error: 2.971158504486084 mm for frame 218

Saving results

Total time: 41.02245020866394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415033
Iteration 2/25 | Loss: 0.00107869
Iteration 3/25 | Loss: 0.00091431
Iteration 4/25 | Loss: 0.00089847
Iteration 5/25 | Loss: 0.00088809
Iteration 6/25 | Loss: 0.00088570
Iteration 7/25 | Loss: 0.00088521
Iteration 8/25 | Loss: 0.00088521
Iteration 9/25 | Loss: 0.00088521
Iteration 10/25 | Loss: 0.00088521
Iteration 11/25 | Loss: 0.00088521
Iteration 12/25 | Loss: 0.00088521
Iteration 13/25 | Loss: 0.00088521
Iteration 14/25 | Loss: 0.00088521
Iteration 15/25 | Loss: 0.00088521
Iteration 16/25 | Loss: 0.00088521
Iteration 17/25 | Loss: 0.00088521
Iteration 18/25 | Loss: 0.00088521
Iteration 19/25 | Loss: 0.00088521
Iteration 20/25 | Loss: 0.00088521
Iteration 21/25 | Loss: 0.00088521
Iteration 22/25 | Loss: 0.00088521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008852064493112266, 0.0008852064493112266, 0.0008852064493112266, 0.0008852064493112266, 0.0008852064493112266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008852064493112266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61302459
Iteration 2/25 | Loss: 0.00038511
Iteration 3/25 | Loss: 0.00038511
Iteration 4/25 | Loss: 0.00038510
Iteration 5/25 | Loss: 0.00038510
Iteration 6/25 | Loss: 0.00038510
Iteration 7/25 | Loss: 0.00038510
Iteration 8/25 | Loss: 0.00038510
Iteration 9/25 | Loss: 0.00038510
Iteration 10/25 | Loss: 0.00038510
Iteration 11/25 | Loss: 0.00038510
Iteration 12/25 | Loss: 0.00038510
Iteration 13/25 | Loss: 0.00038510
Iteration 14/25 | Loss: 0.00038510
Iteration 15/25 | Loss: 0.00038510
Iteration 16/25 | Loss: 0.00038510
Iteration 17/25 | Loss: 0.00038510
Iteration 18/25 | Loss: 0.00038510
Iteration 19/25 | Loss: 0.00038510
Iteration 20/25 | Loss: 0.00038510
Iteration 21/25 | Loss: 0.00038510
Iteration 22/25 | Loss: 0.00038510
Iteration 23/25 | Loss: 0.00038510
Iteration 24/25 | Loss: 0.00038510
Iteration 25/25 | Loss: 0.00038510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00038510255399160087, 0.00038510255399160087, 0.00038510255399160087, 0.00038510255399160087, 0.00038510255399160087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038510255399160087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038510
Iteration 2/1000 | Loss: 0.00002777
Iteration 3/1000 | Loss: 0.00002138
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001727
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001658
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001614
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001605
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001603
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001601
Iteration 39/1000 | Loss: 0.00001601
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001600
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001598
Iteration 49/1000 | Loss: 0.00001598
Iteration 50/1000 | Loss: 0.00001598
Iteration 51/1000 | Loss: 0.00001598
Iteration 52/1000 | Loss: 0.00001597
Iteration 53/1000 | Loss: 0.00001597
Iteration 54/1000 | Loss: 0.00001597
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001596
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001595
Iteration 65/1000 | Loss: 0.00001595
Iteration 66/1000 | Loss: 0.00001595
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.595253161212895e-05, 1.595253161212895e-05, 1.595253161212895e-05, 1.595253161212895e-05, 1.595253161212895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.595253161212895e-05

Optimization complete. Final v2v error: 3.473045825958252 mm

Highest mean error: 3.8096301555633545 mm for frame 171

Lowest mean error: 2.8574163913726807 mm for frame 261

Saving results

Total time: 37.799742460250854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_5417/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_5417/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791303
Iteration 2/25 | Loss: 0.00192957
Iteration 3/25 | Loss: 0.00130124
Iteration 4/25 | Loss: 0.00111853
Iteration 5/25 | Loss: 0.00109143
Iteration 6/25 | Loss: 0.00101430
Iteration 7/25 | Loss: 0.00100588
Iteration 8/25 | Loss: 0.00100379
Iteration 9/25 | Loss: 0.00099909
Iteration 10/25 | Loss: 0.00099831
Iteration 11/25 | Loss: 0.00100002
Iteration 12/25 | Loss: 0.00099819
Iteration 13/25 | Loss: 0.00099735
Iteration 14/25 | Loss: 0.00099382
Iteration 15/25 | Loss: 0.00099306
Iteration 16/25 | Loss: 0.00099205
Iteration 17/25 | Loss: 0.00099183
Iteration 18/25 | Loss: 0.00099182
Iteration 19/25 | Loss: 0.00099182
Iteration 20/25 | Loss: 0.00099182
Iteration 21/25 | Loss: 0.00099182
Iteration 22/25 | Loss: 0.00099182
Iteration 23/25 | Loss: 0.00099182
Iteration 24/25 | Loss: 0.00099181
Iteration 25/25 | Loss: 0.00099181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37145352
Iteration 2/25 | Loss: 0.00046537
Iteration 3/25 | Loss: 0.00046535
Iteration 4/25 | Loss: 0.00046535
Iteration 5/25 | Loss: 0.00046535
Iteration 6/25 | Loss: 0.00046535
Iteration 7/25 | Loss: 0.00046535
Iteration 8/25 | Loss: 0.00046535
Iteration 9/25 | Loss: 0.00046535
Iteration 10/25 | Loss: 0.00046535
Iteration 11/25 | Loss: 0.00046535
Iteration 12/25 | Loss: 0.00046535
Iteration 13/25 | Loss: 0.00046535
Iteration 14/25 | Loss: 0.00046535
Iteration 15/25 | Loss: 0.00046535
Iteration 16/25 | Loss: 0.00046535
Iteration 17/25 | Loss: 0.00046535
Iteration 18/25 | Loss: 0.00046535
Iteration 19/25 | Loss: 0.00046535
Iteration 20/25 | Loss: 0.00046535
Iteration 21/25 | Loss: 0.00046535
Iteration 22/25 | Loss: 0.00046535
Iteration 23/25 | Loss: 0.00046535
Iteration 24/25 | Loss: 0.00046535
Iteration 25/25 | Loss: 0.00046535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046535
Iteration 2/1000 | Loss: 0.00005280
Iteration 3/1000 | Loss: 0.00003934
Iteration 4/1000 | Loss: 0.00005451
Iteration 5/1000 | Loss: 0.00003532
Iteration 6/1000 | Loss: 0.00003340
Iteration 7/1000 | Loss: 0.00004210
Iteration 8/1000 | Loss: 0.00003927
Iteration 9/1000 | Loss: 0.00003552
Iteration 10/1000 | Loss: 0.00003122
Iteration 11/1000 | Loss: 0.00003064
Iteration 12/1000 | Loss: 0.00003033
Iteration 13/1000 | Loss: 0.00003022
Iteration 14/1000 | Loss: 0.00003015
Iteration 15/1000 | Loss: 0.00003014
Iteration 16/1000 | Loss: 0.00003013
Iteration 17/1000 | Loss: 0.00003013
Iteration 18/1000 | Loss: 0.00003012
Iteration 19/1000 | Loss: 0.00003012
Iteration 20/1000 | Loss: 0.00003012
Iteration 21/1000 | Loss: 0.00003009
Iteration 22/1000 | Loss: 0.00003007
Iteration 23/1000 | Loss: 0.00002999
Iteration 24/1000 | Loss: 0.00002996
Iteration 25/1000 | Loss: 0.00002995
Iteration 26/1000 | Loss: 0.00002992
Iteration 27/1000 | Loss: 0.00002986
Iteration 28/1000 | Loss: 0.00002981
Iteration 29/1000 | Loss: 0.00002981
Iteration 30/1000 | Loss: 0.00002981
Iteration 31/1000 | Loss: 0.00002980
Iteration 32/1000 | Loss: 0.00002980
Iteration 33/1000 | Loss: 0.00002979
Iteration 34/1000 | Loss: 0.00002976
Iteration 35/1000 | Loss: 0.00002976
Iteration 36/1000 | Loss: 0.00002975
Iteration 37/1000 | Loss: 0.00002975
Iteration 38/1000 | Loss: 0.00002974
Iteration 39/1000 | Loss: 0.00002974
Iteration 40/1000 | Loss: 0.00002974
Iteration 41/1000 | Loss: 0.00002973
Iteration 42/1000 | Loss: 0.00002973
Iteration 43/1000 | Loss: 0.00002973
Iteration 44/1000 | Loss: 0.00002973
Iteration 45/1000 | Loss: 0.00002973
Iteration 46/1000 | Loss: 0.00002973
Iteration 47/1000 | Loss: 0.00002973
Iteration 48/1000 | Loss: 0.00002972
Iteration 49/1000 | Loss: 0.00002972
Iteration 50/1000 | Loss: 0.00002972
Iteration 51/1000 | Loss: 0.00002972
Iteration 52/1000 | Loss: 0.00002972
Iteration 53/1000 | Loss: 0.00002972
Iteration 54/1000 | Loss: 0.00002972
Iteration 55/1000 | Loss: 0.00002972
Iteration 56/1000 | Loss: 0.00002972
Iteration 57/1000 | Loss: 0.00002972
Iteration 58/1000 | Loss: 0.00002972
Iteration 59/1000 | Loss: 0.00002972
Iteration 60/1000 | Loss: 0.00002971
Iteration 61/1000 | Loss: 0.00002971
Iteration 62/1000 | Loss: 0.00002971
Iteration 63/1000 | Loss: 0.00003421
Iteration 64/1000 | Loss: 0.00003030
Iteration 65/1000 | Loss: 0.00003008
Iteration 66/1000 | Loss: 0.00002993
Iteration 67/1000 | Loss: 0.00002990
Iteration 68/1000 | Loss: 0.00002989
Iteration 69/1000 | Loss: 0.00002984
Iteration 70/1000 | Loss: 0.00002980
Iteration 71/1000 | Loss: 0.00002979
Iteration 72/1000 | Loss: 0.00002979
Iteration 73/1000 | Loss: 0.00002978
Iteration 74/1000 | Loss: 0.00002976
Iteration 75/1000 | Loss: 0.00002974
Iteration 76/1000 | Loss: 0.00002972
Iteration 77/1000 | Loss: 0.00002940
Iteration 78/1000 | Loss: 0.00002908
Iteration 79/1000 | Loss: 0.00002882
Iteration 80/1000 | Loss: 0.00002871
Iteration 81/1000 | Loss: 0.00002869
Iteration 82/1000 | Loss: 0.00002868
Iteration 83/1000 | Loss: 0.00002867
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002849
Iteration 86/1000 | Loss: 0.00002849
Iteration 87/1000 | Loss: 0.00002848
Iteration 88/1000 | Loss: 0.00002848
Iteration 89/1000 | Loss: 0.00002847
Iteration 90/1000 | Loss: 0.00002847
Iteration 91/1000 | Loss: 0.00002847
Iteration 92/1000 | Loss: 0.00002846
Iteration 93/1000 | Loss: 0.00002846
Iteration 94/1000 | Loss: 0.00002846
Iteration 95/1000 | Loss: 0.00002845
Iteration 96/1000 | Loss: 0.00002845
Iteration 97/1000 | Loss: 0.00002844
Iteration 98/1000 | Loss: 0.00002843
Iteration 99/1000 | Loss: 0.00002843
Iteration 100/1000 | Loss: 0.00002843
Iteration 101/1000 | Loss: 0.00002843
Iteration 102/1000 | Loss: 0.00002843
Iteration 103/1000 | Loss: 0.00002843
Iteration 104/1000 | Loss: 0.00002843
Iteration 105/1000 | Loss: 0.00002843
Iteration 106/1000 | Loss: 0.00002842
Iteration 107/1000 | Loss: 0.00002842
Iteration 108/1000 | Loss: 0.00002840
Iteration 109/1000 | Loss: 0.00002840
Iteration 110/1000 | Loss: 0.00002839
Iteration 111/1000 | Loss: 0.00002839
Iteration 112/1000 | Loss: 0.00002839
Iteration 113/1000 | Loss: 0.00002839
Iteration 114/1000 | Loss: 0.00002839
Iteration 115/1000 | Loss: 0.00002838
Iteration 116/1000 | Loss: 0.00002838
Iteration 117/1000 | Loss: 0.00002837
Iteration 118/1000 | Loss: 0.00002837
Iteration 119/1000 | Loss: 0.00002837
Iteration 120/1000 | Loss: 0.00002837
Iteration 121/1000 | Loss: 0.00002837
Iteration 122/1000 | Loss: 0.00002837
Iteration 123/1000 | Loss: 0.00002837
Iteration 124/1000 | Loss: 0.00002837
Iteration 125/1000 | Loss: 0.00002837
Iteration 126/1000 | Loss: 0.00002837
Iteration 127/1000 | Loss: 0.00002837
Iteration 128/1000 | Loss: 0.00002837
Iteration 129/1000 | Loss: 0.00002837
Iteration 130/1000 | Loss: 0.00002837
Iteration 131/1000 | Loss: 0.00002837
Iteration 132/1000 | Loss: 0.00002836
Iteration 133/1000 | Loss: 0.00002836
Iteration 134/1000 | Loss: 0.00002836
Iteration 135/1000 | Loss: 0.00002836
Iteration 136/1000 | Loss: 0.00002836
Iteration 137/1000 | Loss: 0.00002836
Iteration 138/1000 | Loss: 0.00002836
Iteration 139/1000 | Loss: 0.00002835
Iteration 140/1000 | Loss: 0.00002835
Iteration 141/1000 | Loss: 0.00002835
Iteration 142/1000 | Loss: 0.00002835
Iteration 143/1000 | Loss: 0.00002835
Iteration 144/1000 | Loss: 0.00002835
Iteration 145/1000 | Loss: 0.00002835
Iteration 146/1000 | Loss: 0.00002835
Iteration 147/1000 | Loss: 0.00002835
Iteration 148/1000 | Loss: 0.00002835
Iteration 149/1000 | Loss: 0.00002835
Iteration 150/1000 | Loss: 0.00002835
Iteration 151/1000 | Loss: 0.00002835
Iteration 152/1000 | Loss: 0.00002835
Iteration 153/1000 | Loss: 0.00002835
Iteration 154/1000 | Loss: 0.00002834
Iteration 155/1000 | Loss: 0.00002834
Iteration 156/1000 | Loss: 0.00002834
Iteration 157/1000 | Loss: 0.00002834
Iteration 158/1000 | Loss: 0.00002834
Iteration 159/1000 | Loss: 0.00002834
Iteration 160/1000 | Loss: 0.00002834
Iteration 161/1000 | Loss: 0.00002834
Iteration 162/1000 | Loss: 0.00002834
Iteration 163/1000 | Loss: 0.00002834
Iteration 164/1000 | Loss: 0.00002834
Iteration 165/1000 | Loss: 0.00002834
Iteration 166/1000 | Loss: 0.00002833
Iteration 167/1000 | Loss: 0.00002833
Iteration 168/1000 | Loss: 0.00002833
Iteration 169/1000 | Loss: 0.00002833
Iteration 170/1000 | Loss: 0.00002833
Iteration 171/1000 | Loss: 0.00002833
Iteration 172/1000 | Loss: 0.00002833
Iteration 173/1000 | Loss: 0.00002833
Iteration 174/1000 | Loss: 0.00002833
Iteration 175/1000 | Loss: 0.00002833
Iteration 176/1000 | Loss: 0.00002833
Iteration 177/1000 | Loss: 0.00002833
Iteration 178/1000 | Loss: 0.00002833
Iteration 179/1000 | Loss: 0.00002833
Iteration 180/1000 | Loss: 0.00002833
Iteration 181/1000 | Loss: 0.00002833
Iteration 182/1000 | Loss: 0.00002833
Iteration 183/1000 | Loss: 0.00002833
Iteration 184/1000 | Loss: 0.00002833
Iteration 185/1000 | Loss: 0.00002833
Iteration 186/1000 | Loss: 0.00002833
Iteration 187/1000 | Loss: 0.00002833
Iteration 188/1000 | Loss: 0.00002833
Iteration 189/1000 | Loss: 0.00002833
Iteration 190/1000 | Loss: 0.00002833
Iteration 191/1000 | Loss: 0.00002833
Iteration 192/1000 | Loss: 0.00002833
Iteration 193/1000 | Loss: 0.00002833
Iteration 194/1000 | Loss: 0.00002833
Iteration 195/1000 | Loss: 0.00002833
Iteration 196/1000 | Loss: 0.00002833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.8331745852483436e-05, 2.8331745852483436e-05, 2.8331745852483436e-05, 2.8331745852483436e-05, 2.8331745852483436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8331745852483436e-05

Optimization complete. Final v2v error: 4.539975643157959 mm

Highest mean error: 5.51605224609375 mm for frame 59

Lowest mean error: 4.097265720367432 mm for frame 0

Saving results

Total time: 86.96470785140991
