Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=154, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8624-8679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580899
Iteration 2/25 | Loss: 0.00096070
Iteration 3/25 | Loss: 0.00075103
Iteration 4/25 | Loss: 0.00072834
Iteration 5/25 | Loss: 0.00072217
Iteration 6/25 | Loss: 0.00072108
Iteration 7/25 | Loss: 0.00072108
Iteration 8/25 | Loss: 0.00072108
Iteration 9/25 | Loss: 0.00072108
Iteration 10/25 | Loss: 0.00072108
Iteration 11/25 | Loss: 0.00072108
Iteration 12/25 | Loss: 0.00072108
Iteration 13/25 | Loss: 0.00072108
Iteration 14/25 | Loss: 0.00072108
Iteration 15/25 | Loss: 0.00072108
Iteration 16/25 | Loss: 0.00072108
Iteration 17/25 | Loss: 0.00072108
Iteration 18/25 | Loss: 0.00072108
Iteration 19/25 | Loss: 0.00072108
Iteration 20/25 | Loss: 0.00072108
Iteration 21/25 | Loss: 0.00072108
Iteration 22/25 | Loss: 0.00072108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007210816838778555, 0.0007210816838778555, 0.0007210816838778555, 0.0007210816838778555, 0.0007210816838778555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007210816838778555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32293332
Iteration 2/25 | Loss: 0.00016094
Iteration 3/25 | Loss: 0.00016089
Iteration 4/25 | Loss: 0.00016089
Iteration 5/25 | Loss: 0.00016089
Iteration 6/25 | Loss: 0.00016088
Iteration 7/25 | Loss: 0.00016088
Iteration 8/25 | Loss: 0.00016088
Iteration 9/25 | Loss: 0.00016088
Iteration 10/25 | Loss: 0.00016088
Iteration 11/25 | Loss: 0.00016088
Iteration 12/25 | Loss: 0.00016088
Iteration 13/25 | Loss: 0.00016088
Iteration 14/25 | Loss: 0.00016088
Iteration 15/25 | Loss: 0.00016088
Iteration 16/25 | Loss: 0.00016088
Iteration 17/25 | Loss: 0.00016088
Iteration 18/25 | Loss: 0.00016088
Iteration 19/25 | Loss: 0.00016088
Iteration 20/25 | Loss: 0.00016088
Iteration 21/25 | Loss: 0.00016088
Iteration 22/25 | Loss: 0.00016088
Iteration 23/25 | Loss: 0.00016088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00016088351549115032, 0.00016088351549115032, 0.00016088351549115032, 0.00016088351549115032, 0.00016088351549115032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016088351549115032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016088
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00002358
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001978
Iteration 12/1000 | Loss: 0.00001977
Iteration 13/1000 | Loss: 0.00001977
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001976
Iteration 16/1000 | Loss: 0.00001962
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001957
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001950
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001946
Iteration 26/1000 | Loss: 0.00001945
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001934
Iteration 42/1000 | Loss: 0.00001934
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001929
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001925
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001924
Iteration 72/1000 | Loss: 0.00001924
Iteration 73/1000 | Loss: 0.00001924
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001923
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001921
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001919
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001918
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001918
Iteration 127/1000 | Loss: 0.00001918
Iteration 128/1000 | Loss: 0.00001918
Iteration 129/1000 | Loss: 0.00001918
Iteration 130/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9178953152731992e-05, 1.9178953152731992e-05, 1.9178953152731992e-05, 1.9178953152731992e-05, 1.9178953152731992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9178953152731992e-05

Optimization complete. Final v2v error: 3.7758870124816895 mm

Highest mean error: 4.1235761642456055 mm for frame 54

Lowest mean error: 3.5866551399230957 mm for frame 117

Saving results

Total time: 39.13639736175537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559255
Iteration 2/25 | Loss: 0.00126058
Iteration 3/25 | Loss: 0.00100969
Iteration 4/25 | Loss: 0.00099452
Iteration 5/25 | Loss: 0.00099147
Iteration 6/25 | Loss: 0.00099119
Iteration 7/25 | Loss: 0.00099119
Iteration 8/25 | Loss: 0.00099119
Iteration 9/25 | Loss: 0.00099119
Iteration 10/25 | Loss: 0.00099119
Iteration 11/25 | Loss: 0.00099119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000991185661405325, 0.000991185661405325, 0.000991185661405325, 0.000991185661405325, 0.000991185661405325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991185661405325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10341322
Iteration 2/25 | Loss: 0.00040849
Iteration 3/25 | Loss: 0.00040847
Iteration 4/25 | Loss: 0.00040847
Iteration 5/25 | Loss: 0.00040847
Iteration 6/25 | Loss: 0.00040847
Iteration 7/25 | Loss: 0.00040847
Iteration 8/25 | Loss: 0.00040847
Iteration 9/25 | Loss: 0.00040847
Iteration 10/25 | Loss: 0.00040847
Iteration 11/25 | Loss: 0.00040847
Iteration 12/25 | Loss: 0.00040847
Iteration 13/25 | Loss: 0.00040847
Iteration 14/25 | Loss: 0.00040847
Iteration 15/25 | Loss: 0.00040847
Iteration 16/25 | Loss: 0.00040847
Iteration 17/25 | Loss: 0.00040847
Iteration 18/25 | Loss: 0.00040847
Iteration 19/25 | Loss: 0.00040847
Iteration 20/25 | Loss: 0.00040847
Iteration 21/25 | Loss: 0.00040847
Iteration 22/25 | Loss: 0.00040847
Iteration 23/25 | Loss: 0.00040847
Iteration 24/25 | Loss: 0.00040847
Iteration 25/25 | Loss: 0.00040847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040847
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00002094
Iteration 4/1000 | Loss: 0.00001889
Iteration 5/1000 | Loss: 0.00001790
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001657
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001631
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001611
Iteration 26/1000 | Loss: 0.00001610
Iteration 27/1000 | Loss: 0.00001610
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001609
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001607
Iteration 35/1000 | Loss: 0.00001607
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001604
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001603
Iteration 55/1000 | Loss: 0.00001603
Iteration 56/1000 | Loss: 0.00001603
Iteration 57/1000 | Loss: 0.00001603
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001602
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001601
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00001601
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001600
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001600
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001600
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001600
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.6002239135559648e-05, 1.6002239135559648e-05, 1.6002239135559648e-05, 1.6002239135559648e-05, 1.6002239135559648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6002239135559648e-05

Optimization complete. Final v2v error: 3.380582332611084 mm

Highest mean error: 3.8054137229919434 mm for frame 50

Lowest mean error: 3.0517609119415283 mm for frame 74

Saving results

Total time: 33.28623557090759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889067
Iteration 2/25 | Loss: 0.00120302
Iteration 3/25 | Loss: 0.00104993
Iteration 4/25 | Loss: 0.00103020
Iteration 5/25 | Loss: 0.00102222
Iteration 6/25 | Loss: 0.00102064
Iteration 7/25 | Loss: 0.00102064
Iteration 8/25 | Loss: 0.00102062
Iteration 9/25 | Loss: 0.00102062
Iteration 10/25 | Loss: 0.00102062
Iteration 11/25 | Loss: 0.00102062
Iteration 12/25 | Loss: 0.00102062
Iteration 13/25 | Loss: 0.00102062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010206187143921852, 0.0010206187143921852, 0.0010206187143921852, 0.0010206187143921852, 0.0010206187143921852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010206187143921852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28903174
Iteration 2/25 | Loss: 0.00063858
Iteration 3/25 | Loss: 0.00063857
Iteration 4/25 | Loss: 0.00063857
Iteration 5/25 | Loss: 0.00063856
Iteration 6/25 | Loss: 0.00063856
Iteration 7/25 | Loss: 0.00063856
Iteration 8/25 | Loss: 0.00063856
Iteration 9/25 | Loss: 0.00063856
Iteration 10/25 | Loss: 0.00063856
Iteration 11/25 | Loss: 0.00063856
Iteration 12/25 | Loss: 0.00063856
Iteration 13/25 | Loss: 0.00063856
Iteration 14/25 | Loss: 0.00063856
Iteration 15/25 | Loss: 0.00063856
Iteration 16/25 | Loss: 0.00063856
Iteration 17/25 | Loss: 0.00063856
Iteration 18/25 | Loss: 0.00063856
Iteration 19/25 | Loss: 0.00063856
Iteration 20/25 | Loss: 0.00063856
Iteration 21/25 | Loss: 0.00063856
Iteration 22/25 | Loss: 0.00063856
Iteration 23/25 | Loss: 0.00063856
Iteration 24/25 | Loss: 0.00063856
Iteration 25/25 | Loss: 0.00063856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063856
Iteration 2/1000 | Loss: 0.00004164
Iteration 3/1000 | Loss: 0.00002519
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00001966
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001768
Iteration 10/1000 | Loss: 0.00001737
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001710
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001694
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001689
Iteration 30/1000 | Loss: 0.00001689
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.6817280993564054e-05, 1.6817280993564054e-05, 1.6817280993564054e-05, 1.6817280993564054e-05, 1.6817280993564054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6817280993564054e-05

Optimization complete. Final v2v error: 3.2827558517456055 mm

Highest mean error: 4.463051795959473 mm for frame 174

Lowest mean error: 2.5040390491485596 mm for frame 4

Saving results

Total time: 32.19843006134033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747014
Iteration 2/25 | Loss: 0.00113962
Iteration 3/25 | Loss: 0.00103213
Iteration 4/25 | Loss: 0.00102193
Iteration 5/25 | Loss: 0.00101889
Iteration 6/25 | Loss: 0.00101838
Iteration 7/25 | Loss: 0.00101838
Iteration 8/25 | Loss: 0.00101838
Iteration 9/25 | Loss: 0.00101838
Iteration 10/25 | Loss: 0.00101838
Iteration 11/25 | Loss: 0.00101838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001018382259644568, 0.001018382259644568, 0.001018382259644568, 0.001018382259644568, 0.001018382259644568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018382259644568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34906054
Iteration 2/25 | Loss: 0.00056474
Iteration 3/25 | Loss: 0.00056474
Iteration 4/25 | Loss: 0.00056473
Iteration 5/25 | Loss: 0.00056473
Iteration 6/25 | Loss: 0.00056473
Iteration 7/25 | Loss: 0.00056473
Iteration 8/25 | Loss: 0.00056473
Iteration 9/25 | Loss: 0.00056473
Iteration 10/25 | Loss: 0.00056473
Iteration 11/25 | Loss: 0.00056473
Iteration 12/25 | Loss: 0.00056473
Iteration 13/25 | Loss: 0.00056473
Iteration 14/25 | Loss: 0.00056473
Iteration 15/25 | Loss: 0.00056473
Iteration 16/25 | Loss: 0.00056473
Iteration 17/25 | Loss: 0.00056473
Iteration 18/25 | Loss: 0.00056473
Iteration 19/25 | Loss: 0.00056473
Iteration 20/25 | Loss: 0.00056473
Iteration 21/25 | Loss: 0.00056473
Iteration 22/25 | Loss: 0.00056473
Iteration 23/25 | Loss: 0.00056473
Iteration 24/25 | Loss: 0.00056473
Iteration 25/25 | Loss: 0.00056473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056473
Iteration 2/1000 | Loss: 0.00003663
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001688
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001542
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001423
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001406
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001400
Iteration 39/1000 | Loss: 0.00001400
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001394
Iteration 46/1000 | Loss: 0.00001394
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001394
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001391
Iteration 64/1000 | Loss: 0.00001391
Iteration 65/1000 | Loss: 0.00001390
Iteration 66/1000 | Loss: 0.00001390
Iteration 67/1000 | Loss: 0.00001390
Iteration 68/1000 | Loss: 0.00001390
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001390
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001389
Iteration 76/1000 | Loss: 0.00001389
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001389
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001388
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001387
Iteration 95/1000 | Loss: 0.00001387
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001386
Iteration 108/1000 | Loss: 0.00001386
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001386
Iteration 118/1000 | Loss: 0.00001386
Iteration 119/1000 | Loss: 0.00001385
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001385
Iteration 122/1000 | Loss: 0.00001385
Iteration 123/1000 | Loss: 0.00001385
Iteration 124/1000 | Loss: 0.00001385
Iteration 125/1000 | Loss: 0.00001385
Iteration 126/1000 | Loss: 0.00001385
Iteration 127/1000 | Loss: 0.00001385
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001384
Iteration 134/1000 | Loss: 0.00001384
Iteration 135/1000 | Loss: 0.00001384
Iteration 136/1000 | Loss: 0.00001384
Iteration 137/1000 | Loss: 0.00001384
Iteration 138/1000 | Loss: 0.00001384
Iteration 139/1000 | Loss: 0.00001384
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001383
Iteration 152/1000 | Loss: 0.00001383
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001383
Iteration 172/1000 | Loss: 0.00001383
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3827419024892151e-05, 1.3827419024892151e-05, 1.3827419024892151e-05, 1.3827419024892151e-05, 1.3827419024892151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3827419024892151e-05

Optimization complete. Final v2v error: 3.1266117095947266 mm

Highest mean error: 4.185525894165039 mm for frame 210

Lowest mean error: 2.6775481700897217 mm for frame 18

Saving results

Total time: 42.7886848449707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451479
Iteration 2/25 | Loss: 0.00110520
Iteration 3/25 | Loss: 0.00097626
Iteration 4/25 | Loss: 0.00096488
Iteration 5/25 | Loss: 0.00096243
Iteration 6/25 | Loss: 0.00096213
Iteration 7/25 | Loss: 0.00096213
Iteration 8/25 | Loss: 0.00096213
Iteration 9/25 | Loss: 0.00096213
Iteration 10/25 | Loss: 0.00096213
Iteration 11/25 | Loss: 0.00096213
Iteration 12/25 | Loss: 0.00096213
Iteration 13/25 | Loss: 0.00096213
Iteration 14/25 | Loss: 0.00096213
Iteration 15/25 | Loss: 0.00096213
Iteration 16/25 | Loss: 0.00096213
Iteration 17/25 | Loss: 0.00096213
Iteration 18/25 | Loss: 0.00096213
Iteration 19/25 | Loss: 0.00096213
Iteration 20/25 | Loss: 0.00096213
Iteration 21/25 | Loss: 0.00096213
Iteration 22/25 | Loss: 0.00096213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009621338685974479, 0.0009621338685974479, 0.0009621338685974479, 0.0009621338685974479, 0.0009621338685974479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009621338685974479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34309983
Iteration 2/25 | Loss: 0.00064715
Iteration 3/25 | Loss: 0.00064715
Iteration 4/25 | Loss: 0.00064715
Iteration 5/25 | Loss: 0.00064715
Iteration 6/25 | Loss: 0.00064715
Iteration 7/25 | Loss: 0.00064715
Iteration 8/25 | Loss: 0.00064715
Iteration 9/25 | Loss: 0.00064715
Iteration 10/25 | Loss: 0.00064715
Iteration 11/25 | Loss: 0.00064715
Iteration 12/25 | Loss: 0.00064715
Iteration 13/25 | Loss: 0.00064715
Iteration 14/25 | Loss: 0.00064715
Iteration 15/25 | Loss: 0.00064715
Iteration 16/25 | Loss: 0.00064715
Iteration 17/25 | Loss: 0.00064715
Iteration 18/25 | Loss: 0.00064715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006471468368545175, 0.0006471468368545175, 0.0006471468368545175, 0.0006471468368545175, 0.0006471468368545175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006471468368545175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064715
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00001497
Iteration 4/1000 | Loss: 0.00001302
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001143
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001123
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001108
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001085
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001084
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001080
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001079
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001078
Iteration 66/1000 | Loss: 0.00001078
Iteration 67/1000 | Loss: 0.00001078
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001077
Iteration 70/1000 | Loss: 0.00001077
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001077
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001076
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001068
Iteration 112/1000 | Loss: 0.00001068
Iteration 113/1000 | Loss: 0.00001068
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Iteration 123/1000 | Loss: 0.00001067
Iteration 124/1000 | Loss: 0.00001067
Iteration 125/1000 | Loss: 0.00001067
Iteration 126/1000 | Loss: 0.00001067
Iteration 127/1000 | Loss: 0.00001067
Iteration 128/1000 | Loss: 0.00001067
Iteration 129/1000 | Loss: 0.00001067
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001066
Iteration 133/1000 | Loss: 0.00001066
Iteration 134/1000 | Loss: 0.00001066
Iteration 135/1000 | Loss: 0.00001066
Iteration 136/1000 | Loss: 0.00001065
Iteration 137/1000 | Loss: 0.00001065
Iteration 138/1000 | Loss: 0.00001065
Iteration 139/1000 | Loss: 0.00001065
Iteration 140/1000 | Loss: 0.00001065
Iteration 141/1000 | Loss: 0.00001065
Iteration 142/1000 | Loss: 0.00001065
Iteration 143/1000 | Loss: 0.00001065
Iteration 144/1000 | Loss: 0.00001065
Iteration 145/1000 | Loss: 0.00001065
Iteration 146/1000 | Loss: 0.00001065
Iteration 147/1000 | Loss: 0.00001065
Iteration 148/1000 | Loss: 0.00001065
Iteration 149/1000 | Loss: 0.00001065
Iteration 150/1000 | Loss: 0.00001065
Iteration 151/1000 | Loss: 0.00001064
Iteration 152/1000 | Loss: 0.00001064
Iteration 153/1000 | Loss: 0.00001064
Iteration 154/1000 | Loss: 0.00001064
Iteration 155/1000 | Loss: 0.00001064
Iteration 156/1000 | Loss: 0.00001064
Iteration 157/1000 | Loss: 0.00001064
Iteration 158/1000 | Loss: 0.00001064
Iteration 159/1000 | Loss: 0.00001064
Iteration 160/1000 | Loss: 0.00001064
Iteration 161/1000 | Loss: 0.00001064
Iteration 162/1000 | Loss: 0.00001064
Iteration 163/1000 | Loss: 0.00001064
Iteration 164/1000 | Loss: 0.00001064
Iteration 165/1000 | Loss: 0.00001064
Iteration 166/1000 | Loss: 0.00001064
Iteration 167/1000 | Loss: 0.00001064
Iteration 168/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.0636770639393944e-05, 1.0636770639393944e-05, 1.0636770639393944e-05, 1.0636770639393944e-05, 1.0636770639393944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0636770639393944e-05

Optimization complete. Final v2v error: 2.6752564907073975 mm

Highest mean error: 3.046769142150879 mm for frame 142

Lowest mean error: 2.4020636081695557 mm for frame 93

Saving results

Total time: 38.99465298652649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097556
Iteration 2/25 | Loss: 0.01097556
Iteration 3/25 | Loss: 0.01097556
Iteration 4/25 | Loss: 0.00169561
Iteration 5/25 | Loss: 0.00104554
Iteration 6/25 | Loss: 0.00097103
Iteration 7/25 | Loss: 0.00096023
Iteration 8/25 | Loss: 0.00095185
Iteration 9/25 | Loss: 0.00093408
Iteration 10/25 | Loss: 0.00089405
Iteration 11/25 | Loss: 0.00087928
Iteration 12/25 | Loss: 0.00087103
Iteration 13/25 | Loss: 0.00086859
Iteration 14/25 | Loss: 0.00086317
Iteration 15/25 | Loss: 0.00086225
Iteration 16/25 | Loss: 0.00086633
Iteration 17/25 | Loss: 0.00086321
Iteration 18/25 | Loss: 0.00086178
Iteration 19/25 | Loss: 0.00086144
Iteration 20/25 | Loss: 0.00086144
Iteration 21/25 | Loss: 0.00086144
Iteration 22/25 | Loss: 0.00086189
Iteration 23/25 | Loss: 0.00086278
Iteration 24/25 | Loss: 0.00086148
Iteration 25/25 | Loss: 0.00086158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40075791
Iteration 2/25 | Loss: 0.00086599
Iteration 3/25 | Loss: 0.00082666
Iteration 4/25 | Loss: 0.00082666
Iteration 5/25 | Loss: 0.00082666
Iteration 6/25 | Loss: 0.00082666
Iteration 7/25 | Loss: 0.00082666
Iteration 8/25 | Loss: 0.00082666
Iteration 9/25 | Loss: 0.00082666
Iteration 10/25 | Loss: 0.00082666
Iteration 11/25 | Loss: 0.00082666
Iteration 12/25 | Loss: 0.00082666
Iteration 13/25 | Loss: 0.00082666
Iteration 14/25 | Loss: 0.00082666
Iteration 15/25 | Loss: 0.00082666
Iteration 16/25 | Loss: 0.00082666
Iteration 17/25 | Loss: 0.00082666
Iteration 18/25 | Loss: 0.00082666
Iteration 19/25 | Loss: 0.00082666
Iteration 20/25 | Loss: 0.00082666
Iteration 21/25 | Loss: 0.00082666
Iteration 22/25 | Loss: 0.00082666
Iteration 23/25 | Loss: 0.00082666
Iteration 24/25 | Loss: 0.00082666
Iteration 25/25 | Loss: 0.00082666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082666
Iteration 2/1000 | Loss: 0.00002990
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00006975
Iteration 5/1000 | Loss: 0.00002946
Iteration 6/1000 | Loss: 0.00006492
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00003892
Iteration 9/1000 | Loss: 0.00004602
Iteration 10/1000 | Loss: 0.00000901
Iteration 11/1000 | Loss: 0.00004770
Iteration 12/1000 | Loss: 0.00017186
Iteration 13/1000 | Loss: 0.00005205
Iteration 14/1000 | Loss: 0.00012734
Iteration 15/1000 | Loss: 0.00002304
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00000810
Iteration 18/1000 | Loss: 0.00000801
Iteration 19/1000 | Loss: 0.00000801
Iteration 20/1000 | Loss: 0.00000801
Iteration 21/1000 | Loss: 0.00000800
Iteration 22/1000 | Loss: 0.00000800
Iteration 23/1000 | Loss: 0.00000799
Iteration 24/1000 | Loss: 0.00000799
Iteration 25/1000 | Loss: 0.00000799
Iteration 26/1000 | Loss: 0.00000798
Iteration 27/1000 | Loss: 0.00000798
Iteration 28/1000 | Loss: 0.00000797
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00002772
Iteration 31/1000 | Loss: 0.00002700
Iteration 32/1000 | Loss: 0.00000795
Iteration 33/1000 | Loss: 0.00000791
Iteration 34/1000 | Loss: 0.00000791
Iteration 35/1000 | Loss: 0.00000791
Iteration 36/1000 | Loss: 0.00000921
Iteration 37/1000 | Loss: 0.00000799
Iteration 38/1000 | Loss: 0.00000794
Iteration 39/1000 | Loss: 0.00000794
Iteration 40/1000 | Loss: 0.00000788
Iteration 41/1000 | Loss: 0.00000788
Iteration 42/1000 | Loss: 0.00000788
Iteration 43/1000 | Loss: 0.00000788
Iteration 44/1000 | Loss: 0.00000788
Iteration 45/1000 | Loss: 0.00000788
Iteration 46/1000 | Loss: 0.00000788
Iteration 47/1000 | Loss: 0.00000788
Iteration 48/1000 | Loss: 0.00000788
Iteration 49/1000 | Loss: 0.00000788
Iteration 50/1000 | Loss: 0.00000788
Iteration 51/1000 | Loss: 0.00000788
Iteration 52/1000 | Loss: 0.00000788
Iteration 53/1000 | Loss: 0.00000788
Iteration 54/1000 | Loss: 0.00000788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [7.881149031163659e-06, 7.881149031163659e-06, 7.881149031163659e-06, 7.881149031163659e-06, 7.881149031163659e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.881149031163659e-06

Optimization complete. Final v2v error: 2.281092643737793 mm

Highest mean error: 9.038081169128418 mm for frame 113

Lowest mean error: 2.03757381439209 mm for frame 84

Saving results

Total time: 63.99728488922119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506723
Iteration 2/25 | Loss: 0.00126286
Iteration 3/25 | Loss: 0.00098674
Iteration 4/25 | Loss: 0.00095517
Iteration 5/25 | Loss: 0.00095189
Iteration 6/25 | Loss: 0.00095079
Iteration 7/25 | Loss: 0.00095077
Iteration 8/25 | Loss: 0.00095077
Iteration 9/25 | Loss: 0.00095077
Iteration 10/25 | Loss: 0.00095077
Iteration 11/25 | Loss: 0.00095077
Iteration 12/25 | Loss: 0.00095077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009507650975137949, 0.0009507650975137949, 0.0009507650975137949, 0.0009507650975137949, 0.0009507650975137949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009507650975137949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34995425
Iteration 2/25 | Loss: 0.00056628
Iteration 3/25 | Loss: 0.00056628
Iteration 4/25 | Loss: 0.00056628
Iteration 5/25 | Loss: 0.00056628
Iteration 6/25 | Loss: 0.00056628
Iteration 7/25 | Loss: 0.00056628
Iteration 8/25 | Loss: 0.00056628
Iteration 9/25 | Loss: 0.00056628
Iteration 10/25 | Loss: 0.00056628
Iteration 11/25 | Loss: 0.00056628
Iteration 12/25 | Loss: 0.00056628
Iteration 13/25 | Loss: 0.00056628
Iteration 14/25 | Loss: 0.00056628
Iteration 15/25 | Loss: 0.00056628
Iteration 16/25 | Loss: 0.00056628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005662766634486616, 0.0005662766634486616, 0.0005662766634486616, 0.0005662766634486616, 0.0005662766634486616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005662766634486616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056628
Iteration 2/1000 | Loss: 0.00004214
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001620
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001607
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001591
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001577
Iteration 31/1000 | Loss: 0.00001576
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001541
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001540
Iteration 132/1000 | Loss: 0.00001540
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00001539
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001539
Iteration 146/1000 | Loss: 0.00001539
Iteration 147/1000 | Loss: 0.00001539
Iteration 148/1000 | Loss: 0.00001539
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001538
Iteration 152/1000 | Loss: 0.00001538
Iteration 153/1000 | Loss: 0.00001538
Iteration 154/1000 | Loss: 0.00001538
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001537
Iteration 165/1000 | Loss: 0.00001537
Iteration 166/1000 | Loss: 0.00001537
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001537
Iteration 179/1000 | Loss: 0.00001537
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.536775926069822e-05, 1.536775926069822e-05, 1.536775926069822e-05, 1.536775926069822e-05, 1.536775926069822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536775926069822e-05

Optimization complete. Final v2v error: 2.8771355152130127 mm

Highest mean error: 4.645256042480469 mm for frame 61

Lowest mean error: 2.191234827041626 mm for frame 101

Saving results

Total time: 37.978267669677734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00361349
Iteration 2/25 | Loss: 0.00115071
Iteration 3/25 | Loss: 0.00102447
Iteration 4/25 | Loss: 0.00100337
Iteration 5/25 | Loss: 0.00099529
Iteration 6/25 | Loss: 0.00099284
Iteration 7/25 | Loss: 0.00099238
Iteration 8/25 | Loss: 0.00099238
Iteration 9/25 | Loss: 0.00099238
Iteration 10/25 | Loss: 0.00099238
Iteration 11/25 | Loss: 0.00099238
Iteration 12/25 | Loss: 0.00099238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009923754259943962, 0.0009923754259943962, 0.0009923754259943962, 0.0009923754259943962, 0.0009923754259943962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009923754259943962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31198692
Iteration 2/25 | Loss: 0.00095876
Iteration 3/25 | Loss: 0.00095876
Iteration 4/25 | Loss: 0.00095875
Iteration 5/25 | Loss: 0.00095875
Iteration 6/25 | Loss: 0.00095875
Iteration 7/25 | Loss: 0.00095875
Iteration 8/25 | Loss: 0.00095875
Iteration 9/25 | Loss: 0.00095875
Iteration 10/25 | Loss: 0.00095875
Iteration 11/25 | Loss: 0.00095875
Iteration 12/25 | Loss: 0.00095875
Iteration 13/25 | Loss: 0.00095875
Iteration 14/25 | Loss: 0.00095875
Iteration 15/25 | Loss: 0.00095875
Iteration 16/25 | Loss: 0.00095875
Iteration 17/25 | Loss: 0.00095875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009587528184056282, 0.0009587528184056282, 0.0009587528184056282, 0.0009587528184056282, 0.0009587528184056282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009587528184056282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095875
Iteration 2/1000 | Loss: 0.00004924
Iteration 3/1000 | Loss: 0.00002655
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001709
Iteration 7/1000 | Loss: 0.00001634
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001523
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001516
Iteration 24/1000 | Loss: 0.00001512
Iteration 25/1000 | Loss: 0.00001510
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001502
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001501
Iteration 44/1000 | Loss: 0.00001501
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001491
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001486
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001486
Iteration 105/1000 | Loss: 0.00001485
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001481
Iteration 127/1000 | Loss: 0.00001481
Iteration 128/1000 | Loss: 0.00001481
Iteration 129/1000 | Loss: 0.00001481
Iteration 130/1000 | Loss: 0.00001481
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001481
Iteration 133/1000 | Loss: 0.00001481
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001480
Iteration 136/1000 | Loss: 0.00001480
Iteration 137/1000 | Loss: 0.00001480
Iteration 138/1000 | Loss: 0.00001480
Iteration 139/1000 | Loss: 0.00001480
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001479
Iteration 146/1000 | Loss: 0.00001479
Iteration 147/1000 | Loss: 0.00001479
Iteration 148/1000 | Loss: 0.00001479
Iteration 149/1000 | Loss: 0.00001479
Iteration 150/1000 | Loss: 0.00001479
Iteration 151/1000 | Loss: 0.00001479
Iteration 152/1000 | Loss: 0.00001479
Iteration 153/1000 | Loss: 0.00001479
Iteration 154/1000 | Loss: 0.00001479
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001478
Iteration 158/1000 | Loss: 0.00001478
Iteration 159/1000 | Loss: 0.00001478
Iteration 160/1000 | Loss: 0.00001478
Iteration 161/1000 | Loss: 0.00001478
Iteration 162/1000 | Loss: 0.00001478
Iteration 163/1000 | Loss: 0.00001478
Iteration 164/1000 | Loss: 0.00001478
Iteration 165/1000 | Loss: 0.00001478
Iteration 166/1000 | Loss: 0.00001478
Iteration 167/1000 | Loss: 0.00001478
Iteration 168/1000 | Loss: 0.00001478
Iteration 169/1000 | Loss: 0.00001478
Iteration 170/1000 | Loss: 0.00001478
Iteration 171/1000 | Loss: 0.00001478
Iteration 172/1000 | Loss: 0.00001478
Iteration 173/1000 | Loss: 0.00001478
Iteration 174/1000 | Loss: 0.00001478
Iteration 175/1000 | Loss: 0.00001478
Iteration 176/1000 | Loss: 0.00001478
Iteration 177/1000 | Loss: 0.00001478
Iteration 178/1000 | Loss: 0.00001478
Iteration 179/1000 | Loss: 0.00001478
Iteration 180/1000 | Loss: 0.00001478
Iteration 181/1000 | Loss: 0.00001478
Iteration 182/1000 | Loss: 0.00001478
Iteration 183/1000 | Loss: 0.00001478
Iteration 184/1000 | Loss: 0.00001478
Iteration 185/1000 | Loss: 0.00001478
Iteration 186/1000 | Loss: 0.00001478
Iteration 187/1000 | Loss: 0.00001478
Iteration 188/1000 | Loss: 0.00001478
Iteration 189/1000 | Loss: 0.00001478
Iteration 190/1000 | Loss: 0.00001478
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001478
Iteration 194/1000 | Loss: 0.00001478
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001478
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Iteration 200/1000 | Loss: 0.00001478
Iteration 201/1000 | Loss: 0.00001478
Iteration 202/1000 | Loss: 0.00001478
Iteration 203/1000 | Loss: 0.00001478
Iteration 204/1000 | Loss: 0.00001478
Iteration 205/1000 | Loss: 0.00001478
Iteration 206/1000 | Loss: 0.00001478
Iteration 207/1000 | Loss: 0.00001478
Iteration 208/1000 | Loss: 0.00001478
Iteration 209/1000 | Loss: 0.00001478
Iteration 210/1000 | Loss: 0.00001478
Iteration 211/1000 | Loss: 0.00001478
Iteration 212/1000 | Loss: 0.00001478
Iteration 213/1000 | Loss: 0.00001478
Iteration 214/1000 | Loss: 0.00001478
Iteration 215/1000 | Loss: 0.00001478
Iteration 216/1000 | Loss: 0.00001478
Iteration 217/1000 | Loss: 0.00001478
Iteration 218/1000 | Loss: 0.00001478
Iteration 219/1000 | Loss: 0.00001478
Iteration 220/1000 | Loss: 0.00001478
Iteration 221/1000 | Loss: 0.00001478
Iteration 222/1000 | Loss: 0.00001478
Iteration 223/1000 | Loss: 0.00001478
Iteration 224/1000 | Loss: 0.00001478
Iteration 225/1000 | Loss: 0.00001478
Iteration 226/1000 | Loss: 0.00001478
Iteration 227/1000 | Loss: 0.00001478
Iteration 228/1000 | Loss: 0.00001478
Iteration 229/1000 | Loss: 0.00001478
Iteration 230/1000 | Loss: 0.00001478
Iteration 231/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.477744535804959e-05, 1.477744535804959e-05, 1.477744535804959e-05, 1.477744535804959e-05, 1.477744535804959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.477744535804959e-05

Optimization complete. Final v2v error: 3.176161050796509 mm

Highest mean error: 3.9763646125793457 mm for frame 130

Lowest mean error: 2.50313401222229 mm for frame 48

Saving results

Total time: 44.825117111206055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034503
Iteration 2/25 | Loss: 0.00457988
Iteration 3/25 | Loss: 0.00256322
Iteration 4/25 | Loss: 0.00219920
Iteration 5/25 | Loss: 0.00203760
Iteration 6/25 | Loss: 0.00191760
Iteration 7/25 | Loss: 0.00185730
Iteration 8/25 | Loss: 0.00182117
Iteration 9/25 | Loss: 0.00178189
Iteration 10/25 | Loss: 0.00181470
Iteration 11/25 | Loss: 0.00176299
Iteration 12/25 | Loss: 0.00179251
Iteration 13/25 | Loss: 0.00171486
Iteration 14/25 | Loss: 0.00169878
Iteration 15/25 | Loss: 0.00167715
Iteration 16/25 | Loss: 0.00164124
Iteration 17/25 | Loss: 0.00163987
Iteration 18/25 | Loss: 0.00164683
Iteration 19/25 | Loss: 0.00163696
Iteration 20/25 | Loss: 0.00163600
Iteration 21/25 | Loss: 0.00163582
Iteration 22/25 | Loss: 0.00163582
Iteration 23/25 | Loss: 0.00163582
Iteration 24/25 | Loss: 0.00163582
Iteration 25/25 | Loss: 0.00163582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28840649
Iteration 2/25 | Loss: 0.01574377
Iteration 3/25 | Loss: 0.00367642
Iteration 4/25 | Loss: 0.00367641
Iteration 5/25 | Loss: 0.00367641
Iteration 6/25 | Loss: 0.00367641
Iteration 7/25 | Loss: 0.00367641
Iteration 8/25 | Loss: 0.00367641
Iteration 9/25 | Loss: 0.00367640
Iteration 10/25 | Loss: 0.00367640
Iteration 11/25 | Loss: 0.00367640
Iteration 12/25 | Loss: 0.00367640
Iteration 13/25 | Loss: 0.00367640
Iteration 14/25 | Loss: 0.00367640
Iteration 15/25 | Loss: 0.00367640
Iteration 16/25 | Loss: 0.00367640
Iteration 17/25 | Loss: 0.00367640
Iteration 18/25 | Loss: 0.00367640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003676403546705842, 0.003676403546705842, 0.003676403546705842, 0.003676403546705842, 0.003676403546705842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003676403546705842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00367640
Iteration 2/1000 | Loss: 0.01318638
Iteration 3/1000 | Loss: 0.00096611
Iteration 4/1000 | Loss: 0.00047021
Iteration 5/1000 | Loss: 0.00456250
Iteration 6/1000 | Loss: 0.00184739
Iteration 7/1000 | Loss: 0.01168463
Iteration 8/1000 | Loss: 0.00106599
Iteration 9/1000 | Loss: 0.00058793
Iteration 10/1000 | Loss: 0.00123167
Iteration 11/1000 | Loss: 0.00049503
Iteration 12/1000 | Loss: 0.00220247
Iteration 13/1000 | Loss: 0.00103443
Iteration 14/1000 | Loss: 0.00185082
Iteration 15/1000 | Loss: 0.01562440
Iteration 16/1000 | Loss: 0.00759650
Iteration 17/1000 | Loss: 0.00570655
Iteration 18/1000 | Loss: 0.00138142
Iteration 19/1000 | Loss: 0.00157527
Iteration 20/1000 | Loss: 0.00098134
Iteration 21/1000 | Loss: 0.00042586
Iteration 22/1000 | Loss: 0.00131967
Iteration 23/1000 | Loss: 0.00342632
Iteration 24/1000 | Loss: 0.00018558
Iteration 25/1000 | Loss: 0.00047151
Iteration 26/1000 | Loss: 0.00013555
Iteration 27/1000 | Loss: 0.00061557
Iteration 28/1000 | Loss: 0.00005382
Iteration 29/1000 | Loss: 0.00038744
Iteration 30/1000 | Loss: 0.00031067
Iteration 31/1000 | Loss: 0.00008225
Iteration 32/1000 | Loss: 0.00011850
Iteration 33/1000 | Loss: 0.00011476
Iteration 34/1000 | Loss: 0.00047111
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00030411
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001279
Iteration 40/1000 | Loss: 0.00013572
Iteration 41/1000 | Loss: 0.00011827
Iteration 42/1000 | Loss: 0.00009437
Iteration 43/1000 | Loss: 0.00004375
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00007134
Iteration 46/1000 | Loss: 0.00024756
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00006754
Iteration 49/1000 | Loss: 0.00001048
Iteration 50/1000 | Loss: 0.00001018
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00012812
Iteration 53/1000 | Loss: 0.00019871
Iteration 54/1000 | Loss: 0.00001061
Iteration 55/1000 | Loss: 0.00004556
Iteration 56/1000 | Loss: 0.00000981
Iteration 57/1000 | Loss: 0.00009653
Iteration 58/1000 | Loss: 0.00016665
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00008679
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000969
Iteration 63/1000 | Loss: 0.00000962
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000951
Iteration 67/1000 | Loss: 0.00000950
Iteration 68/1000 | Loss: 0.00000950
Iteration 69/1000 | Loss: 0.00000950
Iteration 70/1000 | Loss: 0.00000950
Iteration 71/1000 | Loss: 0.00000950
Iteration 72/1000 | Loss: 0.00000950
Iteration 73/1000 | Loss: 0.00000950
Iteration 74/1000 | Loss: 0.00000950
Iteration 75/1000 | Loss: 0.00000949
Iteration 76/1000 | Loss: 0.00000949
Iteration 77/1000 | Loss: 0.00000949
Iteration 78/1000 | Loss: 0.00000949
Iteration 79/1000 | Loss: 0.00000949
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000949
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000948
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000948
Iteration 89/1000 | Loss: 0.00000948
Iteration 90/1000 | Loss: 0.00000948
Iteration 91/1000 | Loss: 0.00000948
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000947
Iteration 97/1000 | Loss: 0.00000947
Iteration 98/1000 | Loss: 0.00000947
Iteration 99/1000 | Loss: 0.00000947
Iteration 100/1000 | Loss: 0.00000947
Iteration 101/1000 | Loss: 0.00000947
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000946
Iteration 105/1000 | Loss: 0.00000946
Iteration 106/1000 | Loss: 0.00000946
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000946
Iteration 112/1000 | Loss: 0.00000946
Iteration 113/1000 | Loss: 0.00000946
Iteration 114/1000 | Loss: 0.00000946
Iteration 115/1000 | Loss: 0.00000946
Iteration 116/1000 | Loss: 0.00000946
Iteration 117/1000 | Loss: 0.00000946
Iteration 118/1000 | Loss: 0.00000946
Iteration 119/1000 | Loss: 0.00000946
Iteration 120/1000 | Loss: 0.00000946
Iteration 121/1000 | Loss: 0.00000946
Iteration 122/1000 | Loss: 0.00000946
Iteration 123/1000 | Loss: 0.00000946
Iteration 124/1000 | Loss: 0.00000946
Iteration 125/1000 | Loss: 0.00000946
Iteration 126/1000 | Loss: 0.00000946
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000946
Iteration 134/1000 | Loss: 0.00000946
Iteration 135/1000 | Loss: 0.00000946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [9.456077350478154e-06, 9.456077350478154e-06, 9.456077350478154e-06, 9.456077350478154e-06, 9.456077350478154e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.456077350478154e-06

Optimization complete. Final v2v error: 2.5169849395751953 mm

Highest mean error: 9.829011917114258 mm for frame 100

Lowest mean error: 2.3182926177978516 mm for frame 110

Saving results

Total time: 122.98136782646179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876680
Iteration 2/25 | Loss: 0.00105773
Iteration 3/25 | Loss: 0.00091312
Iteration 4/25 | Loss: 0.00090382
Iteration 5/25 | Loss: 0.00090159
Iteration 6/25 | Loss: 0.00090130
Iteration 7/25 | Loss: 0.00090130
Iteration 8/25 | Loss: 0.00090130
Iteration 9/25 | Loss: 0.00090130
Iteration 10/25 | Loss: 0.00090130
Iteration 11/25 | Loss: 0.00090130
Iteration 12/25 | Loss: 0.00090130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009013035451062024, 0.0009013035451062024, 0.0009013035451062024, 0.0009013035451062024, 0.0009013035451062024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009013035451062024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33012652
Iteration 2/25 | Loss: 0.00060457
Iteration 3/25 | Loss: 0.00060457
Iteration 4/25 | Loss: 0.00060457
Iteration 5/25 | Loss: 0.00060457
Iteration 6/25 | Loss: 0.00060457
Iteration 7/25 | Loss: 0.00060457
Iteration 8/25 | Loss: 0.00060457
Iteration 9/25 | Loss: 0.00060457
Iteration 10/25 | Loss: 0.00060457
Iteration 11/25 | Loss: 0.00060457
Iteration 12/25 | Loss: 0.00060457
Iteration 13/25 | Loss: 0.00060457
Iteration 14/25 | Loss: 0.00060457
Iteration 15/25 | Loss: 0.00060457
Iteration 16/25 | Loss: 0.00060457
Iteration 17/25 | Loss: 0.00060457
Iteration 18/25 | Loss: 0.00060457
Iteration 19/25 | Loss: 0.00060457
Iteration 20/25 | Loss: 0.00060457
Iteration 21/25 | Loss: 0.00060457
Iteration 22/25 | Loss: 0.00060457
Iteration 23/25 | Loss: 0.00060457
Iteration 24/25 | Loss: 0.00060457
Iteration 25/25 | Loss: 0.00060457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060457
Iteration 2/1000 | Loss: 0.00001551
Iteration 3/1000 | Loss: 0.00001024
Iteration 4/1000 | Loss: 0.00000896
Iteration 5/1000 | Loss: 0.00000848
Iteration 6/1000 | Loss: 0.00000823
Iteration 7/1000 | Loss: 0.00000797
Iteration 8/1000 | Loss: 0.00000784
Iteration 9/1000 | Loss: 0.00000783
Iteration 10/1000 | Loss: 0.00000782
Iteration 11/1000 | Loss: 0.00000776
Iteration 12/1000 | Loss: 0.00000775
Iteration 13/1000 | Loss: 0.00000775
Iteration 14/1000 | Loss: 0.00000775
Iteration 15/1000 | Loss: 0.00000774
Iteration 16/1000 | Loss: 0.00000770
Iteration 17/1000 | Loss: 0.00000769
Iteration 18/1000 | Loss: 0.00000769
Iteration 19/1000 | Loss: 0.00000769
Iteration 20/1000 | Loss: 0.00000769
Iteration 21/1000 | Loss: 0.00000768
Iteration 22/1000 | Loss: 0.00000765
Iteration 23/1000 | Loss: 0.00000764
Iteration 24/1000 | Loss: 0.00000764
Iteration 25/1000 | Loss: 0.00000763
Iteration 26/1000 | Loss: 0.00000763
Iteration 27/1000 | Loss: 0.00000763
Iteration 28/1000 | Loss: 0.00000763
Iteration 29/1000 | Loss: 0.00000763
Iteration 30/1000 | Loss: 0.00000763
Iteration 31/1000 | Loss: 0.00000763
Iteration 32/1000 | Loss: 0.00000763
Iteration 33/1000 | Loss: 0.00000763
Iteration 34/1000 | Loss: 0.00000763
Iteration 35/1000 | Loss: 0.00000763
Iteration 36/1000 | Loss: 0.00000762
Iteration 37/1000 | Loss: 0.00000762
Iteration 38/1000 | Loss: 0.00000760
Iteration 39/1000 | Loss: 0.00000760
Iteration 40/1000 | Loss: 0.00000759
Iteration 41/1000 | Loss: 0.00000759
Iteration 42/1000 | Loss: 0.00000759
Iteration 43/1000 | Loss: 0.00000759
Iteration 44/1000 | Loss: 0.00000759
Iteration 45/1000 | Loss: 0.00000758
Iteration 46/1000 | Loss: 0.00000758
Iteration 47/1000 | Loss: 0.00000757
Iteration 48/1000 | Loss: 0.00000757
Iteration 49/1000 | Loss: 0.00000757
Iteration 50/1000 | Loss: 0.00000756
Iteration 51/1000 | Loss: 0.00000756
Iteration 52/1000 | Loss: 0.00000756
Iteration 53/1000 | Loss: 0.00000756
Iteration 54/1000 | Loss: 0.00000755
Iteration 55/1000 | Loss: 0.00000755
Iteration 56/1000 | Loss: 0.00000755
Iteration 57/1000 | Loss: 0.00000754
Iteration 58/1000 | Loss: 0.00000753
Iteration 59/1000 | Loss: 0.00000753
Iteration 60/1000 | Loss: 0.00000753
Iteration 61/1000 | Loss: 0.00000752
Iteration 62/1000 | Loss: 0.00000752
Iteration 63/1000 | Loss: 0.00000752
Iteration 64/1000 | Loss: 0.00000752
Iteration 65/1000 | Loss: 0.00000752
Iteration 66/1000 | Loss: 0.00000752
Iteration 67/1000 | Loss: 0.00000752
Iteration 68/1000 | Loss: 0.00000752
Iteration 69/1000 | Loss: 0.00000752
Iteration 70/1000 | Loss: 0.00000752
Iteration 71/1000 | Loss: 0.00000752
Iteration 72/1000 | Loss: 0.00000752
Iteration 73/1000 | Loss: 0.00000752
Iteration 74/1000 | Loss: 0.00000752
Iteration 75/1000 | Loss: 0.00000752
Iteration 76/1000 | Loss: 0.00000752
Iteration 77/1000 | Loss: 0.00000752
Iteration 78/1000 | Loss: 0.00000752
Iteration 79/1000 | Loss: 0.00000752
Iteration 80/1000 | Loss: 0.00000752
Iteration 81/1000 | Loss: 0.00000752
Iteration 82/1000 | Loss: 0.00000752
Iteration 83/1000 | Loss: 0.00000752
Iteration 84/1000 | Loss: 0.00000752
Iteration 85/1000 | Loss: 0.00000752
Iteration 86/1000 | Loss: 0.00000752
Iteration 87/1000 | Loss: 0.00000752
Iteration 88/1000 | Loss: 0.00000752
Iteration 89/1000 | Loss: 0.00000752
Iteration 90/1000 | Loss: 0.00000752
Iteration 91/1000 | Loss: 0.00000752
Iteration 92/1000 | Loss: 0.00000752
Iteration 93/1000 | Loss: 0.00000752
Iteration 94/1000 | Loss: 0.00000752
Iteration 95/1000 | Loss: 0.00000752
Iteration 96/1000 | Loss: 0.00000752
Iteration 97/1000 | Loss: 0.00000752
Iteration 98/1000 | Loss: 0.00000752
Iteration 99/1000 | Loss: 0.00000752
Iteration 100/1000 | Loss: 0.00000752
Iteration 101/1000 | Loss: 0.00000752
Iteration 102/1000 | Loss: 0.00000752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [7.518442998843966e-06, 7.518442998843966e-06, 7.518442998843966e-06, 7.518442998843966e-06, 7.518442998843966e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.518442998843966e-06

Optimization complete. Final v2v error: 2.315737724304199 mm

Highest mean error: 2.4818596839904785 mm for frame 142

Lowest mean error: 2.1578376293182373 mm for frame 55

Saving results

Total time: 28.763221740722656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082463
Iteration 2/25 | Loss: 0.00257006
Iteration 3/25 | Loss: 0.00166922
Iteration 4/25 | Loss: 0.00140713
Iteration 5/25 | Loss: 0.00147849
Iteration 6/25 | Loss: 0.00117584
Iteration 7/25 | Loss: 0.00105078
Iteration 8/25 | Loss: 0.00098340
Iteration 9/25 | Loss: 0.00095833
Iteration 10/25 | Loss: 0.00095019
Iteration 11/25 | Loss: 0.00090863
Iteration 12/25 | Loss: 0.00090462
Iteration 13/25 | Loss: 0.00090023
Iteration 14/25 | Loss: 0.00090085
Iteration 15/25 | Loss: 0.00090071
Iteration 16/25 | Loss: 0.00090155
Iteration 17/25 | Loss: 0.00090286
Iteration 18/25 | Loss: 0.00090097
Iteration 19/25 | Loss: 0.00090186
Iteration 20/25 | Loss: 0.00090086
Iteration 21/25 | Loss: 0.00090046
Iteration 22/25 | Loss: 0.00090078
Iteration 23/25 | Loss: 0.00090078
Iteration 24/25 | Loss: 0.00090290
Iteration 25/25 | Loss: 0.00090249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42171335
Iteration 2/25 | Loss: 0.00078322
Iteration 3/25 | Loss: 0.00078327
Iteration 4/25 | Loss: 0.00078267
Iteration 5/25 | Loss: 0.00075962
Iteration 6/25 | Loss: 0.00075962
Iteration 7/25 | Loss: 0.00075962
Iteration 8/25 | Loss: 0.00075962
Iteration 9/25 | Loss: 0.00075962
Iteration 10/25 | Loss: 0.00075962
Iteration 11/25 | Loss: 0.00075962
Iteration 12/25 | Loss: 0.00075962
Iteration 13/25 | Loss: 0.00075962
Iteration 14/25 | Loss: 0.00075962
Iteration 15/25 | Loss: 0.00075962
Iteration 16/25 | Loss: 0.00075962
Iteration 17/25 | Loss: 0.00075962
Iteration 18/25 | Loss: 0.00075962
Iteration 19/25 | Loss: 0.00075962
Iteration 20/25 | Loss: 0.00075962
Iteration 21/25 | Loss: 0.00075962
Iteration 22/25 | Loss: 0.00075962
Iteration 23/25 | Loss: 0.00075962
Iteration 24/25 | Loss: 0.00075962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007596202194690704, 0.0007596202194690704, 0.0007596202194690704, 0.0007596202194690704, 0.0007596202194690704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007596202194690704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075962
Iteration 2/1000 | Loss: 0.00006995
Iteration 3/1000 | Loss: 0.00003985
Iteration 4/1000 | Loss: 0.00016139
Iteration 5/1000 | Loss: 0.00004654
Iteration 6/1000 | Loss: 0.00007312
Iteration 7/1000 | Loss: 0.00004966
Iteration 8/1000 | Loss: 0.00004525
Iteration 9/1000 | Loss: 0.00004829
Iteration 10/1000 | Loss: 0.00008080
Iteration 11/1000 | Loss: 0.00047673
Iteration 12/1000 | Loss: 0.00020252
Iteration 13/1000 | Loss: 0.00056642
Iteration 14/1000 | Loss: 0.00018287
Iteration 15/1000 | Loss: 0.00057347
Iteration 16/1000 | Loss: 0.00042760
Iteration 17/1000 | Loss: 0.00004918
Iteration 18/1000 | Loss: 0.00028115
Iteration 19/1000 | Loss: 0.00053928
Iteration 20/1000 | Loss: 0.00025235
Iteration 21/1000 | Loss: 0.00047003
Iteration 22/1000 | Loss: 0.00099941
Iteration 23/1000 | Loss: 0.00007124
Iteration 24/1000 | Loss: 0.00006167
Iteration 25/1000 | Loss: 0.00005583
Iteration 26/1000 | Loss: 0.00005201
Iteration 27/1000 | Loss: 0.00004571
Iteration 28/1000 | Loss: 0.00003860
Iteration 29/1000 | Loss: 0.00003458
Iteration 30/1000 | Loss: 0.00003968
Iteration 31/1000 | Loss: 0.00004049
Iteration 32/1000 | Loss: 0.00004787
Iteration 33/1000 | Loss: 0.00004103
Iteration 34/1000 | Loss: 0.00003969
Iteration 35/1000 | Loss: 0.00003933
Iteration 36/1000 | Loss: 0.00003587
Iteration 37/1000 | Loss: 0.00003037
Iteration 38/1000 | Loss: 0.00004066
Iteration 39/1000 | Loss: 0.00004450
Iteration 40/1000 | Loss: 0.00004311
Iteration 41/1000 | Loss: 0.00004402
Iteration 42/1000 | Loss: 0.00008089
Iteration 43/1000 | Loss: 0.00036277
Iteration 44/1000 | Loss: 0.00018900
Iteration 45/1000 | Loss: 0.00017047
Iteration 46/1000 | Loss: 0.00004178
Iteration 47/1000 | Loss: 0.00004398
Iteration 48/1000 | Loss: 0.00005475
Iteration 49/1000 | Loss: 0.00005474
Iteration 50/1000 | Loss: 0.00003940
Iteration 51/1000 | Loss: 0.00004721
Iteration 52/1000 | Loss: 0.00002519
Iteration 53/1000 | Loss: 0.00037881
Iteration 54/1000 | Loss: 0.00013891
Iteration 55/1000 | Loss: 0.00005034
Iteration 56/1000 | Loss: 0.00003075
Iteration 57/1000 | Loss: 0.00002331
Iteration 58/1000 | Loss: 0.00027129
Iteration 59/1000 | Loss: 0.00003152
Iteration 60/1000 | Loss: 0.00003282
Iteration 61/1000 | Loss: 0.00003119
Iteration 62/1000 | Loss: 0.00002722
Iteration 63/1000 | Loss: 0.00002455
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00003034
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002815
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001989
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001987
Iteration 87/1000 | Loss: 0.00001987
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00002600
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00002586
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001969
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001967
Iteration 105/1000 | Loss: 0.00001967
Iteration 106/1000 | Loss: 0.00001967
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001962
Iteration 109/1000 | Loss: 0.00001962
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00002140
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00001954
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001951
Iteration 120/1000 | Loss: 0.00002602
Iteration 121/1000 | Loss: 0.00002140
Iteration 122/1000 | Loss: 0.00002380
Iteration 123/1000 | Loss: 0.00001974
Iteration 124/1000 | Loss: 0.00002069
Iteration 125/1000 | Loss: 0.00002069
Iteration 126/1000 | Loss: 0.00002069
Iteration 127/1000 | Loss: 0.00002069
Iteration 128/1000 | Loss: 0.00002068
Iteration 129/1000 | Loss: 0.00004232
Iteration 130/1000 | Loss: 0.00002295
Iteration 131/1000 | Loss: 0.00002066
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001936
Iteration 134/1000 | Loss: 0.00001936
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001935
Iteration 140/1000 | Loss: 0.00001935
Iteration 141/1000 | Loss: 0.00001935
Iteration 142/1000 | Loss: 0.00001935
Iteration 143/1000 | Loss: 0.00001935
Iteration 144/1000 | Loss: 0.00001935
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.9349112335476093e-05, 1.9349112335476093e-05, 1.9349112335476093e-05, 1.9349112335476093e-05, 1.9349112335476093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9349112335476093e-05

Optimization complete. Final v2v error: 2.496452569961548 mm

Highest mean error: 20.370004653930664 mm for frame 74

Lowest mean error: 1.8644747734069824 mm for frame 112

Saving results

Total time: 162.51050209999084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871684
Iteration 2/25 | Loss: 0.00158649
Iteration 3/25 | Loss: 0.00117635
Iteration 4/25 | Loss: 0.00111565
Iteration 5/25 | Loss: 0.00111214
Iteration 6/25 | Loss: 0.00111726
Iteration 7/25 | Loss: 0.00109326
Iteration 8/25 | Loss: 0.00107237
Iteration 9/25 | Loss: 0.00106675
Iteration 10/25 | Loss: 0.00106502
Iteration 11/25 | Loss: 0.00106463
Iteration 12/25 | Loss: 0.00106452
Iteration 13/25 | Loss: 0.00106445
Iteration 14/25 | Loss: 0.00106436
Iteration 15/25 | Loss: 0.00106837
Iteration 16/25 | Loss: 0.00106763
Iteration 17/25 | Loss: 0.00106395
Iteration 18/25 | Loss: 0.00106298
Iteration 19/25 | Loss: 0.00106279
Iteration 20/25 | Loss: 0.00106268
Iteration 21/25 | Loss: 0.00106268
Iteration 22/25 | Loss: 0.00106268
Iteration 23/25 | Loss: 0.00106268
Iteration 24/25 | Loss: 0.00106268
Iteration 25/25 | Loss: 0.00106268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24562824
Iteration 2/25 | Loss: 0.00052017
Iteration 3/25 | Loss: 0.00052012
Iteration 4/25 | Loss: 0.00052012
Iteration 5/25 | Loss: 0.00052012
Iteration 6/25 | Loss: 0.00052012
Iteration 7/25 | Loss: 0.00052012
Iteration 8/25 | Loss: 0.00052012
Iteration 9/25 | Loss: 0.00052012
Iteration 10/25 | Loss: 0.00052012
Iteration 11/25 | Loss: 0.00052012
Iteration 12/25 | Loss: 0.00052012
Iteration 13/25 | Loss: 0.00052012
Iteration 14/25 | Loss: 0.00052012
Iteration 15/25 | Loss: 0.00052012
Iteration 16/25 | Loss: 0.00052012
Iteration 17/25 | Loss: 0.00052012
Iteration 18/25 | Loss: 0.00052012
Iteration 19/25 | Loss: 0.00052012
Iteration 20/25 | Loss: 0.00052012
Iteration 21/25 | Loss: 0.00052012
Iteration 22/25 | Loss: 0.00052012
Iteration 23/25 | Loss: 0.00052012
Iteration 24/25 | Loss: 0.00052012
Iteration 25/25 | Loss: 0.00052012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005201214225962758, 0.0005201214225962758, 0.0005201214225962758, 0.0005201214225962758, 0.0005201214225962758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005201214225962758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052012
Iteration 2/1000 | Loss: 0.00005465
Iteration 3/1000 | Loss: 0.00002850
Iteration 4/1000 | Loss: 0.00002276
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001867
Iteration 9/1000 | Loss: 0.00001838
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001818
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001764
Iteration 17/1000 | Loss: 0.00001759
Iteration 18/1000 | Loss: 0.00001758
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001757
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001755
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001730
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001728
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001724
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001724
Iteration 66/1000 | Loss: 0.00001724
Iteration 67/1000 | Loss: 0.00001724
Iteration 68/1000 | Loss: 0.00001724
Iteration 69/1000 | Loss: 0.00001724
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001723
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001714
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.713077654130757e-05, 1.713077654130757e-05, 1.713077654130757e-05, 1.713077654130757e-05, 1.713077654130757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.713077654130757e-05

Optimization complete. Final v2v error: 3.4905660152435303 mm

Highest mean error: 3.8153324127197266 mm for frame 70

Lowest mean error: 3.07501220703125 mm for frame 35

Saving results

Total time: 72.31334495544434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830085
Iteration 2/25 | Loss: 0.00122294
Iteration 3/25 | Loss: 0.00103875
Iteration 4/25 | Loss: 0.00101298
Iteration 5/25 | Loss: 0.00100406
Iteration 6/25 | Loss: 0.00100209
Iteration 7/25 | Loss: 0.00100127
Iteration 8/25 | Loss: 0.00100118
Iteration 9/25 | Loss: 0.00100118
Iteration 10/25 | Loss: 0.00100118
Iteration 11/25 | Loss: 0.00100118
Iteration 12/25 | Loss: 0.00100118
Iteration 13/25 | Loss: 0.00100118
Iteration 14/25 | Loss: 0.00100118
Iteration 15/25 | Loss: 0.00100118
Iteration 16/25 | Loss: 0.00100118
Iteration 17/25 | Loss: 0.00100118
Iteration 18/25 | Loss: 0.00100118
Iteration 19/25 | Loss: 0.00100118
Iteration 20/25 | Loss: 0.00100118
Iteration 21/25 | Loss: 0.00100118
Iteration 22/25 | Loss: 0.00100118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010011792182922363, 0.0010011792182922363, 0.0010011792182922363, 0.0010011792182922363, 0.0010011792182922363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010011792182922363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29052758
Iteration 2/25 | Loss: 0.00067683
Iteration 3/25 | Loss: 0.00067683
Iteration 4/25 | Loss: 0.00067683
Iteration 5/25 | Loss: 0.00067683
Iteration 6/25 | Loss: 0.00067683
Iteration 7/25 | Loss: 0.00067683
Iteration 8/25 | Loss: 0.00067683
Iteration 9/25 | Loss: 0.00067683
Iteration 10/25 | Loss: 0.00067683
Iteration 11/25 | Loss: 0.00067682
Iteration 12/25 | Loss: 0.00067682
Iteration 13/25 | Loss: 0.00067682
Iteration 14/25 | Loss: 0.00067682
Iteration 15/25 | Loss: 0.00067682
Iteration 16/25 | Loss: 0.00067682
Iteration 17/25 | Loss: 0.00067682
Iteration 18/25 | Loss: 0.00067682
Iteration 19/25 | Loss: 0.00067682
Iteration 20/25 | Loss: 0.00067682
Iteration 21/25 | Loss: 0.00067682
Iteration 22/25 | Loss: 0.00067682
Iteration 23/25 | Loss: 0.00067682
Iteration 24/25 | Loss: 0.00067682
Iteration 25/25 | Loss: 0.00067682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067682
Iteration 2/1000 | Loss: 0.00004009
Iteration 3/1000 | Loss: 0.00002695
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001946
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001718
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001701
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001700
Iteration 23/1000 | Loss: 0.00001699
Iteration 24/1000 | Loss: 0.00001699
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001692
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001690
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001683
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001677
Iteration 87/1000 | Loss: 0.00001677
Iteration 88/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.676578540354967e-05, 1.676578540354967e-05, 1.676578540354967e-05, 1.676578540354967e-05, 1.676578540354967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.676578540354967e-05

Optimization complete. Final v2v error: 3.2171332836151123 mm

Highest mean error: 3.760512113571167 mm for frame 124

Lowest mean error: 2.456831693649292 mm for frame 0

Saving results

Total time: 34.88499593734741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818074
Iteration 2/25 | Loss: 0.00170219
Iteration 3/25 | Loss: 0.00133737
Iteration 4/25 | Loss: 0.00116778
Iteration 5/25 | Loss: 0.00109797
Iteration 6/25 | Loss: 0.00110678
Iteration 7/25 | Loss: 0.00107680
Iteration 8/25 | Loss: 0.00105048
Iteration 9/25 | Loss: 0.00102027
Iteration 10/25 | Loss: 0.00100396
Iteration 11/25 | Loss: 0.00100358
Iteration 12/25 | Loss: 0.00099919
Iteration 13/25 | Loss: 0.00099558
Iteration 14/25 | Loss: 0.00099428
Iteration 15/25 | Loss: 0.00099390
Iteration 16/25 | Loss: 0.00099379
Iteration 17/25 | Loss: 0.00099376
Iteration 18/25 | Loss: 0.00099376
Iteration 19/25 | Loss: 0.00099376
Iteration 20/25 | Loss: 0.00099376
Iteration 21/25 | Loss: 0.00099376
Iteration 22/25 | Loss: 0.00099376
Iteration 23/25 | Loss: 0.00099376
Iteration 24/25 | Loss: 0.00099376
Iteration 25/25 | Loss: 0.00099376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79586339
Iteration 2/25 | Loss: 0.00074303
Iteration 3/25 | Loss: 0.00074303
Iteration 4/25 | Loss: 0.00074303
Iteration 5/25 | Loss: 0.00074303
Iteration 6/25 | Loss: 0.00074303
Iteration 7/25 | Loss: 0.00074303
Iteration 8/25 | Loss: 0.00074303
Iteration 9/25 | Loss: 0.00074303
Iteration 10/25 | Loss: 0.00074303
Iteration 11/25 | Loss: 0.00074303
Iteration 12/25 | Loss: 0.00074303
Iteration 13/25 | Loss: 0.00074303
Iteration 14/25 | Loss: 0.00074303
Iteration 15/25 | Loss: 0.00074303
Iteration 16/25 | Loss: 0.00074303
Iteration 17/25 | Loss: 0.00074303
Iteration 18/25 | Loss: 0.00074303
Iteration 19/25 | Loss: 0.00074303
Iteration 20/25 | Loss: 0.00074303
Iteration 21/25 | Loss: 0.00074303
Iteration 22/25 | Loss: 0.00074303
Iteration 23/25 | Loss: 0.00074303
Iteration 24/25 | Loss: 0.00074303
Iteration 25/25 | Loss: 0.00074303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074303
Iteration 2/1000 | Loss: 0.00003990
Iteration 3/1000 | Loss: 0.00010016
Iteration 4/1000 | Loss: 0.00031834
Iteration 5/1000 | Loss: 0.00003550
Iteration 6/1000 | Loss: 0.00002169
Iteration 7/1000 | Loss: 0.00031901
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001668
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00015569
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00032899
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001503
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001494
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001492
Iteration 47/1000 | Loss: 0.00001492
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001480
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001476
Iteration 104/1000 | Loss: 0.00001476
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001475
Iteration 107/1000 | Loss: 0.00001475
Iteration 108/1000 | Loss: 0.00001475
Iteration 109/1000 | Loss: 0.00001475
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001473
Iteration 129/1000 | Loss: 0.00001473
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001470
Iteration 150/1000 | Loss: 0.00001470
Iteration 151/1000 | Loss: 0.00001470
Iteration 152/1000 | Loss: 0.00001470
Iteration 153/1000 | Loss: 0.00001470
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001469
Iteration 159/1000 | Loss: 0.00001469
Iteration 160/1000 | Loss: 0.00001469
Iteration 161/1000 | Loss: 0.00001469
Iteration 162/1000 | Loss: 0.00001469
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00001469
Iteration 165/1000 | Loss: 0.00001469
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001468
Iteration 168/1000 | Loss: 0.00001468
Iteration 169/1000 | Loss: 0.00001468
Iteration 170/1000 | Loss: 0.00001467
Iteration 171/1000 | Loss: 0.00001467
Iteration 172/1000 | Loss: 0.00001467
Iteration 173/1000 | Loss: 0.00001467
Iteration 174/1000 | Loss: 0.00001466
Iteration 175/1000 | Loss: 0.00001466
Iteration 176/1000 | Loss: 0.00001466
Iteration 177/1000 | Loss: 0.00001466
Iteration 178/1000 | Loss: 0.00001466
Iteration 179/1000 | Loss: 0.00001466
Iteration 180/1000 | Loss: 0.00001466
Iteration 181/1000 | Loss: 0.00001465
Iteration 182/1000 | Loss: 0.00001465
Iteration 183/1000 | Loss: 0.00001465
Iteration 184/1000 | Loss: 0.00001465
Iteration 185/1000 | Loss: 0.00001465
Iteration 186/1000 | Loss: 0.00001465
Iteration 187/1000 | Loss: 0.00001465
Iteration 188/1000 | Loss: 0.00001465
Iteration 189/1000 | Loss: 0.00001465
Iteration 190/1000 | Loss: 0.00001465
Iteration 191/1000 | Loss: 0.00001465
Iteration 192/1000 | Loss: 0.00001465
Iteration 193/1000 | Loss: 0.00001464
Iteration 194/1000 | Loss: 0.00001464
Iteration 195/1000 | Loss: 0.00001464
Iteration 196/1000 | Loss: 0.00001464
Iteration 197/1000 | Loss: 0.00001464
Iteration 198/1000 | Loss: 0.00001464
Iteration 199/1000 | Loss: 0.00001464
Iteration 200/1000 | Loss: 0.00001464
Iteration 201/1000 | Loss: 0.00001464
Iteration 202/1000 | Loss: 0.00001464
Iteration 203/1000 | Loss: 0.00001464
Iteration 204/1000 | Loss: 0.00001464
Iteration 205/1000 | Loss: 0.00001463
Iteration 206/1000 | Loss: 0.00001463
Iteration 207/1000 | Loss: 0.00001463
Iteration 208/1000 | Loss: 0.00001463
Iteration 209/1000 | Loss: 0.00001463
Iteration 210/1000 | Loss: 0.00001463
Iteration 211/1000 | Loss: 0.00001463
Iteration 212/1000 | Loss: 0.00001463
Iteration 213/1000 | Loss: 0.00001463
Iteration 214/1000 | Loss: 0.00001463
Iteration 215/1000 | Loss: 0.00001463
Iteration 216/1000 | Loss: 0.00001463
Iteration 217/1000 | Loss: 0.00001463
Iteration 218/1000 | Loss: 0.00001463
Iteration 219/1000 | Loss: 0.00001463
Iteration 220/1000 | Loss: 0.00001462
Iteration 221/1000 | Loss: 0.00001462
Iteration 222/1000 | Loss: 0.00001462
Iteration 223/1000 | Loss: 0.00001462
Iteration 224/1000 | Loss: 0.00001462
Iteration 225/1000 | Loss: 0.00001462
Iteration 226/1000 | Loss: 0.00001462
Iteration 227/1000 | Loss: 0.00001462
Iteration 228/1000 | Loss: 0.00001462
Iteration 229/1000 | Loss: 0.00001462
Iteration 230/1000 | Loss: 0.00001462
Iteration 231/1000 | Loss: 0.00001462
Iteration 232/1000 | Loss: 0.00001462
Iteration 233/1000 | Loss: 0.00001461
Iteration 234/1000 | Loss: 0.00001461
Iteration 235/1000 | Loss: 0.00001461
Iteration 236/1000 | Loss: 0.00001461
Iteration 237/1000 | Loss: 0.00001461
Iteration 238/1000 | Loss: 0.00001461
Iteration 239/1000 | Loss: 0.00001460
Iteration 240/1000 | Loss: 0.00001460
Iteration 241/1000 | Loss: 0.00001460
Iteration 242/1000 | Loss: 0.00001460
Iteration 243/1000 | Loss: 0.00001460
Iteration 244/1000 | Loss: 0.00001460
Iteration 245/1000 | Loss: 0.00001460
Iteration 246/1000 | Loss: 0.00001460
Iteration 247/1000 | Loss: 0.00001460
Iteration 248/1000 | Loss: 0.00001460
Iteration 249/1000 | Loss: 0.00001460
Iteration 250/1000 | Loss: 0.00001460
Iteration 251/1000 | Loss: 0.00001460
Iteration 252/1000 | Loss: 0.00001460
Iteration 253/1000 | Loss: 0.00001460
Iteration 254/1000 | Loss: 0.00001460
Iteration 255/1000 | Loss: 0.00001460
Iteration 256/1000 | Loss: 0.00001460
Iteration 257/1000 | Loss: 0.00001460
Iteration 258/1000 | Loss: 0.00001460
Iteration 259/1000 | Loss: 0.00001460
Iteration 260/1000 | Loss: 0.00001459
Iteration 261/1000 | Loss: 0.00001459
Iteration 262/1000 | Loss: 0.00001459
Iteration 263/1000 | Loss: 0.00001459
Iteration 264/1000 | Loss: 0.00001459
Iteration 265/1000 | Loss: 0.00001459
Iteration 266/1000 | Loss: 0.00001459
Iteration 267/1000 | Loss: 0.00001459
Iteration 268/1000 | Loss: 0.00001459
Iteration 269/1000 | Loss: 0.00001459
Iteration 270/1000 | Loss: 0.00001459
Iteration 271/1000 | Loss: 0.00001459
Iteration 272/1000 | Loss: 0.00001459
Iteration 273/1000 | Loss: 0.00001459
Iteration 274/1000 | Loss: 0.00001459
Iteration 275/1000 | Loss: 0.00001459
Iteration 276/1000 | Loss: 0.00001459
Iteration 277/1000 | Loss: 0.00001459
Iteration 278/1000 | Loss: 0.00001459
Iteration 279/1000 | Loss: 0.00001459
Iteration 280/1000 | Loss: 0.00001459
Iteration 281/1000 | Loss: 0.00001459
Iteration 282/1000 | Loss: 0.00001459
Iteration 283/1000 | Loss: 0.00001459
Iteration 284/1000 | Loss: 0.00001459
Iteration 285/1000 | Loss: 0.00001459
Iteration 286/1000 | Loss: 0.00001459
Iteration 287/1000 | Loss: 0.00001459
Iteration 288/1000 | Loss: 0.00001459
Iteration 289/1000 | Loss: 0.00001459
Iteration 290/1000 | Loss: 0.00001459
Iteration 291/1000 | Loss: 0.00001459
Iteration 292/1000 | Loss: 0.00001459
Iteration 293/1000 | Loss: 0.00001459
Iteration 294/1000 | Loss: 0.00001459
Iteration 295/1000 | Loss: 0.00001459
Iteration 296/1000 | Loss: 0.00001459
Iteration 297/1000 | Loss: 0.00001459
Iteration 298/1000 | Loss: 0.00001459
Iteration 299/1000 | Loss: 0.00001459
Iteration 300/1000 | Loss: 0.00001459
Iteration 301/1000 | Loss: 0.00001459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.4591279068554286e-05, 1.4591279068554286e-05, 1.4591279068554286e-05, 1.4591279068554286e-05, 1.4591279068554286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4591279068554286e-05

Optimization complete. Final v2v error: 3.157487630844116 mm

Highest mean error: 4.496124744415283 mm for frame 168

Lowest mean error: 2.520854949951172 mm for frame 121

Saving results

Total time: 79.88463282585144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591947
Iteration 2/25 | Loss: 0.00125153
Iteration 3/25 | Loss: 0.00106606
Iteration 4/25 | Loss: 0.00102772
Iteration 5/25 | Loss: 0.00101549
Iteration 6/25 | Loss: 0.00101254
Iteration 7/25 | Loss: 0.00101165
Iteration 8/25 | Loss: 0.00101129
Iteration 9/25 | Loss: 0.00101103
Iteration 10/25 | Loss: 0.00101090
Iteration 11/25 | Loss: 0.00101078
Iteration 12/25 | Loss: 0.00101070
Iteration 13/25 | Loss: 0.00101067
Iteration 14/25 | Loss: 0.00101064
Iteration 15/25 | Loss: 0.00101063
Iteration 16/25 | Loss: 0.00101063
Iteration 17/25 | Loss: 0.00101063
Iteration 18/25 | Loss: 0.00101063
Iteration 19/25 | Loss: 0.00101063
Iteration 20/25 | Loss: 0.00101062
Iteration 21/25 | Loss: 0.00101062
Iteration 22/25 | Loss: 0.00101062
Iteration 23/25 | Loss: 0.00101062
Iteration 24/25 | Loss: 0.00101062
Iteration 25/25 | Loss: 0.00101062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12786412
Iteration 2/25 | Loss: 0.00064010
Iteration 3/25 | Loss: 0.00064009
Iteration 4/25 | Loss: 0.00064008
Iteration 5/25 | Loss: 0.00064008
Iteration 6/25 | Loss: 0.00064008
Iteration 7/25 | Loss: 0.00064008
Iteration 8/25 | Loss: 0.00064008
Iteration 9/25 | Loss: 0.00064008
Iteration 10/25 | Loss: 0.00064008
Iteration 11/25 | Loss: 0.00064008
Iteration 12/25 | Loss: 0.00064008
Iteration 13/25 | Loss: 0.00064008
Iteration 14/25 | Loss: 0.00064008
Iteration 15/25 | Loss: 0.00064008
Iteration 16/25 | Loss: 0.00064008
Iteration 17/25 | Loss: 0.00064008
Iteration 18/25 | Loss: 0.00064008
Iteration 19/25 | Loss: 0.00064008
Iteration 20/25 | Loss: 0.00064008
Iteration 21/25 | Loss: 0.00064008
Iteration 22/25 | Loss: 0.00064008
Iteration 23/25 | Loss: 0.00064008
Iteration 24/25 | Loss: 0.00064008
Iteration 25/25 | Loss: 0.00064008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064008
Iteration 2/1000 | Loss: 0.00004757
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002881
Iteration 5/1000 | Loss: 0.00002731
Iteration 6/1000 | Loss: 0.00008454
Iteration 7/1000 | Loss: 0.00002645
Iteration 8/1000 | Loss: 0.00002484
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002230
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002172
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002128
Iteration 19/1000 | Loss: 0.00002128
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002125
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002123
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002116
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002116
Iteration 37/1000 | Loss: 0.00002116
Iteration 38/1000 | Loss: 0.00002116
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002115
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002115
Iteration 45/1000 | Loss: 0.00002115
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002115
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002114
Iteration 50/1000 | Loss: 0.00002113
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002112
Iteration 53/1000 | Loss: 0.00002112
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002112
Iteration 57/1000 | Loss: 0.00002112
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002111
Iteration 60/1000 | Loss: 0.00002111
Iteration 61/1000 | Loss: 0.00002111
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002110
Iteration 66/1000 | Loss: 0.00002109
Iteration 67/1000 | Loss: 0.00002109
Iteration 68/1000 | Loss: 0.00002109
Iteration 69/1000 | Loss: 0.00002109
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002108
Iteration 72/1000 | Loss: 0.00002108
Iteration 73/1000 | Loss: 0.00002108
Iteration 74/1000 | Loss: 0.00002108
Iteration 75/1000 | Loss: 0.00002108
Iteration 76/1000 | Loss: 0.00002108
Iteration 77/1000 | Loss: 0.00002108
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002107
Iteration 82/1000 | Loss: 0.00002107
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002107
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002106
Iteration 99/1000 | Loss: 0.00002106
Iteration 100/1000 | Loss: 0.00002106
Iteration 101/1000 | Loss: 0.00002106
Iteration 102/1000 | Loss: 0.00002105
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002105
Iteration 105/1000 | Loss: 0.00002105
Iteration 106/1000 | Loss: 0.00002105
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002105
Iteration 110/1000 | Loss: 0.00002105
Iteration 111/1000 | Loss: 0.00002105
Iteration 112/1000 | Loss: 0.00002105
Iteration 113/1000 | Loss: 0.00002105
Iteration 114/1000 | Loss: 0.00002105
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002105
Iteration 120/1000 | Loss: 0.00002105
Iteration 121/1000 | Loss: 0.00002105
Iteration 122/1000 | Loss: 0.00002105
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002105
Iteration 127/1000 | Loss: 0.00002105
Iteration 128/1000 | Loss: 0.00002105
Iteration 129/1000 | Loss: 0.00002105
Iteration 130/1000 | Loss: 0.00002105
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002105
Iteration 136/1000 | Loss: 0.00002105
Iteration 137/1000 | Loss: 0.00002105
Iteration 138/1000 | Loss: 0.00002105
Iteration 139/1000 | Loss: 0.00002105
Iteration 140/1000 | Loss: 0.00002105
Iteration 141/1000 | Loss: 0.00002105
Iteration 142/1000 | Loss: 0.00002105
Iteration 143/1000 | Loss: 0.00002105
Iteration 144/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.1048243070254102e-05, 2.1048243070254102e-05, 2.1048243070254102e-05, 2.1048243070254102e-05, 2.1048243070254102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1048243070254102e-05

Optimization complete. Final v2v error: 3.7150135040283203 mm

Highest mean error: 6.2466912269592285 mm for frame 114

Lowest mean error: 3.0761969089508057 mm for frame 136

Saving results

Total time: 52.633209466934204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910440
Iteration 2/25 | Loss: 0.00107649
Iteration 3/25 | Loss: 0.00098375
Iteration 4/25 | Loss: 0.00096674
Iteration 5/25 | Loss: 0.00096084
Iteration 6/25 | Loss: 0.00095940
Iteration 7/25 | Loss: 0.00095935
Iteration 8/25 | Loss: 0.00095935
Iteration 9/25 | Loss: 0.00095935
Iteration 10/25 | Loss: 0.00095935
Iteration 11/25 | Loss: 0.00095935
Iteration 12/25 | Loss: 0.00095935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009593490394763649, 0.0009593490394763649, 0.0009593490394763649, 0.0009593490394763649, 0.0009593490394763649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009593490394763649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30375433
Iteration 2/25 | Loss: 0.00076177
Iteration 3/25 | Loss: 0.00076175
Iteration 4/25 | Loss: 0.00076175
Iteration 5/25 | Loss: 0.00076175
Iteration 6/25 | Loss: 0.00076175
Iteration 7/25 | Loss: 0.00076175
Iteration 8/25 | Loss: 0.00076175
Iteration 9/25 | Loss: 0.00076175
Iteration 10/25 | Loss: 0.00076175
Iteration 11/25 | Loss: 0.00076175
Iteration 12/25 | Loss: 0.00076175
Iteration 13/25 | Loss: 0.00076175
Iteration 14/25 | Loss: 0.00076175
Iteration 15/25 | Loss: 0.00076175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007617502706125379, 0.0007617502706125379, 0.0007617502706125379, 0.0007617502706125379, 0.0007617502706125379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007617502706125379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076175
Iteration 2/1000 | Loss: 0.00003882
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001309
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001306
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001305
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001296
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001295
Iteration 64/1000 | Loss: 0.00001295
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001294
Iteration 67/1000 | Loss: 0.00001294
Iteration 68/1000 | Loss: 0.00001294
Iteration 69/1000 | Loss: 0.00001294
Iteration 70/1000 | Loss: 0.00001294
Iteration 71/1000 | Loss: 0.00001294
Iteration 72/1000 | Loss: 0.00001293
Iteration 73/1000 | Loss: 0.00001293
Iteration 74/1000 | Loss: 0.00001293
Iteration 75/1000 | Loss: 0.00001293
Iteration 76/1000 | Loss: 0.00001293
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001291
Iteration 87/1000 | Loss: 0.00001290
Iteration 88/1000 | Loss: 0.00001290
Iteration 89/1000 | Loss: 0.00001290
Iteration 90/1000 | Loss: 0.00001290
Iteration 91/1000 | Loss: 0.00001290
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001290
Iteration 94/1000 | Loss: 0.00001290
Iteration 95/1000 | Loss: 0.00001290
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001290
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001289
Iteration 120/1000 | Loss: 0.00001289
Iteration 121/1000 | Loss: 0.00001289
Iteration 122/1000 | Loss: 0.00001289
Iteration 123/1000 | Loss: 0.00001289
Iteration 124/1000 | Loss: 0.00001288
Iteration 125/1000 | Loss: 0.00001288
Iteration 126/1000 | Loss: 0.00001288
Iteration 127/1000 | Loss: 0.00001288
Iteration 128/1000 | Loss: 0.00001288
Iteration 129/1000 | Loss: 0.00001288
Iteration 130/1000 | Loss: 0.00001288
Iteration 131/1000 | Loss: 0.00001288
Iteration 132/1000 | Loss: 0.00001288
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001288
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001288
Iteration 138/1000 | Loss: 0.00001288
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2877424524049275e-05, 1.2877424524049275e-05, 1.2877424524049275e-05, 1.2877424524049275e-05, 1.2877424524049275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2877424524049275e-05

Optimization complete. Final v2v error: 2.909294605255127 mm

Highest mean error: 4.092990875244141 mm for frame 39

Lowest mean error: 2.321769952774048 mm for frame 68

Saving results

Total time: 34.93232488632202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483790
Iteration 2/25 | Loss: 0.00107840
Iteration 3/25 | Loss: 0.00097967
Iteration 4/25 | Loss: 0.00097216
Iteration 5/25 | Loss: 0.00096958
Iteration 6/25 | Loss: 0.00096887
Iteration 7/25 | Loss: 0.00096887
Iteration 8/25 | Loss: 0.00096887
Iteration 9/25 | Loss: 0.00096887
Iteration 10/25 | Loss: 0.00096887
Iteration 11/25 | Loss: 0.00096887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009688716963864863, 0.0009688716963864863, 0.0009688716963864863, 0.0009688716963864863, 0.0009688716963864863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009688716963864863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83565176
Iteration 2/25 | Loss: 0.00054471
Iteration 3/25 | Loss: 0.00054470
Iteration 4/25 | Loss: 0.00054470
Iteration 5/25 | Loss: 0.00054470
Iteration 6/25 | Loss: 0.00054470
Iteration 7/25 | Loss: 0.00054470
Iteration 8/25 | Loss: 0.00054470
Iteration 9/25 | Loss: 0.00054470
Iteration 10/25 | Loss: 0.00054470
Iteration 11/25 | Loss: 0.00054470
Iteration 12/25 | Loss: 0.00054470
Iteration 13/25 | Loss: 0.00054470
Iteration 14/25 | Loss: 0.00054470
Iteration 15/25 | Loss: 0.00054470
Iteration 16/25 | Loss: 0.00054470
Iteration 17/25 | Loss: 0.00054470
Iteration 18/25 | Loss: 0.00054470
Iteration 19/25 | Loss: 0.00054470
Iteration 20/25 | Loss: 0.00054470
Iteration 21/25 | Loss: 0.00054470
Iteration 22/25 | Loss: 0.00054470
Iteration 23/25 | Loss: 0.00054470
Iteration 24/25 | Loss: 0.00054470
Iteration 25/25 | Loss: 0.00054470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054470
Iteration 2/1000 | Loss: 0.00002655
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001641
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001557
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001533
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001532
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001519
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001499
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001488
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001463
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001445
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00001437
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001434
Iteration 148/1000 | Loss: 0.00001434
Iteration 149/1000 | Loss: 0.00001434
Iteration 150/1000 | Loss: 0.00001433
Iteration 151/1000 | Loss: 0.00001433
Iteration 152/1000 | Loss: 0.00001433
Iteration 153/1000 | Loss: 0.00001433
Iteration 154/1000 | Loss: 0.00001433
Iteration 155/1000 | Loss: 0.00001433
Iteration 156/1000 | Loss: 0.00001433
Iteration 157/1000 | Loss: 0.00001433
Iteration 158/1000 | Loss: 0.00001433
Iteration 159/1000 | Loss: 0.00001433
Iteration 160/1000 | Loss: 0.00001433
Iteration 161/1000 | Loss: 0.00001433
Iteration 162/1000 | Loss: 0.00001433
Iteration 163/1000 | Loss: 0.00001433
Iteration 164/1000 | Loss: 0.00001433
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Iteration 173/1000 | Loss: 0.00001433
Iteration 174/1000 | Loss: 0.00001433
Iteration 175/1000 | Loss: 0.00001433
Iteration 176/1000 | Loss: 0.00001433
Iteration 177/1000 | Loss: 0.00001433
Iteration 178/1000 | Loss: 0.00001433
Iteration 179/1000 | Loss: 0.00001433
Iteration 180/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.4330847079691011e-05, 1.4330847079691011e-05, 1.4330847079691011e-05, 1.4330847079691011e-05, 1.4330847079691011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4330847079691011e-05

Optimization complete. Final v2v error: 3.191779136657715 mm

Highest mean error: 3.519103527069092 mm for frame 0

Lowest mean error: 3.0734286308288574 mm for frame 187

Saving results

Total time: 50.8344943523407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917917
Iteration 2/25 | Loss: 0.00157476
Iteration 3/25 | Loss: 0.00123109
Iteration 4/25 | Loss: 0.00116213
Iteration 5/25 | Loss: 0.00119268
Iteration 6/25 | Loss: 0.00106609
Iteration 7/25 | Loss: 0.00104139
Iteration 8/25 | Loss: 0.00105793
Iteration 9/25 | Loss: 0.00104877
Iteration 10/25 | Loss: 0.00101862
Iteration 11/25 | Loss: 0.00100376
Iteration 12/25 | Loss: 0.00099762
Iteration 13/25 | Loss: 0.00099591
Iteration 14/25 | Loss: 0.00100056
Iteration 15/25 | Loss: 0.00099920
Iteration 16/25 | Loss: 0.00099619
Iteration 17/25 | Loss: 0.00099126
Iteration 18/25 | Loss: 0.00099010
Iteration 19/25 | Loss: 0.00098993
Iteration 20/25 | Loss: 0.00098990
Iteration 21/25 | Loss: 0.00098990
Iteration 22/25 | Loss: 0.00098989
Iteration 23/25 | Loss: 0.00098989
Iteration 24/25 | Loss: 0.00098989
Iteration 25/25 | Loss: 0.00098989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.49495888
Iteration 2/25 | Loss: 0.00055846
Iteration 3/25 | Loss: 0.00055846
Iteration 4/25 | Loss: 0.00055846
Iteration 5/25 | Loss: 0.00055846
Iteration 6/25 | Loss: 0.00055845
Iteration 7/25 | Loss: 0.00055845
Iteration 8/25 | Loss: 0.00055845
Iteration 9/25 | Loss: 0.00055845
Iteration 10/25 | Loss: 0.00055845
Iteration 11/25 | Loss: 0.00055845
Iteration 12/25 | Loss: 0.00055845
Iteration 13/25 | Loss: 0.00055845
Iteration 14/25 | Loss: 0.00055845
Iteration 15/25 | Loss: 0.00055845
Iteration 16/25 | Loss: 0.00055845
Iteration 17/25 | Loss: 0.00055845
Iteration 18/25 | Loss: 0.00055845
Iteration 19/25 | Loss: 0.00055845
Iteration 20/25 | Loss: 0.00055845
Iteration 21/25 | Loss: 0.00055845
Iteration 22/25 | Loss: 0.00055845
Iteration 23/25 | Loss: 0.00055845
Iteration 24/25 | Loss: 0.00055845
Iteration 25/25 | Loss: 0.00055845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055845
Iteration 2/1000 | Loss: 0.00004073
Iteration 3/1000 | Loss: 0.00015460
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001492
Iteration 11/1000 | Loss: 0.00001491
Iteration 12/1000 | Loss: 0.00007089
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001463
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00006289
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00003454
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001438
Iteration 96/1000 | Loss: 0.00001438
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001438
Iteration 103/1000 | Loss: 0.00001438
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001438
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001438
Iteration 108/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.4378938431036659e-05, 1.4378938431036659e-05, 1.4378938431036659e-05, 1.4378938431036659e-05, 1.4378938431036659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4378938431036659e-05

Optimization complete. Final v2v error: 3.101219654083252 mm

Highest mean error: 3.8599400520324707 mm for frame 166

Lowest mean error: 2.5664772987365723 mm for frame 137

Saving results

Total time: 65.0719347000122
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060494
Iteration 2/25 | Loss: 0.00260669
Iteration 3/25 | Loss: 0.00162176
Iteration 4/25 | Loss: 0.00131649
Iteration 5/25 | Loss: 0.00126592
Iteration 6/25 | Loss: 0.00124070
Iteration 7/25 | Loss: 0.00126716
Iteration 8/25 | Loss: 0.00118666
Iteration 9/25 | Loss: 0.00114754
Iteration 10/25 | Loss: 0.00109429
Iteration 11/25 | Loss: 0.00107495
Iteration 12/25 | Loss: 0.00104879
Iteration 13/25 | Loss: 0.00104000
Iteration 14/25 | Loss: 0.00103351
Iteration 15/25 | Loss: 0.00102558
Iteration 16/25 | Loss: 0.00101889
Iteration 17/25 | Loss: 0.00100055
Iteration 18/25 | Loss: 0.00099462
Iteration 19/25 | Loss: 0.00099062
Iteration 20/25 | Loss: 0.00099489
Iteration 21/25 | Loss: 0.00099283
Iteration 22/25 | Loss: 0.00098341
Iteration 23/25 | Loss: 0.00097693
Iteration 24/25 | Loss: 0.00097220
Iteration 25/25 | Loss: 0.00096995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02241898
Iteration 2/25 | Loss: 0.00057284
Iteration 3/25 | Loss: 0.00057284
Iteration 4/25 | Loss: 0.00057284
Iteration 5/25 | Loss: 0.00057284
Iteration 6/25 | Loss: 0.00057284
Iteration 7/25 | Loss: 0.00057284
Iteration 8/25 | Loss: 0.00057284
Iteration 9/25 | Loss: 0.00057284
Iteration 10/25 | Loss: 0.00057284
Iteration 11/25 | Loss: 0.00057284
Iteration 12/25 | Loss: 0.00057284
Iteration 13/25 | Loss: 0.00057284
Iteration 14/25 | Loss: 0.00057284
Iteration 15/25 | Loss: 0.00057284
Iteration 16/25 | Loss: 0.00057284
Iteration 17/25 | Loss: 0.00057284
Iteration 18/25 | Loss: 0.00057284
Iteration 19/25 | Loss: 0.00057284
Iteration 20/25 | Loss: 0.00057284
Iteration 21/25 | Loss: 0.00057284
Iteration 22/25 | Loss: 0.00057284
Iteration 23/25 | Loss: 0.00057284
Iteration 24/25 | Loss: 0.00057284
Iteration 25/25 | Loss: 0.00057284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005728363175876439, 0.0005728363175876439, 0.0005728363175876439, 0.0005728363175876439, 0.0005728363175876439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005728363175876439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057284
Iteration 2/1000 | Loss: 0.00010757
Iteration 3/1000 | Loss: 0.00002545
Iteration 4/1000 | Loss: 0.00009491
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00011548
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00017348
Iteration 12/1000 | Loss: 0.00012456
Iteration 13/1000 | Loss: 0.00003412
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001292
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001283
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001279
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001279
Iteration 79/1000 | Loss: 0.00001279
Iteration 80/1000 | Loss: 0.00001279
Iteration 81/1000 | Loss: 0.00001279
Iteration 82/1000 | Loss: 0.00001279
Iteration 83/1000 | Loss: 0.00001279
Iteration 84/1000 | Loss: 0.00001279
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001279
Iteration 87/1000 | Loss: 0.00001279
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.2788902495231014e-05, 1.2788902495231014e-05, 1.2788902495231014e-05, 1.2788902495231014e-05, 1.2788902495231014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2788902495231014e-05

Optimization complete. Final v2v error: 2.9577090740203857 mm

Highest mean error: 8.69192123413086 mm for frame 77

Lowest mean error: 2.5695223808288574 mm for frame 72

Saving results

Total time: 78.37973380088806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067950
Iteration 2/25 | Loss: 0.00236323
Iteration 3/25 | Loss: 0.00173446
Iteration 4/25 | Loss: 0.00176917
Iteration 5/25 | Loss: 0.00125219
Iteration 6/25 | Loss: 0.00107407
Iteration 7/25 | Loss: 0.00105465
Iteration 8/25 | Loss: 0.00109591
Iteration 9/25 | Loss: 0.00106503
Iteration 10/25 | Loss: 0.00099558
Iteration 11/25 | Loss: 0.00097495
Iteration 12/25 | Loss: 0.00097399
Iteration 13/25 | Loss: 0.00097220
Iteration 14/25 | Loss: 0.00096784
Iteration 15/25 | Loss: 0.00096685
Iteration 16/25 | Loss: 0.00096665
Iteration 17/25 | Loss: 0.00096657
Iteration 18/25 | Loss: 0.00096657
Iteration 19/25 | Loss: 0.00096657
Iteration 20/25 | Loss: 0.00096657
Iteration 21/25 | Loss: 0.00096657
Iteration 22/25 | Loss: 0.00096657
Iteration 23/25 | Loss: 0.00096657
Iteration 24/25 | Loss: 0.00096656
Iteration 25/25 | Loss: 0.00096656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29825354
Iteration 2/25 | Loss: 0.00041929
Iteration 3/25 | Loss: 0.00041929
Iteration 4/25 | Loss: 0.00041929
Iteration 5/25 | Loss: 0.00041929
Iteration 6/25 | Loss: 0.00041929
Iteration 7/25 | Loss: 0.00041929
Iteration 8/25 | Loss: 0.00041929
Iteration 9/25 | Loss: 0.00041929
Iteration 10/25 | Loss: 0.00041929
Iteration 11/25 | Loss: 0.00041929
Iteration 12/25 | Loss: 0.00041929
Iteration 13/25 | Loss: 0.00041929
Iteration 14/25 | Loss: 0.00041929
Iteration 15/25 | Loss: 0.00041929
Iteration 16/25 | Loss: 0.00041929
Iteration 17/25 | Loss: 0.00041929
Iteration 18/25 | Loss: 0.00041929
Iteration 19/25 | Loss: 0.00041929
Iteration 20/25 | Loss: 0.00041929
Iteration 21/25 | Loss: 0.00041929
Iteration 22/25 | Loss: 0.00041929
Iteration 23/25 | Loss: 0.00041929
Iteration 24/25 | Loss: 0.00041929
Iteration 25/25 | Loss: 0.00041929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041929
Iteration 2/1000 | Loss: 0.00002210
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001442
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001219
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001196
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00021271
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001180
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001158
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001157
Iteration 108/1000 | Loss: 0.00001157
Iteration 109/1000 | Loss: 0.00001157
Iteration 110/1000 | Loss: 0.00001157
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001156
Iteration 113/1000 | Loss: 0.00001156
Iteration 114/1000 | Loss: 0.00001156
Iteration 115/1000 | Loss: 0.00001156
Iteration 116/1000 | Loss: 0.00001156
Iteration 117/1000 | Loss: 0.00001156
Iteration 118/1000 | Loss: 0.00001156
Iteration 119/1000 | Loss: 0.00001155
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001154
Iteration 128/1000 | Loss: 0.00001154
Iteration 129/1000 | Loss: 0.00001154
Iteration 130/1000 | Loss: 0.00001154
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001154
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001154
Iteration 139/1000 | Loss: 0.00001154
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001154
Iteration 143/1000 | Loss: 0.00001154
Iteration 144/1000 | Loss: 0.00001154
Iteration 145/1000 | Loss: 0.00001154
Iteration 146/1000 | Loss: 0.00001154
Iteration 147/1000 | Loss: 0.00001154
Iteration 148/1000 | Loss: 0.00001154
Iteration 149/1000 | Loss: 0.00001154
Iteration 150/1000 | Loss: 0.00001154
Iteration 151/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1541730600583833e-05, 1.1541730600583833e-05, 1.1541730600583833e-05, 1.1541730600583833e-05, 1.1541730600583833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1541730600583833e-05

Optimization complete. Final v2v error: 2.798513412475586 mm

Highest mean error: 4.123612403869629 mm for frame 92

Lowest mean error: 2.6911368370056152 mm for frame 188

Saving results

Total time: 70.84110188484192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776988
Iteration 2/25 | Loss: 0.00166328
Iteration 3/25 | Loss: 0.00115300
Iteration 4/25 | Loss: 0.00109106
Iteration 5/25 | Loss: 0.00108141
Iteration 6/25 | Loss: 0.00107885
Iteration 7/25 | Loss: 0.00107797
Iteration 8/25 | Loss: 0.00107785
Iteration 9/25 | Loss: 0.00107785
Iteration 10/25 | Loss: 0.00107785
Iteration 11/25 | Loss: 0.00107785
Iteration 12/25 | Loss: 0.00107785
Iteration 13/25 | Loss: 0.00107785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010778503492474556, 0.0010778503492474556, 0.0010778503492474556, 0.0010778503492474556, 0.0010778503492474556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010778503492474556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54426950
Iteration 2/25 | Loss: 0.00059971
Iteration 3/25 | Loss: 0.00059970
Iteration 4/25 | Loss: 0.00059970
Iteration 5/25 | Loss: 0.00059970
Iteration 6/25 | Loss: 0.00059970
Iteration 7/25 | Loss: 0.00059970
Iteration 8/25 | Loss: 0.00059970
Iteration 9/25 | Loss: 0.00059970
Iteration 10/25 | Loss: 0.00059970
Iteration 11/25 | Loss: 0.00059970
Iteration 12/25 | Loss: 0.00059970
Iteration 13/25 | Loss: 0.00059970
Iteration 14/25 | Loss: 0.00059970
Iteration 15/25 | Loss: 0.00059970
Iteration 16/25 | Loss: 0.00059970
Iteration 17/25 | Loss: 0.00059970
Iteration 18/25 | Loss: 0.00059970
Iteration 19/25 | Loss: 0.00059970
Iteration 20/25 | Loss: 0.00059970
Iteration 21/25 | Loss: 0.00059970
Iteration 22/25 | Loss: 0.00059970
Iteration 23/25 | Loss: 0.00059970
Iteration 24/25 | Loss: 0.00059970
Iteration 25/25 | Loss: 0.00059970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059970
Iteration 2/1000 | Loss: 0.00005492
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00002993
Iteration 5/1000 | Loss: 0.00002752
Iteration 6/1000 | Loss: 0.00002634
Iteration 7/1000 | Loss: 0.00002554
Iteration 8/1000 | Loss: 0.00002494
Iteration 9/1000 | Loss: 0.00002462
Iteration 10/1000 | Loss: 0.00002436
Iteration 11/1000 | Loss: 0.00002418
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002366
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002346
Iteration 19/1000 | Loss: 0.00002341
Iteration 20/1000 | Loss: 0.00002336
Iteration 21/1000 | Loss: 0.00002336
Iteration 22/1000 | Loss: 0.00002331
Iteration 23/1000 | Loss: 0.00002330
Iteration 24/1000 | Loss: 0.00002330
Iteration 25/1000 | Loss: 0.00002326
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00002324
Iteration 29/1000 | Loss: 0.00002324
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002322
Iteration 32/1000 | Loss: 0.00002322
Iteration 33/1000 | Loss: 0.00002322
Iteration 34/1000 | Loss: 0.00002322
Iteration 35/1000 | Loss: 0.00002322
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002322
Iteration 38/1000 | Loss: 0.00002322
Iteration 39/1000 | Loss: 0.00002322
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002320
Iteration 42/1000 | Loss: 0.00002320
Iteration 43/1000 | Loss: 0.00002320
Iteration 44/1000 | Loss: 0.00002320
Iteration 45/1000 | Loss: 0.00002320
Iteration 46/1000 | Loss: 0.00002320
Iteration 47/1000 | Loss: 0.00002320
Iteration 48/1000 | Loss: 0.00002320
Iteration 49/1000 | Loss: 0.00002319
Iteration 50/1000 | Loss: 0.00002319
Iteration 51/1000 | Loss: 0.00002319
Iteration 52/1000 | Loss: 0.00002319
Iteration 53/1000 | Loss: 0.00002319
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002319
Iteration 58/1000 | Loss: 0.00002318
Iteration 59/1000 | Loss: 0.00002318
Iteration 60/1000 | Loss: 0.00002318
Iteration 61/1000 | Loss: 0.00002317
Iteration 62/1000 | Loss: 0.00002317
Iteration 63/1000 | Loss: 0.00002317
Iteration 64/1000 | Loss: 0.00002316
Iteration 65/1000 | Loss: 0.00002316
Iteration 66/1000 | Loss: 0.00002316
Iteration 67/1000 | Loss: 0.00002316
Iteration 68/1000 | Loss: 0.00002316
Iteration 69/1000 | Loss: 0.00002316
Iteration 70/1000 | Loss: 0.00002316
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002316
Iteration 76/1000 | Loss: 0.00002316
Iteration 77/1000 | Loss: 0.00002316
Iteration 78/1000 | Loss: 0.00002315
Iteration 79/1000 | Loss: 0.00002315
Iteration 80/1000 | Loss: 0.00002314
Iteration 81/1000 | Loss: 0.00002314
Iteration 82/1000 | Loss: 0.00002314
Iteration 83/1000 | Loss: 0.00002314
Iteration 84/1000 | Loss: 0.00002314
Iteration 85/1000 | Loss: 0.00002314
Iteration 86/1000 | Loss: 0.00002314
Iteration 87/1000 | Loss: 0.00002313
Iteration 88/1000 | Loss: 0.00002313
Iteration 89/1000 | Loss: 0.00002313
Iteration 90/1000 | Loss: 0.00002313
Iteration 91/1000 | Loss: 0.00002312
Iteration 92/1000 | Loss: 0.00002312
Iteration 93/1000 | Loss: 0.00002312
Iteration 94/1000 | Loss: 0.00002312
Iteration 95/1000 | Loss: 0.00002312
Iteration 96/1000 | Loss: 0.00002312
Iteration 97/1000 | Loss: 0.00002312
Iteration 98/1000 | Loss: 0.00002312
Iteration 99/1000 | Loss: 0.00002312
Iteration 100/1000 | Loss: 0.00002312
Iteration 101/1000 | Loss: 0.00002312
Iteration 102/1000 | Loss: 0.00002311
Iteration 103/1000 | Loss: 0.00002311
Iteration 104/1000 | Loss: 0.00002311
Iteration 105/1000 | Loss: 0.00002311
Iteration 106/1000 | Loss: 0.00002310
Iteration 107/1000 | Loss: 0.00002310
Iteration 108/1000 | Loss: 0.00002310
Iteration 109/1000 | Loss: 0.00002310
Iteration 110/1000 | Loss: 0.00002309
Iteration 111/1000 | Loss: 0.00002309
Iteration 112/1000 | Loss: 0.00002309
Iteration 113/1000 | Loss: 0.00002309
Iteration 114/1000 | Loss: 0.00002309
Iteration 115/1000 | Loss: 0.00002308
Iteration 116/1000 | Loss: 0.00002308
Iteration 117/1000 | Loss: 0.00002308
Iteration 118/1000 | Loss: 0.00002308
Iteration 119/1000 | Loss: 0.00002308
Iteration 120/1000 | Loss: 0.00002308
Iteration 121/1000 | Loss: 0.00002307
Iteration 122/1000 | Loss: 0.00002307
Iteration 123/1000 | Loss: 0.00002307
Iteration 124/1000 | Loss: 0.00002307
Iteration 125/1000 | Loss: 0.00002307
Iteration 126/1000 | Loss: 0.00002307
Iteration 127/1000 | Loss: 0.00002307
Iteration 128/1000 | Loss: 0.00002307
Iteration 129/1000 | Loss: 0.00002307
Iteration 130/1000 | Loss: 0.00002306
Iteration 131/1000 | Loss: 0.00002306
Iteration 132/1000 | Loss: 0.00002306
Iteration 133/1000 | Loss: 0.00002306
Iteration 134/1000 | Loss: 0.00002306
Iteration 135/1000 | Loss: 0.00002306
Iteration 136/1000 | Loss: 0.00002306
Iteration 137/1000 | Loss: 0.00002305
Iteration 138/1000 | Loss: 0.00002305
Iteration 139/1000 | Loss: 0.00002305
Iteration 140/1000 | Loss: 0.00002305
Iteration 141/1000 | Loss: 0.00002304
Iteration 142/1000 | Loss: 0.00002304
Iteration 143/1000 | Loss: 0.00002304
Iteration 144/1000 | Loss: 0.00002304
Iteration 145/1000 | Loss: 0.00002304
Iteration 146/1000 | Loss: 0.00002304
Iteration 147/1000 | Loss: 0.00002304
Iteration 148/1000 | Loss: 0.00002303
Iteration 149/1000 | Loss: 0.00002303
Iteration 150/1000 | Loss: 0.00002303
Iteration 151/1000 | Loss: 0.00002303
Iteration 152/1000 | Loss: 0.00002303
Iteration 153/1000 | Loss: 0.00002303
Iteration 154/1000 | Loss: 0.00002303
Iteration 155/1000 | Loss: 0.00002302
Iteration 156/1000 | Loss: 0.00002302
Iteration 157/1000 | Loss: 0.00002302
Iteration 158/1000 | Loss: 0.00002302
Iteration 159/1000 | Loss: 0.00002302
Iteration 160/1000 | Loss: 0.00002302
Iteration 161/1000 | Loss: 0.00002302
Iteration 162/1000 | Loss: 0.00002302
Iteration 163/1000 | Loss: 0.00002302
Iteration 164/1000 | Loss: 0.00002302
Iteration 165/1000 | Loss: 0.00002301
Iteration 166/1000 | Loss: 0.00002301
Iteration 167/1000 | Loss: 0.00002301
Iteration 168/1000 | Loss: 0.00002301
Iteration 169/1000 | Loss: 0.00002301
Iteration 170/1000 | Loss: 0.00002301
Iteration 171/1000 | Loss: 0.00002301
Iteration 172/1000 | Loss: 0.00002301
Iteration 173/1000 | Loss: 0.00002301
Iteration 174/1000 | Loss: 0.00002301
Iteration 175/1000 | Loss: 0.00002301
Iteration 176/1000 | Loss: 0.00002300
Iteration 177/1000 | Loss: 0.00002300
Iteration 178/1000 | Loss: 0.00002300
Iteration 179/1000 | Loss: 0.00002300
Iteration 180/1000 | Loss: 0.00002300
Iteration 181/1000 | Loss: 0.00002300
Iteration 182/1000 | Loss: 0.00002300
Iteration 183/1000 | Loss: 0.00002300
Iteration 184/1000 | Loss: 0.00002300
Iteration 185/1000 | Loss: 0.00002300
Iteration 186/1000 | Loss: 0.00002300
Iteration 187/1000 | Loss: 0.00002300
Iteration 188/1000 | Loss: 0.00002300
Iteration 189/1000 | Loss: 0.00002300
Iteration 190/1000 | Loss: 0.00002300
Iteration 191/1000 | Loss: 0.00002299
Iteration 192/1000 | Loss: 0.00002299
Iteration 193/1000 | Loss: 0.00002299
Iteration 194/1000 | Loss: 0.00002299
Iteration 195/1000 | Loss: 0.00002299
Iteration 196/1000 | Loss: 0.00002299
Iteration 197/1000 | Loss: 0.00002299
Iteration 198/1000 | Loss: 0.00002299
Iteration 199/1000 | Loss: 0.00002299
Iteration 200/1000 | Loss: 0.00002299
Iteration 201/1000 | Loss: 0.00002299
Iteration 202/1000 | Loss: 0.00002299
Iteration 203/1000 | Loss: 0.00002299
Iteration 204/1000 | Loss: 0.00002299
Iteration 205/1000 | Loss: 0.00002299
Iteration 206/1000 | Loss: 0.00002299
Iteration 207/1000 | Loss: 0.00002299
Iteration 208/1000 | Loss: 0.00002299
Iteration 209/1000 | Loss: 0.00002299
Iteration 210/1000 | Loss: 0.00002299
Iteration 211/1000 | Loss: 0.00002299
Iteration 212/1000 | Loss: 0.00002299
Iteration 213/1000 | Loss: 0.00002299
Iteration 214/1000 | Loss: 0.00002299
Iteration 215/1000 | Loss: 0.00002299
Iteration 216/1000 | Loss: 0.00002299
Iteration 217/1000 | Loss: 0.00002299
Iteration 218/1000 | Loss: 0.00002299
Iteration 219/1000 | Loss: 0.00002299
Iteration 220/1000 | Loss: 0.00002299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.299122752447147e-05, 2.299122752447147e-05, 2.299122752447147e-05, 2.299122752447147e-05, 2.299122752447147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.299122752447147e-05

Optimization complete. Final v2v error: 3.901289224624634 mm

Highest mean error: 5.668191909790039 mm for frame 34

Lowest mean error: 3.0246760845184326 mm for frame 0

Saving results

Total time: 48.4916045665741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968178
Iteration 2/25 | Loss: 0.00153118
Iteration 3/25 | Loss: 0.00113080
Iteration 4/25 | Loss: 0.00108953
Iteration 5/25 | Loss: 0.00107743
Iteration 6/25 | Loss: 0.00107226
Iteration 7/25 | Loss: 0.00107701
Iteration 8/25 | Loss: 0.00105397
Iteration 9/25 | Loss: 0.00104746
Iteration 10/25 | Loss: 0.00104103
Iteration 11/25 | Loss: 0.00103554
Iteration 12/25 | Loss: 0.00103735
Iteration 13/25 | Loss: 0.00103505
Iteration 14/25 | Loss: 0.00103555
Iteration 15/25 | Loss: 0.00103313
Iteration 16/25 | Loss: 0.00103183
Iteration 17/25 | Loss: 0.00103103
Iteration 18/25 | Loss: 0.00103029
Iteration 19/25 | Loss: 0.00102947
Iteration 20/25 | Loss: 0.00102904
Iteration 21/25 | Loss: 0.00102895
Iteration 22/25 | Loss: 0.00102895
Iteration 23/25 | Loss: 0.00102895
Iteration 24/25 | Loss: 0.00102894
Iteration 25/25 | Loss: 0.00102894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78541601
Iteration 2/25 | Loss: 0.00075336
Iteration 3/25 | Loss: 0.00075335
Iteration 4/25 | Loss: 0.00075335
Iteration 5/25 | Loss: 0.00075335
Iteration 6/25 | Loss: 0.00075335
Iteration 7/25 | Loss: 0.00075335
Iteration 8/25 | Loss: 0.00075335
Iteration 9/25 | Loss: 0.00075335
Iteration 10/25 | Loss: 0.00075335
Iteration 11/25 | Loss: 0.00075335
Iteration 12/25 | Loss: 0.00075335
Iteration 13/25 | Loss: 0.00075335
Iteration 14/25 | Loss: 0.00075335
Iteration 15/25 | Loss: 0.00075335
Iteration 16/25 | Loss: 0.00075335
Iteration 17/25 | Loss: 0.00075335
Iteration 18/25 | Loss: 0.00075335
Iteration 19/25 | Loss: 0.00075335
Iteration 20/25 | Loss: 0.00075335
Iteration 21/25 | Loss: 0.00075335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007533487514592707, 0.0007533487514592707, 0.0007533487514592707, 0.0007533487514592707, 0.0007533487514592707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007533487514592707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075335
Iteration 2/1000 | Loss: 0.00009632
Iteration 3/1000 | Loss: 0.00007149
Iteration 4/1000 | Loss: 0.00006029
Iteration 5/1000 | Loss: 0.00005545
Iteration 6/1000 | Loss: 0.00005170
Iteration 7/1000 | Loss: 0.00051685
Iteration 8/1000 | Loss: 0.00004793
Iteration 9/1000 | Loss: 0.00004401
Iteration 10/1000 | Loss: 0.00224408
Iteration 11/1000 | Loss: 0.00018110
Iteration 12/1000 | Loss: 0.00019706
Iteration 13/1000 | Loss: 0.00010014
Iteration 14/1000 | Loss: 0.00004222
Iteration 15/1000 | Loss: 0.00004046
Iteration 16/1000 | Loss: 0.00218742
Iteration 17/1000 | Loss: 0.00017791
Iteration 18/1000 | Loss: 0.00005955
Iteration 19/1000 | Loss: 0.00013479
Iteration 20/1000 | Loss: 0.00005102
Iteration 21/1000 | Loss: 0.00149451
Iteration 22/1000 | Loss: 0.00020590
Iteration 23/1000 | Loss: 0.00004401
Iteration 24/1000 | Loss: 0.00006438
Iteration 25/1000 | Loss: 0.00225772
Iteration 26/1000 | Loss: 0.00039806
Iteration 27/1000 | Loss: 0.00013871
Iteration 28/1000 | Loss: 0.00004002
Iteration 29/1000 | Loss: 0.00223433
Iteration 30/1000 | Loss: 0.00023586
Iteration 31/1000 | Loss: 0.00075241
Iteration 32/1000 | Loss: 0.00110618
Iteration 33/1000 | Loss: 0.00042253
Iteration 34/1000 | Loss: 0.00004436
Iteration 35/1000 | Loss: 0.00003887
Iteration 36/1000 | Loss: 0.00003837
Iteration 37/1000 | Loss: 0.00003803
Iteration 38/1000 | Loss: 0.00003781
Iteration 39/1000 | Loss: 0.00073487
Iteration 40/1000 | Loss: 0.00043385
Iteration 41/1000 | Loss: 0.00007930
Iteration 42/1000 | Loss: 0.00003817
Iteration 43/1000 | Loss: 0.00003745
Iteration 44/1000 | Loss: 0.00003716
Iteration 45/1000 | Loss: 0.00068168
Iteration 46/1000 | Loss: 0.00009536
Iteration 47/1000 | Loss: 0.00003741
Iteration 48/1000 | Loss: 0.00003698
Iteration 49/1000 | Loss: 0.00003682
Iteration 50/1000 | Loss: 0.00003682
Iteration 51/1000 | Loss: 0.00003672
Iteration 52/1000 | Loss: 0.00003665
Iteration 53/1000 | Loss: 0.00003659
Iteration 54/1000 | Loss: 0.00003655
Iteration 55/1000 | Loss: 0.00003653
Iteration 56/1000 | Loss: 0.00003652
Iteration 57/1000 | Loss: 0.00003652
Iteration 58/1000 | Loss: 0.00003647
Iteration 59/1000 | Loss: 0.00003644
Iteration 60/1000 | Loss: 0.00003643
Iteration 61/1000 | Loss: 0.00003643
Iteration 62/1000 | Loss: 0.00003642
Iteration 63/1000 | Loss: 0.00003642
Iteration 64/1000 | Loss: 0.00003642
Iteration 65/1000 | Loss: 0.00003642
Iteration 66/1000 | Loss: 0.00003642
Iteration 67/1000 | Loss: 0.00003641
Iteration 68/1000 | Loss: 0.00003641
Iteration 69/1000 | Loss: 0.00003641
Iteration 70/1000 | Loss: 0.00003641
Iteration 71/1000 | Loss: 0.00003641
Iteration 72/1000 | Loss: 0.00003640
Iteration 73/1000 | Loss: 0.00003640
Iteration 74/1000 | Loss: 0.00003640
Iteration 75/1000 | Loss: 0.00003640
Iteration 76/1000 | Loss: 0.00003640
Iteration 77/1000 | Loss: 0.00003640
Iteration 78/1000 | Loss: 0.00003640
Iteration 79/1000 | Loss: 0.00003640
Iteration 80/1000 | Loss: 0.00003640
Iteration 81/1000 | Loss: 0.00003640
Iteration 82/1000 | Loss: 0.00003639
Iteration 83/1000 | Loss: 0.00003639
Iteration 84/1000 | Loss: 0.00003639
Iteration 85/1000 | Loss: 0.00003639
Iteration 86/1000 | Loss: 0.00003638
Iteration 87/1000 | Loss: 0.00003638
Iteration 88/1000 | Loss: 0.00003638
Iteration 89/1000 | Loss: 0.00003638
Iteration 90/1000 | Loss: 0.00003638
Iteration 91/1000 | Loss: 0.00003638
Iteration 92/1000 | Loss: 0.00003638
Iteration 93/1000 | Loss: 0.00003638
Iteration 94/1000 | Loss: 0.00003637
Iteration 95/1000 | Loss: 0.00003637
Iteration 96/1000 | Loss: 0.00003637
Iteration 97/1000 | Loss: 0.00003636
Iteration 98/1000 | Loss: 0.00003636
Iteration 99/1000 | Loss: 0.00003636
Iteration 100/1000 | Loss: 0.00003636
Iteration 101/1000 | Loss: 0.00003635
Iteration 102/1000 | Loss: 0.00003635
Iteration 103/1000 | Loss: 0.00003635
Iteration 104/1000 | Loss: 0.00003635
Iteration 105/1000 | Loss: 0.00003635
Iteration 106/1000 | Loss: 0.00003634
Iteration 107/1000 | Loss: 0.00003634
Iteration 108/1000 | Loss: 0.00003634
Iteration 109/1000 | Loss: 0.00003634
Iteration 110/1000 | Loss: 0.00003634
Iteration 111/1000 | Loss: 0.00003634
Iteration 112/1000 | Loss: 0.00003634
Iteration 113/1000 | Loss: 0.00003634
Iteration 114/1000 | Loss: 0.00003633
Iteration 115/1000 | Loss: 0.00003633
Iteration 116/1000 | Loss: 0.00003633
Iteration 117/1000 | Loss: 0.00003633
Iteration 118/1000 | Loss: 0.00003633
Iteration 119/1000 | Loss: 0.00003633
Iteration 120/1000 | Loss: 0.00003633
Iteration 121/1000 | Loss: 0.00003633
Iteration 122/1000 | Loss: 0.00003633
Iteration 123/1000 | Loss: 0.00003633
Iteration 124/1000 | Loss: 0.00003633
Iteration 125/1000 | Loss: 0.00003633
Iteration 126/1000 | Loss: 0.00003633
Iteration 127/1000 | Loss: 0.00003633
Iteration 128/1000 | Loss: 0.00003633
Iteration 129/1000 | Loss: 0.00003632
Iteration 130/1000 | Loss: 0.00003632
Iteration 131/1000 | Loss: 0.00003632
Iteration 132/1000 | Loss: 0.00003632
Iteration 133/1000 | Loss: 0.00003632
Iteration 134/1000 | Loss: 0.00003631
Iteration 135/1000 | Loss: 0.00003631
Iteration 136/1000 | Loss: 0.00003631
Iteration 137/1000 | Loss: 0.00003631
Iteration 138/1000 | Loss: 0.00003631
Iteration 139/1000 | Loss: 0.00003631
Iteration 140/1000 | Loss: 0.00003631
Iteration 141/1000 | Loss: 0.00003631
Iteration 142/1000 | Loss: 0.00003631
Iteration 143/1000 | Loss: 0.00003631
Iteration 144/1000 | Loss: 0.00003631
Iteration 145/1000 | Loss: 0.00003630
Iteration 146/1000 | Loss: 0.00003630
Iteration 147/1000 | Loss: 0.00003630
Iteration 148/1000 | Loss: 0.00003630
Iteration 149/1000 | Loss: 0.00003630
Iteration 150/1000 | Loss: 0.00003630
Iteration 151/1000 | Loss: 0.00003630
Iteration 152/1000 | Loss: 0.00003630
Iteration 153/1000 | Loss: 0.00003630
Iteration 154/1000 | Loss: 0.00003630
Iteration 155/1000 | Loss: 0.00003630
Iteration 156/1000 | Loss: 0.00003630
Iteration 157/1000 | Loss: 0.00003630
Iteration 158/1000 | Loss: 0.00003630
Iteration 159/1000 | Loss: 0.00003630
Iteration 160/1000 | Loss: 0.00003630
Iteration 161/1000 | Loss: 0.00003630
Iteration 162/1000 | Loss: 0.00003630
Iteration 163/1000 | Loss: 0.00003630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.629563798313029e-05, 3.629563798313029e-05, 3.629563798313029e-05, 3.629563798313029e-05, 3.629563798313029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.629563798313029e-05

Optimization complete. Final v2v error: 3.9838576316833496 mm

Highest mean error: 12.639856338500977 mm for frame 145

Lowest mean error: 2.815570592880249 mm for frame 213

Saving results

Total time: 133.95970344543457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451389
Iteration 2/25 | Loss: 0.00121405
Iteration 3/25 | Loss: 0.00100284
Iteration 4/25 | Loss: 0.00098723
Iteration 5/25 | Loss: 0.00098485
Iteration 6/25 | Loss: 0.00098480
Iteration 7/25 | Loss: 0.00098480
Iteration 8/25 | Loss: 0.00098480
Iteration 9/25 | Loss: 0.00098480
Iteration 10/25 | Loss: 0.00098480
Iteration 11/25 | Loss: 0.00098480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000984796672128141, 0.000984796672128141, 0.000984796672128141, 0.000984796672128141, 0.000984796672128141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000984796672128141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29958940
Iteration 2/25 | Loss: 0.00059396
Iteration 3/25 | Loss: 0.00059396
Iteration 4/25 | Loss: 0.00059396
Iteration 5/25 | Loss: 0.00059396
Iteration 6/25 | Loss: 0.00059396
Iteration 7/25 | Loss: 0.00059395
Iteration 8/25 | Loss: 0.00059395
Iteration 9/25 | Loss: 0.00059395
Iteration 10/25 | Loss: 0.00059395
Iteration 11/25 | Loss: 0.00059395
Iteration 12/25 | Loss: 0.00059395
Iteration 13/25 | Loss: 0.00059395
Iteration 14/25 | Loss: 0.00059395
Iteration 15/25 | Loss: 0.00059395
Iteration 16/25 | Loss: 0.00059395
Iteration 17/25 | Loss: 0.00059395
Iteration 18/25 | Loss: 0.00059395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005939538241364062, 0.0005939538241364062, 0.0005939538241364062, 0.0005939538241364062, 0.0005939538241364062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005939538241364062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059395
Iteration 2/1000 | Loss: 0.00002581
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001289
Iteration 5/1000 | Loss: 0.00001197
Iteration 6/1000 | Loss: 0.00001144
Iteration 7/1000 | Loss: 0.00001104
Iteration 8/1000 | Loss: 0.00001077
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001042
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001019
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001014
Iteration 26/1000 | Loss: 0.00001008
Iteration 27/1000 | Loss: 0.00001007
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001005
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001004
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001004
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001003
Iteration 38/1000 | Loss: 0.00001003
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000997
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000997
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000997
Iteration 56/1000 | Loss: 0.00000997
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000996
Iteration 59/1000 | Loss: 0.00000996
Iteration 60/1000 | Loss: 0.00000996
Iteration 61/1000 | Loss: 0.00000996
Iteration 62/1000 | Loss: 0.00000995
Iteration 63/1000 | Loss: 0.00000995
Iteration 64/1000 | Loss: 0.00000995
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000994
Iteration 67/1000 | Loss: 0.00000994
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000994
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000993
Iteration 72/1000 | Loss: 0.00000993
Iteration 73/1000 | Loss: 0.00000993
Iteration 74/1000 | Loss: 0.00000993
Iteration 75/1000 | Loss: 0.00000993
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000993
Iteration 80/1000 | Loss: 0.00000993
Iteration 81/1000 | Loss: 0.00000993
Iteration 82/1000 | Loss: 0.00000993
Iteration 83/1000 | Loss: 0.00000993
Iteration 84/1000 | Loss: 0.00000992
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000992
Iteration 88/1000 | Loss: 0.00000992
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000992
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000992
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000990
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000990
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000990
Iteration 123/1000 | Loss: 0.00000990
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000990
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000990
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [9.900784789351746e-06, 9.900784789351746e-06, 9.900784789351746e-06, 9.900784789351746e-06, 9.900784789351746e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.900784789351746e-06

Optimization complete. Final v2v error: 2.6335842609405518 mm

Highest mean error: 2.965182065963745 mm for frame 94

Lowest mean error: 2.4305949211120605 mm for frame 70

Saving results

Total time: 32.438371419906616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107807
Iteration 2/25 | Loss: 0.01107807
Iteration 3/25 | Loss: 0.01107806
Iteration 4/25 | Loss: 0.01107806
Iteration 5/25 | Loss: 0.01107806
Iteration 6/25 | Loss: 0.01107806
Iteration 7/25 | Loss: 0.01107806
Iteration 8/25 | Loss: 0.01107806
Iteration 9/25 | Loss: 0.01107806
Iteration 10/25 | Loss: 0.01107806
Iteration 11/25 | Loss: 0.01107806
Iteration 12/25 | Loss: 0.01107805
Iteration 13/25 | Loss: 0.01107805
Iteration 14/25 | Loss: 0.01107805
Iteration 15/25 | Loss: 0.01107805
Iteration 16/25 | Loss: 0.01107805
Iteration 17/25 | Loss: 0.01107805
Iteration 18/25 | Loss: 0.01107805
Iteration 19/25 | Loss: 0.01107804
Iteration 20/25 | Loss: 0.01107804
Iteration 21/25 | Loss: 0.01107804
Iteration 22/25 | Loss: 0.01107804
Iteration 23/25 | Loss: 0.01107804
Iteration 24/25 | Loss: 0.01107804
Iteration 25/25 | Loss: 0.01107804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46093750
Iteration 2/25 | Loss: 0.11878181
Iteration 3/25 | Loss: 0.11736699
Iteration 4/25 | Loss: 0.11337712
Iteration 5/25 | Loss: 0.11337711
Iteration 6/25 | Loss: 0.11337710
Iteration 7/25 | Loss: 0.11337709
Iteration 8/25 | Loss: 0.11337709
Iteration 9/25 | Loss: 0.11337709
Iteration 10/25 | Loss: 0.11337709
Iteration 11/25 | Loss: 0.11337709
Iteration 12/25 | Loss: 0.11337708
Iteration 13/25 | Loss: 0.11337709
Iteration 14/25 | Loss: 0.11337709
Iteration 15/25 | Loss: 0.11337709
Iteration 16/25 | Loss: 0.11337708
Iteration 17/25 | Loss: 0.11337709
Iteration 18/25 | Loss: 0.11337709
Iteration 19/25 | Loss: 0.11337708
Iteration 20/25 | Loss: 0.11337708
Iteration 21/25 | Loss: 0.11337708
Iteration 22/25 | Loss: 0.11337709
Iteration 23/25 | Loss: 0.11337710
Iteration 24/25 | Loss: 0.11337708
Iteration 25/25 | Loss: 0.11337708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11337708
Iteration 2/1000 | Loss: 0.00135156
Iteration 3/1000 | Loss: 0.00090697
Iteration 4/1000 | Loss: 0.00016494
Iteration 5/1000 | Loss: 0.00008241
Iteration 6/1000 | Loss: 0.00004538
Iteration 7/1000 | Loss: 0.00003161
Iteration 8/1000 | Loss: 0.00002410
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00001665
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001138
Iteration 14/1000 | Loss: 0.00001045
Iteration 15/1000 | Loss: 0.00000980
Iteration 16/1000 | Loss: 0.00000929
Iteration 17/1000 | Loss: 0.00000890
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000882
Iteration 21/1000 | Loss: 0.00000854
Iteration 22/1000 | Loss: 0.00000848
Iteration 23/1000 | Loss: 0.00000829
Iteration 24/1000 | Loss: 0.00000809
Iteration 25/1000 | Loss: 0.00000807
Iteration 26/1000 | Loss: 0.00000806
Iteration 27/1000 | Loss: 0.00000795
Iteration 28/1000 | Loss: 0.00000783
Iteration 29/1000 | Loss: 0.00000778
Iteration 30/1000 | Loss: 0.00000778
Iteration 31/1000 | Loss: 0.00000777
Iteration 32/1000 | Loss: 0.00000776
Iteration 33/1000 | Loss: 0.00000775
Iteration 34/1000 | Loss: 0.00000775
Iteration 35/1000 | Loss: 0.00000775
Iteration 36/1000 | Loss: 0.00000774
Iteration 37/1000 | Loss: 0.00000774
Iteration 38/1000 | Loss: 0.00000774
Iteration 39/1000 | Loss: 0.00000774
Iteration 40/1000 | Loss: 0.00000773
Iteration 41/1000 | Loss: 0.00000773
Iteration 42/1000 | Loss: 0.00000773
Iteration 43/1000 | Loss: 0.00000773
Iteration 44/1000 | Loss: 0.00000773
Iteration 45/1000 | Loss: 0.00000772
Iteration 46/1000 | Loss: 0.00000772
Iteration 47/1000 | Loss: 0.00000771
Iteration 48/1000 | Loss: 0.00000771
Iteration 49/1000 | Loss: 0.00000771
Iteration 50/1000 | Loss: 0.00000771
Iteration 51/1000 | Loss: 0.00000771
Iteration 52/1000 | Loss: 0.00000771
Iteration 53/1000 | Loss: 0.00000771
Iteration 54/1000 | Loss: 0.00000771
Iteration 55/1000 | Loss: 0.00000771
Iteration 56/1000 | Loss: 0.00000771
Iteration 57/1000 | Loss: 0.00000771
Iteration 58/1000 | Loss: 0.00000771
Iteration 59/1000 | Loss: 0.00000771
Iteration 60/1000 | Loss: 0.00000770
Iteration 61/1000 | Loss: 0.00000770
Iteration 62/1000 | Loss: 0.00000770
Iteration 63/1000 | Loss: 0.00000770
Iteration 64/1000 | Loss: 0.00000770
Iteration 65/1000 | Loss: 0.00000770
Iteration 66/1000 | Loss: 0.00000770
Iteration 67/1000 | Loss: 0.00000769
Iteration 68/1000 | Loss: 0.00000769
Iteration 69/1000 | Loss: 0.00000769
Iteration 70/1000 | Loss: 0.00000769
Iteration 71/1000 | Loss: 0.00000769
Iteration 72/1000 | Loss: 0.00000769
Iteration 73/1000 | Loss: 0.00000769
Iteration 74/1000 | Loss: 0.00000768
Iteration 75/1000 | Loss: 0.00000768
Iteration 76/1000 | Loss: 0.00000768
Iteration 77/1000 | Loss: 0.00000768
Iteration 78/1000 | Loss: 0.00000768
Iteration 79/1000 | Loss: 0.00000768
Iteration 80/1000 | Loss: 0.00000768
Iteration 81/1000 | Loss: 0.00000768
Iteration 82/1000 | Loss: 0.00000768
Iteration 83/1000 | Loss: 0.00000768
Iteration 84/1000 | Loss: 0.00000768
Iteration 85/1000 | Loss: 0.00000768
Iteration 86/1000 | Loss: 0.00000767
Iteration 87/1000 | Loss: 0.00000767
Iteration 88/1000 | Loss: 0.00000767
Iteration 89/1000 | Loss: 0.00000767
Iteration 90/1000 | Loss: 0.00000767
Iteration 91/1000 | Loss: 0.00000767
Iteration 92/1000 | Loss: 0.00000767
Iteration 93/1000 | Loss: 0.00000767
Iteration 94/1000 | Loss: 0.00000767
Iteration 95/1000 | Loss: 0.00000767
Iteration 96/1000 | Loss: 0.00000767
Iteration 97/1000 | Loss: 0.00000767
Iteration 98/1000 | Loss: 0.00000767
Iteration 99/1000 | Loss: 0.00000767
Iteration 100/1000 | Loss: 0.00000767
Iteration 101/1000 | Loss: 0.00000767
Iteration 102/1000 | Loss: 0.00000766
Iteration 103/1000 | Loss: 0.00000766
Iteration 104/1000 | Loss: 0.00000766
Iteration 105/1000 | Loss: 0.00000766
Iteration 106/1000 | Loss: 0.00000766
Iteration 107/1000 | Loss: 0.00000766
Iteration 108/1000 | Loss: 0.00000766
Iteration 109/1000 | Loss: 0.00000766
Iteration 110/1000 | Loss: 0.00000766
Iteration 111/1000 | Loss: 0.00000766
Iteration 112/1000 | Loss: 0.00000766
Iteration 113/1000 | Loss: 0.00000766
Iteration 114/1000 | Loss: 0.00000766
Iteration 115/1000 | Loss: 0.00000766
Iteration 116/1000 | Loss: 0.00000765
Iteration 117/1000 | Loss: 0.00000765
Iteration 118/1000 | Loss: 0.00000765
Iteration 119/1000 | Loss: 0.00000765
Iteration 120/1000 | Loss: 0.00000765
Iteration 121/1000 | Loss: 0.00000765
Iteration 122/1000 | Loss: 0.00000765
Iteration 123/1000 | Loss: 0.00000765
Iteration 124/1000 | Loss: 0.00000765
Iteration 125/1000 | Loss: 0.00000765
Iteration 126/1000 | Loss: 0.00000765
Iteration 127/1000 | Loss: 0.00000765
Iteration 128/1000 | Loss: 0.00000765
Iteration 129/1000 | Loss: 0.00000765
Iteration 130/1000 | Loss: 0.00000765
Iteration 131/1000 | Loss: 0.00000764
Iteration 132/1000 | Loss: 0.00000764
Iteration 133/1000 | Loss: 0.00000764
Iteration 134/1000 | Loss: 0.00000764
Iteration 135/1000 | Loss: 0.00000764
Iteration 136/1000 | Loss: 0.00000764
Iteration 137/1000 | Loss: 0.00000764
Iteration 138/1000 | Loss: 0.00000764
Iteration 139/1000 | Loss: 0.00000764
Iteration 140/1000 | Loss: 0.00000764
Iteration 141/1000 | Loss: 0.00000764
Iteration 142/1000 | Loss: 0.00000763
Iteration 143/1000 | Loss: 0.00000763
Iteration 144/1000 | Loss: 0.00000763
Iteration 145/1000 | Loss: 0.00000763
Iteration 146/1000 | Loss: 0.00000763
Iteration 147/1000 | Loss: 0.00000763
Iteration 148/1000 | Loss: 0.00000763
Iteration 149/1000 | Loss: 0.00000763
Iteration 150/1000 | Loss: 0.00000763
Iteration 151/1000 | Loss: 0.00000763
Iteration 152/1000 | Loss: 0.00000763
Iteration 153/1000 | Loss: 0.00000763
Iteration 154/1000 | Loss: 0.00000763
Iteration 155/1000 | Loss: 0.00000763
Iteration 156/1000 | Loss: 0.00000763
Iteration 157/1000 | Loss: 0.00000763
Iteration 158/1000 | Loss: 0.00000763
Iteration 159/1000 | Loss: 0.00000763
Iteration 160/1000 | Loss: 0.00000763
Iteration 161/1000 | Loss: 0.00000763
Iteration 162/1000 | Loss: 0.00000763
Iteration 163/1000 | Loss: 0.00000763
Iteration 164/1000 | Loss: 0.00000763
Iteration 165/1000 | Loss: 0.00000763
Iteration 166/1000 | Loss: 0.00000763
Iteration 167/1000 | Loss: 0.00000763
Iteration 168/1000 | Loss: 0.00000763
Iteration 169/1000 | Loss: 0.00000763
Iteration 170/1000 | Loss: 0.00000763
Iteration 171/1000 | Loss: 0.00000763
Iteration 172/1000 | Loss: 0.00000763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [7.632081178599037e-06, 7.632081178599037e-06, 7.632081178599037e-06, 7.632081178599037e-06, 7.632081178599037e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.632081178599037e-06

Optimization complete. Final v2v error: 2.3450262546539307 mm

Highest mean error: 8.889236450195312 mm for frame 139

Lowest mean error: 2.158109188079834 mm for frame 238

Saving results

Total time: 58.82149696350098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028371
Iteration 2/25 | Loss: 0.00244986
Iteration 3/25 | Loss: 0.00147273
Iteration 4/25 | Loss: 0.00133803
Iteration 5/25 | Loss: 0.00133078
Iteration 6/25 | Loss: 0.00125640
Iteration 7/25 | Loss: 0.00115885
Iteration 8/25 | Loss: 0.00109511
Iteration 9/25 | Loss: 0.00107965
Iteration 10/25 | Loss: 0.00108299
Iteration 11/25 | Loss: 0.00106527
Iteration 12/25 | Loss: 0.00106228
Iteration 13/25 | Loss: 0.00105850
Iteration 14/25 | Loss: 0.00105367
Iteration 15/25 | Loss: 0.00105263
Iteration 16/25 | Loss: 0.00105234
Iteration 17/25 | Loss: 0.00105219
Iteration 18/25 | Loss: 0.00105205
Iteration 19/25 | Loss: 0.00105195
Iteration 20/25 | Loss: 0.00105185
Iteration 21/25 | Loss: 0.00105292
Iteration 22/25 | Loss: 0.00105190
Iteration 23/25 | Loss: 0.00105108
Iteration 24/25 | Loss: 0.00105073
Iteration 25/25 | Loss: 0.00105056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31559181
Iteration 2/25 | Loss: 0.00159142
Iteration 3/25 | Loss: 0.00117669
Iteration 4/25 | Loss: 0.00117669
Iteration 5/25 | Loss: 0.00117669
Iteration 6/25 | Loss: 0.00117669
Iteration 7/25 | Loss: 0.00117669
Iteration 8/25 | Loss: 0.00117669
Iteration 9/25 | Loss: 0.00117669
Iteration 10/25 | Loss: 0.00117669
Iteration 11/25 | Loss: 0.00117669
Iteration 12/25 | Loss: 0.00117669
Iteration 13/25 | Loss: 0.00117669
Iteration 14/25 | Loss: 0.00117669
Iteration 15/25 | Loss: 0.00117669
Iteration 16/25 | Loss: 0.00117669
Iteration 17/25 | Loss: 0.00117669
Iteration 18/25 | Loss: 0.00117669
Iteration 19/25 | Loss: 0.00117669
Iteration 20/25 | Loss: 0.00117669
Iteration 21/25 | Loss: 0.00117669
Iteration 22/25 | Loss: 0.00117669
Iteration 23/25 | Loss: 0.00117669
Iteration 24/25 | Loss: 0.00117669
Iteration 25/25 | Loss: 0.00117669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117669
Iteration 2/1000 | Loss: 0.00044048
Iteration 3/1000 | Loss: 0.00008759
Iteration 4/1000 | Loss: 0.00007297
Iteration 5/1000 | Loss: 0.00006624
Iteration 6/1000 | Loss: 0.00006199
Iteration 7/1000 | Loss: 0.00005907
Iteration 8/1000 | Loss: 0.00005760
Iteration 9/1000 | Loss: 0.00045506
Iteration 10/1000 | Loss: 0.00006324
Iteration 11/1000 | Loss: 0.00047244
Iteration 12/1000 | Loss: 0.00005817
Iteration 13/1000 | Loss: 0.00035875
Iteration 14/1000 | Loss: 0.00005335
Iteration 15/1000 | Loss: 0.00004919
Iteration 16/1000 | Loss: 0.00115345
Iteration 17/1000 | Loss: 0.00031433
Iteration 18/1000 | Loss: 0.00031632
Iteration 19/1000 | Loss: 0.00004872
Iteration 20/1000 | Loss: 0.00031345
Iteration 21/1000 | Loss: 0.00004630
Iteration 22/1000 | Loss: 0.00054701
Iteration 23/1000 | Loss: 0.00005142
Iteration 24/1000 | Loss: 0.00034052
Iteration 25/1000 | Loss: 0.00004272
Iteration 26/1000 | Loss: 0.00003908
Iteration 27/1000 | Loss: 0.00093813
Iteration 28/1000 | Loss: 0.00006911
Iteration 29/1000 | Loss: 0.00003783
Iteration 30/1000 | Loss: 0.00065024
Iteration 31/1000 | Loss: 0.00003828
Iteration 32/1000 | Loss: 0.00003365
Iteration 33/1000 | Loss: 0.00003096
Iteration 34/1000 | Loss: 0.00002934
Iteration 35/1000 | Loss: 0.00038811
Iteration 36/1000 | Loss: 0.00004072
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00042815
Iteration 39/1000 | Loss: 0.00003060
Iteration 40/1000 | Loss: 0.00002525
Iteration 41/1000 | Loss: 0.00002398
Iteration 42/1000 | Loss: 0.00045890
Iteration 43/1000 | Loss: 0.00004093
Iteration 44/1000 | Loss: 0.00003112
Iteration 45/1000 | Loss: 0.00002515
Iteration 46/1000 | Loss: 0.00002221
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001902
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001704
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001693
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00047540
Iteration 137/1000 | Loss: 0.00019714
Iteration 138/1000 | Loss: 0.00002137
Iteration 139/1000 | Loss: 0.00001926
Iteration 140/1000 | Loss: 0.00001787
Iteration 141/1000 | Loss: 0.00001740
Iteration 142/1000 | Loss: 0.00001728
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001697
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001692
Iteration 159/1000 | Loss: 0.00001691
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001688
Iteration 165/1000 | Loss: 0.00001688
Iteration 166/1000 | Loss: 0.00001687
Iteration 167/1000 | Loss: 0.00001687
Iteration 168/1000 | Loss: 0.00001686
Iteration 169/1000 | Loss: 0.00001686
Iteration 170/1000 | Loss: 0.00001685
Iteration 171/1000 | Loss: 0.00001685
Iteration 172/1000 | Loss: 0.00001685
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00049841
Iteration 185/1000 | Loss: 0.00045755
Iteration 186/1000 | Loss: 0.00010742
Iteration 187/1000 | Loss: 0.00002769
Iteration 188/1000 | Loss: 0.00023230
Iteration 189/1000 | Loss: 0.00002517
Iteration 190/1000 | Loss: 0.00028572
Iteration 191/1000 | Loss: 0.00021275
Iteration 192/1000 | Loss: 0.00002274
Iteration 193/1000 | Loss: 0.00002033
Iteration 194/1000 | Loss: 0.00001910
Iteration 195/1000 | Loss: 0.00034526
Iteration 196/1000 | Loss: 0.00002066
Iteration 197/1000 | Loss: 0.00009051
Iteration 198/1000 | Loss: 0.00002413
Iteration 199/1000 | Loss: 0.00002011
Iteration 200/1000 | Loss: 0.00001823
Iteration 201/1000 | Loss: 0.00001769
Iteration 202/1000 | Loss: 0.00022089
Iteration 203/1000 | Loss: 0.00033223
Iteration 204/1000 | Loss: 0.00042250
Iteration 205/1000 | Loss: 0.00027244
Iteration 206/1000 | Loss: 0.00024147
Iteration 207/1000 | Loss: 0.00029715
Iteration 208/1000 | Loss: 0.00029436
Iteration 209/1000 | Loss: 0.00027050
Iteration 210/1000 | Loss: 0.00002573
Iteration 211/1000 | Loss: 0.00023606
Iteration 212/1000 | Loss: 0.00002940
Iteration 213/1000 | Loss: 0.00002145
Iteration 214/1000 | Loss: 0.00031383
Iteration 215/1000 | Loss: 0.00014132
Iteration 216/1000 | Loss: 0.00007420
Iteration 217/1000 | Loss: 0.00026822
Iteration 218/1000 | Loss: 0.00014898
Iteration 219/1000 | Loss: 0.00028922
Iteration 220/1000 | Loss: 0.00011849
Iteration 221/1000 | Loss: 0.00002131
Iteration 222/1000 | Loss: 0.00025775
Iteration 223/1000 | Loss: 0.00028499
Iteration 224/1000 | Loss: 0.00014198
Iteration 225/1000 | Loss: 0.00024263
Iteration 226/1000 | Loss: 0.00031102
Iteration 227/1000 | Loss: 0.00013204
Iteration 228/1000 | Loss: 0.00002999
Iteration 229/1000 | Loss: 0.00036355
Iteration 230/1000 | Loss: 0.00004661
Iteration 231/1000 | Loss: 0.00024137
Iteration 232/1000 | Loss: 0.00005143
Iteration 233/1000 | Loss: 0.00002522
Iteration 234/1000 | Loss: 0.00002109
Iteration 235/1000 | Loss: 0.00001896
Iteration 236/1000 | Loss: 0.00001753
Iteration 237/1000 | Loss: 0.00001675
Iteration 238/1000 | Loss: 0.00001622
Iteration 239/1000 | Loss: 0.00001582
Iteration 240/1000 | Loss: 0.00001567
Iteration 241/1000 | Loss: 0.00001563
Iteration 242/1000 | Loss: 0.00001563
Iteration 243/1000 | Loss: 0.00001560
Iteration 244/1000 | Loss: 0.00001560
Iteration 245/1000 | Loss: 0.00001552
Iteration 246/1000 | Loss: 0.00001551
Iteration 247/1000 | Loss: 0.00001544
Iteration 248/1000 | Loss: 0.00001542
Iteration 249/1000 | Loss: 0.00001542
Iteration 250/1000 | Loss: 0.00001538
Iteration 251/1000 | Loss: 0.00001538
Iteration 252/1000 | Loss: 0.00001536
Iteration 253/1000 | Loss: 0.00001536
Iteration 254/1000 | Loss: 0.00001536
Iteration 255/1000 | Loss: 0.00001536
Iteration 256/1000 | Loss: 0.00001535
Iteration 257/1000 | Loss: 0.00001535
Iteration 258/1000 | Loss: 0.00001535
Iteration 259/1000 | Loss: 0.00001534
Iteration 260/1000 | Loss: 0.00001534
Iteration 261/1000 | Loss: 0.00001533
Iteration 262/1000 | Loss: 0.00001532
Iteration 263/1000 | Loss: 0.00001532
Iteration 264/1000 | Loss: 0.00001532
Iteration 265/1000 | Loss: 0.00001532
Iteration 266/1000 | Loss: 0.00001531
Iteration 267/1000 | Loss: 0.00001531
Iteration 268/1000 | Loss: 0.00001531
Iteration 269/1000 | Loss: 0.00001530
Iteration 270/1000 | Loss: 0.00001529
Iteration 271/1000 | Loss: 0.00001525
Iteration 272/1000 | Loss: 0.00001525
Iteration 273/1000 | Loss: 0.00001525
Iteration 274/1000 | Loss: 0.00001525
Iteration 275/1000 | Loss: 0.00001525
Iteration 276/1000 | Loss: 0.00001525
Iteration 277/1000 | Loss: 0.00001524
Iteration 278/1000 | Loss: 0.00001524
Iteration 279/1000 | Loss: 0.00001524
Iteration 280/1000 | Loss: 0.00001523
Iteration 281/1000 | Loss: 0.00001523
Iteration 282/1000 | Loss: 0.00001523
Iteration 283/1000 | Loss: 0.00001523
Iteration 284/1000 | Loss: 0.00001523
Iteration 285/1000 | Loss: 0.00001522
Iteration 286/1000 | Loss: 0.00001522
Iteration 287/1000 | Loss: 0.00001521
Iteration 288/1000 | Loss: 0.00001521
Iteration 289/1000 | Loss: 0.00001521
Iteration 290/1000 | Loss: 0.00001520
Iteration 291/1000 | Loss: 0.00001520
Iteration 292/1000 | Loss: 0.00001519
Iteration 293/1000 | Loss: 0.00001518
Iteration 294/1000 | Loss: 0.00001518
Iteration 295/1000 | Loss: 0.00001518
Iteration 296/1000 | Loss: 0.00001518
Iteration 297/1000 | Loss: 0.00001518
Iteration 298/1000 | Loss: 0.00001518
Iteration 299/1000 | Loss: 0.00001517
Iteration 300/1000 | Loss: 0.00001517
Iteration 301/1000 | Loss: 0.00001517
Iteration 302/1000 | Loss: 0.00001517
Iteration 303/1000 | Loss: 0.00001517
Iteration 304/1000 | Loss: 0.00001517
Iteration 305/1000 | Loss: 0.00001517
Iteration 306/1000 | Loss: 0.00001517
Iteration 307/1000 | Loss: 0.00001516
Iteration 308/1000 | Loss: 0.00001516
Iteration 309/1000 | Loss: 0.00001516
Iteration 310/1000 | Loss: 0.00001516
Iteration 311/1000 | Loss: 0.00001515
Iteration 312/1000 | Loss: 0.00001515
Iteration 313/1000 | Loss: 0.00001515
Iteration 314/1000 | Loss: 0.00001515
Iteration 315/1000 | Loss: 0.00001515
Iteration 316/1000 | Loss: 0.00001514
Iteration 317/1000 | Loss: 0.00001514
Iteration 318/1000 | Loss: 0.00001514
Iteration 319/1000 | Loss: 0.00001514
Iteration 320/1000 | Loss: 0.00001514
Iteration 321/1000 | Loss: 0.00001514
Iteration 322/1000 | Loss: 0.00001514
Iteration 323/1000 | Loss: 0.00001514
Iteration 324/1000 | Loss: 0.00001514
Iteration 325/1000 | Loss: 0.00001514
Iteration 326/1000 | Loss: 0.00001514
Iteration 327/1000 | Loss: 0.00001514
Iteration 328/1000 | Loss: 0.00001514
Iteration 329/1000 | Loss: 0.00001514
Iteration 330/1000 | Loss: 0.00001514
Iteration 331/1000 | Loss: 0.00001514
Iteration 332/1000 | Loss: 0.00001514
Iteration 333/1000 | Loss: 0.00001514
Iteration 334/1000 | Loss: 0.00001514
Iteration 335/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [1.5141482435865328e-05, 1.5141482435865328e-05, 1.5141482435865328e-05, 1.5141482435865328e-05, 1.5141482435865328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5141482435865328e-05

Optimization complete. Final v2v error: 2.7729084491729736 mm

Highest mean error: 11.813554763793945 mm for frame 69

Lowest mean error: 2.179365873336792 mm for frame 91

Saving results

Total time: 272.0503935813904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610292
Iteration 2/25 | Loss: 0.00127555
Iteration 3/25 | Loss: 0.00111874
Iteration 4/25 | Loss: 0.00108036
Iteration 5/25 | Loss: 0.00107039
Iteration 6/25 | Loss: 0.00106762
Iteration 7/25 | Loss: 0.00106675
Iteration 8/25 | Loss: 0.00106675
Iteration 9/25 | Loss: 0.00106675
Iteration 10/25 | Loss: 0.00106675
Iteration 11/25 | Loss: 0.00106675
Iteration 12/25 | Loss: 0.00106675
Iteration 13/25 | Loss: 0.00106675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010667471215128899, 0.0010667471215128899, 0.0010667471215128899, 0.0010667471215128899, 0.0010667471215128899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010667471215128899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29487658
Iteration 2/25 | Loss: 0.00078488
Iteration 3/25 | Loss: 0.00078485
Iteration 4/25 | Loss: 0.00078484
Iteration 5/25 | Loss: 0.00078484
Iteration 6/25 | Loss: 0.00078484
Iteration 7/25 | Loss: 0.00078484
Iteration 8/25 | Loss: 0.00078484
Iteration 9/25 | Loss: 0.00078484
Iteration 10/25 | Loss: 0.00078484
Iteration 11/25 | Loss: 0.00078484
Iteration 12/25 | Loss: 0.00078484
Iteration 13/25 | Loss: 0.00078484
Iteration 14/25 | Loss: 0.00078484
Iteration 15/25 | Loss: 0.00078484
Iteration 16/25 | Loss: 0.00078484
Iteration 17/25 | Loss: 0.00078484
Iteration 18/25 | Loss: 0.00078484
Iteration 19/25 | Loss: 0.00078484
Iteration 20/25 | Loss: 0.00078484
Iteration 21/25 | Loss: 0.00078484
Iteration 22/25 | Loss: 0.00078484
Iteration 23/25 | Loss: 0.00078484
Iteration 24/25 | Loss: 0.00078484
Iteration 25/25 | Loss: 0.00078484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078484
Iteration 2/1000 | Loss: 0.00007273
Iteration 3/1000 | Loss: 0.00004803
Iteration 4/1000 | Loss: 0.00003911
Iteration 5/1000 | Loss: 0.00003639
Iteration 6/1000 | Loss: 0.00003471
Iteration 7/1000 | Loss: 0.00003325
Iteration 8/1000 | Loss: 0.00003258
Iteration 9/1000 | Loss: 0.00003199
Iteration 10/1000 | Loss: 0.00003143
Iteration 11/1000 | Loss: 0.00003107
Iteration 12/1000 | Loss: 0.00003080
Iteration 13/1000 | Loss: 0.00003061
Iteration 14/1000 | Loss: 0.00003055
Iteration 15/1000 | Loss: 0.00003047
Iteration 16/1000 | Loss: 0.00003041
Iteration 17/1000 | Loss: 0.00003038
Iteration 18/1000 | Loss: 0.00003038
Iteration 19/1000 | Loss: 0.00003037
Iteration 20/1000 | Loss: 0.00003036
Iteration 21/1000 | Loss: 0.00003036
Iteration 22/1000 | Loss: 0.00003035
Iteration 23/1000 | Loss: 0.00003034
Iteration 24/1000 | Loss: 0.00003034
Iteration 25/1000 | Loss: 0.00003034
Iteration 26/1000 | Loss: 0.00003034
Iteration 27/1000 | Loss: 0.00003033
Iteration 28/1000 | Loss: 0.00003033
Iteration 29/1000 | Loss: 0.00003033
Iteration 30/1000 | Loss: 0.00003033
Iteration 31/1000 | Loss: 0.00003032
Iteration 32/1000 | Loss: 0.00003032
Iteration 33/1000 | Loss: 0.00003032
Iteration 34/1000 | Loss: 0.00003032
Iteration 35/1000 | Loss: 0.00003032
Iteration 36/1000 | Loss: 0.00003032
Iteration 37/1000 | Loss: 0.00003031
Iteration 38/1000 | Loss: 0.00003031
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00003030
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00003029
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003029
Iteration 45/1000 | Loss: 0.00003029
Iteration 46/1000 | Loss: 0.00003029
Iteration 47/1000 | Loss: 0.00003029
Iteration 48/1000 | Loss: 0.00003028
Iteration 49/1000 | Loss: 0.00003028
Iteration 50/1000 | Loss: 0.00003028
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00003028
Iteration 53/1000 | Loss: 0.00003028
Iteration 54/1000 | Loss: 0.00003027
Iteration 55/1000 | Loss: 0.00003027
Iteration 56/1000 | Loss: 0.00003027
Iteration 57/1000 | Loss: 0.00003027
Iteration 58/1000 | Loss: 0.00003027
Iteration 59/1000 | Loss: 0.00003027
Iteration 60/1000 | Loss: 0.00003027
Iteration 61/1000 | Loss: 0.00003027
Iteration 62/1000 | Loss: 0.00003027
Iteration 63/1000 | Loss: 0.00003027
Iteration 64/1000 | Loss: 0.00003026
Iteration 65/1000 | Loss: 0.00003026
Iteration 66/1000 | Loss: 0.00003026
Iteration 67/1000 | Loss: 0.00003026
Iteration 68/1000 | Loss: 0.00003026
Iteration 69/1000 | Loss: 0.00003025
Iteration 70/1000 | Loss: 0.00003025
Iteration 71/1000 | Loss: 0.00003025
Iteration 72/1000 | Loss: 0.00003025
Iteration 73/1000 | Loss: 0.00003024
Iteration 74/1000 | Loss: 0.00003024
Iteration 75/1000 | Loss: 0.00003024
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00003024
Iteration 79/1000 | Loss: 0.00003024
Iteration 80/1000 | Loss: 0.00003024
Iteration 81/1000 | Loss: 0.00003024
Iteration 82/1000 | Loss: 0.00003024
Iteration 83/1000 | Loss: 0.00003024
Iteration 84/1000 | Loss: 0.00003024
Iteration 85/1000 | Loss: 0.00003024
Iteration 86/1000 | Loss: 0.00003023
Iteration 87/1000 | Loss: 0.00003023
Iteration 88/1000 | Loss: 0.00003023
Iteration 89/1000 | Loss: 0.00003023
Iteration 90/1000 | Loss: 0.00003023
Iteration 91/1000 | Loss: 0.00003023
Iteration 92/1000 | Loss: 0.00003023
Iteration 93/1000 | Loss: 0.00003023
Iteration 94/1000 | Loss: 0.00003023
Iteration 95/1000 | Loss: 0.00003022
Iteration 96/1000 | Loss: 0.00003022
Iteration 97/1000 | Loss: 0.00003022
Iteration 98/1000 | Loss: 0.00003022
Iteration 99/1000 | Loss: 0.00003022
Iteration 100/1000 | Loss: 0.00003022
Iteration 101/1000 | Loss: 0.00003022
Iteration 102/1000 | Loss: 0.00003022
Iteration 103/1000 | Loss: 0.00003022
Iteration 104/1000 | Loss: 0.00003022
Iteration 105/1000 | Loss: 0.00003022
Iteration 106/1000 | Loss: 0.00003022
Iteration 107/1000 | Loss: 0.00003022
Iteration 108/1000 | Loss: 0.00003021
Iteration 109/1000 | Loss: 0.00003021
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00003021
Iteration 112/1000 | Loss: 0.00003021
Iteration 113/1000 | Loss: 0.00003021
Iteration 114/1000 | Loss: 0.00003021
Iteration 115/1000 | Loss: 0.00003021
Iteration 116/1000 | Loss: 0.00003021
Iteration 117/1000 | Loss: 0.00003021
Iteration 118/1000 | Loss: 0.00003020
Iteration 119/1000 | Loss: 0.00003020
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00003020
Iteration 122/1000 | Loss: 0.00003020
Iteration 123/1000 | Loss: 0.00003020
Iteration 124/1000 | Loss: 0.00003020
Iteration 125/1000 | Loss: 0.00003020
Iteration 126/1000 | Loss: 0.00003020
Iteration 127/1000 | Loss: 0.00003020
Iteration 128/1000 | Loss: 0.00003020
Iteration 129/1000 | Loss: 0.00003020
Iteration 130/1000 | Loss: 0.00003019
Iteration 131/1000 | Loss: 0.00003019
Iteration 132/1000 | Loss: 0.00003019
Iteration 133/1000 | Loss: 0.00003019
Iteration 134/1000 | Loss: 0.00003019
Iteration 135/1000 | Loss: 0.00003019
Iteration 136/1000 | Loss: 0.00003019
Iteration 137/1000 | Loss: 0.00003019
Iteration 138/1000 | Loss: 0.00003019
Iteration 139/1000 | Loss: 0.00003019
Iteration 140/1000 | Loss: 0.00003019
Iteration 141/1000 | Loss: 0.00003019
Iteration 142/1000 | Loss: 0.00003018
Iteration 143/1000 | Loss: 0.00003018
Iteration 144/1000 | Loss: 0.00003018
Iteration 145/1000 | Loss: 0.00003018
Iteration 146/1000 | Loss: 0.00003018
Iteration 147/1000 | Loss: 0.00003018
Iteration 148/1000 | Loss: 0.00003018
Iteration 149/1000 | Loss: 0.00003018
Iteration 150/1000 | Loss: 0.00003018
Iteration 151/1000 | Loss: 0.00003018
Iteration 152/1000 | Loss: 0.00003018
Iteration 153/1000 | Loss: 0.00003018
Iteration 154/1000 | Loss: 0.00003018
Iteration 155/1000 | Loss: 0.00003018
Iteration 156/1000 | Loss: 0.00003018
Iteration 157/1000 | Loss: 0.00003018
Iteration 158/1000 | Loss: 0.00003018
Iteration 159/1000 | Loss: 0.00003018
Iteration 160/1000 | Loss: 0.00003018
Iteration 161/1000 | Loss: 0.00003018
Iteration 162/1000 | Loss: 0.00003018
Iteration 163/1000 | Loss: 0.00003018
Iteration 164/1000 | Loss: 0.00003018
Iteration 165/1000 | Loss: 0.00003018
Iteration 166/1000 | Loss: 0.00003017
Iteration 167/1000 | Loss: 0.00003017
Iteration 168/1000 | Loss: 0.00003017
Iteration 169/1000 | Loss: 0.00003017
Iteration 170/1000 | Loss: 0.00003017
Iteration 171/1000 | Loss: 0.00003017
Iteration 172/1000 | Loss: 0.00003017
Iteration 173/1000 | Loss: 0.00003017
Iteration 174/1000 | Loss: 0.00003017
Iteration 175/1000 | Loss: 0.00003017
Iteration 176/1000 | Loss: 0.00003017
Iteration 177/1000 | Loss: 0.00003017
Iteration 178/1000 | Loss: 0.00003017
Iteration 179/1000 | Loss: 0.00003017
Iteration 180/1000 | Loss: 0.00003017
Iteration 181/1000 | Loss: 0.00003017
Iteration 182/1000 | Loss: 0.00003017
Iteration 183/1000 | Loss: 0.00003017
Iteration 184/1000 | Loss: 0.00003017
Iteration 185/1000 | Loss: 0.00003017
Iteration 186/1000 | Loss: 0.00003017
Iteration 187/1000 | Loss: 0.00003017
Iteration 188/1000 | Loss: 0.00003017
Iteration 189/1000 | Loss: 0.00003017
Iteration 190/1000 | Loss: 0.00003017
Iteration 191/1000 | Loss: 0.00003017
Iteration 192/1000 | Loss: 0.00003017
Iteration 193/1000 | Loss: 0.00003017
Iteration 194/1000 | Loss: 0.00003017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [3.0168779630912468e-05, 3.0168779630912468e-05, 3.0168779630912468e-05, 3.0168779630912468e-05, 3.0168779630912468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0168779630912468e-05

Optimization complete. Final v2v error: 4.530940055847168 mm

Highest mean error: 5.397251129150391 mm for frame 121

Lowest mean error: 3.2752959728240967 mm for frame 17

Saving results

Total time: 41.1722731590271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881693
Iteration 2/25 | Loss: 0.00106605
Iteration 3/25 | Loss: 0.00095921
Iteration 4/25 | Loss: 0.00094136
Iteration 5/25 | Loss: 0.00093467
Iteration 6/25 | Loss: 0.00093308
Iteration 7/25 | Loss: 0.00093308
Iteration 8/25 | Loss: 0.00093308
Iteration 9/25 | Loss: 0.00093308
Iteration 10/25 | Loss: 0.00093308
Iteration 11/25 | Loss: 0.00093308
Iteration 12/25 | Loss: 0.00093308
Iteration 13/25 | Loss: 0.00093308
Iteration 14/25 | Loss: 0.00093308
Iteration 15/25 | Loss: 0.00093308
Iteration 16/25 | Loss: 0.00093308
Iteration 17/25 | Loss: 0.00093308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009330775937996805, 0.0009330775937996805, 0.0009330775937996805, 0.0009330775937996805, 0.0009330775937996805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009330775937996805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98114586
Iteration 2/25 | Loss: 0.00063842
Iteration 3/25 | Loss: 0.00063839
Iteration 4/25 | Loss: 0.00063839
Iteration 5/25 | Loss: 0.00063839
Iteration 6/25 | Loss: 0.00063839
Iteration 7/25 | Loss: 0.00063839
Iteration 8/25 | Loss: 0.00063839
Iteration 9/25 | Loss: 0.00063839
Iteration 10/25 | Loss: 0.00063839
Iteration 11/25 | Loss: 0.00063839
Iteration 12/25 | Loss: 0.00063839
Iteration 13/25 | Loss: 0.00063839
Iteration 14/25 | Loss: 0.00063839
Iteration 15/25 | Loss: 0.00063839
Iteration 16/25 | Loss: 0.00063839
Iteration 17/25 | Loss: 0.00063839
Iteration 18/25 | Loss: 0.00063839
Iteration 19/25 | Loss: 0.00063839
Iteration 20/25 | Loss: 0.00063839
Iteration 21/25 | Loss: 0.00063839
Iteration 22/25 | Loss: 0.00063839
Iteration 23/25 | Loss: 0.00063839
Iteration 24/25 | Loss: 0.00063839
Iteration 25/25 | Loss: 0.00063839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063839
Iteration 2/1000 | Loss: 0.00001618
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001173
Iteration 5/1000 | Loss: 0.00001130
Iteration 6/1000 | Loss: 0.00001102
Iteration 7/1000 | Loss: 0.00001090
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001066
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001054
Iteration 19/1000 | Loss: 0.00001054
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001051
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001050
Iteration 40/1000 | Loss: 0.00001050
Iteration 41/1000 | Loss: 0.00001050
Iteration 42/1000 | Loss: 0.00001050
Iteration 43/1000 | Loss: 0.00001050
Iteration 44/1000 | Loss: 0.00001050
Iteration 45/1000 | Loss: 0.00001050
Iteration 46/1000 | Loss: 0.00001050
Iteration 47/1000 | Loss: 0.00001050
Iteration 48/1000 | Loss: 0.00001050
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001048
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001048
Iteration 57/1000 | Loss: 0.00001048
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001045
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001044
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001043
Iteration 106/1000 | Loss: 0.00001043
Iteration 107/1000 | Loss: 0.00001043
Iteration 108/1000 | Loss: 0.00001043
Iteration 109/1000 | Loss: 0.00001043
Iteration 110/1000 | Loss: 0.00001043
Iteration 111/1000 | Loss: 0.00001043
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001043
Iteration 114/1000 | Loss: 0.00001043
Iteration 115/1000 | Loss: 0.00001042
Iteration 116/1000 | Loss: 0.00001042
Iteration 117/1000 | Loss: 0.00001042
Iteration 118/1000 | Loss: 0.00001042
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001041
Iteration 122/1000 | Loss: 0.00001041
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001041
Iteration 128/1000 | Loss: 0.00001040
Iteration 129/1000 | Loss: 0.00001040
Iteration 130/1000 | Loss: 0.00001040
Iteration 131/1000 | Loss: 0.00001040
Iteration 132/1000 | Loss: 0.00001040
Iteration 133/1000 | Loss: 0.00001040
Iteration 134/1000 | Loss: 0.00001039
Iteration 135/1000 | Loss: 0.00001039
Iteration 136/1000 | Loss: 0.00001039
Iteration 137/1000 | Loss: 0.00001039
Iteration 138/1000 | Loss: 0.00001039
Iteration 139/1000 | Loss: 0.00001038
Iteration 140/1000 | Loss: 0.00001038
Iteration 141/1000 | Loss: 0.00001038
Iteration 142/1000 | Loss: 0.00001038
Iteration 143/1000 | Loss: 0.00001038
Iteration 144/1000 | Loss: 0.00001038
Iteration 145/1000 | Loss: 0.00001038
Iteration 146/1000 | Loss: 0.00001038
Iteration 147/1000 | Loss: 0.00001038
Iteration 148/1000 | Loss: 0.00001038
Iteration 149/1000 | Loss: 0.00001038
Iteration 150/1000 | Loss: 0.00001038
Iteration 151/1000 | Loss: 0.00001038
Iteration 152/1000 | Loss: 0.00001038
Iteration 153/1000 | Loss: 0.00001038
Iteration 154/1000 | Loss: 0.00001038
Iteration 155/1000 | Loss: 0.00001038
Iteration 156/1000 | Loss: 0.00001038
Iteration 157/1000 | Loss: 0.00001038
Iteration 158/1000 | Loss: 0.00001038
Iteration 159/1000 | Loss: 0.00001038
Iteration 160/1000 | Loss: 0.00001038
Iteration 161/1000 | Loss: 0.00001038
Iteration 162/1000 | Loss: 0.00001038
Iteration 163/1000 | Loss: 0.00001038
Iteration 164/1000 | Loss: 0.00001038
Iteration 165/1000 | Loss: 0.00001038
Iteration 166/1000 | Loss: 0.00001038
Iteration 167/1000 | Loss: 0.00001038
Iteration 168/1000 | Loss: 0.00001038
Iteration 169/1000 | Loss: 0.00001038
Iteration 170/1000 | Loss: 0.00001038
Iteration 171/1000 | Loss: 0.00001038
Iteration 172/1000 | Loss: 0.00001038
Iteration 173/1000 | Loss: 0.00001038
Iteration 174/1000 | Loss: 0.00001038
Iteration 175/1000 | Loss: 0.00001038
Iteration 176/1000 | Loss: 0.00001038
Iteration 177/1000 | Loss: 0.00001038
Iteration 178/1000 | Loss: 0.00001038
Iteration 179/1000 | Loss: 0.00001038
Iteration 180/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.0375144483987242e-05, 1.0375144483987242e-05, 1.0375144483987242e-05, 1.0375144483987242e-05, 1.0375144483987242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0375144483987242e-05

Optimization complete. Final v2v error: 2.7554924488067627 mm

Highest mean error: 3.2960433959960938 mm for frame 24

Lowest mean error: 2.481684923171997 mm for frame 181

Saving results

Total time: 32.78220272064209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037755
Iteration 2/25 | Loss: 0.00124487
Iteration 3/25 | Loss: 0.00107954
Iteration 4/25 | Loss: 0.00105971
Iteration 5/25 | Loss: 0.00105334
Iteration 6/25 | Loss: 0.00105223
Iteration 7/25 | Loss: 0.00105223
Iteration 8/25 | Loss: 0.00105223
Iteration 9/25 | Loss: 0.00105223
Iteration 10/25 | Loss: 0.00105223
Iteration 11/25 | Loss: 0.00105223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001052228850312531, 0.001052228850312531, 0.001052228850312531, 0.001052228850312531, 0.001052228850312531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001052228850312531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62601495
Iteration 2/25 | Loss: 0.00072236
Iteration 3/25 | Loss: 0.00072234
Iteration 4/25 | Loss: 0.00072234
Iteration 5/25 | Loss: 0.00072234
Iteration 6/25 | Loss: 0.00072234
Iteration 7/25 | Loss: 0.00072234
Iteration 8/25 | Loss: 0.00072234
Iteration 9/25 | Loss: 0.00072234
Iteration 10/25 | Loss: 0.00072234
Iteration 11/25 | Loss: 0.00072234
Iteration 12/25 | Loss: 0.00072234
Iteration 13/25 | Loss: 0.00072234
Iteration 14/25 | Loss: 0.00072234
Iteration 15/25 | Loss: 0.00072234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007223419961519539, 0.0007223419961519539, 0.0007223419961519539, 0.0007223419961519539, 0.0007223419961519539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007223419961519539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072234
Iteration 2/1000 | Loss: 0.00004333
Iteration 3/1000 | Loss: 0.00002377
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001787
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001771
Iteration 17/1000 | Loss: 0.00001771
Iteration 18/1000 | Loss: 0.00001771
Iteration 19/1000 | Loss: 0.00001770
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001770
Iteration 25/1000 | Loss: 0.00001770
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001770
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001755
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001755
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001753
Iteration 129/1000 | Loss: 0.00001753
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001753
Iteration 132/1000 | Loss: 0.00001753
Iteration 133/1000 | Loss: 0.00001753
Iteration 134/1000 | Loss: 0.00001753
Iteration 135/1000 | Loss: 0.00001753
Iteration 136/1000 | Loss: 0.00001752
Iteration 137/1000 | Loss: 0.00001752
Iteration 138/1000 | Loss: 0.00001752
Iteration 139/1000 | Loss: 0.00001752
Iteration 140/1000 | Loss: 0.00001752
Iteration 141/1000 | Loss: 0.00001752
Iteration 142/1000 | Loss: 0.00001752
Iteration 143/1000 | Loss: 0.00001752
Iteration 144/1000 | Loss: 0.00001752
Iteration 145/1000 | Loss: 0.00001752
Iteration 146/1000 | Loss: 0.00001752
Iteration 147/1000 | Loss: 0.00001752
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00001752
Iteration 150/1000 | Loss: 0.00001752
Iteration 151/1000 | Loss: 0.00001752
Iteration 152/1000 | Loss: 0.00001752
Iteration 153/1000 | Loss: 0.00001752
Iteration 154/1000 | Loss: 0.00001751
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001751
Iteration 161/1000 | Loss: 0.00001751
Iteration 162/1000 | Loss: 0.00001751
Iteration 163/1000 | Loss: 0.00001751
Iteration 164/1000 | Loss: 0.00001751
Iteration 165/1000 | Loss: 0.00001751
Iteration 166/1000 | Loss: 0.00001751
Iteration 167/1000 | Loss: 0.00001751
Iteration 168/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.7514574210508727e-05, 1.7514574210508727e-05, 1.7514574210508727e-05, 1.7514574210508727e-05, 1.7514574210508727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7514574210508727e-05

Optimization complete. Final v2v error: 3.4813594818115234 mm

Highest mean error: 3.867666006088257 mm for frame 21

Lowest mean error: 3.091825485229492 mm for frame 119

Saving results

Total time: 31.256282329559326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400396
Iteration 2/25 | Loss: 0.00109601
Iteration 3/25 | Loss: 0.00094716
Iteration 4/25 | Loss: 0.00092939
Iteration 5/25 | Loss: 0.00092679
Iteration 6/25 | Loss: 0.00092641
Iteration 7/25 | Loss: 0.00092641
Iteration 8/25 | Loss: 0.00092641
Iteration 9/25 | Loss: 0.00092641
Iteration 10/25 | Loss: 0.00092641
Iteration 11/25 | Loss: 0.00092641
Iteration 12/25 | Loss: 0.00092641
Iteration 13/25 | Loss: 0.00092641
Iteration 14/25 | Loss: 0.00092641
Iteration 15/25 | Loss: 0.00092641
Iteration 16/25 | Loss: 0.00092641
Iteration 17/25 | Loss: 0.00092641
Iteration 18/25 | Loss: 0.00092641
Iteration 19/25 | Loss: 0.00092641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009264135733246803, 0.0009264135733246803, 0.0009264135733246803, 0.0009264135733246803, 0.0009264135733246803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009264135733246803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32337606
Iteration 2/25 | Loss: 0.00055484
Iteration 3/25 | Loss: 0.00055484
Iteration 4/25 | Loss: 0.00055484
Iteration 5/25 | Loss: 0.00055484
Iteration 6/25 | Loss: 0.00055484
Iteration 7/25 | Loss: 0.00055484
Iteration 8/25 | Loss: 0.00055484
Iteration 9/25 | Loss: 0.00055484
Iteration 10/25 | Loss: 0.00055484
Iteration 11/25 | Loss: 0.00055484
Iteration 12/25 | Loss: 0.00055484
Iteration 13/25 | Loss: 0.00055484
Iteration 14/25 | Loss: 0.00055484
Iteration 15/25 | Loss: 0.00055484
Iteration 16/25 | Loss: 0.00055484
Iteration 17/25 | Loss: 0.00055484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005548367626033723, 0.0005548367626033723, 0.0005548367626033723, 0.0005548367626033723, 0.0005548367626033723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005548367626033723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055484
Iteration 2/1000 | Loss: 0.00001642
Iteration 3/1000 | Loss: 0.00001141
Iteration 4/1000 | Loss: 0.00001039
Iteration 5/1000 | Loss: 0.00000977
Iteration 6/1000 | Loss: 0.00000941
Iteration 7/1000 | Loss: 0.00000913
Iteration 8/1000 | Loss: 0.00000901
Iteration 9/1000 | Loss: 0.00000900
Iteration 10/1000 | Loss: 0.00000895
Iteration 11/1000 | Loss: 0.00000894
Iteration 12/1000 | Loss: 0.00000893
Iteration 13/1000 | Loss: 0.00000891
Iteration 14/1000 | Loss: 0.00000889
Iteration 15/1000 | Loss: 0.00000889
Iteration 16/1000 | Loss: 0.00000888
Iteration 17/1000 | Loss: 0.00000887
Iteration 18/1000 | Loss: 0.00000886
Iteration 19/1000 | Loss: 0.00000885
Iteration 20/1000 | Loss: 0.00000885
Iteration 21/1000 | Loss: 0.00000884
Iteration 22/1000 | Loss: 0.00000884
Iteration 23/1000 | Loss: 0.00000884
Iteration 24/1000 | Loss: 0.00000884
Iteration 25/1000 | Loss: 0.00000884
Iteration 26/1000 | Loss: 0.00000884
Iteration 27/1000 | Loss: 0.00000883
Iteration 28/1000 | Loss: 0.00000881
Iteration 29/1000 | Loss: 0.00000880
Iteration 30/1000 | Loss: 0.00000880
Iteration 31/1000 | Loss: 0.00000880
Iteration 32/1000 | Loss: 0.00000880
Iteration 33/1000 | Loss: 0.00000880
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000878
Iteration 36/1000 | Loss: 0.00000878
Iteration 37/1000 | Loss: 0.00000878
Iteration 38/1000 | Loss: 0.00000877
Iteration 39/1000 | Loss: 0.00000877
Iteration 40/1000 | Loss: 0.00000877
Iteration 41/1000 | Loss: 0.00000877
Iteration 42/1000 | Loss: 0.00000877
Iteration 43/1000 | Loss: 0.00000877
Iteration 44/1000 | Loss: 0.00000877
Iteration 45/1000 | Loss: 0.00000877
Iteration 46/1000 | Loss: 0.00000877
Iteration 47/1000 | Loss: 0.00000876
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000875
Iteration 51/1000 | Loss: 0.00000875
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000874
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000871
Iteration 64/1000 | Loss: 0.00000871
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000870
Iteration 69/1000 | Loss: 0.00000870
Iteration 70/1000 | Loss: 0.00000870
Iteration 71/1000 | Loss: 0.00000870
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000867
Iteration 83/1000 | Loss: 0.00000867
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000866
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000866
Iteration 89/1000 | Loss: 0.00000866
Iteration 90/1000 | Loss: 0.00000866
Iteration 91/1000 | Loss: 0.00000866
Iteration 92/1000 | Loss: 0.00000866
Iteration 93/1000 | Loss: 0.00000865
Iteration 94/1000 | Loss: 0.00000865
Iteration 95/1000 | Loss: 0.00000865
Iteration 96/1000 | Loss: 0.00000865
Iteration 97/1000 | Loss: 0.00000865
Iteration 98/1000 | Loss: 0.00000865
Iteration 99/1000 | Loss: 0.00000865
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000865
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000865
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000864
Iteration 108/1000 | Loss: 0.00000864
Iteration 109/1000 | Loss: 0.00000864
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000864
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000863
Iteration 116/1000 | Loss: 0.00000863
Iteration 117/1000 | Loss: 0.00000863
Iteration 118/1000 | Loss: 0.00000863
Iteration 119/1000 | Loss: 0.00000863
Iteration 120/1000 | Loss: 0.00000863
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000863
Iteration 124/1000 | Loss: 0.00000863
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000863
Iteration 131/1000 | Loss: 0.00000863
Iteration 132/1000 | Loss: 0.00000863
Iteration 133/1000 | Loss: 0.00000863
Iteration 134/1000 | Loss: 0.00000863
Iteration 135/1000 | Loss: 0.00000863
Iteration 136/1000 | Loss: 0.00000863
Iteration 137/1000 | Loss: 0.00000863
Iteration 138/1000 | Loss: 0.00000863
Iteration 139/1000 | Loss: 0.00000863
Iteration 140/1000 | Loss: 0.00000863
Iteration 141/1000 | Loss: 0.00000863
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000863
Iteration 144/1000 | Loss: 0.00000863
Iteration 145/1000 | Loss: 0.00000863
Iteration 146/1000 | Loss: 0.00000863
Iteration 147/1000 | Loss: 0.00000863
Iteration 148/1000 | Loss: 0.00000863
Iteration 149/1000 | Loss: 0.00000863
Iteration 150/1000 | Loss: 0.00000863
Iteration 151/1000 | Loss: 0.00000863
Iteration 152/1000 | Loss: 0.00000863
Iteration 153/1000 | Loss: 0.00000863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [8.626825547253247e-06, 8.626825547253247e-06, 8.626825547253247e-06, 8.626825547253247e-06, 8.626825547253247e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.626825547253247e-06

Optimization complete. Final v2v error: 2.4811742305755615 mm

Highest mean error: 2.748839855194092 mm for frame 114

Lowest mean error: 2.2250754833221436 mm for frame 14

Saving results

Total time: 28.438949823379517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476821
Iteration 2/25 | Loss: 0.00101428
Iteration 3/25 | Loss: 0.00092595
Iteration 4/25 | Loss: 0.00091470
Iteration 5/25 | Loss: 0.00091042
Iteration 6/25 | Loss: 0.00090946
Iteration 7/25 | Loss: 0.00090946
Iteration 8/25 | Loss: 0.00090946
Iteration 9/25 | Loss: 0.00090946
Iteration 10/25 | Loss: 0.00090946
Iteration 11/25 | Loss: 0.00090946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009094623965211213, 0.0009094623965211213, 0.0009094623965211213, 0.0009094623965211213, 0.0009094623965211213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009094623965211213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44884145
Iteration 2/25 | Loss: 0.00060249
Iteration 3/25 | Loss: 0.00060249
Iteration 4/25 | Loss: 0.00060249
Iteration 5/25 | Loss: 0.00060248
Iteration 6/25 | Loss: 0.00060248
Iteration 7/25 | Loss: 0.00060248
Iteration 8/25 | Loss: 0.00060248
Iteration 9/25 | Loss: 0.00060248
Iteration 10/25 | Loss: 0.00060248
Iteration 11/25 | Loss: 0.00060248
Iteration 12/25 | Loss: 0.00060248
Iteration 13/25 | Loss: 0.00060248
Iteration 14/25 | Loss: 0.00060248
Iteration 15/25 | Loss: 0.00060248
Iteration 16/25 | Loss: 0.00060248
Iteration 17/25 | Loss: 0.00060248
Iteration 18/25 | Loss: 0.00060248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006024825852364302, 0.0006024825852364302, 0.0006024825852364302, 0.0006024825852364302, 0.0006024825852364302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006024825852364302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060248
Iteration 2/1000 | Loss: 0.00001880
Iteration 3/1000 | Loss: 0.00001202
Iteration 4/1000 | Loss: 0.00001100
Iteration 5/1000 | Loss: 0.00001027
Iteration 6/1000 | Loss: 0.00000989
Iteration 7/1000 | Loss: 0.00000959
Iteration 8/1000 | Loss: 0.00000950
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000949
Iteration 11/1000 | Loss: 0.00000948
Iteration 12/1000 | Loss: 0.00000948
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000948
Iteration 16/1000 | Loss: 0.00000947
Iteration 17/1000 | Loss: 0.00000947
Iteration 18/1000 | Loss: 0.00000946
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000942
Iteration 21/1000 | Loss: 0.00000941
Iteration 22/1000 | Loss: 0.00000939
Iteration 23/1000 | Loss: 0.00000937
Iteration 24/1000 | Loss: 0.00000937
Iteration 25/1000 | Loss: 0.00000936
Iteration 26/1000 | Loss: 0.00000936
Iteration 27/1000 | Loss: 0.00000936
Iteration 28/1000 | Loss: 0.00000931
Iteration 29/1000 | Loss: 0.00000931
Iteration 30/1000 | Loss: 0.00000931
Iteration 31/1000 | Loss: 0.00000930
Iteration 32/1000 | Loss: 0.00000930
Iteration 33/1000 | Loss: 0.00000929
Iteration 34/1000 | Loss: 0.00000929
Iteration 35/1000 | Loss: 0.00000929
Iteration 36/1000 | Loss: 0.00000929
Iteration 37/1000 | Loss: 0.00000928
Iteration 38/1000 | Loss: 0.00000928
Iteration 39/1000 | Loss: 0.00000928
Iteration 40/1000 | Loss: 0.00000927
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000927
Iteration 46/1000 | Loss: 0.00000927
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000926
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000925
Iteration 54/1000 | Loss: 0.00000925
Iteration 55/1000 | Loss: 0.00000925
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000925
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000925
Iteration 64/1000 | Loss: 0.00000925
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000922
Iteration 91/1000 | Loss: 0.00000922
Iteration 92/1000 | Loss: 0.00000922
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000921
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000921
Iteration 111/1000 | Loss: 0.00000921
Iteration 112/1000 | Loss: 0.00000921
Iteration 113/1000 | Loss: 0.00000921
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000921
Iteration 116/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [9.2085319920443e-06, 9.2085319920443e-06, 9.2085319920443e-06, 9.2085319920443e-06, 9.2085319920443e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.2085319920443e-06

Optimization complete. Final v2v error: 2.585038900375366 mm

Highest mean error: 3.100390911102295 mm for frame 84

Lowest mean error: 2.3475537300109863 mm for frame 14

Saving results

Total time: 26.89420175552368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761981
Iteration 2/25 | Loss: 0.00218846
Iteration 3/25 | Loss: 0.00160277
Iteration 4/25 | Loss: 0.00146314
Iteration 5/25 | Loss: 0.00141083
Iteration 6/25 | Loss: 0.00138251
Iteration 7/25 | Loss: 0.00136070
Iteration 8/25 | Loss: 0.00141922
Iteration 9/25 | Loss: 0.00156746
Iteration 10/25 | Loss: 0.00135585
Iteration 11/25 | Loss: 0.00116451
Iteration 12/25 | Loss: 0.00114279
Iteration 13/25 | Loss: 0.00110186
Iteration 14/25 | Loss: 0.00107123
Iteration 15/25 | Loss: 0.00106360
Iteration 16/25 | Loss: 0.00105787
Iteration 17/25 | Loss: 0.00106271
Iteration 18/25 | Loss: 0.00105239
Iteration 19/25 | Loss: 0.00104593
Iteration 20/25 | Loss: 0.00104921
Iteration 21/25 | Loss: 0.00104161
Iteration 22/25 | Loss: 0.00104035
Iteration 23/25 | Loss: 0.00103988
Iteration 24/25 | Loss: 0.00103968
Iteration 25/25 | Loss: 0.00103959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44644690
Iteration 2/25 | Loss: 0.00091025
Iteration 3/25 | Loss: 0.00087292
Iteration 4/25 | Loss: 0.00087287
Iteration 5/25 | Loss: 0.00087287
Iteration 6/25 | Loss: 0.00087287
Iteration 7/25 | Loss: 0.00087287
Iteration 8/25 | Loss: 0.00087287
Iteration 9/25 | Loss: 0.00087287
Iteration 10/25 | Loss: 0.00087287
Iteration 11/25 | Loss: 0.00087287
Iteration 12/25 | Loss: 0.00087287
Iteration 13/25 | Loss: 0.00087287
Iteration 14/25 | Loss: 0.00087287
Iteration 15/25 | Loss: 0.00087287
Iteration 16/25 | Loss: 0.00087287
Iteration 17/25 | Loss: 0.00087287
Iteration 18/25 | Loss: 0.00087287
Iteration 19/25 | Loss: 0.00087286
Iteration 20/25 | Loss: 0.00087286
Iteration 21/25 | Loss: 0.00087286
Iteration 22/25 | Loss: 0.00087286
Iteration 23/25 | Loss: 0.00087286
Iteration 24/25 | Loss: 0.00087286
Iteration 25/25 | Loss: 0.00087286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087286
Iteration 2/1000 | Loss: 0.00005592
Iteration 3/1000 | Loss: 0.00003436
Iteration 4/1000 | Loss: 0.00002938
Iteration 5/1000 | Loss: 0.00002784
Iteration 6/1000 | Loss: 0.00002661
Iteration 7/1000 | Loss: 0.00005872
Iteration 8/1000 | Loss: 0.00012156
Iteration 9/1000 | Loss: 0.00005725
Iteration 10/1000 | Loss: 0.00002776
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002442
Iteration 13/1000 | Loss: 0.00002403
Iteration 14/1000 | Loss: 0.00002366
Iteration 15/1000 | Loss: 0.00002337
Iteration 16/1000 | Loss: 0.00006691
Iteration 17/1000 | Loss: 0.00002505
Iteration 18/1000 | Loss: 0.00002307
Iteration 19/1000 | Loss: 0.00002279
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002259
Iteration 23/1000 | Loss: 0.00002258
Iteration 24/1000 | Loss: 0.00002258
Iteration 25/1000 | Loss: 0.00002254
Iteration 26/1000 | Loss: 0.00002253
Iteration 27/1000 | Loss: 0.00002252
Iteration 28/1000 | Loss: 0.00002251
Iteration 29/1000 | Loss: 0.00007662
Iteration 30/1000 | Loss: 0.00004428
Iteration 31/1000 | Loss: 0.00007438
Iteration 32/1000 | Loss: 0.00002337
Iteration 33/1000 | Loss: 0.00002255
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002220
Iteration 36/1000 | Loss: 0.00002220
Iteration 37/1000 | Loss: 0.00002220
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002219
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00002219
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002218
Iteration 47/1000 | Loss: 0.00002218
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002217
Iteration 50/1000 | Loss: 0.00002217
Iteration 51/1000 | Loss: 0.00002217
Iteration 52/1000 | Loss: 0.00002217
Iteration 53/1000 | Loss: 0.00002216
Iteration 54/1000 | Loss: 0.00002216
Iteration 55/1000 | Loss: 0.00002216
Iteration 56/1000 | Loss: 0.00002216
Iteration 57/1000 | Loss: 0.00002215
Iteration 58/1000 | Loss: 0.00002215
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002215
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002213
Iteration 65/1000 | Loss: 0.00002213
Iteration 66/1000 | Loss: 0.00002213
Iteration 67/1000 | Loss: 0.00002213
Iteration 68/1000 | Loss: 0.00002213
Iteration 69/1000 | Loss: 0.00002213
Iteration 70/1000 | Loss: 0.00002213
Iteration 71/1000 | Loss: 0.00002212
Iteration 72/1000 | Loss: 0.00002212
Iteration 73/1000 | Loss: 0.00002212
Iteration 74/1000 | Loss: 0.00002212
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002212
Iteration 77/1000 | Loss: 0.00002212
Iteration 78/1000 | Loss: 0.00002212
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002211
Iteration 81/1000 | Loss: 0.00002211
Iteration 82/1000 | Loss: 0.00002209
Iteration 83/1000 | Loss: 0.00002209
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002209
Iteration 86/1000 | Loss: 0.00002209
Iteration 87/1000 | Loss: 0.00002209
Iteration 88/1000 | Loss: 0.00002209
Iteration 89/1000 | Loss: 0.00002209
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002209
Iteration 92/1000 | Loss: 0.00002209
Iteration 93/1000 | Loss: 0.00002209
Iteration 94/1000 | Loss: 0.00002208
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00002208
Iteration 97/1000 | Loss: 0.00002208
Iteration 98/1000 | Loss: 0.00002208
Iteration 99/1000 | Loss: 0.00002208
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002208
Iteration 102/1000 | Loss: 0.00002208
Iteration 103/1000 | Loss: 0.00002207
Iteration 104/1000 | Loss: 0.00002207
Iteration 105/1000 | Loss: 0.00002206
Iteration 106/1000 | Loss: 0.00002206
Iteration 107/1000 | Loss: 0.00002206
Iteration 108/1000 | Loss: 0.00002205
Iteration 109/1000 | Loss: 0.00002205
Iteration 110/1000 | Loss: 0.00002205
Iteration 111/1000 | Loss: 0.00002205
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002205
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002203
Iteration 119/1000 | Loss: 0.00002203
Iteration 120/1000 | Loss: 0.00002203
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002203
Iteration 128/1000 | Loss: 0.00002203
Iteration 129/1000 | Loss: 0.00002203
Iteration 130/1000 | Loss: 0.00002203
Iteration 131/1000 | Loss: 0.00002203
Iteration 132/1000 | Loss: 0.00002202
Iteration 133/1000 | Loss: 0.00002202
Iteration 134/1000 | Loss: 0.00002202
Iteration 135/1000 | Loss: 0.00002202
Iteration 136/1000 | Loss: 0.00002202
Iteration 137/1000 | Loss: 0.00002202
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002201
Iteration 143/1000 | Loss: 0.00002201
Iteration 144/1000 | Loss: 0.00002201
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002201
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002200
Iteration 151/1000 | Loss: 0.00002200
Iteration 152/1000 | Loss: 0.00002200
Iteration 153/1000 | Loss: 0.00002200
Iteration 154/1000 | Loss: 0.00002200
Iteration 155/1000 | Loss: 0.00002200
Iteration 156/1000 | Loss: 0.00002200
Iteration 157/1000 | Loss: 0.00002200
Iteration 158/1000 | Loss: 0.00002200
Iteration 159/1000 | Loss: 0.00002200
Iteration 160/1000 | Loss: 0.00002199
Iteration 161/1000 | Loss: 0.00002199
Iteration 162/1000 | Loss: 0.00002199
Iteration 163/1000 | Loss: 0.00002199
Iteration 164/1000 | Loss: 0.00002199
Iteration 165/1000 | Loss: 0.00002199
Iteration 166/1000 | Loss: 0.00002199
Iteration 167/1000 | Loss: 0.00002199
Iteration 168/1000 | Loss: 0.00002199
Iteration 169/1000 | Loss: 0.00002199
Iteration 170/1000 | Loss: 0.00002199
Iteration 171/1000 | Loss: 0.00002199
Iteration 172/1000 | Loss: 0.00002199
Iteration 173/1000 | Loss: 0.00002198
Iteration 174/1000 | Loss: 0.00002198
Iteration 175/1000 | Loss: 0.00002198
Iteration 176/1000 | Loss: 0.00002198
Iteration 177/1000 | Loss: 0.00002198
Iteration 178/1000 | Loss: 0.00002197
Iteration 179/1000 | Loss: 0.00002197
Iteration 180/1000 | Loss: 0.00002197
Iteration 181/1000 | Loss: 0.00002197
Iteration 182/1000 | Loss: 0.00002197
Iteration 183/1000 | Loss: 0.00002197
Iteration 184/1000 | Loss: 0.00002197
Iteration 185/1000 | Loss: 0.00002197
Iteration 186/1000 | Loss: 0.00002197
Iteration 187/1000 | Loss: 0.00002196
Iteration 188/1000 | Loss: 0.00002196
Iteration 189/1000 | Loss: 0.00002196
Iteration 190/1000 | Loss: 0.00002196
Iteration 191/1000 | Loss: 0.00002196
Iteration 192/1000 | Loss: 0.00002196
Iteration 193/1000 | Loss: 0.00002196
Iteration 194/1000 | Loss: 0.00002196
Iteration 195/1000 | Loss: 0.00002196
Iteration 196/1000 | Loss: 0.00002196
Iteration 197/1000 | Loss: 0.00002195
Iteration 198/1000 | Loss: 0.00002195
Iteration 199/1000 | Loss: 0.00002195
Iteration 200/1000 | Loss: 0.00002195
Iteration 201/1000 | Loss: 0.00002195
Iteration 202/1000 | Loss: 0.00002194
Iteration 203/1000 | Loss: 0.00002194
Iteration 204/1000 | Loss: 0.00002194
Iteration 205/1000 | Loss: 0.00002194
Iteration 206/1000 | Loss: 0.00002194
Iteration 207/1000 | Loss: 0.00002194
Iteration 208/1000 | Loss: 0.00002194
Iteration 209/1000 | Loss: 0.00002194
Iteration 210/1000 | Loss: 0.00002194
Iteration 211/1000 | Loss: 0.00002194
Iteration 212/1000 | Loss: 0.00002194
Iteration 213/1000 | Loss: 0.00002194
Iteration 214/1000 | Loss: 0.00002194
Iteration 215/1000 | Loss: 0.00002194
Iteration 216/1000 | Loss: 0.00002194
Iteration 217/1000 | Loss: 0.00002194
Iteration 218/1000 | Loss: 0.00002194
Iteration 219/1000 | Loss: 0.00002194
Iteration 220/1000 | Loss: 0.00002194
Iteration 221/1000 | Loss: 0.00002194
Iteration 222/1000 | Loss: 0.00002194
Iteration 223/1000 | Loss: 0.00002194
Iteration 224/1000 | Loss: 0.00002194
Iteration 225/1000 | Loss: 0.00002194
Iteration 226/1000 | Loss: 0.00002194
Iteration 227/1000 | Loss: 0.00002194
Iteration 228/1000 | Loss: 0.00002194
Iteration 229/1000 | Loss: 0.00002194
Iteration 230/1000 | Loss: 0.00002194
Iteration 231/1000 | Loss: 0.00002194
Iteration 232/1000 | Loss: 0.00002194
Iteration 233/1000 | Loss: 0.00002194
Iteration 234/1000 | Loss: 0.00002194
Iteration 235/1000 | Loss: 0.00002194
Iteration 236/1000 | Loss: 0.00002194
Iteration 237/1000 | Loss: 0.00002194
Iteration 238/1000 | Loss: 0.00002194
Iteration 239/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.193523323512636e-05, 2.193523323512636e-05, 2.193523323512636e-05, 2.193523323512636e-05, 2.193523323512636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.193523323512636e-05

Optimization complete. Final v2v error: 3.70076847076416 mm

Highest mean error: 6.07304048538208 mm for frame 158

Lowest mean error: 2.7515547275543213 mm for frame 202

Saving results

Total time: 111.45655727386475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104468
Iteration 2/25 | Loss: 0.00195911
Iteration 3/25 | Loss: 0.00128826
Iteration 4/25 | Loss: 0.00119857
Iteration 5/25 | Loss: 0.00119890
Iteration 6/25 | Loss: 0.00118598
Iteration 7/25 | Loss: 0.00112836
Iteration 8/25 | Loss: 0.00111019
Iteration 9/25 | Loss: 0.00108415
Iteration 10/25 | Loss: 0.00107200
Iteration 11/25 | Loss: 0.00106476
Iteration 12/25 | Loss: 0.00106121
Iteration 13/25 | Loss: 0.00105966
Iteration 14/25 | Loss: 0.00105645
Iteration 15/25 | Loss: 0.00105460
Iteration 16/25 | Loss: 0.00105127
Iteration 17/25 | Loss: 0.00105008
Iteration 18/25 | Loss: 0.00105013
Iteration 19/25 | Loss: 0.00105107
Iteration 20/25 | Loss: 0.00104876
Iteration 21/25 | Loss: 0.00104814
Iteration 22/25 | Loss: 0.00104849
Iteration 23/25 | Loss: 0.00104847
Iteration 24/25 | Loss: 0.00104827
Iteration 25/25 | Loss: 0.00104840

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35040581
Iteration 2/25 | Loss: 0.00060665
Iteration 3/25 | Loss: 0.00054340
Iteration 4/25 | Loss: 0.00054339
Iteration 5/25 | Loss: 0.00054339
Iteration 6/25 | Loss: 0.00054339
Iteration 7/25 | Loss: 0.00054339
Iteration 8/25 | Loss: 0.00054339
Iteration 9/25 | Loss: 0.00054339
Iteration 10/25 | Loss: 0.00054339
Iteration 11/25 | Loss: 0.00054339
Iteration 12/25 | Loss: 0.00054339
Iteration 13/25 | Loss: 0.00054339
Iteration 14/25 | Loss: 0.00054339
Iteration 15/25 | Loss: 0.00054339
Iteration 16/25 | Loss: 0.00054339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005433927290141582, 0.0005433927290141582, 0.0005433927290141582, 0.0005433927290141582, 0.0005433927290141582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005433927290141582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054339
Iteration 2/1000 | Loss: 0.00012512
Iteration 3/1000 | Loss: 0.00022911
Iteration 4/1000 | Loss: 0.00006532
Iteration 5/1000 | Loss: 0.00014972
Iteration 6/1000 | Loss: 0.00006916
Iteration 7/1000 | Loss: 0.00010105
Iteration 8/1000 | Loss: 0.00003420
Iteration 9/1000 | Loss: 0.00003253
Iteration 10/1000 | Loss: 0.00003063
Iteration 11/1000 | Loss: 0.00003530
Iteration 12/1000 | Loss: 0.00003603
Iteration 13/1000 | Loss: 0.00007922
Iteration 14/1000 | Loss: 0.00011877
Iteration 15/1000 | Loss: 0.00008459
Iteration 16/1000 | Loss: 0.00002795
Iteration 17/1000 | Loss: 0.00007114
Iteration 18/1000 | Loss: 0.00002505
Iteration 19/1000 | Loss: 0.00002449
Iteration 20/1000 | Loss: 0.00010766
Iteration 21/1000 | Loss: 0.00002416
Iteration 22/1000 | Loss: 0.00002373
Iteration 23/1000 | Loss: 0.00002362
Iteration 24/1000 | Loss: 0.00007087
Iteration 25/1000 | Loss: 0.00002468
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002335
Iteration 28/1000 | Loss: 0.00002330
Iteration 29/1000 | Loss: 0.00002330
Iteration 30/1000 | Loss: 0.00002319
Iteration 31/1000 | Loss: 0.00002319
Iteration 32/1000 | Loss: 0.00002301
Iteration 33/1000 | Loss: 0.00010214
Iteration 34/1000 | Loss: 0.00004947
Iteration 35/1000 | Loss: 0.00002558
Iteration 36/1000 | Loss: 0.00002307
Iteration 37/1000 | Loss: 0.00002288
Iteration 38/1000 | Loss: 0.00003886
Iteration 39/1000 | Loss: 0.00002284
Iteration 40/1000 | Loss: 0.00002276
Iteration 41/1000 | Loss: 0.00002275
Iteration 42/1000 | Loss: 0.00002275
Iteration 43/1000 | Loss: 0.00008520
Iteration 44/1000 | Loss: 0.00003376
Iteration 45/1000 | Loss: 0.00002521
Iteration 46/1000 | Loss: 0.00003840
Iteration 47/1000 | Loss: 0.00002291
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002270
Iteration 50/1000 | Loss: 0.00002270
Iteration 51/1000 | Loss: 0.00002269
Iteration 52/1000 | Loss: 0.00002269
Iteration 53/1000 | Loss: 0.00002266
Iteration 54/1000 | Loss: 0.00002266
Iteration 55/1000 | Loss: 0.00002265
Iteration 56/1000 | Loss: 0.00002265
Iteration 57/1000 | Loss: 0.00002265
Iteration 58/1000 | Loss: 0.00002265
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002265
Iteration 61/1000 | Loss: 0.00002265
Iteration 62/1000 | Loss: 0.00002265
Iteration 63/1000 | Loss: 0.00002265
Iteration 64/1000 | Loss: 0.00002265
Iteration 65/1000 | Loss: 0.00002265
Iteration 66/1000 | Loss: 0.00002264
Iteration 67/1000 | Loss: 0.00002263
Iteration 68/1000 | Loss: 0.00002263
Iteration 69/1000 | Loss: 0.00002263
Iteration 70/1000 | Loss: 0.00002263
Iteration 71/1000 | Loss: 0.00002263
Iteration 72/1000 | Loss: 0.00002263
Iteration 73/1000 | Loss: 0.00002262
Iteration 74/1000 | Loss: 0.00002262
Iteration 75/1000 | Loss: 0.00002262
Iteration 76/1000 | Loss: 0.00002262
Iteration 77/1000 | Loss: 0.00002262
Iteration 78/1000 | Loss: 0.00002262
Iteration 79/1000 | Loss: 0.00002262
Iteration 80/1000 | Loss: 0.00002262
Iteration 81/1000 | Loss: 0.00002261
Iteration 82/1000 | Loss: 0.00002261
Iteration 83/1000 | Loss: 0.00002261
Iteration 84/1000 | Loss: 0.00002261
Iteration 85/1000 | Loss: 0.00002261
Iteration 86/1000 | Loss: 0.00002261
Iteration 87/1000 | Loss: 0.00002261
Iteration 88/1000 | Loss: 0.00002261
Iteration 89/1000 | Loss: 0.00002261
Iteration 90/1000 | Loss: 0.00002260
Iteration 91/1000 | Loss: 0.00002260
Iteration 92/1000 | Loss: 0.00002260
Iteration 93/1000 | Loss: 0.00002260
Iteration 94/1000 | Loss: 0.00002260
Iteration 95/1000 | Loss: 0.00002260
Iteration 96/1000 | Loss: 0.00002259
Iteration 97/1000 | Loss: 0.00002259
Iteration 98/1000 | Loss: 0.00002259
Iteration 99/1000 | Loss: 0.00002259
Iteration 100/1000 | Loss: 0.00002259
Iteration 101/1000 | Loss: 0.00002259
Iteration 102/1000 | Loss: 0.00002259
Iteration 103/1000 | Loss: 0.00002259
Iteration 104/1000 | Loss: 0.00002259
Iteration 105/1000 | Loss: 0.00002259
Iteration 106/1000 | Loss: 0.00002259
Iteration 107/1000 | Loss: 0.00002259
Iteration 108/1000 | Loss: 0.00002259
Iteration 109/1000 | Loss: 0.00002259
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Iteration 112/1000 | Loss: 0.00002259
Iteration 113/1000 | Loss: 0.00002259
Iteration 114/1000 | Loss: 0.00002259
Iteration 115/1000 | Loss: 0.00002259
Iteration 116/1000 | Loss: 0.00002259
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002259
Iteration 122/1000 | Loss: 0.00002259
Iteration 123/1000 | Loss: 0.00002259
Iteration 124/1000 | Loss: 0.00002259
Iteration 125/1000 | Loss: 0.00002259
Iteration 126/1000 | Loss: 0.00002259
Iteration 127/1000 | Loss: 0.00002259
Iteration 128/1000 | Loss: 0.00002259
Iteration 129/1000 | Loss: 0.00002259
Iteration 130/1000 | Loss: 0.00002259
Iteration 131/1000 | Loss: 0.00002259
Iteration 132/1000 | Loss: 0.00002259
Iteration 133/1000 | Loss: 0.00002259
Iteration 134/1000 | Loss: 0.00002259
Iteration 135/1000 | Loss: 0.00002259
Iteration 136/1000 | Loss: 0.00002259
Iteration 137/1000 | Loss: 0.00002259
Iteration 138/1000 | Loss: 0.00002259
Iteration 139/1000 | Loss: 0.00002259
Iteration 140/1000 | Loss: 0.00002259
Iteration 141/1000 | Loss: 0.00002259
Iteration 142/1000 | Loss: 0.00002259
Iteration 143/1000 | Loss: 0.00002259
Iteration 144/1000 | Loss: 0.00002259
Iteration 145/1000 | Loss: 0.00002259
Iteration 146/1000 | Loss: 0.00002259
Iteration 147/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.2586382328881882e-05, 2.2586382328881882e-05, 2.2586382328881882e-05, 2.2586382328881882e-05, 2.2586382328881882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2586382328881882e-05

Optimization complete. Final v2v error: 3.8885245323181152 mm

Highest mean error: 4.304474830627441 mm for frame 69

Lowest mean error: 3.381054162979126 mm for frame 123

Saving results

Total time: 122.80094337463379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077341
Iteration 2/25 | Loss: 0.00197892
Iteration 3/25 | Loss: 0.00133265
Iteration 4/25 | Loss: 0.00122761
Iteration 5/25 | Loss: 0.00113000
Iteration 6/25 | Loss: 0.00108026
Iteration 7/25 | Loss: 0.00104864
Iteration 8/25 | Loss: 0.00103859
Iteration 9/25 | Loss: 0.00100874
Iteration 10/25 | Loss: 0.00099289
Iteration 11/25 | Loss: 0.00099396
Iteration 12/25 | Loss: 0.00098334
Iteration 13/25 | Loss: 0.00098353
Iteration 14/25 | Loss: 0.00097954
Iteration 15/25 | Loss: 0.00098142
Iteration 16/25 | Loss: 0.00098140
Iteration 17/25 | Loss: 0.00098335
Iteration 18/25 | Loss: 0.00097485
Iteration 19/25 | Loss: 0.00097523
Iteration 20/25 | Loss: 0.00097297
Iteration 21/25 | Loss: 0.00097256
Iteration 22/25 | Loss: 0.00097247
Iteration 23/25 | Loss: 0.00097246
Iteration 24/25 | Loss: 0.00097245
Iteration 25/25 | Loss: 0.00097245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43550992
Iteration 2/25 | Loss: 0.00089904
Iteration 3/25 | Loss: 0.00080574
Iteration 4/25 | Loss: 0.00080574
Iteration 5/25 | Loss: 0.00080574
Iteration 6/25 | Loss: 0.00080574
Iteration 7/25 | Loss: 0.00080574
Iteration 8/25 | Loss: 0.00080574
Iteration 9/25 | Loss: 0.00080574
Iteration 10/25 | Loss: 0.00080574
Iteration 11/25 | Loss: 0.00080574
Iteration 12/25 | Loss: 0.00080574
Iteration 13/25 | Loss: 0.00080574
Iteration 14/25 | Loss: 0.00080574
Iteration 15/25 | Loss: 0.00080574
Iteration 16/25 | Loss: 0.00080574
Iteration 17/25 | Loss: 0.00080574
Iteration 18/25 | Loss: 0.00080574
Iteration 19/25 | Loss: 0.00080574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008057394879870117, 0.0008057394879870117, 0.0008057394879870117, 0.0008057394879870117, 0.0008057394879870117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008057394879870117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080574
Iteration 2/1000 | Loss: 0.00010240
Iteration 3/1000 | Loss: 0.00005027
Iteration 4/1000 | Loss: 0.00004214
Iteration 5/1000 | Loss: 0.00003918
Iteration 6/1000 | Loss: 0.00003750
Iteration 7/1000 | Loss: 0.00042188
Iteration 8/1000 | Loss: 0.00013051
Iteration 9/1000 | Loss: 0.00060625
Iteration 10/1000 | Loss: 0.00037779
Iteration 11/1000 | Loss: 0.00003648
Iteration 12/1000 | Loss: 0.00003357
Iteration 13/1000 | Loss: 0.00003212
Iteration 14/1000 | Loss: 0.00003108
Iteration 15/1000 | Loss: 0.00003050
Iteration 16/1000 | Loss: 0.00003014
Iteration 17/1000 | Loss: 0.00002987
Iteration 18/1000 | Loss: 0.00002981
Iteration 19/1000 | Loss: 0.00002953
Iteration 20/1000 | Loss: 0.00002934
Iteration 21/1000 | Loss: 0.00002929
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002912
Iteration 24/1000 | Loss: 0.00002907
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00002903
Iteration 27/1000 | Loss: 0.00002902
Iteration 28/1000 | Loss: 0.00002901
Iteration 29/1000 | Loss: 0.00002900
Iteration 30/1000 | Loss: 0.00002896
Iteration 31/1000 | Loss: 0.00002892
Iteration 32/1000 | Loss: 0.00002892
Iteration 33/1000 | Loss: 0.00002892
Iteration 34/1000 | Loss: 0.00002892
Iteration 35/1000 | Loss: 0.00002892
Iteration 36/1000 | Loss: 0.00002892
Iteration 37/1000 | Loss: 0.00002892
Iteration 38/1000 | Loss: 0.00002892
Iteration 39/1000 | Loss: 0.00002892
Iteration 40/1000 | Loss: 0.00002892
Iteration 41/1000 | Loss: 0.00002892
Iteration 42/1000 | Loss: 0.00002892
Iteration 43/1000 | Loss: 0.00002891
Iteration 44/1000 | Loss: 0.00002891
Iteration 45/1000 | Loss: 0.00002890
Iteration 46/1000 | Loss: 0.00002890
Iteration 47/1000 | Loss: 0.00002890
Iteration 48/1000 | Loss: 0.00002889
Iteration 49/1000 | Loss: 0.00002889
Iteration 50/1000 | Loss: 0.00002888
Iteration 51/1000 | Loss: 0.00002888
Iteration 52/1000 | Loss: 0.00002888
Iteration 53/1000 | Loss: 0.00002887
Iteration 54/1000 | Loss: 0.00002887
Iteration 55/1000 | Loss: 0.00002887
Iteration 56/1000 | Loss: 0.00002887
Iteration 57/1000 | Loss: 0.00002886
Iteration 58/1000 | Loss: 0.00002886
Iteration 59/1000 | Loss: 0.00002886
Iteration 60/1000 | Loss: 0.00002886
Iteration 61/1000 | Loss: 0.00002885
Iteration 62/1000 | Loss: 0.00002885
Iteration 63/1000 | Loss: 0.00002885
Iteration 64/1000 | Loss: 0.00002885
Iteration 65/1000 | Loss: 0.00002885
Iteration 66/1000 | Loss: 0.00002885
Iteration 67/1000 | Loss: 0.00002884
Iteration 68/1000 | Loss: 0.00002884
Iteration 69/1000 | Loss: 0.00002884
Iteration 70/1000 | Loss: 0.00002884
Iteration 71/1000 | Loss: 0.00002884
Iteration 72/1000 | Loss: 0.00002883
Iteration 73/1000 | Loss: 0.00002883
Iteration 74/1000 | Loss: 0.00002883
Iteration 75/1000 | Loss: 0.00002883
Iteration 76/1000 | Loss: 0.00002882
Iteration 77/1000 | Loss: 0.00002882
Iteration 78/1000 | Loss: 0.00002882
Iteration 79/1000 | Loss: 0.00002882
Iteration 80/1000 | Loss: 0.00002882
Iteration 81/1000 | Loss: 0.00002882
Iteration 82/1000 | Loss: 0.00002881
Iteration 83/1000 | Loss: 0.00002881
Iteration 84/1000 | Loss: 0.00002881
Iteration 85/1000 | Loss: 0.00024734
Iteration 86/1000 | Loss: 0.00044319
Iteration 87/1000 | Loss: 0.00075294
Iteration 88/1000 | Loss: 0.00061707
Iteration 89/1000 | Loss: 0.00087704
Iteration 90/1000 | Loss: 0.00100503
Iteration 91/1000 | Loss: 0.00065103
Iteration 92/1000 | Loss: 0.00074093
Iteration 93/1000 | Loss: 0.00048476
Iteration 94/1000 | Loss: 0.00067046
Iteration 95/1000 | Loss: 0.00027188
Iteration 96/1000 | Loss: 0.00054122
Iteration 97/1000 | Loss: 0.00016523
Iteration 98/1000 | Loss: 0.00076509
Iteration 99/1000 | Loss: 0.00063247
Iteration 100/1000 | Loss: 0.00082006
Iteration 101/1000 | Loss: 0.00045025
Iteration 102/1000 | Loss: 0.00120181
Iteration 103/1000 | Loss: 0.00045310
Iteration 104/1000 | Loss: 0.00080505
Iteration 105/1000 | Loss: 0.00037650
Iteration 106/1000 | Loss: 0.00075209
Iteration 107/1000 | Loss: 0.00028336
Iteration 108/1000 | Loss: 0.00060817
Iteration 109/1000 | Loss: 0.00060022
Iteration 110/1000 | Loss: 0.00050639
Iteration 111/1000 | Loss: 0.00049045
Iteration 112/1000 | Loss: 0.00029392
Iteration 113/1000 | Loss: 0.00022707
Iteration 114/1000 | Loss: 0.00032675
Iteration 115/1000 | Loss: 0.00035884
Iteration 116/1000 | Loss: 0.00079085
Iteration 117/1000 | Loss: 0.00029974
Iteration 118/1000 | Loss: 0.00061329
Iteration 119/1000 | Loss: 0.00035531
Iteration 120/1000 | Loss: 0.00038756
Iteration 121/1000 | Loss: 0.00059611
Iteration 122/1000 | Loss: 0.00038858
Iteration 123/1000 | Loss: 0.00032408
Iteration 124/1000 | Loss: 0.00038352
Iteration 125/1000 | Loss: 0.00020667
Iteration 126/1000 | Loss: 0.00018765
Iteration 127/1000 | Loss: 0.00045222
Iteration 128/1000 | Loss: 0.00070599
Iteration 129/1000 | Loss: 0.00040618
Iteration 130/1000 | Loss: 0.00038655
Iteration 131/1000 | Loss: 0.00035744
Iteration 132/1000 | Loss: 0.00045622
Iteration 133/1000 | Loss: 0.00039209
Iteration 134/1000 | Loss: 0.00032376
Iteration 135/1000 | Loss: 0.00004819
Iteration 136/1000 | Loss: 0.00003893
Iteration 137/1000 | Loss: 0.00003599
Iteration 138/1000 | Loss: 0.00003377
Iteration 139/1000 | Loss: 0.00003068
Iteration 140/1000 | Loss: 0.00002827
Iteration 141/1000 | Loss: 0.00002694
Iteration 142/1000 | Loss: 0.00002522
Iteration 143/1000 | Loss: 0.00002374
Iteration 144/1000 | Loss: 0.00002319
Iteration 145/1000 | Loss: 0.00002270
Iteration 146/1000 | Loss: 0.00002240
Iteration 147/1000 | Loss: 0.00002231
Iteration 148/1000 | Loss: 0.00002207
Iteration 149/1000 | Loss: 0.00002185
Iteration 150/1000 | Loss: 0.00002185
Iteration 151/1000 | Loss: 0.00002181
Iteration 152/1000 | Loss: 0.00002181
Iteration 153/1000 | Loss: 0.00002180
Iteration 154/1000 | Loss: 0.00002180
Iteration 155/1000 | Loss: 0.00002179
Iteration 156/1000 | Loss: 0.00002178
Iteration 157/1000 | Loss: 0.00002178
Iteration 158/1000 | Loss: 0.00002178
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002177
Iteration 161/1000 | Loss: 0.00002175
Iteration 162/1000 | Loss: 0.00002175
Iteration 163/1000 | Loss: 0.00002174
Iteration 164/1000 | Loss: 0.00002174
Iteration 165/1000 | Loss: 0.00002174
Iteration 166/1000 | Loss: 0.00002173
Iteration 167/1000 | Loss: 0.00002173
Iteration 168/1000 | Loss: 0.00002172
Iteration 169/1000 | Loss: 0.00002172
Iteration 170/1000 | Loss: 0.00002171
Iteration 171/1000 | Loss: 0.00002171
Iteration 172/1000 | Loss: 0.00002171
Iteration 173/1000 | Loss: 0.00002171
Iteration 174/1000 | Loss: 0.00002171
Iteration 175/1000 | Loss: 0.00002170
Iteration 176/1000 | Loss: 0.00002170
Iteration 177/1000 | Loss: 0.00002170
Iteration 178/1000 | Loss: 0.00002170
Iteration 179/1000 | Loss: 0.00002169
Iteration 180/1000 | Loss: 0.00002169
Iteration 181/1000 | Loss: 0.00002168
Iteration 182/1000 | Loss: 0.00002168
Iteration 183/1000 | Loss: 0.00002168
Iteration 184/1000 | Loss: 0.00002168
Iteration 185/1000 | Loss: 0.00002168
Iteration 186/1000 | Loss: 0.00002167
Iteration 187/1000 | Loss: 0.00002166
Iteration 188/1000 | Loss: 0.00002166
Iteration 189/1000 | Loss: 0.00002166
Iteration 190/1000 | Loss: 0.00002166
Iteration 191/1000 | Loss: 0.00002165
Iteration 192/1000 | Loss: 0.00002165
Iteration 193/1000 | Loss: 0.00002165
Iteration 194/1000 | Loss: 0.00002164
Iteration 195/1000 | Loss: 0.00002164
Iteration 196/1000 | Loss: 0.00002164
Iteration 197/1000 | Loss: 0.00002164
Iteration 198/1000 | Loss: 0.00002163
Iteration 199/1000 | Loss: 0.00002163
Iteration 200/1000 | Loss: 0.00002163
Iteration 201/1000 | Loss: 0.00002163
Iteration 202/1000 | Loss: 0.00002163
Iteration 203/1000 | Loss: 0.00002163
Iteration 204/1000 | Loss: 0.00002163
Iteration 205/1000 | Loss: 0.00002162
Iteration 206/1000 | Loss: 0.00002162
Iteration 207/1000 | Loss: 0.00002162
Iteration 208/1000 | Loss: 0.00002162
Iteration 209/1000 | Loss: 0.00002162
Iteration 210/1000 | Loss: 0.00002162
Iteration 211/1000 | Loss: 0.00002162
Iteration 212/1000 | Loss: 0.00002162
Iteration 213/1000 | Loss: 0.00002162
Iteration 214/1000 | Loss: 0.00002162
Iteration 215/1000 | Loss: 0.00002161
Iteration 216/1000 | Loss: 0.00002161
Iteration 217/1000 | Loss: 0.00002161
Iteration 218/1000 | Loss: 0.00002161
Iteration 219/1000 | Loss: 0.00002161
Iteration 220/1000 | Loss: 0.00002161
Iteration 221/1000 | Loss: 0.00002161
Iteration 222/1000 | Loss: 0.00002161
Iteration 223/1000 | Loss: 0.00002161
Iteration 224/1000 | Loss: 0.00002161
Iteration 225/1000 | Loss: 0.00002160
Iteration 226/1000 | Loss: 0.00002160
Iteration 227/1000 | Loss: 0.00002160
Iteration 228/1000 | Loss: 0.00002160
Iteration 229/1000 | Loss: 0.00002160
Iteration 230/1000 | Loss: 0.00002160
Iteration 231/1000 | Loss: 0.00002159
Iteration 232/1000 | Loss: 0.00002159
Iteration 233/1000 | Loss: 0.00002159
Iteration 234/1000 | Loss: 0.00002159
Iteration 235/1000 | Loss: 0.00002159
Iteration 236/1000 | Loss: 0.00002159
Iteration 237/1000 | Loss: 0.00002159
Iteration 238/1000 | Loss: 0.00002159
Iteration 239/1000 | Loss: 0.00002159
Iteration 240/1000 | Loss: 0.00002159
Iteration 241/1000 | Loss: 0.00002159
Iteration 242/1000 | Loss: 0.00002159
Iteration 243/1000 | Loss: 0.00002159
Iteration 244/1000 | Loss: 0.00002159
Iteration 245/1000 | Loss: 0.00002159
Iteration 246/1000 | Loss: 0.00002158
Iteration 247/1000 | Loss: 0.00002158
Iteration 248/1000 | Loss: 0.00002158
Iteration 249/1000 | Loss: 0.00002158
Iteration 250/1000 | Loss: 0.00002158
Iteration 251/1000 | Loss: 0.00002158
Iteration 252/1000 | Loss: 0.00002158
Iteration 253/1000 | Loss: 0.00002158
Iteration 254/1000 | Loss: 0.00002158
Iteration 255/1000 | Loss: 0.00002158
Iteration 256/1000 | Loss: 0.00002158
Iteration 257/1000 | Loss: 0.00002158
Iteration 258/1000 | Loss: 0.00002158
Iteration 259/1000 | Loss: 0.00002158
Iteration 260/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.1580173779511824e-05, 2.1580173779511824e-05, 2.1580173779511824e-05, 2.1580173779511824e-05, 2.1580173779511824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1580173779511824e-05

Optimization complete. Final v2v error: 2.826343297958374 mm

Highest mean error: 21.75706672668457 mm for frame 137

Lowest mean error: 2.2889251708984375 mm for frame 208

Saving results

Total time: 181.17334485054016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360806
Iteration 2/25 | Loss: 0.00101061
Iteration 3/25 | Loss: 0.00092584
Iteration 4/25 | Loss: 0.00091437
Iteration 5/25 | Loss: 0.00091009
Iteration 6/25 | Loss: 0.00090894
Iteration 7/25 | Loss: 0.00090894
Iteration 8/25 | Loss: 0.00090894
Iteration 9/25 | Loss: 0.00090894
Iteration 10/25 | Loss: 0.00090894
Iteration 11/25 | Loss: 0.00090894
Iteration 12/25 | Loss: 0.00090894
Iteration 13/25 | Loss: 0.00090894
Iteration 14/25 | Loss: 0.00090894
Iteration 15/25 | Loss: 0.00090894
Iteration 16/25 | Loss: 0.00090894
Iteration 17/25 | Loss: 0.00090894
Iteration 18/25 | Loss: 0.00090894
Iteration 19/25 | Loss: 0.00090894
Iteration 20/25 | Loss: 0.00090894
Iteration 21/25 | Loss: 0.00090894
Iteration 22/25 | Loss: 0.00090894
Iteration 23/25 | Loss: 0.00090894
Iteration 24/25 | Loss: 0.00090894
Iteration 25/25 | Loss: 0.00090894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60281861
Iteration 2/25 | Loss: 0.00067853
Iteration 3/25 | Loss: 0.00067853
Iteration 4/25 | Loss: 0.00067853
Iteration 5/25 | Loss: 0.00067853
Iteration 6/25 | Loss: 0.00067852
Iteration 7/25 | Loss: 0.00067852
Iteration 8/25 | Loss: 0.00067852
Iteration 9/25 | Loss: 0.00067852
Iteration 10/25 | Loss: 0.00067852
Iteration 11/25 | Loss: 0.00067852
Iteration 12/25 | Loss: 0.00067852
Iteration 13/25 | Loss: 0.00067852
Iteration 14/25 | Loss: 0.00067852
Iteration 15/25 | Loss: 0.00067852
Iteration 16/25 | Loss: 0.00067852
Iteration 17/25 | Loss: 0.00067852
Iteration 18/25 | Loss: 0.00067852
Iteration 19/25 | Loss: 0.00067852
Iteration 20/25 | Loss: 0.00067852
Iteration 21/25 | Loss: 0.00067852
Iteration 22/25 | Loss: 0.00067852
Iteration 23/25 | Loss: 0.00067852
Iteration 24/25 | Loss: 0.00067852
Iteration 25/25 | Loss: 0.00067852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067852
Iteration 2/1000 | Loss: 0.00001468
Iteration 3/1000 | Loss: 0.00001079
Iteration 4/1000 | Loss: 0.00000996
Iteration 5/1000 | Loss: 0.00000962
Iteration 6/1000 | Loss: 0.00000952
Iteration 7/1000 | Loss: 0.00000927
Iteration 8/1000 | Loss: 0.00000914
Iteration 9/1000 | Loss: 0.00000914
Iteration 10/1000 | Loss: 0.00000914
Iteration 11/1000 | Loss: 0.00000914
Iteration 12/1000 | Loss: 0.00000911
Iteration 13/1000 | Loss: 0.00000911
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000904
Iteration 16/1000 | Loss: 0.00000904
Iteration 17/1000 | Loss: 0.00000903
Iteration 18/1000 | Loss: 0.00000902
Iteration 19/1000 | Loss: 0.00000902
Iteration 20/1000 | Loss: 0.00000902
Iteration 21/1000 | Loss: 0.00000901
Iteration 22/1000 | Loss: 0.00000901
Iteration 23/1000 | Loss: 0.00000900
Iteration 24/1000 | Loss: 0.00000899
Iteration 25/1000 | Loss: 0.00000899
Iteration 26/1000 | Loss: 0.00000898
Iteration 27/1000 | Loss: 0.00000898
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000896
Iteration 33/1000 | Loss: 0.00000895
Iteration 34/1000 | Loss: 0.00000895
Iteration 35/1000 | Loss: 0.00000894
Iteration 36/1000 | Loss: 0.00000894
Iteration 37/1000 | Loss: 0.00000894
Iteration 38/1000 | Loss: 0.00000894
Iteration 39/1000 | Loss: 0.00000894
Iteration 40/1000 | Loss: 0.00000894
Iteration 41/1000 | Loss: 0.00000894
Iteration 42/1000 | Loss: 0.00000893
Iteration 43/1000 | Loss: 0.00000893
Iteration 44/1000 | Loss: 0.00000893
Iteration 45/1000 | Loss: 0.00000893
Iteration 46/1000 | Loss: 0.00000893
Iteration 47/1000 | Loss: 0.00000893
Iteration 48/1000 | Loss: 0.00000892
Iteration 49/1000 | Loss: 0.00000892
Iteration 50/1000 | Loss: 0.00000892
Iteration 51/1000 | Loss: 0.00000891
Iteration 52/1000 | Loss: 0.00000891
Iteration 53/1000 | Loss: 0.00000891
Iteration 54/1000 | Loss: 0.00000891
Iteration 55/1000 | Loss: 0.00000891
Iteration 56/1000 | Loss: 0.00000891
Iteration 57/1000 | Loss: 0.00000891
Iteration 58/1000 | Loss: 0.00000891
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000890
Iteration 61/1000 | Loss: 0.00000890
Iteration 62/1000 | Loss: 0.00000890
Iteration 63/1000 | Loss: 0.00000890
Iteration 64/1000 | Loss: 0.00000890
Iteration 65/1000 | Loss: 0.00000890
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000890
Iteration 69/1000 | Loss: 0.00000890
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000890
Iteration 75/1000 | Loss: 0.00000890
Iteration 76/1000 | Loss: 0.00000889
Iteration 77/1000 | Loss: 0.00000889
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000887
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000885
Iteration 98/1000 | Loss: 0.00000885
Iteration 99/1000 | Loss: 0.00000885
Iteration 100/1000 | Loss: 0.00000885
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000884
Iteration 103/1000 | Loss: 0.00000884
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000883
Iteration 106/1000 | Loss: 0.00000883
Iteration 107/1000 | Loss: 0.00000883
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000883
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [8.826438715914264e-06, 8.826438715914264e-06, 8.826438715914264e-06, 8.826438715914264e-06, 8.826438715914264e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.826438715914264e-06

Optimization complete. Final v2v error: 2.5029654502868652 mm

Highest mean error: 3.141767978668213 mm for frame 139

Lowest mean error: 2.2549667358398438 mm for frame 156

Saving results

Total time: 32.02704572677612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454745
Iteration 2/25 | Loss: 0.00103059
Iteration 3/25 | Loss: 0.00091944
Iteration 4/25 | Loss: 0.00090853
Iteration 5/25 | Loss: 0.00090468
Iteration 6/25 | Loss: 0.00090389
Iteration 7/25 | Loss: 0.00090389
Iteration 8/25 | Loss: 0.00090389
Iteration 9/25 | Loss: 0.00090389
Iteration 10/25 | Loss: 0.00090389
Iteration 11/25 | Loss: 0.00090389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009038866846822202, 0.0009038866846822202, 0.0009038866846822202, 0.0009038866846822202, 0.0009038866846822202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009038866846822202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47508264
Iteration 2/25 | Loss: 0.00058809
Iteration 3/25 | Loss: 0.00058809
Iteration 4/25 | Loss: 0.00058809
Iteration 5/25 | Loss: 0.00058809
Iteration 6/25 | Loss: 0.00058809
Iteration 7/25 | Loss: 0.00058809
Iteration 8/25 | Loss: 0.00058809
Iteration 9/25 | Loss: 0.00058809
Iteration 10/25 | Loss: 0.00058809
Iteration 11/25 | Loss: 0.00058809
Iteration 12/25 | Loss: 0.00058809
Iteration 13/25 | Loss: 0.00058809
Iteration 14/25 | Loss: 0.00058809
Iteration 15/25 | Loss: 0.00058809
Iteration 16/25 | Loss: 0.00058809
Iteration 17/25 | Loss: 0.00058809
Iteration 18/25 | Loss: 0.00058809
Iteration 19/25 | Loss: 0.00058809
Iteration 20/25 | Loss: 0.00058809
Iteration 21/25 | Loss: 0.00058809
Iteration 22/25 | Loss: 0.00058809
Iteration 23/25 | Loss: 0.00058809
Iteration 24/25 | Loss: 0.00058809
Iteration 25/25 | Loss: 0.00058809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058809
Iteration 2/1000 | Loss: 0.00002016
Iteration 3/1000 | Loss: 0.00001162
Iteration 4/1000 | Loss: 0.00001065
Iteration 5/1000 | Loss: 0.00000986
Iteration 6/1000 | Loss: 0.00000958
Iteration 7/1000 | Loss: 0.00000934
Iteration 8/1000 | Loss: 0.00000922
Iteration 9/1000 | Loss: 0.00000921
Iteration 10/1000 | Loss: 0.00000916
Iteration 11/1000 | Loss: 0.00000916
Iteration 12/1000 | Loss: 0.00000916
Iteration 13/1000 | Loss: 0.00000916
Iteration 14/1000 | Loss: 0.00000916
Iteration 15/1000 | Loss: 0.00000916
Iteration 16/1000 | Loss: 0.00000915
Iteration 17/1000 | Loss: 0.00000915
Iteration 18/1000 | Loss: 0.00000914
Iteration 19/1000 | Loss: 0.00000914
Iteration 20/1000 | Loss: 0.00000914
Iteration 21/1000 | Loss: 0.00000913
Iteration 22/1000 | Loss: 0.00000912
Iteration 23/1000 | Loss: 0.00000911
Iteration 24/1000 | Loss: 0.00000911
Iteration 25/1000 | Loss: 0.00000910
Iteration 26/1000 | Loss: 0.00000908
Iteration 27/1000 | Loss: 0.00000907
Iteration 28/1000 | Loss: 0.00000907
Iteration 29/1000 | Loss: 0.00000906
Iteration 30/1000 | Loss: 0.00000906
Iteration 31/1000 | Loss: 0.00000906
Iteration 32/1000 | Loss: 0.00000906
Iteration 33/1000 | Loss: 0.00000906
Iteration 34/1000 | Loss: 0.00000905
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000902
Iteration 39/1000 | Loss: 0.00000901
Iteration 40/1000 | Loss: 0.00000901
Iteration 41/1000 | Loss: 0.00000901
Iteration 42/1000 | Loss: 0.00000899
Iteration 43/1000 | Loss: 0.00000899
Iteration 44/1000 | Loss: 0.00000899
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000899
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000898
Iteration 55/1000 | Loss: 0.00000898
Iteration 56/1000 | Loss: 0.00000898
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000897
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000897
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000895
Iteration 73/1000 | Loss: 0.00000895
Iteration 74/1000 | Loss: 0.00000895
Iteration 75/1000 | Loss: 0.00000895
Iteration 76/1000 | Loss: 0.00000895
Iteration 77/1000 | Loss: 0.00000895
Iteration 78/1000 | Loss: 0.00000894
Iteration 79/1000 | Loss: 0.00000894
Iteration 80/1000 | Loss: 0.00000894
Iteration 81/1000 | Loss: 0.00000894
Iteration 82/1000 | Loss: 0.00000894
Iteration 83/1000 | Loss: 0.00000894
Iteration 84/1000 | Loss: 0.00000894
Iteration 85/1000 | Loss: 0.00000894
Iteration 86/1000 | Loss: 0.00000894
Iteration 87/1000 | Loss: 0.00000894
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000893
Iteration 91/1000 | Loss: 0.00000893
Iteration 92/1000 | Loss: 0.00000893
Iteration 93/1000 | Loss: 0.00000893
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000893
Iteration 96/1000 | Loss: 0.00000893
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000892
Iteration 99/1000 | Loss: 0.00000892
Iteration 100/1000 | Loss: 0.00000892
Iteration 101/1000 | Loss: 0.00000892
Iteration 102/1000 | Loss: 0.00000892
Iteration 103/1000 | Loss: 0.00000892
Iteration 104/1000 | Loss: 0.00000892
Iteration 105/1000 | Loss: 0.00000892
Iteration 106/1000 | Loss: 0.00000892
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000892
Iteration 109/1000 | Loss: 0.00000892
Iteration 110/1000 | Loss: 0.00000892
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000891
Iteration 113/1000 | Loss: 0.00000891
Iteration 114/1000 | Loss: 0.00000891
Iteration 115/1000 | Loss: 0.00000891
Iteration 116/1000 | Loss: 0.00000891
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000891
Iteration 125/1000 | Loss: 0.00000890
Iteration 126/1000 | Loss: 0.00000890
Iteration 127/1000 | Loss: 0.00000890
Iteration 128/1000 | Loss: 0.00000890
Iteration 129/1000 | Loss: 0.00000890
Iteration 130/1000 | Loss: 0.00000890
Iteration 131/1000 | Loss: 0.00000890
Iteration 132/1000 | Loss: 0.00000890
Iteration 133/1000 | Loss: 0.00000890
Iteration 134/1000 | Loss: 0.00000890
Iteration 135/1000 | Loss: 0.00000890
Iteration 136/1000 | Loss: 0.00000890
Iteration 137/1000 | Loss: 0.00000890
Iteration 138/1000 | Loss: 0.00000890
Iteration 139/1000 | Loss: 0.00000890
Iteration 140/1000 | Loss: 0.00000890
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000890
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000889
Iteration 147/1000 | Loss: 0.00000889
Iteration 148/1000 | Loss: 0.00000889
Iteration 149/1000 | Loss: 0.00000889
Iteration 150/1000 | Loss: 0.00000889
Iteration 151/1000 | Loss: 0.00000889
Iteration 152/1000 | Loss: 0.00000889
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000889
Iteration 155/1000 | Loss: 0.00000889
Iteration 156/1000 | Loss: 0.00000889
Iteration 157/1000 | Loss: 0.00000889
Iteration 158/1000 | Loss: 0.00000889
Iteration 159/1000 | Loss: 0.00000889
Iteration 160/1000 | Loss: 0.00000889
Iteration 161/1000 | Loss: 0.00000889
Iteration 162/1000 | Loss: 0.00000889
Iteration 163/1000 | Loss: 0.00000889
Iteration 164/1000 | Loss: 0.00000889
Iteration 165/1000 | Loss: 0.00000889
Iteration 166/1000 | Loss: 0.00000889
Iteration 167/1000 | Loss: 0.00000889
Iteration 168/1000 | Loss: 0.00000889
Iteration 169/1000 | Loss: 0.00000889
Iteration 170/1000 | Loss: 0.00000889
Iteration 171/1000 | Loss: 0.00000889
Iteration 172/1000 | Loss: 0.00000889
Iteration 173/1000 | Loss: 0.00000889
Iteration 174/1000 | Loss: 0.00000889
Iteration 175/1000 | Loss: 0.00000889
Iteration 176/1000 | Loss: 0.00000889
Iteration 177/1000 | Loss: 0.00000889
Iteration 178/1000 | Loss: 0.00000889
Iteration 179/1000 | Loss: 0.00000889
Iteration 180/1000 | Loss: 0.00000889
Iteration 181/1000 | Loss: 0.00000889
Iteration 182/1000 | Loss: 0.00000889
Iteration 183/1000 | Loss: 0.00000889
Iteration 184/1000 | Loss: 0.00000889
Iteration 185/1000 | Loss: 0.00000889
Iteration 186/1000 | Loss: 0.00000889
Iteration 187/1000 | Loss: 0.00000889
Iteration 188/1000 | Loss: 0.00000889
Iteration 189/1000 | Loss: 0.00000889
Iteration 190/1000 | Loss: 0.00000889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [8.88992963155033e-06, 8.88992963155033e-06, 8.88992963155033e-06, 8.88992963155033e-06, 8.88992963155033e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.88992963155033e-06

Optimization complete. Final v2v error: 2.544468879699707 mm

Highest mean error: 2.8405935764312744 mm for frame 30

Lowest mean error: 2.3513035774230957 mm for frame 111

Saving results

Total time: 29.68717360496521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796698
Iteration 2/25 | Loss: 0.00144811
Iteration 3/25 | Loss: 0.00103688
Iteration 4/25 | Loss: 0.00099463
Iteration 5/25 | Loss: 0.00099048
Iteration 6/25 | Loss: 0.00098965
Iteration 7/25 | Loss: 0.00098954
Iteration 8/25 | Loss: 0.00098954
Iteration 9/25 | Loss: 0.00098954
Iteration 10/25 | Loss: 0.00098954
Iteration 11/25 | Loss: 0.00098954
Iteration 12/25 | Loss: 0.00098954
Iteration 13/25 | Loss: 0.00098954
Iteration 14/25 | Loss: 0.00098954
Iteration 15/25 | Loss: 0.00098954
Iteration 16/25 | Loss: 0.00098954
Iteration 17/25 | Loss: 0.00098954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009895431576296687, 0.0009895431576296687, 0.0009895431576296687, 0.0009895431576296687, 0.0009895431576296687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009895431576296687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29898942
Iteration 2/25 | Loss: 0.00045461
Iteration 3/25 | Loss: 0.00045460
Iteration 4/25 | Loss: 0.00045460
Iteration 5/25 | Loss: 0.00045460
Iteration 6/25 | Loss: 0.00045460
Iteration 7/25 | Loss: 0.00045460
Iteration 8/25 | Loss: 0.00045460
Iteration 9/25 | Loss: 0.00045460
Iteration 10/25 | Loss: 0.00045460
Iteration 11/25 | Loss: 0.00045460
Iteration 12/25 | Loss: 0.00045460
Iteration 13/25 | Loss: 0.00045460
Iteration 14/25 | Loss: 0.00045460
Iteration 15/25 | Loss: 0.00045460
Iteration 16/25 | Loss: 0.00045460
Iteration 17/25 | Loss: 0.00045460
Iteration 18/25 | Loss: 0.00045460
Iteration 19/25 | Loss: 0.00045460
Iteration 20/25 | Loss: 0.00045460
Iteration 21/25 | Loss: 0.00045460
Iteration 22/25 | Loss: 0.00045460
Iteration 23/25 | Loss: 0.00045460
Iteration 24/25 | Loss: 0.00045460
Iteration 25/25 | Loss: 0.00045460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045460
Iteration 2/1000 | Loss: 0.00004224
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001101
Iteration 9/1000 | Loss: 0.00001063
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001016
Iteration 12/1000 | Loss: 0.00001014
Iteration 13/1000 | Loss: 0.00001013
Iteration 14/1000 | Loss: 0.00001005
Iteration 15/1000 | Loss: 0.00001005
Iteration 16/1000 | Loss: 0.00000999
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000992
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000992
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000991
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000987
Iteration 37/1000 | Loss: 0.00000987
Iteration 38/1000 | Loss: 0.00000986
Iteration 39/1000 | Loss: 0.00000986
Iteration 40/1000 | Loss: 0.00000986
Iteration 41/1000 | Loss: 0.00000985
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000985
Iteration 44/1000 | Loss: 0.00000985
Iteration 45/1000 | Loss: 0.00000984
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000984
Iteration 49/1000 | Loss: 0.00000983
Iteration 50/1000 | Loss: 0.00000983
Iteration 51/1000 | Loss: 0.00000982
Iteration 52/1000 | Loss: 0.00000982
Iteration 53/1000 | Loss: 0.00000982
Iteration 54/1000 | Loss: 0.00000981
Iteration 55/1000 | Loss: 0.00000981
Iteration 56/1000 | Loss: 0.00000980
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000979
Iteration 59/1000 | Loss: 0.00000979
Iteration 60/1000 | Loss: 0.00000979
Iteration 61/1000 | Loss: 0.00000979
Iteration 62/1000 | Loss: 0.00000979
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000977
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000976
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000975
Iteration 92/1000 | Loss: 0.00000975
Iteration 93/1000 | Loss: 0.00000975
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000975
Iteration 96/1000 | Loss: 0.00000975
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000975
Iteration 100/1000 | Loss: 0.00000975
Iteration 101/1000 | Loss: 0.00000975
Iteration 102/1000 | Loss: 0.00000975
Iteration 103/1000 | Loss: 0.00000975
Iteration 104/1000 | Loss: 0.00000975
Iteration 105/1000 | Loss: 0.00000975
Iteration 106/1000 | Loss: 0.00000975
Iteration 107/1000 | Loss: 0.00000975
Iteration 108/1000 | Loss: 0.00000975
Iteration 109/1000 | Loss: 0.00000975
Iteration 110/1000 | Loss: 0.00000975
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000975
Iteration 116/1000 | Loss: 0.00000975
Iteration 117/1000 | Loss: 0.00000975
Iteration 118/1000 | Loss: 0.00000975
Iteration 119/1000 | Loss: 0.00000975
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [9.75398415903328e-06, 9.75398415903328e-06, 9.75398415903328e-06, 9.75398415903328e-06, 9.75398415903328e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.75398415903328e-06

Optimization complete. Final v2v error: 2.607595205307007 mm

Highest mean error: 3.060858726501465 mm for frame 109

Lowest mean error: 2.245736837387085 mm for frame 17

Saving results

Total time: 33.18610143661499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695189
Iteration 2/25 | Loss: 0.00141547
Iteration 3/25 | Loss: 0.00105206
Iteration 4/25 | Loss: 0.00102133
Iteration 5/25 | Loss: 0.00101947
Iteration 6/25 | Loss: 0.00101623
Iteration 7/25 | Loss: 0.00101466
Iteration 8/25 | Loss: 0.00101441
Iteration 9/25 | Loss: 0.00101435
Iteration 10/25 | Loss: 0.00101435
Iteration 11/25 | Loss: 0.00101435
Iteration 12/25 | Loss: 0.00101435
Iteration 13/25 | Loss: 0.00101435
Iteration 14/25 | Loss: 0.00101435
Iteration 15/25 | Loss: 0.00101435
Iteration 16/25 | Loss: 0.00101435
Iteration 17/25 | Loss: 0.00101435
Iteration 18/25 | Loss: 0.00101434
Iteration 19/25 | Loss: 0.00101434
Iteration 20/25 | Loss: 0.00101434
Iteration 21/25 | Loss: 0.00101434
Iteration 22/25 | Loss: 0.00101434
Iteration 23/25 | Loss: 0.00101434
Iteration 24/25 | Loss: 0.00101434
Iteration 25/25 | Loss: 0.00101434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.67346954
Iteration 2/25 | Loss: 0.00056878
Iteration 3/25 | Loss: 0.00056870
Iteration 4/25 | Loss: 0.00056870
Iteration 5/25 | Loss: 0.00056870
Iteration 6/25 | Loss: 0.00056870
Iteration 7/25 | Loss: 0.00056870
Iteration 8/25 | Loss: 0.00056870
Iteration 9/25 | Loss: 0.00056870
Iteration 10/25 | Loss: 0.00056870
Iteration 11/25 | Loss: 0.00056870
Iteration 12/25 | Loss: 0.00056870
Iteration 13/25 | Loss: 0.00056870
Iteration 14/25 | Loss: 0.00056870
Iteration 15/25 | Loss: 0.00056870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005686998483724892, 0.0005686998483724892, 0.0005686998483724892, 0.0005686998483724892, 0.0005686998483724892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005686998483724892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056870
Iteration 2/1000 | Loss: 0.00002800
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001421
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001366
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.3572907846537419e-05, 1.3572907846537419e-05, 1.3572907846537419e-05, 1.3572907846537419e-05, 1.3572907846537419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3572907846537419e-05

Optimization complete. Final v2v error: 3.018566131591797 mm

Highest mean error: 3.7084805965423584 mm for frame 148

Lowest mean error: 2.5550172328948975 mm for frame 135

Saving results

Total time: 37.38060712814331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915868
Iteration 2/25 | Loss: 0.00238003
Iteration 3/25 | Loss: 0.00176509
Iteration 4/25 | Loss: 0.00141109
Iteration 5/25 | Loss: 0.00124745
Iteration 6/25 | Loss: 0.00123846
Iteration 7/25 | Loss: 0.00123954
Iteration 8/25 | Loss: 0.00119214
Iteration 9/25 | Loss: 0.00114734
Iteration 10/25 | Loss: 0.00112791
Iteration 11/25 | Loss: 0.00112576
Iteration 12/25 | Loss: 0.00112376
Iteration 13/25 | Loss: 0.00112201
Iteration 14/25 | Loss: 0.00112196
Iteration 15/25 | Loss: 0.00112217
Iteration 16/25 | Loss: 0.00112143
Iteration 17/25 | Loss: 0.00112298
Iteration 18/25 | Loss: 0.00112134
Iteration 19/25 | Loss: 0.00112076
Iteration 20/25 | Loss: 0.00111883
Iteration 21/25 | Loss: 0.00111969
Iteration 22/25 | Loss: 0.00111982
Iteration 23/25 | Loss: 0.00112080
Iteration 24/25 | Loss: 0.00111961
Iteration 25/25 | Loss: 0.00112044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09999776
Iteration 2/25 | Loss: 0.00038347
Iteration 3/25 | Loss: 0.00038347
Iteration 4/25 | Loss: 0.00038347
Iteration 5/25 | Loss: 0.00038347
Iteration 6/25 | Loss: 0.00038347
Iteration 7/25 | Loss: 0.00038347
Iteration 8/25 | Loss: 0.00038347
Iteration 9/25 | Loss: 0.00038347
Iteration 10/25 | Loss: 0.00038347
Iteration 11/25 | Loss: 0.00038347
Iteration 12/25 | Loss: 0.00038347
Iteration 13/25 | Loss: 0.00038347
Iteration 14/25 | Loss: 0.00038347
Iteration 15/25 | Loss: 0.00038347
Iteration 16/25 | Loss: 0.00038347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00038346651126630604, 0.00038346651126630604, 0.00038346651126630604, 0.00038346651126630604, 0.00038346651126630604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038346651126630604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038347
Iteration 2/1000 | Loss: 0.00006717
Iteration 3/1000 | Loss: 0.00009057
Iteration 4/1000 | Loss: 0.00008228
Iteration 5/1000 | Loss: 0.00007935
Iteration 6/1000 | Loss: 0.00005009
Iteration 7/1000 | Loss: 0.00007840
Iteration 8/1000 | Loss: 0.00008370
Iteration 9/1000 | Loss: 0.00009167
Iteration 10/1000 | Loss: 0.00007586
Iteration 11/1000 | Loss: 0.00007968
Iteration 12/1000 | Loss: 0.00008792
Iteration 13/1000 | Loss: 0.00006472
Iteration 14/1000 | Loss: 0.00008980
Iteration 15/1000 | Loss: 0.00008714
Iteration 16/1000 | Loss: 0.00008607
Iteration 17/1000 | Loss: 0.00009418
Iteration 18/1000 | Loss: 0.00009547
Iteration 19/1000 | Loss: 0.00009955
Iteration 20/1000 | Loss: 0.00013199
Iteration 21/1000 | Loss: 0.00010809
Iteration 22/1000 | Loss: 0.00009241
Iteration 23/1000 | Loss: 0.00010563
Iteration 24/1000 | Loss: 0.00008750
Iteration 25/1000 | Loss: 0.00006736
Iteration 26/1000 | Loss: 0.00007937
Iteration 27/1000 | Loss: 0.00009689
Iteration 28/1000 | Loss: 0.00009710
Iteration 29/1000 | Loss: 0.00009712
Iteration 30/1000 | Loss: 0.00009456
Iteration 31/1000 | Loss: 0.00009493
Iteration 32/1000 | Loss: 0.00006538
Iteration 33/1000 | Loss: 0.00006283
Iteration 34/1000 | Loss: 0.00005664
Iteration 35/1000 | Loss: 0.00005514
Iteration 36/1000 | Loss: 0.00004694
Iteration 37/1000 | Loss: 0.00005523
Iteration 38/1000 | Loss: 0.00005913
Iteration 39/1000 | Loss: 0.00005694
Iteration 40/1000 | Loss: 0.00006774
Iteration 41/1000 | Loss: 0.00006072
Iteration 42/1000 | Loss: 0.00005399
Iteration 43/1000 | Loss: 0.00004347
Iteration 44/1000 | Loss: 0.00007057
Iteration 45/1000 | Loss: 0.00004997
Iteration 46/1000 | Loss: 0.00006336
Iteration 47/1000 | Loss: 0.00007145
Iteration 48/1000 | Loss: 0.00005747
Iteration 49/1000 | Loss: 0.00006474
Iteration 50/1000 | Loss: 0.00006593
Iteration 51/1000 | Loss: 0.00004938
Iteration 52/1000 | Loss: 0.00006330
Iteration 53/1000 | Loss: 0.00006242
Iteration 54/1000 | Loss: 0.00005237
Iteration 55/1000 | Loss: 0.00005769
Iteration 56/1000 | Loss: 0.00006833
Iteration 57/1000 | Loss: 0.00006517
Iteration 58/1000 | Loss: 0.00005556
Iteration 59/1000 | Loss: 0.00006027
Iteration 60/1000 | Loss: 0.00007564
Iteration 61/1000 | Loss: 0.00006987
Iteration 62/1000 | Loss: 0.00006430
Iteration 63/1000 | Loss: 0.00006762
Iteration 64/1000 | Loss: 0.00008152
Iteration 65/1000 | Loss: 0.00006427
Iteration 66/1000 | Loss: 0.00004967
Iteration 67/1000 | Loss: 0.00005972
Iteration 68/1000 | Loss: 0.00005558
Iteration 69/1000 | Loss: 0.00006870
Iteration 70/1000 | Loss: 0.00003561
Iteration 71/1000 | Loss: 0.00005943
Iteration 72/1000 | Loss: 0.00002866
Iteration 73/1000 | Loss: 0.00006407
Iteration 74/1000 | Loss: 0.00005457
Iteration 75/1000 | Loss: 0.00006887
Iteration 76/1000 | Loss: 0.00006282
Iteration 77/1000 | Loss: 0.00008482
Iteration 78/1000 | Loss: 0.00005287
Iteration 79/1000 | Loss: 0.00004163
Iteration 80/1000 | Loss: 0.00003166
Iteration 81/1000 | Loss: 0.00003452
Iteration 82/1000 | Loss: 0.00002947
Iteration 83/1000 | Loss: 0.00002528
Iteration 84/1000 | Loss: 0.00004177
Iteration 85/1000 | Loss: 0.00006245
Iteration 86/1000 | Loss: 0.00006195
Iteration 87/1000 | Loss: 0.00003506
Iteration 88/1000 | Loss: 0.00002610
Iteration 89/1000 | Loss: 0.00004507
Iteration 90/1000 | Loss: 0.00006261
Iteration 91/1000 | Loss: 0.00008764
Iteration 92/1000 | Loss: 0.00004572
Iteration 93/1000 | Loss: 0.00003473
Iteration 94/1000 | Loss: 0.00005843
Iteration 95/1000 | Loss: 0.00003486
Iteration 96/1000 | Loss: 0.00003132
Iteration 97/1000 | Loss: 0.00004799
Iteration 98/1000 | Loss: 0.00003431
Iteration 99/1000 | Loss: 0.00004505
Iteration 100/1000 | Loss: 0.00004833
Iteration 101/1000 | Loss: 0.00004679
Iteration 102/1000 | Loss: 0.00005549
Iteration 103/1000 | Loss: 0.00004262
Iteration 104/1000 | Loss: 0.00003225
Iteration 105/1000 | Loss: 0.00005316
Iteration 106/1000 | Loss: 0.00005187
Iteration 107/1000 | Loss: 0.00002970
Iteration 108/1000 | Loss: 0.00003030
Iteration 109/1000 | Loss: 0.00002313
Iteration 110/1000 | Loss: 0.00003069
Iteration 111/1000 | Loss: 0.00003013
Iteration 112/1000 | Loss: 0.00003271
Iteration 113/1000 | Loss: 0.00002493
Iteration 114/1000 | Loss: 0.00002601
Iteration 115/1000 | Loss: 0.00003024
Iteration 116/1000 | Loss: 0.00003143
Iteration 117/1000 | Loss: 0.00002961
Iteration 118/1000 | Loss: 0.00003110
Iteration 119/1000 | Loss: 0.00003008
Iteration 120/1000 | Loss: 0.00003119
Iteration 121/1000 | Loss: 0.00002965
Iteration 122/1000 | Loss: 0.00005010
Iteration 123/1000 | Loss: 0.00004372
Iteration 124/1000 | Loss: 0.00004523
Iteration 125/1000 | Loss: 0.00003680
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00002272
Iteration 128/1000 | Loss: 0.00002917
Iteration 129/1000 | Loss: 0.00003114
Iteration 130/1000 | Loss: 0.00003262
Iteration 131/1000 | Loss: 0.00002277
Iteration 132/1000 | Loss: 0.00002365
Iteration 133/1000 | Loss: 0.00002231
Iteration 134/1000 | Loss: 0.00002510
Iteration 135/1000 | Loss: 0.00002257
Iteration 136/1000 | Loss: 0.00002735
Iteration 137/1000 | Loss: 0.00003147
Iteration 138/1000 | Loss: 0.00002908
Iteration 139/1000 | Loss: 0.00002368
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002630
Iteration 142/1000 | Loss: 0.00005052
Iteration 143/1000 | Loss: 0.00003334
Iteration 144/1000 | Loss: 0.00004307
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002157
Iteration 147/1000 | Loss: 0.00002698
Iteration 148/1000 | Loss: 0.00002844
Iteration 149/1000 | Loss: 0.00003062
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00003140
Iteration 152/1000 | Loss: 0.00002629
Iteration 153/1000 | Loss: 0.00002789
Iteration 154/1000 | Loss: 0.00002924
Iteration 155/1000 | Loss: 0.00002724
Iteration 156/1000 | Loss: 0.00003200
Iteration 157/1000 | Loss: 0.00002838
Iteration 158/1000 | Loss: 0.00004624
Iteration 159/1000 | Loss: 0.00003336
Iteration 160/1000 | Loss: 0.00003780
Iteration 161/1000 | Loss: 0.00003543
Iteration 162/1000 | Loss: 0.00002923
Iteration 163/1000 | Loss: 0.00002803
Iteration 164/1000 | Loss: 0.00002112
Iteration 165/1000 | Loss: 0.00002913
Iteration 166/1000 | Loss: 0.00002704
Iteration 167/1000 | Loss: 0.00003700
Iteration 168/1000 | Loss: 0.00003345
Iteration 169/1000 | Loss: 0.00003401
Iteration 170/1000 | Loss: 0.00003050
Iteration 171/1000 | Loss: 0.00003351
Iteration 172/1000 | Loss: 0.00003734
Iteration 173/1000 | Loss: 0.00004257
Iteration 174/1000 | Loss: 0.00003574
Iteration 175/1000 | Loss: 0.00003835
Iteration 176/1000 | Loss: 0.00003731
Iteration 177/1000 | Loss: 0.00005137
Iteration 178/1000 | Loss: 0.00003367
Iteration 179/1000 | Loss: 0.00003232
Iteration 180/1000 | Loss: 0.00003446
Iteration 181/1000 | Loss: 0.00002230
Iteration 182/1000 | Loss: 0.00002735
Iteration 183/1000 | Loss: 0.00002388
Iteration 184/1000 | Loss: 0.00003421
Iteration 185/1000 | Loss: 0.00004852
Iteration 186/1000 | Loss: 0.00004207
Iteration 187/1000 | Loss: 0.00004569
Iteration 188/1000 | Loss: 0.00003232
Iteration 189/1000 | Loss: 0.00003523
Iteration 190/1000 | Loss: 0.00002604
Iteration 191/1000 | Loss: 0.00002651
Iteration 192/1000 | Loss: 0.00002794
Iteration 193/1000 | Loss: 0.00002687
Iteration 194/1000 | Loss: 0.00002458
Iteration 195/1000 | Loss: 0.00002066
Iteration 196/1000 | Loss: 0.00002626
Iteration 197/1000 | Loss: 0.00003481
Iteration 198/1000 | Loss: 0.00003756
Iteration 199/1000 | Loss: 0.00003015
Iteration 200/1000 | Loss: 0.00002777
Iteration 201/1000 | Loss: 0.00003579
Iteration 202/1000 | Loss: 0.00002993
Iteration 203/1000 | Loss: 0.00004005
Iteration 204/1000 | Loss: 0.00003806
Iteration 205/1000 | Loss: 0.00004911
Iteration 206/1000 | Loss: 0.00003720
Iteration 207/1000 | Loss: 0.00004464
Iteration 208/1000 | Loss: 0.00003669
Iteration 209/1000 | Loss: 0.00002907
Iteration 210/1000 | Loss: 0.00002937
Iteration 211/1000 | Loss: 0.00002523
Iteration 212/1000 | Loss: 0.00002818
Iteration 213/1000 | Loss: 0.00003456
Iteration 214/1000 | Loss: 0.00003151
Iteration 215/1000 | Loss: 0.00002567
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002748
Iteration 218/1000 | Loss: 0.00002461
Iteration 219/1000 | Loss: 0.00002930
Iteration 220/1000 | Loss: 0.00002047
Iteration 221/1000 | Loss: 0.00003185
Iteration 222/1000 | Loss: 0.00002388
Iteration 223/1000 | Loss: 0.00002629
Iteration 224/1000 | Loss: 0.00002689
Iteration 225/1000 | Loss: 0.00002203
Iteration 226/1000 | Loss: 0.00002371
Iteration 227/1000 | Loss: 0.00002754
Iteration 228/1000 | Loss: 0.00002211
Iteration 229/1000 | Loss: 0.00002760
Iteration 230/1000 | Loss: 0.00003204
Iteration 231/1000 | Loss: 0.00003036
Iteration 232/1000 | Loss: 0.00003157
Iteration 233/1000 | Loss: 0.00002972
Iteration 234/1000 | Loss: 0.00003105
Iteration 235/1000 | Loss: 0.00002872
Iteration 236/1000 | Loss: 0.00003301
Iteration 237/1000 | Loss: 0.00003255
Iteration 238/1000 | Loss: 0.00002402
Iteration 239/1000 | Loss: 0.00002928
Iteration 240/1000 | Loss: 0.00003879
Iteration 241/1000 | Loss: 0.00003564
Iteration 242/1000 | Loss: 0.00002888
Iteration 243/1000 | Loss: 0.00001976
Iteration 244/1000 | Loss: 0.00001873
Iteration 245/1000 | Loss: 0.00001796
Iteration 246/1000 | Loss: 0.00001761
Iteration 247/1000 | Loss: 0.00001737
Iteration 248/1000 | Loss: 0.00001728
Iteration 249/1000 | Loss: 0.00001707
Iteration 250/1000 | Loss: 0.00001703
Iteration 251/1000 | Loss: 0.00001695
Iteration 252/1000 | Loss: 0.00001685
Iteration 253/1000 | Loss: 0.00001683
Iteration 254/1000 | Loss: 0.00001680
Iteration 255/1000 | Loss: 0.00001660
Iteration 256/1000 | Loss: 0.00001643
Iteration 257/1000 | Loss: 0.00001625
Iteration 258/1000 | Loss: 0.00001617
Iteration 259/1000 | Loss: 0.00001615
Iteration 260/1000 | Loss: 0.00001614
Iteration 261/1000 | Loss: 0.00001611
Iteration 262/1000 | Loss: 0.00001611
Iteration 263/1000 | Loss: 0.00001609
Iteration 264/1000 | Loss: 0.00001609
Iteration 265/1000 | Loss: 0.00001609
Iteration 266/1000 | Loss: 0.00001609
Iteration 267/1000 | Loss: 0.00001608
Iteration 268/1000 | Loss: 0.00001608
Iteration 269/1000 | Loss: 0.00001608
Iteration 270/1000 | Loss: 0.00001608
Iteration 271/1000 | Loss: 0.00001608
Iteration 272/1000 | Loss: 0.00001608
Iteration 273/1000 | Loss: 0.00001608
Iteration 274/1000 | Loss: 0.00001607
Iteration 275/1000 | Loss: 0.00001607
Iteration 276/1000 | Loss: 0.00001607
Iteration 277/1000 | Loss: 0.00001607
Iteration 278/1000 | Loss: 0.00001606
Iteration 279/1000 | Loss: 0.00001606
Iteration 280/1000 | Loss: 0.00001606
Iteration 281/1000 | Loss: 0.00001606
Iteration 282/1000 | Loss: 0.00001606
Iteration 283/1000 | Loss: 0.00001606
Iteration 284/1000 | Loss: 0.00001605
Iteration 285/1000 | Loss: 0.00001605
Iteration 286/1000 | Loss: 0.00001605
Iteration 287/1000 | Loss: 0.00001605
Iteration 288/1000 | Loss: 0.00001604
Iteration 289/1000 | Loss: 0.00001603
Iteration 290/1000 | Loss: 0.00001601
Iteration 291/1000 | Loss: 0.00001601
Iteration 292/1000 | Loss: 0.00001601
Iteration 293/1000 | Loss: 0.00001601
Iteration 294/1000 | Loss: 0.00001601
Iteration 295/1000 | Loss: 0.00001601
Iteration 296/1000 | Loss: 0.00001601
Iteration 297/1000 | Loss: 0.00001601
Iteration 298/1000 | Loss: 0.00001600
Iteration 299/1000 | Loss: 0.00001600
Iteration 300/1000 | Loss: 0.00001600
Iteration 301/1000 | Loss: 0.00001600
Iteration 302/1000 | Loss: 0.00001599
Iteration 303/1000 | Loss: 0.00001599
Iteration 304/1000 | Loss: 0.00001599
Iteration 305/1000 | Loss: 0.00001599
Iteration 306/1000 | Loss: 0.00001599
Iteration 307/1000 | Loss: 0.00001599
Iteration 308/1000 | Loss: 0.00001599
Iteration 309/1000 | Loss: 0.00001598
Iteration 310/1000 | Loss: 0.00001598
Iteration 311/1000 | Loss: 0.00001598
Iteration 312/1000 | Loss: 0.00001598
Iteration 313/1000 | Loss: 0.00001598
Iteration 314/1000 | Loss: 0.00001597
Iteration 315/1000 | Loss: 0.00001597
Iteration 316/1000 | Loss: 0.00001597
Iteration 317/1000 | Loss: 0.00001596
Iteration 318/1000 | Loss: 0.00001596
Iteration 319/1000 | Loss: 0.00001596
Iteration 320/1000 | Loss: 0.00001596
Iteration 321/1000 | Loss: 0.00001596
Iteration 322/1000 | Loss: 0.00001596
Iteration 323/1000 | Loss: 0.00001596
Iteration 324/1000 | Loss: 0.00001596
Iteration 325/1000 | Loss: 0.00001596
Iteration 326/1000 | Loss: 0.00001596
Iteration 327/1000 | Loss: 0.00001595
Iteration 328/1000 | Loss: 0.00001595
Iteration 329/1000 | Loss: 0.00001595
Iteration 330/1000 | Loss: 0.00001595
Iteration 331/1000 | Loss: 0.00001595
Iteration 332/1000 | Loss: 0.00001595
Iteration 333/1000 | Loss: 0.00001595
Iteration 334/1000 | Loss: 0.00001595
Iteration 335/1000 | Loss: 0.00001595
Iteration 336/1000 | Loss: 0.00001595
Iteration 337/1000 | Loss: 0.00001595
Iteration 338/1000 | Loss: 0.00001595
Iteration 339/1000 | Loss: 0.00001595
Iteration 340/1000 | Loss: 0.00001595
Iteration 341/1000 | Loss: 0.00001594
Iteration 342/1000 | Loss: 0.00001594
Iteration 343/1000 | Loss: 0.00001594
Iteration 344/1000 | Loss: 0.00001594
Iteration 345/1000 | Loss: 0.00001594
Iteration 346/1000 | Loss: 0.00001594
Iteration 347/1000 | Loss: 0.00001594
Iteration 348/1000 | Loss: 0.00001594
Iteration 349/1000 | Loss: 0.00001594
Iteration 350/1000 | Loss: 0.00001594
Iteration 351/1000 | Loss: 0.00001594
Iteration 352/1000 | Loss: 0.00001594
Iteration 353/1000 | Loss: 0.00001594
Iteration 354/1000 | Loss: 0.00001594
Iteration 355/1000 | Loss: 0.00001594
Iteration 356/1000 | Loss: 0.00001594
Iteration 357/1000 | Loss: 0.00001593
Iteration 358/1000 | Loss: 0.00001593
Iteration 359/1000 | Loss: 0.00001593
Iteration 360/1000 | Loss: 0.00001593
Iteration 361/1000 | Loss: 0.00001593
Iteration 362/1000 | Loss: 0.00001593
Iteration 363/1000 | Loss: 0.00001593
Iteration 364/1000 | Loss: 0.00001593
Iteration 365/1000 | Loss: 0.00001593
Iteration 366/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [1.5934734619804658e-05, 1.5934734619804658e-05, 1.5934734619804658e-05, 1.5934734619804658e-05, 1.5934734619804658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5934734619804658e-05

Optimization complete. Final v2v error: 3.388286590576172 mm

Highest mean error: 4.388682842254639 mm for frame 16

Lowest mean error: 3.2103793621063232 mm for frame 144

Saving results

Total time: 462.25088906288147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565600
Iteration 2/25 | Loss: 0.00136888
Iteration 3/25 | Loss: 0.00111640
Iteration 4/25 | Loss: 0.00108442
Iteration 5/25 | Loss: 0.00108010
Iteration 6/25 | Loss: 0.00107991
Iteration 7/25 | Loss: 0.00107991
Iteration 8/25 | Loss: 0.00107991
Iteration 9/25 | Loss: 0.00107991
Iteration 10/25 | Loss: 0.00107991
Iteration 11/25 | Loss: 0.00107991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010799075243994594, 0.0010799075243994594, 0.0010799075243994594, 0.0010799075243994594, 0.0010799075243994594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010799075243994594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34763622
Iteration 2/25 | Loss: 0.00074583
Iteration 3/25 | Loss: 0.00074583
Iteration 4/25 | Loss: 0.00074583
Iteration 5/25 | Loss: 0.00074583
Iteration 6/25 | Loss: 0.00074583
Iteration 7/25 | Loss: 0.00074583
Iteration 8/25 | Loss: 0.00074583
Iteration 9/25 | Loss: 0.00074583
Iteration 10/25 | Loss: 0.00074583
Iteration 11/25 | Loss: 0.00074583
Iteration 12/25 | Loss: 0.00074583
Iteration 13/25 | Loss: 0.00074583
Iteration 14/25 | Loss: 0.00074583
Iteration 15/25 | Loss: 0.00074583
Iteration 16/25 | Loss: 0.00074583
Iteration 17/25 | Loss: 0.00074583
Iteration 18/25 | Loss: 0.00074583
Iteration 19/25 | Loss: 0.00074583
Iteration 20/25 | Loss: 0.00074583
Iteration 21/25 | Loss: 0.00074583
Iteration 22/25 | Loss: 0.00074583
Iteration 23/25 | Loss: 0.00074583
Iteration 24/25 | Loss: 0.00074583
Iteration 25/25 | Loss: 0.00074583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007458289037458599, 0.0007458289037458599, 0.0007458289037458599, 0.0007458289037458599, 0.0007458289037458599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007458289037458599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074583
Iteration 2/1000 | Loss: 0.00004552
Iteration 3/1000 | Loss: 0.00003522
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003111
Iteration 6/1000 | Loss: 0.00003046
Iteration 7/1000 | Loss: 0.00002974
Iteration 8/1000 | Loss: 0.00002932
Iteration 9/1000 | Loss: 0.00002900
Iteration 10/1000 | Loss: 0.00002887
Iteration 11/1000 | Loss: 0.00002870
Iteration 12/1000 | Loss: 0.00002867
Iteration 13/1000 | Loss: 0.00002860
Iteration 14/1000 | Loss: 0.00002860
Iteration 15/1000 | Loss: 0.00002860
Iteration 16/1000 | Loss: 0.00002857
Iteration 17/1000 | Loss: 0.00002857
Iteration 18/1000 | Loss: 0.00002857
Iteration 19/1000 | Loss: 0.00002857
Iteration 20/1000 | Loss: 0.00002857
Iteration 21/1000 | Loss: 0.00002857
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002857
Iteration 24/1000 | Loss: 0.00002857
Iteration 25/1000 | Loss: 0.00002857
Iteration 26/1000 | Loss: 0.00002857
Iteration 27/1000 | Loss: 0.00002857
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002857
Iteration 30/1000 | Loss: 0.00002857
Iteration 31/1000 | Loss: 0.00002857
Iteration 32/1000 | Loss: 0.00002857
Iteration 33/1000 | Loss: 0.00002857
Iteration 34/1000 | Loss: 0.00002857
Iteration 35/1000 | Loss: 0.00002857
Iteration 36/1000 | Loss: 0.00002857
Iteration 37/1000 | Loss: 0.00002857
Iteration 38/1000 | Loss: 0.00002857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [2.8566566470544785e-05, 2.8566566470544785e-05, 2.8566566470544785e-05, 2.8566566470544785e-05, 2.8566566470544785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8566566470544785e-05

Optimization complete. Final v2v error: 4.207648277282715 mm

Highest mean error: 4.886398792266846 mm for frame 88

Lowest mean error: 3.753624200820923 mm for frame 120

Saving results

Total time: 28.97060489654541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467652
Iteration 2/25 | Loss: 0.00112529
Iteration 3/25 | Loss: 0.00101848
Iteration 4/25 | Loss: 0.00100055
Iteration 5/25 | Loss: 0.00099335
Iteration 6/25 | Loss: 0.00099171
Iteration 7/25 | Loss: 0.00099171
Iteration 8/25 | Loss: 0.00099171
Iteration 9/25 | Loss: 0.00099171
Iteration 10/25 | Loss: 0.00099171
Iteration 11/25 | Loss: 0.00099171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000991714303381741, 0.000991714303381741, 0.000991714303381741, 0.000991714303381741, 0.000991714303381741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991714303381741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.75142670
Iteration 2/25 | Loss: 0.00075579
Iteration 3/25 | Loss: 0.00075578
Iteration 4/25 | Loss: 0.00075578
Iteration 5/25 | Loss: 0.00075578
Iteration 6/25 | Loss: 0.00075578
Iteration 7/25 | Loss: 0.00075578
Iteration 8/25 | Loss: 0.00075578
Iteration 9/25 | Loss: 0.00075578
Iteration 10/25 | Loss: 0.00075578
Iteration 11/25 | Loss: 0.00075578
Iteration 12/25 | Loss: 0.00075578
Iteration 13/25 | Loss: 0.00075578
Iteration 14/25 | Loss: 0.00075578
Iteration 15/25 | Loss: 0.00075578
Iteration 16/25 | Loss: 0.00075578
Iteration 17/25 | Loss: 0.00075578
Iteration 18/25 | Loss: 0.00075578
Iteration 19/25 | Loss: 0.00075578
Iteration 20/25 | Loss: 0.00075578
Iteration 21/25 | Loss: 0.00075578
Iteration 22/25 | Loss: 0.00075578
Iteration 23/25 | Loss: 0.00075578
Iteration 24/25 | Loss: 0.00075578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000755778222810477, 0.000755778222810477, 0.000755778222810477, 0.000755778222810477, 0.000755778222810477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000755778222810477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075578
Iteration 2/1000 | Loss: 0.00002573
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001716
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00001681
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001652
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001648
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001648
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001646
Iteration 92/1000 | Loss: 0.00001646
Iteration 93/1000 | Loss: 0.00001646
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001646
Iteration 96/1000 | Loss: 0.00001646
Iteration 97/1000 | Loss: 0.00001646
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001645
Iteration 100/1000 | Loss: 0.00001645
Iteration 101/1000 | Loss: 0.00001645
Iteration 102/1000 | Loss: 0.00001645
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001645
Iteration 107/1000 | Loss: 0.00001645
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001645
Iteration 112/1000 | Loss: 0.00001645
Iteration 113/1000 | Loss: 0.00001645
Iteration 114/1000 | Loss: 0.00001645
Iteration 115/1000 | Loss: 0.00001645
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.6447091184090823e-05, 1.6447091184090823e-05, 1.6447091184090823e-05, 1.6447091184090823e-05, 1.6447091184090823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6447091184090823e-05

Optimization complete. Final v2v error: 3.4341509342193604 mm

Highest mean error: 3.8594863414764404 mm for frame 229

Lowest mean error: 3.0381968021392822 mm for frame 18

Saving results

Total time: 35.51035737991333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605766
Iteration 2/25 | Loss: 0.00117468
Iteration 3/25 | Loss: 0.00104624
Iteration 4/25 | Loss: 0.00101167
Iteration 5/25 | Loss: 0.00100269
Iteration 6/25 | Loss: 0.00100030
Iteration 7/25 | Loss: 0.00100030
Iteration 8/25 | Loss: 0.00100030
Iteration 9/25 | Loss: 0.00100030
Iteration 10/25 | Loss: 0.00100030
Iteration 11/25 | Loss: 0.00100030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010003024945035577, 0.0010003024945035577, 0.0010003024945035577, 0.0010003024945035577, 0.0010003024945035577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010003024945035577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63934374
Iteration 2/25 | Loss: 0.00075041
Iteration 3/25 | Loss: 0.00075040
Iteration 4/25 | Loss: 0.00075040
Iteration 5/25 | Loss: 0.00075040
Iteration 6/25 | Loss: 0.00075040
Iteration 7/25 | Loss: 0.00075040
Iteration 8/25 | Loss: 0.00075040
Iteration 9/25 | Loss: 0.00075040
Iteration 10/25 | Loss: 0.00075040
Iteration 11/25 | Loss: 0.00075040
Iteration 12/25 | Loss: 0.00075040
Iteration 13/25 | Loss: 0.00075040
Iteration 14/25 | Loss: 0.00075040
Iteration 15/25 | Loss: 0.00075040
Iteration 16/25 | Loss: 0.00075040
Iteration 17/25 | Loss: 0.00075040
Iteration 18/25 | Loss: 0.00075040
Iteration 19/25 | Loss: 0.00075040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007503997185267508, 0.0007503997185267508, 0.0007503997185267508, 0.0007503997185267508, 0.0007503997185267508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007503997185267508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075040
Iteration 2/1000 | Loss: 0.00002651
Iteration 3/1000 | Loss: 0.00002177
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001921
Iteration 6/1000 | Loss: 0.00001858
Iteration 7/1000 | Loss: 0.00001828
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001784
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001775
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001772
Iteration 23/1000 | Loss: 0.00001771
Iteration 24/1000 | Loss: 0.00001771
Iteration 25/1000 | Loss: 0.00001771
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001770
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001757
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001756
Iteration 79/1000 | Loss: 0.00001756
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001753
Iteration 101/1000 | Loss: 0.00001753
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001753
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001753
Iteration 129/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.7530757759232074e-05, 1.7530757759232074e-05, 1.7530757759232074e-05, 1.7530757759232074e-05, 1.7530757759232074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7530757759232074e-05

Optimization complete. Final v2v error: 3.5390310287475586 mm

Highest mean error: 4.137506008148193 mm for frame 67

Lowest mean error: 3.1774072647094727 mm for frame 31

Saving results

Total time: 35.493610858917236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954188
Iteration 2/25 | Loss: 0.00305444
Iteration 3/25 | Loss: 0.00226329
Iteration 4/25 | Loss: 0.00179193
Iteration 5/25 | Loss: 0.00159765
Iteration 6/25 | Loss: 0.00159124
Iteration 7/25 | Loss: 0.00161033
Iteration 8/25 | Loss: 0.00152504
Iteration 9/25 | Loss: 0.00144459
Iteration 10/25 | Loss: 0.00136357
Iteration 11/25 | Loss: 0.00134149
Iteration 12/25 | Loss: 0.00133038
Iteration 13/25 | Loss: 0.00136169
Iteration 14/25 | Loss: 0.00132728
Iteration 15/25 | Loss: 0.00131477
Iteration 16/25 | Loss: 0.00131934
Iteration 17/25 | Loss: 0.00130914
Iteration 18/25 | Loss: 0.00130771
Iteration 19/25 | Loss: 0.00130838
Iteration 20/25 | Loss: 0.00130617
Iteration 21/25 | Loss: 0.00130366
Iteration 22/25 | Loss: 0.00130255
Iteration 23/25 | Loss: 0.00130070
Iteration 24/25 | Loss: 0.00129822
Iteration 25/25 | Loss: 0.00129754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27566862
Iteration 2/25 | Loss: 0.00466163
Iteration 3/25 | Loss: 0.00362204
Iteration 4/25 | Loss: 0.00362204
Iteration 5/25 | Loss: 0.00362204
Iteration 6/25 | Loss: 0.00362204
Iteration 7/25 | Loss: 0.00362204
Iteration 8/25 | Loss: 0.00362204
Iteration 9/25 | Loss: 0.00362204
Iteration 10/25 | Loss: 0.00362204
Iteration 11/25 | Loss: 0.00362204
Iteration 12/25 | Loss: 0.00362204
Iteration 13/25 | Loss: 0.00362204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003622036660090089, 0.003622036660090089, 0.003622036660090089, 0.003622036660090089, 0.003622036660090089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003622036660090089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362204
Iteration 2/1000 | Loss: 0.00076771
Iteration 3/1000 | Loss: 0.00064096
Iteration 4/1000 | Loss: 0.00087656
Iteration 5/1000 | Loss: 0.00361323
Iteration 6/1000 | Loss: 0.01057392
Iteration 7/1000 | Loss: 0.00063395
Iteration 8/1000 | Loss: 0.01011445
Iteration 9/1000 | Loss: 0.00117082
Iteration 10/1000 | Loss: 0.00301337
Iteration 11/1000 | Loss: 0.00429693
Iteration 12/1000 | Loss: 0.00253965
Iteration 13/1000 | Loss: 0.00206981
Iteration 14/1000 | Loss: 0.00173114
Iteration 15/1000 | Loss: 0.00092590
Iteration 16/1000 | Loss: 0.00049602
Iteration 17/1000 | Loss: 0.00022792
Iteration 18/1000 | Loss: 0.00034746
Iteration 19/1000 | Loss: 0.00018922
Iteration 20/1000 | Loss: 0.00057892
Iteration 21/1000 | Loss: 0.00017306
Iteration 22/1000 | Loss: 0.00015362
Iteration 23/1000 | Loss: 0.00014294
Iteration 24/1000 | Loss: 0.00047550
Iteration 25/1000 | Loss: 0.00114768
Iteration 26/1000 | Loss: 0.00047236
Iteration 27/1000 | Loss: 0.00065435
Iteration 28/1000 | Loss: 0.00085549
Iteration 29/1000 | Loss: 0.00048764
Iteration 30/1000 | Loss: 0.00022584
Iteration 31/1000 | Loss: 0.00102493
Iteration 32/1000 | Loss: 0.00087664
Iteration 33/1000 | Loss: 0.00112967
Iteration 34/1000 | Loss: 0.00082775
Iteration 35/1000 | Loss: 0.00080840
Iteration 36/1000 | Loss: 0.00060084
Iteration 37/1000 | Loss: 0.00058937
Iteration 38/1000 | Loss: 0.00035271
Iteration 39/1000 | Loss: 0.00012972
Iteration 40/1000 | Loss: 0.00012480
Iteration 41/1000 | Loss: 0.00051662
Iteration 42/1000 | Loss: 0.00036293
Iteration 43/1000 | Loss: 0.00039358
Iteration 44/1000 | Loss: 0.00022610
Iteration 45/1000 | Loss: 0.00044597
Iteration 46/1000 | Loss: 0.00022454
Iteration 47/1000 | Loss: 0.00037476
Iteration 48/1000 | Loss: 0.00282050
Iteration 49/1000 | Loss: 0.01623751
Iteration 50/1000 | Loss: 0.00669048
Iteration 51/1000 | Loss: 0.00648045
Iteration 52/1000 | Loss: 0.00823983
Iteration 53/1000 | Loss: 0.00324525
Iteration 54/1000 | Loss: 0.00191113
Iteration 55/1000 | Loss: 0.00333937
Iteration 56/1000 | Loss: 0.00068295
Iteration 57/1000 | Loss: 0.00144858
Iteration 58/1000 | Loss: 0.00069387
Iteration 59/1000 | Loss: 0.00082258
Iteration 60/1000 | Loss: 0.00047877
Iteration 61/1000 | Loss: 0.00075220
Iteration 62/1000 | Loss: 0.00020706
Iteration 63/1000 | Loss: 0.00037691
Iteration 64/1000 | Loss: 0.00008293
Iteration 65/1000 | Loss: 0.00015869
Iteration 66/1000 | Loss: 0.00005601
Iteration 67/1000 | Loss: 0.00005242
Iteration 68/1000 | Loss: 0.00005305
Iteration 69/1000 | Loss: 0.00003659
Iteration 70/1000 | Loss: 0.00007876
Iteration 71/1000 | Loss: 0.00003073
Iteration 72/1000 | Loss: 0.00007758
Iteration 73/1000 | Loss: 0.00002819
Iteration 74/1000 | Loss: 0.00002705
Iteration 75/1000 | Loss: 0.00002602
Iteration 76/1000 | Loss: 0.00005668
Iteration 77/1000 | Loss: 0.00002492
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002297
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002242
Iteration 83/1000 | Loss: 0.00002217
Iteration 84/1000 | Loss: 0.00002212
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002183
Iteration 91/1000 | Loss: 0.00002182
Iteration 92/1000 | Loss: 0.00002182
Iteration 93/1000 | Loss: 0.00002182
Iteration 94/1000 | Loss: 0.00002182
Iteration 95/1000 | Loss: 0.00002182
Iteration 96/1000 | Loss: 0.00002182
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002181
Iteration 99/1000 | Loss: 0.00002181
Iteration 100/1000 | Loss: 0.00002179
Iteration 101/1000 | Loss: 0.00002174
Iteration 102/1000 | Loss: 0.00002174
Iteration 103/1000 | Loss: 0.00002174
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00002173
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002171
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002169
Iteration 111/1000 | Loss: 0.00002169
Iteration 112/1000 | Loss: 0.00002169
Iteration 113/1000 | Loss: 0.00002168
Iteration 114/1000 | Loss: 0.00002168
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002177
Iteration 117/1000 | Loss: 0.00002177
Iteration 118/1000 | Loss: 0.00002177
Iteration 119/1000 | Loss: 0.00002165
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002161
Iteration 124/1000 | Loss: 0.00002160
Iteration 125/1000 | Loss: 0.00002160
Iteration 126/1000 | Loss: 0.00002160
Iteration 127/1000 | Loss: 0.00002159
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002159
Iteration 131/1000 | Loss: 0.00002159
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00002158
Iteration 134/1000 | Loss: 0.00002158
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002157
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002151
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002146
Iteration 141/1000 | Loss: 0.00002146
Iteration 142/1000 | Loss: 0.00002146
Iteration 143/1000 | Loss: 0.00002146
Iteration 144/1000 | Loss: 0.00002146
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Iteration 147/1000 | Loss: 0.00002145
Iteration 148/1000 | Loss: 0.00002145
Iteration 149/1000 | Loss: 0.00002145
Iteration 150/1000 | Loss: 0.00002145
Iteration 151/1000 | Loss: 0.00002145
Iteration 152/1000 | Loss: 0.00002144
Iteration 153/1000 | Loss: 0.00002144
Iteration 154/1000 | Loss: 0.00002144
Iteration 155/1000 | Loss: 0.00002144
Iteration 156/1000 | Loss: 0.00002144
Iteration 157/1000 | Loss: 0.00002144
Iteration 158/1000 | Loss: 0.00002144
Iteration 159/1000 | Loss: 0.00002144
Iteration 160/1000 | Loss: 0.00002144
Iteration 161/1000 | Loss: 0.00002144
Iteration 162/1000 | Loss: 0.00002144
Iteration 163/1000 | Loss: 0.00002144
Iteration 164/1000 | Loss: 0.00002144
Iteration 165/1000 | Loss: 0.00002143
Iteration 166/1000 | Loss: 0.00002143
Iteration 167/1000 | Loss: 0.00002143
Iteration 168/1000 | Loss: 0.00002143
Iteration 169/1000 | Loss: 0.00002143
Iteration 170/1000 | Loss: 0.00002143
Iteration 171/1000 | Loss: 0.00002142
Iteration 172/1000 | Loss: 0.00002142
Iteration 173/1000 | Loss: 0.00002142
Iteration 174/1000 | Loss: 0.00002142
Iteration 175/1000 | Loss: 0.00002142
Iteration 176/1000 | Loss: 0.00002142
Iteration 177/1000 | Loss: 0.00002142
Iteration 178/1000 | Loss: 0.00002142
Iteration 179/1000 | Loss: 0.00002141
Iteration 180/1000 | Loss: 0.00002141
Iteration 181/1000 | Loss: 0.00002141
Iteration 182/1000 | Loss: 0.00002141
Iteration 183/1000 | Loss: 0.00002141
Iteration 184/1000 | Loss: 0.00002141
Iteration 185/1000 | Loss: 0.00002141
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002140
Iteration 188/1000 | Loss: 0.00002140
Iteration 189/1000 | Loss: 0.00002140
Iteration 190/1000 | Loss: 0.00002140
Iteration 191/1000 | Loss: 0.00002139
Iteration 192/1000 | Loss: 0.00002139
Iteration 193/1000 | Loss: 0.00002139
Iteration 194/1000 | Loss: 0.00002139
Iteration 195/1000 | Loss: 0.00002139
Iteration 196/1000 | Loss: 0.00002139
Iteration 197/1000 | Loss: 0.00002139
Iteration 198/1000 | Loss: 0.00002138
Iteration 199/1000 | Loss: 0.00002138
Iteration 200/1000 | Loss: 0.00002138
Iteration 201/1000 | Loss: 0.00002138
Iteration 202/1000 | Loss: 0.00002138
Iteration 203/1000 | Loss: 0.00002138
Iteration 204/1000 | Loss: 0.00002137
Iteration 205/1000 | Loss: 0.00002137
Iteration 206/1000 | Loss: 0.00002137
Iteration 207/1000 | Loss: 0.00002137
Iteration 208/1000 | Loss: 0.00002137
Iteration 209/1000 | Loss: 0.00002137
Iteration 210/1000 | Loss: 0.00002137
Iteration 211/1000 | Loss: 0.00002137
Iteration 212/1000 | Loss: 0.00002137
Iteration 213/1000 | Loss: 0.00002137
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002137
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002136
Iteration 218/1000 | Loss: 0.00002136
Iteration 219/1000 | Loss: 0.00002136
Iteration 220/1000 | Loss: 0.00002136
Iteration 221/1000 | Loss: 0.00002136
Iteration 222/1000 | Loss: 0.00002136
Iteration 223/1000 | Loss: 0.00002136
Iteration 224/1000 | Loss: 0.00002136
Iteration 225/1000 | Loss: 0.00002136
Iteration 226/1000 | Loss: 0.00002136
Iteration 227/1000 | Loss: 0.00002136
Iteration 228/1000 | Loss: 0.00002136
Iteration 229/1000 | Loss: 0.00002136
Iteration 230/1000 | Loss: 0.00002135
Iteration 231/1000 | Loss: 0.00002135
Iteration 232/1000 | Loss: 0.00002135
Iteration 233/1000 | Loss: 0.00002135
Iteration 234/1000 | Loss: 0.00002134
Iteration 235/1000 | Loss: 0.00002134
Iteration 236/1000 | Loss: 0.00002134
Iteration 237/1000 | Loss: 0.00002134
Iteration 238/1000 | Loss: 0.00002134
Iteration 239/1000 | Loss: 0.00002134
Iteration 240/1000 | Loss: 0.00002133
Iteration 241/1000 | Loss: 0.00002133
Iteration 242/1000 | Loss: 0.00002133
Iteration 243/1000 | Loss: 0.00002133
Iteration 244/1000 | Loss: 0.00002133
Iteration 245/1000 | Loss: 0.00002133
Iteration 246/1000 | Loss: 0.00002133
Iteration 247/1000 | Loss: 0.00002133
Iteration 248/1000 | Loss: 0.00002133
Iteration 249/1000 | Loss: 0.00002133
Iteration 250/1000 | Loss: 0.00002132
Iteration 251/1000 | Loss: 0.00002132
Iteration 252/1000 | Loss: 0.00002132
Iteration 253/1000 | Loss: 0.00002132
Iteration 254/1000 | Loss: 0.00002132
Iteration 255/1000 | Loss: 0.00002132
Iteration 256/1000 | Loss: 0.00002132
Iteration 257/1000 | Loss: 0.00002132
Iteration 258/1000 | Loss: 0.00002132
Iteration 259/1000 | Loss: 0.00002132
Iteration 260/1000 | Loss: 0.00002132
Iteration 261/1000 | Loss: 0.00002132
Iteration 262/1000 | Loss: 0.00002132
Iteration 263/1000 | Loss: 0.00002132
Iteration 264/1000 | Loss: 0.00002132
Iteration 265/1000 | Loss: 0.00002131
Iteration 266/1000 | Loss: 0.00002131
Iteration 267/1000 | Loss: 0.00002131
Iteration 268/1000 | Loss: 0.00002131
Iteration 269/1000 | Loss: 0.00002131
Iteration 270/1000 | Loss: 0.00002131
Iteration 271/1000 | Loss: 0.00002131
Iteration 272/1000 | Loss: 0.00002131
Iteration 273/1000 | Loss: 0.00002131
Iteration 274/1000 | Loss: 0.00002131
Iteration 275/1000 | Loss: 0.00002131
Iteration 276/1000 | Loss: 0.00002131
Iteration 277/1000 | Loss: 0.00002131
Iteration 278/1000 | Loss: 0.00002131
Iteration 279/1000 | Loss: 0.00002131
Iteration 280/1000 | Loss: 0.00002131
Iteration 281/1000 | Loss: 0.00002131
Iteration 282/1000 | Loss: 0.00002131
Iteration 283/1000 | Loss: 0.00002131
Iteration 284/1000 | Loss: 0.00002131
Iteration 285/1000 | Loss: 0.00002131
Iteration 286/1000 | Loss: 0.00002131
Iteration 287/1000 | Loss: 0.00002131
Iteration 288/1000 | Loss: 0.00002131
Iteration 289/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [2.1309346266207285e-05, 2.1309346266207285e-05, 2.1309346266207285e-05, 2.1309346266207285e-05, 2.1309346266207285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1309346266207285e-05

Optimization complete. Final v2v error: 3.357908010482788 mm

Highest mean error: 22.514371871948242 mm for frame 44

Lowest mean error: 2.4303977489471436 mm for frame 164

Saving results

Total time: 195.83976078033447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081167
Iteration 2/25 | Loss: 0.00364506
Iteration 3/25 | Loss: 0.00346096
Iteration 4/25 | Loss: 0.00315205
Iteration 5/25 | Loss: 0.00255301
Iteration 6/25 | Loss: 0.00236043
Iteration 7/25 | Loss: 0.00212494
Iteration 8/25 | Loss: 0.00241150
Iteration 9/25 | Loss: 0.00193066
Iteration 10/25 | Loss: 0.00185564
Iteration 11/25 | Loss: 0.00178724
Iteration 12/25 | Loss: 0.00173529
Iteration 13/25 | Loss: 0.00164996
Iteration 14/25 | Loss: 0.00162368
Iteration 15/25 | Loss: 0.00160306
Iteration 16/25 | Loss: 0.00158302
Iteration 17/25 | Loss: 0.00157640
Iteration 18/25 | Loss: 0.00157135
Iteration 19/25 | Loss: 0.00157585
Iteration 20/25 | Loss: 0.00156680
Iteration 21/25 | Loss: 0.00156283
Iteration 22/25 | Loss: 0.00156422
Iteration 23/25 | Loss: 0.00155828
Iteration 24/25 | Loss: 0.00155845
Iteration 25/25 | Loss: 0.00155871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29566216
Iteration 2/25 | Loss: 0.00464227
Iteration 3/25 | Loss: 0.00464226
Iteration 4/25 | Loss: 0.00464226
Iteration 5/25 | Loss: 0.00464226
Iteration 6/25 | Loss: 0.00464226
Iteration 7/25 | Loss: 0.00464226
Iteration 8/25 | Loss: 0.00464226
Iteration 9/25 | Loss: 0.00464226
Iteration 10/25 | Loss: 0.00464226
Iteration 11/25 | Loss: 0.00464226
Iteration 12/25 | Loss: 0.00464226
Iteration 13/25 | Loss: 0.00464226
Iteration 14/25 | Loss: 0.00464226
Iteration 15/25 | Loss: 0.00464226
Iteration 16/25 | Loss: 0.00464226
Iteration 17/25 | Loss: 0.00464226
Iteration 18/25 | Loss: 0.00464226
Iteration 19/25 | Loss: 0.00464226
Iteration 20/25 | Loss: 0.00464226
Iteration 21/25 | Loss: 0.00464226
Iteration 22/25 | Loss: 0.00464226
Iteration 23/25 | Loss: 0.00464226
Iteration 24/25 | Loss: 0.00464226
Iteration 25/25 | Loss: 0.00464226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00464226258918643, 0.00464226258918643, 0.00464226258918643, 0.00464226258918643, 0.00464226258918643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00464226258918643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00464226
Iteration 2/1000 | Loss: 0.00057049
Iteration 3/1000 | Loss: 0.00046859
Iteration 4/1000 | Loss: 0.00041369
Iteration 5/1000 | Loss: 0.00037034
Iteration 6/1000 | Loss: 0.00035962
Iteration 7/1000 | Loss: 0.00034474
Iteration 8/1000 | Loss: 0.00211242
Iteration 9/1000 | Loss: 0.00674201
Iteration 10/1000 | Loss: 0.00835375
Iteration 11/1000 | Loss: 0.00069432
Iteration 12/1000 | Loss: 0.00181897
Iteration 13/1000 | Loss: 0.00250990
Iteration 14/1000 | Loss: 0.00148185
Iteration 15/1000 | Loss: 0.00335557
Iteration 16/1000 | Loss: 0.00136772
Iteration 17/1000 | Loss: 0.00064460
Iteration 18/1000 | Loss: 0.00047231
Iteration 19/1000 | Loss: 0.00088230
Iteration 20/1000 | Loss: 0.00030090
Iteration 21/1000 | Loss: 0.00018766
Iteration 22/1000 | Loss: 0.00017216
Iteration 23/1000 | Loss: 0.00015960
Iteration 24/1000 | Loss: 0.00015300
Iteration 25/1000 | Loss: 0.00032443
Iteration 26/1000 | Loss: 0.00181642
Iteration 27/1000 | Loss: 0.00277537
Iteration 28/1000 | Loss: 0.00051315
Iteration 29/1000 | Loss: 0.00030412
Iteration 30/1000 | Loss: 0.00019624
Iteration 31/1000 | Loss: 0.00013923
Iteration 32/1000 | Loss: 0.00008496
Iteration 33/1000 | Loss: 0.00005974
Iteration 34/1000 | Loss: 0.00005041
Iteration 35/1000 | Loss: 0.00004139
Iteration 36/1000 | Loss: 0.00003537
Iteration 37/1000 | Loss: 0.00003115
Iteration 38/1000 | Loss: 0.00002835
Iteration 39/1000 | Loss: 0.00002622
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002397
Iteration 42/1000 | Loss: 0.00002286
Iteration 43/1000 | Loss: 0.00002227
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002165
Iteration 46/1000 | Loss: 0.00002151
Iteration 47/1000 | Loss: 0.00002146
Iteration 48/1000 | Loss: 0.00002141
Iteration 49/1000 | Loss: 0.00002141
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002123
Iteration 52/1000 | Loss: 0.00002122
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002113
Iteration 60/1000 | Loss: 0.00002113
Iteration 61/1000 | Loss: 0.00002113
Iteration 62/1000 | Loss: 0.00002112
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002112
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002110
Iteration 71/1000 | Loss: 0.00002109
Iteration 72/1000 | Loss: 0.00002109
Iteration 73/1000 | Loss: 0.00002108
Iteration 74/1000 | Loss: 0.00002108
Iteration 75/1000 | Loss: 0.00002108
Iteration 76/1000 | Loss: 0.00002107
Iteration 77/1000 | Loss: 0.00002107
Iteration 78/1000 | Loss: 0.00002107
Iteration 79/1000 | Loss: 0.00002107
Iteration 80/1000 | Loss: 0.00002107
Iteration 81/1000 | Loss: 0.00002107
Iteration 82/1000 | Loss: 0.00002106
Iteration 83/1000 | Loss: 0.00002106
Iteration 84/1000 | Loss: 0.00002106
Iteration 85/1000 | Loss: 0.00002106
Iteration 86/1000 | Loss: 0.00002106
Iteration 87/1000 | Loss: 0.00002105
Iteration 88/1000 | Loss: 0.00002105
Iteration 89/1000 | Loss: 0.00002105
Iteration 90/1000 | Loss: 0.00002105
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002105
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002104
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002103
Iteration 105/1000 | Loss: 0.00002103
Iteration 106/1000 | Loss: 0.00002103
Iteration 107/1000 | Loss: 0.00002103
Iteration 108/1000 | Loss: 0.00002103
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002102
Iteration 113/1000 | Loss: 0.00002102
Iteration 114/1000 | Loss: 0.00002102
Iteration 115/1000 | Loss: 0.00002102
Iteration 116/1000 | Loss: 0.00002102
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00002101
Iteration 125/1000 | Loss: 0.00002101
Iteration 126/1000 | Loss: 0.00002101
Iteration 127/1000 | Loss: 0.00002101
Iteration 128/1000 | Loss: 0.00002101
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002100
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00002100
Iteration 138/1000 | Loss: 0.00002100
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002099
Iteration 145/1000 | Loss: 0.00002099
Iteration 146/1000 | Loss: 0.00002099
Iteration 147/1000 | Loss: 0.00002099
Iteration 148/1000 | Loss: 0.00002099
Iteration 149/1000 | Loss: 0.00002099
Iteration 150/1000 | Loss: 0.00002099
Iteration 151/1000 | Loss: 0.00002099
Iteration 152/1000 | Loss: 0.00002099
Iteration 153/1000 | Loss: 0.00002099
Iteration 154/1000 | Loss: 0.00002099
Iteration 155/1000 | Loss: 0.00002099
Iteration 156/1000 | Loss: 0.00002099
Iteration 157/1000 | Loss: 0.00002099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.099212542816531e-05, 2.099212542816531e-05, 2.099212542816531e-05, 2.099212542816531e-05, 2.099212542816531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.099212542816531e-05

Optimization complete. Final v2v error: 3.9517107009887695 mm

Highest mean error: 4.551446914672852 mm for frame 184

Lowest mean error: 3.7925004959106445 mm for frame 8

Saving results

Total time: 138.58850049972534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495718
Iteration 2/25 | Loss: 0.00114015
Iteration 3/25 | Loss: 0.00098445
Iteration 4/25 | Loss: 0.00096678
Iteration 5/25 | Loss: 0.00096311
Iteration 6/25 | Loss: 0.00096249
Iteration 7/25 | Loss: 0.00096249
Iteration 8/25 | Loss: 0.00096249
Iteration 9/25 | Loss: 0.00096249
Iteration 10/25 | Loss: 0.00096249
Iteration 11/25 | Loss: 0.00096249
Iteration 12/25 | Loss: 0.00096249
Iteration 13/25 | Loss: 0.00096249
Iteration 14/25 | Loss: 0.00096249
Iteration 15/25 | Loss: 0.00096249
Iteration 16/25 | Loss: 0.00096249
Iteration 17/25 | Loss: 0.00096249
Iteration 18/25 | Loss: 0.00096249
Iteration 19/25 | Loss: 0.00096249
Iteration 20/25 | Loss: 0.00096249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009624873055145144, 0.0009624873055145144, 0.0009624873055145144, 0.0009624873055145144, 0.0009624873055145144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009624873055145144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38694930
Iteration 2/25 | Loss: 0.00065398
Iteration 3/25 | Loss: 0.00065397
Iteration 4/25 | Loss: 0.00065397
Iteration 5/25 | Loss: 0.00065397
Iteration 6/25 | Loss: 0.00065397
Iteration 7/25 | Loss: 0.00065397
Iteration 8/25 | Loss: 0.00065397
Iteration 9/25 | Loss: 0.00065397
Iteration 10/25 | Loss: 0.00065397
Iteration 11/25 | Loss: 0.00065397
Iteration 12/25 | Loss: 0.00065397
Iteration 13/25 | Loss: 0.00065397
Iteration 14/25 | Loss: 0.00065397
Iteration 15/25 | Loss: 0.00065397
Iteration 16/25 | Loss: 0.00065397
Iteration 17/25 | Loss: 0.00065397
Iteration 18/25 | Loss: 0.00065397
Iteration 19/25 | Loss: 0.00065397
Iteration 20/25 | Loss: 0.00065397
Iteration 21/25 | Loss: 0.00065397
Iteration 22/25 | Loss: 0.00065397
Iteration 23/25 | Loss: 0.00065397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006539672031067312, 0.0006539672031067312, 0.0006539672031067312, 0.0006539672031067312, 0.0006539672031067312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006539672031067312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065397
Iteration 2/1000 | Loss: 0.00002440
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001304
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001140
Iteration 34/1000 | Loss: 0.00001140
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001139
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001138
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001137
Iteration 53/1000 | Loss: 0.00001137
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001137
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.136848186433781e-05, 1.136848186433781e-05, 1.136848186433781e-05, 1.136848186433781e-05, 1.136848186433781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.136848186433781e-05

Optimization complete. Final v2v error: 2.850782871246338 mm

Highest mean error: 3.466763734817505 mm for frame 47

Lowest mean error: 2.3206207752227783 mm for frame 219

Saving results

Total time: 29.574438095092773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867578
Iteration 2/25 | Loss: 0.00104195
Iteration 3/25 | Loss: 0.00091219
Iteration 4/25 | Loss: 0.00090415
Iteration 5/25 | Loss: 0.00090371
Iteration 6/25 | Loss: 0.00090371
Iteration 7/25 | Loss: 0.00090371
Iteration 8/25 | Loss: 0.00090371
Iteration 9/25 | Loss: 0.00090371
Iteration 10/25 | Loss: 0.00090371
Iteration 11/25 | Loss: 0.00090371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000903705891687423, 0.000903705891687423, 0.000903705891687423, 0.000903705891687423, 0.000903705891687423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000903705891687423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33209193
Iteration 2/25 | Loss: 0.00061858
Iteration 3/25 | Loss: 0.00061857
Iteration 4/25 | Loss: 0.00061857
Iteration 5/25 | Loss: 0.00061857
Iteration 6/25 | Loss: 0.00061857
Iteration 7/25 | Loss: 0.00061857
Iteration 8/25 | Loss: 0.00061857
Iteration 9/25 | Loss: 0.00061857
Iteration 10/25 | Loss: 0.00061857
Iteration 11/25 | Loss: 0.00061857
Iteration 12/25 | Loss: 0.00061857
Iteration 13/25 | Loss: 0.00061857
Iteration 14/25 | Loss: 0.00061857
Iteration 15/25 | Loss: 0.00061857
Iteration 16/25 | Loss: 0.00061857
Iteration 17/25 | Loss: 0.00061857
Iteration 18/25 | Loss: 0.00061857
Iteration 19/25 | Loss: 0.00061857
Iteration 20/25 | Loss: 0.00061857
Iteration 21/25 | Loss: 0.00061857
Iteration 22/25 | Loss: 0.00061857
Iteration 23/25 | Loss: 0.00061857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006185706588439643, 0.0006185706588439643, 0.0006185706588439643, 0.0006185706588439643, 0.0006185706588439643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006185706588439643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061857
Iteration 2/1000 | Loss: 0.00001542
Iteration 3/1000 | Loss: 0.00000983
Iteration 4/1000 | Loss: 0.00000914
Iteration 5/1000 | Loss: 0.00000856
Iteration 6/1000 | Loss: 0.00000827
Iteration 7/1000 | Loss: 0.00000816
Iteration 8/1000 | Loss: 0.00000796
Iteration 9/1000 | Loss: 0.00000792
Iteration 10/1000 | Loss: 0.00000790
Iteration 11/1000 | Loss: 0.00000790
Iteration 12/1000 | Loss: 0.00000789
Iteration 13/1000 | Loss: 0.00000789
Iteration 14/1000 | Loss: 0.00000788
Iteration 15/1000 | Loss: 0.00000786
Iteration 16/1000 | Loss: 0.00000784
Iteration 17/1000 | Loss: 0.00000784
Iteration 18/1000 | Loss: 0.00000784
Iteration 19/1000 | Loss: 0.00000783
Iteration 20/1000 | Loss: 0.00000783
Iteration 21/1000 | Loss: 0.00000777
Iteration 22/1000 | Loss: 0.00000775
Iteration 23/1000 | Loss: 0.00000775
Iteration 24/1000 | Loss: 0.00000773
Iteration 25/1000 | Loss: 0.00000772
Iteration 26/1000 | Loss: 0.00000772
Iteration 27/1000 | Loss: 0.00000771
Iteration 28/1000 | Loss: 0.00000771
Iteration 29/1000 | Loss: 0.00000770
Iteration 30/1000 | Loss: 0.00000770
Iteration 31/1000 | Loss: 0.00000770
Iteration 32/1000 | Loss: 0.00000769
Iteration 33/1000 | Loss: 0.00000769
Iteration 34/1000 | Loss: 0.00000769
Iteration 35/1000 | Loss: 0.00000769
Iteration 36/1000 | Loss: 0.00000769
Iteration 37/1000 | Loss: 0.00000768
Iteration 38/1000 | Loss: 0.00000768
Iteration 39/1000 | Loss: 0.00000768
Iteration 40/1000 | Loss: 0.00000768
Iteration 41/1000 | Loss: 0.00000768
Iteration 42/1000 | Loss: 0.00000768
Iteration 43/1000 | Loss: 0.00000768
Iteration 44/1000 | Loss: 0.00000768
Iteration 45/1000 | Loss: 0.00000768
Iteration 46/1000 | Loss: 0.00000768
Iteration 47/1000 | Loss: 0.00000768
Iteration 48/1000 | Loss: 0.00000767
Iteration 49/1000 | Loss: 0.00000767
Iteration 50/1000 | Loss: 0.00000767
Iteration 51/1000 | Loss: 0.00000766
Iteration 52/1000 | Loss: 0.00000766
Iteration 53/1000 | Loss: 0.00000766
Iteration 54/1000 | Loss: 0.00000765
Iteration 55/1000 | Loss: 0.00000765
Iteration 56/1000 | Loss: 0.00000765
Iteration 57/1000 | Loss: 0.00000765
Iteration 58/1000 | Loss: 0.00000764
Iteration 59/1000 | Loss: 0.00000764
Iteration 60/1000 | Loss: 0.00000764
Iteration 61/1000 | Loss: 0.00000764
Iteration 62/1000 | Loss: 0.00000764
Iteration 63/1000 | Loss: 0.00000764
Iteration 64/1000 | Loss: 0.00000764
Iteration 65/1000 | Loss: 0.00000764
Iteration 66/1000 | Loss: 0.00000763
Iteration 67/1000 | Loss: 0.00000763
Iteration 68/1000 | Loss: 0.00000763
Iteration 69/1000 | Loss: 0.00000763
Iteration 70/1000 | Loss: 0.00000763
Iteration 71/1000 | Loss: 0.00000763
Iteration 72/1000 | Loss: 0.00000763
Iteration 73/1000 | Loss: 0.00000762
Iteration 74/1000 | Loss: 0.00000762
Iteration 75/1000 | Loss: 0.00000762
Iteration 76/1000 | Loss: 0.00000762
Iteration 77/1000 | Loss: 0.00000762
Iteration 78/1000 | Loss: 0.00000762
Iteration 79/1000 | Loss: 0.00000762
Iteration 80/1000 | Loss: 0.00000762
Iteration 81/1000 | Loss: 0.00000762
Iteration 82/1000 | Loss: 0.00000762
Iteration 83/1000 | Loss: 0.00000762
Iteration 84/1000 | Loss: 0.00000762
Iteration 85/1000 | Loss: 0.00000762
Iteration 86/1000 | Loss: 0.00000762
Iteration 87/1000 | Loss: 0.00000762
Iteration 88/1000 | Loss: 0.00000761
Iteration 89/1000 | Loss: 0.00000761
Iteration 90/1000 | Loss: 0.00000761
Iteration 91/1000 | Loss: 0.00000761
Iteration 92/1000 | Loss: 0.00000761
Iteration 93/1000 | Loss: 0.00000761
Iteration 94/1000 | Loss: 0.00000761
Iteration 95/1000 | Loss: 0.00000761
Iteration 96/1000 | Loss: 0.00000761
Iteration 97/1000 | Loss: 0.00000761
Iteration 98/1000 | Loss: 0.00000761
Iteration 99/1000 | Loss: 0.00000761
Iteration 100/1000 | Loss: 0.00000761
Iteration 101/1000 | Loss: 0.00000761
Iteration 102/1000 | Loss: 0.00000761
Iteration 103/1000 | Loss: 0.00000760
Iteration 104/1000 | Loss: 0.00000760
Iteration 105/1000 | Loss: 0.00000760
Iteration 106/1000 | Loss: 0.00000760
Iteration 107/1000 | Loss: 0.00000760
Iteration 108/1000 | Loss: 0.00000760
Iteration 109/1000 | Loss: 0.00000760
Iteration 110/1000 | Loss: 0.00000760
Iteration 111/1000 | Loss: 0.00000760
Iteration 112/1000 | Loss: 0.00000760
Iteration 113/1000 | Loss: 0.00000760
Iteration 114/1000 | Loss: 0.00000760
Iteration 115/1000 | Loss: 0.00000760
Iteration 116/1000 | Loss: 0.00000759
Iteration 117/1000 | Loss: 0.00000759
Iteration 118/1000 | Loss: 0.00000759
Iteration 119/1000 | Loss: 0.00000759
Iteration 120/1000 | Loss: 0.00000759
Iteration 121/1000 | Loss: 0.00000758
Iteration 122/1000 | Loss: 0.00000758
Iteration 123/1000 | Loss: 0.00000758
Iteration 124/1000 | Loss: 0.00000758
Iteration 125/1000 | Loss: 0.00000757
Iteration 126/1000 | Loss: 0.00000757
Iteration 127/1000 | Loss: 0.00000757
Iteration 128/1000 | Loss: 0.00000757
Iteration 129/1000 | Loss: 0.00000757
Iteration 130/1000 | Loss: 0.00000757
Iteration 131/1000 | Loss: 0.00000757
Iteration 132/1000 | Loss: 0.00000757
Iteration 133/1000 | Loss: 0.00000756
Iteration 134/1000 | Loss: 0.00000756
Iteration 135/1000 | Loss: 0.00000756
Iteration 136/1000 | Loss: 0.00000755
Iteration 137/1000 | Loss: 0.00000755
Iteration 138/1000 | Loss: 0.00000755
Iteration 139/1000 | Loss: 0.00000755
Iteration 140/1000 | Loss: 0.00000755
Iteration 141/1000 | Loss: 0.00000755
Iteration 142/1000 | Loss: 0.00000755
Iteration 143/1000 | Loss: 0.00000755
Iteration 144/1000 | Loss: 0.00000755
Iteration 145/1000 | Loss: 0.00000755
Iteration 146/1000 | Loss: 0.00000755
Iteration 147/1000 | Loss: 0.00000755
Iteration 148/1000 | Loss: 0.00000755
Iteration 149/1000 | Loss: 0.00000755
Iteration 150/1000 | Loss: 0.00000755
Iteration 151/1000 | Loss: 0.00000755
Iteration 152/1000 | Loss: 0.00000755
Iteration 153/1000 | Loss: 0.00000755
Iteration 154/1000 | Loss: 0.00000754
Iteration 155/1000 | Loss: 0.00000754
Iteration 156/1000 | Loss: 0.00000754
Iteration 157/1000 | Loss: 0.00000754
Iteration 158/1000 | Loss: 0.00000754
Iteration 159/1000 | Loss: 0.00000754
Iteration 160/1000 | Loss: 0.00000754
Iteration 161/1000 | Loss: 0.00000754
Iteration 162/1000 | Loss: 0.00000754
Iteration 163/1000 | Loss: 0.00000754
Iteration 164/1000 | Loss: 0.00000753
Iteration 165/1000 | Loss: 0.00000753
Iteration 166/1000 | Loss: 0.00000753
Iteration 167/1000 | Loss: 0.00000753
Iteration 168/1000 | Loss: 0.00000753
Iteration 169/1000 | Loss: 0.00000753
Iteration 170/1000 | Loss: 0.00000753
Iteration 171/1000 | Loss: 0.00000753
Iteration 172/1000 | Loss: 0.00000753
Iteration 173/1000 | Loss: 0.00000753
Iteration 174/1000 | Loss: 0.00000753
Iteration 175/1000 | Loss: 0.00000753
Iteration 176/1000 | Loss: 0.00000753
Iteration 177/1000 | Loss: 0.00000753
Iteration 178/1000 | Loss: 0.00000753
Iteration 179/1000 | Loss: 0.00000753
Iteration 180/1000 | Loss: 0.00000753
Iteration 181/1000 | Loss: 0.00000752
Iteration 182/1000 | Loss: 0.00000752
Iteration 183/1000 | Loss: 0.00000752
Iteration 184/1000 | Loss: 0.00000752
Iteration 185/1000 | Loss: 0.00000752
Iteration 186/1000 | Loss: 0.00000752
Iteration 187/1000 | Loss: 0.00000752
Iteration 188/1000 | Loss: 0.00000752
Iteration 189/1000 | Loss: 0.00000752
Iteration 190/1000 | Loss: 0.00000752
Iteration 191/1000 | Loss: 0.00000751
Iteration 192/1000 | Loss: 0.00000751
Iteration 193/1000 | Loss: 0.00000751
Iteration 194/1000 | Loss: 0.00000751
Iteration 195/1000 | Loss: 0.00000751
Iteration 196/1000 | Loss: 0.00000751
Iteration 197/1000 | Loss: 0.00000751
Iteration 198/1000 | Loss: 0.00000751
Iteration 199/1000 | Loss: 0.00000751
Iteration 200/1000 | Loss: 0.00000751
Iteration 201/1000 | Loss: 0.00000751
Iteration 202/1000 | Loss: 0.00000751
Iteration 203/1000 | Loss: 0.00000751
Iteration 204/1000 | Loss: 0.00000751
Iteration 205/1000 | Loss: 0.00000751
Iteration 206/1000 | Loss: 0.00000751
Iteration 207/1000 | Loss: 0.00000750
Iteration 208/1000 | Loss: 0.00000750
Iteration 209/1000 | Loss: 0.00000750
Iteration 210/1000 | Loss: 0.00000750
Iteration 211/1000 | Loss: 0.00000750
Iteration 212/1000 | Loss: 0.00000750
Iteration 213/1000 | Loss: 0.00000750
Iteration 214/1000 | Loss: 0.00000750
Iteration 215/1000 | Loss: 0.00000750
Iteration 216/1000 | Loss: 0.00000750
Iteration 217/1000 | Loss: 0.00000750
Iteration 218/1000 | Loss: 0.00000750
Iteration 219/1000 | Loss: 0.00000750
Iteration 220/1000 | Loss: 0.00000750
Iteration 221/1000 | Loss: 0.00000750
Iteration 222/1000 | Loss: 0.00000750
Iteration 223/1000 | Loss: 0.00000750
Iteration 224/1000 | Loss: 0.00000750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [7.501125764974859e-06, 7.501125764974859e-06, 7.501125764974859e-06, 7.501125764974859e-06, 7.501125764974859e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.501125764974859e-06

Optimization complete. Final v2v error: 2.334949493408203 mm

Highest mean error: 2.5349819660186768 mm for frame 128

Lowest mean error: 2.1840994358062744 mm for frame 65

Saving results

Total time: 36.59556770324707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618354
Iteration 2/25 | Loss: 0.00113909
Iteration 3/25 | Loss: 0.00101601
Iteration 4/25 | Loss: 0.00099631
Iteration 5/25 | Loss: 0.00099127
Iteration 6/25 | Loss: 0.00099055
Iteration 7/25 | Loss: 0.00099055
Iteration 8/25 | Loss: 0.00099055
Iteration 9/25 | Loss: 0.00099055
Iteration 10/25 | Loss: 0.00099055
Iteration 11/25 | Loss: 0.00099055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009905466577038169, 0.0009905466577038169, 0.0009905466577038169, 0.0009905466577038169, 0.0009905466577038169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009905466577038169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12641501
Iteration 2/25 | Loss: 0.00065804
Iteration 3/25 | Loss: 0.00065804
Iteration 4/25 | Loss: 0.00065804
Iteration 5/25 | Loss: 0.00065804
Iteration 6/25 | Loss: 0.00065804
Iteration 7/25 | Loss: 0.00065803
Iteration 8/25 | Loss: 0.00065803
Iteration 9/25 | Loss: 0.00065803
Iteration 10/25 | Loss: 0.00065803
Iteration 11/25 | Loss: 0.00065803
Iteration 12/25 | Loss: 0.00065803
Iteration 13/25 | Loss: 0.00065803
Iteration 14/25 | Loss: 0.00065803
Iteration 15/25 | Loss: 0.00065803
Iteration 16/25 | Loss: 0.00065803
Iteration 17/25 | Loss: 0.00065803
Iteration 18/25 | Loss: 0.00065803
Iteration 19/25 | Loss: 0.00065803
Iteration 20/25 | Loss: 0.00065803
Iteration 21/25 | Loss: 0.00065803
Iteration 22/25 | Loss: 0.00065803
Iteration 23/25 | Loss: 0.00065803
Iteration 24/25 | Loss: 0.00065803
Iteration 25/25 | Loss: 0.00065803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065803
Iteration 2/1000 | Loss: 0.00002824
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001720
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001710
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001704
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.7004000255838037e-05, 1.7004000255838037e-05, 1.7004000255838037e-05, 1.7004000255838037e-05, 1.7004000255838037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7004000255838037e-05

Optimization complete. Final v2v error: 3.3186628818511963 mm

Highest mean error: 4.361517429351807 mm for frame 99

Lowest mean error: 2.793393135070801 mm for frame 60

Saving results

Total time: 29.604243278503418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568799
Iteration 2/25 | Loss: 0.00125425
Iteration 3/25 | Loss: 0.00104314
Iteration 4/25 | Loss: 0.00102833
Iteration 5/25 | Loss: 0.00102472
Iteration 6/25 | Loss: 0.00102361
Iteration 7/25 | Loss: 0.00102361
Iteration 8/25 | Loss: 0.00102361
Iteration 9/25 | Loss: 0.00102361
Iteration 10/25 | Loss: 0.00102361
Iteration 11/25 | Loss: 0.00102361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010236097732558846, 0.0010236097732558846, 0.0010236097732558846, 0.0010236097732558846, 0.0010236097732558846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010236097732558846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35126543
Iteration 2/25 | Loss: 0.00064994
Iteration 3/25 | Loss: 0.00064993
Iteration 4/25 | Loss: 0.00064992
Iteration 5/25 | Loss: 0.00064992
Iteration 6/25 | Loss: 0.00064992
Iteration 7/25 | Loss: 0.00064992
Iteration 8/25 | Loss: 0.00064992
Iteration 9/25 | Loss: 0.00064992
Iteration 10/25 | Loss: 0.00064992
Iteration 11/25 | Loss: 0.00064992
Iteration 12/25 | Loss: 0.00064992
Iteration 13/25 | Loss: 0.00064992
Iteration 14/25 | Loss: 0.00064992
Iteration 15/25 | Loss: 0.00064992
Iteration 16/25 | Loss: 0.00064992
Iteration 17/25 | Loss: 0.00064992
Iteration 18/25 | Loss: 0.00064992
Iteration 19/25 | Loss: 0.00064992
Iteration 20/25 | Loss: 0.00064992
Iteration 21/25 | Loss: 0.00064992
Iteration 22/25 | Loss: 0.00064992
Iteration 23/25 | Loss: 0.00064992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006499219452962279, 0.0006499219452962279, 0.0006499219452962279, 0.0006499219452962279, 0.0006499219452962279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006499219452962279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064992
Iteration 2/1000 | Loss: 0.00004386
Iteration 3/1000 | Loss: 0.00003556
Iteration 4/1000 | Loss: 0.00003318
Iteration 5/1000 | Loss: 0.00003203
Iteration 6/1000 | Loss: 0.00003132
Iteration 7/1000 | Loss: 0.00003082
Iteration 8/1000 | Loss: 0.00003051
Iteration 9/1000 | Loss: 0.00003031
Iteration 10/1000 | Loss: 0.00003025
Iteration 11/1000 | Loss: 0.00003018
Iteration 12/1000 | Loss: 0.00003015
Iteration 13/1000 | Loss: 0.00003007
Iteration 14/1000 | Loss: 0.00003001
Iteration 15/1000 | Loss: 0.00003000
Iteration 16/1000 | Loss: 0.00003000
Iteration 17/1000 | Loss: 0.00002992
Iteration 18/1000 | Loss: 0.00002988
Iteration 19/1000 | Loss: 0.00002988
Iteration 20/1000 | Loss: 0.00002981
Iteration 21/1000 | Loss: 0.00002981
Iteration 22/1000 | Loss: 0.00002978
Iteration 23/1000 | Loss: 0.00002978
Iteration 24/1000 | Loss: 0.00002977
Iteration 25/1000 | Loss: 0.00002977
Iteration 26/1000 | Loss: 0.00002977
Iteration 27/1000 | Loss: 0.00002977
Iteration 28/1000 | Loss: 0.00002976
Iteration 29/1000 | Loss: 0.00002976
Iteration 30/1000 | Loss: 0.00002975
Iteration 31/1000 | Loss: 0.00002975
Iteration 32/1000 | Loss: 0.00002975
Iteration 33/1000 | Loss: 0.00002975
Iteration 34/1000 | Loss: 0.00002975
Iteration 35/1000 | Loss: 0.00002975
Iteration 36/1000 | Loss: 0.00002975
Iteration 37/1000 | Loss: 0.00002975
Iteration 38/1000 | Loss: 0.00002975
Iteration 39/1000 | Loss: 0.00002975
Iteration 40/1000 | Loss: 0.00002975
Iteration 41/1000 | Loss: 0.00002974
Iteration 42/1000 | Loss: 0.00002974
Iteration 43/1000 | Loss: 0.00002974
Iteration 44/1000 | Loss: 0.00002974
Iteration 45/1000 | Loss: 0.00002974
Iteration 46/1000 | Loss: 0.00002974
Iteration 47/1000 | Loss: 0.00002974
Iteration 48/1000 | Loss: 0.00002974
Iteration 49/1000 | Loss: 0.00002973
Iteration 50/1000 | Loss: 0.00002973
Iteration 51/1000 | Loss: 0.00002973
Iteration 52/1000 | Loss: 0.00002973
Iteration 53/1000 | Loss: 0.00002971
Iteration 54/1000 | Loss: 0.00002971
Iteration 55/1000 | Loss: 0.00002971
Iteration 56/1000 | Loss: 0.00002971
Iteration 57/1000 | Loss: 0.00002970
Iteration 58/1000 | Loss: 0.00002970
Iteration 59/1000 | Loss: 0.00002970
Iteration 60/1000 | Loss: 0.00002970
Iteration 61/1000 | Loss: 0.00002970
Iteration 62/1000 | Loss: 0.00002970
Iteration 63/1000 | Loss: 0.00002970
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002969
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002968
Iteration 73/1000 | Loss: 0.00002967
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002965
Iteration 76/1000 | Loss: 0.00002965
Iteration 77/1000 | Loss: 0.00002965
Iteration 78/1000 | Loss: 0.00002965
Iteration 79/1000 | Loss: 0.00002964
Iteration 80/1000 | Loss: 0.00002963
Iteration 81/1000 | Loss: 0.00002963
Iteration 82/1000 | Loss: 0.00002962
Iteration 83/1000 | Loss: 0.00002961
Iteration 84/1000 | Loss: 0.00002961
Iteration 85/1000 | Loss: 0.00002961
Iteration 86/1000 | Loss: 0.00002961
Iteration 87/1000 | Loss: 0.00002961
Iteration 88/1000 | Loss: 0.00002961
Iteration 89/1000 | Loss: 0.00002961
Iteration 90/1000 | Loss: 0.00002961
Iteration 91/1000 | Loss: 0.00002961
Iteration 92/1000 | Loss: 0.00002961
Iteration 93/1000 | Loss: 0.00002961
Iteration 94/1000 | Loss: 0.00002961
Iteration 95/1000 | Loss: 0.00002961
Iteration 96/1000 | Loss: 0.00002960
Iteration 97/1000 | Loss: 0.00002960
Iteration 98/1000 | Loss: 0.00002959
Iteration 99/1000 | Loss: 0.00002958
Iteration 100/1000 | Loss: 0.00002958
Iteration 101/1000 | Loss: 0.00002958
Iteration 102/1000 | Loss: 0.00002958
Iteration 103/1000 | Loss: 0.00002958
Iteration 104/1000 | Loss: 0.00002958
Iteration 105/1000 | Loss: 0.00002957
Iteration 106/1000 | Loss: 0.00002957
Iteration 107/1000 | Loss: 0.00002957
Iteration 108/1000 | Loss: 0.00002957
Iteration 109/1000 | Loss: 0.00002955
Iteration 110/1000 | Loss: 0.00002955
Iteration 111/1000 | Loss: 0.00002955
Iteration 112/1000 | Loss: 0.00002955
Iteration 113/1000 | Loss: 0.00002955
Iteration 114/1000 | Loss: 0.00002954
Iteration 115/1000 | Loss: 0.00002954
Iteration 116/1000 | Loss: 0.00002954
Iteration 117/1000 | Loss: 0.00002952
Iteration 118/1000 | Loss: 0.00002951
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00002950
Iteration 121/1000 | Loss: 0.00002948
Iteration 122/1000 | Loss: 0.00002948
Iteration 123/1000 | Loss: 0.00002947
Iteration 124/1000 | Loss: 0.00002947
Iteration 125/1000 | Loss: 0.00002947
Iteration 126/1000 | Loss: 0.00002947
Iteration 127/1000 | Loss: 0.00002947
Iteration 128/1000 | Loss: 0.00002945
Iteration 129/1000 | Loss: 0.00002945
Iteration 130/1000 | Loss: 0.00002945
Iteration 131/1000 | Loss: 0.00002944
Iteration 132/1000 | Loss: 0.00002944
Iteration 133/1000 | Loss: 0.00002944
Iteration 134/1000 | Loss: 0.00002944
Iteration 135/1000 | Loss: 0.00002944
Iteration 136/1000 | Loss: 0.00002944
Iteration 137/1000 | Loss: 0.00002944
Iteration 138/1000 | Loss: 0.00002944
Iteration 139/1000 | Loss: 0.00002944
Iteration 140/1000 | Loss: 0.00002944
Iteration 141/1000 | Loss: 0.00002944
Iteration 142/1000 | Loss: 0.00002944
Iteration 143/1000 | Loss: 0.00002944
Iteration 144/1000 | Loss: 0.00002943
Iteration 145/1000 | Loss: 0.00002943
Iteration 146/1000 | Loss: 0.00002943
Iteration 147/1000 | Loss: 0.00002943
Iteration 148/1000 | Loss: 0.00002943
Iteration 149/1000 | Loss: 0.00002943
Iteration 150/1000 | Loss: 0.00002943
Iteration 151/1000 | Loss: 0.00002943
Iteration 152/1000 | Loss: 0.00002943
Iteration 153/1000 | Loss: 0.00002943
Iteration 154/1000 | Loss: 0.00002943
Iteration 155/1000 | Loss: 0.00002943
Iteration 156/1000 | Loss: 0.00002943
Iteration 157/1000 | Loss: 0.00002943
Iteration 158/1000 | Loss: 0.00002943
Iteration 159/1000 | Loss: 0.00002943
Iteration 160/1000 | Loss: 0.00002943
Iteration 161/1000 | Loss: 0.00002943
Iteration 162/1000 | Loss: 0.00002943
Iteration 163/1000 | Loss: 0.00002943
Iteration 164/1000 | Loss: 0.00002943
Iteration 165/1000 | Loss: 0.00002943
Iteration 166/1000 | Loss: 0.00002943
Iteration 167/1000 | Loss: 0.00002943
Iteration 168/1000 | Loss: 0.00002943
Iteration 169/1000 | Loss: 0.00002943
Iteration 170/1000 | Loss: 0.00002943
Iteration 171/1000 | Loss: 0.00002943
Iteration 172/1000 | Loss: 0.00002943
Iteration 173/1000 | Loss: 0.00002943
Iteration 174/1000 | Loss: 0.00002943
Iteration 175/1000 | Loss: 0.00002943
Iteration 176/1000 | Loss: 0.00002943
Iteration 177/1000 | Loss: 0.00002943
Iteration 178/1000 | Loss: 0.00002943
Iteration 179/1000 | Loss: 0.00002943
Iteration 180/1000 | Loss: 0.00002943
Iteration 181/1000 | Loss: 0.00002943
Iteration 182/1000 | Loss: 0.00002943
Iteration 183/1000 | Loss: 0.00002943
Iteration 184/1000 | Loss: 0.00002943
Iteration 185/1000 | Loss: 0.00002943
Iteration 186/1000 | Loss: 0.00002943
Iteration 187/1000 | Loss: 0.00002943
Iteration 188/1000 | Loss: 0.00002943
Iteration 189/1000 | Loss: 0.00002943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.9433875170070678e-05, 2.9433875170070678e-05, 2.9433875170070678e-05, 2.9433875170070678e-05, 2.9433875170070678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9433875170070678e-05

Optimization complete. Final v2v error: 4.144782543182373 mm

Highest mean error: 4.61957311630249 mm for frame 133

Lowest mean error: 3.5911903381347656 mm for frame 234

Saving results

Total time: 45.40679883956909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718501
Iteration 2/25 | Loss: 0.00114559
Iteration 3/25 | Loss: 0.00100170
Iteration 4/25 | Loss: 0.00097858
Iteration 5/25 | Loss: 0.00096841
Iteration 6/25 | Loss: 0.00096414
Iteration 7/25 | Loss: 0.00096299
Iteration 8/25 | Loss: 0.00096299
Iteration 9/25 | Loss: 0.00096299
Iteration 10/25 | Loss: 0.00096299
Iteration 11/25 | Loss: 0.00096299
Iteration 12/25 | Loss: 0.00096299
Iteration 13/25 | Loss: 0.00096299
Iteration 14/25 | Loss: 0.00096299
Iteration 15/25 | Loss: 0.00096299
Iteration 16/25 | Loss: 0.00096299
Iteration 17/25 | Loss: 0.00096299
Iteration 18/25 | Loss: 0.00096299
Iteration 19/25 | Loss: 0.00096299
Iteration 20/25 | Loss: 0.00096299
Iteration 21/25 | Loss: 0.00096299
Iteration 22/25 | Loss: 0.00096299
Iteration 23/25 | Loss: 0.00096299
Iteration 24/25 | Loss: 0.00096299
Iteration 25/25 | Loss: 0.00096299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.72679806
Iteration 2/25 | Loss: 0.00073342
Iteration 3/25 | Loss: 0.00073341
Iteration 4/25 | Loss: 0.00073341
Iteration 5/25 | Loss: 0.00073341
Iteration 6/25 | Loss: 0.00073341
Iteration 7/25 | Loss: 0.00073341
Iteration 8/25 | Loss: 0.00073341
Iteration 9/25 | Loss: 0.00073341
Iteration 10/25 | Loss: 0.00073340
Iteration 11/25 | Loss: 0.00073340
Iteration 12/25 | Loss: 0.00073340
Iteration 13/25 | Loss: 0.00073340
Iteration 14/25 | Loss: 0.00073340
Iteration 15/25 | Loss: 0.00073340
Iteration 16/25 | Loss: 0.00073340
Iteration 17/25 | Loss: 0.00073340
Iteration 18/25 | Loss: 0.00073340
Iteration 19/25 | Loss: 0.00073340
Iteration 20/25 | Loss: 0.00073340
Iteration 21/25 | Loss: 0.00073340
Iteration 22/25 | Loss: 0.00073340
Iteration 23/25 | Loss: 0.00073340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007334044203162193, 0.0007334044203162193, 0.0007334044203162193, 0.0007334044203162193, 0.0007334044203162193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007334044203162193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073340
Iteration 2/1000 | Loss: 0.00002122
Iteration 3/1000 | Loss: 0.00001457
Iteration 4/1000 | Loss: 0.00001321
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001124
Iteration 15/1000 | Loss: 0.00001121
Iteration 16/1000 | Loss: 0.00001121
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001117
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001105
Iteration 44/1000 | Loss: 0.00001105
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001103
Iteration 48/1000 | Loss: 0.00001103
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.1012519280484412e-05, 1.1012519280484412e-05, 1.1012519280484412e-05, 1.1012519280484412e-05, 1.1012519280484412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1012519280484412e-05

Optimization complete. Final v2v error: 2.815777063369751 mm

Highest mean error: 3.1540327072143555 mm for frame 107

Lowest mean error: 2.4917004108428955 mm for frame 35

Saving results

Total time: 31.130887985229492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485669
Iteration 2/25 | Loss: 0.00104337
Iteration 3/25 | Loss: 0.00093088
Iteration 4/25 | Loss: 0.00091334
Iteration 5/25 | Loss: 0.00090765
Iteration 6/25 | Loss: 0.00090579
Iteration 7/25 | Loss: 0.00090562
Iteration 8/25 | Loss: 0.00090561
Iteration 9/25 | Loss: 0.00090561
Iteration 10/25 | Loss: 0.00090561
Iteration 11/25 | Loss: 0.00090561
Iteration 12/25 | Loss: 0.00090561
Iteration 13/25 | Loss: 0.00090561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009056099224835634, 0.0009056099224835634, 0.0009056099224835634, 0.0009056099224835634, 0.0009056099224835634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009056099224835634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64910245
Iteration 2/25 | Loss: 0.00060443
Iteration 3/25 | Loss: 0.00060443
Iteration 4/25 | Loss: 0.00060442
Iteration 5/25 | Loss: 0.00060442
Iteration 6/25 | Loss: 0.00060442
Iteration 7/25 | Loss: 0.00060442
Iteration 8/25 | Loss: 0.00060442
Iteration 9/25 | Loss: 0.00060442
Iteration 10/25 | Loss: 0.00060442
Iteration 11/25 | Loss: 0.00060442
Iteration 12/25 | Loss: 0.00060442
Iteration 13/25 | Loss: 0.00060442
Iteration 14/25 | Loss: 0.00060442
Iteration 15/25 | Loss: 0.00060442
Iteration 16/25 | Loss: 0.00060442
Iteration 17/25 | Loss: 0.00060442
Iteration 18/25 | Loss: 0.00060442
Iteration 19/25 | Loss: 0.00060442
Iteration 20/25 | Loss: 0.00060442
Iteration 21/25 | Loss: 0.00060442
Iteration 22/25 | Loss: 0.00060442
Iteration 23/25 | Loss: 0.00060442
Iteration 24/25 | Loss: 0.00060442
Iteration 25/25 | Loss: 0.00060442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060442
Iteration 2/1000 | Loss: 0.00001979
Iteration 3/1000 | Loss: 0.00001262
Iteration 4/1000 | Loss: 0.00001144
Iteration 5/1000 | Loss: 0.00001071
Iteration 6/1000 | Loss: 0.00001029
Iteration 7/1000 | Loss: 0.00001003
Iteration 8/1000 | Loss: 0.00000977
Iteration 9/1000 | Loss: 0.00000964
Iteration 10/1000 | Loss: 0.00000963
Iteration 11/1000 | Loss: 0.00000962
Iteration 12/1000 | Loss: 0.00000962
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000944
Iteration 16/1000 | Loss: 0.00000943
Iteration 17/1000 | Loss: 0.00000942
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000940
Iteration 20/1000 | Loss: 0.00000940
Iteration 21/1000 | Loss: 0.00000939
Iteration 22/1000 | Loss: 0.00000939
Iteration 23/1000 | Loss: 0.00000939
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000938
Iteration 26/1000 | Loss: 0.00000938
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000936
Iteration 30/1000 | Loss: 0.00000935
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000935
Iteration 33/1000 | Loss: 0.00000935
Iteration 34/1000 | Loss: 0.00000934
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000933
Iteration 37/1000 | Loss: 0.00000932
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000931
Iteration 44/1000 | Loss: 0.00000931
Iteration 45/1000 | Loss: 0.00000931
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000930
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000929
Iteration 51/1000 | Loss: 0.00000929
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000928
Iteration 54/1000 | Loss: 0.00000928
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000927
Iteration 57/1000 | Loss: 0.00000927
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000924
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000923
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000923
Iteration 72/1000 | Loss: 0.00000923
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000923
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000920
Iteration 107/1000 | Loss: 0.00000920
Iteration 108/1000 | Loss: 0.00000920
Iteration 109/1000 | Loss: 0.00000920
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000919
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000919
Iteration 117/1000 | Loss: 0.00000919
Iteration 118/1000 | Loss: 0.00000919
Iteration 119/1000 | Loss: 0.00000919
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000918
Iteration 122/1000 | Loss: 0.00000918
Iteration 123/1000 | Loss: 0.00000918
Iteration 124/1000 | Loss: 0.00000918
Iteration 125/1000 | Loss: 0.00000918
Iteration 126/1000 | Loss: 0.00000918
Iteration 127/1000 | Loss: 0.00000918
Iteration 128/1000 | Loss: 0.00000918
Iteration 129/1000 | Loss: 0.00000918
Iteration 130/1000 | Loss: 0.00000918
Iteration 131/1000 | Loss: 0.00000918
Iteration 132/1000 | Loss: 0.00000918
Iteration 133/1000 | Loss: 0.00000918
Iteration 134/1000 | Loss: 0.00000918
Iteration 135/1000 | Loss: 0.00000918
Iteration 136/1000 | Loss: 0.00000918
Iteration 137/1000 | Loss: 0.00000918
Iteration 138/1000 | Loss: 0.00000918
Iteration 139/1000 | Loss: 0.00000918
Iteration 140/1000 | Loss: 0.00000917
Iteration 141/1000 | Loss: 0.00000917
Iteration 142/1000 | Loss: 0.00000917
Iteration 143/1000 | Loss: 0.00000917
Iteration 144/1000 | Loss: 0.00000917
Iteration 145/1000 | Loss: 0.00000917
Iteration 146/1000 | Loss: 0.00000916
Iteration 147/1000 | Loss: 0.00000916
Iteration 148/1000 | Loss: 0.00000916
Iteration 149/1000 | Loss: 0.00000916
Iteration 150/1000 | Loss: 0.00000916
Iteration 151/1000 | Loss: 0.00000916
Iteration 152/1000 | Loss: 0.00000916
Iteration 153/1000 | Loss: 0.00000916
Iteration 154/1000 | Loss: 0.00000916
Iteration 155/1000 | Loss: 0.00000916
Iteration 156/1000 | Loss: 0.00000916
Iteration 157/1000 | Loss: 0.00000916
Iteration 158/1000 | Loss: 0.00000916
Iteration 159/1000 | Loss: 0.00000916
Iteration 160/1000 | Loss: 0.00000916
Iteration 161/1000 | Loss: 0.00000916
Iteration 162/1000 | Loss: 0.00000916
Iteration 163/1000 | Loss: 0.00000915
Iteration 164/1000 | Loss: 0.00000915
Iteration 165/1000 | Loss: 0.00000915
Iteration 166/1000 | Loss: 0.00000915
Iteration 167/1000 | Loss: 0.00000915
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000915
Iteration 172/1000 | Loss: 0.00000915
Iteration 173/1000 | Loss: 0.00000915
Iteration 174/1000 | Loss: 0.00000915
Iteration 175/1000 | Loss: 0.00000914
Iteration 176/1000 | Loss: 0.00000914
Iteration 177/1000 | Loss: 0.00000914
Iteration 178/1000 | Loss: 0.00000914
Iteration 179/1000 | Loss: 0.00000914
Iteration 180/1000 | Loss: 0.00000914
Iteration 181/1000 | Loss: 0.00000914
Iteration 182/1000 | Loss: 0.00000914
Iteration 183/1000 | Loss: 0.00000914
Iteration 184/1000 | Loss: 0.00000914
Iteration 185/1000 | Loss: 0.00000914
Iteration 186/1000 | Loss: 0.00000914
Iteration 187/1000 | Loss: 0.00000914
Iteration 188/1000 | Loss: 0.00000914
Iteration 189/1000 | Loss: 0.00000914
Iteration 190/1000 | Loss: 0.00000914
Iteration 191/1000 | Loss: 0.00000914
Iteration 192/1000 | Loss: 0.00000914
Iteration 193/1000 | Loss: 0.00000914
Iteration 194/1000 | Loss: 0.00000914
Iteration 195/1000 | Loss: 0.00000913
Iteration 196/1000 | Loss: 0.00000913
Iteration 197/1000 | Loss: 0.00000913
Iteration 198/1000 | Loss: 0.00000913
Iteration 199/1000 | Loss: 0.00000913
Iteration 200/1000 | Loss: 0.00000913
Iteration 201/1000 | Loss: 0.00000913
Iteration 202/1000 | Loss: 0.00000913
Iteration 203/1000 | Loss: 0.00000913
Iteration 204/1000 | Loss: 0.00000913
Iteration 205/1000 | Loss: 0.00000913
Iteration 206/1000 | Loss: 0.00000913
Iteration 207/1000 | Loss: 0.00000913
Iteration 208/1000 | Loss: 0.00000913
Iteration 209/1000 | Loss: 0.00000913
Iteration 210/1000 | Loss: 0.00000913
Iteration 211/1000 | Loss: 0.00000913
Iteration 212/1000 | Loss: 0.00000913
Iteration 213/1000 | Loss: 0.00000913
Iteration 214/1000 | Loss: 0.00000913
Iteration 215/1000 | Loss: 0.00000913
Iteration 216/1000 | Loss: 0.00000913
Iteration 217/1000 | Loss: 0.00000913
Iteration 218/1000 | Loss: 0.00000913
Iteration 219/1000 | Loss: 0.00000913
Iteration 220/1000 | Loss: 0.00000913
Iteration 221/1000 | Loss: 0.00000913
Iteration 222/1000 | Loss: 0.00000913
Iteration 223/1000 | Loss: 0.00000913
Iteration 224/1000 | Loss: 0.00000913
Iteration 225/1000 | Loss: 0.00000913
Iteration 226/1000 | Loss: 0.00000913
Iteration 227/1000 | Loss: 0.00000913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [9.129990758083295e-06, 9.129990758083295e-06, 9.129990758083295e-06, 9.129990758083295e-06, 9.129990758083295e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.129990758083295e-06

Optimization complete. Final v2v error: 2.583077907562256 mm

Highest mean error: 2.986462116241455 mm for frame 75

Lowest mean error: 2.3781845569610596 mm for frame 46

Saving results

Total time: 37.0561683177948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087220
Iteration 2/25 | Loss: 0.00299679
Iteration 3/25 | Loss: 0.00194230
Iteration 4/25 | Loss: 0.00133330
Iteration 5/25 | Loss: 0.00127234
Iteration 6/25 | Loss: 0.00120459
Iteration 7/25 | Loss: 0.00117946
Iteration 8/25 | Loss: 0.00116124
Iteration 9/25 | Loss: 0.00115493
Iteration 10/25 | Loss: 0.00114256
Iteration 11/25 | Loss: 0.00112429
Iteration 12/25 | Loss: 0.00110781
Iteration 13/25 | Loss: 0.00109458
Iteration 14/25 | Loss: 0.00109102
Iteration 15/25 | Loss: 0.00108814
Iteration 16/25 | Loss: 0.00108487
Iteration 17/25 | Loss: 0.00108159
Iteration 18/25 | Loss: 0.00107863
Iteration 19/25 | Loss: 0.00107734
Iteration 20/25 | Loss: 0.00107689
Iteration 21/25 | Loss: 0.00107671
Iteration 22/25 | Loss: 0.00107650
Iteration 23/25 | Loss: 0.00107630
Iteration 24/25 | Loss: 0.00107974
Iteration 25/25 | Loss: 0.00107651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28625154
Iteration 2/25 | Loss: 0.00518000
Iteration 3/25 | Loss: 0.00202100
Iteration 4/25 | Loss: 0.00202100
Iteration 5/25 | Loss: 0.00202100
Iteration 6/25 | Loss: 0.00202100
Iteration 7/25 | Loss: 0.00202100
Iteration 8/25 | Loss: 0.00202100
Iteration 9/25 | Loss: 0.00202100
Iteration 10/25 | Loss: 0.00202100
Iteration 11/25 | Loss: 0.00202100
Iteration 12/25 | Loss: 0.00202100
Iteration 13/25 | Loss: 0.00202100
Iteration 14/25 | Loss: 0.00202100
Iteration 15/25 | Loss: 0.00202100
Iteration 16/25 | Loss: 0.00202100
Iteration 17/25 | Loss: 0.00202100
Iteration 18/25 | Loss: 0.00202100
Iteration 19/25 | Loss: 0.00202100
Iteration 20/25 | Loss: 0.00202100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020209995564073324, 0.0020209995564073324, 0.0020209995564073324, 0.0020209995564073324, 0.0020209995564073324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020209995564073324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202100
Iteration 2/1000 | Loss: 0.00037535
Iteration 3/1000 | Loss: 0.00173525
Iteration 4/1000 | Loss: 0.00337805
Iteration 5/1000 | Loss: 0.00053498
Iteration 6/1000 | Loss: 0.00070839
Iteration 7/1000 | Loss: 0.00118620
Iteration 8/1000 | Loss: 0.00021038
Iteration 9/1000 | Loss: 0.00024883
Iteration 10/1000 | Loss: 0.00078580
Iteration 11/1000 | Loss: 0.00047316
Iteration 12/1000 | Loss: 0.00010178
Iteration 13/1000 | Loss: 0.00009590
Iteration 14/1000 | Loss: 0.00013055
Iteration 15/1000 | Loss: 0.00011611
Iteration 16/1000 | Loss: 0.00011576
Iteration 17/1000 | Loss: 0.00007605
Iteration 18/1000 | Loss: 0.00007303
Iteration 19/1000 | Loss: 0.00042121
Iteration 20/1000 | Loss: 0.00023633
Iteration 21/1000 | Loss: 0.00028829
Iteration 22/1000 | Loss: 0.00263810
Iteration 23/1000 | Loss: 0.00106550
Iteration 24/1000 | Loss: 0.00107115
Iteration 25/1000 | Loss: 0.00471178
Iteration 26/1000 | Loss: 0.00172725
Iteration 27/1000 | Loss: 0.00548138
Iteration 28/1000 | Loss: 0.00428010
Iteration 29/1000 | Loss: 0.00313224
Iteration 30/1000 | Loss: 0.00251185
Iteration 31/1000 | Loss: 0.00283657
Iteration 32/1000 | Loss: 0.00107891
Iteration 33/1000 | Loss: 0.00210417
Iteration 34/1000 | Loss: 0.00162066
Iteration 35/1000 | Loss: 0.00187477
Iteration 36/1000 | Loss: 0.00128736
Iteration 37/1000 | Loss: 0.00074692
Iteration 38/1000 | Loss: 0.00102822
Iteration 39/1000 | Loss: 0.00030492
Iteration 40/1000 | Loss: 0.00013580
Iteration 41/1000 | Loss: 0.00016749
Iteration 42/1000 | Loss: 0.00076977
Iteration 43/1000 | Loss: 0.00102974
Iteration 44/1000 | Loss: 0.00089693
Iteration 45/1000 | Loss: 0.00087934
Iteration 46/1000 | Loss: 0.00094866
Iteration 47/1000 | Loss: 0.00124983
Iteration 48/1000 | Loss: 0.00168148
Iteration 49/1000 | Loss: 0.00129644
Iteration 50/1000 | Loss: 0.00082575
Iteration 51/1000 | Loss: 0.00128318
Iteration 52/1000 | Loss: 0.00218210
Iteration 53/1000 | Loss: 0.00103538
Iteration 54/1000 | Loss: 0.00077718
Iteration 55/1000 | Loss: 0.00180782
Iteration 56/1000 | Loss: 0.00122081
Iteration 57/1000 | Loss: 0.00061804
Iteration 58/1000 | Loss: 0.00058015
Iteration 59/1000 | Loss: 0.00139531
Iteration 60/1000 | Loss: 0.00096690
Iteration 61/1000 | Loss: 0.00032215
Iteration 62/1000 | Loss: 0.00050516
Iteration 63/1000 | Loss: 0.00078106
Iteration 64/1000 | Loss: 0.00050459
Iteration 65/1000 | Loss: 0.00037520
Iteration 66/1000 | Loss: 0.00249631
Iteration 67/1000 | Loss: 0.00031882
Iteration 68/1000 | Loss: 0.00039349
Iteration 69/1000 | Loss: 0.00034148
Iteration 70/1000 | Loss: 0.00031991
Iteration 71/1000 | Loss: 0.00013342
Iteration 72/1000 | Loss: 0.00082204
Iteration 73/1000 | Loss: 0.00044362
Iteration 74/1000 | Loss: 0.00026119
Iteration 75/1000 | Loss: 0.00057299
Iteration 76/1000 | Loss: 0.00123565
Iteration 77/1000 | Loss: 0.00066929
Iteration 78/1000 | Loss: 0.00049775
Iteration 79/1000 | Loss: 0.00009853
Iteration 80/1000 | Loss: 0.00018061
Iteration 81/1000 | Loss: 0.00004969
Iteration 82/1000 | Loss: 0.00032146
Iteration 83/1000 | Loss: 0.00051746
Iteration 84/1000 | Loss: 0.00004320
Iteration 85/1000 | Loss: 0.00008073
Iteration 86/1000 | Loss: 0.00003556
Iteration 87/1000 | Loss: 0.00025760
Iteration 88/1000 | Loss: 0.00038150
Iteration 89/1000 | Loss: 0.00033337
Iteration 90/1000 | Loss: 0.00034075
Iteration 91/1000 | Loss: 0.00019573
Iteration 92/1000 | Loss: 0.00055721
Iteration 93/1000 | Loss: 0.00033132
Iteration 94/1000 | Loss: 0.00033063
Iteration 95/1000 | Loss: 0.00040718
Iteration 96/1000 | Loss: 0.00015570
Iteration 97/1000 | Loss: 0.00019692
Iteration 98/1000 | Loss: 0.00012179
Iteration 99/1000 | Loss: 0.00004516
Iteration 100/1000 | Loss: 0.00179144
Iteration 101/1000 | Loss: 0.00007269
Iteration 102/1000 | Loss: 0.00009414
Iteration 103/1000 | Loss: 0.00003518
Iteration 104/1000 | Loss: 0.00016873
Iteration 105/1000 | Loss: 0.00009026
Iteration 106/1000 | Loss: 0.00016126
Iteration 107/1000 | Loss: 0.00127499
Iteration 108/1000 | Loss: 0.00022967
Iteration 109/1000 | Loss: 0.00029045
Iteration 110/1000 | Loss: 0.00020440
Iteration 111/1000 | Loss: 0.00057056
Iteration 112/1000 | Loss: 0.00020578
Iteration 113/1000 | Loss: 0.00021586
Iteration 114/1000 | Loss: 0.00003192
Iteration 115/1000 | Loss: 0.00029831
Iteration 116/1000 | Loss: 0.00003079
Iteration 117/1000 | Loss: 0.00002620
Iteration 118/1000 | Loss: 0.00002467
Iteration 119/1000 | Loss: 0.00002369
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002286
Iteration 122/1000 | Loss: 0.00002251
Iteration 123/1000 | Loss: 0.00002222
Iteration 124/1000 | Loss: 0.00002204
Iteration 125/1000 | Loss: 0.00002187
Iteration 126/1000 | Loss: 0.00002179
Iteration 127/1000 | Loss: 0.00002177
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002176
Iteration 130/1000 | Loss: 0.00002174
Iteration 131/1000 | Loss: 0.00002170
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00002166
Iteration 136/1000 | Loss: 0.00002165
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00002162
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002159
Iteration 144/1000 | Loss: 0.00002159
Iteration 145/1000 | Loss: 0.00002157
Iteration 146/1000 | Loss: 0.00002157
Iteration 147/1000 | Loss: 0.00002154
Iteration 148/1000 | Loss: 0.00002154
Iteration 149/1000 | Loss: 0.00002153
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002152
Iteration 153/1000 | Loss: 0.00002149
Iteration 154/1000 | Loss: 0.00002149
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002147
Iteration 160/1000 | Loss: 0.00002147
Iteration 161/1000 | Loss: 0.00002147
Iteration 162/1000 | Loss: 0.00002147
Iteration 163/1000 | Loss: 0.00002146
Iteration 164/1000 | Loss: 0.00002146
Iteration 165/1000 | Loss: 0.00002146
Iteration 166/1000 | Loss: 0.00002145
Iteration 167/1000 | Loss: 0.00002145
Iteration 168/1000 | Loss: 0.00002145
Iteration 169/1000 | Loss: 0.00002145
Iteration 170/1000 | Loss: 0.00002144
Iteration 171/1000 | Loss: 0.00002144
Iteration 172/1000 | Loss: 0.00002144
Iteration 173/1000 | Loss: 0.00002144
Iteration 174/1000 | Loss: 0.00002143
Iteration 175/1000 | Loss: 0.00002143
Iteration 176/1000 | Loss: 0.00002143
Iteration 177/1000 | Loss: 0.00002143
Iteration 178/1000 | Loss: 0.00002143
Iteration 179/1000 | Loss: 0.00002143
Iteration 180/1000 | Loss: 0.00002143
Iteration 181/1000 | Loss: 0.00002142
Iteration 182/1000 | Loss: 0.00002142
Iteration 183/1000 | Loss: 0.00002142
Iteration 184/1000 | Loss: 0.00002142
Iteration 185/1000 | Loss: 0.00002142
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002141
Iteration 188/1000 | Loss: 0.00002141
Iteration 189/1000 | Loss: 0.00002141
Iteration 190/1000 | Loss: 0.00002141
Iteration 191/1000 | Loss: 0.00002141
Iteration 192/1000 | Loss: 0.00002140
Iteration 193/1000 | Loss: 0.00002140
Iteration 194/1000 | Loss: 0.00002140
Iteration 195/1000 | Loss: 0.00002139
Iteration 196/1000 | Loss: 0.00002139
Iteration 197/1000 | Loss: 0.00002139
Iteration 198/1000 | Loss: 0.00002139
Iteration 199/1000 | Loss: 0.00002139
Iteration 200/1000 | Loss: 0.00002138
Iteration 201/1000 | Loss: 0.00002138
Iteration 202/1000 | Loss: 0.00002138
Iteration 203/1000 | Loss: 0.00002138
Iteration 204/1000 | Loss: 0.00002138
Iteration 205/1000 | Loss: 0.00002138
Iteration 206/1000 | Loss: 0.00002138
Iteration 207/1000 | Loss: 0.00002138
Iteration 208/1000 | Loss: 0.00002137
Iteration 209/1000 | Loss: 0.00002137
Iteration 210/1000 | Loss: 0.00002137
Iteration 211/1000 | Loss: 0.00002137
Iteration 212/1000 | Loss: 0.00002137
Iteration 213/1000 | Loss: 0.00002137
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002137
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002137
Iteration 218/1000 | Loss: 0.00002137
Iteration 219/1000 | Loss: 0.00002137
Iteration 220/1000 | Loss: 0.00002137
Iteration 221/1000 | Loss: 0.00002136
Iteration 222/1000 | Loss: 0.00002136
Iteration 223/1000 | Loss: 0.00002136
Iteration 224/1000 | Loss: 0.00002136
Iteration 225/1000 | Loss: 0.00002135
Iteration 226/1000 | Loss: 0.00002135
Iteration 227/1000 | Loss: 0.00002135
Iteration 228/1000 | Loss: 0.00002134
Iteration 229/1000 | Loss: 0.00002134
Iteration 230/1000 | Loss: 0.00002134
Iteration 231/1000 | Loss: 0.00002134
Iteration 232/1000 | Loss: 0.00002134
Iteration 233/1000 | Loss: 0.00002134
Iteration 234/1000 | Loss: 0.00002134
Iteration 235/1000 | Loss: 0.00002134
Iteration 236/1000 | Loss: 0.00002134
Iteration 237/1000 | Loss: 0.00002134
Iteration 238/1000 | Loss: 0.00002134
Iteration 239/1000 | Loss: 0.00002134
Iteration 240/1000 | Loss: 0.00002134
Iteration 241/1000 | Loss: 0.00002134
Iteration 242/1000 | Loss: 0.00002133
Iteration 243/1000 | Loss: 0.00002133
Iteration 244/1000 | Loss: 0.00002133
Iteration 245/1000 | Loss: 0.00002133
Iteration 246/1000 | Loss: 0.00002133
Iteration 247/1000 | Loss: 0.00002133
Iteration 248/1000 | Loss: 0.00002133
Iteration 249/1000 | Loss: 0.00002133
Iteration 250/1000 | Loss: 0.00002133
Iteration 251/1000 | Loss: 0.00002133
Iteration 252/1000 | Loss: 0.00002133
Iteration 253/1000 | Loss: 0.00002133
Iteration 254/1000 | Loss: 0.00002133
Iteration 255/1000 | Loss: 0.00002133
Iteration 256/1000 | Loss: 0.00002133
Iteration 257/1000 | Loss: 0.00002133
Iteration 258/1000 | Loss: 0.00002133
Iteration 259/1000 | Loss: 0.00002133
Iteration 260/1000 | Loss: 0.00002133
Iteration 261/1000 | Loss: 0.00002133
Iteration 262/1000 | Loss: 0.00002133
Iteration 263/1000 | Loss: 0.00002133
Iteration 264/1000 | Loss: 0.00002132
Iteration 265/1000 | Loss: 0.00002132
Iteration 266/1000 | Loss: 0.00002132
Iteration 267/1000 | Loss: 0.00002132
Iteration 268/1000 | Loss: 0.00002132
Iteration 269/1000 | Loss: 0.00002132
Iteration 270/1000 | Loss: 0.00002132
Iteration 271/1000 | Loss: 0.00002132
Iteration 272/1000 | Loss: 0.00002132
Iteration 273/1000 | Loss: 0.00002132
Iteration 274/1000 | Loss: 0.00002132
Iteration 275/1000 | Loss: 0.00002132
Iteration 276/1000 | Loss: 0.00002132
Iteration 277/1000 | Loss: 0.00002132
Iteration 278/1000 | Loss: 0.00002132
Iteration 279/1000 | Loss: 0.00002132
Iteration 280/1000 | Loss: 0.00002132
Iteration 281/1000 | Loss: 0.00002132
Iteration 282/1000 | Loss: 0.00002132
Iteration 283/1000 | Loss: 0.00002132
Iteration 284/1000 | Loss: 0.00002132
Iteration 285/1000 | Loss: 0.00002131
Iteration 286/1000 | Loss: 0.00002131
Iteration 287/1000 | Loss: 0.00002131
Iteration 288/1000 | Loss: 0.00002131
Iteration 289/1000 | Loss: 0.00002131
Iteration 290/1000 | Loss: 0.00002131
Iteration 291/1000 | Loss: 0.00002131
Iteration 292/1000 | Loss: 0.00002131
Iteration 293/1000 | Loss: 0.00002131
Iteration 294/1000 | Loss: 0.00002131
Iteration 295/1000 | Loss: 0.00002131
Iteration 296/1000 | Loss: 0.00002131
Iteration 297/1000 | Loss: 0.00002131
Iteration 298/1000 | Loss: 0.00002131
Iteration 299/1000 | Loss: 0.00002131
Iteration 300/1000 | Loss: 0.00002131
Iteration 301/1000 | Loss: 0.00002130
Iteration 302/1000 | Loss: 0.00002130
Iteration 303/1000 | Loss: 0.00002130
Iteration 304/1000 | Loss: 0.00002130
Iteration 305/1000 | Loss: 0.00002130
Iteration 306/1000 | Loss: 0.00002130
Iteration 307/1000 | Loss: 0.00002130
Iteration 308/1000 | Loss: 0.00002130
Iteration 309/1000 | Loss: 0.00002130
Iteration 310/1000 | Loss: 0.00002130
Iteration 311/1000 | Loss: 0.00002130
Iteration 312/1000 | Loss: 0.00002130
Iteration 313/1000 | Loss: 0.00002130
Iteration 314/1000 | Loss: 0.00002130
Iteration 315/1000 | Loss: 0.00002130
Iteration 316/1000 | Loss: 0.00002130
Iteration 317/1000 | Loss: 0.00002130
Iteration 318/1000 | Loss: 0.00002129
Iteration 319/1000 | Loss: 0.00002129
Iteration 320/1000 | Loss: 0.00002129
Iteration 321/1000 | Loss: 0.00002129
Iteration 322/1000 | Loss: 0.00002129
Iteration 323/1000 | Loss: 0.00002129
Iteration 324/1000 | Loss: 0.00002129
Iteration 325/1000 | Loss: 0.00002129
Iteration 326/1000 | Loss: 0.00002129
Iteration 327/1000 | Loss: 0.00002129
Iteration 328/1000 | Loss: 0.00002129
Iteration 329/1000 | Loss: 0.00002129
Iteration 330/1000 | Loss: 0.00002129
Iteration 331/1000 | Loss: 0.00002129
Iteration 332/1000 | Loss: 0.00002128
Iteration 333/1000 | Loss: 0.00002128
Iteration 334/1000 | Loss: 0.00002128
Iteration 335/1000 | Loss: 0.00002128
Iteration 336/1000 | Loss: 0.00002128
Iteration 337/1000 | Loss: 0.00002128
Iteration 338/1000 | Loss: 0.00002128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.128381129296031e-05, 2.128381129296031e-05, 2.128381129296031e-05, 2.128381129296031e-05, 2.128381129296031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.128381129296031e-05

Optimization complete. Final v2v error: 2.9682986736297607 mm

Highest mean error: 17.051973342895508 mm for frame 24

Lowest mean error: 2.2646286487579346 mm for frame 137

Saving results

Total time: 272.32145285606384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841898
Iteration 2/25 | Loss: 0.00114984
Iteration 3/25 | Loss: 0.00097487
Iteration 4/25 | Loss: 0.00096907
Iteration 5/25 | Loss: 0.00093990
Iteration 6/25 | Loss: 0.00093558
Iteration 7/25 | Loss: 0.00093432
Iteration 8/25 | Loss: 0.00093318
Iteration 9/25 | Loss: 0.00093536
Iteration 10/25 | Loss: 0.00093135
Iteration 11/25 | Loss: 0.00092981
Iteration 12/25 | Loss: 0.00092910
Iteration 13/25 | Loss: 0.00092899
Iteration 14/25 | Loss: 0.00092899
Iteration 15/25 | Loss: 0.00092899
Iteration 16/25 | Loss: 0.00092898
Iteration 17/25 | Loss: 0.00092898
Iteration 18/25 | Loss: 0.00092898
Iteration 19/25 | Loss: 0.00092898
Iteration 20/25 | Loss: 0.00092898
Iteration 21/25 | Loss: 0.00092898
Iteration 22/25 | Loss: 0.00092898
Iteration 23/25 | Loss: 0.00092898
Iteration 24/25 | Loss: 0.00092898
Iteration 25/25 | Loss: 0.00092898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38082731
Iteration 2/25 | Loss: 0.00068969
Iteration 3/25 | Loss: 0.00068960
Iteration 4/25 | Loss: 0.00068960
Iteration 5/25 | Loss: 0.00068960
Iteration 6/25 | Loss: 0.00068960
Iteration 7/25 | Loss: 0.00068960
Iteration 8/25 | Loss: 0.00068960
Iteration 9/25 | Loss: 0.00068960
Iteration 10/25 | Loss: 0.00068960
Iteration 11/25 | Loss: 0.00068960
Iteration 12/25 | Loss: 0.00068960
Iteration 13/25 | Loss: 0.00068960
Iteration 14/25 | Loss: 0.00068960
Iteration 15/25 | Loss: 0.00068960
Iteration 16/25 | Loss: 0.00068960
Iteration 17/25 | Loss: 0.00068960
Iteration 18/25 | Loss: 0.00068960
Iteration 19/25 | Loss: 0.00068960
Iteration 20/25 | Loss: 0.00068960
Iteration 21/25 | Loss: 0.00068960
Iteration 22/25 | Loss: 0.00068960
Iteration 23/25 | Loss: 0.00068960
Iteration 24/25 | Loss: 0.00068960
Iteration 25/25 | Loss: 0.00068960

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068960
Iteration 2/1000 | Loss: 0.00002514
Iteration 3/1000 | Loss: 0.00001394
Iteration 4/1000 | Loss: 0.00001234
Iteration 5/1000 | Loss: 0.00001165
Iteration 6/1000 | Loss: 0.00001124
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001084
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001054
Iteration 20/1000 | Loss: 0.00001053
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001044
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001039
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001037
Iteration 35/1000 | Loss: 0.00001037
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001035
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001033
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001031
Iteration 47/1000 | Loss: 0.00001031
Iteration 48/1000 | Loss: 0.00001031
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001030
Iteration 51/1000 | Loss: 0.00001030
Iteration 52/1000 | Loss: 0.00001029
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001028
Iteration 55/1000 | Loss: 0.00001027
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001026
Iteration 61/1000 | Loss: 0.00001026
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001025
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001024
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001019
Iteration 89/1000 | Loss: 0.00001019
Iteration 90/1000 | Loss: 0.00001019
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001018
Iteration 97/1000 | Loss: 0.00001018
Iteration 98/1000 | Loss: 0.00001018
Iteration 99/1000 | Loss: 0.00001017
Iteration 100/1000 | Loss: 0.00001017
Iteration 101/1000 | Loss: 0.00001017
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001017
Iteration 105/1000 | Loss: 0.00001016
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001016
Iteration 109/1000 | Loss: 0.00001016
Iteration 110/1000 | Loss: 0.00001016
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001016
Iteration 115/1000 | Loss: 0.00001015
Iteration 116/1000 | Loss: 0.00001015
Iteration 117/1000 | Loss: 0.00001015
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001015
Iteration 121/1000 | Loss: 0.00001015
Iteration 122/1000 | Loss: 0.00001015
Iteration 123/1000 | Loss: 0.00001015
Iteration 124/1000 | Loss: 0.00001015
Iteration 125/1000 | Loss: 0.00001015
Iteration 126/1000 | Loss: 0.00001015
Iteration 127/1000 | Loss: 0.00001015
Iteration 128/1000 | Loss: 0.00001014
Iteration 129/1000 | Loss: 0.00001014
Iteration 130/1000 | Loss: 0.00001014
Iteration 131/1000 | Loss: 0.00001014
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001014
Iteration 134/1000 | Loss: 0.00001014
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001013
Iteration 138/1000 | Loss: 0.00001013
Iteration 139/1000 | Loss: 0.00001013
Iteration 140/1000 | Loss: 0.00001013
Iteration 141/1000 | Loss: 0.00001013
Iteration 142/1000 | Loss: 0.00001013
Iteration 143/1000 | Loss: 0.00001013
Iteration 144/1000 | Loss: 0.00001013
Iteration 145/1000 | Loss: 0.00001013
Iteration 146/1000 | Loss: 0.00001013
Iteration 147/1000 | Loss: 0.00001013
Iteration 148/1000 | Loss: 0.00001013
Iteration 149/1000 | Loss: 0.00001013
Iteration 150/1000 | Loss: 0.00001013
Iteration 151/1000 | Loss: 0.00001013
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.0129817383131012e-05, 1.0129817383131012e-05, 1.0129817383131012e-05, 1.0129817383131012e-05, 1.0129817383131012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0129817383131012e-05

Optimization complete. Final v2v error: 2.5585997104644775 mm

Highest mean error: 4.11496114730835 mm for frame 223

Lowest mean error: 2.0941269397735596 mm for frame 62

Saving results

Total time: 53.66891264915466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882309
Iteration 2/25 | Loss: 0.00142152
Iteration 3/25 | Loss: 0.00102083
Iteration 4/25 | Loss: 0.00096693
Iteration 5/25 | Loss: 0.00095943
Iteration 6/25 | Loss: 0.00095773
Iteration 7/25 | Loss: 0.00095753
Iteration 8/25 | Loss: 0.00095753
Iteration 9/25 | Loss: 0.00095753
Iteration 10/25 | Loss: 0.00095753
Iteration 11/25 | Loss: 0.00095753
Iteration 12/25 | Loss: 0.00095753
Iteration 13/25 | Loss: 0.00095753
Iteration 14/25 | Loss: 0.00095753
Iteration 15/25 | Loss: 0.00095753
Iteration 16/25 | Loss: 0.00095753
Iteration 17/25 | Loss: 0.00095753
Iteration 18/25 | Loss: 0.00095753
Iteration 19/25 | Loss: 0.00095753
Iteration 20/25 | Loss: 0.00095753
Iteration 21/25 | Loss: 0.00095753
Iteration 22/25 | Loss: 0.00095753
Iteration 23/25 | Loss: 0.00095753
Iteration 24/25 | Loss: 0.00095753
Iteration 25/25 | Loss: 0.00095753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33392024
Iteration 2/25 | Loss: 0.00060862
Iteration 3/25 | Loss: 0.00060862
Iteration 4/25 | Loss: 0.00060862
Iteration 5/25 | Loss: 0.00060861
Iteration 6/25 | Loss: 0.00060861
Iteration 7/25 | Loss: 0.00060861
Iteration 8/25 | Loss: 0.00060861
Iteration 9/25 | Loss: 0.00060861
Iteration 10/25 | Loss: 0.00060861
Iteration 11/25 | Loss: 0.00060861
Iteration 12/25 | Loss: 0.00060861
Iteration 13/25 | Loss: 0.00060861
Iteration 14/25 | Loss: 0.00060861
Iteration 15/25 | Loss: 0.00060861
Iteration 16/25 | Loss: 0.00060861
Iteration 17/25 | Loss: 0.00060861
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006086133071221411, 0.0006086133071221411, 0.0006086133071221411, 0.0006086133071221411, 0.0006086133071221411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006086133071221411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060861
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001426
Iteration 4/1000 | Loss: 0.00001278
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001118
Iteration 7/1000 | Loss: 0.00001078
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00001003
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000987
Iteration 14/1000 | Loss: 0.00000986
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000985
Iteration 17/1000 | Loss: 0.00000984
Iteration 18/1000 | Loss: 0.00000983
Iteration 19/1000 | Loss: 0.00000981
Iteration 20/1000 | Loss: 0.00000980
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000978
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000978
Iteration 26/1000 | Loss: 0.00000977
Iteration 27/1000 | Loss: 0.00000976
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000976
Iteration 32/1000 | Loss: 0.00000976
Iteration 33/1000 | Loss: 0.00000976
Iteration 34/1000 | Loss: 0.00000975
Iteration 35/1000 | Loss: 0.00000975
Iteration 36/1000 | Loss: 0.00000975
Iteration 37/1000 | Loss: 0.00000975
Iteration 38/1000 | Loss: 0.00000975
Iteration 39/1000 | Loss: 0.00000974
Iteration 40/1000 | Loss: 0.00000974
Iteration 41/1000 | Loss: 0.00000973
Iteration 42/1000 | Loss: 0.00000973
Iteration 43/1000 | Loss: 0.00000973
Iteration 44/1000 | Loss: 0.00000973
Iteration 45/1000 | Loss: 0.00000972
Iteration 46/1000 | Loss: 0.00000972
Iteration 47/1000 | Loss: 0.00000972
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000971
Iteration 50/1000 | Loss: 0.00000971
Iteration 51/1000 | Loss: 0.00000971
Iteration 52/1000 | Loss: 0.00000970
Iteration 53/1000 | Loss: 0.00000970
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000968
Iteration 58/1000 | Loss: 0.00000968
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000967
Iteration 61/1000 | Loss: 0.00000967
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000966
Iteration 64/1000 | Loss: 0.00000965
Iteration 65/1000 | Loss: 0.00000965
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000964
Iteration 68/1000 | Loss: 0.00000964
Iteration 69/1000 | Loss: 0.00000964
Iteration 70/1000 | Loss: 0.00000964
Iteration 71/1000 | Loss: 0.00000964
Iteration 72/1000 | Loss: 0.00000963
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000963
Iteration 76/1000 | Loss: 0.00000963
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000962
Iteration 79/1000 | Loss: 0.00000962
Iteration 80/1000 | Loss: 0.00000962
Iteration 81/1000 | Loss: 0.00000962
Iteration 82/1000 | Loss: 0.00000962
Iteration 83/1000 | Loss: 0.00000962
Iteration 84/1000 | Loss: 0.00000962
Iteration 85/1000 | Loss: 0.00000962
Iteration 86/1000 | Loss: 0.00000962
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000962
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [9.60452962317504e-06, 9.60452962317504e-06, 9.60452962317504e-06, 9.60452962317504e-06, 9.60452962317504e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.60452962317504e-06

Optimization complete. Final v2v error: 2.6004865169525146 mm

Highest mean error: 3.1364617347717285 mm for frame 97

Lowest mean error: 2.262842893600464 mm for frame 35

Saving results

Total time: 37.74495029449463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00692148
Iteration 2/25 | Loss: 0.00114242
Iteration 3/25 | Loss: 0.00100916
Iteration 4/25 | Loss: 0.00099060
Iteration 5/25 | Loss: 0.00098472
Iteration 6/25 | Loss: 0.00098317
Iteration 7/25 | Loss: 0.00098317
Iteration 8/25 | Loss: 0.00098317
Iteration 9/25 | Loss: 0.00098317
Iteration 10/25 | Loss: 0.00098317
Iteration 11/25 | Loss: 0.00098317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000983172096312046, 0.000983172096312046, 0.000983172096312046, 0.000983172096312046, 0.000983172096312046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983172096312046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32616293
Iteration 2/25 | Loss: 0.00066355
Iteration 3/25 | Loss: 0.00066354
Iteration 4/25 | Loss: 0.00066354
Iteration 5/25 | Loss: 0.00066354
Iteration 6/25 | Loss: 0.00066354
Iteration 7/25 | Loss: 0.00066354
Iteration 8/25 | Loss: 0.00066354
Iteration 9/25 | Loss: 0.00066354
Iteration 10/25 | Loss: 0.00066354
Iteration 11/25 | Loss: 0.00066354
Iteration 12/25 | Loss: 0.00066354
Iteration 13/25 | Loss: 0.00066354
Iteration 14/25 | Loss: 0.00066354
Iteration 15/25 | Loss: 0.00066354
Iteration 16/25 | Loss: 0.00066354
Iteration 17/25 | Loss: 0.00066354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006635374738834798, 0.0006635374738834798, 0.0006635374738834798, 0.0006635374738834798, 0.0006635374738834798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006635374738834798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066354
Iteration 2/1000 | Loss: 0.00003608
Iteration 3/1000 | Loss: 0.00002104
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001511
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001455
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001446
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001446
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001445
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.438697745470563e-05, 1.438697745470563e-05, 1.438697745470563e-05, 1.438697745470563e-05, 1.438697745470563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438697745470563e-05

Optimization complete. Final v2v error: 3.2135612964630127 mm

Highest mean error: 3.5143117904663086 mm for frame 12

Lowest mean error: 2.7205677032470703 mm for frame 230

Saving results

Total time: 37.727208375930786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422236
Iteration 2/25 | Loss: 0.00107813
Iteration 3/25 | Loss: 0.00098167
Iteration 4/25 | Loss: 0.00096614
Iteration 5/25 | Loss: 0.00096052
Iteration 6/25 | Loss: 0.00095937
Iteration 7/25 | Loss: 0.00095930
Iteration 8/25 | Loss: 0.00095930
Iteration 9/25 | Loss: 0.00095930
Iteration 10/25 | Loss: 0.00095930
Iteration 11/25 | Loss: 0.00095930
Iteration 12/25 | Loss: 0.00095930
Iteration 13/25 | Loss: 0.00095930
Iteration 14/25 | Loss: 0.00095930
Iteration 15/25 | Loss: 0.00095930
Iteration 16/25 | Loss: 0.00095930
Iteration 17/25 | Loss: 0.00095930
Iteration 18/25 | Loss: 0.00095930
Iteration 19/25 | Loss: 0.00095930
Iteration 20/25 | Loss: 0.00095930
Iteration 21/25 | Loss: 0.00095930
Iteration 22/25 | Loss: 0.00095930
Iteration 23/25 | Loss: 0.00095930
Iteration 24/25 | Loss: 0.00095930
Iteration 25/25 | Loss: 0.00095930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36731720
Iteration 2/25 | Loss: 0.00060938
Iteration 3/25 | Loss: 0.00060937
Iteration 4/25 | Loss: 0.00060937
Iteration 5/25 | Loss: 0.00060937
Iteration 6/25 | Loss: 0.00060937
Iteration 7/25 | Loss: 0.00060937
Iteration 8/25 | Loss: 0.00060937
Iteration 9/25 | Loss: 0.00060937
Iteration 10/25 | Loss: 0.00060937
Iteration 11/25 | Loss: 0.00060937
Iteration 12/25 | Loss: 0.00060937
Iteration 13/25 | Loss: 0.00060937
Iteration 14/25 | Loss: 0.00060937
Iteration 15/25 | Loss: 0.00060937
Iteration 16/25 | Loss: 0.00060937
Iteration 17/25 | Loss: 0.00060937
Iteration 18/25 | Loss: 0.00060937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006093725678510964, 0.0006093725678510964, 0.0006093725678510964, 0.0006093725678510964, 0.0006093725678510964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006093725678510964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060937
Iteration 2/1000 | Loss: 0.00002273
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001560
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001458
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001409
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001409
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001407
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001404
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001404
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001402
Iteration 75/1000 | Loss: 0.00001402
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001401
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001400
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001399
Iteration 107/1000 | Loss: 0.00001399
Iteration 108/1000 | Loss: 0.00001399
Iteration 109/1000 | Loss: 0.00001399
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001398
Iteration 116/1000 | Loss: 0.00001398
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001397
Iteration 131/1000 | Loss: 0.00001397
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001397
Iteration 135/1000 | Loss: 0.00001397
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001397
Iteration 143/1000 | Loss: 0.00001397
Iteration 144/1000 | Loss: 0.00001397
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001397
Iteration 148/1000 | Loss: 0.00001397
Iteration 149/1000 | Loss: 0.00001397
Iteration 150/1000 | Loss: 0.00001397
Iteration 151/1000 | Loss: 0.00001397
Iteration 152/1000 | Loss: 0.00001397
Iteration 153/1000 | Loss: 0.00001397
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001397
Iteration 156/1000 | Loss: 0.00001397
Iteration 157/1000 | Loss: 0.00001397
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001397
Iteration 176/1000 | Loss: 0.00001397
Iteration 177/1000 | Loss: 0.00001397
Iteration 178/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3973108252685051e-05, 1.3973108252685051e-05, 1.3973108252685051e-05, 1.3973108252685051e-05, 1.3973108252685051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3973108252685051e-05

Optimization complete. Final v2v error: 3.1358885765075684 mm

Highest mean error: 3.6394009590148926 mm for frame 24

Lowest mean error: 2.659122943878174 mm for frame 45

Saving results

Total time: 32.028677463531494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870770
Iteration 2/25 | Loss: 0.00138581
Iteration 3/25 | Loss: 0.00104015
Iteration 4/25 | Loss: 0.00099099
Iteration 5/25 | Loss: 0.00098783
Iteration 6/25 | Loss: 0.00098666
Iteration 7/25 | Loss: 0.00098666
Iteration 8/25 | Loss: 0.00098666
Iteration 9/25 | Loss: 0.00098666
Iteration 10/25 | Loss: 0.00098666
Iteration 11/25 | Loss: 0.00098666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009866588516160846, 0.0009866588516160846, 0.0009866588516160846, 0.0009866588516160846, 0.0009866588516160846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009866588516160846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29725158
Iteration 2/25 | Loss: 0.00058920
Iteration 3/25 | Loss: 0.00058920
Iteration 4/25 | Loss: 0.00058920
Iteration 5/25 | Loss: 0.00058920
Iteration 6/25 | Loss: 0.00058920
Iteration 7/25 | Loss: 0.00058920
Iteration 8/25 | Loss: 0.00058920
Iteration 9/25 | Loss: 0.00058920
Iteration 10/25 | Loss: 0.00058920
Iteration 11/25 | Loss: 0.00058920
Iteration 12/25 | Loss: 0.00058920
Iteration 13/25 | Loss: 0.00058920
Iteration 14/25 | Loss: 0.00058920
Iteration 15/25 | Loss: 0.00058920
Iteration 16/25 | Loss: 0.00058920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005891986656934023, 0.0005891986656934023, 0.0005891986656934023, 0.0005891986656934023, 0.0005891986656934023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005891986656934023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058920
Iteration 2/1000 | Loss: 0.00002221
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001307
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001212
Iteration 33/1000 | Loss: 0.00001212
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001208
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001206
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001205
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.2004327800241299e-05, 1.2004327800241299e-05, 1.2004327800241299e-05, 1.2004327800241299e-05, 1.2004327800241299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2004327800241299e-05

Optimization complete. Final v2v error: 2.904667377471924 mm

Highest mean error: 3.0155932903289795 mm for frame 52

Lowest mean error: 2.6285479068756104 mm for frame 169

Saving results

Total time: 33.01646423339844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520129
Iteration 2/25 | Loss: 0.00125552
Iteration 3/25 | Loss: 0.00103361
Iteration 4/25 | Loss: 0.00100745
Iteration 5/25 | Loss: 0.00099980
Iteration 6/25 | Loss: 0.00099908
Iteration 7/25 | Loss: 0.00099908
Iteration 8/25 | Loss: 0.00099908
Iteration 9/25 | Loss: 0.00099908
Iteration 10/25 | Loss: 0.00099908
Iteration 11/25 | Loss: 0.00099908
Iteration 12/25 | Loss: 0.00099908
Iteration 13/25 | Loss: 0.00099908
Iteration 14/25 | Loss: 0.00099908
Iteration 15/25 | Loss: 0.00099908
Iteration 16/25 | Loss: 0.00099908
Iteration 17/25 | Loss: 0.00099908
Iteration 18/25 | Loss: 0.00099908
Iteration 19/25 | Loss: 0.00099908
Iteration 20/25 | Loss: 0.00099908
Iteration 21/25 | Loss: 0.00099908
Iteration 22/25 | Loss: 0.00099908
Iteration 23/25 | Loss: 0.00099908
Iteration 24/25 | Loss: 0.00099908
Iteration 25/25 | Loss: 0.00099908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71548229
Iteration 2/25 | Loss: 0.00069687
Iteration 3/25 | Loss: 0.00069687
Iteration 4/25 | Loss: 0.00069687
Iteration 5/25 | Loss: 0.00069687
Iteration 6/25 | Loss: 0.00069687
Iteration 7/25 | Loss: 0.00069687
Iteration 8/25 | Loss: 0.00069687
Iteration 9/25 | Loss: 0.00069687
Iteration 10/25 | Loss: 0.00069687
Iteration 11/25 | Loss: 0.00069687
Iteration 12/25 | Loss: 0.00069687
Iteration 13/25 | Loss: 0.00069687
Iteration 14/25 | Loss: 0.00069687
Iteration 15/25 | Loss: 0.00069687
Iteration 16/25 | Loss: 0.00069687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006968665984459221, 0.0006968665984459221, 0.0006968665984459221, 0.0006968665984459221, 0.0006968665984459221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006968665984459221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069687
Iteration 2/1000 | Loss: 0.00004960
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002829
Iteration 5/1000 | Loss: 0.00002715
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00002557
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002396
Iteration 12/1000 | Loss: 0.00002376
Iteration 13/1000 | Loss: 0.00002359
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002329
Iteration 17/1000 | Loss: 0.00002320
Iteration 18/1000 | Loss: 0.00002318
Iteration 19/1000 | Loss: 0.00002312
Iteration 20/1000 | Loss: 0.00002312
Iteration 21/1000 | Loss: 0.00002296
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002278
Iteration 24/1000 | Loss: 0.00002263
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002247
Iteration 27/1000 | Loss: 0.00002231
Iteration 28/1000 | Loss: 0.00002229
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002224
Iteration 32/1000 | Loss: 0.00002224
Iteration 33/1000 | Loss: 0.00002222
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002221
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002220
Iteration 43/1000 | Loss: 0.00002220
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00002219
Iteration 47/1000 | Loss: 0.00002219
Iteration 48/1000 | Loss: 0.00002219
Iteration 49/1000 | Loss: 0.00002219
Iteration 50/1000 | Loss: 0.00002219
Iteration 51/1000 | Loss: 0.00002219
Iteration 52/1000 | Loss: 0.00002219
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002219
Iteration 57/1000 | Loss: 0.00002219
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002218
Iteration 60/1000 | Loss: 0.00002218
Iteration 61/1000 | Loss: 0.00002218
Iteration 62/1000 | Loss: 0.00002218
Iteration 63/1000 | Loss: 0.00002217
Iteration 64/1000 | Loss: 0.00002217
Iteration 65/1000 | Loss: 0.00002217
Iteration 66/1000 | Loss: 0.00002217
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00002217
Iteration 70/1000 | Loss: 0.00002217
Iteration 71/1000 | Loss: 0.00002216
Iteration 72/1000 | Loss: 0.00002216
Iteration 73/1000 | Loss: 0.00002216
Iteration 74/1000 | Loss: 0.00002216
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00002216
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002215
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002215
Iteration 81/1000 | Loss: 0.00002215
Iteration 82/1000 | Loss: 0.00002215
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00002215
Iteration 85/1000 | Loss: 0.00002215
Iteration 86/1000 | Loss: 0.00002214
Iteration 87/1000 | Loss: 0.00002214
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002214
Iteration 91/1000 | Loss: 0.00002213
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002212
Iteration 96/1000 | Loss: 0.00002212
Iteration 97/1000 | Loss: 0.00002211
Iteration 98/1000 | Loss: 0.00002211
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002211
Iteration 101/1000 | Loss: 0.00002211
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00002211
Iteration 104/1000 | Loss: 0.00002211
Iteration 105/1000 | Loss: 0.00002210
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002210
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002210
Iteration 111/1000 | Loss: 0.00002210
Iteration 112/1000 | Loss: 0.00002210
Iteration 113/1000 | Loss: 0.00002210
Iteration 114/1000 | Loss: 0.00002209
Iteration 115/1000 | Loss: 0.00002209
Iteration 116/1000 | Loss: 0.00002209
Iteration 117/1000 | Loss: 0.00002209
Iteration 118/1000 | Loss: 0.00002209
Iteration 119/1000 | Loss: 0.00002209
Iteration 120/1000 | Loss: 0.00002209
Iteration 121/1000 | Loss: 0.00002208
Iteration 122/1000 | Loss: 0.00002208
Iteration 123/1000 | Loss: 0.00002208
Iteration 124/1000 | Loss: 0.00002208
Iteration 125/1000 | Loss: 0.00002207
Iteration 126/1000 | Loss: 0.00002207
Iteration 127/1000 | Loss: 0.00002207
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002207
Iteration 130/1000 | Loss: 0.00002206
Iteration 131/1000 | Loss: 0.00002206
Iteration 132/1000 | Loss: 0.00002206
Iteration 133/1000 | Loss: 0.00002206
Iteration 134/1000 | Loss: 0.00002206
Iteration 135/1000 | Loss: 0.00002206
Iteration 136/1000 | Loss: 0.00002206
Iteration 137/1000 | Loss: 0.00002206
Iteration 138/1000 | Loss: 0.00002205
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002205
Iteration 141/1000 | Loss: 0.00002205
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002205
Iteration 144/1000 | Loss: 0.00002204
Iteration 145/1000 | Loss: 0.00002204
Iteration 146/1000 | Loss: 0.00002204
Iteration 147/1000 | Loss: 0.00002204
Iteration 148/1000 | Loss: 0.00002204
Iteration 149/1000 | Loss: 0.00002204
Iteration 150/1000 | Loss: 0.00002204
Iteration 151/1000 | Loss: 0.00002204
Iteration 152/1000 | Loss: 0.00002204
Iteration 153/1000 | Loss: 0.00002204
Iteration 154/1000 | Loss: 0.00002204
Iteration 155/1000 | Loss: 0.00002204
Iteration 156/1000 | Loss: 0.00002204
Iteration 157/1000 | Loss: 0.00002204
Iteration 158/1000 | Loss: 0.00002204
Iteration 159/1000 | Loss: 0.00002204
Iteration 160/1000 | Loss: 0.00002204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.2043303033569828e-05, 2.2043303033569828e-05, 2.2043303033569828e-05, 2.2043303033569828e-05, 2.2043303033569828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2043303033569828e-05

Optimization complete. Final v2v error: 3.7987520694732666 mm

Highest mean error: 4.134485721588135 mm for frame 195

Lowest mean error: 3.6531169414520264 mm for frame 176

Saving results

Total time: 52.60280132293701
